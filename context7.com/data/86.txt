TITLE: Full Pipeline for Training and Evaluating ML Model
DESCRIPTION: Provides a complete Python script for training and evaluating a machine learning model using combined categorical and numerical features. It includes setting up the feature matrix (X) and target vector (y), splitting data into training and testing sets, defining a pipeline with polynomial features and linear regression, training the model, making predictions, and calculating evaluation metrics (Mean Squared Error and Model Determination).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/3-Linear/README.md#_snippet_17

LANGUAGE: python
CODE:
```
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

----------------------------------------

TITLE: Full Regression Pipeline with Combined Features Python
DESCRIPTION: This comprehensive code snippet outlines the complete process for training and evaluating a Polynomial Regression model. It starts by preparing the feature matrix `X` and target vector `y` by combining one-hot encoded categorical features ('Variety', 'City', 'Package') and a numeric feature ('Month'). It then splits the data into training and testing sets, defines a pipeline combining polynomial feature transformation and linear regression, trains the model, makes predictions on the test set, and finally calculates and prints the Mean Squared Error (MSE) and the R-squared determination score.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/README.md#_snippet_18

LANGUAGE: python
CODE:
```
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

----------------------------------------

TITLE: Splitting Data into Training and Test Sets - Python
DESCRIPTION: Splits the feature data (X) and target data (y) into separate training and testing sets using model_selection.train_test_split. This is a standard practice to evaluate the model's performance on unseen data. test_size=0.33 allocates 33% of the data to the test set.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/2-Regression/1-Tools/README.md#_snippet_3

LANGUAGE: python
CODE:
```
X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33)
```

----------------------------------------

TITLE: Split Data for Training and Testing Python Scikit-learn
DESCRIPTION: This code splits the feature data (X) and target data (y) into separate training and testing sets using train_test_split from Scikit-learn. 20% of the data is reserved for testing, and random_state is set for reproducibility.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/2-Regression/3-Linear/README.md#_snippet_7

LANGUAGE: python
CODE:
```
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

----------------------------------------

TITLE: Training a Linear Regression Model (Scikit-learn, Python)
DESCRIPTION: Initializes a `LinearRegression` model from scikit-learn's linear_model module. It then trains this model using the training data (`X_train` and `y_train`) via the `fit()` method. The `fit()` method finds the coefficients and intercept that best fit the linear relationship between the single selected feature (BMI) and the target (disease progression) in the training data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/2-Regression/1-Tools/README.md#_snippet_3

LANGUAGE: python
CODE:
```
model = linear_model.LinearRegression()
model.fit(X_train, y_train)
```

----------------------------------------

TITLE: Splitting Data into Training and Testing Sets
DESCRIPTION: This code splits the prepared feature (X) and label (y) data into training and testing sets using `train_test_split`. The test set size is specified as 20% (0.2) of the data, and a `random_state` is set for reproducibility.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/2-Regression/3-Linear/README.md#_snippet_7

LANGUAGE: python
CODE:
```
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

----------------------------------------

TITLE: Splitting Data into Train/Test Sets (Python)
DESCRIPTION: Splits the feature matrix `X` and target variable `y` into training and testing sets. 30% of the data is allocated to the test set (`test_size=0.3`) for evaluating the model's performance on unseen data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/solution/notebook.ipynb#_snippet_6

LANGUAGE: Python
CODE:
```
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)
```

----------------------------------------

TITLE: Splitting Data - Scikit-learn - Python
DESCRIPTION: Divides the feature dataframe (`cuisines_feature_df`) and label dataframe (`cuisines_label_df`) into training and testing sets using `train_test_split`. 30% of the data is allocated for the test set, a standard practice to evaluate model generalization.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/4-Classification/3-Classifiers-2/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
```

----------------------------------------

TITLE: Splitting Data into Train and Test Sets (Python)
DESCRIPTION: This snippet prepares the data for machine learning by separating the features (X) from the target variable (y). It selects all columns except 'Color' as features and 'Color' as the target. It then uses `sklearn.model_selection.train_test_split` to divide the data into 80% for training and 20% for testing, ensuring reproducibility with `random_state=0`. Requires the `encoded_pumpkins` DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/2-Regression/4-Logistic/README.md#_snippet_2

LANGUAGE: python
CODE:
```
from sklearn.model_selection import train_test_split

X = encoded_pumpkins[encoded_pumpkins.columns.difference(['Color'])]
y = encoded_pumpkins['Color']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

----------------------------------------

TITLE: Splitting Data into Training and Testing Sets (Python)
DESCRIPTION: Imports the `train_test_split` function from scikit-learn's `model_selection`. It defines the feature columns ('Seconds', 'Latitude', 'Longitude') as X and the target column ('Country') as y. The data is then split into training (80%) and testing (20%) sets for both features and the target variable using `train_test_split`, with a fixed random state for reproducibility. Requires scikit-learn.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/3-Web-App/1-Web-App/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
from sklearn.model_selection import train_test_split

Selected_features = ['Seconds','Latitude','Longitude']

X = ufos[Selected_features]
y = ufos['Country']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

----------------------------------------

TITLE: Combining and Applying Encoders with ColumnTransformer (Python)
DESCRIPTION: This snippet imports ColumnTransformer from scikit-learn, creates an instance ct to apply the previously defined OrdinalEncoder and OneHotEncoder to their respective columns, sets the output format to pandas DataFrame, and fits and transforms the pumpkins DataFrame to produce the encoded features.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/README.md#_snippet_5

LANGUAGE: python
CODE:
```
from sklearn.compose import ColumnTransformer

ct = ColumnTransformer(transformers=[
    ('ord', ordinal_encoder, ordinal_features),
    ('cat', categorical_encoder, categorical_features)
    ])

ct.set_output(transform='pandas')
encoded_features = ct.fit_transform(pumpkins)
```

----------------------------------------

TITLE: Load Data and Inspect Head with Pandas (Python)
DESCRIPTION: Loads the US-pumpkins.csv dataset into a Pandas DataFrame. It then displays the first five rows of the DataFrame using the `head()` function to provide an initial view of the data structure and content. Requires the pandas library.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/README.md#_snippet_0

LANGUAGE: python
CODE:
```
import pandas as pd
pumpkins = pd.read_csv('../data/US-pumpkins.csv')
pumpkins.head()
```

----------------------------------------

TITLE: Install Python Dependencies via requirements.txt
DESCRIPTION: Executes the pip package manager to read the list of required Python packages from the `requirements.txt` file and install them. This command ensures all libraries specified in the requirements file are installed, making them available for the Flask application.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/3-Web-App/1-Web-App/README.md#_snippet_2

LANGUAGE: Bash
CODE:
```
pip install -r requirements.txt
```

----------------------------------------

TITLE: Implementing Ingredient Selection and ONNX Inference Logic in JavaScript
DESCRIPTION: Contains JavaScript code to manage the state of selected ingredients in an array based on checkbox input. It adds event listeners to checkboxes to update the ingredient array and implements the `startInference` function, which validates input, loads the ONNX model (`model.onnx`) asynchronously using `ort.InferenceSession.create`, prepares the input data as an ONNX Tensor, runs the model session with the input feeds, and displays the predicted cuisine label from the results.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/4-Classification/4-Applied/README.md#_snippet_3

LANGUAGE: javascript
CODE:
```
const ingredients = Array(380).fill(0);
        
        const checks = [...document.querySelectorAll('.checkbox')];
        
        checks.forEach(check => {
            check.addEventListener('change', function() {
                // toggle the state of the ingredient
                // based on the checkbox's value (1 or 0)
                ingredients[check.value] = check.checked ? 1 : 0;
            });
        });

        function testCheckboxes() {
            // validate if at least one checkbox is checked
            return checks.some(check => check.checked);
        }

        async function startInference() {

            let atLeastOneChecked = testCheckboxes()

            if (!atLeastOneChecked) {
                alert('Please select at least one ingredient.');
                return;
            }
            try {
                // create a new session and load the model.
                
                const session = await ort.InferenceSession.create('./model.onnx');

                const input = new ort.Tensor(new Float32Array(ingredients), [1, 380]);
                const feeds = { float_input: input };

                // feed inputs and run
                const results = await session.run(feeds);

                // read from results
                alert('You can enjoy ' + results.label.data[0] + ' cuisine today!')

            } catch (e) {
                console.log(`failed to inference ONNX model`);
                console.error(e);
            }
        }
               
    
```

----------------------------------------

TITLE: Create Tag Columns using Pandas
DESCRIPTION: This code snippet creates new binary columns in a pandas DataFrame based on the presence of specific 'useful' tags in the original 'Tags' column. It uses the `.apply()` method with lambda functions to check for string containment and assign a value of 1 if the tag is present, otherwise 0. It also handles combining related tags like 'Group' and 'Travelers with friends'.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_0

LANGUAGE: python
CODE:
```
# Process the Tags into new columns
# The file Hotel_Reviews_Tags.py, identifies the most important tags
# Leisure trip, Couple, Solo traveler, Business trip, Group combined with Travelers with friends,
# Family with young children, Family with older children, With a pet
df["Leisure_trip"] = df.Tags.apply(lambda tag: 1 if "Leisure trip" in tag else 0)
df["Couple"] = df.Tags.apply(lambda tag: 1 if "Couple" in tag else 0)
df["Solo_traveler"] = df.Tags.apply(lambda tag: 1 if "Solo traveler" in tag else 0)
df["Business_trip"] = df.Tags.apply(lambda tag: 1 if "Business trip" in tag else 0)
df["Group"] = df.Tags.apply(lambda tag: 1 if "Group" in tag or "Travelers with friends" in tag else 0)
df["Family_with_young_children"] = df.Tags.apply(lambda tag: 1 if "Family with young children" in tag else 0)
df["Family_with_older_children"] = df.Tags.apply(lambda tag: 1 if "Family with older children" in tag else 0)
df["With_a_pet"] = df.Tags.apply(lambda tag: 1 if "With a pet" in tag else 0)

```

----------------------------------------

TITLE: Initializing and Training Linear Regression Model with scikit-learn (Python)
DESCRIPTION: Initializes a `LinearRegression` model from `sklearn.linear_model`. Trains the model using the `fit()` method, providing the training feature data (`X_train`) and training target data (`y_train`). This step calculates the optimal coefficients for the linear model based on the training data. Requires scikit-learn's `linear_model` module.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/2-Regression/1-Tools/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
model = linear_model.LinearRegression()
model.fit(X_train, y_train)
```

----------------------------------------

TITLE: Creating Tag Features in Pandas DataFrame Python
DESCRIPTION: Adds new binary columns to a Pandas DataFrame ('df') based on the presence of specific string tags within the existing 'Tags' column. Each new column corresponds to a tag, containing 1 if the tag is present in the row's 'Tags' value, and 0 otherwise. It specifically combines 'Group' and 'Travelers with friends' into a single 'Group' column.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
# Process the Tags into new columns
# The file Hotel_Reviews_Tags.py, identifies the most important tags
# Leisure trip, Couple, Solo traveler, Business trip, Group combined with Travelers with friends, 
# Family with young children, Family with older children, With a pet
df["Leisure_trip"] = df.Tags.apply(lambda tag: 1 if "Leisure trip" in tag else 0)
df["Couple"] = df.Tags.apply(lambda tag: 1 if "Couple" in tag else 0)
df["Solo_traveler"] = df.Tags.apply(lambda tag: 1 if "Solo traveler" in tag else 0)
df["Business_trip"] = df.Tags.apply(lambda tag: 1 if "Business trip" in tag else 0)
df["Group"] = df.Tags.apply(lambda tag: 1 if "Group" in tag or "Travelers with friends" in tag else 0)
df["Family_with_young_children"] = df.Tags.apply(lambda tag: 1 if "Family with young children" in tag else 0)
df["Family_with_older_children"] = df.Tags.apply(lambda tag: 1 if "Family with older children" in tag else 0)
df["With_a_pet"] = df.Tags.apply(lambda tag: 1 if "With a pet" in tag else 0)
```

----------------------------------------

TITLE: Print Classification Report Python
DESCRIPTION: Generates and prints a classification report comparing the true test labels (`y_test`) and the predicted labels (`y_pred`). This report includes key metrics like precision, recall, f1-score, and support for each class, as well as overall accuracy.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/4-Classification/4-Applied/README.md#_snippet_8

LANGUAGE: python
CODE:
```
print(classification_report(y_test,y_pred))
```

----------------------------------------

TITLE: Importing Core Libraries Python
DESCRIPTION: Imports essential Python libraries required for numerical operations (numpy), plotting (matplotlib), random number generation, and mathematical functions used throughout the tutorial for environment setup, calculations, and visualization.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/notebook.ipynb#_snippet_0

LANGUAGE: Python
CODE:
```
import matplotlib.pyplot as plt
import numpy as np
import random
import math
```

----------------------------------------

TITLE: Training and Evaluating Logistic Regression Model (Python)
DESCRIPTION: Trains a logistic regression model using `sklearn.linear_model.LogisticRegression` on the training data (`X_train`, `y_train`). It then makes predictions on the test data (`X_test`). Finally, it evaluates the model's performance by printing the `classification_report` (precision, recall, f1-score, support, accuracy) and the `f1_score` using the true test labels (`y_test`) and predicted labels (`predictions`). Requires Scikit-learn. Expects trained/tested data splits from `train_test_split`. Outputs evaluation metrics and predictions.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/4-Logistic/README.md#_snippet_3

LANGUAGE: python
CODE:
```
from sklearn.metrics import f1_score, classification_report 
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)

print(classification_report(y_test, predictions))
print('Predicted labels: ', predictions)
print('F1-score: ', f1_score(y_test, predictions))
```

----------------------------------------

TITLE: Extracting Labels with Pandas Python
DESCRIPTION: This snippet selects the 'cuisine' column from the main DataFrame to create a dedicated Series for the target labels. It then displays the head of this new Series to confirm the extraction. This step is crucial for separating the dependent variable from the independent features for model training.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/solution/notebook.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
cuisines_label_df = cuisines_df['cuisine']
cuisines_label_df.head()
```

----------------------------------------

TITLE: Training Polynomial Regression Pipeline with Multiple Features (Python)
DESCRIPTION: Provides a complete machine learning workflow using scikit-learn. It prepares the feature data `X` by combining multiple one-hot encoded categorical and numeric features, splits the data into training and testing sets, defines a pipeline that first applies polynomial features (degree 2) and then linear regression, trains the pipeline model, makes predictions on the test set, and evaluates the model's performance using the Mean Squared Error (MSE) and determination score (R-squared).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/sw/2-Regression/3-Linear/README.md#_snippet_3

LANGUAGE: python
CODE:
```
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

----------------------------------------

TITLE: Loading UFO Data using Pandas Python
DESCRIPTION: This snippet imports the pandas and numpy libraries and loads the UFO sightings data from a CSV file into a pandas DataFrame. It then displays the first few rows of the DataFrame to inspect the data structure and content. Requires pandas and a 'ufos.csv' file in the '../data/' directory.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/3-Web-App/1-Web-App/solution/notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import pandas as pd
import numpy as np

ufos = pd.read_csv('../data/ufos.csv')
ufos.head()
```

----------------------------------------

TITLE: Training Logistic Regression and Reporting Metrics Scikit-learn Python
DESCRIPTION: This snippet trains a Logistic Regression model using the training data (X_train, y_train), makes predictions on the test data (X_test), and prints the classification report and F1-score. The classification report provides precision, recall, f1-score, and support for each class. It requires `LogisticRegression` from `sklearn.linear_model` and `f1_score`, `classification_report` from `sklearn.metrics`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/2-Regression/4-Logistic/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
    from sklearn.metrics import f1_score, classification_report 
    from sklearn.linear_model import LogisticRegression

    model = LogisticRegression()
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)

    print(classification_report(y_test, predictions))
    print('Predicted labels: ', predictions)
    print('F1-score: ', f1_score(y_test, predictions))
```

----------------------------------------

TITLE: Training and Evaluating Classifiers Python
DESCRIPTION: This snippet iterates through the dictionary of predefined classifiers, trains each model on the training data (`X_train`, `y_train`), makes predictions on the test data (`X_test`), and then evaluates the model's performance. It calculates and prints the accuracy score and a detailed classification report for each classifier, providing insights into their effectiveness.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/solution/notebook.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
n_classifiers = len(classifiers)

for index, (name, classifier) in enumerate(classifiers.items()):
    classifier.fit(X_train, np.ravel(y_train))

    y_pred = classifier.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print("Accuracy (train) for %s: %0.1f%% " % (name, accuracy * 100))
    print(classification_report(y_test,y_pred))
```

----------------------------------------

TITLE: Training and Evaluating Logistic Regression Model (Python, Scikit-learn)
DESCRIPTION: This code initializes a `LogisticRegression` model from scikit-learn's `linear_model` module with `multi_class='ovr'` and `solver='liblinear'` for multiclass classification. It then trains the model (`lr.fit`) using the training data (`X_train`, `y_train`), flattening the labels using `np.ravel` (requires NumPy). Finally, it evaluates the model's overall accuracy on the test set (`X_test`, `y_test`) using `model.score` and prints the result. Requires Scikit-learn and NumPy.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/README.md#_snippet_5

LANGUAGE: python
CODE:
```
lr = LogisticRegression(multi_class='ovr',solver='liblinear')
model = lr.fit(X_train, np.ravel(y_train))

accuracy = model.score(X_test, y_test)
print ("Accuracy is {}".format(accuracy))
```

----------------------------------------

TITLE: Train SVC Classification Model Python
DESCRIPTION: Initializes a Support Vector Classifier (SVC) model with a linear kernel, a regularization parameter `C` of 10, and enables probability estimates. The model is then trained using the prepared training data (`X_train`) and corresponding target labels (`y_train`).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/README.md#_snippet_6

LANGUAGE: python
CODE:
```
model = SVC(kernel='linear', C=10, probability=True,random_state=0)
model.fit(X_train,y_train.values.ravel())
```

----------------------------------------

TITLE: Training and Evaluating Logistic Regression Model (Python)
DESCRIPTION: This snippet builds, trains, and evaluates a logistic regression model for binary classification. It initializes the `LogisticRegression` model, fits it to the training data (`X_train`, `y_train`), and generates predictions on the test features (`X_test`). Finally, it prints a comprehensive classification report and the F1-score to assess the model's performance. Requires the training and test data splits.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/2-Regression/4-Logistic/README.md#_snippet_3

LANGUAGE: python
CODE:
```
from sklearn.metrics import f1_score, classification_report 
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)

print(classification_report(y_test, predictions))
print('Predicted labels: ', predictions)
print('F1-score: ', f1_score(y_test, predictions))
```

----------------------------------------

TITLE: Splitting Data into Training and Test Sets in Python
DESCRIPTION: Divides the input features `X` and target variable `y` into training and testing sets using `train_test_split`. 20% of the data is allocated to the test set (`test_size=0.2`), and `random_state=0` ensures reproducible splits.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/2-Regression/3-Linear/README.md#_snippet_7

LANGUAGE: python
CODE:
```
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

----------------------------------------

TITLE: Splitting Data into Training and Test Sets (Python)
DESCRIPTION: This snippet splits the feature matrix (X) and target vector (y) into training and testing sets. It uses train_test_split from scikit-learn, allocating 20% of the data for testing (test_size=0.2) and ensuring reproducibility with a fixed random state (random_state=0). Dependencies: train_test_split from sklearn.model_selection, prepared feature X and target y.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/2-Regression/3-Linear/README.md#_snippet_7

LANGUAGE: python
CODE:
```
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

----------------------------------------

TITLE: Installing Imblearn Package (Shell)
DESCRIPTION: Installs the imbalanced-learn library, which provides tools like SMOTE to handle imbalanced datasets. This command is executed from the shell or terminal.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/notebook.ipynb#_snippet_0

LANGUAGE: Shell
CODE:
```
pip install imblearn
```

----------------------------------------

TITLE: Importing Scikit-learn Modules for Regression in Python
DESCRIPTION: This code imports necessary classes and functions from the Scikit-learn library required for training and evaluating linear regression models: LinearRegression for the model, mean_squared_error for performance metrics, and train_test_split for data partitioning.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/README.md#_snippet_6

LANGUAGE: Python
CODE:
```
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

----------------------------------------

TITLE: Training and Evaluating Logistic Regression Model Python
DESCRIPTION: This snippet imports necessary modules from scikit-learn, including `LogisticRegression` and evaluation metrics. It initializes a Logistic Regression model, trains it using the training data (X_train, y_train), and generates predictions on the test data (X_test). Finally, it prints the classification report, the predicted labels, and the accuracy score to evaluate the model's performance. Requires scikit-learn and the training/testing data split in the previous step.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/3-Web-App/1-Web-App/solution/notebook.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report 
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)

print(classification_report(y_test, predictions))
print('Predicted labels: ', predictions)
print('Accuracy: ', accuracy_score(y_test, predictions))
```

----------------------------------------

TITLE: Implementing the Flask Application (Python)
DESCRIPTION: Contains the core Python logic for the web server. It initializes a Flask app, loads the pre-trained ML model using `pickle`, defines routes for the home page (`/`) and prediction handling (`/predict`), processes form data, makes predictions using the loaded model, maps the output code to a country name, and renders the HTML template with the prediction result. The `debug=True` flag is enabled for development.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/3-Web-App/1-Web-App/README.md#_snippet_5

LANGUAGE: python
CODE:
```
import numpy as np
from flask import Flask, request, render_template
import pickle

app = Flask(__name__)

model = pickle.load(open("./ufo-model.pkl", "rb"))


@app.route("/")
def home():
    return render_template("index.html")


@app.route("/predict", methods=["POST"])
def predict():

    int_features = [int(x) for x in request.form.values()]
    final_features = [np.array(int_features)]
    prediction = model.predict(final_features)

    output = prediction[0]

    countries = ["Australia", "Canada", "Germany", "UK", "US"]

    return render_template(
        "index.html", prediction_text="Likely country: {}".format(countries[output])
    )


if __name__ == "__main__":
    app.run(debug=True)
```

----------------------------------------

TITLE: Training Logistic Regression Model and Evaluating - Python
DESCRIPTION: This code imports `LogisticRegression`, `f1_score`, and `classification_report` from scikit-learn. It initializes and trains a logistic regression model on the training data (`X_train`, `y_train`), makes predictions on the test data (`X_test`), and prints the classification report and F1-score to evaluate the model's performance.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/notebook.ipynb#_snippet_13

LANGUAGE: python
CODE:
```
from sklearn.metrics import f1_score, classification_report
from sklearn.linear_model import LogisticRegression

# Train a logistic regression model on the pumpkin dataset
model = LogisticRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)

# Evaluate the model and print the results
print(classification_report(y_test, predictions))
print('Predicted labels: ', predictions)
print('F1-score: ', f1_score(y_test, predictions))
```

----------------------------------------

TITLE: Visualizing cuisine data distribution using Pandas/Matplotlib
DESCRIPTION: Calculates the frequency of each cuisine type in the dataset and visualizes the distribution as a horizontal bar chart. This helps identify the class distribution and highlights potential data imbalance issues before model training.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/README.md#_snippet_5

LANGUAGE: python
CODE:
```
df.cuisine.value_counts().plot.barh()
```

----------------------------------------

TITLE: Visualizing Data Distribution with Boxplots (Python)
DESCRIPTION: Generates boxplots for selected numerical columns of a pandas DataFrame to visualize the distribution of values and identify potential outliers in the music dataset features before clustering. It creates a figure with multiple subplots, each displaying a boxplot for a specific feature column. Requires `matplotlib.pyplot` and `seaborn`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/de/5-Clustering/2-K-Means/README.md#_snippet_0

LANGUAGE: python
CODE:
```
plt.figure(figsize=(20,20), dpi=200)

plt.subplot(4,3,1)
sns.boxplot(x = 'popularity', data = df)

plt.subplot(4,3,2)
sns.boxplot(x = 'acousticness', data = df)

plt.subplot(4,3,3)
sns.boxplot(x = 'energy', data = df)

plt.subplot(4,3,4)
sns.boxplot(x = 'instrumentalness', data = df)

plt.subplot(4,3,5)
sns.boxplot(x = 'liveness', data = df)

plt.subplot(4,3,6)
sns.boxplot(x = 'loudness', data = df)

plt.subplot(4,3,7)
sns.boxplot(x = 'speechiness', data = df)

plt.subplot(4,3,8)
sns.boxplot(x = 'tempo', data = df)

plt.subplot(4,3,9)
sns.boxplot(x = 'time_signature', data = df)

plt.subplot(4,3,10)
sns.boxplot(x = 'danceability', data = df)

plt.subplot(4,3,11)
sns.boxplot(x = 'length', data = df)

plt.subplot(4,3,12)
sns.boxplot(x = 'release_date', data = df)
```

----------------------------------------

TITLE: Evaluate Linear Regression Model RMSE Python Scikit-learn Numpy
DESCRIPTION: This code makes predictions on the test feature data (X_test) using the trained linear regression model. It then calculates the Root Mean Squared Error (RMSE) between the actual test targets (y_test) and the predictions (pred), and prints the RMSE along with its percentage relative to the mean prediction.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/2-Regression/3-Linear/README.md#_snippet_9

LANGUAGE: python
CODE:
```
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

----------------------------------------

TITLE: Installing Seaborn Library (Python)
DESCRIPTION: Installs the Seaborn data visualization library using pip. This library is a dependency for creating statistical plots and visualizations and is required for subsequent steps.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
pip install seaborn
```

----------------------------------------

TITLE: Splitting Data for Model Training and Testing - Python
DESCRIPTION: This code uses the `train_test_split` function to divide the feature array `X` and the target array `y` into training and testing sets. `test_size=0.2` allocates 20% of the data for testing, and `random_state=0` ensures reproducibility of the split. This separation allows for unbiased model evaluation after training.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/2-Regression/3-Linear/README.md#_snippet_7

LANGUAGE: python
CODE:
```
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

----------------------------------------

TITLE: Evaluating Linear Regression Model with MSE and Printing Results Python
DESCRIPTION: This snippet uses the trained `lin_reg` model to make predictions on the test feature data `X_test` using the `predict()` method. It then calculates the Root Mean Squared Error (RMSE) by taking the square root of the `mean_squared_error` between the true test labels `y_test` and the predictions `pred`. Finally, it prints the RMSE and the percentage error relative to the mean predicted value.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/3-Linear/README.md#_snippet_9

LANGUAGE: python
CODE:
```
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

----------------------------------------

TITLE: Training Q-Learning Algorithm Python
DESCRIPTION: This Python snippet implements the core Q-learning training loop for an OpenAI Gym environment like CartPole. It includes functions to convert Q-values to action probabilities, handles exploration vs. exploitation using epsilon-greedy strategy, updates the Q-table using the Bellman equation, collects cumulative rewards, and periodically saves the Q-table associated with the maximum average cumulative reward observed so far. It requires dependencies like numpy, random, and the gym environment.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/8-Reinforcement/2-Gym/README.md#_snippet_0

LANGUAGE: python
CODE:
```
def probs(v,eps=1e-4):
    v = v-v.min()+eps
    v = v/v.sum()
    return v

Qmax = 0
cum_rewards = []
rewards = []
for epoch in range(100000):
    obs = env.reset()
    done = False
    cum_reward=0
    # == do the simulation ==
    while not done:
        s = discretize(obs)
        if random.random()<epsilon:
            # exploitation - chose the action according to Q-Table probabilities
            v = probs(np.array(qvalues(s)))
            a = random.choices(actions,weights=v)[0]
        else:
            # exploration - randomly chose the action
            a = np.random.randint(env.action_space.n)

        obs, rew, done, info = env.step(a)
        cum_reward+=rew
        ns = discretize(obs)
        Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
    cum_rewards.append(cum_reward)
    rewards.append(cum_reward)
    # == Periodically print results and calculate average reward ==
    if epoch%5000==0:
        print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
        if np.average(cum_rewards) > Qmax:
            Qmax = np.average(cum_rewards)
            Qbest = Q
        cum_rewards=[]
```

----------------------------------------

TITLE: Making Predictions with Trained Model - Python
DESCRIPTION: Uses the trained linear regression model (model) to make predictions on the test set features (X_test). The predict() method applies the learned relationship to the unseen data, generating predicted target values (y_pred).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/2-Regression/1-Tools/README.md#_snippet_5

LANGUAGE: python
CODE:
```
y_pred = model.predict(X_test)
```

----------------------------------------

TITLE: Calculating Confusion Matrix (Python)
DESCRIPTION: This snippet calculates the confusion matrix to provide a detailed breakdown of the model's performance in terms of true positives, true negatives, false positives, and false negatives. It uses `sklearn.metrics.confusion_matrix`, taking the true test labels (`y_test`) and the model's predictions (`predictions`) as input. Requires the true labels and the model predictions from the test set.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/2-Regression/4-Logistic/README.md#_snippet_4

LANGUAGE: python
CODE:
```
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, predictions)
```

----------------------------------------

TITLE: Generating Confusion Matrix with Scikit-learn (Python)
DESCRIPTION: Calculates the confusion matrix for the model's predictions using `sklearn.metrics.confusion_matrix`. This matrix shows the counts of true positives, true negatives, false positives, and false negatives, providing a detailed view of the classification model's performance. Requires Scikit-learn. Expects true test labels (`y_test`) and predicted labels (`predictions`). Outputs a 2x2 NumPy array representing the confusion matrix.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/4-Logistic/README.md#_snippet_4

LANGUAGE: python
CODE:
```
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, predictions)
```

----------------------------------------

TITLE: Handling Missing Values and Checking Info in Pandas DataFrame
DESCRIPTION: Removes rows containing any missing values (NaN) from the `pie_pumpkins` DataFrame using the `dropna()` method with `inplace=True`. It then prints a concise summary of the DataFrame using `info()`, including the index dtype and columns, non-null values, and memory usage.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/2-Regression/3-Linear/README.md#_snippet_4

LANGUAGE: python
CODE:
```
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

----------------------------------------

TITLE: Import Scikit-learn Modules Python
DESCRIPTION: Imports various necessary modules from the scikit-learn library for machine learning model development. This includes functions for splitting data into training and testing sets, the Support Vector Classifier (SVC) model class, and several metrics for evaluating model performance like accuracy and a classification report.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/README.md#_snippet_4

LANGUAGE: python
CODE:
```
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report
```

----------------------------------------

TITLE: Train Logistic Regression Model and Evaluate Accuracy Python
DESCRIPTION: This snippet initializes a Logistic Regression model with 'ovr' multi-class strategy and 'liblinear' solver, trains it using the training data, and then evaluates its accuracy on the test set using the `score` method. The target variable `y_train` is reshaped using `np.ravel` as required by the model.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/solution/notebook.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
lr = LogisticRegression(multi_class='ovr',solver='liblinear')
model = lr.fit(X_train, np.ravel(y_train))

accuracy = model.score(X_test, y_test)
print ("Accuracy is {}".format(accuracy))
```

----------------------------------------

TITLE: Splitting Data into Training and Testing Sets (Python, Scikit-learn)
DESCRIPTION: This snippet uses `train_test_split` from scikit-learn's `model_selection` module to divide the feature and label dataframes (`cuisines_feature_df`, `cuisines_label_df`) into training and testing sets. It sets the test set size to 30% (`test_size=0.3`). The function returns four arrays/dataframes: features for training (`X_train`), features for testing (`X_test`), labels for training (`y_train`), and labels for testing (`y_test`). Requires Scikit-learn.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/README.md#_snippet_4

LANGUAGE: python
CODE:
```
X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
```

----------------------------------------

TITLE: Training Linear Regression Model (Scikit-learn) - Python
DESCRIPTION: This snippet imports necessary modules, splits the data into training and testing sets, instantiates a `LinearRegression` model, trains it on the training data, makes predictions on the test data, and calculates the model's accuracy (R-squared score) on the training set.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/translations/README.ko.md#_snippet_6

LANGUAGE: Python
CODE:
```
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)

pred = lin_reg.predict(X_test)

accuracy_score = lin_reg.score(X_train,y_train)
print('Model Accuracy: ', accuracy_score)
```

----------------------------------------

TITLE: Splitting Data into Train/Test Sets - Scikit-learn Python
DESCRIPTION: This snippet uses `train_test_split` from `sklearn.model_selection` to partition the feature data (`X`) and target data (`y`) into training and testing subsets. The `test_size=0.33` parameter allocates 33% of the data to the test set and the remaining 67% to the training set. This prepares the data for model training and evaluation. Requires `sklearn.model_selection`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/README.md#_snippet_4

LANGUAGE: python
CODE:
```
X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33)
```

----------------------------------------

TITLE: Evaluating Linear Regression Model Performance - Python
DESCRIPTION: This code uses the trained `lin_reg` model to make predictions (`pred`) on the test feature data (`X_test`). It then calculates the Root Mean Squared Error (RMSE) using `mean_squared_error` and `np.sqrt`, comparing the predictions (`pred`) to the actual test targets (`y_test`). Finally, it prints the RMSE and the percentage error relative to the mean prediction.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/2-Regression/3-Linear/README.md#_snippet_9

LANGUAGE: python
CODE:
```
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

----------------------------------------

TITLE: Selecting Columns and Handling Missing Data - Python
DESCRIPTION: This code selects a subset of relevant columns from the full dataset using pandas `.loc` and removes rows containing any missing values (`NaN`) using `.dropna(inplace=True)`. It then displays the head of the resulting DataFrame to verify the changes.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/notebook.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
# Select the columns we want to use
columns_to_select = ['City Name','Package','Variety', 'Origin','Item Size', 'Color']
pumpkins = full_pumpkins.loc[:, columns_to_select]

# Drop rows with missing values
pumpkins.dropna(inplace=True)

pumpkins.head()
```

----------------------------------------

TITLE: Implement Q-Learning Training Loop - Python
DESCRIPTION: Contains the main training loop for the Q-Learning agent over a specified number of epochs. In each epoch, it resets the environment, runs a simulation episode using an epsilon-greedy policy (balancing exploration and exploitation based on `epsilon`), updates the Q-Table using the Q-Learning update rule, and tracks cumulative rewards for plotting and reporting.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/solution/notebook.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
def probs(v,eps=1e-4):
    v = v-v.min()+eps
    v = v/v.sum()
    return v

Qmax = 0
cum_rewards = []
rewards = []
for epoch in range(100000):
    obs = env.reset()
    done = False
    cum_reward=0
    # == do the simulation ==
    while not done:
        s = discretize(obs)
        if random.random()<epsilon:
            # exploitation - chose the action according to Q-Table probabilities
            v = probs(np.array(qvalues(s)))
            a = random.choices(actions,weights=v)[0]
        else:
            # exploration - randomly chose the action
            a = np.random.randint(env.action_space.n)

        obs, rew, done, info = env.step(a)
        cum_reward+=rew
        ns = discretize(obs)
        Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
    cum_rewards.append(cum_reward)
    rewards.append(cum_reward)
    # == Periodically print results and calculate average reward ==
    if epoch%5000==0:
        print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
        if np.average(cum_rewards) > Qmax:
            Qmax = np.average(cum_rewards)
            Qbest = Q
        cum_rewards=[]
```

----------------------------------------

TITLE: Generating Classification Report with Scikit-learn (Python)
DESCRIPTION: This snippet uses the trained model to predict the cuisine labels for the entire test dataset (`X_test`). It then generates and prints a comprehensive classification report using `classification_report` from Scikit-learn, comparing the predicted labels (`y_pred`) against the true labels (`y_test`). The report provides detailed performance metrics (precision, recall, f1-score, support) for each individual class and aggregated averages.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/4-Classification/2-Classifiers-1/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
y_pred = model.predict(X_test)
print(classification_report(y_test,y_pred))
```

----------------------------------------

TITLE: Training and Evaluating Logistic Regression (Python)
DESCRIPTION: Initializes and trains a logistic regression model using the training data. After training, it predicts the target variable on the test set and evaluates the model's performance using a classification report and the F1-score. This provides key metrics like precision, recall, and F1-score per class.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/README.md#_snippet_11

LANGUAGE: python
CODE:
```
from sklearn.metrics import f1_score, classification_report 
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)

print(classification_report(y_test, predictions))
print('Predicted labels: ', predictions)
print('F1-score: ', f1_score(y_test, predictions))
```

----------------------------------------

TITLE: Initializing and Training Linear Regression Model - Python
DESCRIPTION: Creates an instance of the LinearRegression model from Scikit-learn. The model is then trained using the training data (X_train and y_train) via the fit() method. This step learns the relationship between the selected feature (BMI) and the target variable (disease progression).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/2-Regression/1-Tools/README.md#_snippet_4

LANGUAGE: python
CODE:
```
model = linear_model.LinearRegression()
model.fit(X_train, y_train)
```

----------------------------------------

TITLE: Training a Workflow (Fitting a Model) in R
DESCRIPTION: This snippet trains the defined workflow (`lm_wf`) on the training data (`pumpkins_train`). The `fit()` function within `tidymodels` automatically applies the recipe steps to the training data and then fits the specified model to the preprocessed data. The resulting fitted workflow object contains the trained model, and it is then printed to show the model's output (like coefficients).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_13

LANGUAGE: R
CODE:
```
# Train the model
lm_wf_fit <- lm_wf %>%
  fit(data = pumpkins_train)

# Print the model coefficients learned
lm_wf_fit
```

----------------------------------------

TITLE: Training and Evaluating Polynomial Regression with Combined Features - Python
DESCRIPTION: Constructs a scikit-learn pipeline for polynomial regression (degree 2) followed by linear regression. Uses the combined feature matrix `X` (including variety, month, city, package) and target `y` (Price). Splits the data, trains the pipeline, predicts on the test set, and reports the resulting RMSE and R2 score for the model trained on multiple features.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/notebook.ipynb#_snippet_19

LANGUAGE: python
CODE:
```
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

pipeline.fit(X_train,y_train)

pred = pipeline.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

----------------------------------------

TITLE: Implementing Flask App Backend Logic (Python)
DESCRIPTION: Contains the Python code for the Flask web server (`app.py`). It imports necessary libraries (numpy, Flask, pickle), loads the pre-trained scikit-learn model, defines routes for the home page (`/`) and prediction endpoint (`/predict`), handles form submission to get user input, uses the model to make a prediction, and renders the result back to the HTML template. It also includes a standard `if __name__ == "__main__":` block to run the app with debugging enabled.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/3-Web-App/1-Web-App/README.md#_snippet_9

LANGUAGE: python
CODE:
```
import numpy as np
from flask import Flask, request, render_template
import pickle

app = Flask(__name__)

model = pickle.load(open("./ufo-model.pkl", "rb"))


@app.route("/")
def home():
    return render_template("index.html")


@app.route("/predict", methods=["POST"])
def predict():

    int_features = [int(x) for x in request.form.values()]
    final_features = [np.array(int_features)]
    prediction = model.predict(final_features)

    output = prediction[0]

    countries = ["Australia", "Canada", "Germany", "UK", "US"]

    return render_template(
        "index.html", prediction_text="Likely country: {}".format(countries[output])
    )


if __name__ == "__main__":
    app.run(debug=True)
```

----------------------------------------

TITLE: Preparing Data for Scikit-learn
DESCRIPTION: Prepares the feature ('DayOfYear') and target ('Price') data from the `pie_pumpkins` DataFrame for use with scikit-learn models. The feature data `X` is converted to a numpy array and reshaped to a 2D array (-1 rows, 1 column) as required by scikit-learn's `fit` method for a single feature. The target data `y` is selected as a pandas Series. Requires a pandas DataFrame with 'DayOfYear' and 'Price' columns. Outputs numpy arrays/pandas Series `X` and `y`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/fr/2-Regression/3-Linear/README.md#_snippet_6

LANGUAGE: python
CODE:
```
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

----------------------------------------

TITLE: Training a Linear Regression Model - Python
DESCRIPTION: Initializes a `LinearRegression` model from scikit-learn and trains it using the prepared training data (X_train and y_train). The model learns the coefficients and intercept from this data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/solution/notebook.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
model = linear_model.LinearRegression()
model.fit(X_train, y_train)
```

----------------------------------------

TITLE: Counting Rows with Pandas Boolean Indexing Python
DESCRIPTION: This snippet provides an optimized method for counting rows in a Pandas DataFrame (`df`) based on column value conditions, offering an alternative to `apply` with `lambda`. It leverages Pandas' built-in boolean indexing (`df.column == value`) which returns a boolean Series, and then efficiently counts `True` values by applying the `sum()` function directly to the boolean Series. The snippet counts rows for specific "No Negative" and "No Positive" conditions and includes a performance measurement comparing this approach to the `apply` method.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_1

LANGUAGE: python
CODE:
```
# without lambdas (using a mixture of notations to show you can use both)
start = time.time()
no_negative_reviews = sum(df.Negative_Review == "No Negative")
print("Number of No Negative reviews: " + str(no_negative_reviews))

no_positive_reviews = sum(df["Positive_Review"] == "No Positive")
print("Number of No Positive reviews: " + str(no_positive_reviews))

both_no_reviews = sum((df.Negative_Review == "No Negative") & (df.Positive_Review == "No Positive"))
print("Number of both No Negative and No Positive reviews: " + str(both_no_reviews))

end = time.time()
print("Sum took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Evaluating Linear Regression Model (MSE) - Scikit-learn Python
DESCRIPTION: Uses the trained `lin_reg` model to predict prices (`pred`) based on the test feature set (`X_test`). It then calculates the Root Mean Squared Error (RMSE) between the actual test prices (`y_test`) and the predictions (`pred`) using `mean_squared_error` and prints the RMSE and its percentage relative to the mean prediction. Requires scikit-learn and NumPy.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/2-Regression/3-Linear/README.md#_snippet_9

LANGUAGE: Python
CODE:
```
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

----------------------------------------

TITLE: Implementing Flask App with ML Model (Python)
DESCRIPTION: This Python script sets up a Flask web server. It loads a pre-trained scikit-learn model using pickle, defines routes for the home page and prediction handling, processes form data, makes a prediction using the loaded model, and renders the result back into the HTML template.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/3-Web-App/1-Web-App/README.md#_snippet_5

LANGUAGE: python
CODE:
```
import numpy as np
from flask import Flask, request, render_template
import pickle

app = Flask(__name__)

model = pickle.load(open("./ufo-model.pkl", "rb"))


@app.route("/")
def home():
    return render_template("index.html")


@app.route("/predict", methods=["POST"])
def predict():

    int_features = [int(x) for x in request.form.values()]
    final_features = [np.array(int_features)]
    prediction = model.predict(final_features)

    output = prediction[0]

    countries = ["Australia", "Canada", "Germany", "UK", "US"]

    return render_template(
        "index.html", prediction_text="Likely country: {}".format(countries[output])
    )


if __name__ == "__main__":
    app.run(debug=True)
```

----------------------------------------

TITLE: Training and Evaluating Classifiers - Scikit-learn - Python
DESCRIPTION: Iterates through the `classifiers` dictionary, training each model on the training data (`X_train`, `y_train`) and making predictions on the test data (`X_test`). For each classifier, it calculates and prints the accuracy score and a comprehensive classification report using the true labels (`y_test`) and predicted labels (`y_pred`). `np.ravel(y_train)` reshapes the target variable appropriately.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/4-Classification/3-Classifiers-2/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
n_classifiers = len(classifiers)

for index, (name, classifier) in enumerate(classifiers.items()):
    classifier.fit(X_train, np.ravel(y_train))

    y_pred = classifier.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print("Accuracy (train) for %s: %0.1f%% " % (name, accuracy * 100))
    print(classification_report(y_test,y_pred))
```

----------------------------------------

TITLE: Training Logistic Regression Model with Scikit-learn (Python)
DESCRIPTION: This code initializes a `LogisticRegression` model for multiclass classification using the 'one-vs-rest' (`ovr`) scheme and the 'liblinear' solver. It then trains the model using the training data (`X_train`, `y_train`, with `y_train` flattened) and evaluates its performance by calculating and printing the accuracy score on the test data (`X_test`, `y_test`).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/4-Classification/2-Classifiers-1/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
lr = LogisticRegression(multi_class='ovr',solver='liblinear')
model = lr.fit(X_train, np.ravel(y_train))

accuracy = model.score(X_test, y_test)
print ("Accuracy is {}".format(accuracy))
```

----------------------------------------

TITLE: Training a Linear Regression Model with Scikit-learn in Python
DESCRIPTION: This snippet initializes a LinearRegression object and trains it using the training data (X_train, y_train) with the .fit() method. After fitting, the model learns the coefficients and intercept representing the linear relationship.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/README.md#_snippet_9

LANGUAGE: Python
CODE:
```
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

----------------------------------------

TITLE: Training Simple Linear Regression Model - Scikit-learn Python
DESCRIPTION: Initializes a `LinearRegression` object, which represents the linear model. It then trains the model by fitting it to the training data (`X_train`, `y_train`), which calculates the optimal coefficients (slope) and intercept for the linear equation based on the provided data. Requires the scikit-learn library.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/2-Regression/3-Linear/README.md#_snippet_8

LANGUAGE: Python
CODE:
```
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

----------------------------------------

TITLE: Split Data into Train/Test Sets Python
DESCRIPTION: Divides the feature set (`X`) and the target variable (`y`) into separate subsets for training and testing the model. 30% of the data is allocated to the test set (`test_size=0.3`), ensuring that the model is evaluated on unseen data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/README.md#_snippet_5

LANGUAGE: python
CODE:
```
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)
```

----------------------------------------

TITLE: Visualizing Linear Regression Model Predictions - Python
DESCRIPTION: Generates a scatter plot displaying the actual test data points ('DayOfYear' vs `y_test`). Plots the predicted prices generated by the trained linear regression model against the corresponding test 'DayOfYear' values to visually compare predictions with actual values and illustrate the learned linear relationship.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/notebook.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

----------------------------------------

TITLE: Fitting the Workflow to Training Data in R
DESCRIPTION: Trains the model bundled in the workflow ('log_reg_wf') using the training data ('pumpkins_train'). The fit() function automatically applies the preprocessing steps defined in the recipe before fitting the logistic regression model. Requires the workflows package.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/R/lesson_4-R.ipynb#_snippet_11

LANGUAGE: R
CODE:
```
# Train the model
wf_fit <- log_reg_wf %>%
  fit(data = pumpkins_train)

# Print the trained workflow
wf_fit
```

----------------------------------------

TITLE: Checking for Missing Data with Pandas in Python
DESCRIPTION: Uses the `.isnull().sum()` method on the Pandas DataFrame to count the number of missing (null) values in each column. This is a crucial step in data cleaning to identify columns requiring imputation or handling of missing entries.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/2-Regression/2-Data/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
pumpkins.isnull().sum()
```

----------------------------------------

TITLE: Calculating Correlation with Pandas
DESCRIPTION: Calculates and prints the Pearson correlation coefficient between the 'Month' column and 'Price', and between the 'DayOfYear' column and 'Price' in the `new_pumpkins` DataFrame. Requires a pandas DataFrame with 'Month', 'DayOfYear', and 'Price' columns. Outputs the correlation values.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/fr/2-Regression/3-Linear/README.md#_snippet_0

LANGUAGE: python
CODE:
```
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

----------------------------------------

TITLE: Training Logistic Regression Multi-class Model - Python
DESCRIPTION: Initializes and trains a Logistic Regression model for multi-class classification using the 'ovr' scheme and 'liblinear' solver from scikit-learn. It fits the model to training data (X_train, y_train), evaluates its accuracy on the test set (X_test, y_test), and prints the resulting accuracy. Requires scikit-learn, numpy, and potentially pandas for ravel.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/4-Classification/2-Classifiers-1/README.md#_snippet_0

LANGUAGE: python
CODE:
```
lr = LogisticRegression(multi_class='ovr',solver='liblinear')
model = lr.fit(X_train, np.ravel(y_train))

accuracy = model.score(X_test, y_test)
print ("Accuracy is {}".format(accuracy))
```

----------------------------------------

TITLE: Install and Load Required R Packages
DESCRIPTION: This snippet checks if the 'pacman' package is installed and installs it if not. It then uses 'pacman::p_load' to install (if missing) and load several essential packages for data science and machine learning, including 'tidyverse', 'tidymodels', 'janitor', and 'corrplot'.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_0

LANGUAGE: R
CODE:
```
suppressWarnings(if (!require("pacman")) install.packages("pacman"))

pacman::p_load(tidyverse, tidymodels, janitor, corrplot)
```

----------------------------------------

TITLE: Handling Missing Data with Pandas - Python
DESCRIPTION: This snippet removes rows from the `pie_pumpkins` DataFrame that contain any missing or NaN values (`dropna(inplace=True)`). It then prints a concise summary of the DataFrame's structure and content using the `info()` method, showing the column names, non-null counts, and data types after cleaning.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/2-Regression/3-Linear/README.md#_snippet_4

LANGUAGE: python
CODE:
```
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

----------------------------------------

TITLE: Training a Polynomial Regression Pipeline (Python)
DESCRIPTION: This code creates a scikit-learn pipeline that first transforms features into polynomial features of degree 2 using PolynomialFeatures(2) and then applies LinearRegression. It then trains this pipeline using the training data (X_train, y_train). Dependencies: PolynomialFeatures and LinearRegression classes, training data X_train, y_train.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/2-Regression/3-Linear/README.md#_snippet_13

LANGUAGE: python
CODE:
```
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

----------------------------------------

TITLE: Training Linear Regression Model in Python
DESCRIPTION: Initializes a `LinearRegression` object from scikit-learn's `linear_model` module. It then trains the model using the `fit()` method on the training data (`X_train` and `y_train`). This step involves the model learning the coefficients and intercept that define the best-fitting line through the training data points. Requires `sklearn.linear_model.LinearRegression`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/2-Regression/1-Tools/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
model = linear_model.LinearRegression()
model.fit(X_train, y_train)
```

----------------------------------------

TITLE: Making Predictions with Trained Model (Scikit-learn, Python)
DESCRIPTION: Uses the trained `LinearRegression` model to make predictions (`y_pred`) on the unseen test feature data (`X_test`). The `predict()` method applies the learned linear equation (from the training step) to the test features to estimate the corresponding target values (disease progression).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/2-Regression/1-Tools/README.md#_snippet_4

LANGUAGE: python
CODE:
```
y_pred = model.predict(X_test)
```

----------------------------------------

TITLE: Preprocessing and Splitting Cuisine Data in R
DESCRIPTION: Loads the cuisine dataset from a URL, removes irrelevant columns (`id`, `rice`, `garlic`, `ginger`), and encodes the `cuisine` column as a factor. It then splits the data into training (70%) and testing (30%) sets, ensuring proportional representation of cuisine types (`strata`). Finally, it displays the count of each cuisine in the training set. Requires `tidyverse` and `tidymodels` packages.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/solution/R/lesson_12-R.ipynb#_snippet_2

LANGUAGE: R
CODE:
```
# Load the core Tidyverse and Tidymodels packages
library(tidyverse)
library(tidymodels)

# Load the original cuisines data
df <- read_csv(file = "https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/data/cuisines.csv")

# Drop id column, rice, garlic and ginger from our original data set
df_select <- df %>%
  select(-c(1, rice, garlic, ginger)) %>%
  # Encode cuisine column as categorical
  mutate(cuisine = factor(cuisine))


# Create data split specification
set.seed(2056)
cuisines_split <- initial_split(data = df_select,
                                strata = cuisine,
                                prop = 0.7)

# Extract the data in each split
cuisines_train <- training(cuisines_split)
cuisines_test <- testing(cuisines_split)

# Display distribution of cuisines in the training set
cuisines_train %>%
  count(cuisine) %>%
  arrange(desc(n))
```

----------------------------------------

TITLE: Train Linear Regression Model Python Scikit-learn
DESCRIPTION: This snippet instantiates a LinearRegression model object from Scikit-learn and trains it using the training feature data (X_train) and training target data (y_train) with the fit method.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/2-Regression/3-Linear/README.md#_snippet_8

LANGUAGE: python
CODE:
```
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

----------------------------------------

TITLE: Training a Scikit-learn Linear Regression Model - Python
DESCRIPTION: This snippet creates an instance of the `LinearRegression` model. It then trains this model using the `fit` method, providing the training feature data (`X_train`) and the corresponding training target data (`y_train`). After execution, the `lin_reg` object contains the learned coefficients and intercept of the linear model.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/2-Regression/3-Linear/README.md#_snippet_8

LANGUAGE: python
CODE:
```
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

----------------------------------------

TITLE: Initializing and Fitting Linear Regression Model
DESCRIPTION: This snippet creates an instance of the `LinearRegression` model from Scikit-learn and trains it using the training data (`X_train`, `y_train`) by calling the `fit` method. After fitting, the model object contains the learned coefficients and intercept.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/2-Regression/3-Linear/README.md#_snippet_8

LANGUAGE: python
CODE:
```
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

----------------------------------------

TITLE: Training Linear Regression Model - Scikit-learn Python
DESCRIPTION: This code initializes a `LinearRegression` model from `sklearn.linear_model`. It then trains the model using the training data (`X_train` and `y_train`) via the `fit` method. This step learns the linear relationship between the selected feature and the target variable based on the training set. Requires `sklearn.linear_model` and training data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/README.md#_snippet_5

LANGUAGE: python
CODE:
```
model = linear_model.LinearRegression()
model.fit(X_train, y_train)
```

----------------------------------------

TITLE: Encoding Categorical Feature with Pandas Python
DESCRIPTION: This snippet demonstrates how to apply one-hot encoding to the 'Variety' column of the `new_pumpkins` DataFrame using the pandas `get_dummies` function. One-hot encoding converts categorical values into a numerical matrix suitable for machine learning models like linear regression by creating a new binary column for each unique category.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/README.md#_snippet_15

LANGUAGE: python
CODE:
```
pd.get_dummies(new_pumpkins['Variety'])
```

----------------------------------------

TITLE: Training and Evaluating Random Forest Model in R
DESCRIPTION: Defines a Random Forest model specification using the 'ranger' engine, bundles it with a recipe into a workflow, fits the workflow to training data, and evaluates performance on test data. Requires 'cuisines_recipe', 'cuisines_train', 'cuisines_test', and an 'eval_metrics' object.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/solution/R/lesson_12-R.ipynb#_snippet_9

LANGUAGE: R
CODE:
```
# Make a random forest specification
rf_spec <- rand_forest() %>%
  set_engine("ranger") %>%
  set_mode("classification")

# Bundle recipe and model specification into a workflow
rf_wf <- workflow() %>%
  add_recipe(cuisines_recipe) %>%
  add_model(rf_spec)

# Train a random forest model
rf_wf_fit <- rf_wf %>%
  fit(data = cuisines_train)


# Make predictions and Evaluate model performance
rf_wf_fit %>%
  augment(new_data = cuisines_test) %>%
  eval_metrics(truth = cuisine, estimate = .pred_class)
```

----------------------------------------

TITLE: Splitting Data into Training and Test Sets with scikit-learn (Python)
DESCRIPTION: Splits the feature data (X) and target data (y) into training and testing sets using `train_test_split` from `sklearn.model_selection`. The `test_size=0.33` parameter allocates 33% of the data to the test set, with the remaining 67% used for training. This prepares the data for model training and evaluation. Requires scikit-learn's `model_selection` module.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/2-Regression/1-Tools/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33)
```

----------------------------------------

TITLE: Splitting Data into Train/Test Sets in Python
DESCRIPTION: Splits the feature matrix X and the target vector y into random training and testing subsets using `train_test_split`. The `test_size=0.33` parameter allocates 33% of the data to the test set, with the remaining 67% used for training. This is crucial for evaluating the model's performance on unseen data. Requires `sklearn.model_selection.train_test_split`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/2-Regression/1-Tools/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33)
```

----------------------------------------

TITLE: Generating Predictions with Model - Scikit-learn Python
DESCRIPTION: This snippet uses the trained linear regression model's `predict` method to generate predictions (`y_pred`) for the test feature data (`X_test`). The output `y_pred` will be an array of predicted target values corresponding to each sample in `X_test`. These predictions represent the model's output based on the learned relationship. Requires a trained model and test feature data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/README.md#_snippet_6

LANGUAGE: python
CODE:
```
y_pred = model.predict(X_test)
```

----------------------------------------

TITLE: Implementing Q-Learning Training Loop - Python
DESCRIPTION: Contains the main Q-Learning training loop that runs for a specified number of epochs. In each epoch (episode), it resets the environment, simulates until done, applies exploration/exploitation to choose actions based on current Q-values and epsilon, takes steps, receives feedback (obs, rew, done, info), discretizes the next state, updates the Q-Table using the Q-Learning formula, and accumulates rewards. Periodically, it prints the average cumulative reward and saves the best Q-Table found so far.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/translations/README.ko.md#_snippet_10

LANGUAGE: python
CODE:
```
def probs(v,eps=1e-4):\n    v = v-v.min()+eps\n    v = v/v.sum()\n    return v\n\nQmax = 0\ncum_rewards = []\nrewards = []\nfor epoch in range(100000):\n    obs = env.reset()\n    done = False\n    cum_reward=0\n    # == do the simulation ==\n    while not done:\n        s = discretize(obs)\n        if random.random()<epsilon:\n            # exploitation - chose the action according to Q-Table probabilities\n            v = probs(np.array(qvalues(s)))\n            a = random.choices(actions,weights=v)[0]\n        else:\n            # exploration - randomly chose the action\n            a = np.random.randint(env.action_space.n)\n\n        obs, rew, done, info = env.step(a)\n        cum_reward+=rew\n        ns = discretize(obs)\n        Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))\n    cum_rewards.append(cum_reward)\n    rewards.append(cum_reward)\n    # == Periodically print results and calculate average reward ==\n    if epoch%5000==0:\n        print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")\n        if np.average(cum_rewards) > Qmax:\n            Qmax = np.average(cum_rewards)\n            Qbest = Q\n        cum_rewards=[]
```

----------------------------------------

TITLE: Generating SVR Predictions (Python)
DESCRIPTION: Uses the trained SVR model to generate predictions on both the training and testing feature sets. The 'predict' method outputs the forecasted values based on the input sequences, and the results are reshaped for consistency.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_16

LANGUAGE: python
CODE:
```
y_train_pred = model.predict(x_train).reshape(-1,1)
y_test_pred = model.predict(x_test).reshape(-1,1)
```

----------------------------------------

TITLE: Predict with Trained SVC Model Python
DESCRIPTION: Uses the trained SVC model (`model`) to generate predictions on the test feature set (`X_test`). The resulting predicted labels are stored in the `y_pred` variable, which will be used later for evaluating the model's performance.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/README.md#_snippet_7

LANGUAGE: python
CODE:
```
y_pred = model.predict(X_test)
```

----------------------------------------

TITLE: Making Predictions with SVC (Python)
DESCRIPTION: Uses the trained Support Vector Classifier (`model`) to predict the cuisine labels for the test feature set (`X_test`). The resulting predictions are stored in the `y_pred` variable.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/solution/notebook.ipynb#_snippet_8

LANGUAGE: Python
CODE:
```
y_pred = model.predict(X_test)
```

----------------------------------------

TITLE: Performing Q-Learning Training Epoch in Python
DESCRIPTION: This Python snippet implements the core Q-learning update loop for a single epoch. It simulates an agent's movement, calculates rewards, and updates the Q-table using the Bellman equation, incorporating learning rate decay and a discount factor. It continues until an end-of-game condition is met or a negative reward threshold is reached.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/sw/8-Reinforcement/1-QLearning/README.md#_snippet_1

LANGUAGE: python
CODE:
```
for epoch in range(5000):
    
    # Pick initial point
    m.random_start()
    
    # Start travelling
    n=0
    cum_reward = 0
    while True:
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        dpos = actions[a]
        m.move(dpos,check_correctness=False) # we allow player to move outside the board, which terminates episode
        r = reward(m)
        cum_reward += r
        if r==end_reward or cum_reward < -1000:
            lpath.append(n)
            break
        alpha = np.exp(-n / 10e5)
        gamma = 0.5
        ai = action_idx[a]
        Q[x,y,ai] = (1 - alpha) * Q[x,y,ai] + alpha * (r + gamma * Q[x+dpos[0], y+dpos[1]].max())
        n+=1
```

----------------------------------------

TITLE: Splitting Data for Model Training with Scikit-learn (Python)
DESCRIPTION: Splits the `encoded_pumpkins` DataFrame into features (`X`) and target (`y`) for machine learning model training. The target variable is 'Color', and features are all other columns. It then uses `sklearn.model_selection.train_test_split` to divide the data into training (80%) and testing (20%) sets, ensuring reproducibility with `random_state`. Requires Scikit-learn. Expects `encoded_pumpkins` DataFrame. Outputs `X_train`, `X_test`, `y_train`, `y_test`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/4-Logistic/README.md#_snippet_2

LANGUAGE: python
CODE:
```
from sklearn.model_selection import train_test_split
    
X = encoded_pumpkins[encoded_pumpkins.columns.difference(['Color'])]
y = encoded_pumpkins['Color']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

----------------------------------------

TITLE: Splitting Data into Training and Test Sets (Python)
DESCRIPTION: This snippet divides the prepared feature (`X`) and target (`y`) data into separate sets for training and testing the linear regression model. It uses `train_test_split` from Scikit-learn, allocating 20% of the data for testing (`test_size=0.2`) and ensuring reproducibility with `random_state=0`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/de/2-Regression/3-Linear/README.md#_snippet_7

LANGUAGE: python
CODE:
```
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

----------------------------------------

TITLE: Splitting Data for Training and Testing with Scikit-learn in Python
DESCRIPTION: This code splits the feature (X) and label (y) data into training and testing sets using the train_test_split function. It allocates 20% of the data for testing (test_size=0.2) and uses a random_state for reproducibility.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/README.md#_snippet_8

LANGUAGE: Python
CODE:
```
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

----------------------------------------

TITLE: Splitting Data into Train/Test Python
DESCRIPTION: This snippet uses the `train_test_split` function from scikit-learn to divide the feature and label DataFrames into training and testing sets. The `test_size=0.3` parameter allocates 30% of the data for testing, while the rest is used for training. This split is essential for evaluating model performance on unseen data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/solution/notebook.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
```

----------------------------------------

TITLE: Importing Pandas Library (Python)
DESCRIPTION: Imports the pandas library, a fundamental tool for data manipulation and analysis in Python. It is aliased as `pd` for convenience, which is standard practice.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/solution/notebook.ipynb#_snippet_1

LANGUAGE: Python
CODE:
```
import pandas as pd
```

----------------------------------------

TITLE: Splitting Data into Train/Test Sets - Scikit-learn Python
DESCRIPTION: Uses the `train_test_split` function from `sklearn.model_selection` to divide the feature DataFrame (`cuisines_feature_df`) and label DataFrame (`cuisines_label_df`) into training and testing subsets (`X_train`, `X_test`, `y_train`, `y_test`). A `test_size` of 0.3 means 30% of the data will be used for testing, and 70% for training. This is a standard step before training models.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
```

----------------------------------------

TITLE: Splitting Data into Training and Testing Sets
DESCRIPTION: Splits the prepared feature (`X`) and target (`y`) data into training and testing sets. 80% of the data is used for training (`X_train`, `y_train`), and 20% is used for testing (`X_test`, `y_test`). The `random_state` ensures reproducibility of the split. Requires numpy arrays/pandas Series `X` and `y`. Outputs four separate data arrays/Series.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/fr/2-Regression/3-Linear/README.md#_snippet_7

LANGUAGE: python
CODE:
```
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

----------------------------------------

TITLE: Calculating ROC AUC in Python
DESCRIPTION: This snippet calculates the Area Under the Curve (AUC) for the ROC curve generated from the model's results. It takes the `results` object, pipes it to the `roc_auc` function, specifying the true color and prediction columns. The resulting value (close to 1 being good) quantifies the overall performance of the classifier.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/R/lesson_4-R.ipynb#_snippet_16

LANGUAGE: python
CODE:
```
# Calculate area under curve
results %>%
  roc_auc(color, .pred_ORANGE)
```

----------------------------------------

TITLE: Make Predictions & Combine Results (Tidymodels/dplyr) - R
DESCRIPTION: Uses the fitted workflow (`mr_fit`) to make class predictions on the test dataset (`cuisines_test`). It then selects the actual cuisine labels from the test set and combines them with the predicted labels into a new data frame (`results`) for subsequent evaluation.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/solution/R/lesson_11-R.ipynb#_snippet_7

LANGUAGE: R
CODE:
```
# Make predictions on the test set
results <- cuisines_test %>%
  select(cuisine) %>%
  bind_cols(mr_fit %>%
    predict(new_data = cuisines_test))

# Print out results
results %>%
  slice_head(n = 5)
```

----------------------------------------

TITLE: Combine Features & Train Polynomial Model - Pandas/Sklearn Python
DESCRIPTION: This comprehensive snippet combines multiple features (categorical using one-hot encoding and numerical) into a single feature set `X`. It includes 'Variety', 'Month', 'City', and 'Package' as input features, using `pd.get_dummies` for categorical columns and `join` to combine them. The data is then split into training and testing sets, and a polynomial regression pipeline (degree 2) is trained on the combined features. Finally, it makes predictions on the test set and calculates the Mean Squared Error (MSE) and the coefficient of determination (R^2 score) to evaluate the model.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/2-Regression/3-Linear/README.md#_snippet_2

LANGUAGE: python
CODE:
```
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

----------------------------------------

TITLE: Splitting Data for Logistic Regression (Python)
DESCRIPTION: Prepares data for machine learning by splitting the dataset into features (X) and the target variable (y), and then further into training and testing sets. This step is crucial for evaluating the model's performance on unseen data. Uses scikit-learn's `train_test_split` function.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/README.md#_snippet_10

LANGUAGE: python
CODE:
```
from sklearn.model_selection import train_test_split

X = encoded_pumpkins[encoded_pumpkins.columns.difference(['Color'])]
y = encoded_pumpkins['Color']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

----------------------------------------

TITLE: Training Polynomial Regression Pipeline (Scikit-learn) - Python
DESCRIPTION: This snippet creates a Scikit-learn pipeline combining `PolynomialFeatures` (degree 4) and `LinearRegression`. It splits the data, trains the pipeline on the training set, and uses it to predict values for the test set, enabling fitting non-linear relationships.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/translations/README.ko.md#_snippet_12

LANGUAGE: Python
CODE:
```
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(4), LinearRegression())

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

pipeline.fit(np.array(X_train), y_train)

y_pred=pipeline.predict(X_test)
```

----------------------------------------

TITLE: Splitting UFO Data for Model Training Python
DESCRIPTION: This snippet prepares the data for machine learning by separating features (X) and the target variable (y). It defines the selected feature columns ('Seconds', 'Latitude', 'Longitude') and uses `train_test_split` from scikit-learn to divide the data into training (80%) and testing (20%) sets. `random_state` is set for reproducibility. Requires scikit-learn and the processed pandas DataFrame with encoded 'Country'.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/3-Web-App/1-Web-App/solution/notebook.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
from sklearn.model_selection import train_test_split

Selected_features = ['Seconds','Latitude','Longitude']

X = ufos[Selected_features]
y = ufos['Country']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

----------------------------------------

TITLE: Splitting Data into Training and Test Sets with Tidymodels
DESCRIPTION: This code uses the 'initial_split' function from the 'rsample' package (part of 'tidymodels') to divide the cleaned data ('df_select') into training (70%) and testing (30%) sets. The 'strata' argument ensures that the proportion of each cuisine is maintained in both subsets, which is crucial for imbalanced datasets. It also prints the number of cases in each split.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/solution/R/lesson_11-R.ipynb#_snippet_2

LANGUAGE: R
CODE:
```
# Load the core Tidymodels packages into R session
library(tidymodels)

# Create split specification
set.seed(2056)
cuisines_split <- initial_split(data = df_select,
                                strata = cuisine,
                                prop = 0.7)

# Extract the data in each split
cuisines_train <- training(cuisines_split)
cuisines_test <- testing(cuisines_split)

# Print the number of cases in each split
cat("Training cases: ", nrow(cuisines_train), "\n",
    "Test cases: ", nrow(cuisines_test), sep = "")

# Display the first few rows of the training set
cuisines_train %>%
  slice_head(n = 5)

# Display distribution of cuisines in the training set
cuisines_train %>% 
  count(cuisine) %>% 
  arrange(desc(n))
```

----------------------------------------

TITLE: Counting and Plotting Cuisine Distribution in R
DESCRIPTION: Counts the number of observations for each unique cuisine category in the dataset `df` using `count()` and sorts them by count using `arrange()`. Subsequently, it creates a bar plot using `ggplot2` to visualize the distribution of data points per cuisine, highlighting potential imbalances.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb#_snippet_3

LANGUAGE: R
CODE:
```
# Count observations per cuisine
df %>%
  count(cuisine) %>%
  arrange(n)

# Plot the distribution
theme_set(theme_light())
df %>%
  count(cuisine) %>%
  ggplot(mapping = aes(x = n, y = reorder(cuisine, -n))) +
  geom_col(fill = "midnightblue", alpha = 0.7) +
  ylab("cuisine")
```

----------------------------------------

TITLE: Splitting Data into Training and Testing Sets - Python
DESCRIPTION: Divides the feature (X) and target (y) data into training and testing sets using `model_selection.train_test_split`. A test set size of 33% is used.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/solution/notebook.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33)
```

----------------------------------------

TITLE: Splitting Data for Model Training Scikit-learn Python
DESCRIPTION: This snippet splits the dataset (`encoded_pumpkins`) into training and testing sets for features (X) and the target variable (y). It selects all columns except 'Color' as features and 'Color' as the target. It uses a 80/20 split and a fixed random state for reproducibility. It requires the `train_test_split` function from `sklearn.model_selection`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/2-Regression/4-Logistic/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
    from sklearn.model_selection import train_test_split
    
    X = encoded_pumpkins[encoded_pumpkins.columns.difference(['Color'])]
    y = encoded_pumpkins['Color']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

----------------------------------------

TITLE: Defining One-Hot Encoder with Scikit-learn (Python)
DESCRIPTION: This snippet imports the OneHotEncoder class from scikit-learn and defines an instance configured for encoding nominal categorical features like 'City Name', 'Package', 'Variety', and 'Origin'. The sparse_output=False parameter ensures the output is a dense NumPy array (which set_output(transform='pandas') later converts to a DataFrame).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/README.md#_snippet_4

LANGUAGE: python
CODE:
```
from sklearn.preprocessing import OneHotEncoder

categorical_features = ['City Name', 'Package', 'Variety', 'Origin']
categorical_encoder = OneHotEncoder(sparse_output=False)
```

----------------------------------------

TITLE: Calculating Nationality Frequency Python
DESCRIPTION: This code calculates the frequency of each reviewer nationality in the dataset. It uses the `value_counts()` method on the 'Reviewer_Nationality' column of the DataFrame to get a Series of counts.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/4-Hotel-Reviews-1/solution/notebook.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
# value_counts() creates a Series object that has index and values
#                in this case, the country and the frequency they occur in reviewer nationality
nationality_freq = df["Reviewer_Nationality"].value_counts()
```

----------------------------------------

TITLE: Training a Linear Regression Model with Scikit-learn Python
DESCRIPTION: This snippet creates an instance of the `LinearRegression` model from scikit-learn. It then trains this model using the provided training data (`X_train` and `y_train`) via the `fit()` method. After execution, the `lin_reg` object holds the parameters (coefficients and intercept) that define the best-fitting line for the training data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/3-Linear/README.md#_snippet_8

LANGUAGE: python
CODE:
```
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

----------------------------------------

TITLE: Training a Linear Regression Model (Python)
DESCRIPTION: This code initializes a LinearRegression object from scikit-learn and trains it using the prepared training data (X_train, y_train). The fit() method learns the linear relationship between the features and the target. Dependencies: LinearRegression from sklearn.linear_model, training data X_train, y_train.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/2-Regression/3-Linear/README.md#_snippet_8

LANGUAGE: python
CODE:
```
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

----------------------------------------

TITLE: Training a Simple Linear Regression Model with Scikit-learn
DESCRIPTION: Initializes a `LinearRegression` model object and trains it using the training data (`X_train`, `y_train`). The model learns the linear relationship between the input feature (DayOfYear) and the target variable (Price). Requires the `LinearRegression` class imported and training data arrays/Series. Outputs a trained `LinearRegression` model object.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/fr/2-Regression/3-Linear/README.md#_snippet_8

LANGUAGE: python
CODE:
```
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

----------------------------------------

TITLE: Training a Linear Regression Model (Python)
DESCRIPTION: This code initializes an instance of the `LinearRegression` model from Scikit-learn. It then trains the model on the designated training data (`X_train`, `y_train`) using the `fit()` method, allowing the model to learn the linear relationship parameters (slope and intercept).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/de/2-Regression/3-Linear/README.md#_snippet_8

LANGUAGE: python
CODE:
```
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

----------------------------------------

TITLE: Splitting Data for Training and Testing with Scikit-learn (Python)
DESCRIPTION: This snippet splits the input features (`cuisines_feature_df`) and labels (`cuisines_label_df`) into training and testing sets using `train_test_split` from Scikit-learn. It allocates 30% of the data to the test set, creating four resulting variables: `X_train`, `X_test`, `y_train`, and `y_test`, which are essential for the model training and evaluation process.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/4-Classification/2-Classifiers-1/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
```

----------------------------------------

TITLE: Loading Data with Pandas in Python
DESCRIPTION: Imports the pandas library, reads the 'US-pumpkins.csv' file from the specified path into a DataFrame, and displays the first five rows of the DataFrame to provide an initial look at the data structure and content.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/2-Regression/2-Data/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
import pandas as pd
pumpkins = pd.read_csv('../data/US-pumpkins.csv')
pumpkins.head()
```

----------------------------------------

TITLE: Performing One-Hot Encoding on Variety Feature - Python
DESCRIPTION: Applies the `pd.get_dummies()` function to the 'Variety' column of the `new_pumpkins` DataFrame. This converts the categorical 'Variety' column into multiple binary (0 or 1) columns, one for each unique variety, which is suitable for use in linear models.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/notebook.ipynb#_snippet_15

LANGUAGE: python
CODE:
```
pd.get_dummies(new_pumpkins['Variety'])
```

----------------------------------------

TITLE: Perform K-Means Clustering with a Fixed Number of Clusters in Python
DESCRIPTION: Initializes and fits a K-Means clustering model from Scikit-learn with a specified number of clusters (`nclusters`) and a fixed random state for reproducibility. Predicts the cluster label for each data point in the feature matrix X based on the fitted model. Requires `sklearn.cluster.KMeans` and the feature matrix `X`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/README.md#_snippet_2

LANGUAGE: python
CODE:
```
from sklearn.cluster import KMeans

nclusters = 3 
seed = 0

km = KMeans(n_clusters=nclusters, random_state=seed)
km.fit(X)

# Predict the cluster for each data point

y_cluster_kmeans = km.predict(X)
y_cluster_kmeans
```

----------------------------------------

TITLE: Visualizing Model Results - Python
DESCRIPTION: Generates a scatter plot showing the actual test data points (X_test vs y_test). It then overlays a line plot representing the model's predictions (X_test vs y_pred) to visualize the fitted linear regression line.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/solution/notebook.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
plt.scatter(X_test, y_test,  color='black')
plt.plot(X_test, y_pred, color='blue', linewidth=3)
plt.show()
```

----------------------------------------

TITLE: Calculating and Printing MAPE (Python)
DESCRIPTION: This code calculates the Mean Absolute Percentage Error (MAPE) between the predicted values (`Y_pred`) and the actual values (`Y`) using a function named `mape`. The result is then multiplied by 100 to express it as a percentage and printed to the console, providing a quantitative measure of the model's prediction accuracy. It relies on a pre-defined `mape` function.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/7-TimeSeries/3-SVR/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
print('MAPE: ', mape(Y_pred, Y)*100, '%')
```

----------------------------------------

TITLE: Plotting Regression Results - Python
DESCRIPTION: Visualizes the results using Matplotlib. It creates a scatter plot of the actual test data points (X_test vs y_test) and overlays a blue line representing the model's predictions (X_test vs y_pred). Includes labels for axes and a title for the plot.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/2-Regression/1-Tools/README.md#_snippet_6

LANGUAGE: python
CODE:
```
plt.scatter(X_test, y_test,  color='black')
plt.plot(X_test, y_pred, color='blue', linewidth=3)
plt.xlabel('Scaled BMIs')
plt.ylabel('Disease Progression')
plt.title('A Graph Plot Showing Diabetes Progression Against BMI')
plt.show()
```

----------------------------------------

TITLE: Analyzing Prediction Probabilities for a Sample with Scikit-learn (Python)
DESCRIPTION: This code prepares a single test sample (index 50) for prediction by reshaping it. It then uses the trained logistic regression model (`model`) to get the probability prediction for each cuisine class using `predict_proba`. The probabilities are organized into a Pandas DataFrame, and the snippet prints the top predicted cuisines based on these probabilities.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/4-Classification/2-Classifiers-1/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
test= X_test.iloc[50].values.reshape(-1, 1).T
proba = model.predict_proba(test)
classes = model.classes_
resultdf = pd.DataFrame(data=proba, columns=classes)

topPrediction = resultdf.T.sort_values(by=[0], ascending = [False])
topPrediction.head()
```

----------------------------------------

TITLE: Importing Scikit-learn Modules for Linear Regression Python
DESCRIPTION: This snippet imports key components from the scikit-learn library required for performing linear regression analysis. It imports the `LinearRegression` model class, the `mean_squared_error` function for evaluating model performance, and the `train_test_split` function for dividing data into training and testing sets. These imports are prerequisites for building and evaluating a linear regression model.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/3-Linear/README.md#_snippet_5

LANGUAGE: python
CODE:
```
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

----------------------------------------

TITLE: Predicting Pumpkin Price with Linear Regression - Python
DESCRIPTION: Utilizes the trained `LinearRegression` model (`lin_reg`) to make a price prediction for a specific input value of 'DayOfYear', specifically day 256. Demonstrates how to use the trained model for inference on new data points.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/notebook.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
# Pumpkin price on programmer's day

lin_reg.predict([[256]])
```

----------------------------------------

TITLE: One-Hot Encoding Categorical Column - Pandas Python
DESCRIPTION: This code snippet shows how to use the pandas `get_dummies` function to perform one-hot encoding on a single categorical column, 'Variety', from a DataFrame named `new_pumpkins`. One-hot encoding converts categorical variables into a format that can be provided to machine learning algorithms, creating new binary columns for each unique category value.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/2-Regression/3-Linear/README.md#_snippet_1

LANGUAGE: python
CODE:
```
pd.get_dummies(new_pumpkins['Variety'])
```

----------------------------------------

TITLE: Calculating R-squared (Coefficient of Determination) Python
DESCRIPTION: This snippet calculates the coefficient of determination (R-squared) for the trained linear regression model `lin_reg` using the training data (`X_train` and `y_train`) via the `score()` method. The R-squared value indicates the proportion of the variance in the dependent variable that is predictable from the independent variable(s). The resulting score is printed to standard output.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/3-Linear/README.md#_snippet_10

LANGUAGE: python
CODE:
```
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```

----------------------------------------

TITLE: Calculate Reviewer Nationality Frequencies with Pandas Python
DESCRIPTION: This snippet calculates the frequency count for each unique value in the 'Reviewer_Nationality' column of the `df` DataFrame using `.value_counts()`. It then prints the total number of distinct nationalities and the frequency counts for each.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_2

LANGUAGE: python
CODE:
```
# value_counts() creates a Series object that has index and values in this case, the country and the frequency they occur in reviewer nationality
nationality_freq = df["Reviewer_Nationality"].value_counts()
print("There are " + str(nationality_freq.size) + " different nationalities")
# print first and last rows of the Series. Change to nationality_freq.to_string() to print all of the data
print(nationality_freq)
```

----------------------------------------

TITLE: Predicting Prices and Calculating Mean Squared Error
DESCRIPTION: This code uses the trained linear regression model (`lin_reg`) to make predictions on the test feature data (`X_test`). It then calculates the Mean Squared Error (MSE) between the predicted prices (`pred`) and the actual test prices (`y_test`) and prints the square root of the MSE.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/2-Regression/3-Linear/README.md#_snippet_9

LANGUAGE: python
CODE:
```
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

----------------------------------------

TITLE: Importing Libraries and Loading Data (Python)
DESCRIPTION: Imports necessary libraries including pandas for data manipulation, matplotlib for plotting, numpy for numerical operations, and datetime. It then loads the 'US-pumpkins.csv' dataset into a pandas DataFrame and displays the first few rows to inspect the data structure.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime

pumpkins = pd.read_csv('../data/US-pumpkins.csv')

pumpkins.head()
```

----------------------------------------

TITLE: Handling Missing Data with Pandas dropna in Python
DESCRIPTION: This snippet removes rows from the 'pie_pumpkins' DataFrame that contain any missing values using the dropna(inplace=True) method. It then prints the DataFrame's info using .info() to show the updated number of non-null entries.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/README.md#_snippet_5

LANGUAGE: Python
CODE:
```
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

----------------------------------------

TITLE: Preparing Data with Combined Features and Running Regression - Python
DESCRIPTION: Creates a comprehensive feature matrix `X` by combining one-hot encoded 'Variety', the numerical 'Month', one-hot encoded 'City', and one-hot encoded 'Package' columns using pandas `join`. Defines the target `y` as the 'Price'. Calls the `run_linear_regression` function with this combined feature set to train and evaluate a model using multiple attributes.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/notebook.ipynb#_snippet_18

LANGUAGE: python
CODE:
```
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

run_linear_regression(X,y)
```

----------------------------------------

TITLE: Scaling Test Data using Fitted MinMaxScaler in Python
DESCRIPTION: This snippet applies the *already fitted* MinMaxScaler (fitted on the training data) to the 'load' column of the testing data. This ensures that both training and testing data are scaled consistently based on the training set's distribution.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/README.md#_snippet_8

LANGUAGE: python
CODE:
```
test['load'] = scaler.transform(test)
test.head()
```

----------------------------------------

TITLE: Making Predictions with Linear Regression in Python
DESCRIPTION: Uses the trained `LinearRegression` model to generate predictions (`y_pred`) on the testing set features (`X_test`). The `predict()` method applies the learned linear equation to the input features to estimate the corresponding target values.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/2-Regression/1-Tools/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
y_pred = model.predict(X_test)
```

----------------------------------------

TITLE: Scaling Testing Data using Fitted MinMaxScaler (Python)
DESCRIPTION: Applies the `transform` method of the *already fitted* MinMaxScaler to the test data. This ensures the testing data is scaled consistently with the training data using the same parameters.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
test['load'] = scaler.transform(test)
test.head(5)
```

----------------------------------------

TITLE: Plotting Elbow Method Curve (Python)
DESCRIPTION: This code visualizes the results of the K-Means runs performed for calculating WCSS across different cluster counts. It generates a line plot using `seaborn`, plotting the `wcss` values (Within-Cluster Sum of Squares) on the y-axis against the number of clusters (from 1 to 10) on the x-axis. The plot helps identify the 'elbow' point, where the rate of decrease in WCSS slows down significantly, suggesting an optimal number of clusters.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/translations/README.ko.md#_snippet_5

LANGUAGE: Python
CODE:
```
plt.figure(figsize=(10,5))
sns.lineplot(range(1, 11), wcss,marker='o',color='red')
plt.title('Elbow')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()
```

----------------------------------------

TITLE: Implementing Flask App for ML Model Prediction
DESCRIPTION: This Python script initializes the Flask web application, loads the serialized machine learning model (`ufo-model.pkl`), defines the root route (`/`) to render the HTML template, and creates a POST route (`/predict`) to handle form submissions. The predict route extracts input data, formats it for the model, makes a prediction, maps the numerical output to a country name, and re-renders the HTML template displaying the prediction.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/3-Web-App/1-Web-App/README.md#_snippet_5

LANGUAGE: Python
CODE:
```
import numpy as np
from flask import Flask, request, render_template
import pickle

app = Flask(__name__)

model = pickle.load(open("./ufo-model.pkl", "rb"))


@app.route("/")
def home():
    return render_template("index.html")


@app.route("/predict", methods=["POST"])
def predict():

    int_features = [int(x) for x in request.form.values()]
    final_features = [np.array(int_features)]
    prediction = model.predict(final_features)

    output = prediction[0]

    countries = ["Australia", "Canada", "Germany", "UK", "US"]

    return render_template(
        "index.html", prediction_text="Likely country: {}".format(countries[output])
    )


if __name__ == "__main__":
    app.run(debug=True)
```

----------------------------------------

TITLE: Visualizing Linear Regression Results with Matplotlib Python
DESCRIPTION: This snippet uses matplotlib's `pyplot` to visualize the performance of the trained linear regression model on the test data. It creates a scatter plot of the actual test data points (`X_test` against `y_test`) and overlays a line plot representing the model's predictions (`X_test` against `pred`). This visualization helps assess how well the linear model fits the distribution of the test data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/3-Linear/README.md#_snippet_11

LANGUAGE: python
CODE:
```
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

----------------------------------------

TITLE: Define and Fit Initial SARIMAX Model in Python
DESCRIPTION: This code defines the non-seasonal (`order`) and seasonal (`seasonal_order`) parameters for the SARIMAX model. It then initializes the `SARIMAX` model from `statsmodels` with the training data (`train`) and the specified orders, fits the model to the data, and prints a summary of the fitting results, including coefficients, statistics, and diagnostic tests.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/README.md#_snippet_10

LANGUAGE: python
CODE:
```
order = (4, 1, 0)
seasonal_order = (1, 1, 0, 24)

model = SARIMAX(endog=train, order=order, seasonal_order=seasonal_order)
results = model.fit()

print(results.summary())
```

----------------------------------------

TITLE: Predicting Prices and Calculating MSE (Python)
DESCRIPTION: This snippet uses the trained `lin_reg` model to generate price predictions (`pred`) for the test features (`X_test`). It then calculates the Mean Squared Error (MSE) between these predictions and the actual test prices (`y_test`), printing the root mean squared error and its percentage relative to the mean predicted price as evaluation metrics.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/de/2-Regression/3-Linear/README.md#_snippet_9

LANGUAGE: python
CODE:
```
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

----------------------------------------

TITLE: Calculating Model Determination Score (R-squared) (Python)
DESCRIPTION: This snippet computes the coefficient of determination, R-squared score, for the trained linear regression model using the training data (`X_train`, `y_train`). The `lin_reg.score()` method returns this value, which indicates the proportion of the variance in the dependent variable that is predictable from the independent variable(s), ranging from 0 (no prediction power) to 1 (perfect prediction).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/de/2-Regression/3-Linear/README.md#_snippet_10

LANGUAGE: python
CODE:
```
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```

----------------------------------------

TITLE: Defining X and y for Linear Model (Pandas) - Python
DESCRIPTION: This snippet extracts the feature variable (X) and the target variable (y) from the `lin_pumpkins` DataFrame using slicing. X is set to the first column ('Package'), and y is set to the second column ('Price'), preparing the data in NumPy array format for Scikit-learn.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/translations/README.ko.md#_snippet_5

LANGUAGE: Python
CODE:
```
X = lin_pumpkins.values[:, :1]
y = lin_pumpkins.values[:, 1:2]
```

----------------------------------------

TITLE: Evaluating Linear Regression Model using MSE (Python)
DESCRIPTION: This snippet evaluates the trained linear regression model's performance on the test set. It uses the predict() method to get predictions (pred) for X_test, calculates the Root Mean Squared Error (RMSE) between predictions and actual y_test, and prints the RMSE and its percentage relative to the mean prediction. Dependencies: Trained lin_reg model, testing data X_test, y_test, mean_squared_error from sklearn.metrics, numpy.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/2-Regression/3-Linear/README.md#_snippet_9

LANGUAGE: python
CODE:
```
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

----------------------------------------

TITLE: Training a Polynomial Regression Pipeline with Scikit-learn Python
DESCRIPTION: This snippet creates a machine learning pipeline using `make_pipeline` that first applies `PolynomialFeatures` with a degree of 2 to the input data, generating quadratic features. It then trains a `LinearRegression` model on these transformed features using the `fit()` method with the provided training data (`X_train` and `y_train`). This pipeline encapsulates the data transformation and modeling steps for polynomial regression.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/3-Linear/README.md#_snippet_13

LANGUAGE: python
CODE:
```
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

----------------------------------------

TITLE: Removing Stop Words from Reviews - NLTK/Pandas Python
DESCRIPTION: This snippet defines and applies a function to remove common English stop words from the 'Negative_Review' and 'Positive_Review' columns of a pandas DataFrame. It loads the data from 'Hotel_Reviews_Filtered.csv', uses a cached set of stop words from NLTK for efficient lookup within the `remove_stopwords` function, and applies this function to the relevant columns. Requires pandas and NLTK libraries installed and the specified CSV file.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_7

LANGUAGE: Python
CODE:
```
from nltk.corpus import stopwords

# Load the hotel reviews from CSV
df = pd.read_csv("../../data/Hotel_Reviews_Filtered.csv")

# Remove stop words - can be slow for a lot of text!
# Ryan Han (ryanxjhan on Kaggle) has a great post measuring performance of different stop words removal approaches
# https://www.kaggle.com/ryanxjhan/fast-stop-words-removal # using the approach that Ryan recommends
start = time.time()
cache = set(stopwords.words("english"))
def remove_stopwords(review):
    text = " ".join([word for word in review.split() if word not in cache])
    return text

# Remove the stop words from both columns
df.Negative_Review = df.Negative_Review.apply(remove_stopwords)   
df.Positive_Review = df.Positive_Review.apply(remove_stopwords)
```

----------------------------------------

TITLE: Combining Multiple Features for Regression Input Python
DESCRIPTION: This snippet constructs a feature matrix `X` for a regression model by combining several features from the `new_pumpkins` DataFrame. It includes one-hot encoded versions of the 'Variety', 'City', and 'Package' categorical columns, as well as the numeric 'Month' column, joined together into a single DataFrame. The target variable `y` is set to the 'Price' column. This integrated feature set provides a richer input for training a more accurate predictive model.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/README.md#_snippet_17

LANGUAGE: python
CODE:
```
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

----------------------------------------

TITLE: Scaling Testing Data (Python)
DESCRIPTION: Applies the 'transform' method of the *same* MinMaxScaler fitted on the training data to the testing dataset's 'load' column. This scales the test data using parameters learned from the training data, maintaining consistency and preventing data leakage.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_8

LANGUAGE: python
CODE:
```
test['load'] = scaler.transform(test)
```

----------------------------------------

TITLE: Cleaning Data - Drop Nulls and Filter by Duration (Python)
DESCRIPTION: Cleans the DataFrame by removing any rows that contain null or missing values using `dropna(inplace=True)`. It then filters the data to include only sightings with a duration between 1 and 60 seconds, inclusive. Finally, it prints the information about the cleaned DataFrame using `info()`, showing the number of non-null entries and data types.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/3-Web-App/1-Web-App/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
ufos.dropna(inplace=True)

ufos = ufos[(ufos['Seconds'] >= 1) & (ufos['Seconds'] <= 60)]

ufos.info()
```

----------------------------------------

TITLE: Cleaning Data by Removing Missing Values (Python)
DESCRIPTION: This snippet removes rows containing any missing values (NaN) from the pie_pumpkins DataFrame in place using dropna(inplace=True). It then prints a summary of the DataFrame's structure and non-null counts using info(). Dependencies: pandas DataFrame pie_pumpkins.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/2-Regression/3-Linear/README.md#_snippet_4

LANGUAGE: python
CODE:
```
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

----------------------------------------

TITLE: Preparing Features and Target for Scikit-learn (Python)
DESCRIPTION: This code prepares the feature matrix (X) and target vector (y) from the pie_pumpkins DataFrame for use with scikit-learn. 'DayOfYear' is converted to a numpy array and reshaped into a 2D array (-1, 1) as required by scikit-learn models, while 'Price' is assigned as the target. Dependencies: pandas DataFrame pie_pumpkins, numpy, scikit-learn (implicitly for model input format).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/2-Regression/3-Linear/README.md#_snippet_6

LANGUAGE: python
CODE:
```
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

----------------------------------------

TITLE: Performing K-Means Clustering and Predicting Labels (Python)
DESCRIPTION: Initializes and fits a K-Means model from `sklearn.cluster` with a specified number of clusters (3) to the prepared feature data `X`. The `random_state` ensures reproducibility. After fitting, it predicts the cluster assignment for each data point in `X`, storing the resulting array of cluster labels (0, 1, or 2) in `y_cluster_kmeans`. Requires `sklearn.cluster.KMeans`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/de/5-Clustering/2-K-Means/README.md#_snippet_2

LANGUAGE: python
CODE:
```
from sklearn.cluster import KMeans

nclusters = 3 
seed = 0

km = KMeans(n_clusters=nclusters, random_state=seed, n_init=10)
km.fit(X)

# Predict the cluster for each data point

y_cluster_kmeans = km.predict(X)
y_cluster_kmeans
```

----------------------------------------

TITLE: Balancing Data with SMOTE (Imblearn Python)
DESCRIPTION: Applies the Synthetic Minority Over-sampling Technique (SMOTE) to the feature and label dataframes (`feature_df`, `labels_df`). SMOTE generates synthetic samples for minority classes to equalize the number of samples across all classes, producing `transformed_feature_df` and `transformed_label_df`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/notebook.ipynb#_snippet_15

LANGUAGE: Python
CODE:
```
oversample = SMOTE()
transformed_feature_df, transformed_label_df = oversample.fit_resample(feature_df, labels_df)
```

----------------------------------------

TITLE: Defining Ordinal Encoder with Scikit-learn (Python)
DESCRIPTION: This snippet imports the OrdinalEncoder class from scikit-learn and defines an instance configured for encoding the Item Size column. It specifies the ordered categories ('sml', 'med', etc.) and identifies the feature column name to be encoded.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/README.md#_snippet_3

LANGUAGE: python
CODE:
```
from sklearn.preprocessing import OrdinalEncoder

item_size_categories = [['sml', 'med', 'med-lge', 'lge', 'xlge', 'jbo', 'exjbo']]
ordinal_features = ['Item Size']
ordinal_encoder = OrdinalEncoder(categories=item_size_categories)
```

----------------------------------------

TITLE: Preparing Data for Scikit-learn Linear Regression Python
DESCRIPTION: This snippet prepares the feature and label data from the `pie_pumpkins` DataFrame for use with scikit-learn's Linear Regression model. It extracts the 'DayOfYear' column into the feature array `X`, converting it to a NumPy array and reshaping it to the required 2D format (N, 1). The 'Price' column is extracted into the label array `y`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/3-Linear/README.md#_snippet_6

LANGUAGE: python
CODE:
```
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

----------------------------------------

TITLE: Pickling and Loading Trained Model (Python)
DESCRIPTION: Imports the `pickle` module to serialize Python objects. It defines a filename ('ufo-model.pkl') and uses `pickle.dump` to save the trained logistic regression model to this file in binary write mode ('wb'). It then demonstrates loading the model back from the file using `pickle.load` in binary read mode ('rb') and makes a prediction using the loaded model on a sample input array `[[50,44,-12]]` to show how a saved model can be used. Requires the built-in pickle module.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/3-Web-App/1-Web-App/README.md#_snippet_6

LANGUAGE: Python
CODE:
```
import pickle
model_filename = 'ufo-model.pkl'
pickle.dump(model, open(model_filename,'wb'))

model = pickle.load(open('ufo-model.pkl','rb'))
print(model.predict([[50,44,-12]]))
```

----------------------------------------

TITLE: Training Linear Regression Model with Scikit-learn in Python
DESCRIPTION: Initializes a `LinearRegression` model object from Scikit-learn. The model is then trained on the training data (`X_train`, `y_train`) using the `fit()` method. After training, the model learns the optimal coefficients for the linear relationship.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/2-Regression/3-Linear/README.md#_snippet_8

LANGUAGE: python
CODE:
```
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

----------------------------------------

TITLE: Calculating ROC AUC Score with Scikit-learn in Python
DESCRIPTION: This snippet calculates the Area Under the ROC Curve (AUC) score for a classification model. It uses `sklearn.metrics.roc_auc_score` with the true labels (`y_test`) and the predicted probabilities for the positive class (`y_scores[:,1]`). The AUC value provides a single summary metric of the model's ability to discriminate between classes, ranging from 0 to 1, where a higher value indicates better performance.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/4-Logistic/README.md#_snippet_14

LANGUAGE: python
CODE:
```
auc = roc_auc_score(y_test,y_scores[:,1])
print(auc)
```

----------------------------------------

TITLE: Defining and Fitting SARIMAX Model in Python
DESCRIPTION: This code defines and fits a SARIMAX model to the training data (`train`). It specifies the non-seasonal (`order`) and seasonal (`seasonal_order`) parameters for the model. The fitted model's summary is then printed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/7-TimeSeries/2-ARIMA/README.md#_snippet_1

LANGUAGE: python
CODE:
```
order = (4, 1, 0)
seasonal_order = (1, 1, 0, 24)

model = SARIMAX(endog=train, order=order, seasonal_order=seasonal_order)
results = model.fit()

print(results.summary())
```

----------------------------------------

TITLE: Perform K-Means Clustering and Visualize Clusters with Scatter Plot in Python
DESCRIPTION: Initializes and fits a K-Means model with a specified number of clusters (set to 3 based on the elbow method/prior knowledge). Predicts the cluster labels for each data point. Creates a scatter plot visualizing two specific features ('popularity' and 'danceability') from the original DataFrame, with points colored according to their assigned cluster label. Uses `sklearn.cluster.KMeans` and `matplotlib.pyplot`. Requires the feature matrix `X` and the original pandas DataFrame `df`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/README.md#_snippet_6

LANGUAGE: python
CODE:
```
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters = 3)
kmeans.fit(X)
labels = kmeans.predict(X)
plt.scatter(df['popularity'],df['danceability'],c = labels)
plt.xlabel('popularity')
plt.ylabel('danceability')
plt.show()
```

----------------------------------------

TITLE: Training Polynomial Regression Pipeline with Scikit-learn in Python
DESCRIPTION: Creates a Scikit-learn pipeline that first transforms input data by adding polynomial features of degree 2 using `PolynomialFeatures(2)` and then fits a `LinearRegression` model to the transformed data. The pipeline is then trained on the training data (`X_train`, `y_train`) using the `fit()` method.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/2-Regression/3-Linear/README.md#_snippet_13

LANGUAGE: python
CODE:
```
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

----------------------------------------

TITLE: Function to Compare Multiple Models in R
DESCRIPTION: Defines a function 'compare_models' that takes a list of workflows, training data, and test data. It iterates through the workflow list, fits each workflow to the training data, makes predictions on the test data, and calculates a predefined set of performance metrics for each model, returning a data frame of results. Requires the 'purrr' package and a 'metric_set' object.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/solution/R/lesson_12-R.ipynb#_snippet_11

LANGUAGE: R
CODE:
```
set.seed(2056)

# Create a metric set
eval_metrics <- metric_set(ppv, sens, accuracy, f_meas)

# Define a function that returns performance metrics
compare_models <- function(workflow_list, train_set, test_set){
  
  suppressWarnings(
    # Fit each model to the train_set
    map(workflow_list, fit, data = train_set) %>%
    # Make predictions on the test set
      map_dfr(augment, new_data = test_set, .id = "model") %>%
    # Select desired columns
      select(model, cuisine, .pred_class) %>%
    # Evaluate model performance
      group_by(model) %>%
      eval_metrics(truth = cuisine, estimate = .pred_class) %>%
      ungroup()
  )
  
} # End of function
```

----------------------------------------

TITLE: Calculating ROC AUC Score (Python)
DESCRIPTION: Calculates the Area Under the ROC Curve (AUC) score using the `roc_auc_score` function. The AUC provides a single scalar value that summarizes the performance of the classifier across all possible classification thresholds. An AUC of 1.0 represents a perfect classifier.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/README.md#_snippet_14

LANGUAGE: python
CODE:
```
auc = roc_auc_score(y_test,y_scores[:,1])
print(auc)
```

----------------------------------------

TITLE: Preparing Data for Scikit-learn Linear Regression in Python
DESCRIPTION: Selects the 'DayOfYear' column as the input feature `X` and the 'Price' column as the target variable `y` from the `pie_pumpkins` DataFrame. `X` is converted to a NumPy array and reshaped to `(-1, 1)` as required by Scikit-learn models which expect 2D input arrays.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/2-Regression/3-Linear/README.md#_snippet_6

LANGUAGE: python
CODE:
```
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

----------------------------------------

TITLE: Preparing Data for Regression - Pandas/NumPy Python
DESCRIPTION: Selects the 'DayOfYear' column as the feature (`X`) and the 'Price' column as the target (`y`) from the `pie_pumpkins` DataFrame. It converts 'DayOfYear' to a NumPy array and reshapes it to a 2D array `(-1, 1)` as required by scikit-learn's `LinearRegression` model for a single feature input. Requires pandas and NumPy.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/2-Regression/3-Linear/README.md#_snippet_6

LANGUAGE: Python
CODE:
```
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

----------------------------------------

TITLE: Calculating Correlation with Pandas - Python
DESCRIPTION: This snippet calculates the Pearson correlation coefficient between the 'Month' column and the 'Price' column, and between the 'DayOfYear' column and the 'Price' column of a pandas DataFrame named `new_pumpkins`. It then prints these correlation values to the console, indicating the linear relationship strength and direction.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/2-Regression/3-Linear/README.md#_snippet_0

LANGUAGE: python
CODE:
```
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

----------------------------------------

TITLE: Calculating Confusion Matrix Scikit-learn Python
DESCRIPTION: This snippet calculates and prints the confusion matrix based on the true test labels (y_test) and the model's predictions (predictions). The confusion matrix helps visualize the performance of the classification model by showing true positives, true negatives, false positives, and false negatives. It requires the `confusion_matrix` function from `sklearn.metrics`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/2-Regression/4-Logistic/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
    from sklearn.metrics import confusion_matrix
    confusion_matrix(y_test, predictions)
```

----------------------------------------

TITLE: Defining Multiple Classifiers Python
DESCRIPTION: This snippet initializes instances of various scikit-learn classification models, including Support Vector Classifier (SVC), K-Nearest Neighbors (KNN), Random Forest, and AdaBoost. These initialized models are stored in a dictionary, allowing for easy iteration and comparison. Common parameters like `C` for regularization or `n_estimators` are set here.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/solution/notebook.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
C = 10
# Create different classifiers.
classifiers = {
    'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0),
    'KNN classifier': KNeighborsClassifier(C),
    'SVC': SVC(),
    'RFST': RandomForestClassifier(n_estimators=100),
    'ADA': AdaBoostClassifier(n_estimators=100)
    
}
```

----------------------------------------

TITLE: Evaluating Linear Regression Model with MSE in Python
DESCRIPTION: This code makes predictions on the test set (X_test) using the trained linear regression model's .predict() method. It then calculates the root mean squared error (RMSE) between the predicted values ('pred') and the actual test labels ('y_test') using mean_squared_error and np.sqrt.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/README.md#_snippet_10

LANGUAGE: Python
CODE:
```
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

----------------------------------------

TITLE: Selecting and Reshaping BMI Feature - Python
DESCRIPTION: Selects the third column (index 2), which represents the BMI feature, from the dataset's feature matrix X. It then reshapes this selected feature into a 2D array, which is a required format for many Scikit-learn models and plotting functions when dealing with a single feature.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/2-Regression/1-Tools/README.md#_snippet_2

LANGUAGE: python
CODE:
```
X = X[:, 2]
X = X.reshape((-1,1))
```

----------------------------------------

TITLE: Loading & Filtering Pumpkin Data with Pandas
DESCRIPTION: Imports pandas and matplotlib, loads pumpkin data from a CSV file into a DataFrame, and filters the DataFrame to include only entries where the 'Package' column contains 'bushel'. Finally, it displays the first few rows of the filtered data. Requires pandas and matplotlib libraries. Input: CSV file path. Output: Filtered pandas DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/solution/notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import pandas as pd
import matplotlib.pyplot as plt
pumpkins = pd.read_csv('../../data/US-pumpkins.csv')

pumpkins = pumpkins[pumpkins['Package'].str.contains('bushel', case=True, regex=True)]

pumpkins.head()
```

----------------------------------------

TITLE: Select Target Variable with Pandas Python
DESCRIPTION: This snippet selects the 'cuisine' column from the loaded DataFrame to be used as the target variable for the classification task. The resulting pandas Series contains the labels for each data instance.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/solution/notebook.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
cuisines_label_df = cuisines_df['cuisine']
cuisines_label_df.head()
```

----------------------------------------

TITLE: Predict Classes & Probabilities (Tidymodels/workflows/dplyr) - R
DESCRIPTION: Uses the fitted workflow (`mr_fit`) to make two types of predictions on the test data (`cuisines_test`): hard class predictions (default `type`) and class probability predictions (`type = "prob"`). It then combines these predictions along with the true labels for detailed analysis of the model's probabilistic outputs.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/solution/R/lesson_11-R.ipynb#_snippet_11

LANGUAGE: R
CODE:
```
# Make hard class prediction and probabilities
results_prob <- cuisines_test %>%
  select(cuisine) %>%
  bind_cols(mr_fit %>%
    predict(new_data = cuisines_test)) %>%
  bind_cols(mr_fit %>%
    predict(new_data = cuisines_test, type = "prob"))

# Print out results
results_prob %>%
  slice_head(n = 5)
```

----------------------------------------

TITLE: Training an SVC Model (Python)
DESCRIPTION: Initializes a Support Vector Classifier (SVC) model with a linear kernel, a regularization parameter C of 10, and probability estimation enabled. The model is then trained using the training data (`X_train`, `y_train`), reshaping the target variable as needed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/solution/notebook.ipynb#_snippet_7

LANGUAGE: Python
CODE:
```
model = SVC(kernel='linear', C=10, probability=True,random_state=0)
model.fit(X_train,y_train.values.ravel())
```

----------------------------------------

TITLE: Preparing Data for Regression with One-Hot Encoded Variety - Python
DESCRIPTION: Defines the input features `X` for a regression model by selecting the one-hot encoded 'Variety' columns generated by `pd.get_dummies()`. Sets the target variable `y` as the 'Price' column from the processed DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/notebook.ipynb#_snippet_16

LANGUAGE: python
CODE:
```
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

----------------------------------------

TITLE: Creating and Fitting Polynomial Regression Pipeline
DESCRIPTION: This snippet creates a Scikit-learn pipeline that first transforms the input data by adding polynomial features of degree 2 using `PolynomialFeatures(2)` and then trains a `LinearRegression` model on the transformed data. The pipeline is then fitted to the training data (`X_train`, `y_train`).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/2-Regression/3-Linear/README.md#_snippet_13

LANGUAGE: python
CODE:
```
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

----------------------------------------

TITLE: Filter DataFrame by String Content in Column (Python)
DESCRIPTION: Filters the DataFrame to include only rows where the 'Package' column contains the string 'bushel'. This is achieved by using the `.str.contains()` method on the 'Package' series and boolean indexing. The filter helps standardize units for price analysis.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/README.md#_snippet_5

LANGUAGE: python
CODE:
```
pumpkins = pumpkins[pumpkins['Package'].str.contains('bushel', case=True, regex=True)]
```

----------------------------------------

TITLE: Visualizing ROC Curve with Matplotlib (Python)
DESCRIPTION: Generates data points for the Receiver Operating Characteristic (ROC) curve using `roc_curve` and plots it using Matplotlib. The ROC curve visualizes the trade-off between the True Positive Rate and False Positive Rate at various threshold settings, helping assess the classifier's discriminatory power.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/README.md#_snippet_13

LANGUAGE: python
CODE:
```
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline

y_scores = model.predict_proba(X_test)
fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])

fig = plt.figure(figsize=(6, 6))
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()
```

----------------------------------------

TITLE: Retrieving Linear Regression Model Parameters - Python
DESCRIPTION: Prints the coefficient(s) and the intercept calculated by the trained `LinearRegression` model. These values define the equation of the fitted line (y = coef * x + intercept) representing the learned linear relationship between 'DayOfYear' and 'Price'.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/notebook.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
lin_reg.coef_, lin_reg.intercept_
```

----------------------------------------

TITLE: Dropping Columns and Saving DataFrame to CSV - Python
DESCRIPTION: Removes specified columns from the DataFrame in place and then saves the modified DataFrame to a CSV file without the DataFrame index. This step cleans the dataset by removing potentially redundant or less useful columns before saving.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
df.drop(["Review_Total_Negative_Word_Counts", "Review_Total_Positive_Word_Counts", "days_since_review", "Total_Number_of_Reviews_Reviewer_Has_Given"], axis = 1, inplace=True)

# Saving new data file with calculated columns
print("Saving results to Hotel_Reviews_Filtered.csv")
df.to_csv(r'../data/Hotel_Reviews_Filtered.csv', index = False)
```

----------------------------------------

TITLE: Loading Hotel Reviews Data - Python
DESCRIPTION: This snippet loads the hotel reviews dataset from a CSV file into a pandas DataFrame. It also records the starting time to measure the duration of the preprocessing steps.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/1-notebook.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
# Load the hotel reviews from CSV
start = time.time()
df = pd.read_csv('../../data/Hotel_Reviews.csv')
```

----------------------------------------

TITLE: Splitting Data into Training and Test Sets in R
DESCRIPTION: Sets a random seed for reproducible data splitting. It uses initial_split() from rsample to define a 67/33 split of the selected data. The training() and testing() functions then extract the respective data frames for model development and evaluation. The first 10 rows of the training set are displayed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/solution/R/lesson_1-R.ipynb#_snippet_4

LANGUAGE: R
CODE:
```
set.seed(2056)
# Split 67% of the data for training and the rest for tesing
diabetes_split <- diabetes_select %>%
  initial_split(prop = 0.67)

# Extract the resulting train and test sets
diabetes_train <- training(diabetes_split)
diabetes_test <- testing(diabetes_split)

# Print the first 3 rows of the training set
diabetes_train %>%
  slice(1:10)
```

----------------------------------------

TITLE: Splitting Data into Training and Test Sets in R
DESCRIPTION: Splits the 'pumpkins_select' data into 80% for training and 20% for testing using initial_split from the rsample package. Sets a seed for reproducibility and extracts the training and testing dataframes. Requires the rsample package.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/R/lesson_4-R.ipynb#_snippet_8

LANGUAGE: R
CODE:
```
# Split data into 80% for training and 20% for testing
set.seed(2056)
pumpkins_split <- pumpkins_select %>%
  initial_split(prop = 0.8)

# Extract the data in each split
pumpkins_train <- training(pumpkins_split)
pumpkins_test <- testing(pumpkins_split)

# Print out the first 5 rows of the training set
pumpkins_train %>%
  slice_head(n = 5)
```

----------------------------------------

TITLE: Cleaning UFO Data Python
DESCRIPTION: This snippet performs data cleaning on the UFO DataFrame. It removes rows with any missing values using `dropna(inplace=True)`. It then filters the DataFrame to keep only records where the 'Seconds' duration is between 1 and 60 seconds, inclusive. Finally, it prints concise information about the cleaned DataFrame, including the index dtype and column dtypes, non-null values, and memory usage. Requires the processed pandas DataFrame from the previous step.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/3-Web-App/1-Web-App/solution/notebook.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
ufos.dropna(inplace=True)

ufos = ufos[(ufos['Seconds'] >= 1) & (ufos['Seconds'] <= 60)]

ufos.info()
```

----------------------------------------

TITLE: Splitting Data into Training and Testing Sets - Python
DESCRIPTION: This snippet uses `train_test_split` from `sklearn.model_selection` to divide the `encoded_pumpkins` DataFrame into features (X) and target (y) and then splits these into training (X_train, y_train) and testing (X_test, y_test) sets with a test size of 20% and a fixed random state.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/notebook.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
from sklearn.model_selection import train_test_split
# X is the encoded features
X = encoded_pumpkins[encoded_pumpkins.columns.difference(['Color'])]
# y is the encoded label
y = encoded_pumpkins['Color']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

----------------------------------------

TITLE: Split Data into Training and Test Sets Python
DESCRIPTION: This snippet uses the `train_test_split` function from scikit-learn to divide the feature and label DataFrames into training and testing sets. A `test_size` of 0.3 means 30% of the data will be used for testing.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/solution/notebook.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
```

----------------------------------------

TITLE: Function for Running Linear Regression and Evaluation - Python
DESCRIPTION: Defines a reusable function `run_linear_regression` that encapsulates the process of splitting data (80/20 train/test), training a `LinearRegression` model, predicting on the test set, and printing the calculated RMSE and R2 score. Calls this function using the one-hot encoded 'Variety' as features to evaluate its performance.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/notebook.ipynb#_snippet_17

LANGUAGE: python
CODE:
```
def run_linear_regression(X,y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
    lin_reg = LinearRegression()
    lin_reg.fit(X_train,y_train)

    pred = lin_reg.predict(X_test)

    mse = np.sqrt(mean_squared_error(y_test,pred))
    print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

    score = lin_reg.score(X_train,y_train)
    print('Model determination: ', score)

run_linear_regression(X,y)
```

----------------------------------------

TITLE: Training Polynomial Regression Model via Pipeline - Scikit-learn Python
DESCRIPTION: Creates a scikit-learn `pipeline` that first transforms the input data by adding polynomial features up to degree 2 (`PolynomialFeatures(2)`) and then applies `LinearRegression` to the transformed data. The entire pipeline is then trained (`fit`) using the training data (`X_train`, `y_train`). Requires scikit-learn.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/2-Regression/3-Linear/README.md#_snippet_13

LANGUAGE: Python
CODE:
```
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

----------------------------------------

TITLE: Removing Stop Words (NLTK/Pandas/Python)
DESCRIPTION: This snippet defines a function `remove_stopwords` that takes a text string, splits it into words, and reconstructs the string excluding words found in the NLTK English stop words cache. It then applies this function to the 'Negative_Review' and 'Positive_Review' columns of the DataFrame `df` to clean the text data. Requires NLTK's stopwords dataset and a Pandas DataFrame with the specified review columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_3

LANGUAGE: python
CODE:
```
from nltk.corpus import stopwords

# Load the hotel reviews from CSV
df = pd.read_csv("../../data/Hotel_Reviews_Filtered.csv")

# Remove stop words - can be slow for a lot of text!
# Ryan Han (ryanxjhan on Kaggle) has a great post measuring performance of different stop words removal approaches
# https://www.kaggle.com/ryanxjhan/fast-stop-words-removal # using the approach that Ryan recommends
start = time.time()
cache = set(stopwords.words("english"))
def remove_stopwords(review):
    text = " ".join([word for word in review.split() if word not in cache])
    return text

# Remove the stop words from both columns
df.Negative_Review = df.Negative_Review.apply(remove_stopwords)
df.Positive_Review = df.Positive_Review.apply(remove_stopwords)
```

----------------------------------------

TITLE: Plotting ROC Curve with Matplotlib in Python
DESCRIPTION: This snippet visualizes the Receiver Operating Characteristic (ROC) curve to evaluate a classification model's performance. It uses `sklearn.metrics.roc_curve` to compute the False Positive Rate (FPR) and True Positive Rate (TPR) from true labels (`y_test`) and predicted probabilities (`y_scores`), then plots them using `matplotlib.pyplot`. This plot shows the trade-off between true positives and false positives at different classification thresholds.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/4-Logistic/README.md#_snippet_13

LANGUAGE: python
CODE:
```
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline

y_scores = model.predict_proba(X_test)
fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])

fig = plt.figure(figsize=(6, 6))
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()
```

----------------------------------------

TITLE: Inverse Transforming Original Targets (Python)
DESCRIPTION: Applies the inverse transformation to the original, scaled target values for both the training and testing sets. This step ensures that the actual values are also in their original scale, allowing for a direct comparison with the inverse-transformed predictions during evaluation.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_18

LANGUAGE: python
CODE:
```
y_train = scaler.inverse_transform(y_train)
y_test = scaler.inverse_transform(y_test)
```

----------------------------------------

TITLE: Performing Rolling SARIMAX Forecast Evaluation Python
DESCRIPTION: Implements a rolling forecast process over the test data. It maintains a fixed-size history window (`training_window`), iteratively refits the SARIMAX model on the history, forecasts `HORIZON` steps ahead, records the predictions, and updates the history with the actual observed value before moving to the next test point.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/solution/notebook.ipynb#_snippet_13

LANGUAGE: python
CODE:
```
%%time
training_window = 720 # dedicate 30 days (720 hours) for training

train_ts = train['load']
test_ts = test_shifted

history = [x for x in train_ts]
history = history[(-training_window):]

predictions = list()

# let's user simpler model for demonstration
order = (2, 1, 0)
seasonal_order = (1, 1, 0, 24)

for t in range(test_ts.shape[0]):
    model = SARIMAX(endog=history, order=order, seasonal_order=seasonal_order)
    model_fit = model.fit()
    yhat = model_fit.forecast(steps = HORIZON)
    predictions.append(yhat)
    obs = list(test_ts.iloc[t])
    # move the training window
    history.append(obs[0])
    history.pop(0)
    print(test_ts.index[t])
    print(t+1, ': predicted =', yhat, 'expected =', obs)
```

----------------------------------------

TITLE: Cleaning and Preparing Pumpkin Data - Python
DESCRIPTION: Filters the raw data to include only 'bushel' packages. Calculates the average price from high and low prices. Extracts the month and day of the year from the date column. Creates a new DataFrame with relevant and processed columns, adjusting prices for specific package sizes (1 1/9 and 1/2 bushel).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/notebook.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
pumpkins = pumpkins[pumpkins['Package'].str.contains('bushel', case=True, regex=True)]

new_columns = ['Package', 'Variety', 'City Name', 'Month', 'Low Price', 'High Price', 'Date']
pumpkins = pumpkins.drop([c for c in pumpkins.columns if c not in new_columns], axis=1)

price = (pumpkins['Low Price'] + pumpkins['High Price']) / 2

month = pd.DatetimeIndex(pumpkins['Date']).month
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)

new_pumpkins = pd.DataFrame(
    {'Month': month, 
     'DayOfYear' : day_of_year, 
     'Variety': pumpkins['Variety'], 
     'City': pumpkins['City Name'], 
     'Package': pumpkins['Package'], 
     'Low Price': pumpkins['Low Price'],
     'High Price': pumpkins['High Price'], 
     'Price': price})

new_pumpkins.loc[new_pumpkins['Package'].str.contains('1 1/9'), 'Price'] = price/1.1
new_pumpkins.loc[new_pumpkins['Package'].str.contains('1/2'), 'Price'] = price*2

new_pumpkins.head()
```

----------------------------------------

TITLE: Importing necessary Python libraries for data analysis
DESCRIPTION: Imports key libraries for data manipulation, visualization, numerical operations, and handling imbalanced data. This includes pandas for DataFrame operations, matplotlib for plotting, numpy for numerical tasks, and SMOTE from imblearn for oversampling.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/README.md#_snippet_1

LANGUAGE: python
CODE:
```
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
from imblearn.over_sampling import SMOTE
```

----------------------------------------

TITLE: Train SVC Model Python
DESCRIPTION: Initializes a Support Vector Classifier (`SVC`) model with a linear kernel, C=10, probability estimation enabled, and a fixed random state. The model is then trained using the training data (`X_train`, `y_train`). `y_train.values.ravel()` is used to flatten the target array.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/4-Classification/4-Applied/README.md#_snippet_6

LANGUAGE: python
CODE:
```
model = SVC(kernel='linear', C=10, probability=True,random_state=0)
model.fit(X_train,y_train.values.ravel())
```

----------------------------------------

TITLE: Scaling Training Data using MinMaxScaler (Python)
DESCRIPTION: Initializes a MinMaxScaler, fits it to the training data to learn the scaling parameters, and then transforms the training data. This scales the 'load' values to the range (0, 1).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
scaler = MinMaxScaler()
train['load'] = scaler.fit_transform(train)
train.head(5)
```

----------------------------------------

TITLE: Loading CSV Data with Pandas - Python
DESCRIPTION: This snippet demonstrates how to load a large CSV file containing hotel reviews into a pandas DataFrame. It imports the pandas library for data manipulation and the time library to measure the loading duration, providing feedback on the process.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_0

LANGUAGE: python
CODE:
```
# Load the hotel reviews from CSV
import pandas as pd
import time
# importing time so the start and end time can be used to calculate file loading time
print("Loading data file now, this could take a while depending on file size")
start = time.time()
# df is 'DataFrame' - make sure you downloaded the file to the data folder
df = pd.read_csv('../../data/Hotel_Reviews.csv')
end = time.time()
print("Loading took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Getting DataFrame Information (Pandas Python)
DESCRIPTION: Prints a concise summary of the `df` DataFrame. This includes the column names, non-null counts, data types for each column, and memory usage, which is useful for checking data completeness and types.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/notebook.ipynb#_snippet_4

LANGUAGE: Python
CODE:
```
df.info()
```

----------------------------------------

TITLE: Calculating Silhouette Score for R Clustering
DESCRIPTION: This snippet calculates the average silhouette score for a K-Means clustering result. The silhouette score measures how similar an object is to its own cluster compared to other clusters, helping evaluate cluster separation and compactness. A higher score indicates better-defined clusters.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb#_snippet_8

LANGUAGE: R
CODE:
```
library(cluster)

# Compute average silhouette score
ss <- silhouette(kclust$cluster,
                 # Compute euclidean distance
                 dist = dist(df_numeric_select))
mean(ss[, 3])
```

----------------------------------------

TITLE: Training Polynomial Regression Pipeline with Scikit-learn in Python
DESCRIPTION: This snippet creates a Scikit-learn pipeline that first generates polynomial features of degree 2 from the input data and then trains a LinearRegression model on these transformed features. The entire pipeline is trained using the .fit() method on the training data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/README.md#_snippet_14

LANGUAGE: Python
CODE:
```
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

----------------------------------------

TITLE: Generate Classification Report - Python
DESCRIPTION: Generates and prints a detailed classification report for the trained model's performance on the test set (X_test, y_test). It first predicts the class labels for the test set using model.predict and then uses scikit-learn's classification_report to display precision, recall, f1-score, and support for each class. Requires scikit-learn.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/4-Classification/2-Classifiers-1/README.md#_snippet_3

LANGUAGE: python
CODE:
```
y_pred = model.predict(X_test)
print(classification_report(y_test,y_pred))
```

----------------------------------------

TITLE: Processing & Normalizing Pumpkin Prices with Pandas
DESCRIPTION: Selects specific columns, calculates the average price from 'Low Price' and 'High Price', extracts the month from the 'Date' column, creates a new DataFrame with these features, and normalizes the 'Price' based on fractional bushel sizes ('1 1/9' and '1/2'). Requires a pandas DataFrame with 'Package', 'Low Price', 'High Price', and 'Date' columns. Input: pandas DataFrame. Output: New pandas DataFrame with processed and normalized price and month.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/solution/notebook.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
# A set of new columns for a new dataframe. Filter out nonmatching columns
columns_to_select = ['Package', 'Low Price', 'High Price', 'Date']
pumpkins = pumpkins.loc[:, columns_to_select]

# Get an average between low and high price for the base pumpkin price
price = (pumpkins['Low Price'] + pumpkins['High Price']) / 2

# Convert the date to its month only
month = pd.DatetimeIndex(pumpkins['Date']).month

# Create a new dataframe with this basic data
new_pumpkins = pd.DataFrame({'Month': month, 'Package': pumpkins['Package'], 'Low Price': pumpkins['Low Price'],'High Price': pumpkins['High Price'], 'Price': price})

# Convert the price if the Package contains fractional bushel values
new_pumpkins.loc[new_pumpkins['Package'].str.contains('1 1/9'), 'Price'] = price/(1 + 1/9)

new_pumpkins.loc[new_pumpkins['Package'].str.contains('1/2'), 'Price'] = price/(1/2)

print(new_pumpkins)

```

----------------------------------------

TITLE: Generate Classification Report Python
DESCRIPTION: This snippet generates predictions for the entire test set using the trained model's `predict` method. It then prints a detailed classification report, which includes precision, recall, f1-score, and support for each class.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/solution/notebook.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
y_pred = model.predict(X_test)
print(classification_report(y_test,y_pred))
```

----------------------------------------

TITLE: Training and Evaluating Linear SVC Model in R
DESCRIPTION: Fits the defined linear SVC workflow (`svc_linear_wf`) to the training data (`cuisines_train`). It then defines a set of evaluation metrics (`ppv`, `sens`, `accuracy`, `f_meas`). Finally, it uses the fitted model (`svc_linear_fit`) to make predictions on the test data (`cuisines_test`) and calculates the specified evaluation metrics, comparing the true `cuisine` values with the predicted class (`.pred_class`). Requires `tidymodels` and previously fitted workflow.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/solution/R/lesson_12-R.ipynb#_snippet_5

LANGUAGE: R
CODE:
```
# Train a linear SVC model
svc_linear_fit <- svc_linear_wf %>%
  fit(data = cuisines_train)

# Create a metric set
eval_metrics <- metric_set(ppv, sens, accuracy, f_meas)


# Make predictions and Evaluate model performance
svc_linear_fit %>%
  augment(new_data = cuisines_test) %>%
  eval_metrics(truth = cuisine, estimate = .pred_class)
```

----------------------------------------

TITLE: Encoding Target Label with LabelEncoder (Python)
DESCRIPTION: This snippet imports the LabelEncoder class from scikit-learn, creates an instance, and uses it to encode the Color column of the pumpkins DataFrame. It converts the categorical labels ('ORANGE', 'WHITE') into numerical values (0, 1).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/README.md#_snippet_6

LANGUAGE: python
CODE:
```
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
encoded_label = label_encoder.fit_transform(pumpkins['Color'])
```

----------------------------------------

TITLE: Plotting Average Price by Variety (Python)
DESCRIPTION: This snippet calculates the mean price for each pumpkin 'Variety' using pandas `groupby()` and `mean()` functions. The results are then plotted as a bar chart using the pandas plotting capability (`plot(kind='bar')`) to visually represent the average price differences between distinct pumpkin varieties.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/de/2-Regression/3-Linear/README.md#_snippet_2

LANGUAGE: python
CODE:
```
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

----------------------------------------

TITLE: Plotting Mean Price by Category Bar Chart (Python)
DESCRIPTION: This snippet calculates the mean price for each pumpkin variety using pandas' groupby() method, then plots the results as a bar chart. This visualization helps compare the average price across different pumpkin types. Dependencies: pandas DataFrame new_pumpkins, matplotlib (implicitly used by pandas plotting).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/2-Regression/3-Linear/README.md#_snippet_2

LANGUAGE: python
CODE:
```
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

----------------------------------------

TITLE: Calculating VADER Sentiment Score (NLTK/Python)
DESCRIPTION: This snippet initializes the VADER `SentimentIntensityAnalyzer`. It then defines a function `calc_sentiment` designed to compute the sentiment score for a review text. The function returns 0 immediately if the input is the placeholder text "No Negative" or "No Positive"; otherwise, it uses the VADER analyzer to compute polarity scores and returns the 'compound' score, which is a normalized, overall sentiment score.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_4

LANGUAGE: python
CODE:
```
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Create the vader sentiment analyser (there are others in NLTK you can try too)
vader_sentiment = SentimentIntensityAnalyzer()
# Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.

# There are 3 possibilities of input for a review:
# It could be "No Negative", in which case, return 0
# It could be "No Positive", in which case, return 0
# It could be a review, in which case calculate the sentiment
def calc_sentiment(review):
    if review == "No Negative" or review == "No Positive":
        return 0
    return vader_sentiment.polarity_scores(review)["compound"]
```

----------------------------------------

TITLE: Combining Multiple One-Hot Encoded and Numeric Features for Regression (Python)
DESCRIPTION: Shows how to build a more comprehensive feature matrix `X` by combining multiple one-hot encoded categorical columns ('Variety', 'City', 'Package') with a numeric column ('Month') using pandas `get_dummies` and `join`. The target vector `y` remains the 'Price' column, creating a richer dataset for training a more accurate regression model.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/sw/2-Regression/3-Linear/README.md#_snippet_2

LANGUAGE: python
CODE:
```
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

----------------------------------------

TITLE: Encoding Target Variable 'Color' (Python)
DESCRIPTION: Imports `LabelEncoder` from `sklearn.preprocessing`. Initializes a `LabelEncoder` instance. Fits the encoder to the 'Color' column of the `pumpkins` DataFrame and transforms it into numerical labels (likely 0 for one color and 1 for the other).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/4-Logistic/README.md#_snippet_6

LANGUAGE: Python
CODE:
```
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
encoded_label = label_encoder.fit_transform(pumpkins['Color'])
```

----------------------------------------

TITLE: Check for Missing Values with Pandas (Python)
DESCRIPTION: Checks for missing (NaN) values in the DataFrame. The `isnull()` function identifies nulls, and chaining `.sum()` counts the number of missing values per column. This helps assess data completeness before further processing.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/README.md#_snippet_1

LANGUAGE: python
CODE:
```
pumpkins.isnull().sum()
```

----------------------------------------

TITLE: Initializing One-Hot Encoder for Categorical Features - Python
DESCRIPTION: This code imports `OneHotEncoder` from `sklearn.preprocessing` and initializes it with `sparse_output=False` to get a dense array output. It defines the list of categorical features to be encoded.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/notebook.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
from sklearn.preprocessing import OneHotEncoder
# Encode all the other features using one-hot encoding
categorical_features = ['City Name', 'Package', 'Variety', 'Origin']
categorical_encoder = OneHotEncoder(sparse_output=False)
```

----------------------------------------

TITLE: Q-Learning Training Loop Python
DESCRIPTION: This Python code snippet implements the core Q-Learning training loop over a specified number of epochs. In each epoch, it simulates an agent starting at a random position, selects actions probabilistically based on current Q-values using the `probs` function, executes the action, calculates the reward, and updates the Q-Table using the Bellman equation with a decaying learning rate.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/README.md#_snippet_7

LANGUAGE: Python
CODE:
```
for epoch in range(5000):
    
    # Pick initial point
    m.random_start()
    
    # Start travelling
    n=0
    cum_reward = 0
    while True:
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        dpos = actions[a]
        m.move(dpos,check_correctness=False) # we allow player to move outside the board, which terminates episode
        r = reward(m)
        cum_reward += r
        if r==end_reward or cum_reward < -1000:
            lpath.append(n)
            break
        alpha = np.exp(-n / 10e5)
        gamma = 0.5
        ai = action_idx[a]
        Q[x,y,ai] = (1 - alpha) * Q[x,y,ai] + alpha * (r + gamma * Q[x+dpos[0], y+dpos[1]].max())
        n+=1
```

----------------------------------------

TITLE: Generating Confusion Matrix with Scikit-learn - Python
DESCRIPTION: This snippet calculates and prints a confusion matrix to assess the performance of the classification model. It compares the true labels (y_test) with the model's predicted labels (predictions). The matrix helps visualize true/false positives and negatives. Requires the `sklearn.metrics` module.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/4-Logistic/README.md#_snippet_12

LANGUAGE: python
CODE:
```
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, predictions)
```

----------------------------------------

TITLE: Applying Column Transformer for Feature Encoding - Python
DESCRIPTION: This snippet uses `ColumnTransformer` from `sklearn.compose` to apply the previously initialized `OrdinalEncoder` to `ordinal_features` and the `OneHotEncoder` to `categorical_features` simultaneously. It sets the output to pandas DataFrame format and applies the transformation (`fit_transform`) to the `pumpkins` data, displaying the result.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/notebook.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
from sklearn.compose import ColumnTransformer
ct = ColumnTransformer(transformers=[
     ('ord', ordinal_encoder, ordinal_features),
     ('cat', categorical_encoder, categorical_features)
     ])
# Get the encoded features as a pandas DataFrame
ct.set_output(transform='pandas')
encoded_features = ct.fit_transform(pumpkins)
encoded_features.head()
```

----------------------------------------

TITLE: Preparing Data for Linear Regression (Python)
DESCRIPTION: This code prepares the feature and target variables from the `pie_pumpkins` DataFrame for use with Scikit-learn. It extracts 'DayOfYear' as the feature `X`, converting it to a numpy array and reshaping it to the required 2D format (N, 1), and extracts 'Price' as the target variable `y`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/de/2-Regression/3-Linear/README.md#_snippet_6

LANGUAGE: python
CODE:
```
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

----------------------------------------

TITLE: Calculating R-squared (Coefficient of Determination)
DESCRIPTION: Calculates the coefficient of determination (R-squared) for the trained linear regression model (`lin_reg`) using the training data (`X_train`, `y_train`). R-squared indicates the proportion of the variance in the dependent variable that is predictable from the independent variable(s). Requires a trained `LinearRegression` model and training data arrays/Series. Outputs the printed R-squared score.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/fr/2-Regression/3-Linear/README.md#_snippet_10

LANGUAGE: python
CODE:
```
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```

----------------------------------------

TITLE: Creating Recipe to Handle Imbalanced Data with SMOTE (R)
DESCRIPTION: This snippet defines a data preprocessing recipe using 'tidymodels::recipe' for the training data. It includes 'step_smote' from the 'themis' package, which applies the SMOTE algorithm to oversample minority classes in the 'cuisine' column. This recipe is a blueprint for transforming the training data to address class imbalance before model training.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/solution/R/lesson_11-R.ipynb#_snippet_3

LANGUAGE: R
CODE:
```
# Load themis package for dealing with imbalanced data
library(themis)

# Create a recipe for preprocessing training data
cuisines_recipe <- recipe(cuisine ~ ., data = cuisines_train) %>%
  step_smote(cuisine)

# Print recipe
cuisines_recipe
```

----------------------------------------

TITLE: Apply Sentiment Analysis Function to DataFrame Columns using Pandas
DESCRIPTION: This partial code snippet illustrates how to apply the previously defined `calc_sentiment` function (or a similar function) to a column in a pandas DataFrame. It uses the `.apply()` method on a specific DataFrame column to compute the sentiment score for each row. The result of this operation would typically be assigned to a new column in the DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_5

LANGUAGE: python
CODE:
```
df.

```

----------------------------------------

TITLE: Visualizing Average Price per Month with ggplot2 (R)
DESCRIPTION: First calculates the average 'Price' per 'Month' using `group_by` and `summarise` from `dplyr`. The resulting summary data is then piped into `ggplot` to create a bar chart, mapping 'Month' to x, 'mean_price' to y, adding a column layer `geom_col`, setting color and transparency, and labeling the y-axis.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/solution/R/lesson_2-R.ipynb#_snippet_15

LANGUAGE: R
CODE:
```
# Find the average price of pumpkins per month then plot a bar chart
new_pumpkins %>%
  group_by(Month) %>%
  summarise(mean_price = mean(Price)) %>%
  ggplot(aes(x = Month, y = mean_price)) +
  geom_col(fill = "midnightblue", alpha = 0.7) +
  ylab("Pumpkin Price")
```

----------------------------------------

TITLE: Visualizing Average Monthly Pumpkin Price using Pandas/Matplotlib
DESCRIPTION: Groups the `new_pumpkins` DataFrame by 'Month', calculates the mean 'Price' for each month, and generates a bar plot using pandas plotting (which uses matplotlib) to visualize the average pumpkin price per month. Sets the y-axis label. Requires the `new_pumpkins` DataFrame with 'Month' and 'Price' columns and the matplotlib library. Input: pandas DataFrame. Output: Bar plot of average monthly prices displayed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/solution/notebook.ipynb#_snippet_4

LANGUAGE: python
CODE:
```

new_pumpkins.groupby(['Month'])['Price'].mean().plot(kind='bar')
plt.ylabel("Pumpkin Price")

```

----------------------------------------

TITLE: Encoding UFO Country Column Python
DESCRIPTION: This snippet imports the `LabelEncoder` from scikit-learn and applies it to the 'Country' column of the UFO DataFrame. It transforms the categorical country names into numerical labels. The result is assigned back to the 'Country' column, and the head of the modified DataFrame is displayed to show the numerical encoding. Requires scikit-learn and the cleaned pandas DataFrame with the 'Country' column.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/3-Web-App/1-Web-App/solution/notebook.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
from sklearn.preprocessing import LabelEncoder

ufos['Country'] = LabelEncoder().fit_transform(ufos['Country'])

ufos.head()
```

----------------------------------------

TITLE: Applying One-Hot Encoding to Categorical Feature Python
DESCRIPTION: Uses the pandas `get_dummies` function to perform one-hot encoding on the 'Variety' column of the `new_pumpkins` DataFrame. This converts the categorical string values into a numerical representation suitable for machine learning algorithms by creating binary columns for each unique category.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/2-Regression/3-Linear/README.md#_snippet_14

LANGUAGE: python
CODE:
```
pd.get_dummies(new_pumpkins['Variety'])
```

----------------------------------------

TITLE: Calculate Confusion Matrix Summary (Tidymodels/yardstick) - R
DESCRIPTION: Computes and displays summary statistics derived from the confusion matrix calculated from the `results` data. This provides various performance metrics (e.g., accuracy, sensitivity, specificity, precision) that quantitatively assess the model's performance across different classes.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/solution/R/lesson_11-R.ipynb#_snippet_10

LANGUAGE: R
CODE:
```
# Summary stats for confusion matrix
conf_mat(data = results, truth = cuisine, estimate = .pred_class) %>%
summary()
```

----------------------------------------

TITLE: Scaling Training Data MinMaxScaler Python
DESCRIPTION: Initializes a `MinMaxScaler` from scikit-learn and applies it to the 'load' column of the training data (`train`). Scaling transforms the data to a range (default 0 to 1), which can improve the performance of some time series models. The head of the scaled training data is displayed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/solution/notebook.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
scaler = MinMaxScaler()
train['load'] = scaler.fit_transform(train)
train.head(10)
```

----------------------------------------

TITLE: Scaling Training Data (Python)
DESCRIPTION: Initializes a MinMaxScaler and applies it to the training dataset's 'load' column. The 'fit_transform' method calculates the required parameters from the training data and scales it to the range [0, 1], preparing it for the SVR model.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_7

LANGUAGE: python
CODE:
```
scaler = MinMaxScaler()
train['load'] = scaler.fit_transform(train)
```

----------------------------------------

TITLE: Cleaning and Selecting Pumpkin Data (Python)
DESCRIPTION: Selects specific columns ('City Name', 'Package', 'Variety', 'Origin', 'Item Size', 'Color') from the `full_pumpkins` DataFrame and stores them in `pumpkins`. Then, it removes rows with any null values from the `pumpkins` DataFrame in place.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/4-Logistic/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
columns_to_select = ['City Name','Package','Variety', 'Origin','Item Size', 'Color']
pumpkins = full_pumpkins.loc[:, columns_to_select]

pumpkins.dropna(inplace=True)
```

----------------------------------------

TITLE: Dropping Geographic Coordinates - Python
DESCRIPTION: This code removes the 'lat' and 'lng' columns from the DataFrame as they contain geographic coordinates that are not used in the subsequent analysis.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/1-notebook.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
# dropping columns we will not use:
df.drop(["lat", "lng"], axis = 1, inplace=True)
```

----------------------------------------

TITLE: Separating Features and Labels by Dropping Columns - Python
DESCRIPTION: Creates a feature dataframe (`feature_df`) by dropping specified columns ('cuisine', 'Unnamed: 0', 'rice', 'garlic', 'ginger') from the input dataframe (`df`). It also extracts the 'cuisine' column into a separate labels dataframe (`labels_df`). Prepares data for machine learning.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/README.md#_snippet_13

LANGUAGE: python
CODE:
```
feature_df= df.drop(['cuisine','Unnamed: 0','rice','garlic','ginger'], axis=1)
labels_df = df.cuisine #.unique()
feature_df.head()
```

----------------------------------------

TITLE: Visualizing Final R Clusters with Genre
DESCRIPTION: This code snippet assigns the final predicted cluster labels back to the original dataset, incorporating known genre information. It then generates an interactive scatter plot using ggplot2 and plotly, visualizing the clusters based on features like popularity and danceability, while also showing the distribution of genres within these clusters using different shapes.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb#_snippet_13

LANGUAGE: R
CODE:
```
# Add predicted cluster assignment to data set
results <-  augment(final_kmeans, df_numeric_select) %>% 
  bind_cols(df_numeric %>% select(artist_top_genre)) 

# Plot cluster assignments
clust_plt <- results %>% 
  ggplot(mapping = aes(x = popularity, y = danceability, color = .cluster, shape = artist_top_genre)) +
  geom_point(size = 2, alpha = 0.8) +
  paletteer::scale_color_paletteer_d("ggthemes::Tableau_10")

ggplotly(clust_plt)
```

----------------------------------------

TITLE: Generating Descriptive Statistics - Pandas Python
DESCRIPTION: This snippet uses the `describe()` method to generate descriptive statistics for the numerical columns in the DataFrame. It provides count, mean, standard deviation, minimum, maximum, and quartile values. This helps in understanding the central tendency, dispersion, and shape of the data's distribution.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/README.md#_snippet_4

LANGUAGE: python
CODE:
```
df.describe()
```

----------------------------------------

TITLE: Defining Flask App Frontend Structure (HTML)
DESCRIPTION: Creates the HTML structure for the main page (`index.html`) of the Flask application. It includes metadata, links to the CSS stylesheet, a basic layout with a form to input prediction features (seconds, latitude, longitude), a submit button, and a placeholder for displaying the prediction result using Flask's templating syntax (`{{ ... }}`).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/3-Web-App/1-Web-App/README.md#_snippet_8

LANGUAGE: html
CODE:
```
<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8">
    <title>🛸 UFO Appearance Prediction! 👽</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}">
  </head>

  <body>
    <div class="grid">

      <div class="box">

        <p>According to the number of seconds, latitude and longitude, which country is likely to have reported seeing a UFO?</p>

        <form action="{{ url_for('predict')}}" method="post">
          <input type="number" name="seconds" placeholder="Seconds" required="required" min="0" max="60" />
          <input type="text" name="latitude" placeholder="Latitude" required="required" />
          <input type="text" name="longitude" placeholder="Longitude" required="required" />
          <button type="submit" class="btn">Predict country where the UFO is seen</button>
        </form>

        <p>{{ prediction_text }}</p>

      </div>

    </div>

  </body>
</html>
```

----------------------------------------

TITLE: Preparing Features (Variety) and Target for Regression (Python)
DESCRIPTION: Illustrates setting up the feature matrix `X` by applying one-hot encoding to the 'Variety' column using `pd.get_dummies` and defining the target vector `y` using the 'Price' column from the `new_pumpkins` DataFrame. This prepares a basic dataset using only the encoded variety feature for training a regression model.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/sw/2-Regression/3-Linear/README.md#_snippet_1

LANGUAGE: python
CODE:
```
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

----------------------------------------

TITLE: Plotting Average Price by Variety using Pandas Groupby and Plotting Python
DESCRIPTION: This snippet calculates the average price for each unique value in the 'Variety' column of the `new_pumpkins` DataFrame using `groupby()` and `mean()`. The resulting Series, indexed by 'Variety', is then plotted as a bar chart using the `.plot(kind='bar')` method. This visual summary helps compare the mean prices across different pumpkin varieties.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/3-Linear/README.md#_snippet_2

LANGUAGE: python
CODE:
```
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

----------------------------------------

TITLE: Evaluating Policy Performance Statistics Python
DESCRIPTION: Defines a `print_statistics` function that runs the `walk` simulation multiple times (100 in this case) using a specified `policy`. It calculates and prints the average path length for successful episodes and the total number of times the agent encountered a failure state.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/notebook.ipynb#_snippet_5

LANGUAGE: Python
CODE:
```
def print_statistics(policy):
    s,w,n = 0,0,0
    for _ in range(100):
        z = walk(m,policy)
        if z<0:
            w+=1
        else:
            s += z
            n += 1
    print(f"Average path length = {s/n}, eaten by wolf: {w} times")

print_statistics(random_policy)
```

----------------------------------------

TITLE: Extracting Tags into Feature Columns (Python)
DESCRIPTION: This snippet creates new boolean columns in a pandas DataFrame based on the presence of specific useful tags within the 'Tags' column. Each new column corresponds to a tag like 'Leisure trip', 'Couple', etc. It handles the special case of combining 'Group' and 'Travelers with friends' into a single 'Group' column. Requires a pandas DataFrame with a 'Tags' column.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_0

LANGUAGE: python
CODE:
```
# Process the Tags into new columns
# The file Hotel_Reviews_Tags.py, identifies the most important tags
# Leisure trip, Couple, Solo traveler, Business trip, Group combined with Travelers with friends, 
# Family with young children, Family with older children, With a pet
df["Leisure_trip"] = df.Tags.apply(lambda tag: 1 if "Leisure trip" in tag else 0)
df["Couple"] = df.Tags.apply(lambda tag: 1 if "Couple" in tag else 0)
df["Solo_traveler"] = df.Tags.apply(lambda tag: 1 if "Solo traveler" in tag else 0)
df["Business_trip"] = df.Tags.apply(lambda tag: 1 if "Business trip" in tag else 0)
df["Group"] = df.Tags.apply(lambda tag: 1 if "Group" in tag or "Travelers with friends" in tag else 0)
df["Family_with_young_children"] = df.Tags.apply(lambda tag: 1 if "Family with young children" in tag else 0)
df["Family_with_older_children"] = df.Tags.apply(lambda tag: 1 if "Family with older children" in tag else 0)
df["With_a_pet"] = df.Tags.apply(lambda tag: 1 if "With a pet" in tag else 0)

```

----------------------------------------

TITLE: Getting DataFrame Shape with Pandas
DESCRIPTION: Displays the dimensions (rows, columns) of the pandas DataFrame `df`. This is a crucial initial step in data analysis to understand the size and structure of the dataset. Requires a loaded pandas DataFrame named `df`. The output is a string showing the number of rows and columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_0

LANGUAGE: python
CODE:
```
print("The shape of the data (rows, cols) is " + str(df.shape))
```

----------------------------------------

TITLE: Making Predictions with the Trained Model - Python
DESCRIPTION: Uses the trained `LinearRegression` model to predict the target values (`y_pred`) for the feature values in the test set (`X_test`). These predictions represent the line fitted by the model.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/solution/notebook.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
y_pred = model.predict(X_test)
```

----------------------------------------

TITLE: Making Predictions and Getting Probabilities in R
DESCRIPTION: Uses the trained workflow ('wf_fit') to make predictions on the test dataset ('pumpkins_test'). It predicts both the class label and the class probabilities for each observation. The results are combined with the true color labels for comparison. Requires the parsnip and dplyr packages.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/R/lesson_4-R.ipynb#_snippet_12

LANGUAGE: R
CODE:
```
# Make predictions for color and corresponding probabilities
results <- pumpkins_test %>% select(color) %>%
  bind_cols(wf_fit %>%
              predict(new_data = pumpkins_test)) %>%
  bind_cols(wf_fit %>%
              predict(new_data = pumpkins_test, type = "prob"))

# Compare predictions
results %>%
  slice_head(n = 10)
```

----------------------------------------

TITLE: Calculating AUC Score - Python
DESCRIPTION: This snippet uses `roc_auc_score` from `sklearn.metrics` to calculate the Area Under the ROC Curve (AUC) based on the true test labels (`y_test`) and the predicted probabilities for the positive class (`y_scores[:,1]`). The calculated AUC score is then printed as a metric of model performance.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/notebook.ipynb#_snippet_16

LANGUAGE: python
CODE:
```
# Calculate AUC score
auc = roc_auc_score(y_test,y_scores[:,1])
print(auc)
```

----------------------------------------

TITLE: Dropping Column and Recalculating Hotel Metadata in Pandas - Python
DESCRIPTION: Drops the 'Additional_Number_of_Scoring' column from the DataFrame. Recalculates the 'Total_Number_of_Reviews' for each hotel by counting reviews in the current dataset and recalculates the 'Average_Score' for each hotel based on the mean 'Reviewer_Score' in the dataset, rounding to one decimal place. Requires a pandas DataFrame 'df' with 'Additional_Number_of_Scoring', 'Hotel_Name', and 'Reviewer_Score' columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
# Drop `Additional_Number_of_Scoring`
df.drop(["Additional_Number_of_Scoring"], axis = 1, inplace=True)
# Replace `Total_Number_of_Reviews` and `Average_Score` with our own calculated values
df.Total_Number_of_Reviews = df.groupby('Hotel_Name').transform('count')
df.Average_Score = round(df.groupby('Hotel_Name').Reviewer_Score.transform('mean'), 1)
```

----------------------------------------

TITLE: Training Polynomial Regression with Combined Features Python
DESCRIPTION: Implements a complete machine learning workflow for polynomial regression using a rich set of combined features. It splits the data into training and testing sets, builds and trains a scikit-learn pipeline that applies polynomial feature transformation (degree 2) and linear regression, makes predictions, and evaluates the model using Mean Squared Error (MSE) and R-squared (determination) score.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/2-Regression/3-Linear/README.md#_snippet_17

LANGUAGE: python
CODE:
```
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

----------------------------------------

TITLE: Combining Multiple Features for Regression Input Python
DESCRIPTION: Constructs a comprehensive feature matrix (X) by combining multiple features from the `new_pumpkins` DataFrame using pandas join operations. It one-hot encodes the categorical columns ('Variety', 'City', 'Package') and combines them with the numerical 'Month' column to create a rich feature set for model training.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/2-Regression/3-Linear/README.md#_snippet_16

LANGUAGE: python
CODE:
```
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

----------------------------------------

TITLE: Preparing Training Data with One-Hot Encoding Python
DESCRIPTION: This code prepares the input features (`X`) and the target variable (`y`) for a machine learning model. It uses the one-hot encoded 'Variety' column as the feature matrix `X`, generated by `pd.get_dummies`, and the 'Price' column as the target vector `y`. This setup is common for training regression models where the input features include categorical data that has been converted into a numerical format.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/README.md#_snippet_16

LANGUAGE: python
CODE:
```
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

----------------------------------------

TITLE: Training K-Means Model and Predicting Clusters (Python)
DESCRIPTION: This snippet trains a K-Means clustering model on the prepared feature data `X`. It initializes the model specifying 3 clusters (`n_clusters=3`), which is based on the assumption of having 3 underlying music genres. A `random_state` is set for reproducibility. After fitting the model to the data, it predicts the cluster assignment (0, 1, or 2) for each data point in `X` and stores these predictions.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/translations/README.ko.md#_snippet_2

LANGUAGE: Python
CODE:
```
from sklearn.cluster import KMeans

nclusters = 3 
seed = 0

km = KMeans(n_clusters=nclusters, random_state=seed)
km.fit(X)

# Predict the cluster for each data point

y_cluster_kmeans = km.predict(X)
y_cluster_kmeans
```

----------------------------------------

TITLE: Counting Cuisine Occurrences (Pandas Python)
DESCRIPTION: Calculates and displays the count of unique values in the 'cuisine' column of the `df` DataFrame. This helps understand the distribution of different cuisine types in the dataset and identify potential class imbalances.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/notebook.ipynb#_snippet_5

LANGUAGE: Python
CODE:
```
df.cuisine.value_counts()
```

----------------------------------------

TITLE: Getting DataFrame Information - Pandas Python
DESCRIPTION: This snippet calls the `info()` method on the pandas DataFrame. It prints a concise summary of the DataFrame, including the index dtype and columns, non-null values, dtypes, and memory usage. This helps in understanding the data structure and identifying potential data type issues or missing values.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/README.md#_snippet_2

LANGUAGE: python
CODE:
```
df.info()
```

----------------------------------------

TITLE: Making Predictions with Trained Linear Regression Model (Python)
DESCRIPTION: Uses the trained linear regression model (`model`) to make predictions on the test feature data (`X_test`) using the `predict()` method. The resulting predictions (`y_pred`) represent the model's output for the unseen test data, which can then be compared to the actual test targets (`y_test`). Requires a trained scikit-learn model instance.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/2-Regression/1-Tools/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
y_pred = model.predict(X_test)
```

----------------------------------------

TITLE: Dropping Missing Values and Inspecting Dataframe (Python)
DESCRIPTION: This snippet cleans the `pie_pumpkins` DataFrame by removing any rows that contain missing (`NaN`) values using the `dropna(inplace=True)` method, modifying the DataFrame in place. It then displays a summary of the cleaned DataFrame, including column information, non-null counts, and data types, using `pie_pumpkins.info()`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/de/2-Regression/3-Linear/README.md#_snippet_4

LANGUAGE: python
CODE:
```
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

----------------------------------------

TITLE: Evaluating Linear Regression Model with MSE in Python
DESCRIPTION: Uses the trained `lin_reg` model to predict prices on the test set `X_test`. It then calculates the Root Mean Squared Error (RMSE) between the predicted values (`pred`) and the actual test values (`y_test`) using `mean_squared_error` and prints the result, including a percentage error relative to the mean prediction. Requires NumPy for `np.sqrt`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/2-Regression/3-Linear/README.md#_snippet_9

LANGUAGE: python
CODE:
```
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

----------------------------------------

TITLE: Removing Stop Words from Reviews (Python)
DESCRIPTION: This code defines a function `remove_stopwords` that removes common English stop words from a given text using an NLTK stop word cache. It then applies this function to the 'Negative_Review' and 'Positive_Review' columns of the DataFrame to clean the text before sentiment analysis. Requires the NLTK stop words corpus.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_3

LANGUAGE: python
CODE:
```
from nltk.corpus import stopwords

# Load the hotel reviews from CSV
df = pd.read_csv("../../data/Hotel_Reviews_Filtered.csv")

# Remove stop words - can be slow for a lot of text!
# Ryan Han (ryanxjhan on Kaggle) has a great post measuring performance of different stop words removal approaches
# https://www.kaggle.com/ryanxjhan/fast-stop-words-removal # using the approach that Ryan recommends
start = time.time()
cache = set(stopwords.words("english"))
def remove_stopwords(review):
    text = " ".join([word for word in review.split() if word not in cache])
    return text

# Remove the stop words from both columns
df.Negative_Review = df.Negative_Review.apply(remove_stopwords)   
df.Positive_Review = df.Positive_Review.apply(remove_stopwords)

```

----------------------------------------

TITLE: Loading cuisine data into a Pandas DataFrame
DESCRIPTION: Reads the data from the 'cuisines.csv' file into a pandas DataFrame. This DataFrame will be used for subsequent data exploration, cleaning, and machine learning tasks. Requires the 'pandas' library and the CSV file at the specified relative path.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/README.md#_snippet_2

LANGUAGE: python
CODE:
```
df  = pd.read_csv('../data/cuisines.csv')
```

----------------------------------------

TITLE: Training Linear Regression Model with Tidymodels R
DESCRIPTION: Defines the linear regression model specification and then uses the fit() function to train the model (lm_mod) on the training data (diabetes_train) using the formula y ~ ., meaning 'y' is predicted by all other variables (in this case, 'bmi'). The resulting trained model object is printed, showing learned coefficients.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/solution/R/lesson_1-R.ipynb#_snippet_6

LANGUAGE: R
CODE:
```
# Build a linear model specification
lm_spec <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")


# Train a linear regression model
lm_mod <- lm_spec %>%
  fit(y ~ ., data = diabetes_train)

# Print the model
lm_mod
```

----------------------------------------

TITLE: Importing Libraries for RL Tutorial - Python
DESCRIPTION: Imports standard Python libraries required for numerical operations (numpy), plotting results (matplotlib.pyplot), generating random numbers (random), and mathematical functions (math). These libraries are prerequisites for the subsequent code that simulates the environment, agent movement, and Q-Learning algorithm.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import matplotlib.pyplot as plt
import numpy as np
import random
import math
```

----------------------------------------

TITLE: Inspecting and Saving Balanced Data to CSV - Python
DESCRIPTION: Displays the first few rows (`head()`) and summary information (`info()`) of the `transformed_df` to examine the final balanced dataset structure and data types. It then saves the entire dataframe to a CSV file named 'cleaned_cuisines.csv' in the specified data directory.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/README.md#_snippet_17

LANGUAGE: python
CODE:
```
transformed_df.head()
transformed_df.info()
transformed_df.to_csv("../data/cleaned_cuisines.csv")
```

----------------------------------------

TITLE: Defining VADER Sentiment Calculation Function (Python)
DESCRIPTION: This snippet defines the `calc_sentiment` function using NLTK's VADER (Valence Aware Dictionary and sEntiment Reasoner) sentiment analyzer. The function calculates the compound sentiment score for a text review. It returns 0 for placeholder inputs like 'No Negative' or 'No Positive' and the compound score otherwise. Requires NLTK's VADER lexicon.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_4

LANGUAGE: python
CODE:
```
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Create the vader sentiment analyser (there are others in NLTK you can try too)
vader_sentiment = SentimentIntensityAnalyzer()
# Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.

# There are 3 possibilities of input for a review:
# It could be "No Negative", in which case, return 0
# It could be "No Positive", in which case, return 0
# It could be a review, in which case calculate the sentiment
def calc_sentiment(review):    
    if review == "No Negative" or review == "No Positive":
        return 0
    return vader_sentiment.polarity_scores(review)["compound"]    

```

----------------------------------------

TITLE: Calculating VADER Sentiment Score Function (Python)
DESCRIPTION: This snippet defines a function `calc_sentiment` that uses the NLTK VADER SentimentIntensityAnalyzer to compute the compound sentiment score for a review string. It handles special cases like 'No Negative' or 'No Positive' by returning 0, otherwise returning the compound score from `polarity_scores`. Requires an instance of `SentimentIntensityAnalyzer` (typically `vader_sentiment`) to be available.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_4

LANGUAGE: python
CODE:
```
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Create the vader sentiment analyser (there are others in NLTK you can try too)
vader_sentiment = SentimentIntensityAnalyzer()
# Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.

# There are 3 possibilities of input for a review:
# It could be "No Negative", in which case, return 0
# It could be "No Positive", in which case, return 0
# It could be a review, in which case calculate the sentiment
def calc_sentiment(review):    
    if review == "No Negative" or review == "No Positive":
        return 0
    return vader_sentiment.polarity_scores(review)["compound"]
```

----------------------------------------

TITLE: Splitting Data for Training and Testing - Scikit-learn Python
DESCRIPTION: Splits the prepared feature data (`X`) and target data (`y`) into training and testing sets using the `train_test_split` function. `test_size=0.2` allocates 20% of the data to the test set, and `random_state=0` ensures reproducibility of the split across multiple runs. Requires the scikit-learn library.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/2-Regression/3-Linear/README.md#_snippet_7

LANGUAGE: Python
CODE:
```
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

----------------------------------------

TITLE: Plotting ROC Curve - Python
DESCRIPTION: This code imports necessary modules for plotting ROC curves from `sklearn.metrics` and `matplotlib`. It calculates the probabilities predicted by the model (`predict_proba`), computes the False Positive Rate (FPR) and True Positive Rate (TPR) using `roc_curve`, and plots the ROC curve along with the diagonal baseline.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/notebook.ipynb#_snippet_15

LANGUAGE: python
CODE:
```
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline

y_scores = model.predict_proba(X_test)
# calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])

# plot ROC curve
fig = plt.figure(figsize=(6, 6))
# Plot the diagonal 50% line
plt.plot([0, 1], [0, 1], 'k--')
# Plot the FPR and TPR achieved by our model
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()
```

----------------------------------------

TITLE: Scaling Training Data using MinMaxScaler in Python
DESCRIPTION: This snippet initializes a MinMaxScaler object and fits it to the 'load' column of the training data. It then transforms the training data using the fitted scaler, scaling the values to the range [0, 1], and displays the first few rows of the scaled data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/README.md#_snippet_6

LANGUAGE: python
CODE:
```
scaler = MinMaxScaler()
train['load'] = scaler.fit_transform(train)
train.head(10)
```

----------------------------------------

TITLE: Calculating Classification Evaluation Metrics in R
DESCRIPTION: Defines a set of common classification evaluation metrics (Precision, Recall, Specificity, F-Measure, Accuracy) using yardstick's metric_set() and calculates them all at once for the model's predictions stored in the 'results' dataframe. Requires the yardstick package.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/R/lesson_4-R.ipynb#_snippet_14

LANGUAGE: R
CODE:
```
# Combine metric functions and calculate them all at once
eval_metrics <- metric_set(ppv, recall, spec, f_meas, accuracy)
eval_metrics(data = results, truth = color, estimate = .pred_class)
```

----------------------------------------

TITLE: Creating Features with Pandas Python
DESCRIPTION: This snippet constructs a feature-only DataFrame by dropping the label column ('cuisine') and an unwanted index column ('Unnamed: 0') from the original DataFrame. The 'axis=1' argument ensures that columns are dropped instead of rows. The head of the resulting feature DataFrame is then displayed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/solution/notebook.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)
cuisines_feature_df.head()
```

----------------------------------------

TITLE: Plotting Price vs. DayOfYear by Variety (Python)
DESCRIPTION: This code iterates through unique pumpkin varieties in the `new_pumpkins` DataFrame and creates a scatter plot of 'Price' versus 'DayOfYear' for each variety. Different colors are assigned to each variety, and all plots are overlaid on the same matplotlib axes (`ax`) to visually compare relationships across varieties.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/de/2-Regression/3-Linear/README.md#_snippet_1

LANGUAGE: python
CODE:
```
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

----------------------------------------

TITLE: Remove Stop Words from Review Text using NLTK
DESCRIPTION: This snippet defines and applies a function to remove common English stop words from specified text columns in the DataFrame. It utilizes NLTK's stopwords corpus, creating a cache for efficient lookup. The `remove_stopwords` function processes each review string by splitting it into words and filtering out those present in the stop word cache before rejoining the remaining words.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_3

LANGUAGE: python
CODE:
```
from nltk.corpus import stopwords

# Load the hotel reviews from CSV
df = pd.read_csv("../../data/Hotel_Reviews_Filtered.csv")

# Remove stop words - can be slow for a lot of text!
# Ryan Han (ryanxjhan on Kaggle) has a great post measuring performance of different stop words removal approaches
# https://www.kaggle.com/ryanxjhan/fast-stop-words-removal # using the approach that Ryan recommends
start = time.time()
cache = set(stopwords.words("english"))
def remove_stopwords(review):
    text = " ".join([word for word in review.split() if word not in cache])
    return text

# Remove the stop words from both columns
df.Negative_Review = df.Negative_Review.apply(remove_stopwords)
df.Positive_Review = df.Positive_Review.apply(remove_stopwords)

```

----------------------------------------

TITLE: Removing Stop Words from Reviews (Python)
DESCRIPTION: Imports the `stopwords` corpus from NLTK. It loads the filtered hotel review data, creates a set of English stop words for efficient lookup, defines a function `remove_stopwords` to filter out these words from review text, and applies this function to the 'Negative_Review' and 'Positive_Review' columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_3

LANGUAGE: python
CODE:
```
from nltk.corpus import stopwords

# Load the hotel reviews from CSV
df = pd.read_csv("../../data/Hotel_Reviews_Filtered.csv")

# Remove stop words - can be slow for a lot of text!
# Ryan Han (ryanxjhan on Kaggle) has a great post measuring performance of different stop words removal approaches
# https://www.kaggle.com/ryanxjhan/fast-stop-words-removal # using the approach that Ryan recommends
start = time.time()
cache = set(stopwords.words("english"))
def remove_stopwords(review):
    text = " ".join([word for word in review.split() if word not in cache])
    return text

# Remove the stop words from both columns
df.Negative_Review = df.Negative_Review.apply(remove_stopwords)   
df.Positive_Review = df.Positive_Review.apply(remove_stopwords)
```

----------------------------------------

TITLE: Calculating Price Correlation with Month and Day
DESCRIPTION: This snippet calculates and prints the Pearson correlation coefficient between the 'Month' and 'Price' columns, and between the 'DayOfYear' and 'Price' columns of the 'new_pumpkins' DataFrame. It helps assess the linear relationship between these features and the pumpkin price.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/2-Regression/3-Linear/README.md#_snippet_0

LANGUAGE: python
CODE:
```
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

----------------------------------------

TITLE: Calculating Correlation with Pandas in Python
DESCRIPTION: This snippet calculates the Pearson correlation coefficient between the 'Month' or 'DayOfYear' columns and the 'Price' column of a Pandas DataFrame using the .corr() method. It helps quantify the linear relationship between these variables.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

----------------------------------------

TITLE: Counting Rows with Specific Text using Pandas Boolean Indexing and Sum - Python
DESCRIPTION: This snippet provides an alternative, more performant method to count rows in a Pandas DataFrame based on string matching using boolean indexing and the `sum` function. It directly leverages Pandas' vectorized operations for efficiency and measures the execution time. Requires a DataFrame `df` with 'Negative_Review' and 'Positive_Review' columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_8

LANGUAGE: python
CODE:
```
# without lambdas (using a mixture of notations to show you can use both)
start = time.time()
no_negative_reviews = sum(df.Negative_Review == "No Negative")
print("Number of No Negative reviews: " + str(no_negative_reviews))

no_positive_reviews = sum(df["Positive_Review"] == "No Positive")
print("Number of No Positive reviews: " + str(no_positive_reviews))

both_no_reviews = sum((df.Negative_Review == "No Negative") & (df.Positive_Review == "No Positive"))
print("Number of both No Negative and No Positive reviews: " + str(both_no_reviews))

end = time.time()
print("Sum took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Loading Data and Inspecting Head using Pandas
DESCRIPTION: Imports the pandas library, reads the US-pumpkins.csv file into a DataFrame named `pumpkins`, and displays the first 5 rows using the `head()` method to get an initial look at the data structure and content. Requires the pandas library installed and the CSV file at the specified path.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/2-Data/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
import pandas as pd
pumpkins = pd.read_csv('../data/US-pumpkins.csv')
pumpkins.head()
```

----------------------------------------

TITLE: Compute Confusion Matrix (Tidymodels/yardstick) - R
DESCRIPTION: Calculates the confusion matrix based on the `results` data frame, comparing the true labels (`cuisine`) against the predicted class labels (`.pred_class`). The confusion matrix is a fundamental tool for evaluating the performance of a classification model, especially in multiclass scenarios.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/solution/R/lesson_11-R.ipynb#_snippet_8

LANGUAGE: R
CODE:
```
# Confusion matrix for categorical data
conf_mat(data = results, truth = cuisine, estimate = .pred_class)
```

----------------------------------------

TITLE: Identifying Top Nationality Frequencies (Pandas/Python)
DESCRIPTION: Uses the `nationality_freq` Series (presumably created by `value_counts()`) to print the single most frequent nationality and then slices the Series to print the frequencies of the next top 10 nationalities. Requires the `nationality_freq` Series derived from a DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_2

LANGUAGE: python
CODE:
```
print("The highest frequency reviewer nationality is " + str(nationality_freq.index[0]).strip() + " with " + str(nationality_freq[0]) + " reviews.")
# Notice there is a leading space on the values, strip() removes that for printing
# What is the top 10 most common nationalities and their frequencies?
print("The next 10 highest frequency reviewer nationalities are:")
print(nationality_freq[1:11].to_string())
```

----------------------------------------

TITLE: Calculating Nationality Frequency Counts (Pandas/Python)
DESCRIPTION: Calculates the frequency of each unique value in the 'Reviewer_Nationality' column using `.value_counts()`. It then prints the total number of distinct nationalities and the frequency series itself. Requires a pandas DataFrame `df` with a 'Reviewer_Nationality' column.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_1

LANGUAGE: python
CODE:
```
# value_counts() creates a Series object that has index and values in this case, the country and the frequency they occur in reviewer nationality
nationality_freq = df["Reviewer_Nationality"].value_counts()
print("There are " + str(nationality_freq.size) + " different nationalities")
# print first and last rows of the Series. Change to nationality_freq.to_string() to print all of the data
print(nationality_freq)
```

----------------------------------------

TITLE: Handle Inference in JavaScript
DESCRIPTION: Contains JavaScript code to manage ingredient selection, validate that at least one ingredient is selected, and perform ONNX model inference. It initializes an array representing ingredient features, attaches event listeners to checkboxes to update this array, and defines an asynchronous function `startInference` that loads the ONNX model, creates an input tensor from the ingredient array, runs the model session, and displays the predicted cuisine.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/4-Classification/4-Applied/README.md#_snippet_14

LANGUAGE: javascript
CODE:
```
const ingredients = Array(380).fill(0);
        
        const checks = [...document.querySelectorAll('.checkbox')];
        
        checks.forEach(check => {
            check.addEventListener('change', function() {
                // toggle the state of the ingredient
                // based on the checkbox's value (1 or 0)
                ingredients[check.value] = check.checked ? 1 : 0;
            });
        });

        function testCheckboxes() {
            // validate if at least one checkbox is checked
            return checks.some(check => check.checked);
        }

        async function startInference() {

            let atLeastOneChecked = testCheckboxes()

            if (!atLeastOneChecked) {
                alert('Please select at least one ingredient.');
                return;
            }
            try {
                // create a new session and load the model.
                
                const session = await ort.InferenceSession.create('./model.onnx');

                const input = new ort.Tensor(new Float32Array(ingredients), [1, 380]);
                const feeds = { float_input: input };

                // feed inputs and run
                const results = await session.run(feeds);

                // read from results
                alert('You can enjoy ' + results.label.data[0] + ' cuisine today!')

            } catch (e) {
                console.log(`failed to inference ONNX model`);
                console.error(e);
            }
        }
```

----------------------------------------

TITLE: Implementing Strict Q-Learning Policy Python
DESCRIPTION: This Python function defines a strict policy based on a learned Q-Table, always selecting the action with the highest Q-value for the current state. It uses the `probs` function to get a distribution (though only the argmax is used for selection here) and is demonstrated by calling a `walk` function.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/README.md#_snippet_8

LANGUAGE: Python
CODE:
```
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

----------------------------------------

TITLE: Analyzing Text Sentiment with TextBlob in Python
DESCRIPTION: This snippet demonstrates how to perform basic sentiment analysis on two distinct text quotes using the `TextBlob` library. It imports `TextBlob`, initializes `TextBlob` objects with the text, accesses the sentiment property to get polarity and subjectivity scores, and prints the results for each quote.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/6-NLP/3-Translation-Sentiment/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
from textblob import TextBlob

quote1 = """It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife."""

quote2 = """Darcy, as well as Elizabeth, really loved them; and they were both ever sensible of the warmest gratitude towards the persons who, by bringing her into Derbyshire, had been the means of uniting them."""

sentiment1 = TextBlob(quote1).sentiment
sentiment2 = TextBlob(quote2).sentiment

print(quote1 + " has a sentiment of " + str(sentiment1))
print(quote2 + " has a sentiment of " + str(sentiment2))
```

----------------------------------------

TITLE: Visualizing Price vs. Month Scatter Plot (Python)
DESCRIPTION: Imports the matplotlib.pyplot library for plotting if not already imported. It then generates a scatter plot using the 'Month' column on the x-axis and the 'Price' column on the y-axis from the processed `new_pumpkins` DataFrame. This plot visualizes the relationship between pumpkin price and the month of the year.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/notebook.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
import matplotlib.pyplot as plt
plt.scatter('Month','Price',data=new_pumpkins)
```

----------------------------------------

TITLE: Selecting and Reshaping Feature Column with NumPy (Python)
DESCRIPTION: Selects the third column (index 2) of the feature matrix X using slicing (`[:, 2]`) to isolate a single feature (BMI). Reshapes the selected column into a 2D array using `reshape((-1, 1))`, which is a required format for scikit-learn models when working with a single feature. Requires NumPy functionality implicitly used by scikit-learn data structures.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/2-Regression/1-Tools/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
X = X[:, 2]
X = X.reshape((-1,1))
```

----------------------------------------

TITLE: Selecting and Reshaping Feature Column in Python
DESCRIPTION: Selects a specific column (index 2, corresponding to BMI) from the feature matrix X to perform simple linear regression on a single variable. It then reshapes this selected column into a 2D array using `reshape((-1, 1))`, which is the required input format for scikit-learn models and for plotting.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/2-Regression/1-Tools/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
X = X[:, 2]
X = X.reshape((-1,1))
```

----------------------------------------

TITLE: Visualizing Linear Regression Results (Python)
DESCRIPTION: This snippet creates a scatter plot of the actual test data (X_test, y_test) and overlays the linear regression model's predictions. It plots the predicted values (pred) against the test features (X_test) as a line to visualize the learned linear relationship. Dependencies: matplotlib.pyplot (imported as plt), testing data X_test, y_test, prediction results pred.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/2-Regression/3-Linear/README.md#_snippet_11

LANGUAGE: python
CODE:
```
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

----------------------------------------

TITLE: Calculating Reviewer Nationality Frequency with Pandas
DESCRIPTION: Calculates and displays the frequency distribution of reviewer nationalities in the `df` DataFrame using the `value_counts()` method. It determines the total number of unique nationalities and shows the count for each, sorted in descending order. Requires the `df` DataFrame with a 'Reviewer_Nationality' column. Outputs the total count of nationalities and the frequency series.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_1

LANGUAGE: python
CODE:
```
# value_counts() creates a Series object that has index and values in this case, the country and the frequency they occur in reviewer nationality
nationality_freq = df["Reviewer_Nationality"].value_counts()
print("There are " + str(nationality_freq.size) + " different nationalities")
# print first and last rows of the Series. Change to nationality_freq.to_string() to print all of the data
print(nationality_freq)
```

----------------------------------------

TITLE: Calculating Reviewer Nationality Frequency Python
DESCRIPTION: This code calculates the frequency count of each unique value in the `Reviewer_Nationality` column of the `df` DataFrame using the `value_counts()` method, which returns a Pandas Series. It then prints the total number of distinct nationalities and the resulting frequency distribution. Requires a Pandas DataFrame `df` with a `Reviewer_Nationality` column. Output includes the number of distinct nationalities and a Series showing nationality counts.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/sw/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_1

LANGUAGE: python
CODE:
```
# value_counts() creates a Series object that has index and values in this case, the country and the frequency they occur in reviewer nationality
nationality_freq = df["Reviewer_Nationality"].value_counts()
print("There are " + str(nationality_freq.size) + " different nationalities")
# print first and last rows of the Series. Change to nationality_freq.to_string() to print all of the data
print(nationality_freq)
```

----------------------------------------

TITLE: Preparing Data for K-Means Clustering (Python)
DESCRIPTION: This code prepares the data for K-Means clustering. It selects specific features ('artist_top_genre', 'popularity', 'danceability', etc.) into a new DataFrame `X`. The categorical feature 'artist_top_genre' is then numerically encoded using `LabelEncoder` from scikit-learn. A separate variable `y` is created holding the original genre labels, also encoded, for potential later comparison (though K-Means is unsupervised).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/translations/README.ko.md#_snippet_1

LANGUAGE: Python
CODE:
```
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

X = df.loc[:, ('artist_top_genre','popularity','danceability','acousticness','loudness','energy')]

y = df['artist_top_genre']

X['artist_top_genre'] = le.fit_transform(X['artist_top_genre'])

y = le.transform(y)
```

----------------------------------------

TITLE: Defining VADER Sentiment Calculation Function - NLTK Python
DESCRIPTION: This snippet defines a Python function `calc_sentiment` that utilizes the NLTK VADER sentiment analyzer to compute the compound sentiment score for a given text review. It initializes the `SentimentIntensityAnalyzer` and the function returns 0 for the specific input strings "No Negative" or "No Positive", otherwise it returns the compound score obtained from `vader_sentiment.polarity_scores`. Requires the NLTK library and the VADER lexicon downloaded.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_8

LANGUAGE: Python
CODE:
```
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Create the vader sentiment analyser (there are others in NLTK you can try too)
vader_sentiment = SentimentIntensityAnalyzer()
# Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.

# There are 3 possibilities of input for a review:
# It could be "No Negative", in which case, return 0
# It could be "No Positive", in which case, return 0
# It could be a review, in which case calculate the sentiment
def calc_sentiment(review):    
    if review == "No Negative" or review == "No Positive":
        return 0
    return vader_sentiment.polarity_scores(review)["compound"]    
```

----------------------------------------

TITLE: Removing Stop Words from Review Text
DESCRIPTION: This code defines a function `remove_stopwords` that filters out common English stop words from a given text string using an NLTK stop words set. It then applies this function to both the 'Negative_Review' and 'Positive_Review' columns of the DataFrame to clean the text. The approach aims to speed up subsequent sentiment analysis without losing significant meaning.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_3

LANGUAGE: python
CODE:
```
from nltk.corpus import stopwords

# Load the hotel reviews from CSV
df = pd.read_csv("../../data/Hotel_Reviews_Filtered.csv")

# Remove stop words - can be slow for a lot of text!
# Ryan Han (ryanxjhan on Kaggle) has a great post measuring performance of different stop words removal approaches
# https://www.kaggle.com/ryanxjhan/fast-stop-words-removal # using the approach that Ryan recommends
start = time.time()
cache = set(stopwords.words("english"))
def remove_stopwords(review):
    text = " ".join([word for word in review.split() if word not in cache])
    return text

# Remove the stop words from both columns
df.Negative_Review = df.Negative_Review.apply(remove_stopwords)   
df.Positive_Review = df.Positive_Review.apply(remove_stopwords)
```

----------------------------------------

TITLE: Initializing Ordinal Encoder for 'Item Size' - Python
DESCRIPTION: This snippet imports `OrdinalEncoder` from `sklearn.preprocessing` and initializes it. It defines the specific order of categories for the 'Item Size' feature, preparing the encoder to transform this ordinal feature.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/notebook.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
from sklearn.preprocessing import OrdinalEncoder
# Encode the 'Item Size' column using ordinal encoding
item_size_categories = [['sml', 'med', 'med-lge', 'lge', 'xlge', 'jbo', 'exjbo']]
ordinal_features = ['Item Size']
ordinal_encoder = OrdinalEncoder(categories=item_size_categories)
```

----------------------------------------

TITLE: Initializing Ordinal Encoder for Item Size (Python)
DESCRIPTION: Imports `OrdinalEncoder` from `sklearn.preprocessing`. Defines the ordered categories for 'Item Size'. Initializes an `OrdinalEncoder` instance, specifying the defined categories, to prepare for transforming the 'Item Size' column.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/4-Logistic/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
from sklearn.preprocessing import OrdinalEncoder

item_size_categories = [['sml', 'med', 'med-lge', 'lge', 'xlge', 'jbo', 'exjbo']]
ordinal_features = ['Item Size']
ordinal_encoder = OrdinalEncoder(categories=item_size_categories)
```

----------------------------------------

TITLE: Visualizing Regression Results (Matplotlib, Python)
DESCRIPTION: Generates a scatter plot of the actual test data points (`X_test` vs `y_test`) using Matplotlib. It then overlays a line plot representing the model's predictions (`X_test` vs `y_pred`), showing the linear relationship found by the model. The plot is labeled and titled for clarity, demonstrating how well the linear model fits the test data. Requires the Matplotlib library.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/2-Regression/1-Tools/README.md#_snippet_5

LANGUAGE: python
CODE:
```
plt.scatter(X_test, y_test,  color='black')
plt.plot(X_test, y_pred, color='blue', linewidth=3)
plt.xlabel('Scaled BMIs')
plt.ylabel('Disease Progression')
plt.title('A Graph Plot Showing Diabetes Progression Against BMI')
plt.show()
```

----------------------------------------

TITLE: Setting Up Sentiment Analysis - NLTK VADER - Python
DESCRIPTION: This snippet imports necessary libraries for sentiment analysis, including pandas and NLTK's VADER. It downloads the VADER lexicon required for the analysis. It then loads a previously filtered dataset and defines a function `calc_sentiment` that uses the VADER SentimentIntensityAnalyzer to calculate the compound sentiment score for a given review text, handling cases where no review exists.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_2

LANGUAGE: python
CODE:
```
import time
import pandas as pd
import nltk as nltk
from nltk.corpus import stopwords
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

# Load the filtered hotel reviews from CSV
df = pd.read_csv('../../data/Hotel_Reviews_Filtered.csv')

# You code will be added here


# Finally remember to save the hotel reviews with new NLP data added
print("Saving results to Hotel_Reviews_NLP.csv")
df.to_csv(r'../data/Hotel_Reviews_NLP.csv', index = False)

```

LANGUAGE: python
CODE:
```
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Create the vader sentiment analyser (there are others in NLTK you can try too)
vader_sentiment = SentimentIntensityAnalyzer()
# Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.

# There are 3 possibilities of input for a review:
# It could be "No Negative", in which case, return 0
# It could be "No Positive", in which case, return 0
# It could be a review, in which case calculate the sentiment
def calc_sentiment(review):    
    if review == "No Negative" or review == "No Positive":
        return 0
    return vader_sentiment.polarity_scores(review)["compound"]    

```

----------------------------------------

TITLE: Plotting WCSS for Elbow Method (Python)
DESCRIPTION: Generates a line plot showing the relationship between the number of clusters (k, from 1 to 10) on the x-axis and the calculated WCSS values on the y-axis. This visualization helps identify the 'elbow point', which is often considered the optimal number of clusters where the decrease in WCSS starts to level off. Requires `matplotlib.pyplot` and `seaborn`. It uses the `wcss` list generated in the previous step.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/de/5-Clustering/2-K-Means/README.md#_snippet_5

LANGUAGE: python
CODE:
```
plt.figure(figsize=(10,5))
sns.lineplot(x=range(1, 11), y=wcss, marker='o', color='red')
plt.title('Elbow')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()
```

----------------------------------------

TITLE: Calculate Hotel Review Count in Dataset with Pandas Python
DESCRIPTION: This snippet calculates the number of reviews present in the dataset for each hotel. It creates a new DataFrame by dropping unnecessary columns, then groups by 'Hotel_Name' and uses `.transform('count')` to count the rows per group, adding the result to a new column 'Total_Reviews_Found'. Finally, it removes duplicate hotel rows and displays the result.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_5

LANGUAGE: python
CODE:
```
# First create a new dataframe based on the old one, removing the uneeded columns
hotel_freq_df = df.drop(["Hotel_Address", "Additional_Number_of_Scoring", "Review_Date", "Average_Score", "Reviewer_Nationality", "Negative_Review", "Review_Total_Negative_Word_Counts", "Positive_Review", "Review_Total_Positive_Word_Counts", "Total_Number_of_Reviews_Reviewer_Has_Given", "Reviewer_Score", "Tags", "days_since_review", "lat", "lng"], axis = 1)

# Group the rows by Hotel_Name, count them and put the result in a new column Total_Reviews_Found
hotel_freq_df['Total_Reviews_Found'] = hotel_freq_df.groupby('Hotel_Name').transform('count')

# Get rid of all the duplicated rows
hotel_freq_df = hotel_freq_df.drop_duplicates(subset = ["Hotel_Name"])
display(hotel_freq_df)
```

----------------------------------------

TITLE: Removing Stop Words from Review Text in Pandas DataFrame Python
DESCRIPTION: Imports the English stop words list from NLTK. It defines a function 'remove_stopwords' that takes a review string, splits it into words, and returns a new string containing only words not present in the English stop word cache. This function is then applied to the 'Negative_Review' and 'Positive_Review' columns of the DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
from nltk.corpus import stopwords

# Load the hotel reviews from CSV
df = pd.read_csv("../../data/Hotel_Reviews_Filtered.csv")

# Remove stop words - can be slow for a lot of text!
# Ryan Han (ryanxjhan on Kaggle) has a great post measuring performance of different stop words removal approaches
# https://www.kaggle.com/ryanxjhan/fast-stop-words-removal # using the approach that Ryan recommends
start = time.time()
cache = set(stopwords.words("english"))
def remove_stopwords(review):
    text = " ".join([word for word in review.split() if word not in cache])
    return text

# Remove the stop words from both columns
df.Negative_Review = df.Negative_Review.apply(remove_stopwords)   
df.Positive_Review = df.Positive_Review.apply(remove_stopwords)
```

----------------------------------------

TITLE: Visualizing Linear Regression Results - Matplotlib Python
DESCRIPTION: Creates a scatter plot of the actual test data points (`X_test` vs `y_test`) to show the distribution of the data. It then overlays a line plot showing the model's predictions (`X_test` vs `pred`) to visually assess how well the linear regression line fits the test data. Requires the matplotlib plotting library.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/2-Regression/3-Linear/README.md#_snippet_11

LANGUAGE: Python
CODE:
```
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

----------------------------------------

TITLE: Visualize Linear Model Predictions (R)
DESCRIPTION: Encodes the 'package' column of the test set using the trained recipe, binds the resulting integer representation (`package_integer`) to the `lm_results` data frame. It then creates a scatter plot of `package_integer` vs `price` using `ggplot` and overlays the model's predictions (`.pred`) as a line.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_16

LANGUAGE: R
CODE:
```
# Encode package column
package_encode <- lm_pumpkins_recipe %>%
  prep() %>%
  bake(new_data = pumpkins_test) %>%
  select(package)


# Bind encoded package column to the results
lm_results <- lm_results %>%
  bind_cols(package_encode %>%
              rename(package_integer = package)) %>%
  relocate(package_integer, .after = package)


# Print new results data frame
lm_results %>%
  slice_head(n = 5)


# Make a scatter plot
lm_results %>%
  ggplot(mapping = aes(x = package_integer, y = price)) +
  geom_point(size = 1.6) +
  # Overlay a line of best fit
  geom_line(aes(y = .pred), color = "orange", size = 1.2) +
  xlab("package")
```

----------------------------------------

TITLE: Evaluating Clustering with Silhouette Score (Python)
DESCRIPTION: This code calculates the silhouette score to evaluate the quality of the K-Means clustering performed in the previous step. The silhouette score measures how similar a data point is to its own cluster compared to other clusters, ranging from -1 to 1. It takes the feature data `X` and the predicted cluster labels `y_cluster_kmeans` as input. A score closer to 1 indicates better-defined and separated clusters.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/translations/README.ko.md#_snippet_3

LANGUAGE: Python
CODE:
```
from sklearn import metrics
score = metrics.silhouette_score(X, y_cluster_kmeans)
score
```

----------------------------------------

TITLE: Defining VADER Sentiment Calculation Function (Python)
DESCRIPTION: Imports `SentimentIntensityAnalyzer` from NLTK's VADER module and initializes it. Defines a helper function `calc_sentiment` that takes a review string as input, returns 0 if the input is a placeholder like 'No Negative' or 'No Positive', otherwise calculates and returns the compound sentiment score using the VADER analyzer.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_4

LANGUAGE: python
CODE:
```
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Create the vader sentiment analyser (there are others in NLTK you can try too)
vader_sentiment = SentimentIntensityAnalyzer()
# Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.

# There are 3 possibilities of input for a review:
# It could be "No Negative", in which case, return 0
# It could be "No Positive", in which case, return 0
# It could be a review, in which case calculate the sentiment
def calc_sentiment(review):    
    if review == "No Negative" or review == "No Positive":
        return 0
    return vader_sentiment.polarity_scores(review)["compound"]
```

----------------------------------------

TITLE: Loading and Inspecting Data Python
DESCRIPTION: Loads the time series data from a specified path './data' using a custom `load_data` function and selects only the 'load' column. It then displays the first 10 rows of the resulting DataFrame to preview the data structure and content.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/solution/notebook.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
energy = load_data('./data')[['load']]
energy.head(10)
```

----------------------------------------

TITLE: Preparing Features and Target with Single Categorical Feature Python
DESCRIPTION: Prepares the feature matrix (X) and target vector (y) for training a regression model. It uses the one-hot encoded 'Variety' column as the sole feature by applying `pd.get_dummies` and selects the 'Price' column as the target variable from the `new_pumpkins` DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/2-Regression/3-Linear/README.md#_snippet_15

LANGUAGE: python
CODE:
```
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

----------------------------------------

TITLE: Calculating Nationality Frequency (Pandas) - Python
DESCRIPTION: This code calculates the frequency count of each unique value in the 'Reviewer_Nationality' column of the DataFrame 'df' using the '.value_counts()' method. It then prints the total number of distinct nationalities found using the '.size' attribute of the resulting Series and displays the frequency list.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_2

LANGUAGE: python
CODE:
```
# value_counts() creates a Series object that has index and values in this case, the country and the frequency they occur in reviewer nationality
nationality_freq = df["Reviewer_Nationality"].value_counts()
print("There are " + str(nationality_freq.size) + " different nationalities")
# print first and last rows of the Series. Change to nationality_freq.to_string() to print all of the data
print(nationality_freq)
```

----------------------------------------

TITLE: Loading and Inspecting Data (Python)
DESCRIPTION: Specifies the data directory path and uses the `load_data` helper function to load the dataset into a Pandas DataFrame. It then selects only the 'load' column and displays the first five rows to preview the data structure and values. Requires the `common.utils.load_data` function and Pandas.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/1-Introduction/solution/notebook.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
data_dir = './data'
energy = load_data(data_dir)[['load']]
energy.head()
```

----------------------------------------

TITLE: Calculating Total Reviews per Hotel Python
DESCRIPTION: This snippet calculates the total number of reviews available for each hotel in the dataset. It first creates a new DataFrame by dropping many columns from the original `df`, keeping only the `Hotel_Name`. It then groups the rows by `Hotel_Name`, counts the occurrences within each group using `transform('count')`, and stores the result in a new column called `Total_Reviews_Found`. Finally, it removes duplicate rows based on `Hotel_Name` to ensure only one entry per hotel and displays the resulting DataFrame using `display()`. Requires a Pandas DataFrame `df` with at least a `Hotel_Name` column. Output is a Pandas DataFrame listing unique hotel names and their corresponding total review counts found in the dataset.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/sw/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_4

LANGUAGE: python
CODE:
```
# First create a new dataframe based on the old one, removing the uneeded columns
hotel_freq_df = df.drop(["Hotel_Address", "Additional_Number_of_Scoring", "Review_Date", "Average_Score", "Reviewer_Nationality", "Negative_Review", "Review_Total_Negative_Word_Counts", "Positive_Review", "Review_Total_Positive_Word_Counts", "Total_Number_of_Reviews_Reviewer_Has_Given", "Reviewer_Score", "Tags", "days_since_review", "lat", "lng"], axis = 1)

# Group the rows by Hotel_Name, count them and put the result in a new column Total_Reviews_Found
hotel_freq_df['Total_Reviews_Found'] = hotel_freq_df.groupby('Hotel_Name').transform('count')

# Get rid of all the duplicated rows
hotel_freq_df = hotel_freq_df.drop_duplicates(subset = ["Hotel_Name"])
display(hotel_freq_df)
```

----------------------------------------

TITLE: Recalculating Review Aggregates - Python
DESCRIPTION: This drops the 'Additional_Number_of_Scoring' column. It then recalculates 'Total_Number_of_Reviews' and 'Average_Score' by grouping the data by 'Hotel_Name' and applying aggregate functions (count and mean).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/1-notebook.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
# Drop `Additional_Number_of_Scoring`
df.drop(["Additional_Number_of_Scoring"], axis = 1, inplace=True)
# Replace `Total_Number_of_Reviews` and `Average_Score` with our own calculated values
df.Total_Number_of_Reviews = df.groupby('Hotel_Name').transform('count')
df.Average_Score = round(df.groupby('Hotel_Name').Reviewer_Score.transform('mean'), 1)
```

----------------------------------------

TITLE: Calculating and Comparing Average Hotel Review Scores in Pandas
DESCRIPTION: This snippet calculates the average reviewer score for each hotel from the raw reviewer scores in the dataset, adds it as a new column ('Calc_Average_Score'), and then calculates the difference between this new calculated average and the dataset's existing 'Average_Score'. It filters the DataFrame to show only one row per hotel, sorts by the difference, and displays the results.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/sw/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_5

LANGUAGE: python
CODE:
```
# define a function that takes a row and performs some calculation with it
def get_difference_review_avg(row):
  return row["Average_Score"] - row["Calc_Average_Score"]

# 'mean' is mathematical word for 'average'
df['Calc_Average_Score'] = round(df.groupby('Hotel_Name').Reviewer_Score.transform('mean'), 1)

# Add a new column with the difference between the two average scores
df["Average_Score_Difference"] = df.apply(get_difference_review_avg, axis = 1)

# Create a df without all the duplicates of Hotel_Name (so only 1 row per hotel)
review_scores_df = df.drop_duplicates(subset = ["Hotel_Name"])

# Sort the dataframe to find the lowest and highest average score difference
review_scores_df = review_scores_df.sort_values(by=["Average_Score_Difference"])

display(review_scores_df[["Average_Score_Difference", "Average_Score", "Calc_Average_Score", "Hotel_Name"]])
```

----------------------------------------

TITLE: Saving Filtered Data (Pandas/Python)
DESCRIPTION: This snippet first drops several specified columns from the DataFrame `df` using `df.drop()`. It then saves the modified DataFrame to a CSV file named 'Hotel_Reviews_Filtered.csv' using `df.to_csv()`, excluding the DataFrame index. Requires a DataFrame named `df` with the listed columns present.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_1

LANGUAGE: python
CODE:
```
df.drop(["Review_Total_Negative_Word_Counts", "Review_Total_Positive_Word_Counts", "days_since_review", "Total_Number_of_Reviews_Reviewer_Has_Given"], axis = 1, inplace=True)

# Saving new data file with calculated columns
print("Saving results to Hotel_Reviews_Filtered.csv")
df.to_csv(r'../data/Hotel_Reviews_Filtered.csv', index = False)
```

----------------------------------------

TITLE: Calculating Coefficient of Determination (R-squared) with Scikit-learn in Python
DESCRIPTION: This snippet calculates the coefficient of determination (R-squared score) of the trained linear regression model using the .score() method on the training data. The score indicates how well the model explains the variance in the target variable.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/README.md#_snippet_11

LANGUAGE: Python
CODE:
```
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```

----------------------------------------

TITLE: Calculating Confusion Matrix in R
DESCRIPTION: Calculates a confusion matrix to evaluate the performance of the classification model using the yardstick package. It compares the true 'color' labels with the predicted classes ('.pred_class') from the 'results' dataframe. Requires the yardstick package.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/R/lesson_4-R.ipynb#_snippet_13

LANGUAGE: R
CODE:
```
# Confusion matrix for prediction results
conf_mat(data = results, truth = color, estimate = .pred_class)
```

----------------------------------------

TITLE: Visualizing ROC Curve in Python
DESCRIPTION: This snippet visualizes the Receiver Operating Characteristic (ROC) curve for the model's results. It uses the `results` object (likely containing predictions and true labels), pipes it to `roc_curve` specifying the true color and prediction columns, and then pipes the output to `autoplot` for plotting. This visualization helps assess the trade-off between true positive rate and false positive rate.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/R/lesson_4-R.ipynb#_snippet_15

LANGUAGE: python
CODE:
```
# Make a roc_curve
results %>%
  roc_curve(color, .pred_ORANGE) %>%
  autoplot()
```

----------------------------------------

TITLE: Calculating Confusion Matrix - Python
DESCRIPTION: This snippet imports `confusion_matrix` from `sklearn.metrics` and calculates the confusion matrix comparing the true labels (`y_test`) and the model's predictions (`predictions`) on the test set. The matrix is printed to assess classification performance.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/notebook.ipynb#_snippet_14

LANGUAGE: python
CODE:
```
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, predictions)
```

----------------------------------------

TITLE: Generating a Confusion Matrix (Python)
DESCRIPTION: Computes a confusion matrix, which is a table summarizing the performance of a classification algorithm. It compares the actual true labels to the predicted labels, showing the counts of true positives, true negatives, false positives, and false negatives. This helps in understanding the types of errors the model makes.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/README.md#_snippet_12

LANGUAGE: python
CODE:
```
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, predictions)
```

----------------------------------------

TITLE: Evaluating Linear Regression Model (R-squared) - Scikit-learn Python
DESCRIPTION: Calculates the coefficient of determination (R-squared score) for the trained `lin_reg` model using the training data (`X_train`, `y_train`). This score, ranging from 0 to 1, indicates the proportion of the variance in the target variable that is predictable from the independent variable. Requires the scikit-learn library.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/2-Regression/3-Linear/README.md#_snippet_10

LANGUAGE: Python
CODE:
```
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```

----------------------------------------

TITLE: Handling Missing Values and Inspecting Data for 'PIE TYPE'
DESCRIPTION: This snippet removes rows with any missing values from the 'pie_pumpkins' DataFrame in-place using `dropna` and then prints concise information about the DataFrame, including the index dtype, column dtypes, and non-null values, to confirm missing values have been handled.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/2-Regression/3-Linear/README.md#_snippet_4

LANGUAGE: python
CODE:
```
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

----------------------------------------

TITLE: Handling Missing Data with Pandas Dropna and Info Python
DESCRIPTION: This snippet cleans the `pie_pumpkins` DataFrame by removing any rows that contain missing (NaN) values using the `dropna()` method with `inplace=True`. It then prints a summary of the cleaned DataFrame using the `info()` method, showing the data types and the number of non-null entries for each column. This ensures the data is suitable for models that cannot handle missing values.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/3-Linear/README.md#_snippet_4

LANGUAGE: python
CODE:
```
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

----------------------------------------

TITLE: Handling Missing Values and Inspecting DataFrame Info with Pandas
DESCRIPTION: Removes rows with any missing values (`NaN`) from the `pie_pumpkins` DataFrame in place, then prints a concise summary of the DataFrame, including the index dtype and column dtypes, non-null values, and memory usage. Requires a pandas DataFrame. Outputs the DataFrame info summary.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/fr/2-Regression/3-Linear/README.md#_snippet_4

LANGUAGE: python
CODE:
```
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

----------------------------------------

TITLE: Checking for Null Values in DataFrame Python
DESCRIPTION: Calculates the number of null values in each column of the DataFrame. The `.isnull()` method returns a boolean DataFrame, and `.sum()` counts the true values (nulls) per column.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/notebook.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
df.isnull().sum()
```

----------------------------------------

TITLE: Predict with Trained Model Python
DESCRIPTION: Uses the trained `SVC` model to make predictions on the test feature set `X_test`. The results are stored in the `y_pred` variable, which contains the predicted cuisine labels for the test data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/4-Classification/4-Applied/README.md#_snippet_7

LANGUAGE: python
CODE:
```
y_pred = model.predict(X_test)
```

----------------------------------------

TITLE: Visualizing Linear Regression Results
DESCRIPTION: This code generates a scatter plot of the test data (`X_test`, `y_test`) and overlays the regression line by plotting the predicted prices (`pred`) against the test features (`X_test`). This helps visually assess how well the linear model fits the test data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/2-Regression/3-Linear/README.md#_snippet_11

LANGUAGE: python
CODE:
```
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

----------------------------------------

TITLE: Calculating and Comparing Average Scores Python
DESCRIPTION: This code calculates the average reviewer score per hotel based on reviewer scores and adds it as 'Calc_Average_Score'. It then calculates the difference between this and the provided 'Average_Score', sorts the DataFrame by this difference, and prints the scores and difference for each unique hotel.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/4-Hotel-Reviews-1/solution/notebook.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
# While there is an `Average_Score` for each hotel according to the dataset, 
# you can also calculate an average score (getting the average of all reviewer scores in the dataset for each hotel)
# Add a new column to your dataframe with the column header `Calc_Average_Score` that contains that calculated average. 
df['Calc_Average_Score'] = round(df.groupby('Hotel_Name').Reviewer_Score.transform('mean'), 1)
# Add a new column with the difference between the two average scores
df["Average_Score_Difference"] = df.apply(get_difference_review_avg, axis = 1)
# Create a df without all the duplicates of Hotel_Name (so only 1 row per hotel)
review_scores_df = df.drop_duplicates(subset = ["Hotel_Name"])
# Sort the dataframe to find the lowest and highest average score difference
review_scores_df = review_scores_df.sort_values(by=["Average_Score_Difference"])
print(review_scores_df[["Average_Score_Difference", "Average_Score", "Calc_Average_Score", "Hotel_Name"]])
# Do any hotels have the same (rounded to 1 decimal place) `Average_Score` and `Calc_Average_Score`?
```

----------------------------------------

TITLE: Loading and Inspecting Diabetes Dataset (Scikit-learn, Python)
DESCRIPTION: Loads the diabetes dataset using `datasets.load_diabetes` with `return_X_y=True` to get feature matrix X and target vector y separately. It then prints the shape of the feature matrix X and the first sample's features to understand the data dimensions and format. Requires scikit-learn library.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/2-Regression/1-Tools/README.md#_snippet_0

LANGUAGE: python
CODE:
```
X, y = datasets.load_diabetes(return_X_y=True)
print(X.shape)
print(X[0])
```

----------------------------------------

TITLE: Loading & Inspecting Data - Energy Forecasting - Python
DESCRIPTION: Sets the directory for data files. Calls the `load_data` function to load the dataset, then selects only the 'load' column. Finally, it uses the `.head()` method to print the initial rows of the DataFrame, allowing for a quick inspection of the data structure and values.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/7-TimeSeries/1-Introduction/README.md#_snippet_1

LANGUAGE: python
CODE:
```
data_dir = './data'
energy = load_data(data_dir)[['load']]
energy.head()
```

----------------------------------------

TITLE: Calculating Average Price and Extracting Month with Pandas in Python
DESCRIPTION: Calculates the average price for each row by averaging the 'Low Price' and 'High Price' columns. It also extracts the month number (1-12) from the 'Date' column using `pd.DatetimeIndex`, preparing data for time-based analysis.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/2-Regression/2-Data/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
price = (pumpkins['Low Price'] + pumpkins['High Price']) / 2

month = pd.DatetimeIndex(pumpkins['Date']).month
```

----------------------------------------

TITLE: Checking for Null Values in Pandas DataFrame
DESCRIPTION: Calculates and displays the count of null (missing) values for each column in the current `pumpkins` DataFrame. This helps identify columns that may require data cleaning or imputation before further analysis. Operates on a pandas DataFrame. Input: pandas DataFrame. Output: Series showing null counts per column.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/solution/notebook.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
pumpkins.isnull().sum()
```

----------------------------------------

TITLE: Predicting with Polynomial Regression (Scikit-learn) - Python
DESCRIPTION: This snippet uses the trained polynomial regression pipeline to make a price prediction for a new package value (2.75). It demonstrates how the pipeline handles the feature transformation and linear regression steps internally for prediction.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/translations/README.ko.md#_snippet_15

LANGUAGE: Python
CODE:
```
pipeline.predict( np.array([ [2.75] ]) )
```

----------------------------------------

TITLE: Visualizing Linear Regression Results with Matplotlib in Python
DESCRIPTION: This code generates a scatter plot of the actual test data points (X_test, y_test) and overlays the predicted linear regression line by plotting the test features against the model's predictions (X_test, pred), allowing for a visual assessment of the fit.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/README.md#_snippet_12

LANGUAGE: Python
CODE:
```
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

----------------------------------------

TITLE: Loading Data with Pandas Python
DESCRIPTION: This snippet imports the pandas library, reads data from a specified CSV file into a DataFrame, and then displays the first few rows of the loaded DataFrame to inspect its structure and content. It requires the pandas library and the CSV file located at '../../data/cleaned_cuisines.csv'.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/solution/notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import pandas as pd
cuisines_df = pd.read_csv("../../data/cleaned_cuisines.csv")
cuisines_df.head()
```

----------------------------------------

TITLE: Plotting Full Dataset Predictions Matplotlib Python
DESCRIPTION: This code snippet generates a plot comparing the inverse-scaled actual (`Y`) and predicted (`Y_pred`) values for the entire dataset using `matplotlib`. This plot provides an overall visualization of how well the model performs across the complete time series.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_26

LANGUAGE: python
CODE:
```
plt.figure(figsize=(30,8))
plt.plot(Y, color = 'red', linewidth=2.0, alpha = 0.6)
plt.plot(Y_pred, color = 'blue', linewidth=0.8)
plt.legend(['Actual','Predicted'])
plt.xlabel('Timestamp')
plt.show()
```

----------------------------------------

TITLE: Calculating and Printing Full Dataset MAPE (Python)
DESCRIPTION: Calculates the Mean Absolute Percentage Error (MAPE) for the predictions made on the entire dataset compared to the actual values. The result provides an overall error metric for the full time series.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_25

LANGUAGE: python
CODE:
```
print('MAPE: ', mape(Y_pred, Y)*100, '%')
```

----------------------------------------

TITLE: Loading Cuisine Data using Pandas (Python)
DESCRIPTION: Imports the pandas library, reads the 'cleaned_cuisines.csv' file located in the parent directory's 'data' folder into a pandas DataFrame named cuisines_df, and then displays the first five rows of the DataFrame to verify successful loading and data structure. Requires pandas installed and the specified CSV file available at the relative path. The output is a preview of the DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/README.md#_snippet_0

LANGUAGE: python
CODE:
```
import pandas as pd
cuisines_df = pd.read_csv("../data/cleaned_cuisines.csv")
cuisines_df.head()
```

----------------------------------------

TITLE: Load Data with Pandas Python
DESCRIPTION: This snippet loads the cleaned cuisine dataset from a CSV file into a pandas DataFrame. The `pd.read_csv()` function is used for reading the data, and `head()` is called to display the first few rows.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/solution/notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import pandas as pd
cuisines_df = pd.read_csv("../../data/cleaned_cuisines.csv")
cuisines_df.head()
```

----------------------------------------

TITLE: Loading Hotel Reviews Data Python
DESCRIPTION: Reads the hotel review data from a CSV file located at '../../data/Hotel_Reviews_Filtered.csv' into a pandas DataFrame. This DataFrame serves as the primary input for subsequent data cleaning and sentiment analysis steps.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/3-notebook.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
# Load the hotel reviews from CSV
df = pd.read_csv("../../data/Hotel_Reviews_Filtered.csv")

```

----------------------------------------

TITLE: Calculating Model Determination (R-squared) Score
DESCRIPTION: This snippet calculates the coefficient of determination (R-squared) score for the trained linear regression model using the training data (`X_train`, `y_train`). The score indicates the proportion of the variance in the dependent variable that is predictable from the independent variable(s).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/2-Regression/3-Linear/README.md#_snippet_10

LANGUAGE: python
CODE:
```
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```

----------------------------------------

TITLE: Plotting Full Time Series Data (Python)
DESCRIPTION: Visualizes the entire loaded time series dataset using matplotlib. The plot displays the 'load' values over time, providing an overview of the data's trends, seasonality, and overall patterns from the beginning of the dataset to the end.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_3

LANGUAGE: python
CODE:
```
energy.plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)
plt.xlabel('timestamp', fontsize=12)
plt.ylabel('load', fontsize=12)
plt.show()
```

----------------------------------------

TITLE: Plotting Time Series Data Python
DESCRIPTION: Generates a plot of the entire 'load' time series data using pandas' built-in plotting capabilities, which utilize matplotlib. This visualization helps identify overall trends, seasonality, and patterns in the electricity load data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/solution/notebook.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
energy.plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)
plt.xlabel('timestamp', fontsize=12)
plt.ylabel('load', fontsize=12)
plt.show()
```

----------------------------------------

TITLE: Calculate Average Reviewer Score Per Hotel with Pandas Python
DESCRIPTION: This snippet calculates the average 'Reviewer_Score' for each hotel using `groupby('Hotel_Name').transform('mean')`, rounds it, and adds it as a new column 'Calc_Average_Score'. It also calculates the difference between this calculated average and the existing 'Average_Score' column, removes duplicate hotel rows, sorts by the difference, and displays the results.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_6

LANGUAGE: python
CODE:
```
# define a function that takes a row and performs some calculation with it
def get_difference_review_avg(row):
  return row["Average_Score"] - row["Calc_Average_Score"]

# 'mean' is mathematical word for 'average'
df['Calc_Average_Score'] = round(df.groupby('Hotel_Name').Reviewer_Score.transform('mean'), 1)

# Add a new column with the difference between the two average scores
df["Average_Score_Difference"] = df.apply(get_difference_review_avg, axis = 1)

# Create a df without all the duplicates of Hotel_Name (so only 1 row per hotel)
review_scores_df = df.drop_duplicates(subset = ["Hotel_Name"])

# Sort the dataframe to find the lowest and highest average score difference
review_scores_df = review_scores_df.sort_values(by=["Average_Score_Difference"])

display(review_scores_df[["Average_Score_Difference", "Average_Score", "Calc_Average_Score", "Hotel_Name"]])
```

----------------------------------------

TITLE: Counting Review Status with Boolean Indexing (Python)
DESCRIPTION: This snippet shows a more efficient method to count rows in a pandas DataFrame based on conditions in 'Negative_Review' and 'Positive_Review' columns. It uses pandas boolean indexing to create boolean Series and the `sum()` function to count True values, which is significantly faster than iterating with `apply`. Dependencies include pandas for DataFrame operations and time for timing.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_8

LANGUAGE: Python
CODE:
```
# without lambdas (using a mixture of notations to show you can use both)
start = time.time()
no_negative_reviews = sum(df.Negative_Review == "No Negative")
print("Number of No Negative reviews: " + str(no_negative_reviews))

no_positive_reviews = sum(df["Positive_Review"] == "No Positive")
print("Number of No Positive reviews: " + str(no_positive_reviews))

both_no_reviews = sum((df.Negative_Review == "No Negative") & (df.Positive_Review == "No Positive"))
print("Number of both No Negative and No Positive reviews: " + str(both_no_reviews))

end = time.time()
print("Sum took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Prepare Data for Scikit-learn Regression Python Numpy
DESCRIPTION: This snippet prepares the feature and target variables for Scikit-learn models. The 'DayOfYear' column is extracted, converted to a NumPy array, and reshaped to the required 2D format (-1, 1) for single-feature input, while the 'Price' column is used as the target variable.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/2-Regression/3-Linear/README.md#_snippet_6

LANGUAGE: python
CODE:
```
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

----------------------------------------

TITLE: Plotting Average Price by Variety with Pandas - Python
DESCRIPTION: This snippet groups the `new_pumpkins` DataFrame by the 'Variety' column and calculates the mean 'Price' for each group. The resulting mean prices are then plotted as a bar chart, providing a visual comparison of average prices across different pumpkin varieties.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/2-Regression/3-Linear/README.md#_snippet_2

LANGUAGE: python
CODE:
```
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

----------------------------------------

TITLE: Displaying Data Structure with glimpse (R)
DESCRIPTION: Applies the `glimpse()` function to the `pumpkins_select` dataframe. This function provides a concise overview of the data's structure, including column names, data types, and a preview of the values in each column, aiding in understanding the dataset characteristics.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/R/lesson_4-R.ipynb#_snippet_2

LANGUAGE: R
CODE:
```
pumpkins_select %>%
  glimpse()
```

----------------------------------------

TITLE: Visualizing Train-Test Split (Python)
DESCRIPTION: Generates a plot to visually represent the division of the dataset into training and testing periods. It overlays the data from the specified training and testing date ranges, allowing verification that the split is correct and sequential.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_5

LANGUAGE: python
CODE:
```
energy[(energy.index < test_start_dt) & (energy.index >= train_start_dt)][['load']].rename(columns={'load':'train'}) \
    .join(energy[test_start_dt:][['load']].rename(columns={'load':'test'}), how='outer') \
    .plot(y=['train', 'test'], figsize=(15, 8), fontsize=12)
plt.xlabel('timestamp', fontsize=12)
plt.ylabel('load', fontsize=12)
plt.show()
```

----------------------------------------

TITLE: Visualize Polynomial Model Fit with Smoother (R)
DESCRIPTION: Creates a scatter plot similar to the previous one but uses `geom_smooth` with `method = lm` and a polynomial `formula = y ~ poly(x, degree = 4)` to represent the polynomial fit. This provides a smoother curve visualization of the model's relationship between the encoded package and price, without showing individual predicted points.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_21

LANGUAGE: R
CODE:
```
# Make a scatter plot
poly_results %>%
  ggplot(mapping = aes(x = package_integer, y = price)) +
  geom_point(size = 1.6) +
  # Overlay a line of best fit
  geom_smooth(method = lm, formula = y ~ poly(x, degree = 4), color = "midnightblue", size = 1.2, se = FALSE) +
  xlab("package")
```

----------------------------------------

TITLE: Standardizing Price by Bushel Size using loc
DESCRIPTION: Uses the `.loc` accessor to conditionally update the 'Price' column for rows where 'Package' contains '1 1/9' or '1/2'. It divides the existing price by the respective bushel fraction (10/9 or 1/2) to normalize the price to a 'per bushel' unit. Requires the `new_pumpkins` DataFrame with 'Package' and 'Price' columns and the original `price` Series.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/2-Data/README.md#_snippet_6

LANGUAGE: Python
CODE:
```
new_pumpkins.loc[new_pumpkins['Package'].str.contains('1 1/9'), 'Price'] = price/(1 + 1/9)

new_pumpkins.loc[new_pumpkins['Package'].str.contains('1/2'), 'Price'] = price/(1/2)
```

----------------------------------------

TITLE: Preparing Data for Scikit-learn Regression - Python
DESCRIPTION: This snippet extracts the 'DayOfYear' column from the `pie_pumpkins` DataFrame, converts it to a NumPy array, and reshapes it into a 2D array (`-1, 1`) which is the format expected by scikit-learn's `LinearRegression` model for a single feature. It also extracts the 'Price' column as the target variable `y`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/2-Regression/3-Linear/README.md#_snippet_6

LANGUAGE: python
CODE:
```
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

----------------------------------------

TITLE: Perform Walk-Forward Time Series Forecasting in Python
DESCRIPTION: This code performs walk-forward validation. It iteratively trains a SARIMAX model on a sliding window of historical data, makes a multi-step forecast (`HORIZON` steps), and then updates the training window by adding the actual observation for the next time step.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/7-TimeSeries/2-ARIMA/README.md#_snippet_3

LANGUAGE: python
CODE:
```
%%time
training_window = 720 # dedicate 30 days (720 hours) for training

train_ts = train['load']
test_ts = test_shifted

history = [x for x in train_ts]
history = history[(-training_window):]

predictions = list()

order = (2, 1, 0)
seasonal_order = (1, 1, 0, 24)

for t in range(test_ts.shape[0]):
 model = SARIMAX(endog=history, order=order, seasonal_order=seasonal_order)
 model_fit = model.fit()
 yhat = model_fit.forecast(steps = HORIZON)
 predictions.append(yhat)
 obs = list(test_ts.iloc[t])
 # move the training window
 history.append(obs[0])
 history.pop(0)
 print(test_ts.index[t])
 print(t+1, ': predicted =', yhat, 'expected =', obs)
```

----------------------------------------

TITLE: Plotting Time Series Data with Matplotlib in Python
DESCRIPTION: This snippet visualizes the 'load' column of the energy DataFrame using Matplotlib. It creates a plot of the entire time series, setting appropriate labels for the x and y axes and displaying the plot.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/README.md#_snippet_2

LANGUAGE: python
CODE:
```
energy.plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)
plt.xlabel('timestamp', fontsize=12)
plt.ylabel('load', fontsize=12)
plt.show()
```

----------------------------------------

TITLE: Plotting Training Predictions Matplotlib Python
DESCRIPTION: This code snippet generates a plot comparing the actual (`y_train`) and predicted (`y_train_pred`) values for the training dataset using `matplotlib`. The x-axis represents the corresponding `train_timestamps`. This visualization helps assess the model's performance on the data it was trained on.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_20

LANGUAGE: python
CODE:
```
plt.figure(figsize=(25,6))
plt.plot(train_timestamps, y_train, color = 'red', linewidth=2.0, alpha = 0.6)
plt.plot(train_timestamps, y_train_pred, color = 'blue', linewidth=0.8)
plt.legend(['Actual','Predicted'])
plt.xlabel('Timestamp')
plt.title("Training data prediction")
plt.show()
```

----------------------------------------

TITLE: Reordering Columns and Saving Final DataFrame to CSV - Python
DESCRIPTION: Reorders the columns of the DataFrame into a specified order for better readability and organization. Finally, it saves the processed DataFrame, including the newly added sentiment columns, to a new CSV file named 'Hotel_Reviews_NLP.csv', excluding the DataFrame index.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_7

LANGUAGE: Python
CODE:
```
# Reorder the columns (This is cosmetic, but to make it easier to explore the data later)
df = df.reindex(["Hotel_Name", "Hotel_Address", "Total_Number_of_Reviews", "Average_Score", "Reviewer_Score", "Negative_Sentiment", "Positive_Sentiment", "Reviewer_Nationality", "Leisure_trip", "Couple", "Solo_traveler", "Business_trip", "Group", "Family_with_young_children", "Family_with_older_children", "With_a_pet", "Negative_Review", "Positive_Review"], axis=1)

print("Saving results to Hotel_Reviews_NLP.csv")
df.to_csv(r"../data/Hotel_Reviews_NLP.csv", index = False)
```

----------------------------------------

TITLE: Calculating Average Reviewer Score per Hotel with Pandas
DESCRIPTION: Calculates the average `Reviewer_Score` for each hotel based on the scores available in the dataset. It adds this calculated average as a new column `Calc_Average_Score`. It also calculates the difference between the original `Average_Score` column and the calculated average, adding it as `Average_Score_Difference`. Finally, it displays the hotels with their original and calculated average scores and the difference, sorted by the difference. Requires the `df` DataFrame with 'Average_Score' and 'Reviewer_Score' columns. Outputs a DataFrame summarizing scores and their differences per hotel.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_5

LANGUAGE: python
CODE:
```
# define a function that takes a row and performs some calculation with it
def get_difference_review_avg(row):
  return row["Average_Score"] - row["Calc_Average_Score"]

# 'mean' is mathematical word for 'average'
df['Calc_Average_Score'] = round(df.groupby('Hotel_Name').Reviewer_Score.transform('mean'), 1)

# Add a new column with the difference between the two average scores
df["Average_Score_Difference"] = df.apply(get_difference_review_avg, axis = 1)

# Create a df without all the duplicates of Hotel_Name (so only 1 row per hotel)
review_scores_df = df.drop_duplicates(subset = ["Hotel_Name"])

# Sort the dataframe to find the lowest and highest average score difference
review_scores_df = review_scores_df.sort_values(by=["Average_Score_Difference"])

display(review_scores_df[["Average_Score_Difference", "Average_Score", "Calc_Average_Score", "Hotel_Name"]])
```

----------------------------------------

TITLE: Applying Sentiment Calculation Function to DataFrame Columns Python
DESCRIPTION: Illustrates how to apply the previously defined 'calc_sentiment' function to the 'Negative_Review' and 'Positive_Review' columns of a Pandas DataFrame. This operation would create new columns (implicitly, as the example code is incomplete but shows the method of application) containing the calculated sentiment scores for each review.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_5

LANGUAGE: Python
CODE:
```
# df['Negative_Sentiment'] = df.Negative_Review.apply(calc_sentiment)
# df['Positive_Sentiment'] = df.Positive_Review.apply(calc_sentiment)






```

----------------------------------------

TITLE: Plotting Mean Price per Month as Bar Chart in Python
DESCRIPTION: Groups the `new_pumpkins` DataFrame by 'Month', calculates the mean 'Price' for each month, and then generates a bar chart using Pandas' built-in plotting functionality. The y-axis is labeled 'Pumpkin Price' for clarity.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/2-Regression/2-Data/README.md#_snippet_9

LANGUAGE: Python
CODE:
```
new_pumpkins.groupby(['Month'])['Price'].mean().plot(kind='bar')
plt.ylabel("Pumpkin Price")
```

----------------------------------------

TITLE: Cleaning Hotel Tags Column in Pandas - Python
DESCRIPTION: Cleans the 'Tags' column in the pandas DataFrame. It removes the leading and trailing square brackets and single quotes using `str.strip()` and replaces the comma-space-quote pattern (', ') within the string with just a comma (,) using `str.replace()`, effectively converting the string representation of a list into a comma-separated string of tags.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
# Remove opening and closing brackets
df.Tags = df.Tags.str.strip("[']")
# remove all quotes too
df.Tags = df.Tags.str.replace(" ', '", ",", regex = False)
```

----------------------------------------

TITLE: Defining Bin-Based State Discretization Functions Python
DESCRIPTION: Defines helper functions `create_bins` and `discretize_bins` for state discretization using fixed bins. `create_bins` generates bin edges for a given interval and number of bins. `discretize_bins` applies `numpy.digitize` to map each element of an observation vector to its corresponding bin index based on predefined intervals and bin counts.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/README.md#_snippet_6

LANGUAGE: python
CODE:
```
def create_bins(i,num):
    return np.arange(num+1)*(i[1]-i[0])/num+i[0]

print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))

ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
nbins = [20,20,10,10] # number of bins for each parameter
bins = [create_bins(ints[i],nbins[i]) for i in range(4)]

def discretize_bins(x):
    return tuple(np.digitize(x[i],bins[i]) for i in range(4))
```

----------------------------------------

TITLE: Defining Bin-Based State Discretization - Python
DESCRIPTION: Provides an alternative method for state discretization using bins. It defines a helper function `create_bins` to generate bin edges for a given interval and number of bins. It then defines intervals and number of bins for each observation dimension and uses `np.digitize` within the `discretize_bins` function to map continuous observations to bin indices.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/translations/README.ko.md#_snippet_6

LANGUAGE: python
CODE:
```
def create_bins(i,num):\n    return np.arange(num+1)*(i[1]-i[0])/num+i[0]\n\nprint("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))\n\nints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter\nbins = [create_bins(ints[i],nbins[i]) for i in range(4)]\n\ndef discretize_bins(x):\n    return tuple(np.digitize(x[i],bins[i]) for i in range(4))
```

----------------------------------------

TITLE: Run Q-Learning Training Loop - Python
DESCRIPTION: Implements the main Q-learning training loop over a specified number of epochs. In each epoch, it runs a simulation, selects actions based on an epsilon-greedy policy (exploration vs. exploitation), updates Q-values using the Q-learning formula, tracks cumulative rewards, and periodically prints average rewards to monitor progress.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/8-Reinforcement/2-Gym/README.md#_snippet_10

LANGUAGE: Python
CODE:
```
def probs(v,eps=1e-4):
    v = v-v.min()+eps
    v = v/v.sum()
    return v

Qmax = 0
cum_rewards = []
rewards = []
for epoch in range(100000):
    obs = env.reset()
    done = False
    cum_reward=0
    # == do the simulation ==
    while not done:
        s = discretize(obs)
        if random.random()<epsilon:
            # exploitation - chose the action according to Q-Table probabilities
            v = probs(np.array(qvalues(s)))
            a = random.choices(actions,weights=v)[0]
        else:
            # exploration - randomly chose the action
            a = np.random.randint(env.action_space.n)

        obs, rew, done, info = env.step(a)
        cum_reward+=rew
        ns = discretize(obs)
        Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
    cum_rewards.append(cum_reward)
    rewards.append(cum_reward)
    # == Periodically print results and calculate average reward ==
    if epoch%5000==0:
        print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
        if np.average(cum_rewards) > Qmax:
            Qmax = np.average(cum_rewards)
            Qbest = Q
        cum_rewards=[]
```

----------------------------------------

TITLE: Visualizing Categorical Data with Seaborn (Python)
DESCRIPTION: This snippet imports the Seaborn library, defines a color palette for pumpkin colors, and creates a categorical count plot using sns.catplot. It visualizes the distribution of Variety grouped by Color, showing the count of pumpkins for each combination.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/README.md#_snippet_2

LANGUAGE: python
CODE:
```
import seaborn as sns

palette = {
'ORANGE': 'orange',
'WHITE': 'wheat',
}

sns.catplot(
data=pumpkins, y="Variety", hue="Color", kind="count",
palette=palette,
)
```

----------------------------------------

TITLE: Melting Tag Columns with Pandas
DESCRIPTION: Transforms the `df` DataFrame by unpivoting the 'Tag_1' through 'Tag_6' columns into a single column named 'value' in a new DataFrame, `df_tags`. This 'melt' operation reshapes the data to facilitate counting the occurrences of each unique tag across all original tag columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/2-notebook.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
# Merge the 6 columns into one with melt
df_tags = df.melt(value_vars=["Tag_1", "Tag_2", "Tag_3", "Tag_4", "Tag_5", "Tag_6"])
```

----------------------------------------

TITLE: Saving Preprocessed Data to CSV in R
DESCRIPTION: Saves the balanced and preprocessed dataframe `preprocessed_df` to a CSV file named `cleaned_cuisines_R.csv`. The file is saved in the `../../../data/` directory relative to the current script's location using the `write_csv` function from the `readr` package.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb#_snippet_18

LANGUAGE: R
CODE:
```
# Save preprocessed data
write_csv(preprocessed_df, "../../../data/cleaned_cuisines_R.csv")
```

----------------------------------------

TITLE: Plotting Scatter Plot by Category with Pandas (Python)
DESCRIPTION: This code iterates through unique pumpkin varieties in the new_pumpkins DataFrame and creates a scatter plot of 'Price' versus 'DayOfYear' for each variety on the same axes. It uses different colors for each variety to visualize the relationship between price, day, and variety. Dependencies: pandas DataFrame new_pumpkins, matplotlib (implicitly used by pandas plotting).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/2-Regression/3-Linear/README.md#_snippet_1

LANGUAGE: python
CODE:
```
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

----------------------------------------

TITLE: Visualize Confusion Matrix Heatmap (Tidymodels/yardstick/ggplot2) - R
DESCRIPTION: Generates a heatmap visualization of the confusion matrix calculated from the `results` data. It first adjusts ggplot geometry defaults for tiles and then uses `autoplot` with `type = "heatmap"` to create a visually intuitive representation of correct and incorrect classifications.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/solution/R/lesson_11-R.ipynb#_snippet_9

LANGUAGE: R
CODE:
```
update_geom_defaults(geom = "tile", new = list(color = "black", alpha = 0.7))
# Visualize confusion matrix
results %>%
  conf_mat(cuisine, .pred_class) %>%
  autoplot(type = "heatmap")
```

----------------------------------------

TITLE: Exporting SVC Model to ONNX (Python)
DESCRIPTION: Converts the trained scikit-learn SVC model into the ONNX format using `skl2onnx`. It defines the input type as a `FloatTensorType` and specifies conversion options before serializing the ONNX model and saving it to a file named 'model.onnx'.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/solution/notebook.ipynb#_snippet_10

LANGUAGE: Python
CODE:
```
from skl2onnx import convert_sklearn
from skl2onnx.common.data_types import FloatTensorType

initial_type = [('float_input', FloatTensorType([None, 380]))]
options = {id(model): {'nocl': True, 'zipmap': False}}
onx = convert_sklearn(model, initial_types=initial_type, options=options)
with open("./model.onnx", "wb") as f:
    f.write(onx.SerializeToString())
```

----------------------------------------

TITLE: Extracting Features with Pandas Python
DESCRIPTION: Creates a new DataFrame `cuisines_feature_df` by dropping the columns 'Unnamed: 0' (potentially an index column) and 'cuisine' (the label column) from the original `cuisines_df`. The remaining columns are intended to be used as features for the machine learning model. Displays the first few rows of the feature DataFrame. Requires the `cuisines_df` DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/notebook.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)
cuisines_feature_df.head()
```

----------------------------------------

TITLE: Transforming Date Column with lubridate and dplyr in R
DESCRIPTION: Loads the 'lubridate' package. It then uses 'dplyr::mutate()' twice: first with 'lubridate::mdy()' to convert the 'Date' column (assuming MM/DD/YYYY format) into a proper date object, and second with 'lubridate::month()' to extract the month as a number into a new 'Month' column. Finally, it uses 'dplyr::select()' with a negative sign to drop the original 'Date' column and displays the first 7 rows of the modified dataframe.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/solution/R/lesson_2-R.ipynb#_snippet_6

LANGUAGE: R
CODE:
```
library(lubridate)

pumpkins <- pumpkins %>%
  # Convert the Date column to a date object
  mutate(Date = mdy(Date)) %>%
  # Extract month from Date
  mutate(Month = month(Date)) %>%
  # Drop Date column
  select(-Date)

# View the first few rows
pumpkins %>%
  slice_head(n = 7)
```

----------------------------------------

TITLE: Loading and Inspecting UFO Data (Python)
DESCRIPTION: Imports the pandas and numpy libraries and loads the UFO sighting data from a CSV file into a pandas DataFrame. The head() method is used to display the first few rows of the DataFrame to get a quick overview of its structure and content. Requires pandas and numpy.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/3-Web-App/1-Web-App/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
import pandas as pd
import numpy as np

ufos = pd.read_csv('./data/ufos.csv')
ufos.head()
```

----------------------------------------

TITLE: Normalize Price Based on Package Size (Python)
DESCRIPTION: Adjusts the calculated 'Price' in the `new_pumpkins` DataFrame to represent the price per standard bushel. It divides the price for '1 1/9 bushel' packages by (1 + 1/9) and the price for '1/2 bushel' packages by (1/2), standardizing the unit to one bushel.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/README.md#_snippet_6

LANGUAGE: python
CODE:
```
new_pumpkins.loc[new_pumpkins['Package'].str.contains('1 1/9'), 'Price'] = price/(1 + 1/9)

new_pumpkins.loc[new_pumpkins['Package'].str.contains('1/2'), 'Price'] = price/(1/2)
```

----------------------------------------

TITLE: Loading Pumpkin Dataset with Pandas (Python)
DESCRIPTION: Imports the pandas and numpy libraries, then loads the 'US-pumpkins.csv' file located in the '../data/' directory into a pandas DataFrame named 'full_pumpkins'. Finally, it displays the first five rows of the created DataFrame to the console for initial data inspection. This requires the pandas and numpy libraries installed and the specified CSV file available.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import pandas as pd
import numpy as np

full_pumpkins = pd.read_csv('../data/US-pumpkins.csv')

full_pumpkins.head()
```

----------------------------------------

TITLE: Loading Data with Pandas Python
DESCRIPTION: Imports the pandas library, reads data from the 'cleaned_cuisines.csv' file into a DataFrame named `cuisines_df`, and displays the first few rows to inspect the loaded data structure. Requires the pandas library installed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import pandas as pd
cuisines_df = pd.read_csv("../data/cleaned_cuisines.csv")
cuisines_df.head()
```

----------------------------------------

TITLE: Applying Stop Words Removal to Review Columns Python
DESCRIPTION: Applies the `remove_stopwords` function to the 'Negative_Review' and 'Positive_Review' columns of the DataFrame. This cleans the text in these columns by removing common stop words.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/3-notebook.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
# Remove the stop words from both columns
df.Negative_Review = df.Negative_Review.apply(remove_stopwords)   
df.Positive_Review = df.Positive_Review.apply(remove_stopwords)

```

----------------------------------------

TITLE: Visualizing Correlation Matrix with Heatmap Python
DESCRIPTION: Calculates the pairwise correlation of columns in the filtered DataFrame using `df.corr()` and visualizes the resulting correlation matrix as a heatmap using `seaborn.heatmap`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/notebook.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
corrmat = df.corr()
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corrmat, vmax=.8, square=True);
```

----------------------------------------

TITLE: Plotting Actual vs Predicted Forecasts Python
DESCRIPTION: Generates a plot to visually compare the actual load values from the test set against the model's forecasts. It includes logic to handle plotting for both single-step and multi-step horizons, showing how predictions might deviate or spread out over the forecast horizon.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/solution/notebook.ipynb#_snippet_18

LANGUAGE: python
CODE:
```
if(HORIZON == 1):
    ## Plotting single step forecast
    eval_df.plot(x='timestamp', y=['actual', 'prediction'], style=['r', 'b'], figsize=(15, 8))

else:
    ## Plotting multi step forecast
    plot_df = eval_df[(eval_df.h=='t+1')][['timestamp', 'actual']]
    for t in range(1, HORIZON+1):
        plot_df['t+'+str(t)] = eval_df[(eval_df.h=='t+'+str(t))]['prediction'].values

    fig = plt.figure(figsize=(15, 8))
    ax = plt.plot(plot_df['timestamp'], plot_df['actual'], color='red', linewidth=4.0)
    ax = fig.add_subplot(111)
    for t in range(1, HORIZON+1):
        x = plot_df['timestamp'][(t-1):]
        y = plot_df['t+'+str(t)][0:len(x)]
        ax.plot(x, y, color='blue', linewidth=4*math.pow(.9,t), alpha=math.pow(0.8,t))
    
    ax.legend(loc='best')
    
plt.xlabel('timestamp', fontsize=12)
plt.ylabel('load', fontsize=12)
plt.show()
```

----------------------------------------

TITLE: Preparing Data for Clustering (Python)
DESCRIPTION: Selects specific numerical and categorical features from the pandas DataFrame (`df`) that will be used for clustering. It then applies `sklearn.preprocessing.LabelEncoder` to the categorical `artist_top_genre` column, transforming its string values into numerical labels required by the K-Means algorithm.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/de/5-Clustering/2-K-Means/README.md#_snippet_1

LANGUAGE: python
CODE:
```
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

X = df.loc[:, ('artist_top_genre','popularity','danceability','acousticness','loudness','energy')]

y = df['artist_top_genre']

X['artist_top_genre'] = le.fit_transform(X['artist_top_genre'])

y = le.transform(y)
```

----------------------------------------

TITLE: Applying Sentiment Analysis to Review Columns - Python
DESCRIPTION: Applies the previously defined `calc_sentiment` function to the 'Negative_Review' and 'Positive_Review' columns of the DataFrame to calculate and store the sentiment scores in new 'Negative_Sentiment' and 'Positive_Sentiment' columns, respectively. The execution time for this process is also measured and printed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_5

LANGUAGE: Python
CODE:
```
# Add a negative sentiment and positive sentiment column
print("Calculating sentiment columns for both positive and negative reviews")
start = time.time()
df["Negative_Sentiment"] = df.Negative_Review.apply(calc_sentiment)
df["Positive_Sentiment"] = df.Positive_Review.apply(calc_sentiment)
end = time.time()
print("Calculating sentiment took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Defining Environment Reward Function - Python
DESCRIPTION: Defines the reward values associated with different outcomes in the environment: a small negative reward for each step taken (`move_reward`), a positive reward for reaching the goal (apple, `goal_reward`), and a large negative reward for hitting a terminal negative state (wolf or water, `end_reward`). The `reward` function takes the board state and an optional position, returning the corresponding reward based on the cell type at that location.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

----------------------------------------

TITLE: Defining Reinforcement Learning Reward Function Python
DESCRIPTION: Defines the `reward` function, which provides feedback to the agent based on its current position or the state it moves into. Positive reward is given for reaching the goal (apple), negative reward for movement, and large negative reward for failure states (water, wolf, out of bounds).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/notebook.ipynb#_snippet_6

LANGUAGE: Python
CODE:
```
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

----------------------------------------

TITLE: Stripping Whitespace from Tag Columns with Pandas
DESCRIPTION: Applies the `str.strip()` method to the first six columns (index 0 to 5) of the `tag_list_df` DataFrame to remove leading and trailing whitespace from each tag string. The cleaned tags are then assigned to new columns named 'Tag_1' through 'Tag_6' in the original `df` DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/2-notebook.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
# Remove leading and trailing spaces
df["Tag_1"] = tag_list_df[0].str.strip()
df["Tag_2"] = tag_list_df[1].str.strip()
df["Tag_3"] = tag_list_df[2].str.strip()
df["Tag_4"] = tag_list_df[3].str.strip()
df["Tag_5"] = tag_list_df[4].str.strip()
df["Tag_6"] = tag_list_df[5].str.strip()
```

----------------------------------------

TITLE: Convert and Save ONNX Model Python
DESCRIPTION: Converts the trained scikit-learn `model` to the ONNX format using `convert_sklearn`, applying the specified `initial_type` and `options`. The resulting ONNX model object is then serialized and saved to a file named `model.onnx` in binary write mode.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/4-Classification/4-Applied/README.md#_snippet_10

LANGUAGE: python
CODE:
```
onx = convert_sklearn(model, initial_types=initial_type, options=options)
with open("./model.onnx", "wb") as f:
    f.write(onx.SerializeToString())
```

----------------------------------------

TITLE: Plot Forecast Accuracy vs Actual in Python
DESCRIPTION: This snippet generates a plot comparing the model's predictions to the actual data. If `HORIZON` is 1, it plots the single-step prediction against the actuals. If `HORIZON` is greater than 1, it plots the actual time series and overlays the multi-step predictions, with lines potentially fading or thinning for steps further out in the horizon, providing a visual assessment of the model's performance.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/README.md#_snippet_17

LANGUAGE: python
CODE:
```
if(HORIZON == 1):
    ## Plotting single step forecast
    eval_df.plot(x='timestamp', y=['actual', 'prediction'], style=['r', 'b'], figsize=(15, 8))

else:
    ## Plotting multi step forecast
    plot_df = eval_df[(eval_df.h=='t+1')][['timestamp', 'actual']]
    for t in range(1, HORIZON+1):
        plot_df['t+'+str(t)] = eval_df[(eval_df.h=='t+'+str(t))]['prediction'].values

    fig = plt.figure(figsize=(15, 8))
    ax = plt.plot(plot_df['timestamp'], plot_df['actual'], color='red', linewidth=4.0)
    ax = fig.add_subplot(111)
    for t in range(1, HORIZON+1):
        x = plot_df['timestamp'][(t-1):]
        y = plot_df['t+'+str(t)][0:len(x)]
        ax.plot(x, y, color='blue', linewidth=4*math.pow(.9,t), alpha=math.pow(0.8,t))

    ax.legend(loc='best')

plt.xlabel('timestamp', fontsize=12)
plt.ylabel('load', fontsize=12)
plt.show()
```

----------------------------------------

TITLE: Calculating Correlation with Pandas (Python)
DESCRIPTION: This snippet calculates and prints the Pearson correlation coefficient between the 'Month' and 'Price' columns, and between the 'DayOfYear' and 'Price' columns of the new_pumpkins DataFrame. It uses the .corr() method provided by pandas Series. Dependencies: pandas DataFrame new_pumpkins.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/2-Regression/3-Linear/README.md#_snippet_0

LANGUAGE: python
CODE:
```
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

----------------------------------------

TITLE: Predicting Class Probabilities for a Single Sample (Python, Scikit-learn, Pandas)
DESCRIPTION: This code snippet takes a single test sample (row 50) from `X_test`, reshapes its values for prediction (`.reshape(-1, 1).T`), and uses the trained `LogisticRegression` model (`model`) to predict the probability of each class (`predict_proba`). It then uses Pandas to create a DataFrame displaying the probabilities with class names as columns, transposes it, sorts by probability, and shows the top predictions using `.head()`. Requires Scikit-learn and Pandas.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/README.md#_snippet_7

LANGUAGE: python
CODE:
```
test= X_test.iloc[50].values.reshape(-1, 1).T
proba = model.predict_proba(test)
classes = model.classes_
resultdf = pd.DataFrame(data=proba, columns=classes)

topPrediction = resultdf.T.sort_values(by=[0], ascending = [False])
topPrediction.head()
```

----------------------------------------

TITLE: Importing Core Data Science Libraries (Python)
DESCRIPTION: Imports essential libraries for data manipulation, visualization, machine learning, and utility functions. Includes pandas for data handling, numpy for numerical operations, matplotlib for plotting, scikit-learn for SVR and scaling, and custom functions like 'load_data' and 'mape'.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_1

LANGUAGE: python
CODE:
```
import os
import warnings
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import datetime as dt
import math

from sklearn.svm import SVR
from sklearn.preprocessing import MinMaxScaler
from common.utils import load_data, mape
```

----------------------------------------

TITLE: Plotting Mean Price by Variety with Pandas
DESCRIPTION: Calculates the mean 'Price' for each unique 'Variety' in the `new_pumpkins` DataFrame and generates a bar chart of these mean prices. Requires a pandas DataFrame with 'Variety' and 'Price' columns, and matplotlib (implicitly used by pandas plotting). Outputs a bar chart.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/fr/2-Regression/3-Linear/README.md#_snippet_2

LANGUAGE: python
CODE:
```
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

----------------------------------------

TITLE: Standardizing Price Based on Package Size with Pandas in Python
DESCRIPTION: Adjusts the calculated 'Price' in the `new_pumpkins` DataFrame for packages containing '1 1/9' or '1/2' bushel. It divides the price by the respective fraction to standardize the price per full bushel volume.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/2-Regression/2-Data/README.md#_snippet_6

LANGUAGE: Python
CODE:
```
new_pumpkins.loc[new_pumpkins['Package'].str.contains('1 1/9'), 'Price'] = price/(1 + 1/9)

new_pumpkins.loc[new_pumpkins['Package'].str.contains('1/2'), 'Price'] = price/(1/2)
```

----------------------------------------

TITLE: Plotting Linear Regression Results in Python
DESCRIPTION: Generates a scatter plot of the actual test data points (`X_test`, `y_test`). It then overlays the predicted values (`pred`) plotted against the test features (`X_test`) using a line plot to visually represent the linear regression line's fit to the test data. Assumes `matplotlib.pyplot` is imported as `plt`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/2-Regression/3-Linear/README.md#_snippet_11

LANGUAGE: python
CODE:
```
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

----------------------------------------

TITLE: Visualizing Linear Regression Results - Python
DESCRIPTION: This code generates a scatter plot of the actual test data points (`X_test`, `y_test`). It then overlays a line plot representing the model's predictions (`X_test`, `pred`) for the same test data. This visualization helps assess how well the linear regression line fits the test data points.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/2-Regression/3-Linear/README.md#_snippet_11

LANGUAGE: python
CODE:
```
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

----------------------------------------

TITLE: Generating Descriptive Statistics for DataFrame Python
DESCRIPTION: Generates descriptive statistics (count, mean, std, min, max, quartiles) for the numerical columns of the DataFrame. This provides a quick overview of the data's distribution and central tendencies.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/notebook.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
df.describe()
```

----------------------------------------

TITLE: Displaying the first rows of a Pandas DataFrame
DESCRIPTION: Shows the top 5 rows of the loaded DataFrame. This provides a quick visual inspection of the data's structure, column names, and initial values to ensure the data was loaded correctly and understand its format.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/README.md#_snippet_3

LANGUAGE: python
CODE:
```
df.head()
```

----------------------------------------

TITLE: Evaluating Polynomial Regression (Scikit-learn) - Python
DESCRIPTION: This snippet calculates the accuracy score (R-squared) of the trained polynomial regression pipeline on the training data. It provides a metric to assess how well the polynomial model fits the training data compared to the linear model.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/translations/README.ko.md#_snippet_14

LANGUAGE: Python
CODE:
```
accuracy_score = pipeline.score(X_train,y_train)
print('Model Accuracy: ', accuracy_score)
```

----------------------------------------

TITLE: Predict and View Polynomial Model Results (R)
DESCRIPTION: Uses the fitted polynomial regression workflow (`poly_wf_fit`) to make predictions on the test set (`pumpkins_test`). It binds the predictions (`.pred`) to the test data, including original `package` and `price` columns, and prints the first 10 rows of the resulting `poly_results` data frame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_18

LANGUAGE: R
CODE:
```
# Make price predictions on test data
poly_results <- poly_wf_fit %>% predict(new_data = pumpkins_test) %>%
  bind_cols(pumpkins_test %>% select(c(package, price))) %>%
  relocate(.pred, .after = last_col())


# Print the results
poly_results %>%
  slice_head(n = 10)
```

----------------------------------------

TITLE: Making Predictions with Trained Model in R
DESCRIPTION: Applies the trained linear regression model (lm_mod) to the test dataset (diabetes_test) using the predict() function to generate predictions for the outcome variable 'y'. The first 5 rows of the resulting predictions data frame (a tibble) are then displayed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/solution/R/lesson_1-R.ipynb#_snippet_7

LANGUAGE: R
CODE:
```
# Make predictions for the test set
predictions <- lm_mod %>%
  predict(new_data = diabetes_test)

# Print out some of the predictions
predictions %>%
  slice(1:5)
```

----------------------------------------

TITLE: Saving and Loading Trained Model Python
DESCRIPTION: This snippet demonstrates how to save and load a trained machine learning model using Python's `pickle` module. It saves the trained `LogisticRegression` model to a file named 'ufo-model.pkl' in binary write mode ('wb'). It then loads the model back from the same file in binary read mode ('rb') and uses the loaded model to make a prediction on a new sample data point. Requires the trained scikit-learn model and the pickle library.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/3-Web-App/1-Web-App/solution/notebook.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
import pickle
model_filename = 'ufo-model.pkl'
pickle.dump(model, open(model_filename,'wb'))

model = pickle.load(open('ufo-model.pkl','rb'))
print(model.predict([[50,44,-12]]))
```

----------------------------------------

TITLE: Calculating VADER Sentiment Score for Review Text Python
DESCRIPTION: Initializes the NLTK VADER sentiment intensity analyzer. It defines a function 'calc_sentiment' that takes a review string. If the review is 'No Negative' or 'No Positive', it returns 0. Otherwise, it calculates the compound sentiment score using the VADER analyzer and returns the score.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Create the vader sentiment analyser (there are others in NLTK you can try too)
vader_sentiment = SentimentIntensityAnalyzer()
# Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.

# There are 3 possibilities of input for a review:
# It could be "No Negative", in which case, return 0
# It could be "No Positive", in which case, return 0
# It could be a review, in which case calculate the sentiment
def calc_sentiment(review):
    if review == "No Negative" or review == "No Positive":
        return 0
    return vader_sentiment.polarity_scores(review)["compound"]
```

----------------------------------------

TITLE: Loading and Inspecting Data with Pandas - Python
DESCRIPTION: This code imports the necessary libraries, matplotlib for plotting and pandas for data manipulation. It then loads the 'nigerian-songs.csv' file into a pandas DataFrame and displays the first 5 rows of the DataFrame to provide a quick look at the data structure and content. Requires pandas and matplotlib.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/README.md#_snippet_1

LANGUAGE: python
CODE:
```
import matplotlib.pyplot as plt
import pandas as pd

df = pd.read_csv("../data/nigerian-songs.csv")
df.head()
```

----------------------------------------

TITLE: Loading and Inspecting Diabetes Data in R
DESCRIPTION: Reads the diabetes dataset from a specified URL into an R data frame (tibble) using read_table2 from the readr package. It then uses glimpse() to get a summary of the data structure and slice() with the pipe operator (%>%) to display the first 5 rows.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/solution/R/lesson_1-R.ipynb#_snippet_2

LANGUAGE: R
CODE:
```
# Import the data set
diabetes <- read_table2(file = "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.rwrite1.txt")


# Get a glimpse and dimensions of the data
glimpse(diabetes)


# Select the first 5 rows of the data
diabetes %>%
  slice(1:5)
```

----------------------------------------

TITLE: Importing Libraries and Loading Data (Python)
DESCRIPTION: Imports necessary Python libraries (Matplotlib, Pandas, Seaborn) for data manipulation and visualization. It then loads a CSV file containing Nigerian song data into a Pandas DataFrame. The CSV file `../data/nigerian-songs.csv` is a required input.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/notebook.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

df = pd.read_csv("../data/nigerian-songs.csv")
df.head()
```

----------------------------------------

TITLE: Training Q-Learning Algorithm Python
DESCRIPTION: This code implements the main Q-learning training loop over 5000 epochs. In each epoch, the agent starts from a random position, selects actions based on a probability distribution derived from the Q-Table (incorporating exploration), moves, receives a reward, and updates the corresponding Q-Table entry using the Bellman equation. The process terminates upon reaching the end reward or exceeding a negative cumulative reward threshold.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/8-Reinforcement/1-QLearning/README.md#_snippet_1

LANGUAGE: python
CODE:
```
for epoch in range(5000):

    # Pick initial point
    m.random_start()

    # Start travelling
    n=0
    cum_reward = 0
    while True:
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        dpos = actions[a]
        m.move(dpos,check_correctness=False) # we allow player to move outside the board, which terminates episode
        r = reward(m)
        cum_reward += r
        if r==end_reward or cum_reward < -1000:
            lpath.append(n)
            break
        alpha = np.exp(-n / 10e5)
        gamma = 0.5
        ai = action_idx[a]
        Q[x,y,ai] = (1 - alpha) * Q[x,y,ai] + alpha * (r + gamma * Q[x+dpos[0], y+dpos[1]].max())
        n+=1
```

----------------------------------------

TITLE: Predict and View Linear Model Results (R)
DESCRIPTION: Makes predictions for the test set using the fitted linear regression workflow (`lm_wf_fit`), binds the predictions (`.pred`) to the original test data, keeping the `package` and `price` columns. Finally, it prints the first 10 rows of the resulting data frame to inspect the structure and values.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_14

LANGUAGE: R
CODE:
```
# Make predictions for the test set
predictions <- lm_wf_fit %>%
  predict(new_data = pumpkins_test)


# Bind predictions to the test set
lm_results <- pumpkins_test %>%
  select(c(package, price)) %>%
  bind_cols(predictions)


# Print the first ten rows of the tibble
lm_results %>%
  slice_head(n = 10)
```

----------------------------------------

TITLE: Splitting Tags into Columns with Pandas
DESCRIPTION: Splits the cleaned string in the 'Tags' column of the `df` DataFrame into multiple new columns based on the comma delimiter. The `expand=True` argument creates a new DataFrame, `tag_list_df`, where each column represents a single tag from the original string.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/2-notebook.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
# removing this to take advantage of the 'already a phrase' fact of the dataset 
# Now split the strings into a list
tag_list_df = df.Tags.str.split(',', expand = True)
```

----------------------------------------

TITLE: Loading Hotel Review Data with Pandas (Python)
DESCRIPTION: This snippet loads the `Hotel_Reviews.csv` dataset into a pandas DataFrame named `df`. It uses the `pandas` library for efficient CSV reading and the `time` library to calculate how long the loading process takes, printing the elapsed time upon completion. This is the essential first step for subsequent data exploration and analysis.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
# Load the hotel reviews from CSV
import pandas as pd
import time
# importing time so the start and end time can be used to calculate file loading time
print("Loading data file now, this could take a while depending on file size")
start = time.time()
# df is 'DataFrame' - make sure you downloaded the file to the data folder
df = pd.read_csv('../../data/Hotel_Reviews.csv')
end = time.time()
print("Loading took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Creating Preprocessing Recipe with SMOTE in R
DESCRIPTION: Loads the `themis` package, which provides steps for dealing with imbalanced data. Creates a `recipes` object using the formula `cuisine ~ .` to define `cuisine` as the outcome and all other variables in `df_select` as predictors. A `step_smote(cuisine)` is added to specify that Synthetic Minority Over-sampling Technique (SMOTE) should be applied to the `cuisine` variable to balance the classes.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb#_snippet_15

LANGUAGE: R
CODE:
```
# Load themis package for dealing with imbalanced data
library(themis)

# Create a recipe for preprocessing data
cuisines_recipe <- recipe(cuisine ~ ., data = df_select) %>% 
  step_smote(cuisine)

cuisines_recipe
```

----------------------------------------

TITLE: Comparing Classification Models - scikit-learn - Python
DESCRIPTION: Imports necessary modules from scikit-learn for SVM, Label Spreading, and Self-training. Prepares features (danceability, acousticness) and target (energy) from the DataFrame. Compares Label Spreading with 30%, 50%, and 100% labeled data, Self-training with 30% and 50% labeled data (using SVC as base), and a standard RBF kernel SVC. It then generates and plots the decision boundaries for each model on the feature space, visually comparing their performance.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/solution/tester.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
from sklearn.svm import SVC
from sklearn.semi_supervised import LabelSpreading
from sklearn.semi_supervised import SelfTrainingClassifier
from sklearn import datasets

X = df[['danceability','acousticness']].values
y = df['energy'].values

# X = scaler.fit_transform(X)

# step size in the mesh
h = .02

rng = np.random.RandomState(0)
y_rand = rng.rand(y.shape[0])
y_30 = np.copy(y)
y_30[y_rand < 0.3] = -1  # set random samples to be unlabeled
y_50 = np.copy(y)
y_50[y_rand < 0.5] = -1
# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
ls30 = (LabelSpreading().fit(X, y_30), y_30, 'Label Spreading 30% data')
ls50 = (LabelSpreading().fit(X, y_50), y_50, 'Label Spreading 50% data')
ls100 = (LabelSpreading().fit(X, y), y, 'Label Spreading 100% data')

# the base classifier for self-training is identical to the SVC
base_classifier = SVC(kernel='rbf', gamma=.5, probability=True)
st30 = (SelfTrainingClassifier(base_classifier).fit(X, y_30),
        y_30, 'Self-training 30% data')
st50 = (SelfTrainingClassifier(base_classifier).fit(X, y_50),
        y_50, 'Self-training 50% data')

rbf_svc = (SVC(kernel='rbf', gamma=.5).fit(X, y), y, 'SVC with rbf kernel')

# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

color_map = {-1: (1, 1, 1), 0: (0, 0, .9), 1: (1, 0, 0), 2: (.8, .6, 0)}

classifiers = (ls30, st30, ls50, st50, ls100, rbf_svc)
for i, (clf, y_train, title) in enumerate(classifiers):
    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, x_max]x[y_min, y_max].
    plt.subplot(3, 2, i + 1)
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
    plt.axis('off')

    # Plot also the training points
    colors = [color_map[y] for y in y_train]
    plt.scatter(X[:, 0], X[:, 1], c=colors, edgecolors='black')

    plt.title(title)

plt.suptitle("Unlabeled points are colored white", y=0.1)
plt.show()
```

----------------------------------------

TITLE: Selecting Features for Clustering R
DESCRIPTION: Selects a subset of specific numeric columns ('popularity', 'danceability', 'acousticness', 'loudness', 'energy') from the `df_numeric` dataframe based on their suitability and range for the upcoming K-Means clustering task. A commented-out line shows how data could optionally be scaled.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb#_snippet_5

LANGUAGE: R
CODE:
```
# Select variables with similar ranges
df_numeric_select <- df_numeric %>%
  select(popularity, danceability, acousticness, loudness, energy)

# Normalize data
# df_numeric_select <- scale(df_numeric_select)
```

----------------------------------------

TITLE: Preparing Full Dataset for Prediction Python
DESCRIPTION: This snippet prepares the entire dataset for generating predictions from the trained model. It scales the data using a pre-fitted scaler, reshapes it into a time-step sequence tensor considering the `timesteps`, and separates it into input features `X` and corresponding actual outputs `Y`. It prints the shapes of the resulting arrays for verification.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_24

LANGUAGE: python
CODE:
```
# Extracting load values as numpy array
data = energy.copy().values

# Scaling
data = scaler.transform(data)

# Transforming to 2D tensor as per model input requirement
data_timesteps=np.array([[j for j in data[i:i+timesteps]] for i in range(0,len(data)-timesteps+1)])[:,:,0]
print("Tensor shape: ", data_timesteps.shape)

# Selecting inputs and outputs from data
X, Y = data_timesteps[:,:timesteps-1],data_timesteps[:,[timesteps-1]]
print("X shape: ", X.shape,"\nY shape: ", Y.shape)
```

----------------------------------------

TITLE: Converting DataFrames to NumPy Arrays (Python)
DESCRIPTION: Converts the pandas DataFrames containing the scaled training and testing data into NumPy arrays. This transformation is necessary as subsequent steps involving reshaping for time steps and model input typically work directly with NumPy arrays.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_9

LANGUAGE: python
CODE:
```
train_data = train.values
test_data = test.values
```

----------------------------------------

TITLE: Filtering DataFrame by Package Type using String Contains
DESCRIPTION: Filters the `pumpkins` DataFrame in place by selecting rows where the 'Package' column's string content matches the pattern 'bushel', ignoring case sensitivity (`case=True`) and treating the pattern as a regular expression (`regex=True`). This is done to focus analysis on pumpkins sold by the bushel unit. Requires a Pandas DataFrame with a 'Package' column.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/2-Data/README.md#_snippet_5

LANGUAGE: Python
CODE:
```
pumpkins = pumpkins[pumpkins['Package'].str.contains('bushel', case=True, regex=True)]
```

----------------------------------------

TITLE: Applying Sentiment Calculation to Reviews (Python)
DESCRIPTION: Applies the `calc_sentiment` function to the 'Negative_Review' and 'Positive_Review' columns of the DataFrame using pandas `apply`. This creates two new columns, 'Negative_Sentiment' and 'Positive_Sentiment', containing the calculated VADER compound sentiment scores for each respective review. It also measures and prints the time taken for this operation.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_5

LANGUAGE: python
CODE:
```
# Add a negative sentiment and positive sentiment column
print("Calculating sentiment columns for both positive and negative reviews")
start = time.time()
df["Negative_Sentiment"] = df.Negative_Review.apply(calc_sentiment)
df["Positive_Sentiment"] = df.Positive_Review.apply(calc_sentiment)
end = time.time()
print("Calculating sentiment took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Plotting Specific Date Range - Energy Data - Python
DESCRIPTION: Filters the `energy` DataFrame to include only data points between '2014-07-01' and '2014-07-07' (inclusive). It then plots the 'load' column for this filtered subset, using the same plotting parameters (figure size, font size, subplots) as the full data plot. X and Y axis labels are added, and the plot is displayed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/7-TimeSeries/1-Introduction/README.md#_snippet_3

LANGUAGE: python
CODE:
```
energy['2014-07-01':'2014-07-07'].plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)
plt.xlabel('timestamp', fontsize=12)
plt.ylabel('load', fontsize=12)
plt.show()
```

----------------------------------------

TITLE: Filtering and Plotting Scatter Plot for Specific Category (Python)
DESCRIPTION: This code filters the new_pumpkins DataFrame to select only rows where the 'Variety' is 'PIE TYPE', storing the result in pie_pumpkins. It then creates a scatter plot of 'Price' versus 'DayOfYear' specifically for this filtered subset. Dependencies: pandas DataFrame new_pumpkins, matplotlib (implicitly used by pandas plotting).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/2-Regression/3-Linear/README.md#_snippet_3

LANGUAGE: python
CODE:
```
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price')
```

----------------------------------------

TITLE: Calculating and Plotting Running Average Rewards Python
DESCRIPTION: This snippet defines a helper function `running_average` using NumPy's `convolve` to calculate the running average of a list or array over a specified window. It then plots this smoothed average of the `rewards` vector over a window of 100 iterations to provide a clearer view of the training trend.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/fr/8-Reinforcement/2-Gym/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

----------------------------------------

TITLE: Checking Data Shape Python
DESCRIPTION: This snippet prints the dimensions (number of rows and columns) of the loaded pandas DataFrame. This provides a quick overview of the dataset's size.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/4-Hotel-Reviews-1/solution/notebook.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
# What shape is the data (rows, columns)?
print("The shape of the data (rows, cols) is " + str(df.shape))
```

----------------------------------------

TITLE: Predict with New Data (R)
DESCRIPTION: Creates a new tibble representing a hypothetical data point (`package = "bushel baskets"`). It then uses the fitted linear and polynomial workflows (`lm_wf_fit` and `poly_wf_fit`) to predict the price for this hypothetical input. The predictions from both models are returned in a named list for comparison.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_22

LANGUAGE: R
CODE:
```
# Make a hypothetical data frame
hypo_tibble <- tibble(package = "bushel baskets")

# Make predictions using linear model
lm_pred <- lm_wf_fit %>%
 predict(new_data = hypo_tibble)

# Make predictions using polynomial model
poly_pred <- poly_wf_fit %>%
 predict(new_data = hypo_tibble)

# Return predictions in a list
list("linear model prediction" = lm_pred,
     "polynomial model prediction" = poly_pred)
```

----------------------------------------

TITLE: Calculating Average Price and Extracting Month using Pandas
DESCRIPTION: Calculates the average of the 'Low Price' and 'High Price' columns to get a single `price` value for each row. It then uses `pd.DatetimeIndex` on the 'Date' column to extract the month number, preparing these values for inclusion in a new DataFrame. Requires a Pandas DataFrame with 'Low Price', 'High Price', and 'Date' columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/2-Data/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
price = (pumpkins['Low Price'] + pumpkins['High Price']) / 2

month = pd.DatetimeIndex(pumpkins['Date']).month
```

----------------------------------------

TITLE: Plotting Training Data Predictions (Python)
DESCRIPTION: Creates a plot to visualize the model's predictions on the training data compared to the actual training values. Includes labels and a title for clarity.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_19

LANGUAGE: python
CODE:
```
plt.figure(figsize=(25,6))
# plot original output
# plot predicted output
plt.legend(['Actual','Predicted'])
plt.xlabel('Timestamp')
plt.title("Training data prediction")
plt.show()
```

----------------------------------------

TITLE: Filtering DataFrame by String Content with Pandas in Python
DESCRIPTION: Filters the original DataFrame `pumpkins` to include only rows where the 'Package' column contains the string 'bushel'. This step narrows down the data to a specific unit of sale, facilitating more consistent price analysis.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/2-Regression/2-Data/README.md#_snippet_5

LANGUAGE: Python
CODE:
```
pumpkins = pumpkins[pumpkins['Package'].str.contains('bushel', case=True, regex=True)]
```

----------------------------------------

TITLE: Plotting Top Japanese Ingredients (Pandas Matplotlib Python)
DESCRIPTION: Uses the `create_ingredient_df` function to analyze the Japanese cuisine DataFrame (`japanese_df`), selects the top 10 ingredients by count, and generates a horizontal bar plot to visualize their frequency.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/notebook.ipynb#_snippet_10

LANGUAGE: Python
CODE:
```
japanese_ingredient_df = create_ingredient_df(japanese_df)
japanese_ingredient_df.head(10).plot.barh()
```

----------------------------------------

TITLE: Loading Filtered Data for Sentiment Analysis (Pandas/NLTK/Python)
DESCRIPTION: This snippet imports necessary Python libraries including Pandas for data handling and NLTK for NLP tasks, specifically VADER for sentiment analysis. It downloads the 'vader_lexicon' required by VADER. It then loads the dataset 'Hotel_Reviews_Filtered.csv' into a Pandas DataFrame named `df`. This sets up the data and environment for subsequent sentiment analysis steps.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_2

LANGUAGE: python
CODE:
```
import time
import pandas as pd
import nltk as nltk
from nltk.corpus import stopwords
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

# Load the filtered hotel reviews from CSV
df = pd.read_csv('../../data/Hotel_Reviews_Filtered.csv')

# You code will be added here


# Finally remember to save the hotel reviews with new NLP data added
print("Saving results to Hotel_Reviews_NLP.csv")
df.to_csv(r'../data/Hotel_Reviews_NLP.csv', index = False)
```

----------------------------------------

TITLE: Training and Evaluating Polynomial Regression Model - Python
DESCRIPTION: Sets up a machine learning pipeline that first transforms features into polynomial features of degree 2 and then applies linear regression. Trains this pipeline on the 'PIE TYPE' training data, predicts prices on the test set, evaluates performance using RMSE and R2 score, and visualizes the polynomial curve fit to the test data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/notebook.ipynb#_snippet_14

LANGUAGE: python
CODE:
```
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)

pred = pipeline.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)

plt.scatter(X_test,y_test)
plt.plot(sorted(X_test),pipeline.predict(sorted(X_test)))
```

----------------------------------------

TITLE: Visualizing Distribution with Seaborn swarmplot Python
DESCRIPTION: This snippet uses a Seaborn swarmplot to visualize the distribution of 'ord__Item Size' values for each 'Color' category. It applies a custom color palette based on the binary color encoding. The code might issue a warning if there are too many data points. It requires the Seaborn library and the `encoded_pumpkins` DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/2-Regression/4-Logistic/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
    palette = {
    0: 'orange',
    1: 'wheat'
    }
    sns.swarmplot(x="Color", y="ord__Item Size", data=encoded_pumpkins, palette=palette)
```

----------------------------------------

TITLE: Running Q-Learning Training Loop - Python
DESCRIPTION: Executes the main Q-Learning training loop over a specified number of epochs. It performs exploration (random actions) or exploitation (sampling actions based on Q-values) and updates the Q-Table using the Q-Learning formula. The code also tracks cumulative rewards, calculates periodic average rewards, and saves the Q-Table associated with the highest average reward.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/README.md#_snippet_10

LANGUAGE: Python
CODE:
```
def probs(v,eps=1e-4):
    v = v-v.min()+eps
    v = v/v.sum()
    return v

Qmax = 0
cum_rewards = []
rewards = []
for epoch in range(100000):
    obs = env.reset()
    done = False
    cum_reward=0
    # == do the simulation ==
    while not done:
        s = discretize(obs)
        if random.random()<epsilon:
            # exploitation - chose the action according to Q-Table probabilities
            v = probs(np.array(qvalues(s)))
            a = random.choices(actions,weights=v)[0]
        else:
            # exploration - randomly chose the action
            a = np.random.randint(env.action_space.n)

        obs, rew, done, info = env.step(a)
        cum_reward+=rew
        ns = discretize(obs)
        Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
    cum_rewards.append(cum_reward)
    rewards.append(cum_reward)
    # == Periodically print results and calculate average reward ==
    if epoch%5000==0:
        print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
        if np.average(cum_rewards) > Qmax:
            Qmax = np.average(cum_rewards)
            Qbest = Q
        cum_rewards=[]
```

----------------------------------------

TITLE: Defining VADER Sentiment Calculation Function
DESCRIPTION: This snippet defines a Python function `calc_sentiment` that calculates the compound sentiment score for a given review text using NLTK's VADER SentimentIntensityAnalyzer. It handles special cases where the review text is 'No Negative' or 'No Positive', returning 0 for these, and otherwise computes the compound score. This function is intended to be applied to the review columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_4

LANGUAGE: python
CODE:
```
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Create the vader sentiment analyser (there are others in NLTK you can try too)
vader_sentiment = SentimentIntensityAnalyzer()
# Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.

# There are 3 possibilities of input for a review:
# It could be "No Negative", in which case, return 0
# It could be "No Positive", in which case, return 0
# It could be a review, in which case calculate the sentiment
def calc_sentiment(review):    
    if review == "No Negative" or review == "No Positive":
        return 0
    return vader_sentiment.polarity_scores(review)["compound"]
```

----------------------------------------

TITLE: Defining VADER Sentiment Calculation Function for NLTK
DESCRIPTION: This snippet defines a Python function `calc_sentiment` that takes a review string as input and calculates its compound sentiment score using the VADER (Valence Aware Dictionary and sEntiment Reasoner) sentiment analyzer from NLTK. It specifically handles cases where the review text is the placeholder "No Negative" or "No Positive", returning 0 for these. Otherwise, it returns the VADER compound score.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/fr/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_4

LANGUAGE: python
CODE:
```
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Create the vader sentiment analyser (there are others in NLTK you can try too)
vader_sentiment = SentimentIntensityAnalyzer()
# Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.

# There are 3 possibilities of input for a review:
# It could be "No Negative", in which case, return 0
# It could be "No Positive", in which case, return 0
# It could be a review, in which case calculate the sentiment
def calc_sentiment(review):    
    if review == "No Negative" or review == "No Positive":
        return 0
    return vader_sentiment.polarity_scores(review)["compound"]    

```

----------------------------------------

TITLE: Creating Tag Feature Columns - Pandas Python
DESCRIPTION: This snippet processes the 'Tags' column in a pandas DataFrame (`df`) by creating new binary columns for specific useful tags. It uses the `apply` method with lambda functions to check if a review's tags contain a target string (like "Leisure trip") and sets the corresponding new column value to 1 if present, 0 otherwise. It also handles the "Group" and "Travelers with friends" combination. Requires a pandas DataFrame with a 'Tags' column.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
# Process the Tags into new columns
# The file Hotel_Reviews_Tags.py, identifies the most important tags
# Leisure trip, Couple, Solo traveler, Business trip, Group combined with Travelers with friends, 
# Family with young children, Family with older children, With a pet
df["Leisure_trip"] = df.Tags.apply(lambda tag: 1 if "Leisure trip" in tag else 0)
df["Couple"] = df.Tags.apply(lambda tag: 1 if "Couple" in tag else 0)
df["Solo_traveler"] = df.Tags.apply(lambda tag: 1 if "Solo traveler" in tag else 0)
df["Business_trip"] = df.Tags.apply(lambda tag: 1 if "Business trip" in tag else 0)
df["Group"] = df.Tags.apply(lambda tag: 1 if "Group" in tag or "Travelers with friends" in tag else 0)
df["Family_with_young_children"] = df.Tags.apply(lambda tag: 1 if "Family with young children" in tag else 0)
df["Family_with_older_children"] = df.Tags.apply(lambda tag: 1 if "Family with older children" in tag else 0)
df["With_a_pet"] = df.Tags.apply(lambda tag: 1 if "With a pet" in tag else 0)
```

----------------------------------------

TITLE: Preparing Feature (X) and Label (y) Data
DESCRIPTION: This snippet extracts the 'DayOfYear' column as the feature (X) and the 'Price' column as the label (y) from the 'pie_pumpkins' DataFrame. It converts 'DayOfYear' to a NumPy array and reshapes it into a 2D array (N, 1) as required by Scikit-learn models.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/2-Regression/3-Linear/README.md#_snippet_6

LANGUAGE: python
CODE:
```
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

----------------------------------------

TITLE: Counting Specific Review Types using Pandas Boolean Indexing and Sum
DESCRIPTION: This snippet demonstrates a more efficient way to count rows based on conditions in 'Negative_Review' and 'Positive_Review' columns. It uses pandas boolean indexing with comparison operators (`==`, `&`) to create boolean series and then uses the `sum()` method to count the `True` values. Execution time is measured and shown to be faster than the `apply` method.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/sw/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_7

LANGUAGE: python
CODE:
```
# without lambdas (using a mixture of notations to show you can use both)
start = time.time()
no_negative_reviews = sum(df.Negative_Review == "No Negative")
print("Number of No Negative reviews: " + str(no_negative_reviews))

no_positive_reviews = sum(df["Positive_Review"] == "No Positive")
print("Number of No Positive reviews: " + str(no_positive_reviews))

both_no_reviews = sum((df.Negative_Review == "No Negative") & (df.Positive_Review == "No Positive"))
print("Number of both No Negative and No Positive reviews: " + str(both_no_reviews))

end = time.time()
print("Sum took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Plotting Average Price by Variety - Pandas Python
DESCRIPTION: Calculates the mean 'Price' for each unique 'Variety' using `groupby()`, and then plots these average prices as a bar chart. This visualization helps show the difference in average price across different pumpkin varieties and supports the hypothesis that variety is a significant factor. Requires the pandas library and matplotlib for plotting.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/2-Regression/3-Linear/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

----------------------------------------

TITLE: Training and Evaluating Boosted Tree Model in R
DESCRIPTION: Defines a Boosted Tree model specification using the 'xgboost' engine with a specified number of trees, bundles it with a recipe into a workflow, fits the workflow to training data, and evaluates performance on test data. Requires 'cuisines_recipe', 'cuisines_train', 'cuisines_test', and an 'eval_metrics' object.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/solution/R/lesson_12-R.ipynb#_snippet_10

LANGUAGE: R
CODE:
```
# Make a boosted tree specification
boost_spec <- boost_tree(trees = 200) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# Bundle recipe and model specification into a workflow
boost_wf <- workflow() %>%
  add_recipe(cuisines_recipe) %>%
  add_model(boost_spec)

# Train a boosted tree model
boost_wf_fit <- boost_wf %>%
  fit(data = cuisines_train)


# Make predictions and Evaluate model performance
boost_wf_fit %>%
  augment(new_data = cuisines_test) %>%
  eval_metrics(truth = cuisine, estimate = .pred_class)
```

----------------------------------------

TITLE: Analyzing and Plotting Top Chinese Ingredients - Python
DESCRIPTION: Uses the `create_ingredient_df` function on the Chinese cuisine data to get ingredient counts. It then plots the top 10 ingredients from the resulting dataframe as a horizontal bar chart. Requires the function and the Chinese data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/README.md#_snippet_10

LANGUAGE: python
CODE:
```
chinese_ingredient_df = create_ingredient_df(chinese_df)
chinese_ingredient_df.head(10).plot.barh()
```

----------------------------------------

TITLE: Plotting Full Time Series Data (Python)
DESCRIPTION: Generates a line plot of the entire 'load' column from the `energy` DataFrame. It customizes the plot size, axis labels, and font sizes, then displays the plot using `matplotlib`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/1-Introduction/README.md#_snippet_2

LANGUAGE: python
CODE:
```
energy.plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)
plt.xlabel('timestamp', fontsize=12)
plt.ylabel('load', fontsize=12)
plt.show()
```

----------------------------------------

TITLE: Visualize Price vs Month Scatter Plot in R
DESCRIPTION: This snippet sets a light theme for plots and then generates a scatter plot using 'ggplot2'. It visualizes the relationship between the 'month' (on the x-axis) and the 'price' (on the y-axis) from the cleaned 'new_pumpkins' data frame, displaying each data point. Requires 'ggplot2' (part of 'tidyverse').
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_5

LANGUAGE: R
CODE:
```
theme_set(theme_light())

# Make a scatter plot of month and price
new_pumpkins %>%
  ggplot(mapping = aes(x = month, y = price)) +
  geom_point(size = 1.6)
```

----------------------------------------

TITLE: Clean and Transform Pumpkin Data in R
DESCRIPTION: This comprehensive snippet cleans and transforms the pumpkin data. It selects relevant columns, converts date strings to month numbers using 'lubridate', calculates an average price, filters rows to include only "bushel" packages, normalizes the price to a per-bushel equivalent, relocates the 'month' column, and displays the first 5 rows of the resulting 'new_pumpkins' data frame. Requires 'tidyverse' and 'lubridate'.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_4

LANGUAGE: R
CODE:
```
pumpkins <- pumpkins %>%
  select(variety, city_name, package, low_price, high_price, date)



pumpkins <- pumpkins %>%
  mutate(date = mdy(date),
         month = month(date)) %>%
  select(-date)



pumpkins <- pumpkins %>%
  mutate(price = (low_price + high_price)/2)


# Retain only pumpkins with the string "bushel"
new_pumpkins <- pumpkins %>%
  filter(str_detect(string = package, pattern = "bushel"))


# Normalize the pricing so that you show the pricing per bushel, not per 1 1/9 or 1/2 bushel
new_pumpkins <- new_pumpkins %>%
  mutate(price = case_when(
    str_detect(package, "1 1/9") ~ price/(1.1),
    str_detect(package, "1/2") ~ price*2,
    TRUE ~ price))

# Relocate column positions
new_pumpkins <- new_pumpkins %>%
  relocate(month, .before = variety)


# Display the first 5 rows
new_pumpkins %>%
  slice_head(n = 5)
```

----------------------------------------

TITLE: Visualize Data Distribution with Boxplots in Python
DESCRIPTION: Generates a series of boxplots for selected columns of a pandas DataFrame to visualize data distribution and identify outliers. Uses `matplotlib.pyplot` for figure management and `seaborn` for creating the boxplots. Requires a pre-loaded pandas DataFrame `df` containing the music data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/README.md#_snippet_0

LANGUAGE: python
CODE:
```
plt.figure(figsize=(20,20), dpi=200)

plt.subplot(4,3,1)
sns.boxplot(x = 'popularity', data = df)

plt.subplot(4,3,2)
sns.boxplot(x = 'acousticness', data = df)

plt.subplot(4,3,3)
sns.boxplot(x = 'energy', data = df)

plt.subplot(4,3,4)
sns.boxplot(x = 'instrumentalness', data = df)

plt.subplot(4,3,5)
sns.boxplot(x = 'liveness', data = df)

plt.subplot(4,3,6)
sns.boxplot(x = 'loudness', data = df)

plt.subplot(4,3,7)
sns.boxplot(x = 'speechiness', data = df)

plt.subplot(4,3,8)
sns.boxplot(x = 'tempo', data = df)

plt.subplot(4,3,9)
sns.boxplot(x = 'time_signature', data = df)

plt.subplot(4,3,10)
sns.boxplot(x = 'danceability', data = df)

plt.subplot(4,3,11)
sns.boxplot(x = 'length', data = df)

plt.subplot(4,3,12)
sns.boxplot(x = 'release_date', data = df)
```

----------------------------------------

TITLE: Loading Data and Initial Exploration in R
DESCRIPTION: Loads the core Tidyverse packages, reads the pumpkin data directly from a GitHub CSV file into a dataframe named 'pumpkins'. It then uses 'glimpse()' to get a concise summary of the data structure and 'slice_head()' to display the first 50 rows for initial inspection.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/solution/R/lesson_2-R.ipynb#_snippet_1

LANGUAGE: R
CODE:
```
library(tidyverse)

# Import the pumpkins data
pumpkins <- read_csv(file = "https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/data/US-pumpkins.csv")


# Get a glimpse and dimensions of the data
glimpse(pumpkins)


# Print the first 50 rows of the data set
pumpkins %>%
  slice_head(n =50)
```

----------------------------------------

TITLE: Plotting Specific Time Range Data (Python)
DESCRIPTION: Filters the `energy` DataFrame to select data points within the date range '2014-07-01' to '2014-07-07' using pandas time series indexing. It then plots this subset, applying similar formatting as the full plot, and displays the result.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/1-Introduction/README.md#_snippet_3

LANGUAGE: python
CODE:
```
energy['2014-07-01':'2014-07-07'].plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)
plt.xlabel('timestamp', fontsize=12)
plt.ylabel('load', fontsize=12)
plt.show()
```

----------------------------------------

TITLE: Creating Scatter Plot with Matplotlib in Python
DESCRIPTION: Generates a basic scatter plot using Matplotlib, visualizing the relationship between the calculated pumpkin 'Price' and the 'Month'. The `plt.show()` function displays the created plot.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/2-Regression/2-Data/README.md#_snippet_8

LANGUAGE: Python
CODE:
```
price = new_pumpkins.Price
month = new_pumpkins.Month
plt.scatter(price, month)
plt.show()
```

----------------------------------------

TITLE: Listing Flask App Python Dependencies
DESCRIPTION: Specifies the necessary Python packages for the Flask web application. This list is used by pip to install required libraries such as scikit-learn for the ML model, pandas and numpy for data handling, and Flask for the web framework.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/3-Web-App/1-Web-App/README.md#_snippet_0

LANGUAGE: Text
CODE:
```
scikit-learn
pandas
numpy
flask
```

----------------------------------------

TITLE: Defining Python Dependencies (requirements.txt)
DESCRIPTION: Specifies the Python packages required for the Flask web application to run. This includes libraries for machine learning (scikit-learn, pandas, numpy) and the Flask web framework, which are necessary for model loading, data handling, and serving the web application.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/3-Web-App/1-Web-App/README.md#_snippet_0

LANGUAGE: text
CODE:
```
scikit-learn
pandas
numpy
flask
```

----------------------------------------

TITLE: Initializing Q-Table (Numpy) - Python
DESCRIPTION: Initializes a 3D NumPy array `Q` representing the Q-Table. Its dimensions are `width` x `height` (representing states) and `len(actions)` (representing possible actions). All entries are initialized to `1.0 / len(actions)`, effectively setting an equal probability for all actions in every state, simulating a random walk. Requires NumPy and definitions for `width`, `height`, and `actions` (a list or similar structure of available actions).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/8-Reinforcement/1-QLearning/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

----------------------------------------

TITLE: Implementing Ingredient State Management and ONNX Inference - JavaScript
DESCRIPTION: Contains JavaScript code to manage the state of selected ingredients in an array based on checkbox changes, validate if any checkbox is checked, and define the asynchronous `startInference` function. This function loads the local 'model.onnx', prepares the input tensor from the ingredient state array, runs the model inference using `ort.InferenceSession`, and displays the predicted cuisine label using an alert box.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/README.md#_snippet_14

LANGUAGE: JavaScript
CODE:
```
<script>
    const ingredients = Array(380).fill(0);
    
    const checks = [...document.querySelectorAll('.checkbox')];
    
    checks.forEach(check => {
        check.addEventListener('change', function() {
            // toggle the state of the ingredient
            // based on the checkbox's value (1 or 0)
            ingredients[check.value] = check.checked ? 1 : 0;
        });
    });

    function testCheckboxes() {
        // validate if at least one checkbox is checked
        return checks.some(check => check.checked);
    }

    async function startInference() {

        let atLeastOneChecked = testCheckboxes()

        if (!atLeastOneChecked) {
            alert('Please select at least one ingredient.');
            return;
        }
        try {
            // create a new session and load the model.
            
            const session = await ort.InferenceSession.create('./model.onnx');

            const input = new ort.Tensor(new Float32Array(ingredients), [1, 380]);
            const feeds = { float_input: input };

            // feed inputs and run
            const results = await session.run(feeds);

            // read from results
            alert('You can enjoy ' + results.label.data[0] + ' cuisine today!')

        } catch (e) {
            console.log(`failed to inference ONNX model`);
            console.error(e);
        }
    }
               
</script>
```

----------------------------------------

TITLE: Plotting Joint Distribution of Popularity vs Danceability by Genre (KDE) Python
DESCRIPTION: Sets the seaborn theme and generates a joint plot comparing 'popularity' and 'danceability' for each of the top genres using Kernel Density Estimation (KDE) to visualize the distribution.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/notebook.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
sns.set_theme(style="ticks")

# Show the joint distribution using kernel density estimation
g = sns.jointplot(
    data=df,
    x="popularity", y="danceability", hue="artist_top_genre",
    kind="kde",
)
```

----------------------------------------

TITLE: Creating Train and Test DataFrames Python
DESCRIPTION: Creates separate pandas DataFrames, `train` and `test`, containing the 'load' data corresponding to the defined training and testing date ranges. Copies are made to prevent modifying the original DataFrame. The shapes of the resulting DataFrames are printed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/solution/notebook.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
train = energy.copy()[(energy.index >= train_start_dt) & (energy.index < test_start_dt)][['load']]
test = energy.copy()[energy.index >= test_start_dt][['load']]

print('Training data shape: ', train.shape)
print('Test data shape: ', test.shape)
```

----------------------------------------

TITLE: Loading and Cleaning Cuisine Data in R
DESCRIPTION: This snippet loads the raw cuisine dataset from a specified URL using 'read_csv'. It then uses 'dplyr' functions ('select' and 'mutate') to remove the 'id' column and common ingredients ('rice', 'garlic', 'ginger') and converts the 'cuisine' column into a factor variable. Finally, it displays the first few rows and the distribution of cuisines.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/solution/R/lesson_11-R.ipynb#_snippet_1

LANGUAGE: R
CODE:
```
# Load the original cuisines data
df <- read_csv(file = "https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/data/cuisines.csv")

# Drop id column, rice, garlic and ginger from our original data set
df_select <- df %>%
  select(-c(1, rice, garlic, ginger)) %>%
  # Encode cuisine column as categorical
  mutate(cuisine = factor(cuisine))

# Display new data set
df_select %>%
  slice_head(n = 5)

# Display distribution of cuisines
df_select %>% 
  count(cuisine) %>% 
  arrange(desc(n))
```

----------------------------------------

TITLE: Plotting Testing Predictions Matplotlib Python
DESCRIPTION: This code snippet generates a plot comparing the actual (`y_test`) and predicted (`y_test_pred`) values for the testing dataset using `matplotlib`. The x-axis represents the corresponding `test_timestamps`. This visualization is crucial for evaluating the model's generalization performance on unseen data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_22

LANGUAGE: python
CODE:
```
plt.figure(figsize=(10,3))
plt.plot(test_timestamps, y_test, color = 'red', linewidth=2.0, alpha = 0.6)
plt.plot(test_timestamps, y_test_pred, color = 'blue', linewidth=0.8)
plt.legend(['Actual','Predicted'])
plt.xlabel('Timestamp')
plt.show()
```

----------------------------------------

TITLE: Defining Recipe and Logistic Regression Model Spec in R
DESCRIPTION: Creates a data preprocessing recipe using the recipes package, specifying steps to order and integer encode 'item_size' and one-hot encode all other nominal variables except the outcome ('color'). Also defines a logistic regression model specification using the parsnip package, setting the engine to 'glm' and mode to 'classification'. Requires recipes and parsnip packages.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/R/lesson_4-R.ipynb#_snippet_9

LANGUAGE: R
CODE:
```
# Create a recipe that specifies preprocessing steps for modelling
pumpkins_recipe <- recipe(color ~ ., data = pumpkins_train) %>%
  step_mutate(item_size = ordered(item_size, levels = c('sml', 'med', 'med-lge', 'lge', 'xlge', 'jbo', 'exjbo'))) %>%
  step_integer(item_size, zero_based = F) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)

# Create a logistic model specification
log_reg <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")
```

----------------------------------------

TITLE: Select Feature Columns Pandas Python
DESCRIPTION: Identifies and selects the feature columns from the loaded dataset. It extracts all columns starting from the third column (index 2 onwards) using pandas' `iloc` and assigns them to the variable `X`, representing the input features for the machine learning model.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/README.md#_snippet_2

LANGUAGE: python
CODE:
```
X = data.iloc[:,2:]
X.head()
```

----------------------------------------

TITLE: Calculating Sentiment for Hotel Reviews - Python
DESCRIPTION: This snippet calculates sentiment scores for both negative and positive hotel reviews using a `calc_sentiment` function (assumed to be previously defined). It applies this function to the 'Negative_Review' and 'Positive_Review' columns of a pandas DataFrame `df`, storing results in new 'Negative_Sentiment' and 'Positive_Sentiment' columns. It also tracks and prints the time taken for the calculation.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_5

LANGUAGE: python
CODE:
```
print("Calculating sentiment columns for both positive and negative reviews")
start = time.time()
df["Negative_Sentiment"] = df.Negative_Review.apply(calc_sentiment)
df["Positive_Sentiment"] = df.Positive_Review.apply(calc_sentiment)
end = time.time()
print("Calculating sentiment took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Loading Diabetes Dataset - Python
DESCRIPTION: Loads the built-in Scikit-learn diabetes dataset, retrieving the feature matrix (X) and the target variable (y) directly. Prints the shape of the X matrix (number of samples, number of features) and the first sample to show the data structure.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/2-Regression/1-Tools/README.md#_snippet_1

LANGUAGE: python
CODE:
```
X, y = datasets.load_diabetes(return_X_y=True)
print(X.shape)
print(X[0])
```

----------------------------------------

TITLE: Inverse Scaling Training and Testing Predictions (Python)
DESCRIPTION: Applies the `inverse_transform` method of the fitted MinMaxScaler to the training and testing predictions. This converts the predictions back from the scaled range (0, 1) to the original data range.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_16

LANGUAGE: python
CODE:
```
# Scaling the predictions

y_train_pred = scaler.inverse_transform(y_train_pred)
y_test_pred = scaler.inverse_transform(y_test_pred)
```

----------------------------------------

TITLE: Loading Data from CSV Python
DESCRIPTION: This code loads the hotel reviews dataset from a CSV file into a pandas DataFrame. It uses the `time` library to measure and print the duration of the loading process.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/4-Hotel-Reviews-1/solution/notebook.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
# Load the hotel reviews from CSV
print("Loading data file now, this could take a while depending on file size")
start = time.time()
df = pd.read_csv('../../data/Hotel_Reviews.csv')
end = time.time()
print("Loading took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Implementing Q-Learning Training Loop (Python)
DESCRIPTION: Implements the core Q-learning algorithm training loop. It iterates for a fixed number of epochs, simulating episodes. In each step, it selects an action based on Q-values (using `probs` and random choice), updates the state, calculates the reward, and updates the Q-table using the standard Q-learning formula with decaying learning rate (alpha) and a fixed discount factor (gamma).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/assignment-solution.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
from IPython.display import clear_output

lpath = []

for epoch in range(10000):
    clear_output(wait=True)
    print(f"Epoch = {epoch}",end='')

    # Pick initial point
    s = state(m)
    
    # Start travelling
    n=0
    cum_reward = 0
    while True:
        x,y = s.board.human
        v = probs(Q[x,y])
        while True:
            a = random.choices(list(actions),weights=v)[0]
            dpos = actions[a]
            if s.board.is_valid(s.board.move_pos(s.board.human,dpos)):
                break 
        s.move(dpos)
        r = reward(s)
        if abs(r)==100: # end of game
            print(f" {n} steps",end='\r')
            lpath.append(n)
            break
        alpha = np.exp(-n / 3000)
        gamma = 0.5
        ai = action_idx[a]
        Q[x,y,ai] = (1 - alpha) * Q[x,y,ai] + alpha * (r + gamma * Q[x+dpos[0], y+dpos[1]].max())
        n+=1
```

----------------------------------------

TITLE: Plotting Learning Path - Matplotlib Python
DESCRIPTION: This snippet plots the variable `lpath` using Matplotlib. It is used to visualize the change in average path length over training iterations or epochs, providing insight into the agent's learning progress and identifying patterns like initial increases, decreases, or abrupt changes.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/notebook.ipynb#_snippet_14

LANGUAGE: Python
CODE:
```
plt.plot(lpath)
```

----------------------------------------

TITLE: Specify Multinomial Regression Model (Tidymodels/parsnip) - R
DESCRIPTION: Defines the specification for a Multinomial Regression model using `parsnip::multinom_reg()`. It sets the `penalty` hyperparameter, specifies the engine to use ('nnet') along with engine-specific arguments like `MaxNWts`, and sets the model mode to 'classification'. This object (`mr_spec`) represents the model setup before training.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/solution/R/lesson_11-R.ipynb#_snippet_4

LANGUAGE: R
CODE:
```
# Create a multinomial regression model specification
mr_spec <- multinom_reg(penalty = 1) %>%
  set_engine("nnet", MaxNWts = 2086) %>%
  set_mode("classification")

# Print model specification
mr_spec
```

----------------------------------------

TITLE: Visualize Elbow Method using WCSS Plot in Python
DESCRIPTION: Generates a line plot showing the relationship between the number of clusters (k) and the corresponding Within-Cluster Sum of Squares (WCSS). This plot helps identify the 'elbow' point, suggesting the optimal number of clusters where the rate of decrease in WCSS sharply changes. Uses `matplotlib.pyplot` and `seaborn`. Requires the `wcss` list generated from the previous step.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/README.md#_snippet_5

LANGUAGE: python
CODE:
```
plt.figure(figsize=(10,5))
sns.lineplot(x=range(1, 11), y=wcss, marker='o', color='red')
plt.title('Elbow')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()
```

----------------------------------------

TITLE: Visualizing Top Genres - Seaborn Python
DESCRIPTION: This code imports seaborn for enhanced plotting. It counts the occurrences of each 'artist_top_genre' and selects the top 5, then creates a bar plot using seaborn to visualize the frequency distribution of these top genres. Matplotlib functions are used to set figure size, rotate x-axis labels for readability, and set the plot title.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/README.md#_snippet_5

LANGUAGE: python
CODE:
```
import seaborn as sns

top = df['artist_top_genre'].value_counts()
plt.figure(figsize=(10,7))
sns.barplot(x=top[:5].index,y=top[:5].values)
plt.xticks(rotation=45)
plt.title('Top genres',color = 'blue')
```

----------------------------------------

TITLE: Load Data with Pandas Python
DESCRIPTION: Loads the cleaned cuisine dataset from a CSV file into a pandas DataFrame. The `head()` method is then called to display the first few rows of the DataFrame, providing a preview of the data structure.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/4-Classification/4-Applied/README.md#_snippet_1

LANGUAGE: python
CODE:
```
data = pd.read_csv('../data/cleaned_cuisines.csv')
data.head()
```

----------------------------------------

TITLE: Importing Scikit-learn Libraries for Polynomial Regression - Python
DESCRIPTION: Imports essential modules from scikit-learn, including `PolynomialFeatures` to generate polynomial and interaction features from input data and `make_pipeline` to create a streamlined workflow combining multiple processing steps (like feature generation and regression).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/notebook.ipynb#_snippet_13

LANGUAGE: python
CODE:
```
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
```

----------------------------------------

TITLE: Filtering Data and Visualizing Genres - Pandas/Seaborn - Python
DESCRIPTION: Filters the DataFrame `df` to retain only rows where the 'artist_top_genre' is one of 'afro dancehall', 'afropop', or 'nigerian pop', and the 'popularity' is greater than 0. It then calculates the value counts for the filtered genres and creates a bar plot using seaborn to visualize the distribution of these top genres.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/solution/tester.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
df = df[(df['artist_top_genre'] == 'afro dancehall') | (df['artist_top_genre'] == 'afropop') | (df['artist_top_genre'] == 'nigerian pop')]
df = df[(df['popularity'] > 0)]
top = df['artist_top_genre'].value_counts()
plt.figure(figsize=(10,7))
sns.barplot(x=top.index,y=top.values)
plt.xticks(rotation=45)
plt.title('Top genres',color = 'blue')
```

----------------------------------------

TITLE: Train Polynomial Regression Model Pipeline Python Scikit-learn
DESCRIPTION: This snippet imports modules for polynomial features and pipelines. It creates a Scikit-learn pipeline that first transforms the input data by adding polynomial features up to degree 2 and then trains a linear regression model on the transformed data. The pipeline is then trained using the training data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/2-Regression/3-Linear/README.md#_snippet_12

LANGUAGE: python
CODE:
```
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

----------------------------------------

TITLE: Create CartPole Environment and Explore Spaces - Python
DESCRIPTION: Initializes the CartPole-v1 environment provided by OpenAI Gym. It then prints the action space (discrete actions like pushing left or right), the observation space (continuous values representing cart position, velocity, pole angle, and angular velocity), and a sample action from the action space.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/solution/notebook.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
env = gym.make("CartPole-v1")
print(env.action_space)
print(env.observation_space)
print(env.action_space.sample())
```

----------------------------------------

TITLE: Visualizing Top 10 Genres (Including Missing) in R
DESCRIPTION: Sets a theme for ggplot and creates a bar plot showing the counts of the top 10 most frequent genres, including the 'Missing' category. This visualizes the distribution of genres in the dataset.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb#_snippet_7

LANGUAGE: R
CODE:
```
# Change the default gray theme
theme_set(theme_light())

# Visualize popular genres
top_genres %>%
  slice(1:10) %>%
  ggplot(mapping = aes(x = artist_top_genre, y = n,
                       fill = artist_top_genre)) +
  geom_col(alpha = 0.8) +
  paletteer::scale_fill_paletteer_d("rcartocolor::Vivid") +
  ggtitle("Top genres") +
  theme(plot.title = element_text(hjust = 0.5),
        # Rotates the X markers (so we can read them)
    axis.text.x = element_text(angle = 90))
```

----------------------------------------

TITLE: Implementing ONNX Model Inference Logic in JavaScript
DESCRIPTION: Contains the core JavaScript logic for the web application. It initializes an array to store ingredient selections based on checkbox states, handles checkbox change events, validates if at least one ingredient is selected, and implements the asynchronous `startInference` function to load the 'model.onnx', create an ONNX Tensor input, run the inference session, and display the predicted cuisine label using an alert box. Includes error handling.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/sw/4-Classification/4-Applied/README.md#_snippet_3

LANGUAGE: javascript
CODE:
```
<script>
        const ingredients = Array(380).fill(0);
        
        const checks = [...document.querySelectorAll('.checkbox')];
        
        checks.forEach(check => {
            check.addEventListener('change', function() {
                // toggle the state of the ingredient
                // based on the checkbox's value (1 or 0)
                ingredients[check.value] = check.checked ? 1 : 0;
            });
        });

        function testCheckboxes() {
            // validate if at least one checkbox is checked
            return checks.some(check => check.checked);
        }

        async function startInference() {

            let atLeastOneChecked = testCheckboxes()

            if (!atLeastOneChecked) {
                alert('Please select at least one ingredient.');
                return;
            }
            try {
                // create a new session and load the model.
                
                const session = await ort.InferenceSession.create('./model.onnx');

                const input = new ort.Tensor(new Float32Array(ingredients), [1, 380]);
                const feeds = { float_input: input };

                // feed inputs and run
                const results = await session.run(feeds);

                // read from results
                alert('You can enjoy ' + results.label.data[0] + ' cuisine today!')

            } catch (e) {
                console.log(`failed to inference ONNX model`);
                console.error(e);
            }
        }
               
    </script>
```

----------------------------------------

TITLE: Loading and Cleaning Pumpkin Data in R
DESCRIPTION: Loads the US pumpkin dataset from a remote CSV file, cleans column names, selects specific columns (`city_name`, `package`, `variety`, `origin`, `item_size`, `color`), removes rows with missing values (`drop_na`), and converts the `color` column to a factor. It then displays the first 5 rows of the resulting dataframe.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/R/lesson_4-R.ipynb#_snippet_1

LANGUAGE: R
CODE:
```
# Load the core tidyverse packages
library(tidyverse)

# Import the data and clean column names
pumpkins <- read_csv(file = "https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/data/US-pumpkins.csv") %>%
  clean_names()

# Select desired columns
pumpkins_select <- pumpkins %>%
  select(c(city_name, package, variety, origin, item_size, color)) 

# Drop rows containing missing values and encode color as factor (category)
pumpkins_select <- pumpkins_select %>%
  drop_na() %>%
  mutate(color = factor(color))

# View the first few rows
pumpkins_select %>%
  slice_head(n = 5)
```

----------------------------------------

TITLE: Visualizing Price vs Day of Year Relationship - Python
DESCRIPTION: Generates a scatter plot using matplotlib (via pandas plotting interface) to visualize the relationship between the processed pumpkin 'Price' and the 'DayOfYear' of sale. Provides a more granular view of price trends throughout the available season compared to 'Month'.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/notebook.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
new_pumpkins.plot.scatter('DayOfYear','Price')
```

----------------------------------------

TITLE: Visualizing Price vs. DayOfYear Scatter Plot (Python)
DESCRIPTION: Generates a scatter plot using the 'DayOfYear' column on the x-axis and the 'Price' column on the y-axis from the processed `new_pumpkins` DataFrame. This plot visualizes the relationship between pumpkin price and the day of the year, providing a more granular view than the month plot.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/notebook.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
plt.scatter('DayOfYear','Price',data=new_pumpkins)
```

----------------------------------------

TITLE: Visualizing Pumpkin Variety by Color with ggplot2 (R)
DESCRIPTION: Generates a bar plot using `ggplot2` to visualize the distribution of pumpkin varieties (`variety`) based on their `color`. It uses `geom_bar` with `position = "dodge"` to show counts side-by-side for each color within each variety, applies a custom color palette, and sets appropriate labels.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/R/lesson_4-R.ipynb#_snippet_4

LANGUAGE: R
CODE:
```
# Specify colors for each value of the hue variable
palette <- c(ORANGE = "orange", WHITE = "wheat")

# Create the bar plot
ggplot(pumpkins_select, aes(y = variety, fill = color)) +
  geom_bar(position = "dodge") +
  scale_fill_manual(values = palette) +
  labs(y = "Variety", fill = "Color") +
  theme_minimal()
```

----------------------------------------

TITLE: Visualizing Relationships with Seaborn Catplot (Python)
DESCRIPTION: This snippet defines a color palette, updates the 'Item Size' column in the original pumpkins DataFrame with its encoded numerical values, and then uses sns.catplot to create a box plot. It visualizes the distribution of encoded 'Item Size' (x) against 'Color' (y), separated by 'Variety' (row), to show relationships between these variables. It also customizes plot labels and limits.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/README.md#_snippet_8

LANGUAGE: python
CODE:
```
palette = {
'ORANGE': 'orange',
'WHITE': 'wheat',
}
pumpkins['Item Size'] = encoded_pumpkins['ord__Item Size']

g = sns.catplot(
    data=pumpkins,
    x="Item Size", y="Color", row='Variety',
    kind="box", orient="h",
    sharex=False, margin_titles=True,
    height=1.8, aspect=4, palette=palette,
)
g.set(xlabel="Item Size", ylabel="").set(xlim=(0,6))
g.set_titles(row_template="{row_name}")
```

----------------------------------------

TITLE: Visualizing Item Size vs Color by Variety with Box Plots - Python
DESCRIPTION: This code visualizes the relationship between encoded 'Item Size' and 'Color', broken down by 'Variety', using seaborn box plots (`sns.catplot` with `kind='box'`). It requires matplotlib for plot customization and uses the encoded 'Item Size' from the `encoded_pumpkins` DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/notebook.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
}
# We need the encoded Item Size column to use it as the x-axis values in the plot
pumpkins['Item Size'] = encoded_pumpkins['ord__Item Size']

g = sns.catplot(
    data=pumpkins,
    x="Item Size", y="Color", row='Variety',
    kind="box", orient="h",
    sharex=False, margin_titles=True,
    height=1.8, aspect=4, palette=palette,
)
# Defining axis labels
g.set(xlabel="Item Size", ylabel="").set(xlim=(0,6))
g.set_titles(row_template="{row_name}")
```

----------------------------------------

TITLE: Plotting Raw Time Series Data (Python)
DESCRIPTION: Generates a plot of the loaded 'load' time series data using pandas' built-in plotting functionality linked to matplotlib. It visualizes the raw data over time.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
energy.plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)
plt.xlabel('timestamp', fontsize=12)
plt.ylabel('load', fontsize=12)
plt.show()
```

----------------------------------------

TITLE: Loading CSV Data into Pandas DataFrame Python
DESCRIPTION: Loads the Nigerian songs dataset from a local CSV file into a pandas DataFrame and displays the first 5 rows to inspect the data structure.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/notebook.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
df = pd.read_csv("../../data/nigerian-songs.csv")
df.head()
```

----------------------------------------

TITLE: Saving Processed Hotel Reviews - Pandas Python
DESCRIPTION: This snippet drops several intermediate columns from a pandas DataFrame (`df`) that are no longer needed after processing. It then saves the modified DataFrame to a new CSV file named 'Hotel_Reviews_Filtered.csv'. The `index=False` argument prevents pandas from writing the DataFrame index as a column in the CSV. Requires a pandas DataFrame with the listed columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_5

LANGUAGE: Python
CODE:
```
df.drop(["Review_Total_Negative_Word_Counts", "Review_Total_Positive_Word_Counts", "days_since_review", "Total_Number_of_Reviews_Reviewer_Has_Given"], axis = 1, inplace=True)

# Saving new data file with calculated columns
print("Saving results to Hotel_Reviews_Filtered.csv")
df.to_csv(r'../data/Hotel_Reviews_Filtered.csv', index = False)
```

----------------------------------------

TITLE: Implementing Q-Learning Training Loop (Python)
DESCRIPTION: Contains the main training loop running for a fixed number of `epochs`. It simulates episodes, chooses actions based on epsilon-greedy strategy, updates the Q-table using the Q-learning formula, tracks cumulative rewards, and periodically prints progress, saving the Q-table with the highest average reward observed so far. Requires `env`, `discretize`, `probs`, `actions`, `Q`, `alpha`, `gamma`, `epsilon`, `Qmax`, `cum_rewards`, `rewards`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/8-Reinforcement/2-Gym/README.md#_snippet_2

LANGUAGE: python
CODE:
```
def probs(v,eps=1e-4):
    v = v-v.min()+eps
    v = v/v.sum()
    return v

Qmax = 0
cum_rewards = []
rewards = []
for epoch in range(100000):
    obs = env.reset()
    done = False
    cum_reward=0
    # == do the simulation ==
    while not done:
        s = discretize(obs)
        if random.random()<epsilon:
            # exploitation - chose the action according to Q-Table probabilities
            v = probs(np.array(qvalues(s)))
            a = random.choices(actions,weights=v)[0]
        else:
            # exploration - randomly chose the action
            a = np.random.randint(env.action_space.n)

        obs, rew, done, info = env.step(a)
        cum_reward+=rew
        ns = discretize(obs)
        Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
    cum_rewards.append(cum_reward)
    rewards.append(cum_reward)
    # == Periodically print results and calculate average reward ==
    if epoch%5000==0:
        print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
        if np.average(cum_rewards) > Qmax:
            Qmax = np.average(cum_rewards)
            Qbest = Q
        cum_rewards=[]
```

----------------------------------------

TITLE: Implementing Q-Learning Training Loop Python
DESCRIPTION: Executes a training loop for 100,000 epochs. Inside each epoch, it simulates an episode, discretizes the state, selects an action based on an epsilon-greedy strategy (exploring randomly or exploiting Q-values), takes a step in the environment, updates the Q-value for the observed transition using the Q-Learning formula, and accumulates rewards. Periodically, it prints the average cumulative reward and saves the Q-table if it represents the best performance seen so far.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/8-Reinforcement/2-Gym/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
def probs(v,eps=1e-4):
    v = v-v.min()+eps
    v = v/v.sum()
    return v

Qmax = 0
cum_rewards = []
rewards = []
for epoch in range(100000):
    obs = env.reset()
    done = False
    cum_reward=0
    # == do the simulation ==
    while not done:
        s = discretize(obs)
        if random.random()<epsilon:
            # exploitation - chose the action according to Q-Table probabilities
            v = probs(np.array(qvalues(s)))
            a = random.choices(actions,weights=v)[0]
        else:
            # exploration - randomly chose the action
            a = np.random.randint(env.action_space.n)

        obs, rew, done, info = env.step(a);
        cum_reward+=rew
        ns = discretize(obs);
        Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
    cum_rewards.append(cum_reward)
    rewards.append(cum_reward)
    # == Periodically print results and calculate average reward ==
    if epoch%5000==0:
        print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
        if np.average(cum_rewards) > Qmax:
            Qmax = np.average(cum_rewards)
            Qbest = Q
        cum_rewards=[]
```

----------------------------------------

TITLE: Calculate One-Step Forecast MAPE in Python
DESCRIPTION: This snippet calculates and prints the Mean Absolute Percentage Error (MAPE) specifically for the one-step-ahead predictions (`t+1`) in the evaluation DataFrame. This provides a single metric for the immediate forecast accuracy.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/7-TimeSeries/2-ARIMA/README.md#_snippet_6

LANGUAGE: python
CODE:
```
print('One step forecast MAPE: ', (mape(eval_df[eval_df['h'] == 't+1']['prediction'], eval_df[eval_df['h'] == 't+1']['actual']))*100, '%')
```

----------------------------------------

TITLE: Calculating Total Reviews Per Hotel (Pandas) - Python
DESCRIPTION: This code calculates the total number of reviews for each hotel present in the dataset. It creates a new DataFrame by dropping irrelevant columns, groups the data by 'Hotel_Name', uses '.transform('count')' to add a column with the review count for each hotel, removes duplicate hotel rows, and displays the result.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_5

LANGUAGE: python
CODE:
```
# First create a new dataframe based on the old one, removing the uneeded columns
hotel_freq_df = df.drop(["Hotel_Address", "Additional_Number_of_Scoring", "Review_Date", "Average_Score", "Reviewer_Nationality", "Negative_Review", "Review_Total_Negative_Word_Counts", "Positive_Review", "Review_Total_Positive_Word_Counts", "Total_Number_of_Reviews_Reviewer_Has_Given", "Reviewer_Score", "Tags", "days_since_review", "lat", "lng"], axis = 1)

# Group the rows by Hotel_Name, count them and put the result in a new column Total_Reviews_Found
hotel_freq_df['Total_Reviews_Found'] = hotel_freq_df.groupby('Hotel_Name').transform('count')

# Get rid of all the duplicated rows
hotel_freq_df = hotel_freq_df.drop_duplicates(subset = ["Hotel_Name"])
display(hotel_freq_df)
```

----------------------------------------

TITLE: Calculating and Displaying Average Review Scores in Pandas
DESCRIPTION: This snippet calculates a new average review score per hotel using `groupby` and `transform`, computes the difference from an existing score, removes duplicate hotel entries, sorts the resulting dataframe by the difference, and displays the relevant columns. It requires a pandas DataFrame `df` with columns `Hotel_Name`, `Reviewer_Score`, and `Average_Score`, and a function `get_difference_review_avg`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_0

LANGUAGE: python
CODE:
```
df['Calc_Average_Score'] = round(df.groupby('Hotel_Name').Reviewer_Score.transform('mean'), 1)

df["Average_Score_Difference"] = df.apply(get_difference_review_avg, axis = 1)

review_scores_df = df.drop_duplicates(subset = ["Hotel_Name"])

review_scores_df = review_scores_df.sort_values(by=["Average_Score_Difference"])

display(review_scores_df[["Average_Score_Difference", "Average_Score", "Calc_Average_Score", "Hotel_Name"]])
```

----------------------------------------

TITLE: Evaluate Linear Model Performance (R)
DESCRIPTION: Uses the `metrics()` function from the yardstick package to compute performance metrics for the linear regression model. It compares the actual `price` values (`truth`) in the `lm_results` data frame against the predicted values (`estimate = .pred`).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_15

LANGUAGE: R
CODE:
```
# Evaluate performance of linear regression
metrics(data = lm_results,
        truth = price,
        estimate = .pred)
```

----------------------------------------

TITLE: Analyzing Hotel Review Counts Python
DESCRIPTION: This snippet calculates the number of reviews per hotel. It creates a new DataFrame by dropping columns, groups by 'Hotel_Name' to count reviews, adds the count to 'Total_Reviews_Found', removes duplicate hotel rows, and prints the resulting DataFrame and its shape.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/4-Hotel-Reviews-1/solution/notebook.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
# How many reviews are there per hotel (frequency count of hotel) and do the results match the value in `Total_Number_of_Reviews`?
# First create a new dataframe based on the old one, removing the uneeded columns
hotel_freq_df = df.drop(["Hotel_Address", "Additional_Number_of_Scoring", "Review_Date", "Average_Score", "Reviewer_Nationality", "Negative_Review", "Review_Total_Negative_Word_Counts", "Positive_Review", "Review_Total_Positive_Word_Counts", "Total_Number_of_Reviews_Reviewer_Has_Given", "Reviewer_Score", "Tags", "days_since_review", "lat", "lng"], axis = 1)
# Group the rows by Hotel_Name, count them and put the result in a new column Total_Reviews_Found
hotel_freq_df['Total_Reviews_Found'] = hotel_freq_df.groupby('Hotel_Name').transform('count')
# Get rid of all the duplicated rows
hotel_freq_df = hotel_freq_df.drop_duplicates(subset = ["Hotel_Name"])
print()
print(hotel_freq_df.to_string())
print(str(hotel_freq_df.shape))
```

----------------------------------------

TITLE: Calculating Linear Regression Model R-squared Score - Python
DESCRIPTION: This snippet calculates the coefficient of determination (R-squared) for the trained `lin_reg` model using the `score` method on the training data (`X_train`, `y_train`). The R-squared value, printed to the console, indicates the proportion of the variance in the dependent variable that is predictable from the independent variable(s).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/2-Regression/3-Linear/README.md#_snippet_10

LANGUAGE: python
CODE:
```
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```

----------------------------------------

TITLE: Calculating Determination Coefficient (R-squared) in Python
DESCRIPTION: Calculates the coefficient of determination (R-squared score) for the trained linear regression model using the `score()` method on the training data (`X_train`, `y_train`). This score indicates the proportion of the variance in the dependent variable that is predictable from the independent variable(s), ranging from 0 to 1.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/2-Regression/3-Linear/README.md#_snippet_10

LANGUAGE: python
CODE:
```
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```

----------------------------------------

TITLE: Analyzing and Visualizing Pie Type Pumpkin Prices - Python
DESCRIPTION: Filters the DataFrame to isolate data for 'PIE TYPE' pumpkins. Calculates and prints the correlation coefficient between 'DayOfYear' and 'Price' for this specific variety. Generates a scatter plot to visualize the relationship between 'DayOfYear' and 'Price' for 'PIE TYPE' pumpkins.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/notebook.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
print(pie_pumpkins['DayOfYear'].corr(pie_pumpkins['Price']))
pie_pumpkins.plot.scatter('DayOfYear','Price')
```

----------------------------------------

TITLE: Training and Evaluating RBF SVM Model in R
DESCRIPTION: Defines a specification for an RBF SVM using `svm_rbf`, setting the engine to `kernlab` and mode to `classification`. It bundles this specification with the `cuisines_recipe` into a workflow. The workflow is then fitted to the training data. Finally, it uses the fitted model to make predictions on the test data and calculates the same set of evaluation metrics as the linear SVC. Requires `tidymodels` and `kernlab` packages.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/solution/R/lesson_12-R.ipynb#_snippet_6

LANGUAGE: R
CODE:
```
set.seed(2056)

# Make an RBF SVM specification
svm_rbf_spec <- svm_rbf() %>%
  set_engine("kernlab") %>%
  set_mode("classification")

# Bundle specification and recipe into a worklow
svm_rbf_wf <- workflow() %>%
  add_recipe(cuisines_recipe) %>%
  add_model(svm_rbf_spec)


# Train an RBF model
svm_rbf_fit <- svm_rbf_wf %>%
  fit(data = cuisines_train)


# Make predictions and Evaluate model performance
svm_rbf_fit %>%
  augment(new_data = cuisines_test) %>%
  eval_metrics(truth = cuisine, estimate = .pred_class)
```

----------------------------------------

TITLE: Plotting Data & Regression Line - Matplotlib Python
DESCRIPTION: This code uses Matplotlib to create a scatter plot of the actual test data (`X_test` vs `y_test`) using black markers. It then overlays the learned linear regression line by plotting the test features against the predicted values (`X_test` vs `y_pred`) in blue. Labels and a title are added for clarity, and the plot is displayed. Requires Matplotlib.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/README.md#_snippet_7

LANGUAGE: python
CODE:
```
plt.scatter(X_test, y_test,  color='black')
plt.plot(X_test, y_pred, color='blue', linewidth=3)
plt.xlabel('Scaled BMIs')
plt.ylabel('Disease Progression')
plt.title('A Graph Plot Showing Diabetes Progression Against BMI')
plt.show()
```

----------------------------------------

TITLE: Plotting Test Data and Regression Line (Python)
DESCRIPTION: This snippet visualizes the performance of the linear regression model by plotting the test data points and the learned regression line. It creates a scatter plot of the actual test features (`X_test`) and targets (`y_test`) and overlays a line plot showing the predicted prices (`pred`) against the test features (`X_test`). Requires matplotlib (`plt`).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/de/2-Regression/3-Linear/README.md#_snippet_11

LANGUAGE: python
CODE:
```
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

----------------------------------------

TITLE: Importing Required Libraries - Python
DESCRIPTION: This snippet imports the necessary libraries for data manipulation (pandas), timing operations (time), and abstract syntax trees (ast).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/1-notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import pandas as pd
import time
import ast
```

----------------------------------------

TITLE: Mapping Sentiment Polarity to Response Text - Python
DESCRIPTION: This Python snippet demonstrates how to determine a conversational text response based on the sentiment polarity score of user input. It uses conditional logic (if/elif) to map different ranges of the TextBlob object's polarity attribute (from -1 to +1) to specific predefined strings, aiming to make the bot's reaction match the perceived sentiment.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/2-Tasks/README.md#_snippet_1

LANGUAGE: python
CODE:
```
if user_input_blob.polarity <= -0.5:
  response = "Oh dear, that sounds bad. "
elif user_input_blob.polarity <= 0:
  response = "Hmm, that's not great. "
elif user_input_blob.polarity <= 0.5:
  response = "Well, that sounds positive. "
elif user_input_blob.polarity <= 1:
  response = "Wow, that sounds great. "
```

----------------------------------------

TITLE: Importing Libraries and Loading Data - Pandas/Matplotlib - Python
DESCRIPTION: Imports necessary Python libraries including matplotlib for plotting, pandas for data manipulation, seaborn for advanced visualization, and numpy for numerical operations. It then loads the 'nigerian-songs.csv' file into a pandas DataFrame and displays the first 5 rows to verify successful loading.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/solution/tester.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import numpy as np

df = pd.read_csv("../../data/nigerian-songs.csv")
df.head()
```

----------------------------------------

TITLE: Importing Scikit-learn Modules for Linear Regression (Python)
DESCRIPTION: This snippet imports the necessary components from the Scikit-learn library to perform a simple linear regression task. It includes `LinearRegression` for the model algorithm, `mean_squared_error` for evaluating the model's performance, and `train_test_split` for dividing the dataset.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/de/2-Regression/3-Linear/README.md#_snippet_5

LANGUAGE: python
CODE:
```
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

----------------------------------------

TITLE: Importing Scikit-learn Modules for Regression - Python
DESCRIPTION: Imports the `LinearRegression` model class, the `mean_squared_error` metric function, and the `train_test_split` utility function from the scikit-learn library. These modules are essential prerequisites for setting up, training, and evaluating linear regression models.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/2-Regression/3-Linear/README.md#_snippet_5

LANGUAGE: Python
CODE:
```
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

----------------------------------------

TITLE: Cleaning and Selecting Data with Pandas (Python)
DESCRIPTION: This snippet selects specific columns ('City Name', 'Package', 'Variety', 'Origin', 'Item Size', 'Color') from the full_pumpkins DataFrame, assigns the result to pumpkins, and then removes rows with any null values using dropna. It prepares a subset of the data for analysis.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/README.md#_snippet_0

LANGUAGE: python
CODE:
```
columns_to_select = ['City Name','Package','Variety', 'Origin','Item Size', 'Color']
pumpkins = full_pumpkins.loc[:, columns_to_select]

pumpkins.dropna(inplace=True)
```

----------------------------------------

TITLE: Visualizing Correlation Heatmap (Pandas) - Python
DESCRIPTION: This snippet calculates the correlation matrix for the `poly_pumpkins` DataFrame and visualizes it as a heatmap using Pandas styling with the 'coolwarm' colormap. The heatmap shows the pairwise correlations between all selected features and the target variable ('Price').
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/translations/README.ko.md#_snippet_10

LANGUAGE: Python
CODE:
```
corr = poly_pumpkins.corr()
corr.style.background_gradient(cmap='coolwarm')
```

----------------------------------------

TITLE: Normalizing Price Based on Package Size with dplyr::case_when in R
DESCRIPTION: Uses 'dplyr::mutate()' with 'dplyr::case_when()' to conditionally modify the 'Price' column in the 'new_pumpkins' dataframe. If the 'Package' column contains "1 1/9", the price is divided by 1 + 1/9. If it contains "1/2", the price is divided by 1/2. Otherwise (for '1 bushel'), the price remains unchanged. This standardizes the price to represent the cost per standard bushel unit.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/solution/R/lesson_2-R.ipynb#_snippet_10

LANGUAGE: R
CODE:
```
# Convert the price if the Package contains fractional bushel values
new_pumpkins <- new_pumpkins %>%
  mutate(Price = case_when(
    str_detect(Package, "1 1/9") ~ Price/(1 + 1/9),
    str_detect(Package, "1/2") ~ Price/(1/2),
    TRUE ~ Price))
```

----------------------------------------

TITLE: Training and Evaluating KNN Model in R
DESCRIPTION: Bundles a pre-defined recipe and KNN model specification into a workflow, fits the workflow to the training data, and then evaluates its performance on the test data using classification metrics. Requires 'cuisines_recipe', 'knn_spec', 'cuisines_train', 'cuisines_test', and an 'eval_metrics' object to be defined previously.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/solution/R/lesson_12-R.ipynb#_snippet_8

LANGUAGE: R
CODE:
```
knn_wf <- workflow() %>%
  add_recipe(cuisines_recipe) %>%
  add_model(knn_spec)

# Train a boosted tree model
knn_wf_fit <- knn_wf %>%
  fit(data = cuisines_train)


# Make predictions and Evaluate model performance
knn_wf_fit %>%
  augment(new_data = cuisines_test) %>%
  eval_metrics(truth = cuisine, estimate = .pred_class)
```

----------------------------------------

TITLE: Importing Scikit-learn ML Libraries - Python
DESCRIPTION: Imports various classifier models (KNeighbors, LogisticRegression, SVC, RandomForest, AdaBoost), model selection tools (train_test_split, cross_val_score), metric functions (accuracy_score, precision_score, confusion_matrix, classification_report, precision_recall_curve), and the numpy library for numerical operations. These are essential for building, training, evaluating, and reporting on classification models.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
import numpy as np
```

----------------------------------------

TITLE: Importing ML Libraries Python
DESCRIPTION: This snippet imports various classes and functions necessary for machine learning tasks from scikit-learn and numpy. It includes different classifier models, data splitting utilities, and evaluation metrics, providing the building blocks for defining, training, and assessing models.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/solution/notebook.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
import numpy as np
```

----------------------------------------

TITLE: Selecting and Renaming UFO Data Columns Python
DESCRIPTION: This snippet selects specific columns ('duration (seconds)', 'country', 'latitude', 'longitude') from the original DataFrame, renames them to 'Seconds', 'Country', 'Latitude', 'Longitude', and creates a new DataFrame. It then prints the unique values in the 'Country' column, along with a comment showing a potential mapping used later for encoding. Requires the pandas DataFrame created in the previous step.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/3-Web-App/1-Web-App/solution/notebook.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
ufos = pd.DataFrame({'Seconds': ufos['duration (seconds)'], 'Country': ufos['country'],'Latitude': ufos['latitude'],'Longitude': ufos['longitude']})

ufos.Country.unique()

# 0 au, 1 ca, 2 de, 3 gb, 4 us
```

----------------------------------------

TITLE: Printing DataFrame Shape (Pandas) - Python
DESCRIPTION: This snippet prints the dimensions (number of rows and columns) of a Pandas DataFrame using the '.shape' attribute. The shape is returned as a tuple, which is then converted to a string for printing alongside a descriptive message.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_1

LANGUAGE: python
CODE:
```
print("The shape of the data (rows, cols) is " + str(df.shape))
```

----------------------------------------

TITLE: Installing Python Dependencies (Bash)
DESCRIPTION: Installs all Python packages listed in the `requirements.txt` file within the current environment. This command uses `pip`, the standard Python package installer, ensuring all necessary libraries are available for the application.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/3-Web-App/1-Web-App/README.md#_snippet_2

LANGUAGE: bash
CODE:
```
pip install -r requirements.txt
```

----------------------------------------

TITLE: Installing Python Dependencies with Pip (Bash)
DESCRIPTION: Executes the `pip install` command with the `-r` flag to read and install all the libraries listed in the `requirements.txt` file. This command sets up the necessary Python environment for the Flask application to run.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/3-Web-App/1-Web-App/README.md#_snippet_2

LANGUAGE: bash
CODE:
```
pip install -r requirements.txt
```

----------------------------------------

TITLE: Observe Discrete States During Simulation - Python
DESCRIPTION: Runs a simulation loop similar to the previous ones but demonstrates printing the discretized state of the environment observation at each step using one of the defined discretization functions (`discretize` is uncommented here).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/solution/notebook.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
env.reset()

done = False
while not done:
   #env.render()
   obs, rew, done, info = env.step(env.action_space.sample())
   #print(discretize_bins(obs))
   print(discretize(obs))
env.close()
```

----------------------------------------

TITLE: Perform Walk-Forward Validation with SARIMAX in Python
DESCRIPTION: This code implements walk-forward validation using a sliding window approach. It initializes a training history window of a fixed size (720 hours), iterates through the test data, fits a new SARIMAX model (`order=(2, 1, 0)`, `seasonal_order=(1, 1, 0, 24)`) using the current history window, forecasts `HORIZON` steps ahead, stores the predictions, updates the history by adding the actual observation and removing the oldest, and prints the current timestamp and prediction/actual values. The `%%time` command is used to measure the execution time in an IPython/Jupyter environment.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/README.md#_snippet_12

LANGUAGE: python
CODE:
```
%%time
training_window = 720 # dedicate 30 days (720 hours) for training

train_ts = train['load']
test_ts = test_shifted

history = [x for x in train_ts]
history = history[(-training_window):]

predictions = list()

order = (2, 1, 0)
seasonal_order = (1, 1, 0, 24)

for t in range(test_ts.shape[0]):
    model = SARIMAX(endog=history, order=order, seasonal_order=seasonal_order)
    model_fit = model.fit()
    yhat = model_fit.forecast(steps = HORIZON)
    predictions.append(yhat)
    obs = list(test_ts.iloc[t])
    # move the training window
    history.append(obs[0])
    history.pop(0)
    print(test_ts.index[t])
    print(t+1, ': predicted =', yhat, 'expected =', obs)
```

----------------------------------------

TITLE: Extracting Tag Information - Python
DESCRIPTION: This snippet processes the 'Tags' column to create new binary columns for specific important tags like 'Leisure trip', 'Couple', etc. It uses lambda functions with `apply` to check for the presence of each tag string.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/1-notebook.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
# Process the Tags into new columns
# The file Hotel_Reviews_Tags.py, identifies the most important tags
# Leisure trip, Couple, Solo traveler, Business trip, Group combined with Travelers with friends, 
# Family with young children, Family with older children, With a pet
df["Leisure_trip"] = df.Tags.apply(lambda tag: 1 if "Leisure trip" in tag else 0)
df["Couple"] = df.Tags.apply(lambda tag: 1 if "Couple" in tag else 0)
df["Solo_traveler"] = df.Tags.apply(lambda tag: 1 if "Solo traveler" in tag else 0)
df["Business_trip"] = df.Tags.apply(lambda tag: 1 if "Business trip" in tag else 0)
df["Group"] = df.Tags.apply(lambda tag: 1 if "Group" in tag or "Travelers with friends" in tag else 0)
df["Family_with_young_children"] = df.Tags.apply(lambda tag: 1 if "Family with young children" in tag else 0)
df["Family_with_older_children"] = df.Tags.apply(lambda tag: 1 if "Family with older children" in tag else 0)
df["With_a_pet"] = df.Tags.apply(lambda tag: 1 if "With a pet" in tag else 0)
```

----------------------------------------

TITLE: Calculating Average Price with dplyr in R
DESCRIPTION: Uses 'dplyr::mutate()' to create a new column named 'Price'. The value for this column is calculated as the average of the 'Low Price' and 'High Price' columns for each row. The modified dataframe is then displayed using 'slice_head()' to show the first 5 rows, including the new 'Price' column.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/solution/R/lesson_2-R.ipynb#_snippet_7

LANGUAGE: R
CODE:
```
# Create a new column Price
pumpkins <- pumpkins %>%
  mutate(Price = (`Low Price` + `High Price`)/2)

# View the first few rows of the data
pumpkins %>%
  slice_head(n = 5)
```

----------------------------------------

TITLE: Displaying Top 1 and Top 10 Nationalities with Pandas
DESCRIPTION: Identifies and prints the nationality with the highest frequency and its count, based on the `nationality_freq` Series calculated previously. It then displays the next 10 most frequent nationalities and their corresponding counts. Requires the `nationality_freq` pandas Series. Outputs formatted strings showing the top nationalities.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_2

LANGUAGE: python
CODE:
```
print("The highest frequency reviewer nationality is " + str(nationality_freq.index[0]).strip() + " with " + str(nationality_freq[0]) + " reviews.")
# Notice there is a leading space on the values, strip() removes that for printing
# What is the top 10 most common nationalities and their frequencies?
print("The next 10 highest frequency reviewer nationalities are:")
print(nationality_freq[1:11].to_string())
```

----------------------------------------

TITLE: Calculating Correlation between Date/Month and Price (Python)
DESCRIPTION: This snippet calculates the Pearson correlation coefficient between the 'Month' column and 'Price', and between the 'DayOfYear' column and 'Price' in the `new_pumpkins` DataFrame. It uses the `.corr()` method available for pandas Series to quantify the linear relationship between these variables.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/de/2-Regression/3-Linear/README.md#_snippet_0

LANGUAGE: python
CODE:
```
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

----------------------------------------

TITLE: Balancing Dataset with SMOTE - Python
DESCRIPTION: Applies the SMOTE algorithm to the feature and label dataframes (`feature_df`, `labels_df`) to balance the class distribution. It generates synthetic samples for minority classes to achieve equal representation. Requires `imblearn` and the separated feature/label dataframes.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/README.md#_snippet_14

LANGUAGE: python
CODE:
```
oversample = SMOTE()
transformed_feature_df, transformed_label_df = oversample.fit_resample(feature_df, labels_df)
```

----------------------------------------

TITLE: Plot Linear Regression Results Python Matplotlib
DESCRIPTION: This code generates a scatter plot of the actual test data points (X_test vs. y_test) and overlays a line plot representing the predictions made by the linear regression model (X_test vs. pred) to visualize the model's fit.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/2-Regression/3-Linear/README.md#_snippet_11

LANGUAGE: python
CODE:
```
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

----------------------------------------

TITLE: Visualizing Data and Regression Line with Matplotlib (Python)
DESCRIPTION: Uses Matplotlib (`plt`) to create a scatter plot of the actual test data points (`X_test` vs `y_test`). Overlays a line plot representing the model's predictions (`X_test` vs `y_pred`) to show the learned linear relationship. Adds labels for the axes and a title to the plot, then displays the plot. Requires Matplotlib.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/2-Regression/1-Tools/README.md#_snippet_5

LANGUAGE: Python
CODE:
```
plt.scatter(X_test, y_test,  color='black')
plt.plot(X_test, y_pred, color='blue', linewidth=3)
plt.xlabel('Scaled BMIs')
plt.ylabel('Disease Progression')
plt.title('A Graph Plot Showing Diabetes Progression Against BMI')
plt.show()
```

----------------------------------------

TITLE: Counting DataFrame Rows with Apply/Lambda (Python)
DESCRIPTION: This snippet demonstrates counting rows in a pandas DataFrame based on conditions using the `df.apply` method with a lambda function. It creates boolean Series for specific conditions and counts the `True` values using `len(...[... == True].index)`. It also measures the execution time. Requires pandas and time libraries.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_0

LANGUAGE: python
CODE:
```
 print("Number of No Positive reviews: " + str(len(no_positive_reviews[no_positive_reviews == True].index)))

 both_no_reviews = df.apply(lambda x: True if x['Negative_Review'] == "No Negative" and x['Positive_Review'] == "No Positive" else False , axis=1)
 print("Number of both No Negative and No Positive reviews: " + str(len(both_no_reviews[both_no_reviews == True].index)))
 end = time.time()
 print("Lambdas took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Implementing Recommender Logic with ONNX Runtime JavaScript
DESCRIPTION: This JavaScript code manages user input by listening to checkbox changes and updating an ingredient array based on selected items. It includes validation to ensure at least one ingredient is selected before initiating the `startInference` function, which asynchronously loads the `model.onnx`, prepares a tensor input, runs inference using ONNX Runtime, and displays the prediction result in an alert box.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/4-Classification/4-Applied/README.md#_snippet_3

LANGUAGE: javascript
CODE:
```
<script>
        const ingredients = Array(380).fill(0);
        
        const checks = [...document.querySelectorAll('.checkbox')];
        
        checks.forEach(check => {
            check.addEventListener('change', function() {
                // toggle the state of the ingredient
                // based on the checkbox's value (1 or 0)
                ingredients[check.value] = check.checked ? 1 : 0;
            });
        });

        function testCheckboxes() {
            // validate if at least one checkbox is checked
            return checks.some(check => check.checked);
        }

        async function startInference() {

            let atLeastOneChecked = testCheckboxes()

            if (!atLeastOneChecked) {
                alert('Please select at least one ingredient.');
                return;
            }
            try {
                // create a new session and load the model.
                
                const session = await ort.InferenceSession.create('./model.onnx');

                const input = new ort.Tensor(new Float32Array(ingredients), [1, 380]);
                const feeds = { float_input: input };

                // feed inputs and run
                const results = await session.run(feeds);

                // read from results
                alert('You can enjoy ' + results.label.data[0] + ' cuisine today!')

            } catch (e) {
                console.log(`failed to inference ONNX model`);
                console.error(e);
            }
        }
               
    </script>
```

----------------------------------------

TITLE: Dropping Columns and Saving Filtered Data (Python)
DESCRIPTION: This code removes specified columns from the DataFrame `df` using `df.drop()` with `inplace=True` to modify the DataFrame directly. It then prints a message indicating the save location and saves the modified DataFrame to a CSV file named `Hotel_Reviews_Filtered.csv` in the `../data/` directory, excluding the DataFrame index. Requires a pandas DataFrame named `df` with the specified columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_1

LANGUAGE: python
CODE:
```
df.drop(["Review_Total_Negative_Word_Counts", "Review_Total_Positive_Word_Counts", "days_since_review", "Total_Number_of_Reviews_Reviewer_Has_Given"], axis = 1, inplace=True)

# Saving new data file with calculated columns
print("Saving results to Hotel_Reviews_Filtered.csv")
df.to_csv(r'../data/Hotel_Reviews_Filtered.csv', index = False)
```

----------------------------------------

TITLE: Implementing Q-Learning Training Loop - Python
DESCRIPTION: Executes the Q-Learning training process over a specified number of epochs (5000). In each epoch, it simulates an agent's movement in an environment (`m`), selecting actions probabilistically based on the current Q-Table (`Q`) and a `probs` function, calculates rewards, and updates the Q-values using the Bellman equation with learning rate `alpha` and discount factor `gamma`. The loop terminates when a goal is reached or the cumulative reward is too low, recording the path length in `lpath`. Requires environment object `m`, `probs` function, `actions`, `reward` function, `end_reward`, `lpath` (list to store path lengths), `action_idx` mapping actions to indices, and the initialized `Q` table.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/8-Reinforcement/1-QLearning/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
    for epoch in range(5000):
    
        # Pick initial point
        m.random_start()
        
        # Start travelling
        n=0
        cum_reward = 0
        while True:
            x,y = m.human
            v = probs(Q[x,y])
            a = random.choices(list(actions),weights=v)[0]
            dpos = actions[a]
            m.move(dpos,check_correctness=False) # we allow player to move outside the board, which terminates episode
            r = reward(m)
            cum_reward += r
            if r==end_reward or cum_reward < -1000:
                lpath.append(n)
                break
            alpha = np.exp(-n / 10e5)
            gamma = 0.5
            ai = action_idx[a]
            Q[x,y,ai] = (1 - alpha) * Q[x,y,ai] + alpha * (r + gamma * Q[x+dpos[0], y+dpos[1]].max())
            n+=1
```

----------------------------------------

TITLE: Calculating Correlation Coefficients with Pandas Python
DESCRIPTION: This snippet calculates Pearson correlation coefficients between the 'Month' and 'Price' columns, and between the 'DayOfYear' and 'Price' columns of the `new_pumpkins` DataFrame. It uses the `.corr()` method available for pandas Series to determine the linear relationship strength, printing the results to standard output. This helps assess initial relationships before modeling.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/3-Linear/README.md#_snippet_0

LANGUAGE: python
CODE:
```
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

----------------------------------------

TITLE: Filtering Train and Test DataFrames (Python)
DESCRIPTION: Filters the original 'energy' DataFrame based on the defined start dates to create distinct pandas DataFrames for the training and testing sets. This step isolates the data relevant to each period, ensuring no data leakage between sets.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_6

LANGUAGE: python
CODE:
```
train = energy.copy()[(energy.index >= train_start_dt) & (energy.index < test_start_dt)][['load']]
test = energy.copy()[energy.index >= test_start_dt][['load']]
```

----------------------------------------

TITLE: Creating FacetGrid Scatterplot for Popularity vs Danceability by Genre Python
DESCRIPTION: Generates a scatterplot of 'popularity' vs 'danceability' for each genre within the filtered dataset using `seaborn.FacetGrid`, providing separate plots or color-coded points for each genre.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/notebook.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
sns.FacetGrid(df, hue="artist_top_genre", size=5) \
   .map(plt.scatter, "popularity", "danceability") \
   .add_legend()
```

----------------------------------------

TITLE: Plotting Cuisine Counts (Matplotlib Pandas Python)
DESCRIPTION: Generates a horizontal bar plot showing the counts of each cuisine type based on the value counts of the 'cuisine' column. This visualization provides a clear picture of the class distribution and the extent of imbalance.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/notebook.ipynb#_snippet_6

LANGUAGE: Python
CODE:
```
df.cuisine.value_counts().plot.barh()
```

----------------------------------------

TITLE: Applying Label Encoding (Scikit-learn, Pandas) - Python
DESCRIPTION: This snippet applies Label Encoding to the non-numeric columns of a Pandas DataFrame. It iterates through columns (excluding the last one) and transforms string categories into numerical labels, which is a necessary step before feeding data into many machine learning algorithms.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/translations/README.ko.md#_snippet_0

LANGUAGE: Python
CODE:
```
from sklearn.preprocessing import LabelEncoder

new_pumpkins.iloc[:, 0:-1] = new_pumpkins.iloc[:, 0:-1].apply(LabelEncoder().fit_transform)
```

----------------------------------------

TITLE: Counting Reviews with No Negative Text with Pandas
DESCRIPTION: Counts the number of rows in the `df` DataFrame where the value in the `Negative_Review` column is exactly 'No Negative'. It uses the `apply()` method with a lambda function to check the condition for each row and then counts the number of `True` results. Includes timing the operation. Requires the `df` DataFrame with a 'Negative_Review' column and the `time` module imported. Outputs the count of reviews without negative text.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_6

LANGUAGE: python
CODE:
```
# with lambdas:
start = time.time()
no_negative_reviews = df.apply(lambda x: True if x['Negative_Review'] == "No Negative" else False , axis=1)
print("Number of No Negative reviews: " + str(len(no_negative_reviews[no_negative_reviews == True].index)))
```

----------------------------------------

TITLE: Initializing Sentiment Analysis Setup and Loading Data
DESCRIPTION: This snippet imports necessary libraries for sentiment analysis, including time, pandas, and NLTK components (stopwords, VADER). It downloads the 'vader_lexicon' required by the VADER sentiment analyzer and then loads the previously saved 'Hotel_Reviews_Filtered.csv' dataset into a pandas DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_2

LANGUAGE: python
CODE:
```
import time
import pandas as pd
import nltk as nltk
from nltk.corpus import stopwords
from nltk.sentiment.vader import SentimentIntensityAnalyzer
ltk.download('vader_lexicon')

# Load the filtered hotel reviews from CSV
df = pd.read_csv('../../data/Hotel_Reviews_Filtered.csv')

# You code will be added here


# Finally remember to save the hotel reviews with new NLP data added
print("Saving results to Hotel_Reviews_NLP.csv")
df.to_csv(r'../data/Hotel_Reviews_NLP.csv', index = False)
```

----------------------------------------

TITLE: Defining Simple State Discretization Function Python
DESCRIPTION: Defines a Python function `discretize` that takes a continuous observation vector (numpy array) as input and returns a tuple of integers. It performs a simple scaling of the observation values by predefined factors and casts the result to integers for discretization.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/README.md#_snippet_5

LANGUAGE: python
CODE:
```
def discretize(x):
    return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
```

----------------------------------------

TITLE: Viewing Individual Missing Value Indicators in R
DESCRIPTION: Applies the 'is.na()' function to the 'pumpkins' dataframe to create a logical dataframe of the same dimensions, where 'TRUE' indicates a missing value. It then pipes this result to 'head()' to display the first 7 rows, allowing a glimpse of where missing values might be located at the start of the dataset.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/solution/R/lesson_2-R.ipynb#_snippet_3

LANGUAGE: R
CODE:
```
pumpkins %>%
  is.na() %>%
  head(n = 7)
```

----------------------------------------

TITLE: Printing Top 10 Nationalities Python
DESCRIPTION: This code prints the top 10 most frequent reviewer nationalities along with their counts. It slices the `nationality_freq` Series and converts it to a string for printing.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/4-Hotel-Reviews-1/solution/notebook.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
# What is the top 10 most common nationalities and their frequencies?
print("The top 10 highest frequency reviewer nationalities are:")
print(nationality_freq[0:10].to_string())
```

----------------------------------------

TITLE: Plotting Full Time Series - Energy Data - Python
DESCRIPTION: Generates a plot of the 'load' column from the `energy` DataFrame. It uses `subplots=True` (though only one subplot is shown here), sets the figure size to 15x8 inches, and specifies a font size of 12 for labels and ticks. X and Y axis labels are set using `plt.xlabel` and `plt.ylabel`, and the plot is displayed using `plt.show()`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/7-TimeSeries/1-Introduction/README.md#_snippet_2

LANGUAGE: python
CODE:
```
energy.plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)
plt.xlabel('timestamp', fontsize=12)
plt.ylabel('load', fontsize=12)
plt.show()
```

----------------------------------------

TITLE: Initializing One-Hot Encoder for Nominal Features (Python)
DESCRIPTION: Imports `OneHotEncoder` from `sklearn.preprocessing`. Defines a list of nominal categorical features. Initializes a `OneHotEncoder` instance, specifying `sparse_output=False` to get a dense array output, preparing to transform the listed columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/4-Logistic/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
from sklearn.preprocessing import OneHotEncoder

categorical_features = ['City Name', 'Package', 'Variety', 'Origin']
categorical_encoder = OneHotEncoder(sparse_output=False)
```

----------------------------------------

TITLE: Visualizing Price vs Day of Year by Variety - Python
DESCRIPTION: Generates a scatter plot showing the relationship between 'DayOfYear' and 'Price', differentiating points by pumpkin 'Variety' using different colors. This visualization helps identify if distinct price clusters correspond to different varieties.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/notebook.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    ax = new_pumpkins[new_pumpkins['Variety']==var].plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

----------------------------------------

TITLE: Bundling Recipe and Model into a Workflow in R
DESCRIPTION: Combines the previously defined preprocessing recipe ('pumpkins_recipe') and the logistic regression model specification ('log_reg') into a single workflow object using the workflows package. This workflow object streamlines the model training and prediction process. Requires the workflows package.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/R/lesson_4-R.ipynb#_snippet_10

LANGUAGE: R
CODE:
```
# Bundle modelling components in a workflow
log_reg_wf <- workflow() %>%
  add_recipe(pumpkins_recipe) %>%
  add_model(log_reg)

# Print out the workflow
log_reg_wf
```

----------------------------------------

TITLE: Applying Sentiment Calculation to DataFrame (Python)
DESCRIPTION: This snippet illustrates how the previously defined `calc_sentiment` function would be applied to columns of a pandas DataFrame. This is typically done for the review text columns ('Negative_Review', 'Positive_Review') to generate new columns containing the calculated sentiment scores for each review entry. Requires a pandas DataFrame `df` and the `calc_sentiment` function to be defined and available.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_5

LANGUAGE: python
CODE:
```

```

----------------------------------------

TITLE: Selecting Specific Columns with dplyr in R
DESCRIPTION: Uses the 'dplyr::select()' function to retain only the 'Package', 'Low Price', 'High Price', and 'Date' columns from the 'pumpkins' dataframe, overwriting the original dataframe. It then displays the first 5 rows of the modified dataframe using 'slice_head()' to verify the selection.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/solution/R/lesson_2-R.ipynb#_snippet_5

LANGUAGE: R
CODE:
```
pumpkins <- pumpkins %>%
  select(Package, `Low Price`, `High Price`, Date)


# Print data set
pumpkins %>%
  slice_head(n = 5)
```

----------------------------------------

TITLE: Creating Time-Stepped Testing Data (Python)
DESCRIPTION: Reshapes the 1D testing data array into a 2D array using the same time-stepping logic as the training data. Each row contains a sequence of 'timesteps' points, preparing the test data for prediction with the SVR model.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_12

LANGUAGE: python
CODE:
```
test_data_timesteps=np.array([[j for j in test_data[i:i+timesteps]] for i in range(0,len(test_data)-timesteps+1)])[:,:,0]
```

----------------------------------------

TITLE: Analyzing and Plotting Top Indian Ingredients - Python
DESCRIPTION: Applies the `create_ingredient_df` function to the Indian cuisine data. It then extracts the top 10 ingredients from the dataframe and generates a horizontal bar plot for visualization. Requires the function and the Indian data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/README.md#_snippet_11

LANGUAGE: python
CODE:
```
indian_ingredient_df = create_ingredient_df(indian_df)
indian_ingredient_df.head(10).plot.barh()
```

----------------------------------------

TITLE: Plotting Full Dataset Predictions (Python)
DESCRIPTION: Creates a plot to visualize the model's predictions on the entire dataset compared to the actual values. This provides an overall view of the model's fit to the complete time series.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_24

LANGUAGE: python
CODE:
```
plt.figure(figsize=(30,8))
# plot original output
# plot predicted output
plt.legend(['Actual','Predicted'])
plt.xlabel('Timestamp')
plt.show()
```

----------------------------------------

TITLE: Checking for Null Values - Pandas Python
DESCRIPTION: This code checks for the presence of null or missing values in each column of the DataFrame. The `isnull()` method returns a boolean DataFrame of the same shape, and `sum()` is then applied to count the number of True values (nulls) in each column. A result of 0 for all columns indicates no missing data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/README.md#_snippet_3

LANGUAGE: python
CODE:
```
df.isnull().sum()
```

----------------------------------------

TITLE: Visualizing Item Size vs Color with Seaborn Swarmplot (Python)
DESCRIPTION: Uses `seaborn.swarmplot` to visualize the distribution of 'ord__Item Size' values against the 'Color' categories from the `encoded_pumpkins` DataFrame. It applies a custom color palette mapping integer color labels (0, 1) to colors. Requires Seaborn and pandas. Expects `encoded_pumpkins` DataFrame with 'Color' and 'ord__Item Size' columns. Notes a potential warning if data points are too dense.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/4-Logistic/README.md#_snippet_1

LANGUAGE: python
CODE:
```
    palette = {
    0: 'orange',
    1: 'wheat'
    }
    sns.swarmplot(x="Color", y="ord__Item Size", data=encoded_pumpkins, palette=palette)
```

----------------------------------------

TITLE: Subsetting and Plotting Pie Pumpkins - Pandas Python
DESCRIPTION: Filters the `new_pumpkins` DataFrame to select only rows where the 'Variety' is 'PIE TYPE'. It then creates a scatter plot of 'DayOfYear' against 'Price' specifically for this subset, preparing the data for focused analysis on a single variety for regression modeling. Requires the pandas library and matplotlib for plotting.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/2-Regression/3-Linear/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price')
```

----------------------------------------

TITLE: Evaluate Polynomial Model Performance (R)
DESCRIPTION: Evaluates the performance of the polynomial regression model using the `metrics()` function from the yardstick package. It compares the actual `price` values (`truth`) in the `poly_results` data frame against the model's predicted values (`estimate = .pred`).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_19

LANGUAGE: R
CODE:
```
metrics(data = poly_results, truth = price, estimate = .pred)
```

----------------------------------------

TITLE: Visualizing Linear Regression Results in Python
DESCRIPTION: Generates a plot to visualize the linear regression results using Matplotlib. It creates a scatter plot of the actual test data points (`X_test` vs `y_test`) and overlays the regression line by plotting the test features against the model's predictions (`X_test` vs `y_pred`). Includes labels and a title for clarity. Requires `matplotlib.pyplot`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/2-Regression/1-Tools/README.md#_snippet_5

LANGUAGE: Python
CODE:
```
plt.scatter(X_test, y_test,  color='black')
plt.plot(X_test, y_pred, color='blue', linewidth=3)
plt.xlabel('Scaled BMIs')
plt.ylabel('Disease Progression')
plt.title('A Graph Plot Showing Diabetes Progression Against BMI')
plt.show()
```

----------------------------------------

TITLE: Fitting SARIMAX Model and Displaying Summary Python
DESCRIPTION: Defines the order and seasonal order parameters for a SARIMAX model (an extension of ARIMA). It initializes the SARIMAX model with the scaled training data and fits the model, then prints a detailed summary of the model fit, including coefficients and diagnostic statistics.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/solution/notebook.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
order = (4, 1, 0)
seasonal_order = (1, 1, 0, 24)

model = SARIMAX(endog=train, order=order, seasonal_order=seasonal_order)
results = model.fit()

print(results.summary())
```

----------------------------------------

TITLE: Select Specific Columns with Pandas loc (Python)
DESCRIPTION: Selects a subset of columns from the DataFrame using the `loc` indexer. It creates a new DataFrame containing only the 'Package', 'Low Price', 'High Price', and 'Date' columns, discarding others. This simplifies the DataFrame for the specific task.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/README.md#_snippet_2

LANGUAGE: python
CODE:
```
columns_to_select = ['Package', 'Low Price', 'High Price', 'Date']
pumpkins = pumpkins.loc[:, columns_to_select]
```

----------------------------------------

TITLE: Installing Required R Packages with pacman
DESCRIPTION: Checks if the `pacman` package is installed and installs it if missing. Then, it uses `pacman::p_load` to load (and install if necessary) the required R packages (`tidyverse`, `tidymodels`, `janitor`, `ggbeeswarm`) for performing data analysis and modeling tasks in the lesson.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/R/lesson_4-R.ipynb#_snippet_0

LANGUAGE: R
CODE:
```
suppressWarnings(if (!require("pacman"))install.packages("pacman"))

pacman::p_load(tidyverse, tidymodels, janitor, ggbeeswarm)
```

----------------------------------------

TITLE: Calculating and Printing Testing MAPE (Python)
DESCRIPTION: Calculates the Mean Absolute Percentage Error (MAPE) between the inverse-scaled testing predictions and the inverse-scaled actual testing values. The result is printed as a percentage, evaluating the model's performance on unseen data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_22

LANGUAGE: python
CODE:
```
print('MAPE for testing data: ', mape(y_test_pred, y_test)*100, '%')
```

----------------------------------------

TITLE: Visualizing Pumpkin Price vs. Month using Matplotlib
DESCRIPTION: Creates a scatter plot using matplotlib to visualize the relationship between the processed pumpkin 'Price' and the 'Month' of sale. This helps identify potential seasonal trends in pumpkin prices. Requires the `new_pumpkins` DataFrame with 'Price' and 'Month' columns and the matplotlib library. Input: pandas DataFrame. Output: Scatter plot displayed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/solution/notebook.ipynb#_snippet_3

LANGUAGE: python
CODE:
```

price = new_pumpkins.Price
month = new_pumpkins.Month
plt.scatter(price, month)
plt.show()

```

----------------------------------------

TITLE: Initializing VADER Sentiment Analyzer Python
DESCRIPTION: Creates an instance of the NLTK SentimentIntensityAnalyzer. This object is used to calculate the compound sentiment score for text inputs based on its pre-trained lexicon and rules.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/3-notebook.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
vader_sentiment = SentimentIntensityAnalyzer()

```

----------------------------------------

TITLE: Visualizing Variable Relationships with Seaborn Catplot (Python)
DESCRIPTION: Uses `seaborn.catplot` to visualize the relationship between 'Item Size', 'Variety', and 'Color' from the `pumpkins` DataFrame. It configures the plot type ('box'), orientation ('h'), axis sharing, titles, size, aspect ratio, and a custom color palette. It then sets x-axis label, y-axis label, and x-axis limits. Requires Seaborn and pandas. Expects `pumpkins` DataFrame with specified columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/4-Logistic/README.md#_snippet_0

LANGUAGE: python
CODE:
```
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }
    pumpkins['Item Size'] = encoded_pumpkins['ord__Item Size']

    g = sns.catplot(
        data=pumpkins,
        x="Item Size", y="Color", row='Variety',
        kind="box", orient="h",
        sharex=False, margin_titles=True,
        height=1.8, aspect=4, palette=palette,
    )
    g.set(xlabel="Item Size", ylabel="").set(xlim=(0,6))
    g.set_titles(row_template="{row_name}")
```

----------------------------------------

TITLE: Preprocess Categorical Features and Select Data for Clustering in Python
DESCRIPTION: Applies Label Encoding to convert the 'artist_top_genre' categorical column into a numerical format required for K-Means. Selects specific columns ('artist_top_genre', 'popularity', 'danceability', 'acousticness', 'loudness', 'energy') to form the feature matrix X for clustering, while also encoding the original 'artist_top_genre' as a target vector y for potential comparison. Requires `sklearn.preprocessing.LabelEncoder` and a pandas DataFrame `df`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/README.md#_snippet_1

LANGUAGE: python
CODE:
```
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

X = df.loc[:, ('artist_top_genre','popularity','danceability','acousticness','loudness','energy')]

y = df['artist_top_genre']

X['artist_top_genre'] = le.fit_transform(X['artist_top_genre'])

y = le.transform(y)
```

----------------------------------------

TITLE: Visualizing Linear Regression Results
DESCRIPTION: Generates a scatter plot of the actual test data points (`X_test`, `y_test`) and overlays a line plot representing the model's predictions (`X_test`, `pred`) against the test features. This visualization helps to visually assess how well the regression line fits the test data. Requires matplotlib.pyplot (implicitly used, needs `import matplotlib.pyplot as plt`), test features (`X_test`), test targets (`y_test`), and model predictions (`pred`). Outputs a plot visualization.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/fr/2-Regression/3-Linear/README.md#_snippet_11

LANGUAGE: python
CODE:
```
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

----------------------------------------

TITLE: Filtering and Printing Shapes of Train and Test Sets in Python
DESCRIPTION: This code filters the original energy DataFrame using the previously defined start dates to create separate DataFrames for the training and testing sets. It then prints the shape (number of rows, number of columns) of both resulting DataFrames.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/README.md#_snippet_5

LANGUAGE: python
CODE:
```
train = energy.copy()[(energy.index >= train_start_dt) & (energy.index < test_start_dt)][['load']]
test = energy.copy()[energy.index >= test_start_dt][['load']]

print('Training data shape: ', train.shape)
print('Test data shape: ', test.shape)
```

----------------------------------------

TITLE: Printing Classification Report (Python)
DESCRIPTION: Generates and prints a classification report that summarizes the performance of the model on the test set. This report includes precision, recall, F1-score, and support for each predicted class, based on the true test labels (`y_test`) and the predicted labels (`y_pred`).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/solution/notebook.ipynb#_snippet_9

LANGUAGE: Python
CODE:
```
print(classification_report(y_test,y_pred))
```

----------------------------------------

TITLE: Format Predictions for Evaluation in Python
DESCRIPTION: This snippet takes the multi-step predictions generated during walk-forward validation and formats them into a pandas DataFrame (`eval_df`) suitable for evaluation. It aligns predictions with their corresponding actual values and inverse transforms scaled data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/7-TimeSeries/2-ARIMA/README.md#_snippet_4

LANGUAGE: python
CODE:
```
eval_df = pd.DataFrame(predictions, columns=['t+'+str(t) for t in range(1, HORIZON+1)])
eval_df['timestamp'] = test.index[0:len(test.index)-HORIZON+1]
eval_df = pd.melt(eval_df, id_vars='timestamp', value_name='prediction', var_name='h')
eval_df['actual'] = np.array(np.transpose(test_ts)).ravel()
eval_df[['prediction', 'actual']] = scaler.inverse_transform(eval_df[['prediction', 'actual']])
eval_df.head()
```

----------------------------------------

TITLE: Building and Training a Polynomial Regression Pipeline - Python
DESCRIPTION: This snippet creates a scikit-learn pipeline using `make_pipeline`. The pipeline first applies `PolynomialFeatures(2)` to transform the input data by adding second-degree polynomial features, and then fits a `LinearRegression` model to the transformed data. The pipeline is then trained end-to-end using the training data (`X_train`, `y_train`).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/2-Regression/3-Linear/README.md#_snippet_13

LANGUAGE: python
CODE:
```
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

----------------------------------------

TITLE: Reordering Columns and Saving Final Data (Python)
DESCRIPTION: Reorders the columns in the DataFrame to a specified logical sequence for improved readability and organization. Finally, it saves the complete, processed DataFrame, including all original, filtered, and newly generated columns (tag features and sentiment scores), to a new CSV file named 'Hotel_Reviews_NLP.csv', excluding the index column.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_7

LANGUAGE: python
CODE:
```
# Reorder the columns (This is cosmetic, but to make it easier to explore the data later)
df = df.reindex(["Hotel_Name", "Hotel_Address", "Total_Number_of_Reviews", "Average_Score", "Reviewer_Score", "Negative_Sentiment", "Positive_Sentiment", "Reviewer_Nationality", "Leisure_trip", "Couple", "Solo_traveler", "Business_trip", "Group", "Family_with_young_children", "Family_with_older_children", "With_a_pet", "Negative_Review", "Positive_Review"], axis=1)

print("Saving results to Hotel_Reviews_NLP.csv")
df.to_csv(r"../data/Hotel_Reviews_NLP.csv", index = False)
```

----------------------------------------

TITLE: Exploring Dataset Introduction in R
DESCRIPTION: Utilizes the `DataExplorer` package to get a basic overview of the dataset `df`. The `introduce()` function provides summary information such as dimensions, variable types, and missing values. `plot_intro()` generates a visualization of this introductory data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb#_snippet_2

LANGUAGE: R
CODE:
```
# Basic information about the data
df %>%
  introduce()

# Visualize basic information above
df %>%
  plot_intro(ggtheme = theme_light())
```

----------------------------------------

TITLE: Creating Basic Scatter Plot with ggplot2 (R)
DESCRIPTION: Sets a light theme for subsequent plots using `theme_set` from `ggplot2`. It then creates a basic scatter plot object `p` mapping 'Price' to the x-axis and 'Month' to the y-axis, and adds a point layer using `geom_point`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/solution/R/lesson_2-R.ipynb#_snippet_13

LANGUAGE: R
CODE:
```
# Set a theme for the plots
theme_set(theme_light())

# Create a scatter plot
p <- ggplot(data = new_pumpkins, aes(x = Price, y = Month))
p + geom_point()
```

----------------------------------------

TITLE: Load Filtered Data and Initialize NLP using Pandas and NLTK
DESCRIPTION: This code imports necessary libraries for data manipulation and NLP, including `time`, `pandas`, and NLTK components like `stopwords` and `SentimentIntensityAnalyzer`. It downloads the required 'vader_lexicon' from NLTK and loads the previously saved filtered CSV file into a pandas DataFrame named `df` for further processing.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_2

LANGUAGE: python
CODE:
```
import time
import pandas as pd
import nltk as nltk
from nltk.corpus import stopwords
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

# Load the filtered hotel reviews from CSV
df = pd.read_csv('../../data/Hotel_Reviews_Filtered.csv')

# You code will be added here


# Finally remember to save the hotel reviews with new NLP data added
print("Saving results to Hotel_Reviews_NLP.csv")
df.to_csv(r'../data/Hotel_Reviews_NLP.csv', index = False)

```

----------------------------------------

TITLE: Generating Classification Report (Python, Scikit-learn)
DESCRIPTION: This snippet generates and prints a detailed classification report for the trained model's performance on the test set using scikit-learn's `metrics.classification_report`. It first predicts the classes for the test features (`X_test`) using `model.predict()` to get `y_pred`, and then passes the true labels (`y_test`) and predicted labels (`y_pred`) to the `classification_report` function. The output includes precision, recall, f1-score, and support for each class. Requires Scikit-learn.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/README.md#_snippet_8

LANGUAGE: python
CODE:
```
y_pred = model.predict(X_test)
print(classification_report(y_test,y_pred))
```

----------------------------------------

TITLE: Splitting Data, Defining Recipe & Model Spec in R
DESCRIPTION: This comprehensive snippet prepares data for modeling. It sets a seed for reproducibility, splits the `new_pumpkins` data into 80% training and 20% testing sets, defines a specific recipe for the linear model (`price ~ package`) that integer-encodes only the 'package' predictor, and creates a linear regression model specification using the 'lm' engine for regression tasks.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_11

LANGUAGE: R
CODE:
```
set.seed(2056)
# Split the data into training and test sets
pumpkins_split <- new_pumpkins %>%
  initial_split(prop = 0.8)


# Extract training and test data
pumpkins_train <- training(pumpkins_split)
pumpkins_test <- testing(pumpkins_split)



# Create a recipe for preprocessing the data
lm_pumpkins_recipe <- recipe(price ~ package, data = pumpkins_train) %>%
  step_integer(all_predictors(), zero_based = TRUE)



# Create a linear model specification
lm_spec <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")
```

----------------------------------------

TITLE: Summarizing Missing Values Per Column in R
DESCRIPTION: Applies 'is.na()' to generate a logical matrix of missing values. It then pipes this matrix to 'colSums()', which calculates the sum of 'TRUE' values (missing values) for each column, providing a concise summary of missing data counts per feature.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/solution/R/lesson_2-R.ipynb#_snippet_4

LANGUAGE: R
CODE:
```
pumpkins %>%
  is.na() %>%
  colSums()
```

----------------------------------------

TITLE: Preparing Data by Combining Multiple Features
DESCRIPTION: Illustrates how to create a comprehensive feature matrix `X` by combining one-hot encoded categorical features ('Variety', 'City', 'Package') with a numerical feature ('Month'). The `join` method is used to merge these features into a single DataFrame, preparing it for model training.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/3-Linear/README.md#_snippet_16

LANGUAGE: python
CODE:
```
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

----------------------------------------

TITLE: Visualizing Pumpkin Variety by Color (Python)
DESCRIPTION: Imports the Seaborn library. Defines a color palette for 'ORANGE' and 'WHITE' categories. Creates a categorical count plot using `sns.catplot` to show the distribution of pumpkin `Variety` based on `Color`, applying the defined palette.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/4-Logistic/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
import seaborn as sns

palette = {
'ORANGE': 'orange',
'WHITE': 'wheat',
}

sns.catplot(
data=pumpkins, y="Variety", hue="Color", kind="count",
palette=palette
)
```

----------------------------------------

TITLE: Finding Most Reviewed Hotel per Top 10 Nationalities with Pandas
DESCRIPTION: Iterates through the top 10 nationalities from the `nationality_freq` Series. For each nationality, it filters the main `df` DataFrame to get reviews only from that country, then calculates the frequency of hotels reviewed by that nationality and prints the most reviewed hotel. Requires the `df` DataFrame and the `nationality_freq` Series. Outputs the most reviewed hotel for each of the top 10 nationalities.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_3

LANGUAGE: python
CODE:
```
# What was the most frequently reviewed hotel for the top 10 nationalities
# Normally with pandas you will avoid an explicit loop, but wanted to show creating a new dataframe using criteria (don't do this with large amounts of data because it could be very slow)
for nat in nationality_freq[:10].index:
   # First, extract all the rows that match the criteria into a new dataframe
   nat_df = df[df["Reviewer_Nationality"] == nat]   
   # Now get the hotel freq
   freq = nat_df["Hotel_Name"].value_counts()
   print("The most reviewed hotel for " + str(nat).strip() + " was " + str(freq.index[0]) + " with " + str(freq[0]) + " reviews.")
```

----------------------------------------

TITLE: Saving Filtered Hotel Review Data (Python)
DESCRIPTION: This code snippet drops several columns from the pandas DataFrame, including word counts, days since review, and total reviews given by the reviewer. It then saves the modified DataFrame to a new CSV file named 'Hotel_Reviews_Filtered.csv' in the '../data/' directory, excluding the index column. Requires a pandas DataFrame with the specified columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_1

LANGUAGE: python
CODE:
```
df.drop(["Review_Total_Negative_Word_Counts", "Review_Total_Positive_Word_Counts", "days_since_review", "Total_Number_of_Reviews_Reviewer_Has_Given"], axis = 1, inplace=True)

# Saving new data file with calculated columns
print("Saving results to Hotel_Reviews_Filtered.csv")
df.to_csv(r'../data/Hotel_Reviews_Filtered.csv', index = False)

```

----------------------------------------

TITLE: Saving Filtered Hotel Review Data to CSV using Pandas
DESCRIPTION: This snippet removes several numerical columns related to review counts and timing from the pandas DataFrame (`df`). It then saves the resulting DataFrame to a CSV file named `Hotel_Reviews_Filtered.csv` in the `../data/` directory, excluding the DataFrame index. Requires a pandas DataFrame with the specified columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/fr/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_1

LANGUAGE: python
CODE:
```
df.drop(["Review_Total_Negative_Word_Counts", "Review_Total_Positive_Word_Counts", "days_since_review", "Total_Number_of_Reviews_Reviewer_Has_Given"], axis = 1, inplace=True)

# Saving new data file with calculated columns
print("Saving results to Hotel_Reviews_Filtered.csv")
df.to_csv(r'../data/Hotel_Reviews_Filtered.csv', index = False)

```

----------------------------------------

TITLE: Running Policy Simulations and Printing Statistics - Python
DESCRIPTION: Implements a function `print_statistics` to evaluate the performance of a given policy. It runs the `walk` simulation 100 times, recording the outcome of each attempt. It calculates and prints the average path length for successful runs (finding an apple) and the total count of unsuccessful runs (encountering a wolf or water). This function depends on the `walk` function and the board instance `m`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
def print_statistics(policy):
    s,w,n = 0,0,0
    for _ in range(100):
        z = walk(m,policy)
        if z<0:
            w+=1
        else:
            s += z
            n += 1
    print(f"Average path length = {s/n}, eaten by wolf: {w} times")

print_statistics(random_policy)
```

----------------------------------------

TITLE: Calculate Data Correlations Python Pandas
DESCRIPTION: This snippet calculates and prints the Pearson correlation coefficient between the 'Month' column and 'Price' column, and between the 'DayOfYear' column and 'Price' column, within a pandas DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/2-Regression/3-Linear/README.md#_snippet_0

LANGUAGE: python
CODE:
```
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

----------------------------------------

TITLE: Verifying Dataset Balance with Value Counts - Python
DESCRIPTION: Prints the value counts of the 'cuisine' column for both the original dataframe (`df.cuisine`) and the resampled label dataframe (`transformed_label_df`). This helps confirm whether the SMOTE operation successfully balanced the class distribution.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/README.md#_snippet_15

LANGUAGE: python
CODE:
```
print(f'new label count: {transformed_label_df.value_counts()}')
print(f'old label count: {df.cuisine.value_counts()}')
```

----------------------------------------

TITLE: Visualizing Average Price by Variety - Python
DESCRIPTION: Computes the mean 'Price' for each unique 'Variety' in the processed DataFrame using `groupby()`. Generates a bar plot from the resulting series to visualize the average price differences between different pumpkin varieties.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/notebook.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

----------------------------------------

TITLE: Handling Missing Values - Pandas Python
DESCRIPTION: Removes rows containing missing values (`NaN`) from the `pie_pumpkins` DataFrame using the `dropna()` method with `inplace=True`. It then prints concise information about the DataFrame, including the data types and non-null counts, to verify the data cleaning step. Requires the pandas library.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/2-Regression/3-Linear/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

----------------------------------------

TITLE: Select Features by Dropping Columns with Pandas Python
DESCRIPTION: This snippet creates a new DataFrame containing only the features for the model by dropping the 'Unnamed: 0' index column and the 'cuisine' target column from the original DataFrame. The `axis=1` argument specifies that columns should be dropped.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/solution/notebook.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)
cuisines_feature_df.head()
```

----------------------------------------

TITLE: Predict Probability for a Single Item Python
DESCRIPTION: This snippet takes a single test item, reshapes it for prediction, and uses the trained model's `predict_proba` method to get probability scores for each class. It then creates pandas DataFrames to display the results, sorted by the predicted probability.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/solution/notebook.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
#rehsape to 2d array and transpose
test= X_test.iloc[50].values.reshape(-1, 1).T
# predict with score
proba = model.predict_proba(test)
classes = model.classes_
# create df with classes and scores
resultdf = pd.DataFrame(data=proba, columns=classes)

# create df to show results
topPrediction = resultdf.T.sort_values(by=[0], ascending = [False])
topPrediction.head()
```

----------------------------------------

TITLE: Preprocessing Data with tidymodels Recipes (R)
DESCRIPTION: Defines a data preprocessing `recipe` using `tidymodels::recipes`. It applies ordinal encoding to the `item_size` column based on predefined levels and one-hot encoding to all other nominal (categorical) predictor variables. The recipe is then prepared (`prep`) and applied (`bake`) to the `pumpkins_select` data, producing a numerically encoded dataframe.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/R/lesson_4-R.ipynb#_snippet_5

LANGUAGE: R
CODE:
```
# Preprocess and extract data to allow some data analysis
baked_pumpkins <- recipe(color ~ ., data = pumpkins_select) %>%
  # Define ordering for item_size column
  step_mutate(item_size = ordered(item_size, levels = c('sml', 'med', 'med-lge', 'lge', 'xlge', 'jbo', 'exjbo'))) %>%
  # Convert factors to numbers using the order defined above (Ordinal encoding)
  step_integer(item_size, zero_based = F) %>%
  # Encode all other predictors using one hot encoding
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>%
  prep(data = pumpkin_select) %>%
  bake(new_data = NULL)

# Display the first few rows of preprocessed data
baked_pumpkins %>%
  slice_head(n = 5)
```

----------------------------------------

TITLE: Defining Function to Analyze Ingredients (Pandas Python)
DESCRIPTION: Defines a function `create_ingredient_df` that processes a cuisine-specific DataFrame. It transposes the DataFrame, drops irrelevant columns ('cuisine', 'Unnamed: 0'), sums ingredient counts across rows, filters out ingredients with zero occurrences, and returns a DataFrame sorted by ingredient count.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/notebook.ipynb#_snippet_8

LANGUAGE: Python
CODE:
```
def create_ingredient_df(df):
    # transpose df, drop cuisine and unnamed rows, sum the row to get total for ingredient and add value header to new df
    ingredient_df = df.T.drop(['cuisine','Unnamed: 0']).sum(axis=1).to_frame('value')
    # drop ingredients that have a 0 sum
    ingredient_df = ingredient_df[(ingredient_df.T != 0).any()]
    # sort df
    ingredient_df = ingredient_df.sort_values(by='value', ascending=False, inplace=False)
    return ingredient_df
```

----------------------------------------

TITLE: Count Review Types with Pandas Boolean Indexing/Sum (Python)
DESCRIPTION: This Python snippet calculates the count of rows in a pandas DataFrame (`df`) meeting the same conditions as the previous method ('No Negative' review, 'No Positive' review, or both). It uses more efficient vectorized pandas operations: boolean indexing and the `sum()` function, which counts `True` values in a boolean Series. Execution time is also measured to show the performance gain.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_7

LANGUAGE: python
CODE:
```
# without lambdas (using a mixture of notations to show you can use both)
start = time.time()
no_negative_reviews = sum(df.Negative_Review == "No Negative")
print("Number of No Negative reviews: " + str(no_negative_reviews))

no_positive_reviews = sum(df["Positive_Review"] == "No Positive")
print("Number of No Positive reviews: " + str(no_positive_reviews))

both_no_reviews = sum((df.Negative_Review == "No Negative") & (df.Positive_Review == "No Positive"))
print("Number of both No Negative and No Positive reviews: " + str(both_no_reviews))

end = time.time()
print("Sum took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Importing Scikit-learn Modules for Polynomial Regression Pipeline
DESCRIPTION: This code imports the necessary classes from Scikit-learn to build a pipeline for polynomial regression. It imports `PolynomialFeatures` for transforming data into polynomial features and `make_pipeline` to easily construct a sequence of transformations and a final estimator.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/2-Regression/3-Linear/README.md#_snippet_12

LANGUAGE: python
CODE:
```
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
```

----------------------------------------

TITLE: Creating New DataFrame with Processed Data
DESCRIPTION: Constructs a new Pandas DataFrame named `new_pumpkins` using a dictionary where keys become column names and values are the previously calculated `month`, `price`, and selected original columns like 'Package', 'Low Price', and 'High Price'. This consolidates the prepared data into a clean structure. Requires the `month`, `price` Series/arrays and the relevant columns from the original `pumpkins` DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/2-Data/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
new_pumpkins = pd.DataFrame({'Month': month, 'Package': pumpkins['Package'], 'Low Price': pumpkins['Low Price'],'High Price': pumpkins['High Price'], 'Price': price})
```

----------------------------------------

TITLE: Counting DataFrame Rows with Boolean Indexing/Sum (Python)
DESCRIPTION: This snippet shows an alternative method for counting rows in a pandas DataFrame based on conditions. It uses boolean indexing (`df.column == value`) which returns a boolean Series, and then applies the `sum()` function to count the `True` values. This method is shown to be significantly faster than using `apply` with a lambda. Requires pandas and time libraries.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_1

LANGUAGE: python
CODE:
```
 # without lambdas (using a mixture of notations to show you can use both)
 start = time.time()
 no_negative_reviews = sum(df.Negative_Review == "No Negative")
 print("Number of No Negative reviews: " + str(no_negative_reviews))

 no_positive_reviews = sum(df["Positive_Review"] == "No Positive")
 print("Number of No Positive reviews: " + str(no_positive_reviews))

 both_no_reviews = sum((df.Negative_Review == "No Negative") & (df.Positive_Review == "No Positive"))
 print("Number of both No Negative and No Positive reviews: " + str(both_no_reviews))

 end = time.time()
 print("Sum took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Calculating and Plotting Running Average Reward Python
DESCRIPTION: Defines a `running_average` function using `np.convolve` to smooth a data series by calculating the average over a specified window size. It then uses this function to calculate the running average of the training rewards and plots the result, providing a clearer view of the learning trend over time.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/8-Reinforcement/2-Gym/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

----------------------------------------

TITLE: Encoding Target Variable 'Color' with Label Encoder - Python
DESCRIPTION: This code imports `LabelEncoder` from `sklearn.preprocessing`, initializes it, and applies it (`fit_transform`) to the 'Color' column of the `pumpkins` DataFrame. The resulting encoded label is then added as a new 'Color' column to the `encoded_features` DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/notebook.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
from sklearn.preprocessing import LabelEncoder
# Encode the 'Color' column using label encoding
label_encoder = LabelEncoder()
encoded_label = label_encoder.fit_transform(pumpkins['Color'])
encoded_pumpkins = encoded_features.assign(Color=encoded_label)
encoded_pumpkins.head()
```

----------------------------------------

TITLE: Calculating Testing MAPE Python
DESCRIPTION: This snippet calculates the Mean Absolute Percentage Error (MAPE) for the testing data predictions. It uses a `mape` function to compare the predicted (`y_test_pred`) and actual (`y_test`) values. The result is multiplied by 100 to present the error as a percentage, indicating the model's performance on unseen data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_23

LANGUAGE: python
CODE:
```
print('MAPE for testing data: ', mape(y_test_pred, y_test)*100, '%')
```

----------------------------------------

TITLE: Calculating One-Step Forecast MAPE Python
DESCRIPTION: Calculates the Mean Absolute Percentage Error (MAPE) specifically for the one-step-ahead forecasts (h='t+1') using a custom `mape` function. Prints the result as a percentage, indicating the average error magnitude relative to the actual values for immediate predictions.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/solution/notebook.ipynb#_snippet_16

LANGUAGE: python
CODE:
```
print('One step forecast MAPE: ', (mape(eval_df[eval_df['h'] == 't+1']['prediction'], eval_df[eval_df['h'] == 't+1']['actual']))*100, '%')
```

----------------------------------------

TITLE: Save Trained Agent Simulation as GIF - Python
DESCRIPTION: Uses the PIL library to capture frames of the trained agent's simulation and save them as an animated GIF. It runs the simulation, renders each frame as an RGB array, appends it to a list, selects actions based on the best trained Q-Table (`Qbest`), and saves the collected frames as a GIF file.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/solution/notebook.ipynb#_snippet_14

LANGUAGE: python
CODE:
```
from PIL import Image
obs = env.reset()
done = False
i=0
ims = []
while not done:
   s = discretize(obs)
   img=env.render(mode='rgb_array')
   ims.append(Image.fromarray(img))
   v = probs(np.array([Qbest.get((s,a),0) for a in actions]))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
   i+=1
env.close()
ims[0].save('images/cartpole-balance.gif',save_all=True,append_images=ims[1::2],loop=0,duration=5)
print(i)
```

----------------------------------------

TITLE: Creating Interactive Scatter Plot in R
DESCRIPTION: Generates a static scatter plot visualizing the relationship between popularity and danceability, with points colored and shaped according to genre. It then converts this static plot into an interactive version using the ggplotly function from the plotly package, allowing for hover information.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb#_snippet_12

LANGUAGE: R
CODE:
```
# A scatter plot of popularity and danceability
scatter_plot <- nigerian_songs %>%
  ggplot(mapping = aes(x = popularity, y = danceability, color = artist_top_genre, shape = artist_top_genre)) +
  geom_point(size = 2, alpha = 0.8) +
  paletteer::scale_color_paletteer_d("futurevisions::mars")

# Add a touch of interactivity
ggplotly(scatter_plot)
```

----------------------------------------

TITLE: Importing Data Analysis Libraries Python
DESCRIPTION: Imports the necessary Python libraries for data manipulation and plotting: matplotlib.pyplot for plotting and pandas for data handling.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/notebook.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
import matplotlib.pyplot as plt
import pandas as pd
```

----------------------------------------

TITLE: Standardizing Hotel Addresses in Pandas DataFrame - Python
DESCRIPTION: Defines a Python function to standardize hotel addresses based on country/city keywords found in the original address string. Applies this function to the 'Hotel_Address' column of a pandas DataFrame and then prints the value counts of the standardized addresses to verify the transformation. Requires a pandas DataFrame named 'df' with a 'Hotel_Address' column.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
def replace_address(row):
    if "Netherlands" in row["Hotel_Address"]:
        return "Amsterdam, Netherlands"
    elif "Barcelona" in row["Hotel_Address"]:
        return "Barcelona, Spain"
    elif "United Kingdom" in row["Hotel_Address"]:
        return "London, United Kingdom"
    elif "Milan" in row["Hotel_Address"]:
        return "Milan, Italy"
    elif "France" in row["Hotel_Address"]:
        return "Paris, France"
    elif "Vienna" in row["Hotel_Address"]:
        return "Vienna, Austria"

# Replace all the addresses with a shortened, more useful form
df["Hotel_Address"] = df.apply(replace_address, axis = 1)
# The sum of the value_counts() should add up to the total number of reviews
print(df["Hotel_Address"].value_counts())
```

----------------------------------------

TITLE: Counting and Structuring Top Genres in R
DESCRIPTION: Counts the occurrences of each unique genre in the dataset, sorts them in descending order, and converts the genre column into a factor ordered by frequency. This prepares the data for genre-based analysis and visualization.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb#_snippet_6

LANGUAGE: R
CODE:
```
# Popular genres
top_genres <- df %>%
  count(artist_top_genre, sort = TRUE) %>%
# Encode to categorical and reorder the according to count
  mutate(artist_top_genre = factor(artist_top_genre) %>%
  fct_inorder())

# Print the top genres
top_genres
```

----------------------------------------

TITLE: Calculate Multi-Step Forecast MAPE in Python
DESCRIPTION: This code calculates and prints the overall Mean Absolute Percentage Error (MAPE) across all predictions and all steps within the `HORIZON`. This provides a single aggregate metric for the multi-step forecasting performance.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/7-TimeSeries/2-ARIMA/README.md#_snippet_7

LANGUAGE: python
CODE:
```
print('Multi-step forecast MAPE: ', mape(eval_df['prediction'], eval_df['actual'])*100, '%')
```

----------------------------------------

TITLE: Creating Tag Feature Columns (Pandas/Python)
DESCRIPTION: This snippet creates new binary columns in the DataFrame `df` based on whether specific important tags are present in the 'Tags' column for each review. It uses `df.Tags.apply()` with a lambda function to check for substring presence and assign 1 if found, 0 otherwise. It also combines 'Group' and 'Travelers with friends'. Requires a DataFrame named `df` with a 'Tags' column.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_0

LANGUAGE: python
CODE:
```
# Process the Tags into new columns
# The file Hotel_Reviews_Tags.py, identifies the most important tags
# Leisure trip, Couple, Solo traveler, Business trip, Group combined with Travelers with friends,
# Family with young children, Family with older children, With a pet
df["Leisure_trip"] = df.Tags.apply(lambda tag: 1 if "Leisure trip" in tag else 0)
df["Couple"] = df.Tags.apply(lambda tag: 1 if "Couple" in tag else 0)
df["Solo_traveler"] = df.Tags.apply(lambda tag: 1 if "Solo traveler" in tag else 0)
df["Business_trip"] = df.Tags.apply(lambda tag: 1 if "Business trip" in tag else 0)
df["Group"] = df.Tags.apply(lambda tag: 1 if "Group" in tag or "Travelers with friends" in tag else 0)
df["Family_with_young_children"] = df.Tags.apply(lambda tag: 1 if "Family with young children" in tag else 0)
df["Family_with_older_children"] = df.Tags.apply(lambda tag: 1 if "Family with older children" in tag else 0)
df["With_a_pet"] = df.Tags.apply(lambda tag: 1 if "With a pet" in tag else 0)
```

----------------------------------------

TITLE: Checking for Any Missing Values in R
DESCRIPTION: Uses the pipe operator ('%>%') to pass the 'pumpkins' dataframe to the 'anyNA()' function. This function returns 'TRUE' if the dataframe contains at least one 'NA' value, and 'FALSE' otherwise, providing a quick check for missing data presence.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/solution/R/lesson_2-R.ipynb#_snippet_2

LANGUAGE: R
CODE:
```
pumpkins %>%
  anyNA()
```

----------------------------------------

TITLE: Defining a Data Preprocessing Recipe in R
DESCRIPTION: This R code defines a data preprocessing 'recipe' using the `tidymodels` package. It specifies the outcome variable (`price`) and predictors (`.`, meaning all others) and adds a step (`step_integer`) to integer-encode all predictor variables, starting from zero. It then prints the defined recipe object.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_7

LANGUAGE: R
CODE:
```
# Specify a recipe
pumpkins_recipe <- recipe(price ~ ., data = new_pumpkins) %>%
  step_integer(all_predictors(), zero_based = TRUE)


# Print out the recipe
pumpkins_recipe
```

----------------------------------------

TITLE: Defining Linear Regression Model Specification in R
DESCRIPTION: Creates a model specification object for linear regression using parsnip. It sets the model type to 'linear_reg', specifies R's built-in 'lm' function as the computational engine, and sets the model mode to 'regression'. The resulting specification object is then printed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/solution/R/lesson_1-R.ipynb#_snippet_5

LANGUAGE: R
CODE:
```
lm_spec <- 
  # Type
  linear_reg() %>%
  # Engine
  set_engine("lm") %>%
  # Mode
  set_mode("regression")


# Print the model specification
lm_spec
```

----------------------------------------

TITLE: Filtering and Plotting Specific Data Subset with Pandas in Python
DESCRIPTION: This code filters the 'new_pumpkins' DataFrame to select only rows where the 'Variety' is 'PIE TYPE'. It then generates a scatter plot of 'DayOfYear' against 'Price' for this specific subset, helping to visualize the relationship for a single variety.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price')
```

----------------------------------------

TITLE: Visualizing Feature Correlations - Seaborn Python
DESCRIPTION: This snippet calculates the pairwise correlation coefficients between numerical columns in the filtered DataFrame using `df.corr()`. It then generates a heatmap visualization of this correlation matrix using seaborn's `heatmap` function, with matplotlib used to control the figure size. The heatmap color intensity indicates the strength and direction of the correlation between features.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/README.md#_snippet_8

LANGUAGE: python
CODE:
```
corrmat = df.corr(numeric_only=True)
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corrmat, vmax=.8, square=True)
```

----------------------------------------

TITLE: Counting Unique Nationalities Python
DESCRIPTION: This snippet determines and prints the total number of unique reviewer nationalities present in the dataset. It uses the `.size` attribute of the index of the `nationality_freq` Series.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/4-Hotel-Reviews-1/solution/notebook.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
# How many unique nationalities are there?
print("There are " + str(nationality_freq.index.size) + " unique nationalities in the dataset")
```

----------------------------------------

TITLE: Translating Text using TextBlob in Python
DESCRIPTION: This snippet demonstrates how to perform machine learning-based translation using the TextBlob library in Python. It creates a TextBlob object with an English sentence and uses the `translate` method to translate it into French. The output is printed to the console, illustrating TextBlob's ability to leverage underlying services like Google Translate for accurate translation.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/3-Translation-Sentiment/README.md#_snippet_0

LANGUAGE: python
CODE:
```
from textblob import TextBlob

blob = TextBlob(
    "It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife!"
)
print(blob.translate(to="fr"))

```

----------------------------------------

TITLE: Getting DataFrame Information Python
DESCRIPTION: Prints a concise summary of the DataFrame, including the index dtype and column dtypes, non-null values, and memory usage. This is useful for quickly understanding the data types and completeness.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/notebook.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
df.info()
```

----------------------------------------

TITLE: Displaying DataFrame Information with Pandas (Python)
DESCRIPTION: This line prints the information about the pumpkins DataFrame, including the index dtype and columns, non-null values, and memory usage. It is used to inspect the structure and data types after cleaning.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/README.md#_snippet_1

LANGUAGE: python
CODE:
```
pumpkins.info
```

----------------------------------------

TITLE: Calculating & Comparing Hotel Average Scores (Pandas/Python)
DESCRIPTION: Defines a helper function to calculate the difference between two scores. It then calculates the mean 'Reviewer_Score' per 'Hotel_Name' using `groupby().transform('mean')`, rounds it, and adds it as 'Calc_Average_Score'. It calculates the difference using `.apply()` with the helper function, drops duplicates to get one row per hotel, sorts, and displays key score columns. Requires a pandas DataFrame `df` with 'Hotel_Name', 'Reviewer_Score', and 'Average_Score' columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_5

LANGUAGE: python
CODE:
```
# define a function that takes a row and performs some calculation with it
def get_difference_review_avg(row):
  return row["Average_Score"] - row["Calc_Average_Score"]

# 'mean' is mathematical word for 'average'
df['Calc_Average_Score'] = round(df.groupby('Hotel_Name').Reviewer_Score.transform('mean'), 1)

# Add a new column with the difference between the two average scores
df["Average_Score_Difference"] = df.apply(get_difference_review_avg, axis = 1)

# Create a df without all the duplicates of Hotel_Name (so only 1 row per hotel)
review_scores_df = df.drop_duplicates(subset = ["Hotel_Name"])

# Sort the dataframe to find the lowest and highest average score difference
review_scores_df = review_scores_df.sort_values(by=["Average_Score_Difference"])

display(review_scores_df[["Average_Score_Difference", "Average_Score", "Calc_Average_Score", "Hotel_Name"]])
```

----------------------------------------

TITLE: Making Full Dataset Predictions and Inverse Scaling Python
DESCRIPTION: This snippet uses the trained model to generate predictions (`Y_pred`) on the prepared full dataset inputs (`X`). It then applies inverse scaling to both the predictions and the actual values (`Y`) using the original scaler. This brings the values back to their original scale for evaluation and plotting.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_25

LANGUAGE: python
CODE:
```
# Make model predictions
Y_pred = model.predict(X).reshape(-1,1)

# Inverse scale and reshape
Y_pred = scaler.inverse_transform(Y_pred)
Y = scaler.inverse_transform(Y)
```

----------------------------------------

TITLE: Combining Encoded Features and Label (Python)
DESCRIPTION: Combines the DataFrame containing the encoded features (`encoded_features`) with a new 'Color' column containing the encoded target variable (`encoded_label`). The result is a new DataFrame `encoded_pumpkins` ready for model training.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/4-Logistic/README.md#_snippet_7

LANGUAGE: Python
CODE:
```
encoded_pumpkins = encoded_features.assign(Color=encoded_label)
```

----------------------------------------

TITLE: Creating Swarm Plot with ggbeeswarm in R
DESCRIPTION: Generates a swarm plot (using geom_quasirandom from ggbeeswarm) to visualize the distribution of the binary 'color' variable with respect to 'item_size'. It factors the color column and uses a color palette from RColorBrewer. Requires ggplot2, dplyr, and ggbeeswarm packages and assumes 'baked_pumpkins' dataframe is available.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/R/lesson_4-R.ipynb#_snippet_7

LANGUAGE: R
CODE:
```
# Create beeswarm plots of color and item_size
baked_pumpkins %>%
  mutate(color = factor(color)) %>%
  ggplot(mapping = aes(x = color, y = item_size, color = color)) +
  geom_quasirandom() +
  scale_color_brewer(palette = "Dark2", direction = -1) +
  theme(legend.position = "none")
```

----------------------------------------

TITLE: Initializing Polynomial Regression Pipeline - Scikit-learn Python
DESCRIPTION: This snippet demonstrates how to create and train a scikit-learn pipeline for polynomial regression. It first transforms the input data to include polynomial features of a specified degree (here, 2) using `PolynomialFeatures` and then fits a `LinearRegression` model to the transformed data. The pipeline acts as a single estimator object that can be trained and used for prediction.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/2-Regression/3-Linear/README.md#_snippet_0

LANGUAGE: python
CODE:
```
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

----------------------------------------

TITLE: Calculating Average Price per Month with dplyr (R)
DESCRIPTION: Groups the `new_pumpkins` dataframe by the 'Month' column using `group_by` from `dplyr`, and then calculates the mean 'Price' for each month using `summarise`, storing the result in a new column 'mean_price'.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/solution/R/lesson_2-R.ipynb#_snippet_14

LANGUAGE: R
CODE:
```
# Find the average price of pumpkins per month
new_pumpkins %>%
  group_by(Month) %>%
  summarise(mean_price = mean(Price))
```

----------------------------------------

TITLE: Plotting Top 5 Artist Genres Python
DESCRIPTION: Imports seaborn and plots a bar chart showing the top 5 most frequent artist genres in the dataset using `seaborn.barplot`. It sets the figure size and rotates x-axis labels for readability.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/notebook.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
import seaborn as sns

top = df['artist_top_genre'].value_counts()
plt.figure(figsize=(10,7))
sns.barplot(x=top[:5].index,y=top[:5].values)
plt.xticks(rotation=45)
plt.title('Top genres',color = 'blue')
```

----------------------------------------

TITLE: Reordering Columns and Saving DataFrame to CSV in Python
DESCRIPTION: This snippet reorders the columns of the pandas DataFrame `df` to a predefined sequence using `df.reindex`. It then saves the modified DataFrame to a CSV file named 'Hotel_Reviews_NLP.csv' in the '../data/' directory, excluding the DataFrame index. Requires a pandas DataFrame containing all columns listed in the reindex call.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_11

LANGUAGE: python
CODE:
```
# Reorder the columns (This is cosmetic, but to make it easier to explore the data later)
df = df.reindex(["Hotel_Name", "Hotel_Address", "Total_Number_of_Reviews", "Average_Score", "Reviewer_Score", "Negative_Sentiment", "Positive_Sentiment", "Reviewer_Nationality", "Leisure_trip", "Couple", "Solo_traveler", "Business_trip", "Group", "Family_with_young_children", "Family_with_older_children", "With_a_pet", "Negative_Review", "Positive_Review"], axis=1)

print("Saving results to Hotel_Reviews_NLP.csv")
df.to_csv(r"../data/Hotel_Reviews_NLP.csv", index = False)
```

----------------------------------------

TITLE: Adding Ensemble Classifiers (Random Forest, AdaBoost) to Dictionary - Scikit-learn Python
DESCRIPTION: This code snippet is intended to be added as entries to the `classifiers` dictionary. It adds two ensemble models: a `RandomForestClassifier` under the key `'RFST'` and an `AdaBoostClassifier` under the key `'ADA'`. Both are initialized with `n_estimators=100`, setting the number of trees for Random Forest and the number of boosting stages for AdaBoost, aiming to improve performance by combining multiple base estimators.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/README.md#_snippet_6

LANGUAGE: Python
CODE:
```
'RFST': RandomForestClassifier(n_estimators=100),
'ADA': AdaBoostClassifier(n_estimators=100)
```

----------------------------------------

TITLE: Plotting Running Average of Training Rewards - Python
DESCRIPTION: Defines the `running_average` function using numpy convolution to smooth a data series over a specified window. It then applies this function to the `rewards` list collected during training and plots the resulting running average using matplotlib. This helps visualize the smoothed training progress and identify trends.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/README.md#_snippet_11

LANGUAGE: Python
CODE:
```
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

----------------------------------------

TITLE: Inspecting Features and Label of a Test Data Row (Python, Pandas)
DESCRIPTION: This snippet demonstrates how to access and print the ingredients (features) and cuisine (label) for a specific row (index 50) from the test dataframes (`X_test`, `y_test`) using Pandas. It filters `X_test` to show only non-zero ingredients using `.iloc` and `.keys()` and accesses the corresponding label from `y_test` using `.iloc`. Requires Pandas.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/README.md#_snippet_6

LANGUAGE: python
CODE:
```
print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')
print(f'cuisine: {y_test.iloc[50]}')
```

----------------------------------------

TITLE: Reshaping Data with pivot_longer R
DESCRIPTION: Reshapes the `df_numeric` dataframe from a wide format to a long format using the `pivot_longer` function. This transformation is done to prepare the data for creating individual box plots for each numeric feature using faceting in `ggplot2`. The first 15 rows of the reshaped data are printed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb#_snippet_3

LANGUAGE: R
CODE:
```
# Pivot data from wide to long
df_numeric_long <- df_numeric %>%
  pivot_longer(!artist_top_genre, names_to = "feature_names", values_to = "values")

# Print out data
df_numeric_long %>%
  slice_head(n = 15)
```

----------------------------------------

TITLE: Visualizing Train/Test Data Split Python
DESCRIPTION: Plots the time series data, highlighting the segments designated for training and testing using the previously defined start dates. This visualization helps confirm the split points and observe the data distribution within each partition.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/solution/notebook.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
energy[(energy.index < test_start_dt) & (energy.index >= train_start_dt)][['load']].rename(columns={'load':'train'}) \
    .join(energy[test_start_dt:][['load']].rename(columns={'load':'test'}), how='outer') \
    .plot(y=['train', 'test'], figsize=(15, 8), fontsize=12)
plt.xlabel('timestamp', fontsize=12)
plt.ylabel('load', fontsize=12)
plt.show()
```

----------------------------------------

TITLE: Filtering Data by Cuisine and Printing Shapes (Pandas Python)
DESCRIPTION: Filters the main `df` DataFrame to create separate DataFrames for specific cuisines (thai, japanese, chinese, indian, korean). It then prints the shape (number of rows and columns) of each newly created DataFrame to show the size of the subset for each cuisine.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/notebook.ipynb#_snippet_7

LANGUAGE: Python
CODE:
```
thai_df = df[(df.cuisine == "thai")];
japanese_df = df[(df.cuisine == "japanese")];
chinese_df = df[(df.cuisine == "chinese")];
indian_df = df[(df.cuisine == "indian")];
korean_df = df[(df.cuisine == "korean")];

print(f'thai df: {thai_df.shape}');
print(f'japanese df: {japanese_df.shape}');
print(f'chinese df: {chinese_df.shape}');
print(f'indian df: {indian_df.shape}');
print(f'korean df: {korean_df.shape}')
```

----------------------------------------

TITLE: Plotting Scatter Data by Category with Pandas/Matplotlib in Python
DESCRIPTION: This code iterates through unique pumpkin varieties in the DataFrame, filters the data for each variety, and plots 'DayOfYear' vs 'Price' on the same scatter plot using different colors and labels for each variety. The 'ax' parameter allows plotting on the same axes.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

----------------------------------------

TITLE: Querying Hotel Count by Standardized Address in Pandas - Python
DESCRIPTION: Groups the pandas DataFrame by the standardized 'Hotel_Address' and aggregates the number of unique hotels ('Hotel_Name') within each address group. Displays the result, showing the count of distinct hotels for each standardized location. Requires a pandas DataFrame 'df' with 'Hotel_Address' and 'Hotel_Name' columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
display(df.groupby("Hotel_Address").agg({"Hotel_Name": "nunique"}))
```

----------------------------------------

TITLE: Calculating Hotel Review Frequencies (Pandas/Python)
DESCRIPTION: Creates a new DataFrame by dropping unnecessary columns from the original `df`. It then uses `groupby('Hotel_Name').transform('count')` to add a column with the count of reviews per hotel and removes duplicate hotel rows for display. Requires a pandas DataFrame `df` with hotel and review columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_4

LANGUAGE: python
CODE:
```
# First create a new dataframe based on the old one, removing the uneeded columns
hotel_freq_df = df.drop(["Hotel_Address", "Additional_Number_of_Scoring", "Review_Date", "Average_Score", "Reviewer_Nationality", "Negative_Review", "Review_Total_Negative_Word_Counts", "Positive_Review", "Review_Total_Positive_Word_Counts", "Total_Number_of_Reviews_Reviewer_Has_Given", "Reviewer_Score", "Tags", "days_since_review", "lat", "lng"], axis = 1)

# Group the rows by Hotel_Name, count them and put the result in a new column Total_Reviews_Found
hotel_freq_df['Total_Reviews_Found'] = hotel_freq_df.groupby('Hotel_Name').transform('count')

# Get rid of all the duplicated rows
hotel_freq_df = hotel_freq_df.drop_duplicates(subset = ["Hotel_Name"])
display(hotel_freq_df)
```

----------------------------------------

TITLE: Plotting Scatter by Category with Pandas/Matplotlib - Python
DESCRIPTION: This code iterates through unique pumpkin varieties in the `new_pumpkins` DataFrame. For each variety, it filters the data and creates a scatter plot of 'Price' against 'DayOfYear' on the same axes (`ax`), using a different color for each variety. This visualizes potential correlations and clustering by variety.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/2-Regression/3-Linear/README.md#_snippet_1

LANGUAGE: python
CODE:
```
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

----------------------------------------

TITLE: Extracting Labels with Pandas Python
DESCRIPTION: Selects the 'cuisine' column from the `cuisines_df` DataFrame and assigns it to a new DataFrame `cuisines_label_df`. This column typically represents the target labels for the classification task. Displays the first few rows of the new label DataFrame. Requires the `cuisines_df` DataFrame from the previous step.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/notebook.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
cuisines_label_df = cuisines_df['cuisine']
cuisines_label_df.head()
```

----------------------------------------

TITLE: Calculating and Visualizing Correlation Matrix in R
DESCRIPTION: Selects only the numeric columns from the filtered dataset, calculates the correlation matrix, and visualizes it using the corrplot function. This helps identify linear relationships between numerical features.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb#_snippet_10

LANGUAGE: R
CODE:
```
# Narrow down to numeric variables and fid correlation
corr_mat <- nigerian_songs %>%
  select(where(is.numeric)) %>%
  cor()

# Visualize correlation matrix
corrplot(corr_mat, order = 'AOE', col = c('white', 'black'), bg = 'gold2')
```

----------------------------------------

TITLE: Plotting Top Chinese Ingredients (Pandas Matplotlib Python)
DESCRIPTION: Uses the `create_ingredient_df` function to analyze the Chinese cuisine DataFrame (`chinese_df`), selects the top 10 ingredients by count, and generates a horizontal bar plot to visualize their frequency.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/notebook.ipynb#_snippet_11

LANGUAGE: Python
CODE:
```
chinese_ingredient_df = create_ingredient_df(chinese_df)
chinese_ingredient_df.head(10).plot.barh()
```

----------------------------------------

TITLE: Plotting Top Indian Ingredients (Pandas Matplotlib Python)
DESCRIPTION: Uses the `create_ingredient_df` function to analyze the Indian cuisine DataFrame (`indian_df`), selects the top 10 ingredients by count, and generates a horizontal bar plot to visualize their frequency.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/notebook.ipynb#_snippet_12

LANGUAGE: Python
CODE:
```
indian_ingredient_df = create_ingredient_df(indian_df)
indian_ingredient_df.head(10).plot.barh()
```

----------------------------------------

TITLE: Counting Review Categories using Pandas Boolean Indexing and Sum (Python)
DESCRIPTION: This Python snippet provides an alternative, more efficient way to count rows in a pandas DataFrame `df` where specific column values match conditions ("No Negative" or "No Positive"). It uses boolean indexing to create boolean Series and `sum()` to count the `True` values. It also measures the execution time using `time.time()`. Requires pandas and the `time` module.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_2

LANGUAGE: python
CODE:
```
# without lambdas (using a mixture of notations to show you can use both)
start = time.time()
no_negative_reviews = sum(df.Negative_Review == "No Negative")
print("Number of No Negative reviews: " + str(no_negative_reviews))

no_positive_reviews = sum(df["Positive_Review"] == "No Positive")
print("Number of No Positive reviews: " + str(no_positive_reviews))

both_no_reviews = sum((df.Negative_Review == "No Negative") & (df.Positive_Review == "No Positive"))
print("Number of both No Negative and No Positive reviews: " + str(both_no_reviews))

end = time.time()
print("Sum took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Train Workflow (Tidymodels/workflows) - R
DESCRIPTION: Fits the created workflow (`mr_wf`) to the training data (`cuisines_train`). This process applies the recipe transformations to the training data and then trains the specified multinomial regression model on the transformed data, resulting in a fitted workflow object (`mr_fit`).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/solution/R/lesson_11-R.ipynb#_snippet_6

LANGUAGE: R
CODE:
```
# Train a multinomial regression model
mr_fit <- fit(object = mr_wf, data = cuisines_train)

mr_fit
```

----------------------------------------

TITLE: Getting Transformed DataFrame Information (Pandas Python)
DESCRIPTION: Prints a summary of the `transformed_df` DataFrame, which contains the combined balanced features and labels. This verifies the structure, data types, and completeness of the final preprocessed dataset.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/notebook.ipynb#_snippet_19

LANGUAGE: Python
CODE:
```
transformed_df.info()
```

----------------------------------------

TITLE: Visualizing Feature Distribution with Boxplots (Python)
DESCRIPTION: This snippet generates a grid of boxplots for selected numerical columns of the input DataFrame `df`. It uses `matplotlib.pyplot` for setting up the figure and subplots, and `seaborn` for drawing the boxplots for each specified feature ('popularity', 'acousticness', etc.). The purpose is to visualize data distribution and identify potential outliers in each feature column.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/translations/README.ko.md#_snippet_0

LANGUAGE: Python
CODE:
```
plt.figure(figsize=(20,20), dpi=200)

plt.subplot(4,3,1)
sns.boxplot(x = 'popularity', data = df)

plt.subplot(4,3,2)
sns.boxplot(x = 'acousticness', data = df)

plt.subplot(4,3,3)
sns.boxplot(x = 'energy', data = df)

plt.subplot(4,3,4)
sns.boxplot(x = 'instrumentalness', data = df)

plt.subplot(4,3,5)
sns.boxplot(x = 'liveness', data = df)

plt.subplot(4,3,6)
sns.boxplot(x = 'loudness', data = df)

plt.subplot(4,3,7)
sns.boxplot(x = 'speechiness', data = df)

plt.subplot(4,3,8)
sns.boxplot(x = 'tempo', data = df)

plt.subplot(4,3,9)
sns.boxplot(x = 'time_signature', data = df)

plt.subplot(4,3,10)
sns.boxplot(x = 'danceability', data = df)

plt.subplot(4,3,11)
sns.boxplot(x = 'length', data = df)

plt.subplot(4,3,12)
sns.boxplot(x = 'release_date', data = df)
```

----------------------------------------

TITLE: Removing Stop Words from Text Columns - Python
DESCRIPTION: Defines a function to remove common English stop words from a given text string using a pre-cached set of stop words. It then applies this function to the 'Negative_Review' and 'Positive_Review' columns of the DataFrame to clean the text data, potentially speeding up subsequent NLP tasks.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
from nltk.corpus import stopwords

# Load the hotel reviews from CSV
df = pd.read_csv("../../data/Hotel_Reviews_Filtered.csv")

# Remove stop words - can be slow for a lot of text!
# Ryan Han (ryanxjhan on Kaggle) has a great post measuring performance of different stop words removal approaches
# https://www.kaggle.com/ryanxjhan/fast-stop-words-removal # using the approach that Ryan recommends
start = time.time()
cache = set(stopwords.words("english"))
def remove_stopwords(review):
    text = " ".join([word for word in review.split() if word not in cache])
    return text

# Remove the stop words from both columns
df.Negative_Review = df.Negative_Review.apply(remove_stopwords)   
df.Positive_Review = df.Positive_Review.apply(remove_stopwords)
```

----------------------------------------

TITLE: Implement Simple State Discretization Function - Python
DESCRIPTION: Defines a simple function `discretize` that takes a continuous observation array and converts it into a tuple of integers by dividing each element by predefined scaling factors and casting to integer type. This allows mapping continuous states to discrete states required for a Q-Table.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/solution/notebook.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
def discretize(x):
    return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
```

----------------------------------------

TITLE: Applying Recipe and Displaying Preprocessed Data in R
DESCRIPTION: Applies the `cuisines_recipe` using `prep()` to estimate parameters required for the steps (like SMOTE) and then `bake()` to apply these steps to the data (`new_data = NULL` applies it to the data originally used in `recipe()`). The resulting dataframe `preprocessed_df` has the `cuisine` column relocated to the first position, and its head and summary statistics are displayed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb#_snippet_16

LANGUAGE: R
CODE:
```
# Prep and bake the recipe
preprocessed_df <- cuisines_recipe %>% 
  prep() %>% 
  bake(new_data = NULL) %>% 
  relocate(cuisine)

# Display data
preprocessed_df %>% 
  slice_head(n = 5)

# Quick summary stats
preprocessed_df %>% 
  introduce()
```

----------------------------------------

TITLE: Encoding Categorical Data - Country (Python)
DESCRIPTION: Imports the `LabelEncoder` class from scikit-learn's `preprocessing` module. It applies `LabelEncoder` to the 'Country' column to convert the textual country names into numerical labels. This is a common step to prepare categorical data for machine learning models. It then displays the first few rows to show the effect of the encoding.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/3-Web-App/1-Web-App/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
from sklearn.preprocessing import LabelEncoder

ufos['Country'] = LabelEncoder().fit_transform(ufos['Country'])

ufos.head()
```

----------------------------------------

TITLE: Inverse Transforming Predictions (Python)
DESCRIPTION: Reverts the scaling applied earlier to the model's predictions for both the training and testing sets. This transforms the predictions from the scaled range [0, 1] back to the original scale of the energy load data for evaluation and interpretation.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_17

LANGUAGE: python
CODE:
```
y_train_pred = scaler.inverse_transform(y_train_pred)
y_test_pred = scaler.inverse_transform(y_test_pred)
```

----------------------------------------

TITLE: Implementing and Evaluating Probabilistic Q-Based Policy Python
DESCRIPTION: Defines a policy `qpolicy` that selects an action probabilistically based on the Q-values for the current state, using the `probs` helper function. It then uses the `print_statistics` function to evaluate the performance of this Q-based policy over multiple episodes.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/notebook.ipynb#_snippet_13

LANGUAGE: Python
CODE:
```
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

----------------------------------------

TITLE: Comparing Label Counts Before and After SMOTE (Pandas Python)
DESCRIPTION: Prints the value counts for the labels after applying SMOTE (`transformed_label_df`) and compares them to the original label counts (`df.cuisine.value_counts()`). This confirms that SMOTE has successfully balanced the class distribution.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/notebook.ipynb#_snippet_16

LANGUAGE: Python
CODE:
```
print(f'new label count: {transformed_label_df.value_counts()}')
print(f'old label count: {df.cuisine.value_counts()}')
```

----------------------------------------

TITLE: Plotting Top Thai Ingredients Bar Chart in R
DESCRIPTION: Takes the dataframe containing the top 10 Thai ingredients and uses the `ggplot2` library to create a horizontal bar chart. The chart visualizes the frequency (`n_instances`) of each ingredient against the ingredient name, using `geom_bar` with `stat = "identity"` to represent counts directly.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb#_snippet_8

LANGUAGE: R
CODE:
```
# Make a bar chart for popular thai cuisines
thai_ingredient_df %>% 
  slice_head(n = 10) %>% 
  ggplot(aes(x = n_instances, y = ingredients)) +
  geom_bar(stat = "identity", width = 0.5, fill = "steelblue") +
  xlab("") + ylab("")
```

----------------------------------------

TITLE: Creating a Tidymodels Workflow in R
DESCRIPTION: This code creates a `workflow` object, which bundles together a preprocessing recipe and a model specification. This is a convenient way to manage the modeling pipeline, ensuring that the data preprocessing steps are automatically applied before the model is trained and used for prediction. It then prints the workflow object.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_12

LANGUAGE: R
CODE:
```
# Hold modelling components in a workflow
lm_wf <- workflow() %>%
  add_recipe(lm_pumpkins_recipe) %>%
  add_model(lm_spec)

# Print out the workflow
lm_wf
```

----------------------------------------

TITLE: Displaying DataFrame Head - Pandas - Python
DESCRIPTION: Prints the first five rows of the current state of the DataFrame `df` to the console or notebook output. This snippet is used to quickly inspect the structure and content of the DataFrame after previous data manipulation steps like filtering.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/solution/tester.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
df.head()
```

----------------------------------------

TITLE: Plot Running Average of Training Rewards - Python
DESCRIPTION: Defines a helper function `running_average` to calculate a moving average over a list. It then plots the running average of the training rewards, which provides a smoother curve and better visualizes the overall learning trend over time.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/solution/notebook.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

----------------------------------------

TITLE: Loading Filtered Data for Sentiment Analysis (Python)
DESCRIPTION: This snippet imports necessary libraries for data processing (pandas) and NLP/sentiment analysis (nltk, vader). It downloads the 'vader_lexicon' if not already present and loads the `Hotel_Reviews_Filtered.csv` dataset into a pandas DataFrame named `df`. It includes placeholders for further code and saves the final DataFrame as `Hotel_Reviews_NLP.csv`. Requires the specified libraries and the `Hotel_Reviews_Filtered.csv` file.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_2

LANGUAGE: python
CODE:
```
import time
import pandas as pd
import nltk as nltk
from nltk.corpus import stopwords
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

# Load the filtered hotel reviews from CSV
df = pd.read_csv('../../data/Hotel_Reviews_Filtered.csv')

# You code will be added here


# Finally remember to save the hotel reviews with new NLP data added
print("Saving results to Hotel_Reviews_NLP.csv")
df.to_csv(r'../data/Hotel_Reviews_NLP.csv', index = False)
```

----------------------------------------

TITLE: Plotting Mean Price by Category with Pandas in Python
DESCRIPTION: This snippet calculates the mean 'Price' for each unique 'Variety' using Pandas groupby() and mean(), then visualizes the results as a bar chart using the .plot(kind='bar') method to show price differences between varieties.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

----------------------------------------

TITLE: Analyzing and Plotting Top Korean Ingredients - Python
DESCRIPTION: Uses the `create_ingredient_df` function with the Korean cuisine data to calculate ingredient counts. It then selects the top 10 ingredients from the resulting dataframe and plots them using a horizontal bar chart. Requires the function and the Korean data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/README.md#_snippet_12

LANGUAGE: python
CODE:
```
korean_ingredient_df = create_ingredient_df(korean_df)
korean_ingredient_df.head(10).plot.barh()
```

----------------------------------------

TITLE: Calculating Correlation - Pandas Python
DESCRIPTION: Calculates and prints the Pearson correlation coefficient between the 'Month' and 'Price', and 'DayOfYear' and 'Price' columns in the `new_pumpkins` DataFrame using the `.corr()` method. This helps identify the strength and direction of linear relationships between variables. Requires the pandas library and a DataFrame named `new_pumpkins`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/2-Regression/3-Linear/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

----------------------------------------

TITLE: Dropping Columns from Cuisine Data in R
DESCRIPTION: Filters the original dataframe `df` using `dplyr::select` to remove the first column (assumed to be an ID) and specific common ingredient columns ('rice', 'garlic', 'ginger') that are less useful for distinguishing cuisines. Displays the first 5 rows of the resulting dataframe `df_select`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb#_snippet_13

LANGUAGE: R
CODE:
```
# Drop id column, rice, garlic and ginger from our original data set
df_select <- df %>% 
  select(-c(1, rice, garlic, ginger))

# Display new data set
df_select %>% 
  slice_head(n = 5)
```

----------------------------------------

TITLE: Structure Predictions and Actuals for Evaluation in Python
DESCRIPTION: This snippet transforms the prediction list into a pandas DataFrame, assigns columns based on the forecast step (t+1, t+2, etc.), and melts the DataFrame for easier comparison. It then adds the corresponding actual values from the prepared `test_ts` DataFrame and applies an inverse transform using a `scaler` (presumably fitted during data preprocessing) to bring the values back to their original scale. The resulting evaluation DataFrame is then displayed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/README.md#_snippet_13

LANGUAGE: python
CODE:
```
eval_df = pd.DataFrame(predictions, columns=['t+'+str(t) for t in range(1, HORIZON+1)])
eval_df['timestamp'] = test.index[0:len(test.index)-HORIZON+1]
eval_df = pd.melt(eval_df, id_vars='timestamp', value_name='prediction', var_name='h')
eval_df['actual'] = np.array(np.transpose(test_ts)).ravel()
eval_df[['prediction', 'actual']] = scaler.inverse_transform(eval_df[['prediction', 'actual']])
eval_df.head()
```

----------------------------------------

TITLE: Creating Ingredient DataFrame - Python
DESCRIPTION: Creates a dataframe summarizing ingredient counts from a wider dataframe. It drops specified columns, transposes, sums rows (ingredients), filters out zero counts, and sorts by value in descending order. Requires a pandas DataFrame as input.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/README.md#_snippet_7

LANGUAGE: python
CODE:
```
def create_ingredient_df(df):
    ingredient_df = df.T.drop(['cuisine','Unnamed: 0']).sum(axis=1).to_frame('value')
    ingredient_df = ingredient_df[(ingredient_df.T != 0).any()]
    ingredient_df = ingredient_df.sort_values(by='value', ascending=False,
    inplace=False)
    return ingredient_df
```

----------------------------------------

TITLE: Displaying Pumpkin Data Info (Python)
DESCRIPTION: Prints summary information about the `pumpkins` DataFrame, including the index dtype and column dtypes, non-null values, and memory usage. It helps verify the success of the cleaning steps and understand the data structure.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/4-Logistic/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
pumpkins.info
```

----------------------------------------

TITLE: Define Simple Observation Discretization Function - Python
DESCRIPTION: Defines a function `discretize` that takes a continuous observation vector `x` and scales its components by predefined factors, casting the result to integers to create a discrete state representation as a tuple.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/8-Reinforcement/2-Gym/README.md#_snippet_5

LANGUAGE: Python
CODE:
```
def discretize(x):
    return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
```

----------------------------------------

TITLE: Filtering and Visualizing All Genres - Pandas Seaborn Python
DESCRIPTION: This snippet filters the DataFrame to remove rows where the 'artist_top_genre' is 'Missing'. It then recounts the remaining genre values and generates a bar plot displaying the counts of all classified genres. The plot uses matplotlib and seaborn for visualization, including setting figure size, rotating x-axis labels, and adding a title.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/README.md#_snippet_6

LANGUAGE: python
CODE:
```
df = df[df['artist_top_genre'] != 'Missing']
top = df['artist_top_genre'].value_counts()
plt.figure(figsize=(10,7))
sns.barplot(x=top.index,y=top.values)
plt.xticks(rotation=45)
plt.title('Top genres',color = 'blue')
```

----------------------------------------

TITLE: Counting and Filtering Tags with Pandas
DESCRIPTION: Calculates the frequency of each tag in the 'value' column of the `df_tags` DataFrame using `value_counts()`. It then filters out tags containing specific keywords (e.g., 'room', 'Stayed') and drops tags with a count less than 1000. The resulting filtered tag counts are stored in `tag_vc`, and the top 10 are printed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/2-notebook.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
# Get the value counts
tag_vc = df_tags.value.value_counts()
# print(tag_vc)
print("The shape of the tags with no filtering:", str(df_tags.shape))
# Drop rooms, suites, and length of stay, mobile device and anything with less count than a 1000
df_tags = df_tags[~df_tags.value.str.contains("Standard|room|Stayed|device|Beds|Suite|Studio|King|Superior|Double", na=False, case=False)]
tag_vc = df_tags.value.value_counts().reset_index(name="count").query("count > 1000")
# Print the top 10 (there should only be 9 and we'll use these in the filtering section)
print(tag_vc[:10])
```

----------------------------------------

TITLE: Plotting Training Rewards using Matplotlib in Python
DESCRIPTION: This snippet uses the Matplotlib library to create a simple plot of the rewards obtained during each training episode. It visualizes the raw performance per episode, which can show variability due to the stochastic nature of reinforcement learning.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
plt.plot(rewards)
```

----------------------------------------

TITLE: Plotting Testing Data Predictions (Python)
DESCRIPTION: Creates a plot to visualize the model's predictions on the testing data compared to the actual testing values. Includes labels for the legend and axis.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_21

LANGUAGE: python
CODE:
```
plt.figure(figsize=(10,3))
# plot original output
# plot predicted output
plt.legend(['Actual','Predicted'])
plt.xlabel('Timestamp')
plt.show()
```

----------------------------------------

TITLE: Plotting WCSS for R Elbow Method
DESCRIPTION: Using the results from running K-Means with multiple 'k' values, this code generates an elbow plot. The plot shows the Within Cluster Sum of Squares (WCSS) against the number of clusters (k), allowing visual identification of the 'elbow' point where the rate of decrease in WCSS slows significantly, suggesting an optimal k.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb#_snippet_11

LANGUAGE: R
CODE:
```
set.seed(2056)
# Use elbow method to determine optimum number of clusters
kclusts %>% 
  ggplot(mapping = aes(x = k, y = tot.withinss)) +
  geom_line(size = 1.2, alpha = 0.8, color = "#FF7F0EFF") +
  geom_point(size = 2, color = "#FF7F0EFF")
```

----------------------------------------

TITLE: Train Polynomial Regression Model (R)
DESCRIPTION: Defines a data preprocessing recipe for polynomial regression, including integer encoding for predictors and applying a 4th-degree polynomial transformation to all predictors. It specifies a linear regression model engine (`lm`) and bundles the recipe and model spec into a tidymodels workflow, which is then fitted to the training data (`pumpkins_train`).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_17

LANGUAGE: R
CODE:
```
# Specify a recipe
poly_pumpkins_recipe <-
  recipe(price ~ package, data = pumpkins_train) %>%
  step_integer(all_predictors(), zero_based = TRUE) %>%
  step_poly(all_predictors(), degree = 4)


# Create a model specification
poly_spec <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")


# Bundle recipe and model spec into a workflow
poly_wf <- workflow() %>%
  add_recipe(poly_pumpkins_recipe) %>%
  add_model(poly_spec)


# Create a model
poly_wf_fit <- poly_wf %>%
  fit(data = pumpkins_train)


# Print learned model coefficients
poly_wf_fit
```

----------------------------------------

TITLE: Creating Scatter Plot with Matplotlib (Python)
DESCRIPTION: Extracts 'Price' and 'Month' data from a DataFrame (presumably named `new_pumpkins`). Uses Matplotlib's `plt.scatter()` function to create a scatter plot showing the relationship between price and month, then displays the plot using `plt.show()`. This visualization helps in seeing the distribution of data points.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/README.md#_snippet_8

LANGUAGE: Python
CODE:
```
price = new_pumpkins.Price
month = new_pumpkins.Month
plt.scatter(price, month)
plt.show()
```

----------------------------------------

TITLE: Visualizing Price vs Month Relationship - Python
DESCRIPTION: Generates a scatter plot using matplotlib (via pandas plotting interface) to visualize the relationship between the processed pumpkin 'Price' and the 'Month' of sale. Helps identify potential trends based on the time of year.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/notebook.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
new_pumpkins.plot.scatter('Month','Price')
```

----------------------------------------

TITLE: Visualizing Correlation Matrix in R
DESCRIPTION: This snippet demonstrates how to visualize the correlation matrix of the preprocessed data using the `corrplot` package. It first loads the package, calculates the correlation matrix excluding non-informative columns, and then generates a shade-based correlation plot.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_10

LANGUAGE: R
CODE:
```
# Load the corrplot package
library(corrplot)

# Obtain correlation matrix
corr_mat <- cor(baked_pumpkins %>%
                  # Drop columns that are not really informative
                  select(-c(low_price, high_price)))

# Make a correlation plot between the variables
corrplot(corr_mat, method = "shade", shade.col = NA, tl.col = "black", tl.srt = 45, addCoef.col = "black", cl.pos = "n", order = "original")
```

----------------------------------------

TITLE: Load and Inspect Cuisine Dataset Pandas Python
DESCRIPTION: Reads the cleaned cuisine dataset from a CSV file into a pandas DataFrame. It then displays the first few rows of the DataFrame to allow for a quick inspection of the loaded data structure and content.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/README.md#_snippet_1

LANGUAGE: python
CODE:
```
data = pd.read_csv('../data/cleaned_cuisines.csv')
data.head()
```

----------------------------------------

TITLE: Running Trained Q-Learning Agent Simulation (Python)
DESCRIPTION: Initializes a new environment episode, then enters a loop where it discretizes the current observation, retrieves Q-values, chooses an action probabilistically based on Q-values using the `probs` function, steps the environment, and renders the simulation until the episode is done. Finally, it closes the environment rendering. Requires `env`, `discretize`, `qvalues`, `probs`, `actions`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/8-Reinforcement/2-Gym/README.md#_snippet_4

LANGUAGE: python
CODE:
```
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

----------------------------------------

TITLE: Initializing CartPole Environment in Gym Python
DESCRIPTION: Initializes the 'CartPole-v1' environment using OpenAI Gym. It then prints details about the environment's action space (possible actions) and observation space (structure of state information), and demonstrates how to sample a random action from the action space.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/README.md#_snippet_1

LANGUAGE: python
CODE:
```
env = gym.make("CartPole-v1")
print(env.action_space)
print(env.observation_space)
print(env.action_space.sample())
```

----------------------------------------

TITLE: Reordering DataFrame Columns Python
DESCRIPTION: Rearranges the columns of the DataFrame to a predefined order using the `reindex` method. This step organizes the columns for better readability and easier exploration of the processed data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/3-notebook.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
# Reorder the columns (This is cosmetic, but to make it easier to explore the data later)
df = df.reindex(["Hotel_Name", "Hotel_Address", "Total_Number_of_Reviews", "Average_Score", "Reviewer_Score", "Negative_Sentiment", "Positive_Sentiment", "Reviewer_Nationality", "Leisure_trip", "Couple", "Solo_traveler", "Business_trip", "Group", "Family_with_young_children", "Family_with_older_children", "With_a_pet", "Negative_Review", "Positive_Review"], axis=1)

```

----------------------------------------

TITLE: Importing Libraries for ML Model - Python
DESCRIPTION: Imports necessary Python libraries for data science and machine learning. Includes matplotlib.pyplot for plotting, numpy for numerical operations, and specific modules (datasets, linear_model, model_selection) from sklearn for handling datasets, models, and data splitting. These are required dependencies for the subsequent code.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/2-Regression/1-Tools/README.md#_snippet_0

LANGUAGE: python
CODE:
```
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model, model_selection
```

----------------------------------------

TITLE: Filtering and Plotting Specific Variety Data - Python
DESCRIPTION: This code filters the `new_pumpkins` DataFrame to create a new DataFrame `pie_pumpkins` containing only entries where the 'Variety' is 'PIE TYPE'. It then generates a scatter plot of 'Price' against 'DayOfYear' specifically for this subset of data, visualizing the price trend for pie pumpkins over the year.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/2-Regression/3-Linear/README.md#_snippet_3

LANGUAGE: python
CODE:
```
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price')
```

----------------------------------------

TITLE: Run Trained Agent Simulation - Python
DESCRIPTION: Runs a simulation using the trained Q-Table (`Q` or potentially `Qbest` if saved) to select actions. It resets the environment, enters a loop, renders the state, gets the discretized state, calculates action probabilities based on Q-values using the `probs` function, samples an action, and steps the environment until done. This demonstrates the agent's learned policy.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/solution/notebook.ipynb#_snippet_13

LANGUAGE: python
CODE:
```
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

----------------------------------------

TITLE: Plotting Original vs Scaled Training Data Histograms Python
DESCRIPTION: Generates histograms to visually compare the distribution of the 'load' data in the training set before and after applying the MinMaxScaler. This helps understand the effect of scaling on the data's value distribution.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/solution/notebook.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
energy[(energy.index >= train_start_dt) & (energy.index < test_start_dt)][['load']].rename(columns={'load':'original load'}).plot.hist(bins=100, fontsize=12)
train.rename(columns={'load':'scaled load'}).plot.hist(bins=100, fontsize=12)
plt.show()
```

----------------------------------------

TITLE: Defining Data Preprocessing Recipe with SMOTE in R
DESCRIPTION: Loads the `themis` package for handling imbalanced data. It then creates a `tidymodels` recipe specifying that the `cuisine` column is the outcome variable and all other columns are predictors. The recipe includes a `step_smote(cuisine)` step, which will apply the SMOTE algorithm to the training data during preprocessing to oversample the minority cuisine classes, addressing data imbalance. Requires `tidymodels` and `themis` packages.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/solution/R/lesson_12-R.ipynb#_snippet_3

LANGUAGE: R
CODE:
```
# Load themis package for dealing with imbalanced data
library(themis)

# Create a recipe for preprocessing training data
cuisines_recipe <- recipe(cuisine ~ ., data = cuisines_train) %>%
  step_smote(cuisine) 

# Print recipe
cuisines_recipe
```

----------------------------------------

TITLE: Identifying Top Nationalities (Pandas) - Python
DESCRIPTION: Building on the nationality frequency calculation, this snippet prints the most frequent nationality and its count. It then displays the next top 10 most frequently found nationalities and their corresponding frequency counts by slicing the frequency Series.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_3

LANGUAGE: python
CODE:
```
print("The highest frequency reviewer nationality is " + str(nationality_freq.index[0]).strip() + " with " + str(nationality_freq[0]) + " reviews.")
# Notice there is a leading space on the values, strip() removes that for printing
# What is the top 10 most common nationalities and their frequencies?
print("The next 10 highest frequency reviewer nationalities are:")
print(nationality_freq[1:11].to_string())
```

----------------------------------------

TITLE: Combining Test Data and Predictions in R
DESCRIPTION: Uses bind_cols() from dplyr with the pipe operator (%>%) to horizontally combine the original diabetes_test data frame with the predictions data frame. This creates a new data frame (results) that includes the original test features, actual outcomes, and the predicted outcomes for easier comparison and analysis. The first 5 rows of this combined data frame are displayed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/solution/R/lesson_1-R.ipynb#_snippet_8

LANGUAGE: R
CODE:
```
# Combine the predictions and the original test set
results <- diabetes_test %>%
  bind_cols(predictions)


results %>%
  slice(1:5)
```

----------------------------------------

TITLE: Filtering by Specific Genres and Popularity - Pandas Seaborn Python
DESCRIPTION: This code filters the DataFrame further to include only songs belonging to 'afro dancehall', 'afropop', or 'nigerian pop' genres, and additionally removes songs with a popularity score of 0. It then recounts the remaining genre values and plots their distribution using a seaborn bar plot, adjusting figure size, rotating labels, and adding a title via matplotlib.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/README.md#_snippet_7

LANGUAGE: python
CODE:
```
df = df[(df['artist_top_genre'] == 'afro dancehall') | (df['artist_top_genre'] == 'afropop') | (df['artist_top_genre'] == 'nigerian pop')]
df = df[(df['popularity'] > 0)]
top = df['artist_top_genre'].value_counts()
plt.figure(figsize=(10,7))
sns.barplot(x=top.index,y=top.values)
plt.xticks(rotation=45)
plt.title('Top genres',color = 'blue')
```

----------------------------------------

TITLE: Calculating and Comparing Hotel Average Scores (Pandas) - Python
DESCRIPTION: This snippet calculates the average 'Reviewer_Score' for each hotel, rounds it, and adds it as a new column 'Calc_Average_Score' using '.groupby()' and '.transform('mean')'. It then calculates the difference between this calculated average and the existing 'Average_Score' column using a function applied row-wise with '.apply()', and displays the results sorted by the difference.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_6

LANGUAGE: python
CODE:
```
# define a function that takes a row and performs some calculation with it
def get_difference_review_avg(row):
  return row["Average_Score"] - row["Calc_Average_Score"]

# 'mean' is mathematical word for 'average'
df['Calc_Average_Score'] = round(df.groupby('Hotel_Name').Reviewer_Score.transform('mean'), 1)

# Add a new column with the difference between the two average scores
df["Average_Score_Difference"] = df.apply(get_difference_review_avg, axis = 1)

# Create a df without all the duplicates of Hotel_Name (so only 1 row per hotel)
review_scores_df = df.drop_duplicates(subset = ["Hotel_Name"])

# Sort the dataframe to find the lowest and highest average score difference
review_scores_df = review_scores_df.sort_values(by=["Average_Score_Difference"])

display(review_scores_df[["Average_Score_Difference", "Average_Score", "Calc_Average_Score", "Hotel_Name"]])
```

----------------------------------------

TITLE: Creating Tag Indicator Features (Python)
DESCRIPTION: This snippet adds new binary columns to a pandas DataFrame based on the presence of specific tags in the 'Tags' column. It checks for exact tag strings and combines 'Group' with 'Travelers with friends'. This process transforms categorical tag information into numerical features suitable for machine learning models. Requires a pandas DataFrame named `df` with a 'Tags' column.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_0

LANGUAGE: python
CODE:
```
# Process the Tags into new columns
# The file Hotel_Reviews_Tags.py, identifies the most important tags
# Leisure trip, Couple, Solo traveler, Business trip, Group combined with Travelers with friends, 
# Family with young children, Family with older children, With a pet
df["Leisure_trip"] = df.Tags.apply(lambda tag: 1 if "Leisure trip" in tag else 0)
df["Couple"] = df.Tags.apply(lambda tag: 1 if "Couple" in tag else 0)
df["Solo_traveler"] = df.Tags.apply(lambda tag: 1 if "Solo traveler" in tag else 0)
df["Business_trip"] = df.Tags.apply(lambda tag: 1 if "Business trip" in tag else 0)
df["Group"] = df.Tags.apply(lambda tag: 1 if "Group" in tag or "Travelers with friends" in tag else 0)
df["Family_with_young_children"] = df.Tags.apply(lambda tag: 1 if "Family with young children" in tag else 0)
df["Family_with_older_children"] = df.Tags.apply(lambda tag: 1 if "Family with older children" in tag else 0)
df["With_a_pet"] = df.Tags.apply(lambda tag: 1 if "With a pet" in tag else 0)
```

----------------------------------------

TITLE: Initialize and Inspect CartPole Environment - Python
DESCRIPTION: Initializes the 'CartPole-v1' environment from OpenAI Gym and prints details about its action space (possible actions) and observation space (structure of observations). It also demonstrates sampling a random action from the action space.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/8-Reinforcement/2-Gym/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
env = gym.make("CartPole-v1")
print(env.action_space)
print(env.observation_space)
print(env.action_space.sample())
```

----------------------------------------

TITLE: Importing Libraries - Data Loading & Plotting - Python
DESCRIPTION: Imports standard libraries like `os` and `matplotlib.pyplot` for file operations and plotting. It also imports `load_data` from a custom `common.utils` module to handle data loading. The `%matplotlib inline` command is used to ensure plots are displayed directly within the notebook interface.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/7-TimeSeries/1-Introduction/README.md#_snippet_0

LANGUAGE: python
CODE:
```
import os
import matplotlib.pyplot as plt
from common.utils import load_data
%matplotlib inline
```

----------------------------------------

TITLE: Importing ML Libraries (Scikit-learn, Numpy)
DESCRIPTION: Imports essential machine learning libraries and functions. This includes LogisticRegression and SVC for model algorithms, train_test_split and cross_val_score for model selection techniques, various metrics from sklearn.metrics for model evaluation, and the numpy library for numerical operations. Requires scikit-learn and numpy installed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/README.md#_snippet_1

LANGUAGE: python
CODE:
```
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
from sklearn.svm import SVC
import numpy as np
```

----------------------------------------

TITLE: Calculating and printing shape of cuisine-specific DataFrames
DESCRIPTION: Filters the main DataFrame to create subsets for specific cuisines (thai, japanese, chinese, indian, korean). It then prints the shape (number of rows and columns) of each subset, showing the exact count of data points available for each cuisine.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/README.md#_snippet_6

LANGUAGE: python
CODE:
```
thai_df = df[(df.cuisine == "thai")])
japanese_df = df[(df.cuisine == "japanese")])
chinese_df = df[(df.cuisine == "chinese")])
indian_df = df[(df.cuisine == "indian")])
korean_df = df[(df.cuisine == "korean")])

print(f'thai df: {thai_df.shape}')
print(f'japanese df: {japanese_df.shape}')
print(f'chinese df: {chinese_df.shape}')
print(f'indian df: {indian_df.shape}')
print(f'korean df: {korean_df.shape}')
```

----------------------------------------

TITLE: Importing Required Libraries for SVR Time Series
DESCRIPTION: This snippet imports essential libraries for data manipulation, visualization, machine learning, and utility functions. It includes pandas, numpy, matplotlib, scikit-learn's SVR and MinMaxScaler, and custom utility functions for loading data and calculating MAPE.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
import os
import warnings
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import datetime as dt
import math

from sklearn.svm import SVR
from sklearn.preprocessing import MinMaxScaler
from common.utils import load_data, mape
```

----------------------------------------

TITLE: Inverse Scaling Training and Testing Actual Values (Python)
DESCRIPTION: Applies the `inverse_transform` method of the fitted MinMaxScaler to the original training and testing target values. This allows for a direct comparison between predictions and actual values in the original scale.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_17

LANGUAGE: python
CODE:
```
# Scaling the original values

y_train = scaler.inverse_transform(y_train)
y_test = scaler.inverse_transform(y_test)
```

----------------------------------------

TITLE: Visualize Polynomial Model Predictions (R)
DESCRIPTION: Binds the integer-encoded 'package' column (`package_integer`) to the `poly_results` data frame. It then generates a scatter plot of `package_integer` vs `price` using `ggplot` and overlays the polynomial model's predictions (`.pred`) as a line, illustrating the non-linear fit.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_20

LANGUAGE: R
CODE:
```
# Bind encoded package column to the results
poly_results <- poly_results %>%
  bind_cols(package_encode %>%
              rename(package_integer = package)) %>%
  relocate(package_integer, .after = package)


# Print new results data frame
poly_results %>%
  slice_head(n = 5)


# Make a scatter plot
poly_results %>%
  ggplot(mapping = aes(x = package_integer, y = price)) +
  geom_point(size = 1.6) +
  # Overlay a line of best fit
  geom_line(aes(y = .pred), color = "midnightblue", size = 1.2) +
  xlab("package")
```

----------------------------------------

TITLE: Preprocessing Data using Recipe Prep and Bake in R
DESCRIPTION: This snippet applies the previously defined recipe to the data. `prep()` estimates the necessary parameters from the training data (like the mapping for integer encoding), and `bake()` applies these steps to transform the data. It then displays the first 10 rows of the resulting preprocessed data frame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_8

LANGUAGE: R
CODE:
```
# Prep the recipe
pumpkins_prep <- prep(pumpkins_recipe)

# Bake the recipe to extract a preprocessed new_pumpkins data
baked_pumpkins <- bake(pumpkins_prep, new_data = NULL)

# Print out the baked data set
baked_pumpkins %>%
  slice_head(n = 10)
```

----------------------------------------

TITLE: Calculating Correlation in R
DESCRIPTION: This code calculates the Pearson correlation coefficient between specified columns (`city_name` and `price`, then `package` and `price`) of the preprocessed data frame (`baked_pumpkins`). The `cor()` function requires numeric inputs, which is possible here because the columns have been integer-encoded by the recipe.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_9

LANGUAGE: R
CODE:
```
# Find the correlation between the city_name and the price
cor(baked_pumpkins$city_name, baked_pumpkins$price)

# Find the correlation between the package and the price
cor(baked_pumpkins$package, baked_pumpkins$price)
```

----------------------------------------

TITLE: Defining Training and Testing Periods (Python)
DESCRIPTION: Specifies the start dates for the training and testing subsets of the time series data. The training period begins on '2014-11-01', and the testing period begins chronologically later on '2014-12-30', ensuring a realistic forecasting scenario.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_4

LANGUAGE: python
CODE:
```
train_start_dt = '2014-11-01 00:00:00'
test_start_dt = '2014-12-30 00:00:00'
```

----------------------------------------

TITLE: Counting Rows with Specific Text using Pandas Apply and Lambdas - Python
DESCRIPTION: This snippet demonstrates how to count rows in a Pandas DataFrame based on string matching in specified columns using the `apply` method with lambda functions. It iterates through rows to check conditions and measures the execution time. Requires a DataFrame `df` with 'Negative_Review' and 'Positive_Review' columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_7

LANGUAGE: python
CODE:
```
# with lambdas:
start = time.time()
no_negative_reviews = df.apply(lambda x: True if x['Negative_Review'] == "No Negative" else False , axis=1)
print("Number of No Negative reviews: " + str(len(no_negative_reviews[no_negative_reviews == True].index)))

no_positive_reviews = df.apply(lambda x: True if x['Positive_Review'] == "No Positive" else False , axis=1)
print("Number of No Positive reviews: " + str(len(no_positive_reviews[no_positive_reviews == True].index)))

both_no_reviews = df.apply(lambda x: True if x['Negative_Review'] == "No Negative" and x['Positive_Review'] == "No Positive" else False , axis=1)
print("Number of both No Negative and No Positive reviews: " + str(len(both_no_reviews[both_no_reviews == True].index)))
end = time.time()
print("Lambdas took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Selecting Numeric Columns R
DESCRIPTION: Selects the 'artist_top_genre' column and all columns with numeric data types from the filtered `nigerian_songs` dataframe. It uses the `where(is.numeric)` helper function for efficient column selection and displays the first 5 rows of the resulting data frame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb#_snippet_2

LANGUAGE: R
CODE:
```
# Select top genre column and all other numeric columns
df_numeric <- nigerian_songs %>%
  select(artist_top_genre, where(is.numeric))

# Display the data
df_numeric %>%
  slice_head(n = 5)
```

----------------------------------------

TITLE: Plotting Price vs. DayOfYear for PIE TYPE (Python)
DESCRIPTION: This code filters the `new_pumpkins` DataFrame to isolate only the data for 'PIE TYPE' pumpkins. It then generates a scatter plot of 'Price' against 'DayOfYear' specifically for this filtered subset (`pie_pumpkins`) to visualize the price trend over time for this particular variety.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/de/2-Regression/3-Linear/README.md#_snippet_3

LANGUAGE: python
CODE:
```
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price')
```

----------------------------------------

TITLE: Implementing and Testing Strict Q-Based Policy Python
DESCRIPTION: Defines a policy `qpolicy_strict` that selects the action with the highest Q-value for the current state, representing a purely exploitative strategy. It then runs the `walk` simulation using this strict policy to test the learned behavior.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/notebook.ipynb#_snippet_12

LANGUAGE: Python
CODE:
```
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

----------------------------------------

TITLE: Import Scikit-learn Regression Modules Python
DESCRIPTION: This code imports the necessary classes and functions from the Scikit-learn library required for implementing linear regression: the LinearRegression model, the mean_squared_error function for evaluation, and the train_test_split function for data splitting.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/2-Regression/3-Linear/README.md#_snippet_5

LANGUAGE: python
CODE:
```
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

----------------------------------------

TITLE: Introducing and Plotting Basic Data Info in R
DESCRIPTION: Uses the DataExplorer package to generate a basic description and a visual plot of the dataset's introduction. This helps in quickly assessing data completeness, number of variables/observations, and basic types.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb#_snippet_4

LANGUAGE: R
CODE:
```
# Describe basic information for our data
df %>%
  introduce()

# A visual display of the same
df %>%
  plot_intro()
```

----------------------------------------

TITLE: Plotting Linear Regression Results with ggplot2 in R
DESCRIPTION: This R code snippet uses the ggplot2 library to visualize the results of a linear regression model. It creates a scatter plot showing the relationship between bmi and actual y values from the test set (geom_point), and overlays a blue line representing the model's predictions (geom_line). The plot is based on a dataframe named 'results' and uses a light theme.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/solution/R/lesson_1-R.ipynb#_snippet_9

LANGUAGE: R
CODE:
```
# Set a theme for the plot
theme_set(theme_light())
# Create a scatter plot
results %>%
  ggplot(aes(x = bmi)) +
  # Add a scatter plot
  geom_point(aes(y = y), size = 1.6) +
  # Add a line plot
  geom_line(aes(y = .pred), color = "blue", size = 1.5)
```

----------------------------------------

TITLE: Defining Stop Words Removal Function Python
DESCRIPTION: Defines the `remove_stopwords` function to filter common English stop words from review text. It initializes a set of English stopwords for efficient lookup and iterates through words in the review, excluding those found in the stop word set.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/3-notebook.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
# Remove stop words - can be slow for a lot of text!
# Ryan Han (ryanxjhan on Kaggle) has a great post measuring performance of different stop words removal approaches
# https://www.kaggle.com/ryanxjhan/fast-stop-words-removal # using the approach that Ryan recommends
start = time.time()
cache = set(stopwords.words("english"))
def remove_stopwords(review):
    text = " ".join([word for word in review.split() if word not in cache])
    return text

```

----------------------------------------

TITLE: Calculating and Adding Sentiment Columns Python
DESCRIPTION: Calculates sentiment scores for the cleaned 'Negative_Review' and 'Positive_Review' columns by applying the `calc_sentiment` function. Adds these scores as new columns, 'Negative_Sentiment' and 'Positive_Sentiment', to the DataFrame and measures the time taken for this step.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/3-notebook.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
# Add a negative sentiment and positive sentiment column
print("Calculating sentiment columns for both positive and negative reviews")
start = time.time()
df["Negative_Sentiment"] = df.Negative_Review.apply(calc_sentiment)
df["Positive_Sentiment"] = df.Positive_Review.apply(calc_sentiment)
end = time.time()
print("Calculating sentiment took " + str(round(end - start, 2)) + " seconds")

```

----------------------------------------

TITLE: Filtering Dataset by Cuisine Type in R
DESCRIPTION: Filters the main dataset `df` into separate R tibbles, creating one tibble for each specific cuisine type (thai, japanese, chinese, indian, korean). This segmentation allows for easier analysis or processing of data pertaining to individual cuisines using the `filter()` verb from `dplyr`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb#_snippet_4

LANGUAGE: R
CODE:
```
# Create individual tibble for the cuisines
thai_df <- df %>%
  filter(cuisine == "thai")
japanese_df <- df %>%
  filter(cuisine == "japanese")
chinese_df <- df %>%
  filter(cuisine == "chinese")
indian_df <- df %>%
  filter(cuisine == "indian")
korean_df <- df %>%
  filter(cuisine == "korean")
```

----------------------------------------

TITLE: Filtering and Plotting Pie Pumpkins Data in Python
DESCRIPTION: Filters the `new_pumpkins` DataFrame to select only rows where 'Variety' is 'PIE TYPE'. It then creates a scatter plot of 'Price' against 'DayOfYear' specifically for this subset of data to analyze the trend for a single variety.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/2-Regression/3-Linear/README.md#_snippet_3

LANGUAGE: python
CODE:
```
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price')
```

----------------------------------------

TITLE: Run Basic Simulation with Random Actions - Python
DESCRIPTION: Demonstrates how to run a basic simulation loop in the CartPole environment for a fixed number of steps (100). It resets the environment, renders the current state visually, and takes random actions at each step using `env.action_space.sample()`. The environment is closed at the end.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/solution/notebook.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
env.reset()

for i in range(100):
   env.render()
   env.step(env.action_space.sample())
env.close()
```

----------------------------------------

TITLE: Getting DataFrame Shape (Python)
DESCRIPTION: Prints the number of rows and columns in a pandas DataFrame `df` using the `.shape` attribute. The output is formatted into a descriptive string. Requires a pandas DataFrame named `df`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_0

LANGUAGE: python
CODE:
```
print("The shape of the data (rows, cols) is " + str(df.shape))
```

----------------------------------------

TITLE: Visualizing Train vs. Test Time Series Split in Python
DESCRIPTION: This snippet visualizes the split between the training and testing time series data. It filters the original data based on the defined start dates, joins the filtered training and test sets, and plots them on the same graph to show the division.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/README.md#_snippet_4

LANGUAGE: python
CODE:
```
energy[(energy.index < test_start_dt) & (energy.index >= train_start_dt)][['load']].rename(columns={'load':'train'}) \
    .join(energy[test_start_dt:][['load']].rename(columns={'load':'test'}), how='outer') \
    .plot(y=['train', 'test'], figsize=(15, 8), fontsize=12)
plt.xlabel('timestamp', fontsize=12)
plt.ylabel('load', fontsize=12)
plt.show()
```

----------------------------------------

TITLE: Plotting Price vs Day of Year by Variety using Pandas Plotting Python
DESCRIPTION: This code iterates through unique pumpkin varieties in the `new_pumpkins` DataFrame and generates a scatter plot of 'Price' against 'DayOfYear' for each variety. By passing the `ax` parameter, all scatter plots are rendered on the same axes, with points colored according to their variety. This visualization helps identify potential clusters or relationships specific to each pumpkin type.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/3-Linear/README.md#_snippet_1

LANGUAGE: python
CODE:
```
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

----------------------------------------

TITLE: Visualizing Price vs. Day of Year by Variety
DESCRIPTION: This code iterates through unique pumpkin varieties, filters the DataFrame for each variety, and plots a scatter plot of 'Price' vs. 'DayOfYear' using a different color for each variety on the same axes. This visualization helps identify potential clusters or different relationships for different varieties.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/2-Regression/3-Linear/README.md#_snippet_1

LANGUAGE: python
CODE:
```
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

----------------------------------------

TITLE: Importing Scikit-learn Modules for Polynomial Regression Pipeline (Python)
DESCRIPTION: This snippet imports necessary classes and functions from scikit-learn for implementing polynomial regression using a pipeline. It imports PolynomialFeatures to generate polynomial and interaction features and make_pipeline to easily chain estimators. Dependencies: scikit-learn library.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/2-Regression/3-Linear/README.md#_snippet_12

LANGUAGE: python
CODE:
```
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
```

----------------------------------------

TITLE: Importing Libraries for Polynomial Regression Pipeline in Python
DESCRIPTION: Imports necessary classes from Scikit-learn for implementing polynomial regression using a pipeline. This includes `PolynomialFeatures` to generate polynomial features and `make_pipeline` to easily construct a pipeline combining multiple processing steps.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/2-Regression/3-Linear/README.md#_snippet_12

LANGUAGE: python
CODE:
```
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
```

----------------------------------------

TITLE: Plotting Specific Date Range of Load Data (Python)
DESCRIPTION: Filters the `energy` DataFrame to select data specifically for the date range from July 1, 2014, to July 7, 2014. It then plots this subset of the 'load' time series, customizing plot size and label font sizes before displaying the plot. Requires the `energy` DataFrame with a datetime index and Matplotlib.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/1-Introduction/solution/notebook.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
energy['2014-07-01':'2014-07-07'].plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)
plt.xlabel('timestamp', fontsize=12)
plt.ylabel('load', fontsize=12)
plt.show()
```

----------------------------------------

TITLE: Importing Scikit-learn Modules for Regression - Python
DESCRIPTION: This code imports the `LinearRegression` class for building a linear regression model, the `mean_squared_error` function for evaluating model performance, and the `train_test_split` function for dividing data into training and testing sets, all from the scikit-learn library. These are standard imports for building and evaluating regression models.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/2-Regression/3-Linear/README.md#_snippet_5

LANGUAGE: python
CODE:
```
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

----------------------------------------

TITLE: Preparing Data for Linear Regression (Single Feature)
DESCRIPTION: Shows how to prepare the feature matrix `X` and the target vector `y` for a linear regression model using only the one-hot encoded 'Variety' column as the feature and the 'Price' column as the target.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/3-Linear/README.md#_snippet_15

LANGUAGE: python
CODE:
```
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

----------------------------------------

TITLE: Extracting Useful Tags into New Columns
DESCRIPTION: This snippet processes the existing 'Tags' column in the DataFrame, creating new binary columns for a predefined list of 'useful' tags such as 'Leisure trip', 'Couple', etc. It uses the pandas `apply` method with a lambda function to check if the tag string contains specific substrings and assigns 1 if true, 0 otherwise. The 'Group' tag also includes 'Travelers with friends'.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_0

LANGUAGE: python
CODE:
```
# Process the Tags into new columns
# The file Hotel_Reviews_Tags.py, identifies the most important tags
# Leisure trip, Couple, Solo traveler, Business trip, Group combined with Travelers with friends, 
# Family with young children, Family with older children, With a pet
df["Leisure_trip"] = df.Tags.apply(lambda tag: 1 if "Leisure trip" in tag else 0)
df["Couple"] = df.Tags.apply(lambda tag: 1 if "Couple" in tag else 0)
df["Solo_traveler"] = df.Tags.apply(lambda tag: 1 if "Solo traveler" in tag else 0)
df["Business_trip"] = df.Tags.apply(lambda tag: 1 if "Business trip" in tag else 0)
df["Group"] = df.Tags.apply(lambda tag: 1 if "Group" in tag or "Travelers with friends" in tag else 0)
df["Family_with_young_children"] = df.Tags.apply(lambda tag: 1 if "Family with young children" in tag else 0)
df["Family_with_older_children"] = df.Tags.apply(lambda tag: 1 if "Family with older children" in tag else 0)
df["With_a_pet"] = df.Tags.apply(lambda tag: 1 if "With a pet" in tag else 0)
```

----------------------------------------

TITLE: Import Scikit-learn Modules Python
DESCRIPTION: Imports necessary modules from the scikit-learn library for machine learning tasks. This includes modules for splitting data (`train_test_split`), the Support Vector Classifier model (`SVC`), cross-validation (`cross_val_score`), and various evaluation metrics (`accuracy_score`, `precision_score`, `confusion_matrix`, `classification_report`).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/4-Classification/4-Applied/README.md#_snippet_4

LANGUAGE: python
CODE:
```
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report
```

----------------------------------------

TITLE: Installing Gym and Importing Libraries Python
DESCRIPTION: This snippet installs the OpenAI Gym library using pip via the system's Python executable and then imports necessary libraries for reinforcement learning, simulation, plotting, numerical operations, and random number generation.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/README.md#_snippet_0

LANGUAGE: python
CODE:
```
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

----------------------------------------

TITLE: Selecting and Reshaping Feature Data (Numpy/Scikit-learn, Python)
DESCRIPTION: Selects the third column (index 2) from the feature matrix X, effectively choosing a single feature (BMI) for regression. It then reshapes this single column into a 2D array with shape `(n_samples, 1)` using `reshape((-1, 1))`, which is a required format for many scikit-learn models and plotting functions when dealing with a single feature.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/2-Regression/1-Tools/README.md#_snippet_1

LANGUAGE: python
CODE:
```
X = X[:, 2]
X = X.reshape((-1,1))
```

----------------------------------------

TITLE: Running Basic CartPole Simulation with Random Actions Python
DESCRIPTION: Runs a simple simulation of the CartPole environment for 100 steps. It resets the environment, renders the visual simulation window at each step, takes a random action from the action space, and finally closes the rendering window. This demonstrates a basic interaction loop with the environment.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/README.md#_snippet_2

LANGUAGE: python
CODE:
```
env.reset()

for i in range(100):
   env.render()
   env.step(env.action_space.sample())
env.close()
```

----------------------------------------

TITLE: Filtering and Plotting Price vs. Day of Year for 'PIE TYPE'
DESCRIPTION: This code filters the 'new_pumpkins' DataFrame to include only rows where the 'Variety' is 'PIE TYPE' and then generates a scatter plot of 'Price' against 'DayOfYear' for this specific subset. This focuses the analysis on a single variety.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/2-Regression/3-Linear/README.md#_snippet_3

LANGUAGE: python
CODE:
```
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price')
```

----------------------------------------

TITLE: Plotting Price vs DayOfYear by Variety - Pandas Python
DESCRIPTION: Iterates through unique pumpkin varieties, subsets the DataFrame for each variety, and plots 'DayOfYear' against 'Price' as a scatter plot. Each variety is plotted with a different color on the same axes (`ax`) to visualize price clusters based on variety. Requires the pandas library and matplotlib for plotting.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/2-Regression/3-Linear/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

----------------------------------------

TITLE: Plotting Price vs Day by Variety with Pandas
DESCRIPTION: Generates a scatter plot showing 'Price' against 'DayOfYear' for different pumpkin varieties, using distinct colors for each variety. It iterates through unique 'Variety' values, filters the DataFrame, and plots the scatter points on the same axes (`ax`). Requires a pandas DataFrame with 'Variety', 'DayOfYear', and 'Price' columns, and matplotlib (implicitly used by pandas plotting). Outputs a scatter plot.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/fr/2-Regression/3-Linear/README.md#_snippet_1

LANGUAGE: python
CODE:
```
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

----------------------------------------

TITLE: Visualizing Price vs DayOfYear by Variety in Python
DESCRIPTION: Generates a scatter plot of 'Price' against 'DayOfYear' for each unique pumpkin 'Variety'. Different colors are used for each variety, allowing visual inspection of how variety influences the price trend over the year. Uses Pandas plotting functionality, typically backed by Matplotlib.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/2-Regression/3-Linear/README.md#_snippet_1

LANGUAGE: python
CODE:
```
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

----------------------------------------

TITLE: Plotting Running Average Rewards Python
DESCRIPTION: This Python snippet defines a utility function 'running_average' using numpy.convolve to calculate the moving average of a sequence. It then plots the running average of the 'rewards' list collected during training, using a specified window size (e.g., 100). This technique helps visualize the overall trend of learning progress by smoothing out the noise in the raw reward data. It requires numpy and matplotlib.pyplot.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/8-Reinforcement/2-Gym/README.md#_snippet_2

LANGUAGE: python
CODE:
```
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

----------------------------------------

TITLE: Executing Model Comparison Function in R
DESCRIPTION: Creates a list of previously defined workflows (svc, svm, knn, random_forest, xgboost workflows). Calls the 'compare_models' function with this list, training data, and test data to get performance metrics for all models. Finally, it prints the results, showing metrics for top models and specifically comparing accuracy.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/solution/R/lesson_12-R.ipynb#_snippet_12

LANGUAGE: R
CODE:
```
# Make a list of workflows
workflow_list <- list(
  "svc" = svc_linear_wf,
  "svm" = svm_rbf_wf,
  "knn" = knn_wf,
  "random_forest" = rf_wf,
  "xgboost" = boost_wf)

# Call the function
set.seed(2056)
perf_metrics <- compare_models(workflow_list = workflow_list, train_set = cuisines_train, test_set = cuisines_test)

# Print out performance metrics
perf_metrics %>%
  group_by(.metric) %>%
  arrange(desc(.estimate)) %>%
  slice_head(n=7)

# Compare accuracy
perf_metrics %>%
  filter(.metric == "accuracy") %>%
  arrange(desc(.estimate))
```

----------------------------------------

TITLE: Analyzing Text Sentiment with TextBlob (Python)
DESCRIPTION: This snippet showcases the sentiment analysis capabilities of the `TextBlob` library in Python. It calculates the sentiment (polarity and subjectivity scores) for two different text strings. A TextBlob object is created for each quote, and the `.sentiment` property is accessed to retrieve a Sentiment object containing polarity (-1 to 1) and subjectivity (0 to 1) values, which are then printed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/3-Translation-Sentiment/translations/README.ko.md#_snippet_1

LANGUAGE: Python
CODE:
```
from textblob import TextBlob

quote1 = """It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife."""

quote2 = """Darcy, as well as Elizabeth, really loved them; and they were both ever sensible of the warmest gratitude towards the persons who, by bringing her into Derbyshire, had been the means of uniting them."""

sentiment1 = TextBlob(quote1).sentiment
sentiment2 = TextBlob(quote2).sentiment

print(quote1 + " has a sentiment of " + str(sentiment1))
print(quote2 + " has a sentiment of " + str(sentiment2))
```

----------------------------------------

TITLE: Sorting and Printing Reviews by Sentiment Score - Python
DESCRIPTION: Sorts the DataFrame first by 'Negative_Sentiment' in ascending order and prints the 'Negative_Review' and 'Negative_Sentiment' columns to inspect the results. It then sorts by 'Positive_Sentiment' in ascending order and prints 'Positive_Review' and 'Positive_Sentiment'. This helps in validating the sentiment analysis results.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_6

LANGUAGE: Python
CODE:
```
df = df.sort_values(by=["Negative_Sentiment"], ascending=True)
print(df[["Negative_Review", "Negative_Sentiment"]])
df = df.sort_values(by=["Positive_Sentiment"], ascending=True)
print(df[["Positive_Review", "Positive_Sentiment"]])
```

----------------------------------------

TITLE: Defining Q-Learning Hyperparameters - Python
DESCRIPTION: Sets the values for `alpha` (learning rate), `gamma` (discount factor), and `epsilon` (exploration/exploitation factor). These parameters control the learning process within the Q-Learning algorithm, influencing how much the Q-Table is updated, the importance of future rewards, and the balance between exploring new actions and exploiting known good actions.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/README.md#_snippet_9

LANGUAGE: Python
CODE:
```
# hyperparameters
alpha = 0.3
gamma = 0.9
epsilon = 0.90
```

----------------------------------------

TITLE: Implement Bin-Based State Discretization - Python
DESCRIPTION: Defines a function `create_bins` to generate evenly spaced bins for a given interval and number of bins. It then uses this function to create bin boundaries for each dimension of the CartPole observation space and defines `discretize_bins` using `np.digitize` to map continuous observations to discrete bin indices.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/solution/notebook.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
def create_bins(i,num):
    return np.arange(num+1)*(i[1]-i[0])/num+i[0}

print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))

ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
bins = [create_bins(ints[i],nbins[i]) for i in range(4)]

def discretize_bins(x):
    return tuple(np.digitize(x[i],bins[i]) for i in range(4))
```

----------------------------------------

TITLE: Calculating Sentiment Scores with Python
DESCRIPTION: This snippet calculates sentiment scores for the 'Negative_Review' and 'Positive_Review' columns of a pandas DataFrame `df` using a `calc_sentiment` function. It adds the results as new columns 'Negative_Sentiment' and 'Positive_Sentiment' and measures the execution time. It requires the 'time' library and a defined 'calc_sentiment' function.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_9

LANGUAGE: python
CODE:
```
print("Calculating sentiment columns for both positive and negative reviews")
start = time.time()
df["Negative_Sentiment"] = df.Negative_Review.apply(calc_sentiment)
df["Positive_Sentiment"] = df.Positive_Review.apply(calc_sentiment)
end = time.time()
print("Calculating sentiment took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Cleaning and Transforming Pumpkin Data (Python)
DESCRIPTION: Filters the initial DataFrame to include only entries where the 'Package' column contains 'bushel'. It then selects a subset of columns ('Package', 'Variety', 'City Name', 'Low Price', 'High Price', 'Date'). It calculates the average price, extracts the month and day of the year, creates a new DataFrame with these fields, and adjusts the 'Price' based on the specific bushel package size.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/notebook.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
pumpkins = pumpkins[pumpkins['Package'].str.contains('bushel', case=True, regex=True)]

columns_to_select = ['Package', 'Variety', 'City Name', 'Low Price', 'High Price', 'Date']
pumpkins = pumpkins.loc[:, columns_to_select]

price = (pumpkins['Low Price'] + pumpkins['High Price']) / 2

month = pd.DatetimeIndex(pumpkins['Date']).month
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)

new_pumpkins = pd.DataFrame(
    {'Month': month, 
     'DayOfYear' : day_of_year, 
     'Variety': pumpkins['Variety'], 
     'City': pumpkins['City Name'], 
     'Package': pumpkins['Package'], 
     'Low Price': pumpkins['Low Price'],
     'High Price': pumpkins['High Price'], 
     'Price': price})

new_pumpkins.loc[new_pumpkins['Package'].str.contains('1 1/9'), 'Price'] = price/1.1
new_pumpkins.loc[new_pumpkins['Package'].str.contains('1/2'), 'Price'] = price*2

new_pumpkins.head()
```

----------------------------------------

TITLE: Loading Diabetes Dataset in Python
DESCRIPTION: Loads the scikit-learn built-in diabetes dataset, returning the feature matrix X and target vector y. It then prints the shape of the feature matrix and the values of the first sample to inspect the data structure. Requires the `sklearn.datasets` module.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/2-Regression/1-Tools/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
X, y = datasets.load_diabetes(return_X_y=True)
print(X.shape)
print(X[0])
```

----------------------------------------

TITLE: Prepare Features X Python
DESCRIPTION: Selects all columns starting from the third column (index 2) of the DataFrame `data` to create the feature set `X`. This step removes identifier columns. `X.head()` displays the first few rows of the resulting feature DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/4-Classification/4-Applied/README.md#_snippet_2

LANGUAGE: python
CODE:
```
X = data.iloc[:,2:]
X.head()
```

----------------------------------------

TITLE: Installing Required R Packages with pacman
DESCRIPTION: This code block checks for and installs the necessary R packages ('tidyverse', 'tidymodels', 'themis', 'here') required for data manipulation, modeling, and handling imbalanced data using the 'pacman' package management tool. It ensures all prerequisites are met before proceeding with the data analysis and model building.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/solution/R/lesson_11-R.ipynb#_snippet_0

LANGUAGE: R
CODE:
```
suppressWarnings(if (!require("pacman"))install.packages("pacman))

pacman::p_load(tidyverse, tidymodels, themis, here)
```

----------------------------------------

TITLE: Visualizing Top 10 Genres (Excluding Missing) in R
DESCRIPTION: Creates a bar plot showing the counts of the top 10 most frequent genres, specifically filtering out the 'Missing' genre category. This provides a clearer view of the distribution among classified genres.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb#_snippet_8

LANGUAGE: R
CODE:
```
# Visualize popular genres
top_genres %>%
  filter(artist_top_genre != "Missing") %>%
  slice(1:10) %>%
  ggplot(mapping = aes(x = artist_top_genre, y = n,
                       fill = artist_top_genre)) +
  geom_col(alpha = 0.8) +
  paletteer::scale_fill_paletteer_d("rcartocolor::Vivid") +
  ggtitle("Top genres") +
  theme(plot.title = element_text(hjust = 0.5),
        # Rotates the X markers (so we can read them)
    axis.text.x = element_text(angle = 90))
```

----------------------------------------

TITLE: Viewing Data Head (R)
DESCRIPTION: Displays the first 30 rows of the `new_pumpkins` dataframe using `slice_head` from `dplyr` to inspect the data structure and content after previous manipulations.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/solution/R/lesson_2-R.ipynb#_snippet_11

LANGUAGE: R
CODE:
```
new_pumpkins %>%
  slice_head(n = 30)
```

----------------------------------------

TITLE: Adding K-Neighbors Classifier to Dictionary - Scikit-learn Python
DESCRIPTION: This code snippet is intended to be added as an entry to the `classifiers` dictionary defined previously. It adds a `KNeighborsClassifier` model under the key `'KNN classifier'`. The parameter `C` (presumably set to 10 earlier) is passed to the classifier, which likely corresponds to the `n_neighbors` parameter, though the naming is inconsistent with standard practice.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
'KNN classifier': KNeighborsClassifier(C),
```

----------------------------------------

TITLE: Installing/Loading R Packages with pacman
DESCRIPTION: Checks if the 'pacman' package is installed and installs it if necessary. Then, uses 'pacman::p_load' to install and load the 'tidyverse' and 'tidymodels' packages, ensuring all required dependencies for the module are available.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/solution/R/lesson_1-R.ipynb#_snippet_0

LANGUAGE: R
CODE:
```
suppressWarnings(if(!require("pacman")) install.packages("pacman"))
pacman::p_load(tidyverse, tidymodels)
```

----------------------------------------

TITLE: Visualizing Binary Data with Swarm Plot (Python)
DESCRIPTION: Uses the Seaborn library to generate a swarm plot. This visualization technique is suitable for displaying the distribution of values for a categorical variable, especially binary ones, against a numerical variable. It helps visualize potential separation between the categories.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/README.md#_snippet_9

LANGUAGE: python
CODE:
```
palette = {
    0: 'orange',
    1: 'wheat'
}
sns.swarmplot(x="Color", y="ord__Item Size", data=encoded_pumpkins, palette=palette)
```

----------------------------------------

TITLE: Loading & Inspecting Diabetes Dataset - Scikit-learn Python
DESCRIPTION: This snippet loads the diabetes dataset from `sklearn.datasets`. It unpacks the returned tuple into feature matrix `X` and target vector `y` by setting `return_X_y=True`. It then prints the shape of the feature matrix and the contents of the first sample to inspect the data structure and values. Requires scikit-learn's `datasets` module.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/README.md#_snippet_2

LANGUAGE: python
CODE:
```
X, y = datasets.load_diabetes(return_X_y=True)
print(X.shape)
print(X[0])
```

----------------------------------------

TITLE: Selecting Features for Linear Model (Pandas) - Python
DESCRIPTION: This snippet creates a new DataFrame `lin_pumpkins` by dropping all columns from `new_pumpkins` except 'Package' and 'Price'. This prepares the data specifically for a simple linear regression model focusing on these two variables.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/translations/README.ko.md#_snippet_4

LANGUAGE: Python
CODE:
```
new_columns = ['Package', 'Price']
lin_pumpkins = new_pumpkins.drop([c for c in new_pumpkins.columns if c not in new_columns], axis='columns')

lin_pumpkins
```

----------------------------------------

TITLE: Saving Transformed Data to CSV (Pandas Python)
DESCRIPTION: Exports the `transformed_df` DataFrame, containing the balanced and preprocessed data, to a new CSV file named 'cleaned_cuisines.csv' in the '../../data/' directory. This saves the result of the preprocessing steps for future use.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/notebook.ipynb#_snippet_20

LANGUAGE: Python
CODE:
```
transformed_df.to_csv("../../data/cleaned_cuisines.csv")
```

----------------------------------------

TITLE: Categorizing Sentences by Sentiment Polarity Python
DESCRIPTION: Iterates through each sentence within the book_pride TextBlob object. It checks the polarity of each sentence's sentiment. If the polarity is exactly 1, the sentence is added to the positive_sentiment_sentences list; if it's exactly -1, it's added to the negative_sentiment_sentences list.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/3-Translation-Sentiment/solution/notebook.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
for sentence in book_pride.sentences:
    if sentence.sentiment.polarity == 1:
        positive_sentiment_sentences.append(sentence)
    if sentence.sentiment.polarity == -1:
        negative_sentiment_sentences.append(sentence)

```

----------------------------------------

TITLE: Calculate Silhouette Score for K-Means Clustering in Python
DESCRIPTION: Computes the silhouette score to evaluate the quality of the clustering result obtained from K-Means. A score closer to 1 indicates dense and well-separated clusters, while a score near 0 suggests overlapping clusters. Requires `sklearn.metrics` and the feature matrix `X` along with the predicted cluster labels `y_cluster_kmeans`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/README.md#_snippet_3

LANGUAGE: python
CODE:
```
from sklearn import metrics
score = metrics.silhouette_score(X, y_cluster_kmeans)
score
```

----------------------------------------

TITLE: Loading Libraries and Importing Data in R
DESCRIPTION: Loads the necessary 'tidyverse' R package and imports the Nigerian songs dataset from a specified URL into a tibble (a modern data frame). This is the first step to make the data and essential data manipulation tools available.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb#_snippet_1

LANGUAGE: R
CODE:
```
# Load the core tidyverse and make it available in your current R session
library(tidyverse)

# Import the data into a tibble
df <- read_csv(file = "https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/data/nigerian-songs.csv")
```

----------------------------------------

TITLE: Dropping Columns and Saving DataFrame - Pandas - Python
DESCRIPTION: This code removes several columns that are no longer needed or are less relevant for subsequent analysis from the pandas DataFrame. After dropping the columns, it saves the modified DataFrame to a new CSV file named 'Hotel_Reviews_Filtered.csv' without including the DataFrame index.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_1

LANGUAGE: python
CODE:
```
df.drop(["Review_Total_Negative_Word_Counts", "Review_Total_Positive_Word_Counts", "days_since_review", "Total_Number_of_Reviews_Reviewer_Has_Given"], axis = 1, inplace=True)

# Saving new data file with calculated columns
print("Saving results to Hotel_Reviews_Filtered.csv")
df.to_csv(r'../data/Hotel_Reviews_Filtered.csv', index = False)

```

----------------------------------------

TITLE: Defining Timesteps for SVR (Python)
DESCRIPTION: Sets the integer value representing the number of previous time steps the SVR model will use as input features to predict the value of the next time step. A value of 5 means the past 4 values are used to predict the 5th.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_10

LANGUAGE: python
CODE:
```
timesteps=5
```

----------------------------------------

TITLE: Loading, Filtering, and Visualizing Data R
DESCRIPTION: Loads the `tidyverse` package, imports a music dataset from a URL, filters it to include only the top 3 specified genres with non-zero popularity, and visualizes the counts of these filtered genres using a bar plot generated with `ggplot2`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb#_snippet_1

LANGUAGE: R
CODE:
```
# Load the core tidyverse and make it available in your current R session
library(tidyverse)

# Import the data into a tibble
df <- read_csv(file = "https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/data/nigerian-songs.csv", show_col_types = FALSE)

# Narrow down to top 3 popular genres
nigerian_songs <- df %>%
  # Concentrate on top 3 genres
  filter(artist_top_genre %in% c("afro dancehall", "afropop","nigerian pop")) %>%
  # Remove unclassified observations
  filter(popularity != 0)



# Visualize popular genres using bar plots
theme_set(theme_light())
nigerian_songs %>%
  count(artist_top_genre) %>%
  ggplot(mapping = aes(x = artist_top_genre, y = n,
                       fill = artist_top_genre)) +
  geom_col(alpha = 0.8) +
  paletteer::scale_fill_paletteer_d("ggsci::category10_d3") +
  ggtitle("Top genres") +
  theme(plot.title = element_text(hjust = 0.5))
```

----------------------------------------

TITLE: Plot Scatter by Variety Python Pandas Matplotlib
DESCRIPTION: This code iterates through unique pumpkin varieties in a DataFrame and generates a scatter plot of 'DayOfYear' versus 'Price' for each variety, plotting all points on the same figure using different colors.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/2-Regression/3-Linear/README.md#_snippet_1

LANGUAGE: python
CODE:
```
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

----------------------------------------

TITLE: Plotting Histograms of Original and Scaled Data in Python
DESCRIPTION: This code generates and displays histograms to compare the distribution of the original training data's 'load' column with the distribution of the scaled training data. This visual comparison helps understand the effect of scaling.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/README.md#_snippet_7

LANGUAGE: python
CODE:
```
energy[(energy.index >= train_start_dt) & (energy.index < test_start_dt)][['load']].rename(columns={'load':'original load'}).plot.hist(bins=100, fontsize=12)
train.rename(columns={'load':'scaled load'}).plot.hist(bins=100, fontsize=12)
plt.show()
```

----------------------------------------

TITLE: Comparing Preprocessed and Original Cuisine Distributions in R
DESCRIPTION: Compares the distribution of cuisine labels in the `preprocessed_df` (after SMOTE balancing) with the original distribution stored in `old_label_count`. It counts the frequencies in the preprocessed data and presents both sets of counts in a list to demonstrate the effect of the balancing operation.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb#_snippet_17

LANGUAGE: R
CODE:
```
# Distribution of cuisines
new_label_count <- preprocessed_df %>% 
  count(cuisine) %>% 
  arrange(desc(n))

list(new_label_count = new_label_count,
     old_label_count = old_label_count)
```

----------------------------------------

TITLE: Creating Time-Stepped Training Data (Python)
DESCRIPTION: Reshapes the 1D training data array into a 2D array suitable for time series forecasting with SVR. Each row in the new array represents a sequence of 'timesteps' consecutive data points, created by sliding a window across the original data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_11

LANGUAGE: python
CODE:
```
train_data_timesteps=np.array([[j for j in train_data[i:i+timesteps]] for i in range(0,len(train_data)-timesteps+1)])[:,:,0]
```

----------------------------------------

TITLE: Loading/Installing R Packages with pacman
DESCRIPTION: This R code block first checks if the `pacman` package is installed and installs it if not. It then uses `pacman::p_load` to load a list of packages (`tidyverse`, `tidymodels`, `themis`, `kernlab`, `ranger`, `xgboost`, `kknn`), installing any that are missing. This ensures all necessary dependencies for the module are available.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/solution/R/lesson_12-R.ipynb#_snippet_1

LANGUAGE: R
CODE:
```
suppressWarnings(if (!require("pacman"))install.packages("pacman"))

pacman::p_load(tidyverse, tidymodels, themis, kernlab, ranger, xgboost, kknn)
```

----------------------------------------

TITLE: Initializing Q-Table for Q-Learning Python
DESCRIPTION: Initializes the Q-table, a core component of Q-learning. It's a NumPy array with dimensions corresponding to board width, height, and the number of possible actions. All values are initially set to small equal numbers.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/notebook.ipynb#_snippet_7

LANGUAGE: Python
CODE:
```
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

----------------------------------------

TITLE: Calculating WCSS for Elbow Method (Python)
DESCRIPTION: Iterates through a range of potential cluster numbers (from 1 to 10), applies the K-Means algorithm (`k-means++` initialization) for each iteration, and calculates the Within-Cluster Sum of Squares (WCSS), also known as inertia. The calculated WCSS value for each number of clusters is appended to the `wcss` list to be used later for the Elbow method visualization. Requires `sklearn.cluster.KMeans`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/de/5-Clustering/2-K-Means/README.md#_snippet_4

LANGUAGE: python
CODE:
```
from sklearn.cluster import KMeans
wcss = []

for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42, n_init=10)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)
```

----------------------------------------

TITLE: Print Top N Nationality Frequencies with Pandas Python
DESCRIPTION: Building on the `nationality_freq` Series, this snippet prints the most frequent nationality and its count by accessing the first element (`index[0]`, `[0]`). It then prints the next 10 most frequent nationalities and their counts by slicing the Series (`[1:11]`) and formatting the output using `.to_string()`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_3

LANGUAGE: python
CODE:
```
print("The highest frequency reviewer nationality is " + str(nationality_freq.index[0]).strip() + " with " + str(nationality_freq[0]) + " reviews.")
# Notice there is a leading space on the values, strip() removes that for printing
# What is the top 10 most common nationalities and their frequencies?
print("The next 10 highest frequency reviewer nationalities are:")
print(nationality_freq[1:11].to_string())
```

----------------------------------------

TITLE: Initialize Q-Table and Q-Value Helper - Python
DESCRIPTION: Initializes an empty dictionary `Q` to serve as the Q-Table, mapping (state, action) pairs to Q-values. It also defines a helper function `qvalues` that retrieves the Q-values for all possible actions from the Q-Table for a given state, returning 0 if the state-action pair is not found.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/solution/notebook.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
Q = {}
actions = (0,1)

def qvalues(state):
    return [Q.get((state,a),0) for a in actions]
```

----------------------------------------

TITLE: Calculating & Plotting Running Average Rewards Python
DESCRIPTION: Defines a helper function `running_average` that calculates the moving average of a list/array `x` over a specified `window` using NumPy convolution. It then applies this function to the `rewards` data with a window of 100 and plots the resulting smoothed progress. Useful for understanding the underlying trend in noisy reward data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/8-Reinforcement/2-Gym/README.md#_snippet_1

LANGUAGE: python
CODE:
```
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

----------------------------------------

TITLE: Calculating Correlation (Pandas) - Python
DESCRIPTION: This snippet calculates the Pearson correlation coefficient between the 'Package' and 'Price' columns in the 'new_pumpkins' DataFrame. A higher correlation indicates a stronger linear relationship, suggesting 'Package' might be a better predictor for 'Price' than 'City'.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/translations/README.ko.md#_snippet_2

LANGUAGE: Python
CODE:
```
print(new_pumpkins['Package'].corr(new_pumpkins['Price']))
```

----------------------------------------

TITLE: Calculating Correlation using Pandas in Python
DESCRIPTION: Calculates the Pearson correlation coefficient between 'Month'/'DayOfYear' and 'Price' columns in the `new_pumpkins` DataFrame. This helps determine the linear relationship strength and direction between these features and the target variable.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/2-Regression/3-Linear/README.md#_snippet_0

LANGUAGE: python
CODE:
```
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

----------------------------------------

TITLE: Reshaping Feature Data for Model Input - Python
DESCRIPTION: Reshapes the single-feature array X into a 2D array using `reshape(-1, 1)`. This is a required format for scikit-learn models when dealing with a single feature. Prints the reshaped array's shape and contents.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/solution/notebook.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
#Reshaping to get a 2D array
X = X.reshape(-1, 1)
print(X.shape)
print(X)
```

----------------------------------------

TITLE: Loading Tidyverse and Tidymodels Packages in R
DESCRIPTION: Explicitly loads the core packages from the Tidyverse and Tidymodels frameworks into the current R session using the standard library() function, making their functions accessible.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/solution/R/lesson_1-R.ipynb#_snippet_1

LANGUAGE: R
CODE:
```
# load the core Tidyverse packages
library(tidyverse)

# load the core Tidymodels packages
library(tidymodels)

```

----------------------------------------

TITLE: Training RL Agent with Q-Learning (Python)
DESCRIPTION: This code block implements the main training loop for a reinforcement learning agent using the Q-learning algorithm. It iterates for a specified number of epochs, starting from a random point, selecting actions based on a probability distribution derived from the Q-table, moving the agent, receiving a reward, and updating the Q-table using the Q-learning update rule. The episode terminates upon reaching a terminal state or exceeding a cumulative negative reward threshold.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/8-Reinforcement/1-QLearning/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
for epoch in range(5000):

    # Pick initial point
    m.random_start()

    # Start travelling
    n=0
    cum_reward = 0
    while True:
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        dpos = actions[a]
        m.move(dpos,check_correctness=False) # we allow player to move outside the board, which terminates episode
        r = reward(m)
        cum_reward += r
        if r==end_reward or cum_reward < -1000:
            lpath.append(n)
            break
        alpha = np.exp(-n / 10e5)
        gamma = 0.5
        ai = action_idx[a]
        Q[x,y,ai] = (1 - alpha) * Q[x,y,ai] + alpha * (r + gamma * Q[x+dpos[0], y+dpos[1]].max())
        n+=1
```

----------------------------------------

TITLE: Install Dependencies and Import Libraries - Python
DESCRIPTION: This snippet installs the necessary OpenAI Gym library using pip and imports key Python libraries for reinforcement learning, environment interaction, plotting, numerical operations, and random number generation.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/solution/notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import sys
!pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

----------------------------------------

TITLE: Creating New DataFrame with Calculated Data in Python
DESCRIPTION: Constructs a new Pandas DataFrame `new_pumpkins` containing the extracted month, original 'Package', 'Low Price', and 'High Price' columns, along with the newly calculated average 'Price'. This organizes the processed data for further analysis.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/2-Regression/2-Data/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
new_pumpkins = pd.DataFrame({'Month': month, 'Package': pumpkins['Package'], 'Low Price': pumpkins['Low Price'],'High Price': pumpkins['High Price'], 'Price': price})
```

----------------------------------------

TITLE: Visualizing Trained Q-Learning Agent Python
DESCRIPTION: Executes a single episode simulation using the currently loaded Q-table (`Q`) to guide action selection by sampling from the probability distribution derived from Q-values. It renders the environment at each step using `env.render()` to visually display the agent's performance and closes the rendering window at the end of the episode.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/8-Reinforcement/2-Gym/README.md#_snippet_5

LANGUAGE: Python
CODE:
```
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

----------------------------------------

TITLE: Calculating Full Dataset MAPE Python
DESCRIPTION: This snippet calculates the Mean Absolute Percentage Error (MAPE) for the predictions made on the full dataset. It uses a `mape` function to compare the inverse-scaled predicted (`Y_pred`) and actual (`Y`) values. The resulting percentage gives an overall measure of the model's average prediction error across the entire series.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_27

LANGUAGE: python
CODE:
```
print('MAPE: ', mape(Y_pred, Y)*100, '%')
```

----------------------------------------

TITLE: Finding Most Reviewed Hotel Per Nationality Python
DESCRIPTION: This code iterates through the top 10 most frequent nationalities. For each nationality, it filters the DataFrame, finds the most reviewed hotel using `value_counts()`, and prints the nationality, hotel name, and review count.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/4-Hotel-Reviews-1/solution/notebook.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
# What was the most frequently reviewed hotel for the top 10 nationalities - print the hotel and number of reviews
for nat in nationality_freq[:10].index:
   # First, extract all the rows that match the criteria into a new dataframe
   nat_df = df[df["Reviewer_Nationality"] == nat]   
   # Now get the hotel freq
   freq = nat_df["Hotel_Name"].value_counts()
   print("The most reviewed hotel for " + str(nat).strip() + " was " + str(freq.index[0]) + " with " + str(freq[0]) + " reviews.")
```

----------------------------------------

TITLE: Structuring Forecast Predictions for Evaluation Python
DESCRIPTION: Organizes the recorded forecast predictions and corresponding actual values into a pandas DataFrame (`eval_df`). It reshapes the multi-step predictions and aligns them with the actuals, then applies the inverse scaling to return values to their original scale, preparing the data for error calculation and plotting.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/solution/notebook.ipynb#_snippet_14

LANGUAGE: python
CODE:
```
eval_df = pd.DataFrame(predictions, columns=['t+'+str(t) for t in range(1, HORIZON+1)])
eval_df['timestamp'] = test.index[0:len(test.index)-HORIZON+1]
eval_df = pd.melt(eval_df, id_vars='timestamp', value_name='prediction', var_name='h')
eval_df['actual'] = np.array(np.transpose(test_ts)).ravel()
eval_df[['prediction', 'actual']] = scaler.inverse_transform(eval_df[['prediction', 'actual']])
eval_df.head()
```

----------------------------------------

TITLE: Defining Reward Function (Python)
DESCRIPTION: Defines the `reward` function used for reinforcement learning. It returns a large positive reward (100) for winning against the wolf, a large negative reward (-100) for losing or drowning, and the difference between energy and fatigue (s.energy - s.fatigue) as a reward for intermediate steps.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/assignment-solution.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
def reward(s):
    r = s.energy-s.fatigue
    if s.at()==Board.Cell.wolf:
        return 100 if s.is_winning() else -100
    if s.at()==Board.Cell.water:
        return -100
    return r
```

----------------------------------------

TITLE: Processing Tags into Columns - Pandas - Python
DESCRIPTION: This snippet uses pandas to iterate through the 'Tags' column of a DataFrame and create new binary columns based on the presence of specific tag strings. It handles a special case for 'Group' and 'Travelers with friends' tags by combining them into a single 'Group' column. The resulting columns indicate whether a review is associated with a particular tag.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_0

LANGUAGE: python
CODE:
```
# Process the Tags into new columns
# The file Hotel_Reviews_Tags.py, identifies the most important tags
# Leisure trip, Couple, Solo traveler, Business trip, Group combined with Travelers with friends, 
# Family with young children, Family with older children, With a pet
df["Leisure_trip"] = df.Tags.apply(lambda tag: 1 if "Leisure trip" in tag else 0)
df["Couple"] = df.Tags.apply(lambda tag: 1 if "Couple" in tag else 0)
df["Solo_traveler"] = df.Tags.apply(lambda tag: 1 if "Solo traveler" in tag else 0)
df["Business_trip"] = df.Tags.apply(lambda tag: 1 if "Business trip" in tag else 0)
df["Group"] = df.Tags.apply(lambda tag: 1 if "Group" in tag or "Travelers with friends" in tag else 0)
df["Family_with_young_children"] = df.Tags.apply(lambda tag: 1 if "Family with young children" in tag else 0)
df["Family_with_older_children"] = df.Tags.apply(lambda tag: 1 if "Family with older children" in tag else 0)
df["With_a_pet"] = df.Tags.apply(lambda tag: 1 if "With a pet" in tag else 0)

```

----------------------------------------

TITLE: Creating Scatter Plot with Seaborn FacetGrid - Python
DESCRIPTION: This snippet uses seaborn's FacetGrid to generate a scatter plot. It visualizes the correlation between 'popularity' and 'danceability' columns from a DataFrame 'df', using 'artist_top_genre' to color-code the points. FacetGrid allows creating a grid of plots based on categories, although here 'hue' is used within a single plot to differentiate points. It requires the seaborn library and matplotlib.pyplot (aliased as plt), along with a DataFrame 'df' containing the necessary columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/README.md#_snippet_10

LANGUAGE: python
CODE:
```
sns.FacetGrid(df, hue="artist_top_genre", height=5) \
   .map(plt.scatter, "popularity", "danceability") \
   .add_legend()
```

----------------------------------------

TITLE: Defining Sentiment Calculation Function Python
DESCRIPTION: Defines the `calc_sentiment` function. It takes a review string as input and returns 0 if the review text is 'No Negative' or 'No Positive'. Otherwise, it computes and returns the compound sentiment score using the initialized VADER analyzer.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/3-notebook.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
# There are 3 possibilities of input for a review:
# It could be "No Negative", in which case, return 0
# It could be "No Positive", in which case, return 0
# It could be a review, in which case calculate the sentiment
def calc_sentiment(review):    
    if review == "No Negative" or review == "No Positive":
        return 0
    return vader_sentiment.polarity_scores(review)["compound"]    
```

----------------------------------------

TITLE: Initializing CartPole Environment and Spaces - Python
DESCRIPTION: Initializes the 'CartPole-v1' environment from OpenAI Gym. It then prints the structure and contents of the environment's action space (discrete with two actions: left/right) and observation space (continuous values). Finally, it demonstrates sampling a random action from the action space.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/translations/README.ko.md#_snippet_1

LANGUAGE: python
CODE:
```
env = gym.make("CartPole-v1")\nprint(env.action_space)\nprint(env.observation_space)\nprint(env.action_space.sample())
```

----------------------------------------

TITLE: Applying Strict Greedy Q-Policy in Python
DESCRIPTION: Defines a strict greedy policy that selects the action with the maximum Q-value from the Q-Table for the current state. It requires the environment object, the Q-Table, and helper functions like `probs`. This policy is used with a `walk` function to simulate navigation, but can result in infinite loops in cyclical states.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/8-Reinforcement/1-QLearning/README.md#_snippet_0

LANGUAGE: python
CODE:
```
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

----------------------------------------

TITLE: Removing Stopwords from Text Columns using NLTK and Pandas
DESCRIPTION: This snippet demonstrates how to remove common English stopwords from the 'Negative_Review' and 'Positive_Review' columns of a pandas DataFrame (`df`). It utilizes NLTK's stopwords corpus, creating a set for efficient lookup. The defined `remove_stopwords` function filters out these words before joining the remaining words back into a string. This preprocessing step is intended to potentially improve the performance of subsequent text analysis like sentiment analysis.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/fr/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_3

LANGUAGE: python
CODE:
```
from nltk.corpus import stopwords

# Load the hotel reviews from CSV
df = pd.read_csv("../../data/Hotel_Reviews_Filtered.csv")

# Remove stop words - can be slow for a lot of text!
# Ryan Han (ryanxjhan on Kaggle) has a great post measuring performance of different stop words removal approaches
# https://www.kaggle.com/ryanxjhan/fast-stop-words-removal # using the approach that Ryan recommends
start = time.time()
cache = set(stopwords.words("english"))
def remove_stopwords(review):
    text = " ".join([word for word in review.split() if word not in cache])
    return text

# Remove the stop words from both columns
df.Negative_Review = df.Negative_Review.apply(remove_stopwords)   
df.Positive_Review = df.Positive_Review.apply(remove_stopwords)

```

----------------------------------------

TITLE: Creating Tag Indicator Columns using Pandas
DESCRIPTION: This snippet adds new binary columns to a pandas DataFrame (`df`). Each column represents a specific tag (e.g., 'Leisure_trip', 'Couple', 'Group'). The value is 1 if the corresponding tag is found within the 'Tags' column for that row, and 0 otherwise. The 'Group' column combines 'Group' and 'Travelers with friends'. Requires a pandas DataFrame with a 'Tags' column.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/fr/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_0

LANGUAGE: python
CODE:
```
# Process the Tags into new columns
# The file Hotel_Reviews_Tags.py, identifies the most important tags
# Leisure trip, Couple, Solo traveler, Business trip, Group combined with Travelers with friends, 
# Family with young children, Family with older children, With a pet
df["Leisure_trip"] = df.Tags.apply(lambda tag: 1 if "Leisure trip" in tag else 0)
df["Couple"] = df.Tags.apply(lambda tag: 1 if "Couple" in tag else 0)
df["Solo_traveler"] = df.Tags.apply(lambda tag: 1 if "Solo traveler" in tag else 0)
df["Business_trip"] = df.Tags.apply(lambda tag: 1 if "Business trip" in tag else 0)
df["Group"] = df.Tags.apply(lambda tag: 1 if "Group" in tag or "Travelers with friends" in tag else 0)
df["Family_with_young_children"] = df.Tags.apply(lambda tag: 1 if "Family with young children" in tag else 0)
df["Family_with_older_children"] = df.Tags.apply(lambda tag: 1 if "Family with older children" in tag else 0)
df["With_a_pet"] = df.Tags.apply(lambda tag: 1 if "With a pet" in tag else 0)

```

----------------------------------------

TITLE: Defining Probabilistic Q-Learning Policy - Python
DESCRIPTION: Defines a policy function `qpolicy` that takes the environment object `m` as input. It gets the current state, applies the `probs` function to the Q-values for that state to get probabilities, selects an action probabilistically using `random.choices` based on these probabilities, and returns the chosen action. This policy incorporates exploration and exploitation, typically leading to better path discovery than a strict policy. It is then evaluated multiple times using a `print_statistics` function to show performance metrics like average path length. Requires the trained `Q` table, `probs` function, `actions`, Python's `random` module, and a `print_statistics` function.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/8-Reinforcement/1-QLearning/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

----------------------------------------

TITLE: Clean Column Names using Janitor in R
DESCRIPTION: This snippet uses the 'janitor::clean_names()' function to standardize the column names of the 'pumpkins' data frame, converting them to snake_case. It then displays the updated column names to confirm the transformation. This is a common data cleaning step. Requires the 'janitor' package.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_3

LANGUAGE: R
CODE:
```
pumpkins <- pumpkins %>%
  clean_names(case = "snake")

# Return column names
pumpkins %>%
  names()
```

----------------------------------------

TITLE: Visualizing Relationships with Seaborn catplot Python
DESCRIPTION: This snippet visualizes the relationships between 'Item Size', 'Variety', and 'Color' using a Seaborn catplot. It uses a custom color palette for the 'Color' variable, adds the re-encoded 'Item Size' column to the pumpkins DataFrame, and configures the plot's appearance, labels, and limits. It requires the Seaborn library and input DataFrames (`pumpkins`, `encoded_pumpkins`).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/2-Regression/4-Logistic/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }
    pumpkins['Item Size'] = encoded_pumpkins['ord__Item Size']

    g = sns.catplot(
        data=pumpkins,
        x="Item Size", y="Color", row='Variety',
        kind="box", orient="h",
        sharex=False, margin_titles=True,
        height=1.8, aspect=4, palette=palette,
    )
    g.set(xlabel="Item Size", ylabel="").set(xlim=(0,6))
    g.set_titles(row_template="{row_name}")
```

----------------------------------------

TITLE: Calculating Training MAPE Python
DESCRIPTION: This snippet calculates the Mean Absolute Percentage Error (MAPE) for the training data predictions. It uses a `mape` function to compare the predicted (`y_train_pred`) and actual (`y_train`) values. The result is multiplied by 100 to present the error as a percentage.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_21

LANGUAGE: python
CODE:
```
print('MAPE for training data: ', mape(y_train_pred, y_train)*100, '%')
```

----------------------------------------

TITLE: Applying Probabilistic Q-Policy in Python
DESCRIPTION: Defines a probabilistic policy that selects actions based on probabilities derived from the Q-Table values for the current state, using `random.choices`. This combines exploitation (higher Q-values are more likely) and exploration (other actions still have a chance). It is used with `print_statistics` to evaluate average performance over many trials, showing improved path length compared to the strict policy.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/8-Reinforcement/1-QLearning/README.md#_snippet_1

LANGUAGE: python
CODE:
```
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

----------------------------------------

TITLE: Displaying Test Sample Features and Label with Pandas (Python)
DESCRIPTION: This snippet accesses the 50th data point from the test set (`X_test` and `y_test`) to display its contents. It filters the feature data (`X_test.iloc[50]`) to show only the non-zero ingredients and prints them along with the corresponding true cuisine from `y_test.iloc[50]`. This helps visualize the data point the model will predict on.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/4-Classification/2-Classifiers-1/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')
print(f'cuisine: {y_test.iloc[50]}')
```

----------------------------------------

TITLE: Importing Scikit-learn Modules for Polynomial Regression Pipeline Python
DESCRIPTION: This snippet imports the necessary classes and functions from scikit-learn to implement polynomial regression using a pipeline. It imports `PolynomialFeatures`, which is used to generate polynomial and interaction features, and `make_pipeline`, a convenient function to create a pipeline object that chains multiple estimators. These imports are needed to set up the polynomial regression pipeline.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/3-Linear/README.md#_snippet_12

LANGUAGE: python
CODE:
```
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
```

----------------------------------------

TITLE: Implementing Probabilistic Q-Learning Policy Python
DESCRIPTION: This Python function defines a probabilistic policy where actions are selected based on probabilities derived from the Q-values for the current state, using the `probs` function and `random.choices`. This policy balances exploration and exploitation and is evaluated by calling a `print_statistics` function.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/README.md#_snippet_9

LANGUAGE: Python
CODE:
```
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

----------------------------------------

TITLE: Printing Most Frequent Nationality Python
DESCRIPTION: This snippet prints the reviewer nationality that appears most frequently in the dataset and its corresponding count. It accesses the first element of the index (nationality) and values (count) from the `nationality_freq` Series.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/4-Hotel-Reviews-1/solution/notebook.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
# What reviewer nationality is the most common in the dataset?
print("The highest frequency reviewer nationality is " + str(nationality_freq.index[0]).strip() + " with " + str(nationality_freq[0]) + " reviews.")
```

----------------------------------------

TITLE: Find Most Reviewed Hotel Per Top Nationality with Pandas Python
DESCRIPTION: This snippet iterates through the top 10 nationalities identified earlier. For each nationality, it filters the main `df` DataFrame to select only reviews from that nationality, then calculates the frequency of 'Hotel_Name' within that subset to find and print the most reviewed hotel.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_4

LANGUAGE: python
CODE:
```
# What was the most frequently reviewed hotel for the top 10 nationalities
# Normally with pandas you will avoid an explicit loop, but wanted to show creating a new dataframe using criteria (don't do this with large amounts of data because it could be very slow)
for nat in nationality_freq[:10].index:
   # First, extract all the rows that match the criteria into a new dataframe
   nat_df = df[df["Reviewer_Nationality"] == nat]   
   # Now get the hotel freq
   freq = nat_df["Hotel_Name"].value_counts()
   print("The most reviewed hotel for " + str(nat).strip() + " was " + str(freq.index[0]) + " with " + str(freq[0]) + " reviews.")
```

----------------------------------------

TITLE: Plotting Mean Price by Pumpkin Variety
DESCRIPTION: This snippet groups the 'new_pumpkins' DataFrame by 'Variety', calculates the mean 'Price' for each group, and plots the results as a bar chart. This visually confirms the impact of pumpkin variety on the average price.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/2-Regression/3-Linear/README.md#_snippet_2

LANGUAGE: python
CODE:
```
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

----------------------------------------

TITLE: Implementing Q-Learning Training Algorithm Python
DESCRIPTION: Executes the main Q-learning training loop over a specified number of epochs (10,000). In each epoch, it simulates an episode starting from a random position, selects actions probabilistically based on the current Q-values, receives a reward, and updates the Q-table using the Bellman equation with a decaying learning rate (alpha) and a fixed discount factor (gamma).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/notebook.ipynb#_snippet_10

LANGUAGE: Python
CODE:
```
from IPython.display import clear_output

lpath = []

for epoch in range(10000):
    clear_output(wait=True)
    print(f"Epoch = {epoch}",end='')

    # Pick initial point
    m.random_start()

    # Start travelling
    n=0
    cum_reward = 0
    while True:
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        dpos = actions[a]
        m.move(dpos,check_correctness=False) # we allow player to move outside the board, which terminates episode
        r = reward(m)
        cum_reward += r
        if r==end_reward or cum_reward < -1000:
            print(f" {n} steps",end='\r')
            lpath.append(n)
            break
        alpha = np.exp(-n / 3000)
        gamma = 0.5
        ai = action_idx[a]
        Q[x,y,ai] = (1 - alpha) * Q[x,y,ai] + alpha * (r + gamma * Q[x+dpos[0], y+dpos[1]].max())
        n+=1
```

----------------------------------------

TITLE: Visualizing Feature Relationships using Seaborn catplot (Python)
DESCRIPTION: This snippet uses Seaborn's `catplot` function to visualize the relationship between encoded 'Item Size', 'Variety', and 'Color' using a horizontal box plot. It defines a color palette, assigns the encoded size to the main DataFrame, and configures the plot layout and appearance, including setting axis labels and limits. It requires the `pumpkins` and `encoded_pumpkins` DataFrames.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/2-Regression/4-Logistic/README.md#_snippet_0

LANGUAGE: python
CODE:
```
palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }
pumpkins['Item Size'] = encoded_pumpkins['ord__Item Size']

g = sns.catplot(
    data=pumpkins,
    x="Item Size", y="Color", row='Variety',
    kind="box", orient="h",
    sharex=False, margin_titles=True,
    height=1.8, aspect=4, palette=palette,
)
g.set(xlabel="Item Size", ylabel="").set(xlim=(0,6))
g.set_titles(row_template="{row_name}")
```

----------------------------------------

TITLE: Visualizing Data Relationships with Seaborn Catplot - Python
DESCRIPTION: This snippet visualizes the relationships between 'Item Size', 'Variety', and 'Color' using a Seaborn categorical plot ('catplot') with a box plot kind. It uses a custom color palette and configures axes limits and titles for clarity. Requires Seaborn and Pandas (for dataframes like 'pumpkins').
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/4-Logistic/README.md#_snippet_8

LANGUAGE: python
CODE:
```
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }
    pumpkins['Item Size'] = encoded_pumpkins['ord__Item Size']

    g = sns.catplot(
        data=pumpkins,
        x="Item Size", y="Color", row='Variety',
        kind="box", orient="h",
        sharex=False, margin_titles=True,
        height=1.8, aspect=4, palette=palette,
    )
    g.set(xlabel="Item Size", ylabel="").set(xlim=(0,6))
    g.set_titles(row_template="{row_name}")
```

----------------------------------------

TITLE: Visualizing R K-Means Clustering Results
DESCRIPTION: This code visualizes the results of a K-Means clustering model using the factoextra package. It helps in visually inspecting the separation and density of the identified clusters based on the data features used for clustering. This visualization aids in understanding the quality of the clustering.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb#_snippet_9

LANGUAGE: R
CODE:
```
library(factoextra)

# Visualize clustering results
fviz_cluster(kclust, df_numeric_select)
```

----------------------------------------

TITLE: Calculate Mean Absolute Percentage Error (APE) per Step in Python
DESCRIPTION: If the forecasting horizon is greater than 1, this code calculates the Absolute Percentage Error (APE) for each prediction and then groups the results by forecasting step (`h`) to show the average APE at each horizon step.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/7-TimeSeries/2-ARIMA/README.md#_snippet_5

LANGUAGE: python
CODE:
```
if(HORIZON > 1):
 eval_df['APE'] = (eval_df['prediction'] - eval_df['actual']).abs() / eval_df['actual']
 print(eval_df.groupby('h')['APE'].mean())
```

----------------------------------------

TITLE: Combining Balanced DataFrames - Python
DESCRIPTION: Concatenates the resampled label dataframe (`transformed_label_df`) and the resampled feature dataframe (`transformed_feature_df`) horizontally along axis 1. This creates a new combined dataframe (`transformed_df`) ready for saving or analysis. Requires pandas DataFrames.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/README.md#_snippet_16

LANGUAGE: python
CODE:
```
transformed_df = pd.concat([transformed_label_df,transformed_feature_df],axis=1, join='outer')
```

----------------------------------------

TITLE: Creating Binary Features from Hotel Review Tags - Python
DESCRIPTION: Generates new binary (0 or 1) columns in the DataFrame based on the presence of specific travel tags within the 'Tags' column. This transforms categorical text data into numerical features suitable for machine learning models.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
df["Leisure_trip"] = df.Tags.apply(lambda tag: 1 if "Leisure trip" in tag else 0)
df["Couple"] = df.Tags.apply(lambda tag: 1 if "Couple" in tag else 0)
df["Solo_traveler"] = df.Tags.apply(lambda tag: 1 if "Solo traveler" in tag else 0)
df["Business_trip"] = df.Tags.apply(lambda tag: 1 if "Business trip" in tag else 0)
df["Group"] = df.Tags.apply(lambda tag: 1 if "Group" in tag or "Travelers with friends" in tag else 0)
df["Family_with_young_children"] = df.Tags.apply(lambda tag: 1 if "Family with young children" in tag else 0)
df["Family_with_older_children"] = df.Tags.apply(lambda tag: 1 if "Family with older children" in tag else 0)
df["With_a_pet"] = df.Tags.apply(lambda tag: 1 if "With a pet" in tag else 0)
```

----------------------------------------

TITLE: Calculating Mean Absolute Percentage Error by Forecast Step Python
DESCRIPTION: Calculates the Absolute Percentage Error (APE) for each prediction and then groups the results by the forecast horizon step ('h', e.g., t+1, t+2). It prints the mean APE for each step, providing insight into how prediction accuracy changes as the forecast horizon increases.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/solution/notebook.ipynb#_snippet_15

LANGUAGE: python
CODE:
```
if(HORIZON > 1):
    eval_df['APE'] = (eval_df['prediction'] - eval_df['actual']).abs() / eval_df['actual']
    print(eval_df.groupby('h')['APE'].mean())
```

----------------------------------------

TITLE: Plot Running Average of Training Rewards - Python
DESCRIPTION: Defines a `running_average` function to calculate the moving average of a list and then plots the running average of the cumulative rewards. This smooths the training curve and provides a clearer view of the agent's learning trend over time.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/8-Reinforcement/2-Gym/README.md#_snippet_12

LANGUAGE: Python
CODE:
```
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

----------------------------------------

TITLE: Analyze Single Prediction Probabilities - Python
DESCRIPTION: Analyzes the prediction probabilities generated by the trained model (model) for a single test data point (row 50). It reshapes the data, uses predict_proba to get probabilities for each class, creates a pandas DataFrame, and sorts it to show the top predicted classes and their probabilities. Requires scikit-learn, pandas, and numpy.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/4-Classification/2-Classifiers-1/README.md#_snippet_2

LANGUAGE: python
CODE:
```
test= X_test.iloc[50].values.reshape(-1, 1).T
proba = model.predict_proba(test)
classes = model.classes_
resultdf = pd.DataFrame(data=proba, columns=classes)

topPrediction = resultdf.T.sort_values(by=[0], ascending = [False])
topPrediction.head()
```

----------------------------------------

TITLE: Saving Processed Data to CSV Python
DESCRIPTION: Saves the final DataFrame, including the original data, cleaned reviews, and calculated sentiment scores, to a new CSV file named 'Hotel_Reviews_NLP.csv' in the '../../data/' directory. The `index=False` argument prevents writing the DataFrame index to the file.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/3-notebook.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
print("Saving results to Hotel_Reviews_NLP.csv")
df.to_csv(r"../../data/Hotel_Reviews_NLP.csv", index = False)

```

----------------------------------------

TITLE: Importing ONNX Runtime Web Script
DESCRIPTION: Includes a `<script>` tag that loads the ONNX Runtime Web library from a Content Delivery Network (CDN). This library, aliased as `ort`, is essential for running ONNX models directly within a web browser environment.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/4-Classification/4-Applied/README.md#_snippet_2

LANGUAGE: html
CODE:
```
<script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.9.0/dist/ort.min.js"></script> 
```

----------------------------------------

TITLE: Displaying Dimensions of Cuisine Tibbles in R
DESCRIPTION: Prints the dimensions (number of rows and columns) of each individual cuisine-specific tibble created in the previous step. This provides a quick summary of how much data is available for each cuisine category, using the `dim()` function and formatted console output with `cat()`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb#_snippet_5

LANGUAGE: R
CODE:
```
# Find out how much data is available per cuisine
cat(" thai df:", dim(thai_df), "\n",
    "japanese df:", dim(japanese_df), "\n",
    "chinese_df:", dim(chinese_df), "\n",
    "indian_df:", dim(indian_df), "\n",
    "korean_df:", dim(korean_df))
```

----------------------------------------

TITLE: Observe Step Function Output During Simulation - Python
DESCRIPTION: Illustrates the output of the `env.step()` function during a simulation run until the episode is done. It resets the environment, enters a loop, renders the state, takes a random action, and prints the returned observation, reward, done flag, and info dictionary.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/solution/notebook.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
env.reset()

done = False
while not done:
   env.render()
   obs, rew, done, info = env.step(env.action_space.sample())
   print(f"{obs} -> {rew}")
env.close()
```

----------------------------------------

TITLE: Loading Data with Pandas
DESCRIPTION: Imports the pandas library and loads a CSV file located at '../../data/Hotel_Reviews_Filtered.csv' into a pandas DataFrame named `df`. This is the initial step to read the dataset for further processing.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/2-notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import pandas as pd 

df = pd.read_csv('../../data/Hotel_Reviews_Filtered.csv')
```

----------------------------------------

TITLE: Installing R Packages for Classification
DESCRIPTION: Installs a list of R packages required for the classification module, including `tidyverse`, `tidymodels`, `themis`, `kernlab`, `ranger`, `xgboost`, and `kknn`. These packages are used for data manipulation, modeling, handling imbalanced data, and implementing specific classification algorithms like SVM and KNN. Requires an R environment with internet access.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/solution/R/lesson_12-R.ipynb#_snippet_0

LANGUAGE: R
CODE:
```
install.packages(c("tidyverse", "tidymodels", "kernlab", "themis", "ranger", "xgboost", "kknn"))
```

----------------------------------------

TITLE: Saving Processed Hotel Reviews DataFrame to CSV Python
DESCRIPTION: Removes specified columns related to word counts and review metadata from a Pandas DataFrame ('df') in place. It then saves the modified DataFrame to a CSV file named 'Hotel_Reviews_Filtered.csv' in the '../data/' directory, excluding the DataFrame index.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
df.drop(["Review_Total_Negative_Word_Counts", "Review_Total_Positive_Word_Counts", "days_since_review", "Total_Number_of_Reviews_Reviewer_Has_Given"], axis = 1, inplace=True)

# Saving new data file with calculated columns
print("Saving results to Hotel_Reviews_Filtered.csv")
df.to_csv(r'../data/Hotel_Reviews_Filtered.csv', index = False)
```

----------------------------------------

TITLE: Calculate One-Step Forecast MAPE in Python
DESCRIPTION: This snippet calculates and prints the Mean Absolute Percentage Error (MAPE) specifically for the first step of the forecast horizon ('t+1'). It filters the `eval_df` to include only 't+1' predictions and actuals, applies the `mape` function (which needs to be defined elsewhere), multiplies by 100 to get a percentage, and prints the result.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/README.md#_snippet_15

LANGUAGE: python
CODE:
```
print('One step forecast MAPE: ', (mape(eval_df[eval_df['h'] == 't+1']['prediction'], eval_df[eval_df['h'] == 't+1']['actual']))*100, '%')
```

----------------------------------------

TITLE: Plotting Raw Rewards Python
DESCRIPTION: Plots the raw cumulative reward collected during training iterations using `matplotlib`. This visualization often appears noisy due to the stochastic nature of the training process and varying episode lengths. Requires `rewards` data and `matplotlib`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/8-Reinforcement/2-Gym/README.md#_snippet_0

LANGUAGE: python
CODE:
```
plt.plot(rewards)
```

----------------------------------------

TITLE: Visualizing Trained Q-Table Python
DESCRIPTION: Calls the board's plot function, passing the Q-table after the completion of the Q-learning training loop. This visualizes the learned Q-values, showing the 'attractiveness' of different actions at various locations on the board after training.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/notebook.ipynb#_snippet_11

LANGUAGE: Python
CODE:
```
m.plot(Q)
```

----------------------------------------

TITLE: Cleaning Tags Column with Pandas
DESCRIPTION: Cleans the 'Tags' column of the `df` DataFrame by removing leading/trailing bracket characters (`[']`) and replacing the internal separator string (`', '`) with a single comma (`,`). This standardizes the tag strings for splitting.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/2-notebook.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
df.Tags = df.Tags.str.strip("[']")
# remove all quotes too
df.Tags = df.Tags.str.replace(" ', '", ",", regex = False)
```

----------------------------------------

TITLE: Implementing Strict Greedy Q-Policy in Python
DESCRIPTION: This Python code defines a strict greedy policy function that selects the action with the highest Q-value for the current state. It uses the `probs` function to get weighted values and `np.argmax` to find the index of the maximum, then maps it back to an action. The second line demonstrates how this policy can be used with a `walk` function.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/sw/8-Reinforcement/1-QLearning/README.md#_snippet_2

LANGUAGE: python
CODE:
```
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a
```

LANGUAGE: python
CODE:
```
walk(m,qpolicy_strict)
```

----------------------------------------

TITLE: Importing Libraries and Downloading NLTK Data Python
DESCRIPTION: Imports necessary libraries including time for timing operations, pandas for data manipulation, and nltk components (stopwords, SentimentIntensityAnalyzer) for natural language processing. It also downloads the 'vader_lexicon' required by the VADER analyzer.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/3-notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import time
import pandas as pd
import nltk as nltk
from nltk.corpus import stopwords
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')
```

----------------------------------------

TITLE: Checking Label Encoder Mapping - Python
DESCRIPTION: This snippet uses the `inverse_transform` method of the fitted `LabelEncoder` to show the original category names corresponding to the encoded numerical values (0 and 1). This helps verify the encoding mapping for the 'Color' variable.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/notebook.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
# Let's look at the mapping between the encoded values and the original values
list(label_encoder.inverse_transform([0, 1]))
```

----------------------------------------

TITLE: Visualizing Relationships with Grouped Box Plot in R
DESCRIPTION: Uses ggplot2 to create a grouped box plot visualizing the relationship between encoded item size, variety, and color. It defines a custom color palette and applies various theme adjustments for clarity. Requires ggplot2 and assumes 'pumpkins_select_plot' dataframe with 'item_size', 'variety', and 'color' columns is available.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/R/lesson_4-R.ipynb#_snippet_6

LANGUAGE: R
CODE:
```
# Define the color palette
palette <- c(ORANGE = "orange", WHITE = "wheat")

# We need the encoded Item Size column to use it as the x-axis values in the plot
pumpkins_select_plot<-pumpkins_select
pumpkins_select_plot$item_size <- baked_pumpkins$item_size

# Create the grouped box plot
ggplot(pumpkins_select_plot, aes(x = `item_size`, y = color, fill = color)) +
  geom_boxplot() +
  facet_grid(variety ~ ., scales = "free_x") +
  scale_fill_manual(values = palette) +
  labs(x = "Item Size", y = "") +
  theme_minimal() +
  theme(strip.text = element_text(size = 12)) +
  theme(axis.text.x = element_text(size = 10)) +
  theme(axis.title.x = element_text(size = 12)) +
  theme(axis.title.y = element_blank()) +
  theme(legend.position = "bottom") +
  guides(fill = guide_legend(title = "Color")) +
  theme(panel.spacing = unit(0.5, "lines"))+
  theme(strip.text.y = element_text(size = 4, hjust = 0))
```

----------------------------------------

TITLE: Filtering and Plotting Top 3 Genres with Popularity > 0 Python
DESCRIPTION: Filters the DataFrame to include only songs belonging to the top three specified genres ('afro dancehall', 'afropop', 'nigerian pop') and having a popularity greater than 0. It then plots the counts of these selected genres.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/notebook.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
df = df[(df['artist_top_genre'] == 'afro dancehall') | (df['artist_top_genre'] == 'afropop') | (df['artist_top_genre'] == 'nigerian pop')]
df = df[(df['popularity'] > 0)]
top = df['artist_top_genre'].value_counts()
plt.figure(figsize=(10,7))
sns.barplot(x=top.index,y=top.values)
plt.xticks(rotation=45)
plt.title('Top genres',color = 'blue')
```

----------------------------------------

TITLE: Helper Function to Convert Q-Values to Probabilities Python
DESCRIPTION: Defines a utility function `probs` that takes a vector of Q-values for a given state and converts them into a probability distribution over actions. A small epsilon is added to avoid division by zero and ensure all actions have a non-zero probability initially.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/notebook.ipynb#_snippet_9

LANGUAGE: Python
CODE:
```
def probs(v,eps=1e-4):
    v = v-v.min()+eps
    v = v/v.sum()
    return v
```

----------------------------------------

TITLE: Sorting and Displaying Sentiment Results - Python
DESCRIPTION: This snippet demonstrates how to sort the DataFrame `df` based on the newly calculated sentiment scores. It sorts by 'Negative_Sentiment' in ascending order and prints the corresponding 'Negative_Review' and 'Negative_Sentiment' columns. It then repeats the process for 'Positive_Sentiment', sorting ascendingly and printing 'Positive_Review' and 'Positive_Sentiment'.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_6

LANGUAGE: python
CODE:
```
df = df.sort_values(by=["Negative_Sentiment"], ascending=True)
print(df[["Negative_Review", "Negative_Sentiment"]])
df = df.sort_values(by=["Positive_Sentiment"], ascending=True)
print(df[["Positive_Review", "Positive_Sentiment"]])
```

----------------------------------------

TITLE: Calculate Mean Absolute Percentage Error (APE) per Step in Python
DESCRIPTION: This code calculates the Absolute Percentage Error (APE) for each prediction point if the `HORIZON` is greater than 1. It computes the absolute difference between prediction and actual, divides by the actual value, and then groups the results by the forecast step ('h' column, e.g., 't+1', 't+2') to calculate and print the mean APE for each step.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/README.md#_snippet_14

LANGUAGE: python
CODE:
```
if(HORIZON > 1):
    eval_df['APE'] = (eval_df['prediction'] - eval_df['actual']).abs() / eval_df['actual']
    print(eval_df.groupby('h')['APE'].mean())
```

----------------------------------------

TITLE: Initializing Scikit-learn SVR Model (Python)
DESCRIPTION: Creates an instance of the Support Vector Regressor (SVR) model from the scikit-learn library. The model is configured with specific hyperparameters (RBF kernel, gamma, C, epsilon) chosen for this time series forecasting task.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_14

LANGUAGE: python
CODE:
```
model = SVR(kernel='rbf',gamma=0.5, C=10, epsilon = 0.05)
```

----------------------------------------

TITLE: Plotting Raw Training Rewards Python
DESCRIPTION: This Python snippet uses matplotlib to plot the raw cumulative reward obtained at the end of each simulation episode against the episode number. It takes the 'rewards' list collected during the Q-learning training process as input. This plot is shown to be noisy and difficult to interpret directly due to the variability in episode lengths. It requires the matplotlib.pyplot library.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/8-Reinforcement/2-Gym/README.md#_snippet_1

LANGUAGE: python
CODE:
```
plt.plot(rewards)
```

----------------------------------------

TITLE: Defining Actions and Action Mapping Python
DESCRIPTION: Defines the possible actions an agent can take ('U', 'D', 'L', 'R') and maps each action string to a tuple representing the corresponding change in (x, y) coordinates. Also creates a reverse mapping from action string to a numerical index.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/notebook.ipynb#_snippet_3

LANGUAGE: Python
CODE:
```
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

----------------------------------------

TITLE: Visualizing Variety Counts by Color with Seaborn - Python
DESCRIPTION: This snippet uses the seaborn library to create a categorical bar plot (`sns.catplot` with `kind='count'`) visualizing the distribution of pumpkin `Variety` for each `Color` category. A custom color palette is defined for 'ORANGE' and 'WHITE'.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/notebook.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
import seaborn as sns
# Specify colors for each values of the hue variable
palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
}
# Plot a bar plot to visualize how many pumpkins of each variety are orange or white
sns.catplot(
    data=pumpkins, y="Variety", hue="Color", kind="count",
    palette=palette,
)
```

----------------------------------------

TITLE: Removing Stop Words from Review Text (Python)
DESCRIPTION: This snippet defines a function `remove_stopwords` that takes a text review, splits it into words, and joins back only those words not present in the NLTK English stop words list. It then applies this function to the 'Negative_Review' and 'Positive_Review' columns of the DataFrame `df` to clean the text data, aiming to speed up subsequent sentiment analysis. Requires pandas, NLTK stop words, and a DataFrame `df` with the specified columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_3

LANGUAGE: python
CODE:
```
from nltk.corpus import stopwords

# Load the hotel reviews from CSV
df = pd.read_csv("../../data/Hotel_Reviews_Filtered.csv")

# Remove stop words - can be slow for a lot of text!
# Ryan Han (ryanxjhan on Kaggle) has a great post measuring performance of different stop words removal approaches
# https://www.kaggle.com/ryanxjhan/fast-stop-words-removal # using the approach that Ryan recommends
start = time.time()
cache = set(stopwords.words("english"))
def remove_stopwords(review):
    text = " ".join([word for word in review.split() if word not in cache])
    return text

# Remove the stop words from both columns
df.Negative_Review = df.Negative_Review.apply(remove_stopwords)   
df.Positive_Review = df.Positive_Review.apply(remove_stopwords)
```

----------------------------------------

TITLE: Simulating Trained RL Agent Behavior Python
DESCRIPTION: This snippet simulates the trained reinforcement learning agent's behavior in the environment. It resets the environment, then enters a loop where it gets the current observation, discretizes it to find the state `s`, retrieves Q-values, converts them to probabilities `v`, samples an action `a` based on these probabilities, takes a step in the environment, renders the environment, and repeats until the episode is done. It uses the trained Q-Table (via `qvalues` and `probs`) to determine action probabilities.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/fr/8-Reinforcement/2-Gym/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

----------------------------------------

TITLE: Implementing Probability-Based Q-Policy Navigation Python
DESCRIPTION: This code defines an alternative navigation policy (`qpolicy`) that selects actions based on a probability distribution derived from the Q-Table values, similar to the training policy. It then uses this policy with a `print_statistics` function (which runs the simulation 100 times) to evaluate the average path length, typically resulting in shorter paths compared to the strict policy.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/8-Reinforcement/1-QLearning/README.md#_snippet_3

LANGUAGE: python
CODE:
```
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

----------------------------------------

TITLE: Evaluate K-Means Clustering 'Accuracy' by Comparing to Original Labels in Python
DESCRIPTION: Compares the cluster labels predicted by the K-Means model (`kmeans.labels_`) to the original, encoded genre labels (`y`) to estimate the 'accuracy' of the clustering relative to the known categories. This provides a basic evaluation metric, although it's important to note that K-Means is unsupervised and doesn't aim to perfectly replicate pre-existing labels. Requires the fitted `kmeans` model and the encoded original target vector `y`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/README.md#_snippet_7

LANGUAGE: python
CODE:
```
labels = kmeans.labels_

correct_labels = sum(y == labels)

print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))

print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))
```

----------------------------------------

TITLE: Counting Review Status with DataFrame Apply (Python)
DESCRIPTION: This snippet demonstrates how to count rows in a pandas DataFrame based on conditions in 'Negative_Review' and 'Positive_Review' columns using `df.apply` with lambda functions. It iterates through rows to check if values match 'No Negative' or 'No Positive' and measures the execution time. Dependencies include pandas for DataFrame operations and time for timing.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_7

LANGUAGE: Python
CODE:
```
# with lambdas:
start = time.time()
no_negative_reviews = df.apply(lambda x: True if x['Negative_Review'] == "No Negative" else False , axis=1)
print("Number of No Negative reviews: " + str(len(no_negative_reviews[no_negative_reviews == True].index)))

no_positive_reviews = df.apply(lambda x: True if x['Positive_Review'] == "No Positive" else False , axis=1)
print("Number of No Positive reviews: " + str(len(no_positive_reviews[no_positive_reviews == True].index)))

both_no_reviews = df.apply(lambda x: True if x['Negative_Review'] == "No Negative" and x['Positive_Review'] == "No Positive" else False , axis=1)
print("Number of both No Negative and No Positive reviews: " + str(len(both_no_reviews[both_no_reviews == True].index)))
end = time.time()
print("Lambdas took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Plotting Full Load Time Series Data (Python)
DESCRIPTION: Generates a line plot of the entire 'load' time series data available in the `energy` DataFrame. It sets the plot size, enables subplots (though only one series is plotted here), customizes label font sizes, and displays the plot using Matplotlib. Depends on the loaded `energy` DataFrame and Matplotlib.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/1-Introduction/solution/notebook.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
energy.plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)
plt.xlabel('timestamp', fontsize=12)
plt.ylabel('load', fontsize=12)
plt.show()
```

----------------------------------------

TITLE: Simulating Trained Model Performance Python
DESCRIPTION: Runs a single simulation episode in the environment `env` using the trained Q-Table. At each step, it discretizes the observation, gets Q-values, converts them to probabilities, and samples an action based on these probabilities using `random.choices`. The environment is rendered to visualize the process. Requires a trained model (Q-Table) and associated helper functions.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/8-Reinforcement/2-Gym/README.md#_snippet_2

LANGUAGE: python
CODE:
```
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

----------------------------------------

TITLE: Visualizing Data Distribution with Seaborn Jointplot (KDE) - Python
DESCRIPTION: This snippet uses the seaborn library to create a jointplot visualization. It displays the relationship between 'popularity' and 'danceability' data points from a pandas DataFrame, colored by 'artist_top_genre'. The 'kind="kde"' parameter specifies a Kernel Density Estimate plot, useful for visualizing the density distribution across different categories. It requires the seaborn library and a DataFrame named 'df' containing the specified columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/README.md#_snippet_9

LANGUAGE: python
CODE:
```
sns.set_theme(style="ticks")

g = sns.jointplot(
    data=df,
    x="popularity", y="danceability", hue="artist_top_genre",
    kind="kde",
)
```

----------------------------------------

TITLE: Plotting Q-Values on the Board (Python)
DESCRIPTION: Visualizes the learned Q-values on the grid board using the `plot` method of the `Board` object, passing the trained `Q` table. This typically displays arrows or colors indicating the agent's preferred action in each state according to the learned policy.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/assignment-solution.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
m.plot(Q)
```

----------------------------------------

TITLE: Dropping Columns and Saving Filtered DataFrame
DESCRIPTION: This code snippet removes specific columns from the DataFrame that are deemed less useful for subsequent analysis, such as word counts, days since review, and reviewer history. It then saves the modified DataFrame to a new CSV file named 'Hotel_Reviews_Filtered.csv', excluding the DataFrame index.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_1

LANGUAGE: python
CODE:
```
df.drop(["Review_Total_Negative_Word_Counts", "Review_Total_Positive_Word_Counts", "days_since_review", "Total_Number_of_Reviews_Reviewer_Has_Given"], axis = 1, inplace=True)

# Saving new data file with calculated columns
print("Saving results to Hotel_Reviews_Filtered.csv")
df.to_csv(r'../data/Hotel_Reviews_Filtered.csv', index = False)
```

----------------------------------------

TITLE: Converting Q-values to Probabilities in Python
DESCRIPTION: This function converts a vector of raw Q-values into a probability distribution over actions using a method similar to softmax, ensuring all probabilities are positive and sum to 1. It adds a small epsilon value to prevent division by zero, especially when initial Q-values are all the same.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/sw/8-Reinforcement/1-QLearning/README.md#_snippet_0

LANGUAGE: python
CODE:
```
def probs(v,eps=1e-4):
    v = v-v.min()+eps
    v = v/v.sum()
    return v
```

----------------------------------------

TITLE: Calculating and Plotting Running Average (Python)
DESCRIPTION: Defines the `running_average` function using `np.convolve` to smooth a time series. It then calls this function on the collected `rewards` and plots the result using `matplotlib.pyplot`. Requires `numpy` (for `np.convolve` and `np.ones`) and `matplotlib.pyplot` (for `plt.plot`).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/8-Reinforcement/2-Gym/README.md#_snippet_3

LANGUAGE: python
CODE:
```
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

----------------------------------------

TITLE: Filtering and Plotting Artist Genres (Excluding Missing) Python
DESCRIPTION: Filters the DataFrame to remove rows where the 'artist_top_genre' is 'Missing'. It then recalculates the genre counts and plots a bar chart of the remaining genres.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/notebook.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
df = df[df['artist_top_genre'] != 'Missing']
top = df['artist_top_genre'].value_counts()
plt.figure(figsize=(10,7))
sns.barplot(x=top.index,y=top.values)
plt.xticks(rotation=45)
plt.title('Top genres',color = 'blue')
```

----------------------------------------

TITLE: Defining Simulation State Class (Python)
DESCRIPTION: Creates a `state` class to encapsulate the dynamic state of the agent and environment. It tracks the board state, energy, fatigue, and whether the agent is dead. Methods include `update` to adjust state based on the current cell, `move` to apply an action and update state accordingly, and `is_winning` to check if the agent can defeat the wolf.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/assignment-solution.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
class state:
    def __init__(self,board,energy=10,fatigue=0,init=True):
        self.board = board
        self.energy = energy
        self.fatigue = fatigue
        self.dead = False
        if init:
            self.board.random_start()
        self.update()

    def at(self):
        return self.board.at()

    def update(self):
        if self.at() == Board.Cell.water:
            self.dead = True
            return
        if self.at() == Board.Cell.tree:
            self.fatigue = 0
        if self.at() == Board.Cell.apple:
            self.energy = 10

    def move(self,a):
        self.board.move(a)
        self.energy -= 1
        self.fatigue += 1
        self.update()

    def is_winning(self):
        return self.energy > self.fatigue
```

----------------------------------------

TITLE: Simulating Trained Agent Python
DESCRIPTION: This Python snippet demonstrates the behavior of the trained agent using the learned Q-table (either 'Q' or 'Qbest'). It runs a simulation episode, selecting actions based on the probability distribution derived from the Q-values for the current state (as calculated by the 'probs' function), and renders the environment visually using env.render(). It requires the trained Q-table, the gym environment, numpy, and random.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/8-Reinforcement/2-Gym/README.md#_snippet_3

LANGUAGE: python
CODE:
```
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

----------------------------------------

TITLE: Initializing Q-Table and Q-Value Retrieval Function (Python)
DESCRIPTION: Initializes the Q-table as an empty dictionary `Q` and defines possible `actions`. The `qvalues` function retrieves Q-values for all actions in a given `state`, defaulting to 0 if the state-action pair is not found in the table.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/8-Reinforcement/2-Gym/README.md#_snippet_0

LANGUAGE: python
CODE:
```
Q = {}
actions = (0,1)

def qvalues(state):
    return [Q.get((state,a),0) for a in actions]
```

----------------------------------------

TITLE: Counting Specific Review Types using Pandas Apply and Lambda
DESCRIPTION: This snippet demonstrates counting rows that contain 'No Negative' in the 'Negative_Review' column, 'No Positive' in the 'Positive_Review' column, and rows meeting both conditions. It uses the pandas `apply` method with lambda functions to create boolean masks and then counts the `True` values. Execution time is measured.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/sw/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_6

LANGUAGE: python
CODE:
```
# with lambdas:
start = time.time()
no_negative_reviews = df.apply(lambda x: True if x['Negative_Review'] == "No Negative" else False , axis=1)
print("Number of No Negative reviews: " + str(len(no_negative_reviews[no_negative_reviews == True].index)))

no_positive_reviews = df.apply(lambda x: True if x['Positive_Review'] == "No Positive" else False , axis=1)
print("Number of No Positive reviews: " + str(len(no_positive_reviews[no_positive_reviews == True].index)))

both_no_reviews = df.apply(lambda x: True if x['Negative_Review'] == "No Negative" and x['Positive_Review'] == "No Positive" else False , axis=1)
print("Number of both No Negative and No Positive reviews: " + str(len(both_no_reviews[both_no_reviews == True].index)))
end = time.time()
print("Lambdas took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Initializing Classifiers Dictionary with Linear SVC - Scikit-learn Python
DESCRIPTION: Sets the regularization parameter `C` and creates a dictionary named `classifiers` to hold different models. The first entry is a `Linear SVC` model initialized with a linear kernel, `C=10`, `probability=True` for estimates, and `random_state=0` for reproducibility. This dictionary structure allows easy iteration and comparison of multiple models.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
C = 10
# Create different classifiers.
classifiers = {
    'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0)
}
```

----------------------------------------

TITLE: Importing Libraries for Time Series Analysis in Python
DESCRIPTION: This snippet imports standard Python libraries necessary for time series data manipulation, plotting, numerical operations, and specifically, the SARIMAX model from statsmodels, scaling from sklearn, and custom utility functions.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/README.md#_snippet_0

LANGUAGE: python
CODE:
```
import os
import warnings
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import datetime as dt
import math

from pandas.plotting import autocorrelation_plot
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.preprocessing import MinMaxScaler
from common.utils import load_data, mape
from IPython.display import Image

%matplotlib inline
pd.options.display.float_format = '{:,.2f}'.format
np.set_printoptions(precision=2)
warnings.filterwarnings("ignore") # specify to ignore warning messages
```

----------------------------------------

TITLE: Running a Short CartPole Simulation (Rendered) - Python
DESCRIPTION: Runs a short simulation (100 steps) of the CartPole environment. At each step, it renders the environment visually using `env.render()` and takes a random action sampled from the action space using `env.action_space.sample()`. The simulation finishes after 100 steps or if the episode terminates, and the environment is closed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/translations/README.ko.md#_snippet_2

LANGUAGE: python
CODE:
```
env.reset()\n\nfor i in range(100):\n   env.render()\n   env.step(env.action_space.sample())\nenv.close()
```

----------------------------------------

TITLE: Filtering Data and Visualizing Genres (Python)
DESCRIPTION: Filters the DataFrame to include only specific popular genres ('afro dancehall', 'afropop', 'nigerian pop') and songs with a popularity greater than 0. It then calculates the counts for these filtered genres, creates a bar plot using Seaborn to visualize the distribution, and customizes the plot title and x-axis labels. This snippet requires the `df` DataFrame loaded and requires Matplotlib and Seaborn to be imported.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/notebook.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
df = df[(df['artist_top_genre'] == 'afro dancehall') | (df['artist_top_genre'] == 'afropop') | (df['artist_top_genre'] == 'nigerian pop')]
df = df[(df['popularity'] > 0)]
top = df['artist_top_genre'].value_counts()
plt.figure(figsize=(10,7))
sns.barplot(x=top.index,y=top.values)
plt.xticks(rotation=45)
plt.title('Top genres',color = 'blue')
```

----------------------------------------

TITLE: Setting Q-Learning Hyperparameters (Python)
DESCRIPTION: Sets the `alpha` (learning rate), `gamma` (discount factor), and `epsilon` (exploration/exploitation factor) variables. These control how the Q-table is updated and how actions are chosen during training.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/8-Reinforcement/2-Gym/README.md#_snippet_1

LANGUAGE: python
CODE:
```
# hyperparameters
alpha = 0.3
gamma = 0.9
epsilon = 0.90
```

----------------------------------------

TITLE: Selecting a Single Feature - Python
DESCRIPTION: Selects a specific feature (the 3rd feature, indexed 2) from the multi-dimensional feature array X to simplify the problem for univariate linear regression. Prints the new shape of the feature array.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/solution/notebook.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
# Selecting the 3rd feature
X = X[:, 2]
print(X.shape)
```

----------------------------------------

TITLE: Checking Distinct Color Values (R)
DESCRIPTION: Uses the `distinct()` function from `dplyr` to find and list all the unique values present in the `color` column of the `pumpkins_select` dataframe. This step confirms the specific categories ('ORANGE', 'WHITE') available for the binary classification problem.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/R/lesson_4-R.ipynb#_snippet_3

LANGUAGE: R
CODE:
```
# Subset distinct observations in outcome column
pumpkins_select %>%
  distinct(color)
```

----------------------------------------

TITLE: Analyzing and Plotting Top Japanese Ingredients - Python
DESCRIPTION: Applies the `create_ingredient_df` function to the Japanese cuisine data. It then retrieves the top 10 ingredients from the generated dataframe and visualizes them using a horizontal bar plot. Requires the function and the Japanese data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/README.md#_snippet_9

LANGUAGE: python
CODE:
```
japanese_ingredient_df = create_ingredient_df(japanese_df)
japanese_ingredient_df.head(10).plot.barh()
```

----------------------------------------

TITLE: Define VADER Sentiment Analysis Function using NLTK
DESCRIPTION: This code defines a Python function `calc_sentiment` using NLTK's VADER (Valence Aware Dictionary and sEntiment Reasoner) to compute the compound sentiment score of a given text review. It initializes a `SentimentIntensityAnalyzer` and handles specific input strings like 'No Negative' or 'No Positive' by returning a score of 0. For actual review text, it calculates and returns the compound sentiment score.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_4

LANGUAGE: python
CODE:
```
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Create the vader sentiment analyser (there are others in NLTK you can try too)
vader_sentiment = SentimentIntensityAnalyzer()
# Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.

# There are 3 possibilities of input for a review:
# It could be "No Negative", in which case, return 0
# It could be "No Positive", in which case, return 0
# It could be a review, in which case calculate the sentiment
def calc_sentiment(review):
    if review == "No Negative" or review == "No Positive":
        return 0
    return vader_sentiment.polarity_scores(review)["compound"]

```

----------------------------------------

TITLE: Implementing Exploration/Exploitation Q-Policy in Python
DESCRIPTION: This Python code defines a navigation policy that balances exploration and exploitation by selecting actions probabilistically based on their Q-values, using the `probs` function and `random.choices`. The second line shows how this policy is evaluated using a `print_statistics` function, which is expected to yield a shorter average path length compared to purely random exploration.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/sw/8-Reinforcement/1-QLearning/README.md#_snippet_3

LANGUAGE: python
CODE:
```
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a
```

LANGUAGE: python
CODE:
```
print_statistics(qpolicy)
```

----------------------------------------

TITLE: View Column Names in R
DESCRIPTION: This simple snippet uses the pipe operator (`%>%`) to pass the 'pumpkins' data frame to the 'names()' function. It returns and prints a character vector containing the names of all columns in the data frame, useful for initial data inspection.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_2

LANGUAGE: R
CODE:
```
pumpkins %>%
  names()
```

----------------------------------------

TITLE: Plot Raw Training Rewards - Python
DESCRIPTION: Plots the raw cumulative rewards obtained in each training epoch against the epoch number. This plot can appear noisy due to variations in simulation length and random choices.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/8-Reinforcement/2-Gym/README.md#_snippet_11

LANGUAGE: Python
CODE:
```
plt.plot(rewards)
```

----------------------------------------

TITLE: Defining Simple State Discretization Function - Python
DESCRIPTION: Defines a function `discretize` that takes a continuous observation vector `x` (a NumPy array) and converts it into a tuple of integers. It does this by dividing each component of the observation by specific scaling factors and then casting the result to an integer type. This method maps continuous values to a discrete state representation.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/translations/README.ko.md#_snippet_5

LANGUAGE: python
CODE:
```
def discretize(x):\n    return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
```

----------------------------------------

TITLE: Adding Tag-Based Feature Columns (Python)
DESCRIPTION: Creates new binary columns in the DataFrame ('Leisure_trip', 'Couple', 'Solo_traveler', 'Business_trip', 'Group', 'Family_with_young_children', 'Family_with_older_children', 'With_a_pet') based on keywords found in the existing 'Tags' column. Uses the pandas `apply` method with a lambda function.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_0

LANGUAGE: python
CODE:
```
df["Leisure_trip"] = df.Tags.apply(lambda tag: 1 if "Leisure trip" in tag else 0)
df["Couple"] = df.Tags.apply(lambda tag: 1 if "Couple" in tag else 0)
df["Solo_traveler"] = df.Tags.apply(lambda tag: 1 if "Solo traveler" in tag else 0)
df["Business_trip"] = df.Tags.apply(lambda tag: 1 if "Business trip" in tag else 0)
df["Group"] = df.Tags.apply(lambda tag: 1 if "Group" in tag or "Travelers with friends" in tag else 0)
df["Family_with_young_children"] = df.Tags.apply(lambda tag: 1 if "Family with young children" in tag else 0)
df["Family_with_older_children"] = df.Tags.apply(lambda tag: 1 if "Family with older children" in tag else 0)
df["With_a_pet"] = df.Tags.apply(lambda tag: 1 if "With a pet" in tag else 0)
```

----------------------------------------

TITLE: Initializing and Visualizing Environment Board Python
DESCRIPTION: Sets the dimensions for the grid board, creates an instance of the `Board` class, randomizes its layout using a fixed seed for reproducibility, and then visualizes the generated board state using a built-in plot function.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/notebook.ipynb#_snippet_2

LANGUAGE: Python
CODE:
```
width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

----------------------------------------

TITLE: Define Q-Learning Hyperparameters - Python
DESCRIPTION: Sets the key hyperparameters for the Q-Learning algorithm: `alpha` (learning rate), `gamma` (discount factor), and `epsilon` (exploration rate). These values control how the agent learns and balances exploration vs. exploitation.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/solution/notebook.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
# hyperparameters
alpha = 0.3
gamma = 0.9
epsilon = 0.90
```

----------------------------------------

TITLE: Initializing Q-Table - Python
DESCRIPTION: Initializes the Q-Table, a data structure used in Q-Learning to store the expected future reward for taking a specific action in a specific state. It is created as a 3D NumPy array with dimensions corresponding to the board's width, height, and the number of possible actions. All Q-values are initially set to an equal small positive value (1/number of actions) to represent a uniform initial policy. Requires the `numpy` library and the variables `width`, `height`, and `actions` to be defined.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/README.md#_snippet_5

LANGUAGE: Python
CODE:
```
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

----------------------------------------

TITLE: Displaying DataFrame Head (Pandas Python)
DESCRIPTION: Displays the first 5 rows of the `df` DataFrame. This provides a quick preview of the data structure, columns, and initial values, helping to confirm successful data loading and understand the data format.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/notebook.ipynb#_snippet_3

LANGUAGE: Python
CODE:
```
df.head()
```

----------------------------------------

TITLE: Initializing Data Scaler - scikit-learn - Python
DESCRIPTION: Imports the `StandardScaler` class from the scikit-learn preprocessing module. Initializes an instance of the `StandardScaler` named `scaler`. This object is typically used to standardize features by removing the mean and scaling to unit variance, although the actual scaling operation is commented out in this specific snippet.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/solution/tester.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# X = df.loc[:, ('danceability','energy')]
```

----------------------------------------

TITLE: Initializing Q-Table as Dictionary and Helper - Python
DESCRIPTION: Initializes the Q-Table as an empty Python dictionary `Q` because the state space is potentially large and sparse after discretization. It defines the possible actions as a tuple `actions`. A helper function `qvalues(state)` is defined to retrieve the Q-values for a given discrete state for all possible actions, returning 0 if the state-action pair is not found in the dictionary.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/translations/README.ko.md#_snippet_8

LANGUAGE: python
CODE:
```
Q = {}\nactions = (0,1)\n\ndef qvalues(state):\n    return [Q.get((state,a),0) for a in actions]
```

----------------------------------------

TITLE: Calculating Price Correlation with Time - Python
DESCRIPTION: Computes the Pearson correlation coefficient between the 'Month' and 'Price' columns, and between the 'DayOfYear' and 'Price' columns in the processed DataFrame. Prints these values to assess the strength and direction of the linear relationship between price and time.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/notebook.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

----------------------------------------

TITLE: Plotting Raw Training Rewards Python
DESCRIPTION: This snippet plots the raw cumulative reward collected at each training iteration. It uses the `rewards` vector which stores the reward values. This raw plot often shows significant variation due to the stochastic nature of the training process.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/fr/8-Reinforcement/2-Gym/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
plt.plot(rewards)
```

----------------------------------------

TITLE: Running Simulation with State Discretization Output Python
DESCRIPTION: Runs a simulation loop until the episode is done, taking random actions. It retrieves the continuous observation at each step and then applies a chosen discretization function (`discretize` or `discretize_bins`) to convert the observation into a discrete state representation. The resulting discrete state tuple is printed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/README.md#_snippet_7

LANGUAGE: python
CODE:
```
env.reset()

done = False
while not done:
   #env.render()
   obs, rew, done, info = env.step(env.action_space.sample())
   #print(discretize_bins(obs))
   print(discretize(obs))
env.close()
```

----------------------------------------

TITLE: Initializing Q-Table (Python)
DESCRIPTION: Initializes the Q-table (a NumPy array) with dimensions corresponding to the board width, height, and the number of possible actions. The table stores estimated future rewards for taking a specific action in a given state and is initially populated with small positive values.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/assignment-solution.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

----------------------------------------

TITLE: Print Observations and Rewards in Random Simulation - Python
DESCRIPTION: Resets the environment and runs a simulation using random actions until it's done. In each step, it prints the observation vector and the reward received, demonstrating the data structure provided by the environment.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/8-Reinforcement/2-Gym/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
env.reset()

done = False
while not done:
   env.render()
   obs, rew, done, info = env.step(env.action_space.sample())
   print(f"{obs} -> {rew}")
env.close()
```

----------------------------------------

TITLE: Create New DataFrame with Calculated Data (Python)
DESCRIPTION: Constructs a new Pandas DataFrame containing the extracted 'Month', original 'Package', 'Low Price', 'High Price', and the newly calculated 'Price' columns. This consolidates the relevant and transformed data into a clean structure.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/README.md#_snippet_4

LANGUAGE: python
CODE:
```
new_pumpkins = pd.DataFrame({'Month': month, 'Package': pumpkins['Package'], 'Low Price': pumpkins['Low Price'],'High Price': pumpkins['High Price'], 'Price': price})
```

----------------------------------------

TITLE: Adding Support Vector Classifier (SVC) to Dictionary - Scikit-learn Python
DESCRIPTION: This code snippet is intended to be added as an entry to the `classifiers` dictionary. It adds a standard `SVC` model under the key `'SVC'`. By default, this classifier uses a Radial Basis Function (RBF) kernel and default parameters, providing a general-purpose Support Vector Machine for classification.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/README.md#_snippet_5

LANGUAGE: Python
CODE:
```
'SVC': SVC(),
```

----------------------------------------

TITLE: Plotting Top Korean Ingredients (Pandas Matplotlib Python)
DESCRIPTION: Uses the `create_ingredient_df` function to analyze the Korean cuisine DataFrame (`korean_df`), selects the top 10 ingredients by count, and generates a horizontal bar plot to visualize their frequency.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/notebook.ipynb#_snippet_13

LANGUAGE: Python
CODE:
```
korean_ingredient_df = create_ingredient_df(korean_df)
korean_ingredient_df.head(10).plot.barh()
```

----------------------------------------

TITLE: Initializing the Grid Board (Python)
DESCRIPTION: Sets the dimensions for the grid board, creates a Board object, randomizes its content with a fixed seed (13) for reproducibility, and plots the initial state of the board.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/assignment-solution.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

----------------------------------------

TITLE: Visualizing 2D and 1D Density Plots by Genre in R
DESCRIPTION: Generates a 2D kernel density plot of popularity versus danceability colored by genre, along with separate 1D density plots for popularity and danceability, also colored by genre. The patchwork library is used to combine these plots into a single view to explore the distribution of these features across the top genres.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb#_snippet_11

LANGUAGE: R
CODE:
```
# Perform 2D kernel density estimation
density_estimate_2d <- nigerian_songs %>%
  ggplot(mapping = aes(x = popularity, y = danceability, color = artist_top_genre)) +
  geom_density_2d(bins = 5, size = 1) +
  paletteer::scale_color_paletteer_d("RSkittleBrewer::wildberry") +
  xlim(-20, 80) +
  ylim(0, 1.2)

# Density plot based on the popularity
density_estimate_pop <- nigerian_songs %>%
  ggplot(mapping = aes(x = popularity, fill = artist_top_genre, color = artist_top_genre)) +
  geom_density(size = 1, alpha = 0.5) +
  paletteer::scale_fill_paletteer_d("RSkittleBrewer::wildberry") +
  paletteer::scale_color_paletteer_d("RSkittleBrewer::wildberry") +
  theme(legend.position = "none")

# Density plot based on the danceability
density_estimate_dance <- nigerian_songs %>%
  ggplot(mapping = aes(x = danceability, fill = artist_top_genre, color = artist_top_genre)) +
  geom_density(size = 1, alpha = 0.5) +
  paletteer::scale_fill_paletteer_d("RSkittleBrewer::wildberry") +
  paletteer::scale_color_paletteer_d("RSkittleBrewer::wildberry")


# Patch everything together
library(patchwork)
density_estimate_2d / (density_estimate_pop + density_estimate_dance)
```

----------------------------------------

TITLE: Counting Review Categories using Pandas Apply and Lambdas (Python)
DESCRIPTION: This Python snippet counts the number of rows in a pandas DataFrame `df` where the `Negative_Review` column is "No Negative", the `Positive_Review` column is "No Positive", or both conditions are met, using the `apply` method with lambda functions and then filtering the resulting boolean Series. It also measures the execution time using `time.time()`. Requires pandas and the `time` module.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_1

LANGUAGE: python
CODE:
```
# with lambdas:
start = time.time()
no_negative_reviews = df.apply(lambda x: True if x['Negative_Review'] == "No Negative" else False , axis=1)
print("Number of No Negative reviews: " + str(len(no_negative_reviews[no_negative_reviews == True].index)))

no_positive_reviews = df.apply(lambda x: True if x['Positive_Review'] == "No Positive" else False , axis=1)
print("Number of No Positive reviews: " + str(len(no_positive_reviews[no_positive_reviews == True].index)))

both_no_reviews = df.apply(lambda x: True if x['Negative_Review'] == "No Negative" and x['Positive_Review'] == "No Positive" else False , axis=1)
print("Number of both No Negative and No Positive reviews: " + str(len(both_no_reviews[both_no_reviews == True].index)))
end = time.time()
print("Lambdas took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Printing Output in Jupyter Notebook (Python)
DESCRIPTION: This snippet demonstrates how to execute a simple Python print statement within a Jupyter notebook cell. It shows the basic functionality of running code blocks in the notebook environment and viewing the resulting standard output directly below the cell. This is a fundamental step in becoming comfortable with the interactive nature of notebooks for development and experimentation.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/README.md#_snippet_0

LANGUAGE: python
CODE:
```
print('hello notebook')
```

----------------------------------------

TITLE: Calculating Clustering Accuracy (Python)
DESCRIPTION: This code attempts to evaluate the accuracy of the K-Means clustering by comparing the predicted cluster labels to the original, encoded genre labels (`y`). It counts the number of data points where the predicted cluster label matches the original genre label and calculates the accuracy as the ratio of correctly matched samples to the total number of samples. This is a rough measure, as K-Means is unsupervised and doesn't directly aim to recover pre-defined labels.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/translations/README.ko.md#_snippet_7

LANGUAGE: Python
CODE:
```
labels = kmeans.labels_

correct_labels = sum(y == labels)

print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))

print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))
```

----------------------------------------

TITLE: Prepare Target Y Python
DESCRIPTION: Selects the 'cuisine' column from the DataFrame `data` to create the target variable `y` as a DataFrame. `y.head()` then shows the first few rows of the target DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/4-Classification/4-Applied/README.md#_snippet_3

LANGUAGE: python
CODE:
```
y = data[['cuisine']]
y.head()
```

----------------------------------------

TITLE: Calculating and Printing Training MAPE (Python)
DESCRIPTION: Calculates the Mean Absolute Percentage Error (MAPE) between the inverse-scaled training predictions and the inverse-scaled actual training values using a custom `mape` function. The result is printed as a percentage.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_20

LANGUAGE: python
CODE:
```
print('MAPE for training data: ', mape(y_train_pred, y_train)*100, '%')
```

----------------------------------------

TITLE: Exploring Unique Values in 'Item Size' - Python
DESCRIPTION: This code accesses the 'Item Size' column of the pandas DataFrame and prints its unique values using the `.unique()` method. This helps understand the range and types of entries in this specific feature before encoding.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/notebook.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
# Let's look at the different values of the 'Item Size' column
pumpkins['Item Size'].unique()
```

----------------------------------------

TITLE: Calculating Common Statistics in R
DESCRIPTION: Computes common descriptive statistics (like mean, median, standard deviation, etc.) for the numerical columns of the dataset using the summarytools package. This provides a quick overview of the central tendency and dispersion.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb#_snippet_5

LANGUAGE: R
CODE:
```
# Describe common statistics
df %>%
  descr(stats = "common")
```

----------------------------------------

TITLE: Plotting Top Japanese Ingredients Bar Chart in R
DESCRIPTION: Processes the `japanese_df` using the `create_ingredient` function to find popular ingredients. It then takes the top 10 ingredients from the result and generates a bar chart using `ggplot2` to visualize their frequencies, similar to the Thai cuisine plot.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb#_snippet_9

LANGUAGE: R
CODE:
```
# Get popular ingredients for Japanese cuisines and make bar chart
create_ingredient(df = japanese_df) %>% 
  slice_head(n = 10) %>% 
  ggplot(aes(x = n_instances, y = ingredients)) +
  geom_bar(stat = "identity", width = 0.5, fill = "darkorange", alpha = 0.8) +
  xlab("") + ylab("")
```

----------------------------------------

TITLE: Counting Rows with Pandas sum and Boolean Indexing (Python)
DESCRIPTION: This snippet counts rows in a Pandas DataFrame based on conditions in 'Negative_Review' and 'Positive_Review' columns using boolean indexing and the `sum()` method. It efficiently counts rows for 'No Negative', 'No Positive', and where both conditions are met. It requires a DataFrame `df` with the specified columns and the `time` module and demonstrates significantly faster performance compared to the lambda approach.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_1

LANGUAGE: python
CODE:
```
# without lambdas (using a mixture of notations to show you can use both)
start = time.time()
no_negative_reviews = sum(df.Negative_Review == "No Negative")
print("Number of No Negative reviews: " + str(no_negative_reviews))

no_positive_reviews = sum(df["Positive_Review"] == "No Positive")
print("Number of No Positive reviews: " + str(no_positive_reviews))

both_no_reviews = sum((df.Negative_Review == "No Negative") & (df.Positive_Review == "No Positive"))
print("Number of both No Negative and No Positive reviews: " + str(both_no_reviews))

end = time.time()
print("Sum took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Evaluating Model Performance with MSE and Percentage Error
DESCRIPTION: Uses the trained linear regression model (`lin_reg`) to make predictions (`pred`) on the test features (`X_test`). It then calculates the Root Mean Squared Error (RMSE) between the true test values (`y_test`) and the predictions, and prints the RMSE and the percentage error relative to the mean prediction. Requires a trained `LinearRegression` model, `mean_squared_error` imported, and test data arrays/Series, plus numpy for `np.sqrt` and `np.mean`. Outputs printed error metrics.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/fr/2-Regression/3-Linear/README.md#_snippet_9

LANGUAGE: python
CODE:
```
pred = lin_reg.predict(X_test)
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

----------------------------------------

TITLE: Defining Strict Q-Learning Policy - Python
DESCRIPTION: Defines a policy function `qpolicy_strict` that takes the environment object `m` as input. It retrieves the current state, applies the `probs` function to the Q-values for that state (though `argmax` relies on the raw values), selects the action corresponding to the highest Q-value using `np.argmax`, and returns the chosen action. This policy is then executed using a `walk` function. Note that this strict policy can sometimes lead to infinite loops if two states point back to each other as the optimal move. Requires the trained `Q` table, `probs` function, `actions`, NumPy, and a `walk` function.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/8-Reinforcement/1-QLearning/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

----------------------------------------

TITLE: Prepare Test Data for Walk-Forward Validation in Python
DESCRIPTION: This snippet prepares the test dataset for walk-forward validation by creating shifted columns representing the actual values at each step within the defined `HORIZON`. This allows for easy comparison of multi-step predictions against reality.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/7-TimeSeries/2-ARIMA/README.md#_snippet_2

LANGUAGE: python
CODE:
```
test_shifted = test.copy()

for t in range(1, HORIZON+1):
 test_shifted['load+'+str(t)] = test_shifted['load'].shift(-t, freq='H')

test_shifted = test_shifted.dropna(how='any')
test_shifted.head(5)
```

----------------------------------------

TITLE: Defining Difference Calculation Function Python
DESCRIPTION: This function calculates the difference between two columns, "Average_Score" and "Calc_Average_Score", for a given row in a pandas DataFrame. It is designed to be applied row-wise to a DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/4-Hotel-Reviews-1/solution/notebook.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
def get_difference_review_avg(row):
    return row["Average_Score"] - row["Calc_Average_Score"]
```

----------------------------------------

TITLE: Initializing Classifiers Dictionary - Scikit-learn SVC - Python
DESCRIPTION: Creates a dictionary to store different classifier models. It is initially populated with a Linear SVC (`SVC(kernel='linear')`) configured with a regularization parameter C=10, probability estimation enabled, and a fixed random state for reproducibility. This structure allows for easy iteration and comparison of multiple models.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/4-Classification/3-Classifiers-2/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
C = 10
# Create different classifiers.
classifiers = {
    'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0)
}
```

----------------------------------------

TITLE: Adding SVC Classifier - Scikit-learn - Python
DESCRIPTION: Appends a standard Support Vector Classifier (`SVC`) with default parameters to the `classifiers` dictionary. The default kernel for `SVC` is the radial basis function (RBF). This step expands the list of models to be trained and evaluated.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/4-Classification/3-Classifiers-2/README.md#_snippet_5

LANGUAGE: Python
CODE:
```
'SVC': SVC(),
```

----------------------------------------

TITLE: Calculating WCSS for Elbow Method (Python)
DESCRIPTION: This snippet prepares data for the elbow method by calculating the Within-Cluster Sum of Squares (WCSS) for different numbers of clusters (from 1 to 10). It iterates through possible cluster counts, initializes and fits a KMeans model with `k-means++` initialization for each count, and collects the `inertia_` (WCSS) value into a list. This list (`wcss`) will be used to plot the elbow curve to determine the optimal number of clusters.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/translations/README.ko.md#_snippet_4

LANGUAGE: Python
CODE:
```
from sklearn.cluster import KMeans
wcss = []

for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)
```

----------------------------------------

TITLE: Defining Function create_ingredient in R
DESCRIPTION: Defines an R function `create_ingredient` using `tidyverse` functions to process a dataframe containing cuisine and ingredient data. It drops an ID column, reshapes the data to a long format, groups and summarizes ingredient counts per cuisine, filters out zero counts, arranges by frequency, and factorizes ingredients. The function returns a processed dataframe suitable for analysis.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb#_snippet_6

LANGUAGE: R
CODE:
```
create_ingredient <- function(df){
  
  # Drop the id column which is the first colum
  ingredient_df = df %>% select(-1) %>% 
  # Transpose data to a long format
    pivot_longer(!cuisine, names_to = "ingredients", values_to = "count") %>% 
  # Find the top most ingredients for a particular cuisine
    group_by(ingredients) %>% 
    summarise(n_instances = sum(count)) %>% 
    filter(n_instances != 0) %>% 
  # Arrange by descending order
    arrange(desc(n_instances)) %>% 
    mutate(ingredients = factor(ingredients) %>% fct_inorder())
  
  
  return(ingredient_df)
} # End of function
```

----------------------------------------

TITLE: Defining Sentiment Calculation Function using NLTK VADER - Python
DESCRIPTION: Initializes the NLTK VADER sentiment intensity analyzer and defines a helper function, `calc_sentiment`. This function takes a review string as input, returns 0 for placeholder texts like 'No Negative' or 'No Positive', and otherwise calculates and returns the compound sentiment score using VADER.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Create the vader sentiment analyser (there are others in NLTK you can try too)
vader_sentiment = SentimentIntensityAnalyzer()
# Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.

# There are 3 possibilities of input for a review:
# It could be "No Negative", in which case, return 0
# It could be "No Positive", in which case, return 0
# It could be a review, in which case calculate the sentiment
def calc_sentiment(review):    
    if review == "No Negative" or review == "No Positive":
        return 0
    return vader_sentiment.polarity_scores(review)["compound"]
```

----------------------------------------

TITLE: Creating the Prediction Form Page (HTML)
DESCRIPTION: Defines the structure of the web page presented to the user. It includes a form for submitting prediction inputs (seconds, latitude, longitude), links the external CSS file, and uses Jinja templating (`{{ }}`) to display the prediction result returned by the Flask application.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/3-Web-App/1-Web-App/README.md#_snippet_4

LANGUAGE: html
CODE:
```
<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8">
    <title>🛸 UFO Appearance Prediction! 👽</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}">
  </head>

  <body>
    <div class="grid">

      <div class="box">

        <p>According to the number of seconds, latitude and longitude, which country is likely to have reported seeing a UFO?</p>

        <form action="{{ url_for('predict')}}" method="post">
          <input type="number" name="seconds" placeholder="Seconds" required="required" min="0" max="60" />
          <input type="text" name="latitude" placeholder="Latitude" required="required" />
          <input type="text" name="longitude" placeholder="Longitude" required="required" />
          <button type="submit" class="btn">Predict country where the UFO is seen</button>
        </form>

        <p>{{ prediction_text }}</p>

      </div>

    </div>

  </body>
</html>
```

----------------------------------------

TITLE: Simulate and Print Discretized Observations - Python
DESCRIPTION: Runs a random simulation and prints the discretized state representation at each step, demonstrating the output of the chosen discretization function (here, `discretize`). The rendering is commented out for faster execution.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/8-Reinforcement/2-Gym/README.md#_snippet_7

LANGUAGE: Python
CODE:
```
env.reset()

done = False
while not done:
   #env.render()
   obs, rew, done, info = env.step(env.action_space.sample())
   #print(discretize_bins(obs))
   print(discretize(obs))
env.close()
```

----------------------------------------

TITLE: Count Review Types with Pandas Apply/Lambdas (Python)
DESCRIPTION: This Python snippet calculates the count of rows in a pandas DataFrame (`df`) where the 'Negative_Review' column is 'No Negative', the 'Positive_Review' column is 'No Positive', or both conditions are met. It uses `df.apply` with lambda functions for row-wise evaluation and includes basic time measurement.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_6

LANGUAGE: python
CODE:
```
# with lambdas:
start = time.time()
no_negative_reviews = df.apply(lambda x: True if x['Negative_Review'] == "No Negative" else False , axis=1)
print("Number of No Negative reviews: " + str(len(no_negative_reviews[no_negative_reviews == True].index)))

no_positive_reviews = df.apply(lambda x: True if x['Positive_Review'] == "No Positive" else False , axis=1)
print("Number of No Positive reviews: " + str(len(no_positive_reviews[no_positive_reviews == True].index)))

both_no_reviews = df.apply(lambda x: True if x['Negative_Review'] == "No Negative" and x['Positive_Review'] == "No Positive" else False , axis=1)
print("Number of both No Negative and No Positive reviews: " + str(len(both_no_reviews[both_no_reviews == True].index)))
end = time.time()
print("Lambdas took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Displaying Transformed Features Head (Pandas Python)
DESCRIPTION: Displays the first 5 rows of the `transformed_feature_df` DataFrame, which contains the features after SMOTE oversampling. This allows inspection of the feature data structure post-transformation.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/notebook.ipynb#_snippet_17

LANGUAGE: Python
CODE:
```
transformed_feature_df.head()
```

----------------------------------------

TITLE: Import Libraries for Model Training Python
DESCRIPTION: This snippet imports necessary modules from the scikit-learn library and numpy. It includes classes for Logistic Regression, model selection utilities (train_test_split, cross_val_score), and evaluation metrics (accuracy_score, precision_score, confusion_matrix, classification_report).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/solution/notebook.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
from sklearn.svm import SVC
import numpy as np
```

----------------------------------------

TITLE: Inspect a Single Test Item Python
DESCRIPTION: This snippet prints the ingredients (features with non-zero values) and the true cuisine label for a specific item from the test set (index 50). This helps in manually inspecting individual data points and their characteristics.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/solution/notebook.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
# test an item
print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')
print(f'cuisine: {y_test.iloc[50]}')
```

----------------------------------------

TITLE: Concatenating Transformed Features and Labels (Pandas Python)
DESCRIPTION: Combines the balanced labels (`transformed_label_df`) and the oversampled features (`transformed_feature_df`) into a single DataFrame named `transformed_df`. This step prepares the full, balanced dataset for potential saving or further use.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/notebook.ipynb#_snippet_18

LANGUAGE: Python
CODE:
```
transformed_df = pd.concat([transformed_label_df,transformed_feature_df],axis=1, join='outer')
transformed_df
```

----------------------------------------

TITLE: Defining Linear SVC Workflow in R
DESCRIPTION: Defines a specification for a linear Support Vector Classifier using `svm_poly` with `degree = 1`, setting the engine to `kernlab` and the mode to `classification`. It then bundles this model specification with the previously defined `cuisines_recipe` into a `tidymodels` workflow. This workflow encapsulates both the preprocessing steps and the model definition, preparing it for training. Requires `tidymodels` and `kernlab` packages.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/solution/R/lesson_12-R.ipynb#_snippet_4

LANGUAGE: R
CODE:
```
# Make a linear SVC specification
svc_linear_spec <- svm_poly(degree = 1) %>%
  set_engine("kernlab") %>%
  set_mode("classification")

# Bundle specification and recipe into a worklow
svc_linear_wf <- workflow() %>%
  add_recipe(cuisines_recipe) %>%
  add_model(svc_linear_spec)

# Print out workflow
svc_linear_wf
```

----------------------------------------

TITLE: Visualizing K-Means Clusters (Python)
DESCRIPTION: This snippet trains a KMeans model with 3 clusters on the selected features (`X`) and then visualizes the results. It creates a scatter plot displaying the 'popularity' and 'danceability' features from the *original* dataset (`df`). Each data point is colored according to the cluster label predicted by the KMeans model. This visualization helps to visually inspect how well the model grouped the data points based on these two features.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/translations/README.ko.md#_snippet_6

LANGUAGE: Python
CODE:
```
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters = 3)
kmeans.fit(X)
labels = kmeans.predict(X)
plt.scatter(df['popularity'],df['danceability'],c = labels)
plt.xlabel('popularity')
plt.ylabel('danceability')
plt.show()
```

----------------------------------------

TITLE: Defining Q-Learning Policy and Printing Statistics (Python)
DESCRIPTION: Defines a `qpolicy` function that determines the agent's action based on the learned Q-values for the current state, using the `probs` helper to derive probabilities. It then calls the `print_statistics` function to evaluate the performance of this Q-learning-derived policy over multiple episodes.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/assignment-solution.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

----------------------------------------

TITLE: Plot Time Series Forecast Accuracy in Python
DESCRIPTION: This snippet generates a plot comparing the actual time series values to the model's predictions. It handles both single-step and multi-step forecasts, visualizing how prediction accuracy might degrade over the forecasting horizon.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/7-TimeSeries/2-ARIMA/README.md#_snippet_8

LANGUAGE: python
CODE:
```
if(HORIZON == 1):
 ## Plotting single step forecast
 eval_df.plot(x='timestamp', y=['actual', 'prediction'], style=['r', 'b'], figsize=(15, 8))

else:
 ## Plotting multi step forecast
 plot_df = eval_df[(eval_df.h=='t+1')][['timestamp', 'actual']]
 for t in range(1, HORIZON+1):
 plot_df['t+'+str(t)] = eval_df[(eval_df.h=='t+'+str(t))]['prediction'].values

 fig = plt.figure(figsize=(15, 8))
 ax = plt.plot(plot_df['timestamp'], plot_df['actual'], color='red', linewidth=4.0)
 ax = fig.add_subplot(111)
 for t in range(1, HORIZON+1):
 x = plot_df['timestamp'][(t-1):]
 y = plot_df['t+'+str(t)][0:len(x)]
 ax.plot(x, y, color='blue', linewidth=4*math.pow(.9,t), alpha=math.pow(0.8,t))

 ax.legend(loc='best')

plt.xlabel('timestamp', fontsize=12)
plt.ylabel('load', fontsize=12)
plt.show()
```

----------------------------------------

TITLE: Azure Static Web App CI/CD Workflow - GitHub Actions/YAML
DESCRIPTION: This YAML configuration defines a GitHub Actions workflow for Continuous Integration and Continuous Deployment to Azure Static Web Apps. It triggers on pushes and pull requests to the `main` branch, checks out the code, and uses the `static-web-apps-deploy` action to build and upload the application.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/quiz-app/README.md#_snippet_4

LANGUAGE: YAML
CODE:
```
name: Azure Static Web Apps CI/CD
on:
  push:
    branches:
      - main
  pull_request:
    types: [opened, synchronize, reopened, closed]
    branches:
      - main

jobs:
  build_and_deploy_job:
    runs-on: ubuntu-latest
    name: Build and Deploy Job
    steps:
      - uses: actions/checkout@v2
      - name: Build And Deploy
        id: builddeploy
        uses: Azure/static-web-apps-deploy@v1
        with:
          azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN }}
          repo_token: ${{ secrets.GITHUB_TOKEN }}
          action: "upload"
          app_location: "/quiz-app" # App source code path
          api_location: ""API source code path optional
          output_location: "dist" #Built app content directory - optional
```

----------------------------------------

TITLE: Plot Mean Price by Variety Bar Chart Python Pandas
DESCRIPTION: This snippet calculates the average 'Price' for each unique 'Variety' in the DataFrame using groupby and then plots these mean prices as a bar chart.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/2-Regression/3-Linear/README.md#_snippet_2

LANGUAGE: python
CODE:
```
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

----------------------------------------

TITLE: Extracting Specific K-Means Model in R
DESCRIPTION: This snippet extracts the K-Means model object corresponding to a specific number of clusters (k=3 in this example) from a tibble containing results for multiple 'k' values. It uses filtering and pulling/plucking functions to retrieve the desired model for further use, such as visualization or detailed analysis.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb#_snippet_12

LANGUAGE: R
CODE:
```
# Extract k = 3 clustering
final_kmeans <- kclusts %>% 
  filter(k == 3) %>% 
  pull(model) %>% 
  pluck(1)
```

----------------------------------------

TITLE: Calculating Day of Year in Pandas (Python)
DESCRIPTION: This Python snippet calculates the day of the year from a 'Date' column within a pandas DataFrame. It subtracts the date of the beginning of the year (January 1st) from each date entry to get the number of days elapsed. Requires pandas and datetime libraries. Takes a pandas Series ('Date' column) as input and returns a Series of integers representing the day of the year.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/README.md#_snippet_0

LANGUAGE: python
CODE:
```
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

----------------------------------------

TITLE: Initializing Sentiment Analysis Setup with NLTK and Pandas
DESCRIPTION: This snippet imports necessary libraries for sentiment analysis (pandas, NLTK) and timing. It downloads the required NLTK 'vader_lexicon' and loads a previously filtered dataset (`Hotel_Reviews_Filtered.csv`) into a pandas DataFrame. It serves as the setup for applying sentiment analysis, with explicit placeholders for where the analysis and final saving steps will occur.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/fr/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_2

LANGUAGE: python
CODE:
```
import time
import pandas as pd
import nltk as nltk
from nltk.corpus import stopwords
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

# Load the filtered hotel reviews from CSV
df = pd.read_csv('../../data/Hotel_Reviews_Filtered.csv')

# You code will be added here


# Finally remember to save the hotel reviews with new NLP data added
print("Saving results to Hotel_Reviews_NLP.csv")
df.to_csv(r'../data/Hotel_Reviews_NLP.csv', index = False)

```

----------------------------------------

TITLE: Implementing Random Walk Policy and Simulation - Python
DESCRIPTION: Defines a `random_policy` function that selects an action randomly from the available `actions`. It also implements a `walk` function that simulates the agent traversing the board `m` following a given `policy`. The simulation starts randomly or at a specified position and continues until the agent finds an apple (returns step count) or encounters a wolf/water (returns -1). It depends on the `actions` dictionary and a `Board` instance.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
def random_policy(m):
    return random.choice(list(actions))

def walk(m,policy,start_position=None):
    n = 0 # number of steps
    # set initial position
    if start_position:
        m.human = start_position 
    else:
        m.random_start()
    while True:
        if m.at() == Board.Cell.apple:
            return n # success!
        if m.at() in [Board.Cell.wolf, Board.Cell.water]:
            return -1 # eaten by wolf or drowned
        while True:
            a = actions[policy(m)]
            new_pos = m.move_pos(m.human,a)
            if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                m.move(a) # do the actual move
                break
        n+=1

walk(m,random_policy)
```

----------------------------------------

TITLE: Initializing Dictionary Q-Table and qvalues Function Python
DESCRIPTION: Initializes an empty Python dictionary `Q` to store Q-values, keyed by `(state, action)` pairs. Defines the `actions` tuple (0 for pushing left, 1 for pushing right). The `qvalues` function retrieves and returns a list of Q-values for a given state across all possible actions, defaulting to 0 if the entry is not found in the dictionary.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/8-Reinforcement/2-Gym/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
Q = {}
actions = (0,1)

def qvalues(state):
    return [Q.get((state,a),0) for a in actions]
```

----------------------------------------

TITLE: Normalizing Hotel Address - Python
DESCRIPTION: This function takes a row from the DataFrame and replaces specific city/country addresses with a standardized, shorter format for major locations. It is used to simplify the 'Hotel_Address' column.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/1-notebook.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
def replace_address(row):
    if "Netherlands" in row["Hotel_Address"]:
        return "Amsterdam, Netherlands"
    elif "Barcelona" in row["Hotel_Address"]:
        return "Barcelona, Spain"
    elif "United Kingdom" in row["Hotel_Address"]:
        return "London, United Kingdom"
    elif "Milan" in row["Hotel_Address"]:
        return "Milan, Italy"
    elif "France" in row["Hotel_Address"]:
        return "Paris, France"
    elif "Vienna" in row["Hotel_Address"]:
        return "Vienna, Austria" 
    else:
        return row.Hotel_Address
```

----------------------------------------

TITLE: Displaying Filtered DataFrame Head (Python)
DESCRIPTION: Displays the first few rows of the DataFrame after it has been filtered by genre and popularity. This allows for quick inspection of the data structure and content resulting from the filtering operation. It requires the filtered `df` DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/notebook.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
df.head()
```

----------------------------------------

TITLE: Plot Scatter for Specific Variety Python Pandas
DESCRIPTION: This code filters the DataFrame to select only rows where the 'Variety' is 'PIE TYPE' and then generates a scatter plot of 'DayOfYear' versus 'Price' specifically for this subset of data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/2-Regression/3-Linear/README.md#_snippet_3

LANGUAGE: python
CODE:
```
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price')
```

----------------------------------------

TITLE: Plotting Price vs Day for a Specific Variety (Pie Type)
DESCRIPTION: Filters the `new_pumpkins` DataFrame to select only rows where 'Variety' is 'PIE TYPE', then generates a scatter plot showing 'Price' against 'DayOfYear' for this filtered data. Requires a pandas DataFrame with 'Variety', 'DayOfYear', and 'Price' columns, and matplotlib (implicitly used by pandas plotting). Outputs a scatter plot for the 'PIE TYPE' variety.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/fr/2-Regression/3-Linear/README.md#_snippet_3

LANGUAGE: python
CODE:
```
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price')
```

----------------------------------------

TITLE: Merging Encoded Features and Label into DataFrame (Python)
DESCRIPTION: This snippet takes the DataFrame of encoded features (encoded_features), adds the encoded Color column as a new column named 'Color', and assigns the resulting DataFrame to encoded_pumpkins. This creates the final pre-processed dataset ready for model training.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/README.md#_snippet_7

LANGUAGE: python
CODE:
```
encoded_pumpkins = encoded_features.assign(Color=encoded_label)
```

----------------------------------------

TITLE: Training and Evaluating Classifiers - Scikit-learn Python
DESCRIPTION: Iterates through the `classifiers` dictionary. For each classifier, it trains the model on the training data using `fit`, makes predictions on the test data using `predict`, calculates `accuracy_score`, and prints the accuracy percentage and a detailed `classification_report` showing precision, recall, f1-score, and support per class. `np.ravel(y_train)` flattens the label array for fitting.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
n_classifiers = len(classifiers)

for index, (name, classifier) in enumerate(classifiers.items()):
    classifier.fit(X_train, np.ravel(y_train))

    y_pred = classifier.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print("Accuracy (train) for %s: %0.1f%% " % (name, accuracy * 100))
    print(classification_report(y_test,y_pred))
```

----------------------------------------

TITLE: Filtering Rows Based on String Pattern with dplyr and stringr in R
DESCRIPTION: Uses 'dplyr::filter()' in conjunction with 'stringr::str_detect()' to select only those rows from the 'pumpkins' dataframe where the 'Package' column contains the substring "bushel". The result is stored in a new dataframe 'new_pumpkins'. The code then uses 'dim()' to print the number of rows and columns in the new dataframe and displays the first 5 rows using 'slice_head()'.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/solution/R/lesson_2-R.ipynb#_snippet_9

LANGUAGE: R
CODE:
```
# Retain only pumpkins with "bushel"
new_pumpkins <- pumpkins %>%
       filter(str_detect(Package, "bushel"))

# Get the dimensions of the new data
dim(new_pumpkins)

# View a few rows of the new data
new_pumpkins %>%
  slice_head(n = 5)
```

----------------------------------------

TITLE: Load Data and Initial Exploration in R
DESCRIPTION: This code loads the 'tidyverse' and 'lubridate' libraries. It then reads the US pumpkins CSV dataset directly from a GitHub URL into an R data frame named 'pumpkins'. Finally, it provides a brief overview of the data structure using 'glimpse()' and displays the first 5 rows using 'slice_head()'.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_1

LANGUAGE: R
CODE:
```
library(tidyverse)
library(lubridate)

# Import the pumpkins data
pumpkins <- read_csv(file = "https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/data/US-pumpkins.csv")


# Get a glimpse and dimensions of the data
glimpse(pumpkins)


# Print the first 50 rows of the data set
pumpkins %>%
  slice_head(n = 5)
```

----------------------------------------

TITLE: Calculate WCSS for Elbow Method in Python
DESCRIPTION: Iterates through a range of possible cluster numbers (1 to 10) and fits a K-Means model for each, calculating the Within-Cluster Sum of Squares (WCSS), also known as inertia. The inertia for each number of clusters is appended to a list `wcss` to be used for the elbow method visualization. Uses `sklearn.cluster.KMeans` with the 'k-means++' initialization. Requires the feature matrix `X`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/README.md#_snippet_4

LANGUAGE: python
CODE:
```
from sklearn.cluster import KMeans
wcss = []

for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)


```

----------------------------------------

TITLE: Training Linear Regression Model on Pie Pumpkins - Python
DESCRIPTION: Prepares the 'DayOfYear' feature and 'Price' target for the 'PIE TYPE' pumpkin subset. Splits the data into 80% training and 20% testing sets. Initializes and trains a `LinearRegression` model on the training data. Predicts prices on the test set and calculates the Mean Squared Error to assess model accuracy.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/notebook.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)

pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

----------------------------------------

TITLE: Implementing Strict Q-Table Policy (Python)
DESCRIPTION: This code defines a deterministic policy (`qpolicy_strict`) that selects the action with the highest Q-value for the current state, based on the trained Q-table. It demonstrates the use of this policy by calling a `walk` function, which simulates the agent navigating the environment using this greedy strategy. This policy might get stuck in loops if two states point to each other as optimal.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/8-Reinforcement/1-QLearning/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

----------------------------------------

TITLE: Loading Filtered Data and Sentiment Setup - NLTK/Pandas Python
DESCRIPTION: This snippet sets up the environment for sentiment analysis. It imports necessary libraries (`time`, `pandas`, `nltk`, `SentimentIntensityAnalyzer`), downloads the NLTK VADER lexicon, loads the previously filtered hotel review data from 'Hotel_Reviews_Filtered.csv' into a pandas DataFrame, and includes print statements for saving the final results later. Requires pandas and NLTK libraries installed and the specified CSV file.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_6

LANGUAGE: Python
CODE:
```
import time
import pandas as pd
import nltk as nltk
from nltk.corpus import stopwords
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

# Load the filtered hotel reviews from CSV
df = pd.read_csv('../../data/Hotel_Reviews_Filtered.csv')

# You code will be added here


# Finally remember to save the hotel reviews with new NLP data added
print("Saving results to Hotel_Reviews_NLP.csv")
df.to_csv(r'../data/Hotel_Reviews_NLP.csv', index = False)
```

----------------------------------------

TITLE: Applying Address Normalization Function - Python
DESCRIPTION: This applies the previously defined `replace_address` function to the 'Hotel_Address' column of the DataFrame to standardize the addresses.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/1-notebook.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
# Replace all the addresses with a shortened, more useful form
df["Hotel_Address"] = df.apply(replace_address, axis = 1)
```

----------------------------------------

TITLE: Importing Scikit-learn Modules (Python)
DESCRIPTION: Imports necessary classes and functions from the scikit-learn library for building and evaluating a machine learning model. This includes tools for data splitting, model selection (SVC), and various evaluation metrics.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/solution/notebook.ipynb#_snippet_5

LANGUAGE: Python
CODE:
```
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report
```

----------------------------------------

TITLE: Plotting Top Thai Ingredients (Pandas Matplotlib Python)
DESCRIPTION: Uses the `create_ingredient_df` function to analyze the Thai cuisine DataFrame (`thai_df`), selects the top 10 ingredients by count, and generates a horizontal bar plot to visualize their frequency.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/notebook.ipynb#_snippet_9

LANGUAGE: Python
CODE:
```
thai_ingredient_df = create_ingredient_df(thai_df)
thai_ingredient_df.head(10).plot.barh()
```

----------------------------------------

TITLE: Importing Libraries for Scikit-learn Project (Python)
DESCRIPTION: This code snippet imports essential Python libraries required for a machine learning project using Scikit-learn. It imports `matplotlib.pyplot` for plotting, `numpy` for numerical operations, and specific modules (`datasets`, `linear_model`, `model_selection`) from `sklearn` for loading data, using models, and splitting data. These imports prepare the environment to load datasets, define regression models, and manage data splits for training and testing.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/README.md#_snippet_1

LANGUAGE: python
CODE:
```
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model, model_selection
```

----------------------------------------

TITLE: Prepare Test Data for Walk-Forward Validation in Python
DESCRIPTION: This snippet prepares the test dataset for walk-forward validation by creating shifted columns. For each step in the defined `HORIZON`, it shifts the 'load' column backwards, creating new columns like 'load+1', 'load+2', etc., which represent the actual values at future time steps. Rows with any missing values introduced by the shifting are then dropped.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/README.md#_snippet_11

LANGUAGE: python
CODE:
```
test_shifted = test.copy()

for t in range(1, HORIZON+1):
    test_shifted['load+'+str(t)] = test_shifted['load'].shift(-t, freq='H')

test_shifted = test_shifted.dropna(how='any')
test_shifted.head(5)
```

----------------------------------------

TITLE: Loading Filtered Data and Initializing NLP Libraries Python
DESCRIPTION: Imports necessary libraries for data manipulation (Pandas) and sentiment analysis (NLTK). It downloads the 'vader_lexicon' required for the VADER sentiment analyzer. It loads the pre-filtered hotel review data from '../../data/Hotel_Reviews_Filtered.csv' into a Pandas DataFrame and includes placeholders for further processing and saving the final data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
import time
import pandas as pd
import nltk as nltk
from nltk.corpus import stopwords
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

# Load the filtered hotel reviews from CSV
df = pd.read_csv('../../data/Hotel_Reviews_Filtered.csv')

# You code will be added here


# Finally remember to save the hotel reviews with new NLP data added
print("Saving results to Hotel_Reviews_NLP.csv")
df.to_csv(r'../data/Hotel_Reviews_NLP.csv', index = False)
```

----------------------------------------

TITLE: Extracting Noun Phrases using TextBlob with ConllExtractor (Python)
DESCRIPTION: This snippet demonstrates how to initialize TextBlob with a specific noun phrase extractor, ConllExtractor, to find noun phrases in user-provided text. It imports necessary classes, creates the extractor, takes input from the user, processes it using TextBlob configured with the custom extractor, and stores the extracted noun phrases in a variable.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/2-Tasks/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
from textblob import TextBlob
from textblob.np_extractors import ConllExtractor
# import and create a Conll extractor to use later 
extractor = ConllExtractor()

# later when you need a noun phrase extractor:
user_input = input("> ")
user_input_blob = TextBlob(user_input, np_extractor=extractor)  # note non-default extractor specified
np = user_input_blob.noun_phrases
```

----------------------------------------

TITLE: Splitting Data into Training and Test Sets (Scikit-learn, Python)
DESCRIPTION: Splits the feature data (X) and target data (y) into training and testing subsets using `model_selection.train_test_split`. The `test_size=0.33` parameter specifies that 33% of the data will be allocated to the test sets (X_test, y_test), with the remaining 67% going to the training sets (X_train, y_train). This separation prevents overfitting and allows for model evaluation on unseen data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/2-Regression/1-Tools/README.md#_snippet_2

LANGUAGE: python
CODE:
```
X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33)
```

----------------------------------------

TITLE: Selecting & Reshaping Feature Column - Python
DESCRIPTION: This code selects the 3rd column (index 2, corresponding to BMI) from the feature matrix `X` using slicing. It then reshapes the selected column into a 2D array required by scikit-learn models, using `-1` to infer the number of rows and `1` for the single column. This step isolates the input feature for the simple linear regression. Requires a NumPy array-like structure for `X`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/README.md#_snippet_3

LANGUAGE: python
CODE:
```
X = X[:, 2]
X = X.reshape((-1,1))
```

----------------------------------------

TITLE: Training and Evaluating Logistic Regression Model (Python)
DESCRIPTION: Imports the necessary evaluation metrics (`accuracy_score`, `classification_report`) and the `LogisticRegression` model from scikit-learn. It initializes a `LogisticRegression` model and trains it using the training data (`X_train`, `y_train`). After training, it makes predictions on the test set (`X_test`) and prints the classification report and overall accuracy score to evaluate the model's performance. Requires scikit-learn.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/3-Web-App/1-Web-App/README.md#_snippet_5

LANGUAGE: Python
CODE:
```
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)

print(classification_report(y_test, predictions))
print('Predicted labels: ', predictions)
print('Accuracy: ', accuracy_score(y_test, predictions))
```

----------------------------------------

TITLE: Loading CSV Data using Pandas in Python
DESCRIPTION: This Python snippet loads the 'Hotel_Reviews.csv' file into a Pandas DataFrame. It uses the 'time' module to measure and report the duration of the loading process. This prepares the data for subsequent analysis. Dependencies include the 'pandas' library. The input is the specified CSV file, and the output is the loaded DataFrame 'df' and a printed loading time message.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_0

LANGUAGE: python
CODE:
```
# importing time so the start and end time can be used to calculate file loading time
print("Loading data file now, this could take a while depending on file size")
start = time.time()
# df is 'DataFrame' - make sure you downloaded the file to the data folder
df = pd.read_csv('../../data/Hotel_Reviews.csv')
end = time.time()
print("Loading took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Counting Rows with Pandas apply and Lambda (Python)
DESCRIPTION: This snippet counts rows in a Pandas DataFrame based on conditions in 'Negative_Review' and 'Positive_Review' columns using the `apply` method with a lambda function. It identifies rows where both columns indicate 'No Negative' and 'No Positive' reviews and measures the execution time. Requires a DataFrame `df` with the specified columns and the `time` module.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_0

LANGUAGE: python
CODE:
```
both_no_reviews = df.apply(lambda x: True if x['Negative_Review'] == "No Negative" and x['Positive_Review'] == "No Positive" else False , axis=1)
print("Number of both No Negative and No Positive reviews: " + str(len(both_no_reviews[both_no_reviews == True].index)))
end = time.time()
print("Lambdas took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Filtering and Plotting Pie Pumpkins Data using Pandas Plotting Python
DESCRIPTION: This code filters the original `new_pumpkins` DataFrame to create a new DataFrame `pie_pumpkins` containing only records where the 'Variety' is 'PIE TYPE'. It then generates a scatter plot showing the relationship between 'DayOfYear' and 'Price' specifically for this subset of data. This allows for focused analysis on the relationship within a single variety.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/3-Linear/README.md#_snippet_3

LANGUAGE: python
CODE:
```
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price')
```

----------------------------------------

TITLE: Creating Feature Set (Pandas)
DESCRIPTION: Removes the 'Unnamed: 0' column and the 'cuisine' column from the cuisines_df DataFrame along the columns axis (axis=1). The resulting DataFrame, cuisines_feature_df, contains only the features (ingredient data) that will be used to train the model. It then displays the first five rows of cuisines_feature_df. Requires cuisines_df to exist and contain the specified columns. The output is a preview of the feature DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/README.md#_snippet_3

LANGUAGE: python
CODE:
```
cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)
cuisines_feature_df.head()
```

----------------------------------------

TITLE: Defining Training and Testing Date Boundaries Python
DESCRIPTION: Defines string variables specifying the exact start date and time for the training dataset (`train_start_dt`) and the testing dataset (`test_start_dt`). These variables are used later to split the full dataset into training and testing subsets.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/solution/notebook.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
train_start_dt = '2014-11-01 00:00:00'
test_start_dt = '2014-12-30 00:00:00'
```

----------------------------------------

TITLE: Importing Scikit-learn Libraries for Regression - Python
DESCRIPTION: Imports essential modules from the scikit-learn library, including `LinearRegression` for building the model, `r2_score`, `mean_squared_error`, and `mean_absolute_error` for evaluating model performance, and `train_test_split` for dividing the data into training and testing sets.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/notebook.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
```

----------------------------------------

TITLE: Importing Libraries for Linear Regression - Python
DESCRIPTION: Imports necessary libraries: matplotlib for plotting, numpy for numerical operations, and specific modules from scikit-learn for datasets, linear models, and model selection.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/solution/notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model, model_selection
```

----------------------------------------

TITLE: Translating Text with TextBlob (Python)
DESCRIPTION: This snippet demonstrates how to use the `TextBlob` library in Python to translate a given English string to French. It initializes a TextBlob object with the text and calls the `translate()` method, which relies on an external service (like Google Translate) to perform the translation. The translated string is then printed to the console.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/3-Translation-Sentiment/translations/README.ko.md#_snippet_0

LANGUAGE: Python
CODE:
```
from textblob import TextBlob

blob = TextBlob(
    "It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife!"
)
print(blob.translate(to="fr"))
```

----------------------------------------

TITLE: Calculating Action Probabilities from Q-Values Python
DESCRIPTION: This function converts a vector of Q-values for a given state into a probability distribution over possible actions. It adds a small epsilon value to each Q-value before normalization to prevent division by zero errors, especially in initial states where all Q-values might be zero or equal.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/8-Reinforcement/1-QLearning/README.md#_snippet_0

LANGUAGE: python
CODE:
```
def probs(v,eps=1e-4):
    v = v-v.min()+eps
    v = v/v.sum()
    return v
```

----------------------------------------

TITLE: Printing Reviews Sorted by Sentiment (Python)
DESCRIPTION: Sorts the DataFrame first by the 'Negative_Sentiment' column in ascending order and prints the 'Negative_Review' and 'Negative_Sentiment' columns. Then, it sorts by the 'Positive_Sentiment' column in ascending order and prints the 'Positive_Review' and 'Positive_Sentiment' columns. This allows for visual inspection of reviews alongside their calculated sentiment scores.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_6

LANGUAGE: python
CODE:
```
df = df.sort_values(by=["Negative_Sentiment"], ascending=True)
print(df[["Negative_Review", "Negative_Sentiment"]])
df = df.sort_values(by=["Positive_Sentiment"], ascending=True)
print(df[["Positive_Review", "Positive_Sentiment"]])
```

----------------------------------------

TITLE: Initialize Q-Table as Dictionary and Helper - Python
DESCRIPTION: Initializes the Q-table as an empty Python dictionary, suitable for sparse state spaces or unknown bounds. Defines a helper function `qvalues` that retrieves the Q-values for all possible actions from the Q-table for a given state, returning 0 if a state-action pair is not found.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/8-Reinforcement/2-Gym/README.md#_snippet_8

LANGUAGE: Python
CODE:
```
Q = {}
actions = (0,1)

def qvalues(state):
    return [Q.get((state,a),0) for a in actions]
```

----------------------------------------

TITLE: Run Short Simulation with Random Actions - Python
DESCRIPTION: Resets the environment and runs a short simulation of 100 steps. In each step, a random action is chosen from the action space, the environment state is updated using `env.step()`, and the simulation is rendered visually.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/8-Reinforcement/2-Gym/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
env.reset()

for i in range(100):
   env.render()
   env.step(env.action_space.sample())
env.close()
```

----------------------------------------

TITLE: Implementing Stochastic Q-Table Policy (Python)
DESCRIPTION: This code defines a stochastic policy (`qpolicy`) where actions are selected based on probabilities proportional to their Q-values, using the `probs` function. This policy incorporates both exploitation (higher Q-values have higher probability) and exploration (all actions have some non-zero probability). The code then evaluates the performance of this policy over multiple simulations using a `print_statistics` function.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/8-Reinforcement/1-QLearning/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

----------------------------------------

TITLE: Running the Flask Web Application
DESCRIPTION: Executes the main Python script (`app.py`) to start the built-in Flask development web server. The `debug=True` argument enables features like automatic code reloading and interactive debugger, which are useful during development. The server will typically start on `http://127.0.0.1:5000`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/3-Web-App/1-Web-App/README.md#_snippet_6

LANGUAGE: Bash
CODE:
```
python app.py
```

LANGUAGE: Bash
CODE:
```
python3 app.py
```

----------------------------------------

TITLE: Importing Scikit-learn Modules for Regression (Python)
DESCRIPTION: This snippet imports necessary classes and functions from the scikit-learn library for performing linear regression. It imports LinearRegression for the model, mean_squared_error for evaluation, and train_test_split for splitting data. Dependencies: scikit-learn library.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/2-Regression/3-Linear/README.md#_snippet_5

LANGUAGE: python
CODE:
```
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

----------------------------------------

TITLE: Loading the Diabetes Dataset - Python
DESCRIPTION: Loads the diabetes dataset from scikit-learn, returning features (X) and target (y). It also prints the shape of the feature array and the first feature vector to inspect the data structure.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/solution/notebook.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
X, y = datasets.load_diabetes(return_X_y=True)
print(X.shape)
print(X[0])
```

----------------------------------------

TITLE: Loading Time Series Data (Python)
DESCRIPTION: Loads the energy load time series data using a custom utility function and selects only the 'load' column. This prepares the primary dataset for subsequent processing.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
energy = load_data('../../data')[['load']]
energy.head(5)
```

----------------------------------------

TITLE: Import ONNX Conversion Modules Python
DESCRIPTION: Imports necessary components from the `skl2onnx` library needed for converting scikit-learn models to the ONNX format. It also defines the input tensor type for the ONNX model (`FloatTensorType`) and sets conversion options, including specifying the shape of the input tensor (380 features).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/README.md#_snippet_9

LANGUAGE: python
CODE:
```
from skl2onnx import convert_sklearn
from skl2onnx.common.data_types import FloatTensorType

initial_type = [('float_input', FloatTensorType([None, 380]))]
options = {id(model): {'nocl': True, 'zipmap': False}}
```

----------------------------------------

TITLE: Saving CartPole Simulation as GIF using PIL in Python
DESCRIPTION: This code runs a simulation using a trained Q-table, renders the environment for each step, and saves the sequence of images as an animated GIF. It demonstrates the agent's learned behavior visually and requires the PIL library for image manipulation.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/notebook.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
from PIL import Image
obs = env.reset()
done = False
i=0
ims = []
while not done:
   s = discretize(obs)
   img=env.render(mode='rgb_array')
   ims.append(Image.fromarray(img))
   v = probs(np.array([Qbest.get((s,a),0) for a in actions]))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
   i+=1
env.close()
ims[0].save('images/cartpole-balance.gif',save_all=True,append_images=ims[1::2],loop=0,duration=5)
print(i)
```

----------------------------------------

TITLE: Simulating and Printing Discretized States - Python
DESCRIPTION: Runs a simulation episode of the CartPole environment. In each step, it takes a random action and then discretizes the resulting observation using the `discretize` function (or `discretize_bins` if uncommented), printing the resulting tuple representing the discrete state. Rendering is commented out by default for faster execution.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/translations/README.ko.md#_snippet_7

LANGUAGE: python
CODE:
```
env.reset()\n\ndone = False\nwhile not done:\n   #env.render()\n   obs, rew, done, info = env.step(env.action_space.sample())\n   #print(discretize_bins(obs))\n   print(discretize(obs))\nenv.close()
```

----------------------------------------

TITLE: Calculating R Clustering Accuracy by Genre
DESCRIPTION: This code attempts to quantify the 'accuracy' of the K-Means clustering by comparing the assigned cluster labels to predefined genre labels. It assigns integer IDs to genres and sums the cases where the cluster ID matches the genre ID, providing a simple count and percentage that align with this specific genre-based comparison.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb#_snippet_14

LANGUAGE: R
CODE:
```
# Assign genres to predefined integers
label_count <- results %>% 
  group_by(artist_top_genre) %>% 
  mutate(id = cur_group_id()) %>% 
  ungroup() %>% 
  summarise(correct_labels = sum(.cluster == id))


# Print results  
cat("Result:", label_count$correct_labels, "out of", nrow(results), "samples were correctly labeled.")

cat("\nAccuracy score:", label_count$correct_labels/nrow(results))
```

----------------------------------------

TITLE: Finding Most Reviewed Hotel per Nationality (Pandas/Python)
DESCRIPTION: Iterates through the index of the top 10 nationalities from the `nationality_freq` Series. Inside the loop, it filters the original DataFrame `df` for the current nationality, calculates the frequency count of 'Hotel_Name' for that subset, and prints the most frequent hotel. Requires the `nationality_freq` Series and the original DataFrame `df`. This approach can be slow for large datasets.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_3

LANGUAGE: python
CODE:
```
# What was the most frequently reviewed hotel for the top 10 nationalities
# Normally with pandas you will avoid an explicit loop, but wanted to show creating a new dataframe using criteria (don't do this with large amounts of data because it could be very slow)
for nat in nationality_freq[:10].index:
   # First, extract all the rows that match the criteria into a new dataframe
   nat_df = df[df["Reviewer_Nationality"] == nat]   
   # Now get the hotel freq
   freq = nat_df["Hotel_Name"].value_counts()
   print("The most reviewed hotel for " + str(nat).strip() + " was " + str(freq.index[0]) + " with " + str(freq[0]) + " reviews.")
```

----------------------------------------

TITLE: Finding Most Reviewed Hotel per Top Nationality Python
DESCRIPTION: This code iterates through the top 10 reviewer nationalities obtained from the `nationality_freq` Series. For each nationality, it filters the original DataFrame (`df`) to select only the reviews by that nationality, then calculates the frequency of hotel names within that subset using `value_counts()`, and finally prints the most reviewed hotel for that specific nationality along with the count. This demonstrates filtering DataFrames based on criteria and performing aggregations within a loop. Requires a Pandas DataFrame `df` with `Reviewer_Nationality` and `Hotel_Name` columns, and the `nationality_freq` Series. Output is a list of the most reviewed hotels, one for each of the top 10 nationalities.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/sw/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_3

LANGUAGE: python
CODE:
```
# What was the most frequently reviewed hotel for the top 10 nationalities
# Normally with pandas you will avoid an explicit loop, but wanted to show creating a new dataframe using criteria (don't do this with large amounts of data because it could be very slow)
for nat in nationality_freq[:10].index:
   # First, extract all the rows that match the criteria into a new dataframe
   nat_df = df[df["Reviewer_Nationality"] == nat]   
   # Now get the hotel freq
   freq = nat_df["Hotel_Name"].value_counts()
   print("The most reviewed hotel for " + str(nat).strip() + " was " + str(freq.index[0]) + " with " + str(freq[0]) + " reviews.")
```

----------------------------------------

TITLE: Importing TextBlob Library Python
DESCRIPTION: Imports the TextBlob class from the textblob library, which is required for performing sentiment analysis and text processing operations. This is the initial setup step for using the library's functionalities.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/3-Translation-Sentiment/solution/notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
from textblob import TextBlob

```

----------------------------------------

TITLE: Importing ONNX Runtime for Web
DESCRIPTION: This snippet adds a `<script>` tag to the HTML document that loads the ONNX Runtime for Web library from a CDN. This library is essential for enabling the web application to load and execute the `model.onnx` file directly within the user's browser environment.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/4-Classification/4-Applied/README.md#_snippet_2

LANGUAGE: html
CODE:
```
<script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.9.0/dist/ort.min.js"></script>
```

----------------------------------------

TITLE: Implementing Strict Q-Policy Navigation Python
DESCRIPTION: This code defines a navigation policy (`qpolicy_strict`) that, for a given state, always selects the action corresponding to the highest Q-value in the Q-Table. It then demonstrates using this strict policy with a `walk` function to navigate the environment. Note that this strict policy can potentially lead to the agent getting stuck in loops between states.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/8-Reinforcement/1-QLearning/README.md#_snippet_2

LANGUAGE: python
CODE:
```
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

----------------------------------------

TITLE: Inspect Single Test Row Data - Python
DESCRIPTION: Prints the actual ingredients and cuisine label for a specific data point (row 50) from the test set (X_test, y_test). It uses pandas iloc to access the row and .keys() to get the ingredient names where the value is non-zero. Requires pandas.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/4-Classification/2-Classifiers-1/README.md#_snippet_1

LANGUAGE: python
CODE:
```
print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')
print(f'cuisine: {y_test.iloc[50]}')
```

----------------------------------------

TITLE: Importing Data Analysis Libraries (Python)
DESCRIPTION: Imports essential Python libraries for data manipulation (pandas), plotting (matplotlib), numerical operations (numpy), and handling imbalanced data using SMOTE (imblearn). These imports are necessary for the subsequent data processing and analysis steps.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/notebook.ipynb#_snippet_1

LANGUAGE: Python
CODE:
```
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
from imblearn.over_sampling import SMOTE
```

----------------------------------------

TITLE: Implementing Random Policy and Simulation Walk (Python)
DESCRIPTION: Defines a `random_policy` function that simply selects a random action from the available actions. The `walk` function simulates a single episode using a given `policy`, tracking steps and returning the outcome: positive steps for a win, negative steps for a loss to the wolf, or 0 for drowning.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/assignment-solution.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
def random_policy(state):
    return random.choice(list(actions))

def walk(board,policy):
    n = 0 # number of steps
    s = state(board)
    while True:
        if s.at() == Board.Cell.wolf:
            if s.is_winning():
                return n # success!
            else:
                return -n # failure!
        if s.at() == Board.Cell.water:
            return 0 # died
        a = actions[policy(m)]
        s.move(a)
        n+=1

walk(m,random_policy)
```

----------------------------------------

TITLE: Implementing Random Walk Policy and Simulation Python
DESCRIPTION: Defines a `random_policy` function that simply chooses a random action from the available options. The `walk` function simulates an agent's movement on the board using a given `policy`, tracking steps until a goal (apple) or failure state (wolf, water) is reached.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/notebook.ipynb#_snippet_4

LANGUAGE: Python
CODE:
```
def random_policy(m):
    return random.choice(list(actions))

def walk(m,policy,start_position=None):
    n = 0 # number of steps
    # set initial position
    if start_position:
        m.human = start_position
    else:
        m.random_start()
    while True:
        if m.at() == Board.Cell.apple:
            return n # success!
        if m.at() in [Board.Cell.wolf, Board.Cell.water]:
            return -1 # eaten by wolf or drowned
        while True:
            a = actions[policy(m)]
            new_pos = m.move_pos(m.human,a)
            if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                m.move(a) # do the actual move
                break
        n+=1

walk(m,random_policy)
```

----------------------------------------

TITLE: Reordering Columns and Saving Processed Data - Python
DESCRIPTION: This snippet reorders the columns in the DataFrame `df` to place the sentiment columns and other key information at the beginning, improving readability and ease of exploration. After reordering, it saves the entire processed DataFrame to a CSV file named 'Hotel_Reviews_NLP.csv' in the '../data/' directory, excluding the DataFrame index from the output.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_7

LANGUAGE: python
CODE:
```
# Reorder the columns (This is cosmetic, but to make it easier to explore the data later)
df = df.reindex(["Hotel_Name", "Hotel_Address", "Total_Number_of_Reviews", "Average_Score", "Reviewer_Score", "Negative_Sentiment", "Positive_Sentiment", "Reviewer_Nationality", "Leisure_trip", "Couple", "Solo_traveler", "Business_trip", "Group", "Family_with_young_children", "Family_with_older_children", "With_a_pet", "Negative_Review", "Positive_Review"], axis=1)

print("Saving results to Hotel_Reviews_NLP.csv")
df.to_csv(r"../data/Hotel_Reviews_NLP.csv", index = False)
```

----------------------------------------

TITLE: Filtering and Visualizing Top 3 Genres in R
DESCRIPTION: Filters the main dataset to include only the top 3 genres ('afro dancehall', 'afropop', 'nigerian pop') and removes observations with a popularity of 0. It then visualizes the counts of these filtered genres.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb#_snippet_9

LANGUAGE: R
CODE:
```
nigerian_songs <- df %>%
  # Concentrate on top 3 genres
  filter(artist_top_genre %in% c("afro dancehall", "afropop","nigerian pop")) %>%
  # Remove unclassified observations
  filter(popularity != 0)



# Visualize popular genres
nigerian_songs %>%
  count(artist_top_genre) %>%
  ggplot(mapping = aes(x = artist_top_genre, y = n,
                       fill = artist_top_genre)) +
  geom_col(alpha = 0.8) +
  paletteer::scale_fill_paletteer_d("ggsci::category10_d3") +
  ggtitle("Top genres") +
  theme(plot.title = element_text(hjust = 0.5))
```

----------------------------------------

TITLE: Finding Most Reviewed Hotel by Nationality (Pandas) - Python
DESCRIPTION: This snippet iterates through the top 10 nationalities. For each nationality, it filters the main DataFrame to create a subset, then calculates the frequency of hotel names within that subset using '.value_counts()' to find the most reviewed hotel for that specific nationality.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_4

LANGUAGE: python
CODE:
```
# What was the most frequently reviewed hotel for the top 10 nationalities
# Normally with pandas you will avoid an explicit loop, but wanted to show creating a new dataframe using criteria (don't do this with large amounts of data because it could be very slow)
for nat in nationality_freq[:10].index:
   # First, extract all the rows that match the criteria into a new dataframe
   nat_df = df[df["Reviewer_Nationality"] == nat]   
   # Now get the hotel freq
   freq = nat_df["Hotel_Name"].value_counts()
   print("The most reviewed hotel for " + str(nat).strip() + " was " + str(freq.index[0]) + " with " + str(freq[0]) + " reviews.")
```

----------------------------------------

TITLE: Splitting Data into Training and Test Sets with Scikit-learn Python
DESCRIPTION: This code uses the `train_test_split` function from scikit-learn to divide the feature array `X` and label array `y` into separate sets for training and testing a machine learning model. It allocates 20% of the data to the test set and uses a fixed `random_state` to ensure the split is the same each time the code runs, which is important for reproducibility.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/3-Linear/README.md#_snippet_7

LANGUAGE: python
CODE:
```
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

----------------------------------------

TITLE: Splitting Data for Training and Testing with Scikit-learn - Python
DESCRIPTION: This snippet splits the dataset into training and testing sets for features (X) and the target variable (y) using Scikit-learn's `train_test_split` function. It excludes the 'Color' column from features and sets the test size to 20% and a fixed random state for reproducibility. Requires the `sklearn.model_selection` module.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/4-Logistic/README.md#_snippet_10

LANGUAGE: python
CODE:
```
from sklearn.model_selection import train_test_split
    
X = encoded_pumpkins[encoded_pumpkins.columns.difference(['Color'])]
y = encoded_pumpkins['Color']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

----------------------------------------

TITLE: Define Bin-Based Observation Discretization Function - Python
DESCRIPTION: Defines helper functions `create_bins` and `discretize_bins` to perform discretization using value binning. `create_bins` generates bin edges for a given interval, and `discretize_bins` maps each component of an observation to its corresponding bin index.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/8-Reinforcement/2-Gym/README.md#_snippet_6

LANGUAGE: Python
CODE:
```
def create_bins(i,num):
    return np.arange(num+1)*(i[1]-i[0])/num+i[0]

print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))

ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
nbins = [20,20,10,10] # number of bins for each parameter
bins = [create_bins(ints[i],nbins[i]) for i in range(4)]

def discretize_bins(x):
    return tuple(np.digitize(x[i],bins[i]) for i in range(4))
```

----------------------------------------

TITLE: Select Target Variable Pandas Python
DESCRIPTION: Selects the 'cuisine' column from the dataset to be used as the target variable or label for the classification task. The selected column is assigned to the variable `y`, representing the output that the model will predict.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/README.md#_snippet_3

LANGUAGE: python
CODE:
```
y = data[['cuisine']]
y.head()
```

----------------------------------------

TITLE: Extracting Target Labels (Pandas)
DESCRIPTION: Selects the 'cuisine' column from the cuisines_df DataFrame and assigns it to a new DataFrame called cuisines_label_df. This DataFrame will serve as the target variable (y) for the machine learning model. It then displays the first five rows of cuisines_label_df to show the extracted labels. Requires cuisines_df to exist and contain a 'cuisine' column. The output is a preview of the label Series/DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/README.md#_snippet_2

LANGUAGE: python
CODE:
```
cuisines_label_df = cuisines_df['cuisine']
cuisines_label_df.head()
```

----------------------------------------

TITLE: Visualizing Polynomial Regression (Matplotlib) - Python
DESCRIPTION: This snippet prepares data for plotting the polynomial regression curve. It creates a DataFrame from test features and predictions, sorts it by the feature values, converts it to a NumPy array, and then plots the sorted predicted curve along with the original scatter points using Matplotlib.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/translations/README.ko.md#_snippet_13

LANGUAGE: Python
CODE:
```
df = pd.DataFrame({'x': X_test[:,0], 'y': y_pred[:,0]})
df.sort_values(by='x',inplace = True)
points = pd.DataFrame(df).to_numpy()

plt.plot(points[:, 0], points[:, 1],color="blue", linewidth=3)
plt.xlabel('Package')
plt.ylabel('Price')
plt.scatter(X,y, color="black")
plt.show()
```

----------------------------------------

TITLE: Selecting Target Column (Python)
DESCRIPTION: Selects the 'cuisine' column from the `data` DataFrame and assigns it to the variable `y`. This column represents the target variable or labels that the machine learning model will predict. It then displays the first few values of the target variable.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/solution/notebook.ipynb#_snippet_4

LANGUAGE: Python
CODE:
```
y = data[['cuisine']]
y.head()
```

----------------------------------------

TITLE: Analyzing Sentiment with TextBlob in Python
DESCRIPTION: This Python snippet demonstrates basic sentiment analysis using the TextBlob library. It initializes TextBlob objects with two sample quotes, calculates their sentiment (polarity and subjectivity), and prints the original quotes along with their calculated sentiment scores. It requires the TextBlob library to be installed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/3-Translation-Sentiment/README.md#_snippet_1

LANGUAGE: python
CODE:
```
from textblob import TextBlob

quote1 = """It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife."""

quote2 = """Darcy, as well as Elizabeth, really loved them; and they were both ever sensible of the warmest gratitude towards the persons who, by bringing her into Derbyshire, had been the means of uniting them."""

sentiment1 = TextBlob(quote1).sentiment
sentiment2 = TextBlob(quote2).sentiment

print(quote1 + " has a sentiment of " + str(sentiment1))
print(quote2 + " has a sentiment of " + str(sentiment2))
```

----------------------------------------

TITLE: Plotting Actual vs Predicted Time Series Data (Python)
DESCRIPTION: This snippet generates a plot comparing the actual time series data (`Y`) with the model's predictions (`Y_pred`). It uses Matplotlib to create a figure, plot both series with distinct colors and line styles, add a legend and x-axis label, and then displays the resulting graph. The plot helps visualize the model's performance visually.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/7-TimeSeries/3-SVR/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
plt.figure(figsize=(30,8))
plt.plot(Y, color = 'red', linewidth=2.0, alpha = 0.6)
plt.plot(Y_pred, color = 'blue', linewidth=0.8)
plt.legend(['Actual','Predicted'])
plt.xlabel('Timestamp')
plt.show()
```

----------------------------------------

TITLE: Running Trained Agent Simulation - Python
DESCRIPTION: Simulates the agent's behavior in the environment using the trained Q-Table. In each step, it discretizes the observation, selects an action by sampling from the probability distribution derived from Q-values using the `probs` and `qvalues` functions, renders the environment visually, and steps the environment forward. The simulation runs until the episode is finished.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/README.md#_snippet_12

LANGUAGE: Python
CODE:
```
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

----------------------------------------

TITLE: Styling the Web Application (CSS)
DESCRIPTION: Provides basic CSS rules for styling the HTML page. It sets body background and text color, styles input fields, and defines a grid layout for the main content block, improving the visual presentation of the prediction form.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/3-Web-App/1-Web-App/README.md#_snippet_3

LANGUAGE: css
CODE:
```
body {
\twidth: 100%;
\theight: 100%;
\tfont-family: 'Helvetica';
\tbackground: black;
\tcolor: #fff;
\ttext-align: center;
\tletter-spacing: 1.4px;
\tfont-size: 30px;
}

input {
\tmin-width: 150px;
}

.grid {
\twidth: 300px;
\tborder: 1px solid #2d2d2d;
\tdisplay: grid;
\tjustify-content: center;
\tmargin: 20px auto;
}

.box {
\tcolor: #fff;
\tbackground: #2d2d2d;
\tpadding: 12px;
\tdisplay: inline-block;
}
```

----------------------------------------

TITLE: Selecting Feature Columns (Python)
DESCRIPTION: Selects a subset of columns from the `data` DataFrame, specifically all columns starting from the third column (`[:, 2:]`), to be used as the feature matrix `X` for the machine learning model. It then displays the first few rows of the selected features.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/solution/notebook.ipynb#_snippet_3

LANGUAGE: Python
CODE:
```
X = data.iloc[:,2:]
X.head()
```

----------------------------------------

TITLE: Calculate Overall Multi-Step Forecast MAPE in Python
DESCRIPTION: This code calculates and prints the overall Mean Absolute Percentage Error (MAPE) across all predictions in the `eval_df`, regardless of the forecast step. It calls the `mape` function on the entire 'prediction' and 'actual' columns of the evaluation DataFrame, multiplies by 100, and prints the final percentage representing the average error over the entire forecasting period and horizon.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/README.md#_snippet_16

LANGUAGE: python
CODE:
```
print('Multi-step forecast MAPE: ', mape(eval_df['prediction'], eval_df['actual'])*100, '%')
```

----------------------------------------

TITLE: Performing K-Means Clustering R
DESCRIPTION: Executes the K-Means clustering algorithm on the selected numeric features (`df_numeric_select`) using the built-in `kmeans()` function. It specifies 3 clusters (`centers = 3`), sets a seed for reproducibility, and runs the algorithm 25 times with different random initializations (`nstart = 25`) to find a robust solution. The resulting clustering object is displayed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb#_snippet_6

LANGUAGE: R
CODE:
```
set.seed(2056)
# Kmeans clustering for 3 clusters
kclust <- kmeans(
  df_numeric_select,
  # Specify the number of clusters
  centers = 3,
  # How many random initial configurations
  nstart = 25
)

# Display clustering object
kclust
```

----------------------------------------

TITLE: Save Processed DataFrame to CSV using Pandas
DESCRIPTION: This snippet saves the current state of the pandas DataFrame after processing to a new CSV file. It first removes several original columns using `df.drop()`. Then, it prints a message indicating the save location and uses `df.to_csv()` to write the DataFrame to disk, excluding the DataFrame index.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_1

LANGUAGE: python
CODE:
```
df.drop(["Review_Total_Negative_Word_Counts", "Review_Total_Positive_Word_Counts", "days_since_review", "Total_Number_of_Reviews_Reviewer_Has_Given"], axis = 1, inplace=True)

# Saving new data file with calculated columns
print("Saving results to Hotel_Reviews_Filtered.csv")
df.to_csv(r'../data/Hotel_Reviews_Filtered.csv', index = False)

```

----------------------------------------

TITLE: Convert and Save Model to ONNX Python
DESCRIPTION: Converts the trained scikit-learn SVC model into the ONNX format using `convert_sklearn`, applying the previously defined input types and options. The resulting ONNX model object is then serialized and saved as a binary file named 'model.onnx', making it ready for use in other environments like a web application.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/README.md#_snippet_10

LANGUAGE: python
CODE:
```
onx = convert_sklearn(model, initial_types=initial_type, options=options)
with open("./model.onnx", "wb") as f:
    f.write(onx.SerializeToString())
```

----------------------------------------

TITLE: Relocating Column and Viewing Data Head (R)
DESCRIPTION: Relocates the 'Month' column to the position before the 'Package' column in the `new_pumpkins` dataframe using `relocate` from `dplyr`. It then displays the first 7 rows to show the result.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/solution/R/lesson_2-R.ipynb#_snippet_12

LANGUAGE: R
CODE:
```
# Create a new data frame new_pumpkins
new_pumpkins <- new_pumpkins %>%
  relocate(Month, .before = Package)

new_pumpkins %>%
  slice_head(n = 7)
```

----------------------------------------

TITLE: Dropping Additional Review Columns - Python
DESCRIPTION: This removes several other columns related to the review date, word counts in reviews, days since review, and the total number of reviews by the reviewer, as they are no longer needed for the transformed dataset.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/1-notebook.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
# No longer need any of these columns
df.drop(["Review_Date", "Review_Total_Negative_Word_Counts", "Review_Total_Positive_Word_Counts", "days_since_review", "Total_Number_of_Reviews_Reviewer_Has_Given"], axis = 1, inplace=True)
```

----------------------------------------

TITLE: Selecting Specific Columns with Pandas loc
DESCRIPTION: Defines a list of column names to keep and then uses the `.loc` accessor to select all rows (`:`) and only the specified columns from the original DataFrame, effectively overwriting the DataFrame with the subset. This is done to simplify the DataFrame by keeping only relevant columns for the task. Requires a Pandas DataFrame with the specified columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/2-Data/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
columns_to_select = ['Package', 'Low Price', 'High Price', 'Date']
pumpkins = pumpkins.loc[:, columns_to_select]
```

----------------------------------------

TITLE: Dropping Columns and Saving Filtered Data (Python)
DESCRIPTION: Removes several original columns ('Review_Total_Negative_Word_Counts', 'Review_Total_Positive_Word_Counts', 'days_since_review', 'Total_Number_of_Reviews_Reviewer_Has_Given') from the pandas DataFrame. It then saves the modified DataFrame to a new CSV file named 'Hotel_Reviews_Filtered.csv', excluding the index column.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_1

LANGUAGE: python
CODE:
```
df.drop(["Review_Total_Negative_Word_Counts", "Review_Total_Positive_Word_Counts", "days_since_review", "Total_Number_of_Reviews_Reviewer_Has_Given"], axis = 1, inplace=True)

# Saving new data file with calculated columns
print("Saving results to Hotel_Reviews_Filtered.csv")
df.to_csv(r'../data/Hotel_Reviews_Filtered.csv', index = False)
```

----------------------------------------

TITLE: Install skl2onnx and Import Pandas Python
DESCRIPTION: This snippet installs the `skl2onnx` library required for converting scikit-learn models to the ONNX format and imports the pandas library, which is essential for data manipulation and reading the dataset.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/README.md#_snippet_0

LANGUAGE: python
CODE:
```
!pip install skl2onnx
import pandas as pd
```

----------------------------------------

TITLE: Visualizing Item Size vs Color with Swarm Plot - Python
DESCRIPTION: This code generates a seaborn swarm plot (`sns.swarmplot`) to visualize the distribution of encoded 'Item Size' values for each encoded 'Color' category. It applies the warning filter defined previously and uses the `encoded_pumpkins` DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/notebook.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
# Suppressing warning message claiming that a portion of points cannot be placed into the plot due to the high number of data points
import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='seaborn')

palette = {
    0: 'orange',
    1: 'wheat'
}
sns.swarmplot(x="Color", y="ord__Item Size", hue="Color", data=encoded_pumpkins, palette=palette)
```

----------------------------------------

TITLE: Initializing TextBlob Object and Sentiment Lists Python
DESCRIPTION: Creates a TextBlob object named book_pride using the text content read from the file. It also initializes two empty lists, positive_sentiment_sentences and negative_sentiment_sentences, which will store sentences categorized by their sentiment.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/3-Translation-Sentiment/solution/notebook.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
book_pride = TextBlob(file_contents)
positive_sentiment_sentences = []
negative_sentiment_sentences = []

```

----------------------------------------

TITLE: Preparing Shifted Test Data for Multi-Step Evaluation Python
DESCRIPTION: Creates a modified version of the test DataFrame (`test_shifted`) by adding columns representing future actual values (`load+1`, `load+2`, etc.) shifted by `HORIZON` steps. This structure aligns rows to facilitate easy comparison between a multi-step forecast and the corresponding actual values for those steps.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/solution/notebook.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
test_shifted = test.copy()

for t in range(1, HORIZON):
    test_shifted['load+'+str(t)] = test_shifted['load'].shift(-t, freq='H')
    
test_shifted = test_shifted.dropna(how='any')
test_shifted.head(5)
```

----------------------------------------

TITLE: Saving Processed Data and Timing - Python
DESCRIPTION: This final snippet saves the processed DataFrame to a new CSV file named 'Hotel_Reviews_Filtered.csv' without including the DataFrame index. It also calculates the total time taken for the script execution and prints it.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/1-notebook.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
# Saving new data file with calculated columns
print("Saving results to Hotel_Reviews_Filtered.csv")
df.to_csv(r'../../data/Hotel_Reviews_Filtered.csv', index = False)
end = time.time()
print("Filtering took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Install Gym and Import Libraries - Python
DESCRIPTION: Installs the OpenAI Gym library using pip via the system's Python executable and imports necessary libraries for environment interaction, plotting, numerical operations, and random number generation.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/8-Reinforcement/2-Gym/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

----------------------------------------

TITLE: Displaying information about a Pandas DataFrame
DESCRIPTION: Prints a concise summary of the DataFrame. This includes the index dtype, column dtypes, non-null values, and memory usage, which is useful for understanding data types, missing values, and overall structure.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/README.md#_snippet_4

LANGUAGE: python
CODE:
```
df.info()
```

----------------------------------------

TITLE: Converting Scaled Data to NumPy Arrays (Python)
DESCRIPTION: Converts the scaled training and testing pandas DataFrames into NumPy arrays. This is a common step before reshaping data for models like SVR that expect array inputs.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
# Converting to numpy arrays

train_data = train.values
test_data = test.values
```

----------------------------------------

TITLE: Visualizing Binary Relationships with Seaborn Swarmplot - Python
DESCRIPTION: This snippet creates a Seaborn swarm plot to visualize the relationship between the binary 'Color' category and the 'ord__Item Size' feature. It uses a specific color palette for the binary categories. Note that this may generate a warning for large datasets due to the nature of swarm plots.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/4-Logistic/README.md#_snippet_9

LANGUAGE: python
CODE:
```
    palette = {
    0: 'orange',
    1: 'wheat'
    }
    sns.swarmplot(x="Color", y="ord__Item Size", data=encoded_pumpkins, palette=palette)
```

----------------------------------------

TITLE: Performing One-Hot Encoding with Pandas (Variety)
DESCRIPTION: Demonstrates using pandas' `get_dummies` function to perform one-hot encoding on the 'Variety' column of the `new_pumpkins` DataFrame. This converts distinct text categories into a set of binary (0 or 1) columns, one for each unique variety, which is a common method to prepare categorical data for numerical machine learning algorithms.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/sw/2-Regression/3-Linear/README.md#_snippet_0

LANGUAGE: python
CODE:
```
pd.get_dummies(new_pumpkins['Variety'])
```

----------------------------------------

TITLE: Adding Cluster Assignments to Data R
DESCRIPTION: Uses the `augment()` function (likely from the `broom` package, compatible with `tidymodels`) to merge the predicted cluster assignments (`.cluster`) from the `kclust` object back into the original `df_numeric_select` dataframe. It then rearranges columns to place `.cluster` at the beginning and displays the first 10 rows of the augmented data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb#_snippet_7

LANGUAGE: R
CODE:
```
# Add predicted cluster assignment to data set
augment(kclust, df_numeric_select) %>%
  relocate(.cluster) %>%
  slice_head(n = 10)
```

----------------------------------------

TITLE: Importing ML Libraries - Scikit-learn - Python
DESCRIPTION: Imports necessary modules from scikit-learn and numpy for machine learning tasks. This includes classifiers (KNeighborsClassifier, LogisticRegression, SVC, RandomForestClassifier, AdaBoostClassifier), model selection utilities (train_test_split, cross_val_score), and evaluation metrics (accuracy_score, precision_score, confusion_matrix, classification_report, precision_recall_curve).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/4-Classification/3-Classifiers-2/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
import numpy as np
```

----------------------------------------

TITLE: Selecting BMI and Target Columns in R
DESCRIPTION: Uses the select() function from dplyr with the pipe operator (%>%) to create a new data frame containing only the 'bmi' predictor and 'y' outcome columns from the original 'diabetes' dataset. It then displays the first 10 rows of this new data frame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/1-Tools/solution/R/lesson_1-R.ipynb#_snippet_3

LANGUAGE: R
CODE:
```
# Select predictor feature `bmi` and outcome `y`
diabetes_select <- diabetes %>%
  select(c(bmi, y))

# Print the first 5 rows
diabetes_select %>%
  slice(1:10)
```

----------------------------------------

TITLE: Selecting Features and Checking Country Distribution (Python)
DESCRIPTION: Creates a new DataFrame containing a subset of the original columns ('duration (seconds)', 'country', 'latitude', 'longitude') and renames them for clarity. It then prints the unique values present in the 'Country' column to see the different countries represented in the dataset.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/3-Web-App/1-Web-App/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
ufos = pd.DataFrame({'Seconds': ufos['duration (seconds)'], 'Country': ufos['country'],'Latitude': ufos['latitude'],'Longitude': ufos['longitude']})

ufos.Country.unique()
```

----------------------------------------

TITLE: Dropping Columns and Defining Features/Labels (Pandas Python)
DESCRIPTION: Creates a feature DataFrame (`feature_df`) by dropping the 'cuisine', 'Unnamed: 0', and common ingredients ('rice', 'garlic', 'ginger') columns from the original `df`. It also isolates the 'cuisine' column into a labels Series (`labels_df`), preparing the data for machine learning.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/notebook.ipynb#_snippet_14

LANGUAGE: Python
CODE:
```
feature_df= df.drop(['cuisine','Unnamed: 0','rice','garlic','ginger'], axis=1)
labels_df = df.cuisine #.unique()
feature_df.head()
```

----------------------------------------

TITLE: Running Multiple Random Walks and Printing Statistics (Python)
DESCRIPTION: Defines a function `print_statistics` that evaluates a given `policy` by running the `walk` simulation 100 times. It counts the number of wins, losses to the wolf, and drownings, then prints a summary of these statistics.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/assignment-solution.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
def print_statistics(policy):
    s,w,n = 0,0,0
    for _ in range(100):
        z = walk(m,policy)
        if z<0:
            w+=1
        elif z==0:
            n+=1
        else:
            s+=1
    print(f"Killed by wolf = {w}, won: {s} times, drown: {n} times")

print_statistics(random_policy)
```

----------------------------------------

TITLE: Loading and Inspecting Time Series Data (Python)
DESCRIPTION: Sets the data directory path and uses the `load_data` utility function to load the energy data, selecting only the 'load' column. It then displays the initial rows of the resulting DataFrame to inspect its structure and contents.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/1-Introduction/README.md#_snippet_1

LANGUAGE: python
CODE:
```
data_dir = './data'
energy = load_data(data_dir)[['load']]
energy.head()
```

----------------------------------------

TITLE: Defining KNN Classifier Specification in R
DESCRIPTION: Defines a specification for a K-Nearest Neighbor model using `nearest_neighbor`, setting the engine to `kknn` and the mode to `classification`. This specification can then be added to a `tidymodels` workflow, similar to the SVM models, for training and evaluation. Requires `tidymodels` and `kknn` packages.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/3-Classifiers-2/solution/R/lesson_12-R.ipynb#_snippet_7

LANGUAGE: R
CODE:
```
# Make a KNN specification
knn_spec <- nearest_neighbor() %>%
  set_engine("kknn") %>%
  set_mode("classification")
```

----------------------------------------

TITLE: Calculating Silhouette Score for Clustering (Python)
DESCRIPTION: Computes the silhouette score to evaluate the quality of the K-Means clustering result obtained using 3 clusters. This metric ranges from -1 to 1, where a higher score (closer to 1) indicates better-defined and well-separated clusters. Requires `sklearn.metrics`. It takes the data `X` and the predicted cluster labels `y_cluster_kmeans` as input.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/de/5-Clustering/2-K-Means/README.md#_snippet_3

LANGUAGE: python
CODE:
```
from sklearn import metrics
score = metrics.silhouette_score(X, y_cluster_kmeans)
score
```

----------------------------------------

TITLE: Importing and Viewing Cuisine Dataset in R
DESCRIPTION: Reads the cuisine dataset from a raw GitHub URL into an R tibble named `df` using `read_csv`. It then displays the first 5 rows of the loaded dataset using the pipe operator (`%>%`) and the `slice_head` function for a quick preview of the data structure and content.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb#_snippet_1

LANGUAGE: R
CODE:
```
# Import data
df <- read_csv(file = "https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/data/cuisines.csv")

# View the first 5 rows
df %>%
  slice_head(n = 5)
```

----------------------------------------

TITLE: Applying Multiple Feature Encoders (Python)
DESCRIPTION: Imports `ColumnTransformer` from `sklearn.compose`. Creates a `ColumnTransformer` instance that applies the ordinal encoder to `ordinal_features` and the categorical encoder to `categorical_features`. Sets the output to a pandas DataFrame and fits/transforms the `pumpkins` data to get the encoded features.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/4-Logistic/README.md#_snippet_5

LANGUAGE: Python
CODE:
```
from sklearn.compose import ColumnTransformer

ct = ColumnTransformer(transformers=[
    ('ord', ordinal_encoder, ordinal_features),
    ('cat', categorical_encoder, categorical_features)
    ])

ct.set_output(transform='pandas')
encoded_features = ct.fit_transform(pumpkins)
```

----------------------------------------

TITLE: Creating Grouped Bar Chart with Pandas/Matplotlib (Python)
DESCRIPTION: Groups the `new_pumpkins` DataFrame by 'Month', calculates the mean 'Price' for each month, and then uses Pandas' built-in plotting functionality (which uses Matplotlib internally) to create a bar chart. It also sets the y-axis label for clarity.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/README.md#_snippet_9

LANGUAGE: Python
CODE:
```
new_pumpkins.groupby(['Month'])['Price'].mean().plot(kind='bar')
plt.ylabel("Pumpkin Price")
```

----------------------------------------

TITLE: Selecting Features for Polynomial Model (Pandas) - Python
DESCRIPTION: This snippet creates a new DataFrame `poly_pumpkins` by selecting specific columns ('Variety', 'Package', 'City', 'Month', 'Price') from the cleaned data. This broader set of features is used for exploring potentially non-linear relationships with polynomial regression.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/translations/README.ko.md#_snippet_9

LANGUAGE: Python
CODE:
```
new_columns = ['Variety', 'Package', 'City', 'Month', 'Price']
poly_pumpkins = new_pumpkins.drop([c for c in new_pumpkins.columns if c not in new_columns], axis='columns')

poly_pumpkins
```

----------------------------------------

TITLE: Print Classification Report Python
DESCRIPTION: Generates and displays a detailed classification report comparing the actual test labels (`y_test`) with the model's predictions (`y_pred`). This report provides key metrics such as precision, recall, f1-score, and support for each class, along with overall accuracy, offering a comprehensive view of the model's performance.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/README.md#_snippet_8

LANGUAGE: python
CODE:
```
print(classification_report(y_test,y_pred))
```

----------------------------------------

TITLE: Calculate Average Price and Extract Month with Pandas (Python)
DESCRIPTION: Calculates the average price from 'Low Price' and 'High Price' columns and extracts the month number from the 'Date' column. These calculations create new data series that will be added to a new DataFrame for regression analysis. Requires pandas.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/README.md#_snippet_3

LANGUAGE: python
CODE:
```
price = (pumpkins['Low Price'] + pumpkins['High Price']) / 2

month = pd.DatetimeIndex(pumpkins['Date']).month
```

----------------------------------------

TITLE: Creating Box Plots with Faceting R
DESCRIPTION: Generates individual box plots for each numeric feature present in the long-format `df_numeric_long` dataframe using `ggplot2`. `facet_wrap` is used to display each feature's box plot in a separate panel, allowing visual inspection of data distribution and outliers.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb#_snippet_4

LANGUAGE: R
CODE:
```
# Make a box plot
df_numeric_long %>%
  ggplot(mapping = aes(x = feature_names, y = values, fill = feature_names)) +
  geom_boxplot() +
  facet_wrap(~ feature_names, ncol = 4, scales = "free") +
  theme(legend.position = "none")
```

----------------------------------------

TITLE: Initializing Q-Table and Q-Value Helper Function - Python
DESCRIPTION: Initializes the Q-Table as an empty Python dictionary and defines the `qvalues` function. This function retrieves Q-values for a specific state across all possible actions, returning 0 if an entry doesn't exist. It uses a tuple `(state, action)` as the dictionary key.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/README.md#_snippet_8

LANGUAGE: Python
CODE:
```
Q = {}
actions = (0,1)

def qvalues(state):
    return [Q.get((state,a),0) for a in actions]
```

----------------------------------------

TITLE: Training and Evaluating Logistic Regression Model with Scikit-learn - Python
DESCRIPTION: This snippet trains a Logistic Regression model using the training data (X_train, y_train) and then evaluates its performance on the test data (X_test) by generating a classification report and calculating the F1-score. It requires `sklearn.linear_model` and `sklearn.metrics`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/4-Logistic/README.md#_snippet_11

LANGUAGE: python
CODE:
```
from sklearn.metrics import f1_score, classification_report 
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)

print(classification_report(y_test, predictions))
print('Predicted labels: ', predictions)
print('F1-score: ', f1_score(y_test, predictions))
```

----------------------------------------

TITLE: Converting Q-Values to Probabilities (Python)
DESCRIPTION: This function converts a vector of Q-values into a probability distribution over actions. It adds a small epsilon value to prevent division by zero, especially in initial states where all values might be the same. The resulting vector represents the probability of selecting each corresponding action.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/8-Reinforcement/1-QLearning/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
def probs(v,eps=1e-4):
    v = v-v.min()+eps
    v = v/v.sum()
    return v
```

----------------------------------------

TITLE: Calculating Action Probabilities Python
DESCRIPTION: This Python function converts an array of values (like Q-values for a state) into a probability distribution. It adds a small epsilon to all values to avoid division by zero, especially in the initial state when values might be identical, and then normalizes the values so they sum to 1.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/README.md#_snippet_6

LANGUAGE: Python
CODE:
```
def probs(v,eps=1e-4):
    v = v-v.min()+eps
    v = v/v.sum()
    return v
```

----------------------------------------

TITLE: Calculating Model Determination Score (R-squared) (Python)
DESCRIPTION: This snippet calculates the coefficient of determination (R-squared) for the trained linear regression model on the training data using the score() method. R-squared indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. Dependencies: Trained lin_reg model, training data X_train, y_train.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/2-Regression/3-Linear/README.md#_snippet_10

LANGUAGE: python
CODE:
```
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```

----------------------------------------

TITLE: Importing ONNX Runtime Web Library - HTML
DESCRIPTION: Includes a `<script>` tag to import the ONNX Runtime Web library (ort.min.js) from a Content Delivery Network (CDN). This external script provides the necessary JavaScript objects and functions (`ort`) to load and run ONNX machine learning models in the browser environment.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/README.md#_snippet_13

LANGUAGE: HTML
CODE:
```
<script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.9.0/dist/ort.min.js"></script>
```

----------------------------------------

TITLE: Visualizing Linear Regression (Matplotlib) - Python
DESCRIPTION: This snippet uses Matplotlib to create a scatter plot of the actual test data points (Package vs. Price). It then overlays the fitted linear regression line generated by the model's predictions on the test data. Axes are labeled for clarity.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/translations/README.ko.md#_snippet_7

LANGUAGE: Python
CODE:
```
plt.scatter(X_test, y_test,  color='black')
plt.plot(X_test, pred, color='blue', linewidth=3)

plt.xlabel('Package')
plt.ylabel('Price')

plt.show()
```

----------------------------------------

TITLE: Create Workflow (Tidymodels/workflows) - R
DESCRIPTION: Bundles the defined model specification (`mr_spec`) and a preprocessing recipe (`cuisines_recipe`) into a single `workflow` object. This workflow simplifies the modeling process by ensuring that data preprocessing steps are automatically applied before model fitting and prediction.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/2-Classifiers-1/solution/R/lesson_11-R.ipynb#_snippet_5

LANGUAGE: R
CODE:
```
# Bundle recipe and model specification
mr_wf <- workflow() %>%
  add_recipe(cuisines_recipe) %>%
  add_model(mr_spec)

# Print out workflow
mr_wf
```

----------------------------------------

TITLE: Cleaning Data (Pandas) - Python
DESCRIPTION: This snippet removes rows containing any missing (NaN) values directly from the 'new_pumpkins' DataFrame using `dropna(inplace=True)`. It then prints information about the cleaned DataFrame, including column types and non-null counts, to verify the cleaning step.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/translations/README.ko.md#_snippet_3

LANGUAGE: Python
CODE:
```
new_pumpkins.dropna(inplace=True)
new_pumpkins.info()
```

----------------------------------------

TITLE: Defining Training and Testing Start Dates (Python)
DESCRIPTION: Sets string variables for the start dates of the training and testing periods. These dates are used to split the original time series data into distinct subsets.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
train_start_dt = '2014-11-01 00:00:00'
test_start_dt = '2014-12-30 00:00:00'
```

----------------------------------------

TITLE: Splitting Time-Stepped Data into Features and Targets (Python)
DESCRIPTION: Splits the time-stepped 2D arrays for training and testing into feature sets (inputs, X) and target sets (outputs, y). Features consist of the first 'timesteps-1' columns, and the target is the last column in each sequence.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_13

LANGUAGE: python
CODE:
```
x_train, y_train = train_data_timesteps[:,:timesteps-1],train_data_timesteps[:,[timesteps-1]]
x_test, y_test = test_data_timesteps[:,:timesteps-1],test_data_timesteps[:,[timesteps-1]]
```

----------------------------------------

TITLE: Visualizing Training and Testing Data Split (Python)
DESCRIPTION: Filters the energy data based on the defined training and testing start dates and plots the respective segments side-by-side. This helps visualize how the data is partitioned.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
energy[(energy.index < test_start_dt) & (energy.index >= train_start_dt)][['load']].rename(columns={'load':'train'}) \
    .join(energy[test_start_dt:][['load']].rename(columns={'load':'test'}), how='outer') \
    .plot(y=['train', 'test'], figsize=(15, 8), fontsize=12)
plt.xlabel('timestamp', fontsize=12)
plt.ylabel('load', fontsize=12)
plt.show()
```

----------------------------------------

TITLE: Defining X and y for Polynomial Model (Pandas) - Python
DESCRIPTION: This snippet extracts the feature variable (X) and the target variable (y) for the polynomial model from the `poly_pumpkins` DataFrame using `iloc` for index-based selection. X is set to the 'Month' column (index 3), and y to the 'Price' column (index 4).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/translations/README.ko.md#_snippet_11

LANGUAGE: Python
CODE:
```
X=poly_pumpkins.iloc[:,3:4].values
y=poly_pumpkins.iloc[:,4:5].values
```

----------------------------------------

TITLE: Defining Train and Test Set Start Dates in Python
DESCRIPTION: This code block defines string variables representing the start dates for the training and testing periods of the time series data. These dates are used later to filter the DataFrame into distinct training and testing sets.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/README.md#_snippet_3

LANGUAGE: python
CODE:
```
train_start_dt = '2014-11-01 00:00:00'
test_start_dt = '2014-12-30 00:00:00'
```

----------------------------------------

TITLE: Displaying Top Thai Ingredients in R
DESCRIPTION: Calls the previously defined `create_ingredient` function with the `thai_df` dataframe to process the Thai cuisine data. It then uses `slice_head` from `dplyr` to display the top 10 most frequently occurring ingredients for Thai cuisine based on the processed data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb#_snippet_7

LANGUAGE: R
CODE:
```
# Call create_ingredient and display popular ingredients
thai_ingredient_df <- create_ingredient(df = thai_df)

thai_ingredient_df %>% 
  slice_head(n = 10)
```

----------------------------------------

TITLE: Loading and Inspecting Diabetes Dataset with scikit-learn (Python)
DESCRIPTION: Loads the diabetes dataset using `load_diabetes` from `sklearn.datasets`, requesting the data matrix (X) and target vector (y) separately. Prints the shape of the data matrix X and the first element of X to inspect the data structure. Requires scikit-learn's `datasets` module.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/2-Regression/1-Tools/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
X, y = datasets.load_diabetes(return_X_y=True)
print(X.shape)
print(X[0])
```

----------------------------------------

TITLE: Clean Data Drop NA Print Info Python Pandas
DESCRIPTION: This snippet removes rows containing any missing values (NaN) from the 'pie_pumpkins' DataFrame in place and then prints a concise summary of the DataFrame, including the index dtype, columns and their dtypes, and non-null values.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/2-Regression/3-Linear/README.md#_snippet_4

LANGUAGE: python
CODE:
```
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

----------------------------------------

TITLE: Importing Libraries for Simulation and RL (Python)
DESCRIPTION: Imports necessary libraries for numerical operations (numpy), plotting (matplotlib), random number generation (random, math), and the custom grid board environment (rlboard). These are prerequisites for running the simulation and training algorithm.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/assignment-solution.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import matplotlib.pyplot as plt
import numpy as np
import random
import math
from rlboard import *
```

----------------------------------------

TITLE: Calculating Hotel Review Frequency with Pandas
DESCRIPTION: Calculates the number of reviews available for each hotel within the dataset. It creates a temporary DataFrame by dropping many columns, groups by `Hotel_Name`, counts the occurrences, and adds the count to a new column `Total_Reviews_Found`. Finally, it removes duplicate hotel entries and displays the unique hotel names with their calculated review counts. Requires the `df` DataFrame. Outputs a DataFrame summarizing review counts per hotel.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_4

LANGUAGE: python
CODE:
```
# First create a new dataframe based on the old one, removing the uneeded columns
hotel_freq_df = df.drop(["Hotel_Address", "Additional_Number_of_Scoring", "Review_Date", "Average_Score", "Reviewer_Nationality", "Negative_Review", "Review_Total_Negative_Word_Counts", "Positive_Review", "Review_Total_Positive_Word_Counts", "Total_Number_of_Reviews_Reviewer_Has_Given", "Reviewer_Score", "Tags", "days_since_review", "lat", "lng"], axis = 1)

# Group the rows by Hotel_Name, count them and put the result in a new column Total_Reviews_Found
hotel_freq_df['Total_Reviews_Found'] = hotel_freq_df.groupby('Hotel_Name').transform('count')

# Get rid of all the duplicated rows
hotel_freq_df = hotel_freq_df.drop_duplicates(subset = ["Hotel_Name"])
display(hotel_freq_df)
```

----------------------------------------

TITLE: Loading Time Series Data (Python)
DESCRIPTION: Loads time series data from a specified file path using a custom utility function 'load_data'. It then selects and keeps only the 'load' column from the loaded data, preparing it for subsequent analysis and modeling steps.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_2

LANGUAGE: python
CODE:
```
energy = load_data('../../data')[['load']]
```

----------------------------------------

TITLE: Performing One-Hot Encoding with Pandas
DESCRIPTION: Demonstrates how to apply one-hot encoding to a single categorical column ('Variety') using the `pd.get_dummies` function from the Pandas library. This converts the categorical feature into multiple binary columns, one for each unique category value.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ms/2-Regression/3-Linear/README.md#_snippet_14

LANGUAGE: python
CODE:
```
pd.get_dummies(new_pumpkins['Variety'])
```

----------------------------------------

TITLE: Extracting Time Series Timestamps Python
DESCRIPTION: This snippet extracts timestamps relevant to the output predictions for both training and testing datasets. It accounts for the lag introduced by the `timesteps` parameter, ensuring timestamps align correctly with the model's output. It then prints the counts of timestamps for validation purposes.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_19

LANGUAGE: python
CODE:
```
train_timestamps = energy[(energy.index < test_start_dt) & (energy.index >= train_start_dt)].index[timesteps-1:]
test_timestamps = energy[test_start_dt:].index[timesteps-1:]

print(len(train_timestamps), len(test_timestamps))
```

----------------------------------------

TITLE: Installing R Packages with pacman
DESCRIPTION: Checks if the 'pacman' package is installed and installs it if necessary. Then uses 'pacman' to load the 'tidyverse' package collection, installing it first if not found. This ensures the necessary libraries for data manipulation and visualization are available.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/solution/R/lesson_2-R.ipynb#_snippet_0

LANGUAGE: R
CODE:
```
suppressWarnings(if(!require("pacman")) install.packages("pacman"))
pacman::p_load(tidyverse)
```

----------------------------------------

TITLE: Plotting Top Chinese Ingredients Bar Chart in R
DESCRIPTION: Applies the `create_ingredient` function to the `chinese_df` to find the top 10 ingredients for Chinese cuisine. It then uses `ggplot2` to create a bar chart visualizing the frequencies of these top ingredients.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb#_snippet_10

LANGUAGE: R
CODE:
```
# Get popular ingredients for Chinese cuisines and make bar chart
create_ingredient(df = chinese_df) %>% 
  slice_head(n = 10) %>% 
  ggplot(aes(x = n_instances, y = ingredients)) +
  geom_bar(stat = "identity", width = 0.5, fill = "cyan4", alpha = 0.8) +
  xlab("") + ylab("")
```

----------------------------------------

TITLE: Navigating to the Web App Directory (Bash)
DESCRIPTION: Changes the current directory in the terminal to the `web-app` folder. This is a necessary step before installing dependencies or running the application from within the correct project context.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/3-Web-App/1-Web-App/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
cd web-app
```

----------------------------------------

TITLE: Sentiment Analysis Setup and Data Loading (Python)
DESCRIPTION: This snippet sets up the environment for sentiment analysis. It imports necessary libraries like pandas and nltk, downloads the 'vader_lexicon' required for VADER sentiment analysis, and loads the previously filtered hotel review dataset from a CSV file. It also includes placeholder comments for where sentiment analysis code should be added and saves the data again at the end.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_2

LANGUAGE: python
CODE:
```
import time
import pandas as pd
import nltk as nltk
from nltk.corpus import stopwords
from nltk.sentiment.vader import SentimentIntensityAnalyzer
ltk.download('vader_lexicon')

# Load the filtered hotel reviews from CSV
df = pd.read_csv('../../data/Hotel_Reviews_Filtered.csv')

# You code will be added here


# Finally remember to save the hotel reviews with new NLP data added
print("Saving results to Hotel_Reviews_NLP.csv")
df.to_csv(r'../data/Hotel_Reviews_NLP.csv', index = False)

```

----------------------------------------

TITLE: Importing Required Python Libraries
DESCRIPTION: Imports various necessary Python libraries including pandas for data manipulation, numpy for numerical operations, matplotlib for plotting, and components from statsmodels and scikit-learn for time series modeling and scaling. Also includes utility imports and settings for display and warnings.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/solution/notebook.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
import os
import warnings
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import datetime as dt
import math

from pandas.plotting import autocorrelation_plot
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.preprocessing import MinMaxScaler
from common.utils import load_data, mape
from IPython.display import Image

%matplotlib inline
pd.options.display.float_format = '{:,.2f}'.format
np.set_printoptions(precision=2)
warnings.filterwarnings("ignore") # specify to ignore warning messages
```

----------------------------------------

TITLE: Helper Function for Probability Distribution (Python)
DESCRIPTION: Defines a helper function `probs` that converts an array of raw values (like Q-values for a state) into a probability distribution. It adds a small epsilon for numerical stability, ensuring no probability is zero, and then normalizes the values so they sum to 1.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/assignment-solution.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
def probs(v,eps=1e-4):
    v = v-v.min()+eps
    v = v/v.sum()
    return v
```

----------------------------------------

TITLE: Print DataFrame Shape with Pandas Python
DESCRIPTION: This snippet uses the `.shape` attribute of a pandas DataFrame to retrieve the dimensions (rows, columns) and prints them to the console. It requires a loaded pandas DataFrame, typically named `df` based on common convention.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_1

LANGUAGE: python
CODE:
```
print("The shape of the data (rows, cols) is " + str(df.shape))
```

----------------------------------------

TITLE: Defining Agent Actions and Indices (Python)
DESCRIPTION: Defines a dictionary `actions` mapping directional action names (U, D, L, R) to corresponding coordinate changes (dx, dy). Also creates `action_idx`, a dictionary mapping action names to integer indices for use with arrays like the Q-table.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/assignment-solution.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

----------------------------------------

TITLE: Styling the Flask Web App with CSS
DESCRIPTION: Defines basic CSS styles for the web application's HTML elements. It sets body properties like font, background, color, and text alignment, styles input fields, and defines a simple grid layout for centering content. This improves the visual presentation of the web page.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/3-Web-App/1-Web-App/README.md#_snippet_3

LANGUAGE: CSS
CODE:
```
body {
	width: 100%;
	height: 100%;
	font-family: 'Helvetica';
	background: black;
	color: #fff;
	text-align: center;
	letter-spacing: 1.4px;
	font-size: 30px;
}

input {
	min-width: 150px;
}

.grid {
	width: 300px;
	border: 1px solid #2d2d2d;
	display: grid;
	justify-content: center;
	margin: 20px auto;
}

.box {
	color: #fff;
	background: #2d2d2d;
	padding: 12px;
	display: inline-block;
}
```

----------------------------------------

TITLE: Including ONNX Runtime Library (CDN) in HTML
DESCRIPTION: Adds the ONNX Runtime Web library to the HTML page using a Content Delivery Network (CDN). This script tag allows the page to use the `ort` object for loading and running ONNX models directly in the browser.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/sw/4-Classification/4-Applied/README.md#_snippet_2

LANGUAGE: html
CODE:
```
<script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.9.0/dist/ort.min.js"></script>
```

----------------------------------------

TITLE: Include ONNX Runtime Web Library HTML
DESCRIPTION: Includes the ONNX Runtime Web library in the HTML document by adding a `<script>` tag with a CDN source. This makes the `ort` object available globally in the browser's JavaScript environment, enabling ONNX model inference in the client-side application.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/4-Classification/4-Applied/README.md#_snippet_13

LANGUAGE: html
CODE:
```
<script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.9.0/dist/ort.min.js"></script>
```

----------------------------------------

TITLE: Installing statsmodels Python Library
DESCRIPTION: Installs the `statsmodels` library using pip. This library is essential for implementing ARIMA and SARIMAX models in Python for time series analysis and forecasting.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/solution/notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
!pip install statsmodels
```

----------------------------------------

TITLE: Importing Libraries for Linear Regression
DESCRIPTION: Imports necessary classes and functions from the scikit-learn library for performing linear regression. Specifically, it imports `LinearRegression` for the model, `mean_squared_error` for evaluation, and `train_test_split` for splitting data. This is a prerequisite for the subsequent model training steps. Requires the scikit-learn library installed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/fr/2-Regression/3-Linear/README.md#_snippet_5

LANGUAGE: python
CODE:
```
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

----------------------------------------

TITLE: Analyzing and Plotting Top Thai Ingredients - Python
DESCRIPTION: Uses the `create_ingredient_df` function to get a dataframe for Thai cuisine data. It then selects the top 10 ingredients from the resulting dataframe and generates a horizontal bar plot for visualization. Requires the function and the Thai data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/README.md#_snippet_8

LANGUAGE: python
CODE:
```
thai_ingredient_df = create_ingredient_df(thai_df)
thai_ingredient_df.head(10).plot.barh()
```

----------------------------------------

TITLE: Sorting DataFrame by Sentiment Scores in Python
DESCRIPTION: This snippet sorts the pandas DataFrame `df` first by 'Negative_Sentiment' and then by 'Positive_Sentiment', both in ascending order. It then prints the corresponding review text and sentiment scores for verification. Requires a pandas DataFrame with the specified review and sentiment columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_10

LANGUAGE: python
CODE:
```
df = df.sort_values(by=["Negative_Sentiment"], ascending=True)
print(df[["Negative_Review", "Negative_Sentiment"]])
df = df.sort_values(by=["Positive_Sentiment"], ascending=True)
print(df[["Positive_Review", "Positive_Sentiment"]])
```

----------------------------------------

TITLE: Calculating Overall Multi-Step Forecast MAPE Python
DESCRIPTION: Calculates the overall Mean Absolute Percentage Error (MAPE) across all forecast steps and all test points in the evaluation DataFrame. Prints the aggregate MAPE as a percentage, providing a single overall performance metric for the model on the test set.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/solution/notebook.ipynb#_snippet_17

LANGUAGE: python
CODE:
```
print('Multi-step forecast MAPE: ', mape(eval_df['prediction'], eval_df['actual'])*100, '%')
```

----------------------------------------

TITLE: Creating Training and Testing DataFrames (Python)
DESCRIPTION: Copies and filters the main energy DataFrame to create separate DataFrames for the training and testing sets based on the defined start dates. It also prints the shape of the resulting DataFrames.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
train = energy.copy()[(energy.index >= train_start_dt) & (energy.index < test_start_dt)][['load']]
test = energy.copy()[energy.index >= test_start_dt][['load']]

print('Training data shape: ', train.shape)
print('Test data shape: ', test.shape)
```

----------------------------------------

TITLE: Selecting Specific Columns with Pandas in Python
DESCRIPTION: Filters the DataFrame to retain only a predefined list of columns ('Package', 'Low Price', 'High Price', 'Date') using the `.loc` indexer. This reduces the dataset size and focuses on the features relevant for the regression task.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/2-Regression/2-Data/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
columns_to_select = ['Package', 'Low Price', 'High Price', 'Date']
pumpkins = pumpkins.loc[:, columns_to_select]
```

----------------------------------------

TITLE: Preparing Data for Scikit-learn Regression in Python
DESCRIPTION: This snippet selects the 'DayOfYear' column as the feature (X) and 'Price' as the label (y) from the 'pie_pumpkins' DataFrame. It converts the feature column to a NumPy array and reshapes it into a 2D array (-1, 1) as required by Scikit-learn models for single features.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/README.md#_snippet_7

LANGUAGE: Python
CODE:
```
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

----------------------------------------

TITLE: Importing Libraries for Time Series Analysis (Python)
DESCRIPTION: Imports standard libraries like `os` and `matplotlib.pyplot` for file system operations and plotting. It also imports a custom `load_data` function from a local `common.utils` module and configures matplotlib for inline plotting in a notebook environment.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/1-Introduction/README.md#_snippet_0

LANGUAGE: python
CODE:
```
import os
import matplotlib.pyplot as plt
from common.utils import load_data
%matplotlib inline
```

----------------------------------------

TITLE: Printing Most Negative Sentences Python
DESCRIPTION: Prints a header indicating the number of most negative sentences found. It then iterates through the negative_sentiment_sentences list, printing each sentence prefixed with "- ". Newlines and multiple spaces within sentences are removed for cleaner output.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/3-Translation-Sentiment/solution/notebook.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
print("The " + str(len(negative_sentiment_sentences)) + " most negative sentences:")
for sentence in negative_sentiment_sentences:
    print("- " + str(sentence.replace("\n", "").replace("      ", " ")))

```

----------------------------------------

TITLE: Predicting with Linear Regression (Scikit-learn) - Python
DESCRIPTION: This snippet uses the trained `lin_reg` model to make a price prediction for a hypothetical package value (2.75). It demonstrates how to pass new data points (formatted as a NumPy array) to the model's `predict` method.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/translations/README.ko.md#_snippet_8

LANGUAGE: Python
CODE:
```
lin_reg.predict( np.array([ [2.75] ]) )
```

----------------------------------------

TITLE: Checking for Missing Values with Pandas
DESCRIPTION: Uses the `isnull()` method to create a boolean DataFrame indicating missing values, followed by `sum()` to count the number of missing values per column. This helps identify which columns have incomplete data before cleaning or processing. Requires a Pandas DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/2-Regression/2-Data/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
pumpkins.isnull().sum()
```

----------------------------------------

TITLE: Plotting Top Indian Ingredients Bar Chart in R
DESCRIPTION: Processes the `indian_df` using `create_ingredient` to identify the top 10 ingredients for Indian cuisine. A bar chart is then generated using `ggplot2` to visualize the frequencies of these ingredients.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb#_snippet_11

LANGUAGE: R
CODE:
```
# Get popular ingredients for Indian cuisines and make bar chart
create_ingredient(df = indian_df) %>% 
  slice_head(n = 10) %>% 
  ggplot(aes(x = n_instances, y = ingredients)) +
  geom_bar(stat = "identity", width = 0.5, fill = "#041E42FF", alpha = 0.8) +
  xlab("") + ylab("")
```

----------------------------------------

TITLE: Plotting Episode Lengths During Training (Python)
DESCRIPTION: Plots the values stored in the `lpath` list, which records the number of steps taken in each episode during the Q-learning training process. This graph can illustrate how the agent's performance (measured by episode length) changes over time as it learns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/assignment-solution.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
plt.plot(lpath)
```

----------------------------------------

TITLE: Importing Libraries for Linear Regression in Python
DESCRIPTION: Imports necessary classes from the Scikit-learn library for implementing linear regression. This includes `LinearRegression` for the model, `mean_squared_error` for evaluating the model's performance, and `train_test_split` for dividing data into training and testing sets.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/2-Regression/3-Linear/README.md#_snippet_5

LANGUAGE: python
CODE:
```
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

----------------------------------------

TITLE: Getting Observation Space Bounds in Gym Python
DESCRIPTION: Retrieves and prints the minimum and maximum values for each dimension of the observation space in the CartPole environment. This is useful for understanding the range of values expected for the state variables (cart position, velocity, pole angle, rotation rate).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/README.md#_snippet_4

LANGUAGE: python
CODE:
```
print(env.observation_space.low)
print(env.observation_space.high)
```

----------------------------------------

TITLE: Evaluate Linear Regression Model R2 Score Python Scikit-learn
DESCRIPTION: This snippet calculates the coefficient of determination (R² score) for the trained linear regression model using the training data. The R² score indicates how well the model's predictions fit the actual data, with 1.0 representing a perfect fit.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/2-Regression/3-Linear/README.md#_snippet_10

LANGUAGE: python
CODE:
```
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```

----------------------------------------

TITLE: Loading Filtered Data and Initializing NLP (Python)
DESCRIPTION: Imports necessary Python libraries including time, pandas, and nltk for data manipulation and natural language processing. It then loads the previously saved 'Hotel_Reviews_Filtered.csv' file into a pandas DataFrame and downloads the 'vader_lexicon' required for NLTK's VADER sentiment analysis.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_2

LANGUAGE: python
CODE:
```
import time
import pandas as pd
import nltk as nltk
from nltk.corpus import stopwords
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

# Load the filtered hotel reviews from CSV
df = pd.read_csv('../../data/Hotel_Reviews_Filtered.csv')

# You code will be added here


# Finally remember to save the hotel reviews with new NLP data added
print("Saving results to Hotel_Reviews_NLP.csv")
df.to_csv(r'../data/Hotel_Reviews_NLP.csv', index = False)
```

----------------------------------------

TITLE: Plotting Top Korean Ingredients Bar Chart in R
DESCRIPTION: Uses the `create_ingredient` function on the `korean_df` to get the top 10 ingredients for Korean cuisine. A bar chart is then created using `ggplot2` to visualize their frequencies.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb#_snippet_12

LANGUAGE: R
CODE:
```
# Get popular ingredients for Korean cuisines and make bar chart
create_ingredient(df = korean_df) %>% 
  slice_head(n = 10) %>% 
  ggplot(aes(x = n_instances, y = ingredients)) +
  geom_bar(stat = "identity", width = 0.5, fill = "#852419FF", alpha = 0.8) +
  xlab("") + ylab("")
```

----------------------------------------

TITLE: Counting Initial Cuisine Distribution in R
DESCRIPTION: Calculates and displays the initial distribution of cuisine types within the `df_select` dataframe using `dplyr::count`. The results are arranged in descending order to show which cuisines are most and least frequent, highlighting the data imbalance.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb#_snippet_14

LANGUAGE: R
CODE:
```
# Distribution of cuisines
old_label_count <- df_select %>% 
  count(cuisine) %>% 
  arrange(desc(n))

old_label_count
```

----------------------------------------

TITLE: Adding K-Neighbors Classifier - Scikit-learn - Python
DESCRIPTION: Adds a K-Neighbors Classifier (`KNeighborsClassifier`) to the existing `classifiers` dictionary. This allows the subsequent training and evaluation loop to include this model. Note that the parameter `C` is incorrectly passed to `KNeighborsClassifier` in the source code; it should be `n_neighbors`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/4-Classification/3-Classifiers-2/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
'KNN classifier': KNeighborsClassifier(C),
```

----------------------------------------

TITLE: Listing Unique Values in a Column with dplyr in R
DESCRIPTION: Uses the 'dplyr::distinct()' function to find and return all unique observations present in the 'Package' column of the 'pumpkins' dataframe. This helps identify the different ways pumpkins are packaged in the dataset.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/solution/R/lesson_2-R.ipynb#_snippet_8

LANGUAGE: R
CODE:
```
pumpkins %>% 
  distinct(Package)
```

----------------------------------------

TITLE: Loading and Inspecting Data - Python
DESCRIPTION: This snippet loads the US-pumpkins.csv dataset into a pandas DataFrame using `pd.read_csv` and displays the first few rows using `.head()` to inspect the data structure and content. It requires the pandas and numpy libraries.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import pandas as pd
import numpy as np

full_pumpkins = pd.read_csv('../../data/US-pumpkins.csv')

full_pumpkins.head()
```

----------------------------------------

TITLE: Plotting Mean Price by Variety using Pandas in Python
DESCRIPTION: Calculates the mean price for each unique pumpkin 'Variety' using Pandas `groupby()` and `mean()`. It then plots these mean prices as a bar chart to visually compare average prices across different varieties.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/2-Regression/3-Linear/README.md#_snippet_2

LANGUAGE: python
CODE:
```
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

----------------------------------------

TITLE: Sorting and Printing Reviews by Sentiment Python
DESCRIPTION: Sorts the DataFrame first by 'Negative_Sentiment' in ascending order and prints the 'Negative_Review' and 'Negative_Sentiment' columns. Then, sorts the DataFrame by 'Positive_Sentiment' in ascending order and prints the 'Positive_Review' and 'Positive_Sentiment' columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/3-notebook.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
df = df.sort_values(by=["Negative_Sentiment"], ascending=True)
print(df[["Negative_Review", "Negative_Sentiment"]])
df = df.sort_values(by=["Positive_Sentiment"], ascending=True)
print(df[["Positive_Review", "Positive_Sentiment"]])

```

----------------------------------------

TITLE: Loading CSV Data with Pandas (Python)
DESCRIPTION: Reads a CSV file containing cleaned cuisine data into a pandas DataFrame named `data`. It then displays the first few rows of the DataFrame using the `.head()` method to inspect the data structure and content.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/solution/notebook.ipynb#_snippet_2

LANGUAGE: Python
CODE:
```
data = pd.read_csv('../../data/cleaned_cuisines.csv')
data.head()
```

----------------------------------------

TITLE: Scaling Test Data MinMaxScaler Python
DESCRIPTION: Applies the *same* `MinMaxScaler` instance that was fitted on the training data to the 'load' column of the test data (`test`). It is important to use `transform` only on the test data to prevent data leakage. The head of the scaled test data is displayed.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/solution/notebook.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
test['load'] = scaler.transform(test)
test.head()
```

----------------------------------------

TITLE: Initializing HTML Document Structure
DESCRIPTION: Sets up the basic HTML5 document structure with a title and body tags. This is the standard boilerplate for an HTML page, including the header and body elements where content will be added.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/4-Classification/4-Applied/README.md#_snippet_0

LANGUAGE: html
CODE:
```
<!DOCTYPE html>
<html>
    <header>
        <title>Cuisine Matcher</title>
    </header>
    <body>
        ...
    </body>
</html>
```

----------------------------------------

TITLE: Importing Libraries and Loading Data - Python
DESCRIPTION: Imports necessary libraries like pandas for data manipulation, matplotlib for plotting, numpy for numerical operations, and datetime. Loads the US-pumpkins.csv dataset into a pandas DataFrame for subsequent processing.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime

pumpkins = pd.read_csv('../../data/US-pumpkins.csv')
pumpkins.head()
```

----------------------------------------

TITLE: Split Data for Training Python
DESCRIPTION: Splits the feature data (`X`) and target data (`y`) into training and testing sets. 30% of the data is allocated to the test set (`test_size=0.3`), preparing the data for model training and subsequent evaluation.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/4-Classification/4-Applied/README.md#_snippet_5

LANGUAGE: python
CODE:
```
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)
```

----------------------------------------

TITLE: Visualizing Binary Category with Seaborn swarmplot (Python)
DESCRIPTION: This snippet uses Seaborn's `swarmplot` function to visualize the distribution of encoded 'Item Size' values ('ord__Item Size') for each binary category of 'Color'. It defines a simple color palette for the binary outcomes and plots the data points along the two axes. It requires the `encoded_pumpkins` DataFrame. Note that it might warn about too many points.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/2-Regression/4-Logistic/README.md#_snippet_1

LANGUAGE: python
CODE:
```
palette = {
0: 'orange',
1: 'wheat'
}
sns.swarmplot(x="Color", y="ord__Item Size", data=encoded_pumpkins, palette=palette)
```

----------------------------------------

TITLE: Importing Libraries Python
DESCRIPTION: This snippet imports the necessary Python libraries for data analysis and timing. It imports `pandas` for data manipulation and `time` for measuring execution duration.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/4-Hotel-Reviews-1/solution/notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
# EDA
import pandas as pd
import time
```

----------------------------------------

TITLE: Creating the HTML Frontend (HTML)
DESCRIPTION: Defines the structure of the web page using HTML5. It includes the page title, links to the CSS stylesheet, contains a form for user input (seconds, latitude, longitude), a submit button, and a paragraph where the prediction result will be displayed using Flask's template syntax.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/3-Web-App/1-Web-App/README.md#_snippet_4

LANGUAGE: html
CODE:
```
<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8">
    <title>🛸 UFO Appearance Prediction! 👽</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}">
  </head>

  <body>
    <div class="grid">

      <div class="box">

        <p>According to the number of seconds, latitude and longitude, which country is likely to have reported seeing a UFO?</p>

        <form action="{{ url_for('predict')}}" method="post">
          <input type="number" name="seconds" placeholder="Seconds" required="required" min="0" max="60" />
          <input type="text" name="latitude" placeholder="Latitude" required="required" />
          <input type="text" name="longitude" placeholder="Longitude" required="required" />
          <button type="submit" class="btn">Predict country where the UFO is seen</button>
        </form>

        <p>{{ prediction_text }}</p>

      </div>

    </div>

  </body>
</html>
```

----------------------------------------

TITLE: Basic HTML Structure HTML
DESCRIPTION: Defines the fundamental HTML structure for a web page, including the document type declaration, `html`, `header` with a `title`, and an empty `body` tag where content will be added.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/4-Classification/4-Applied/README.md#_snippet_11

LANGUAGE: html
CODE:
```
<!DOCTYPE html>
<html>
    <header>
        <title>Cuisine Matcher</title>
    </header>
    <body>
        ...
    </body>
</html>
```

----------------------------------------

TITLE: Calculating Correlation (Pandas) - Python
DESCRIPTION: This snippet calculates the Pearson correlation coefficient between the 'City' and 'Price' columns in the 'new_pumpkins' DataFrame. It helps assess the strength and direction of the linear relationship between these two variables.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/translations/README.ko.md#_snippet_1

LANGUAGE: Python
CODE:
```
print(new_pumpkins['City'].corr(new_pumpkins['Price']))
```

----------------------------------------

TITLE: Navigate to Web App Directory
DESCRIPTION: Changes the current directory in the terminal to the 'web-app' folder. This step is essential before installing dependencies or running the main application script (`app.py`) to ensure commands are executed in the correct project context.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/3-Web-App/1-Web-App/README.md#_snippet_1

LANGUAGE: Bash
CODE:
```
cd web-app
```

----------------------------------------

TITLE: Running the Flask Application (Bash)
DESCRIPTION: Executes the main Python script (`app.py`) using the Python interpreter. This command starts the Flask development server locally, making the web application accessible for user interaction and testing. The script will run until manually stopped.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/3-Web-App/1-Web-App/README.md#_snippet_6

LANGUAGE: bash
CODE:
```
python app.py
```

----------------------------------------

TITLE: Configuring Python Path for Imports
DESCRIPTION: This snippet modifies the Python system path to allow importing modules from a parent directory. This is useful when structuring projects with shared utility files.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import sys
sys.path.append('../../')
```

----------------------------------------

TITLE: Training SVR Model (Python)
DESCRIPTION: Trains the initialized SVR model using the prepared training features ('x_train') and target values ('y_train'). The 'fit' method learns the underlying patterns and relationships in the training data to build the forecasting model.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_15

LANGUAGE: python
CODE:
```
model.fit(x_train, y_train[:,0])
```

----------------------------------------

TITLE: Getting Data Structure Glimpse in R
DESCRIPTION: Provides a compact summary of the data frame, showing column names, data types, and the first few values for each column. It's a concise way to understand the structure and content.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb#_snippet_3

LANGUAGE: R
CODE:
```
# Glimpse into the data set
df %>%
  glimpse()
```

----------------------------------------

TITLE: Loading Dataset from CSV (Pandas Python)
DESCRIPTION: Loads the cuisine ingredient dataset from a CSV file into a pandas DataFrame named `df`. The file path specifies the location of the dataset relative to the current working directory.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/notebook.ipynb#_snippet_2

LANGUAGE: Python
CODE:
```
df  = pd.read_csv('../../data/cuisines.csv')
```

----------------------------------------

TITLE: Visualizing Initial Q-Table Python
DESCRIPTION: Calls the board's plot function, passing the initialized Q-table. This visualizes the initial state of the Q-table on the board, likely showing the initial uniform 'attractiveness' of actions at each state.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/notebook.ipynb#_snippet_8

LANGUAGE: Python
CODE:
```
m.plot(Q)
```

----------------------------------------

TITLE: Install Project Dependencies - npm/Node.js
DESCRIPTION: This command installs all necessary project dependencies listed in the `package.json` file. It should be run after cloning the repository to set up the development environment.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/quiz-app/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
npm install
```

----------------------------------------

TITLE: Initializing Game Environment - Python
DESCRIPTION: Imports the custom `rlboard` module to set up the grid-world environment. It creates a new `Board` instance with specified dimensions (8x8), populates it randomly using a fixed seed for reproducibility, and then plots the initial state of the board visually. This requires the `rlboard.py` file to be accessible.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

----------------------------------------

TITLE: Installing imblearn Python package
DESCRIPTION: Installs the 'imblearn' library using pip. This library is a Scikit-learn package designed to help handle imbalanced datasets, which is a common problem in classification tasks.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/README.md#_snippet_0

LANGUAGE: python
CODE:
```
pip install imblearn
```

----------------------------------------

TITLE: Counting Rows with Pandas Apply Python
DESCRIPTION: This snippet demonstrates counting rows in a Pandas DataFrame (`df`) where specific conditions are met in the 'Negative_Review' and 'Positive_Review' columns. It uses the `df.apply()` method with `lambda` functions to check for "No Negative" or "No Positive" values, creating boolean Series, and then counts the `True` values. It also measures and prints the execution time of this operation.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_0

LANGUAGE: python
CODE:
```
no_negative_reviews = df.apply(lambda x: True if x['Negative_Review'] == "No Negative" else False , axis=1)
print("Number of No Negative reviews: " + str(len(no_negative_reviews[no_negative_reviews == True].index)))

no_positive_reviews = df.apply(lambda x: True if x['Positive_Review'] == "No Positive" else False , axis=1)
print("Number of No Positive reviews: " + str(len(no_positive_reviews[no_positive_reviews == True].index)))

both_no_reviews = df.apply(lambda x: True if x['Negative_Review'] == "No Negative" and x['Positive_Review'] == "No Positive" else False , axis=1)
print("Number of both No Negative and No Positive reviews: " + str(len(both_no_reviews[both_no_reviews == True].index)))
end = time.time()
print("Lambdas took " + str(round(end - start, 2)) + " seconds")
```

----------------------------------------

TITLE: Defining Ingredient Selection UI with HTML
DESCRIPTION: Creates the main content for the web page body. It includes a heading, a container div (wrapper), multiple div elements (boxCont) each containing a checkbox (input type="checkbox") with a specific value (ingredient index) and a label. It also includes a button to trigger the recommendation logic.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/sw/4-Classification/4-Applied/README.md#_snippet_1

LANGUAGE: html
CODE:
```
<h1>Check your refrigerator. What can you create?</h1>
            <div id="wrapper">
                <div class="boxCont">
                    <input type="checkbox" value="4" class="checkbox">
                    <label>apple</label>
                </div>
            
                <div class="boxCont">
                    <input type="checkbox" value="247" class="checkbox">
                    <label>pear</label>
                </div>
            
                <div class="boxCont">
                    <input type="checkbox" value="77" class="checkbox">
                    <label>cherry</label>
                </div>
    
                <div class="boxCont">
                    <input type="checkbox" value="126" class="checkbox">
                    <label>fenugreek</label>
                </div>
    
                <div class="boxCont">
                    <input type="checkbox" value="302" class="checkbox">
                    <label>sake</label>
                </div>
    
                <div class="boxCont">
                    <input type="checkbox" value="327" class="checkbox">
                    <label>soy sauce</label>
                </div>
    
                <div class="boxCont">
                    <input type="checkbox" value="112" class="checkbox">
                    <label>cumin</label>
                </div>
            </div>
            <div style="padding-top:10px">
                <button onClick="startInference()">What kind of cuisine can you make?</button>
            </div>
```

----------------------------------------

TITLE: Adding Ingredient Checkboxes and Button - HTML
DESCRIPTION: Adds a heading, a container div (`#wrapper`) with individual divs (`.boxCont`) for ingredient checkboxes and labels, and a button (`<button>`) to trigger the inference process. Each checkbox has a `value` attribute corresponding to an ingredient index, and the button calls the `startInference()` JavaScript function.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/README.md#_snippet_12

LANGUAGE: HTML
CODE:
```
<h1>Check your refrigerator. What can you create?</h1>
            <div id="wrapper">
                <div class="boxCont">
                    <input type="checkbox" value="4" class="checkbox">
                    <label>apple</label>
                </div>
            
                <div class="boxCont">
                    <input type="checkbox" value="247" class="checkbox">
                    <label>pear</label>
                </div>
            
                <div class="boxCont">
                    <input type="checkbox" value="77" class="checkbox">
                    <label>cherry</label>
                </div>
    
                <div class="boxCont">
                    <input type="checkbox" value="126" class="checkbox">
                    <label>fenugreek</label>
                </div>
    
                <div class="boxCont">
                    <input type="checkbox" value="302" class="checkbox">
                    <label>sake</label>
                </div>
    
                <div class="boxCont">
                    <input type="checkbox" value="327" class="checkbox">
                    <label>soy sauce</label>
                </div>
    
                <div class="boxCont">
                    <input type="checkbox" value="112" class="checkbox">
                    <label>cumin</label>
                </div>
            </div>
            <div style="padding-top:10px">
                <button onClick="startInference()">What kind of cuisine can you make?</button>
            </div>
```

----------------------------------------

TITLE: Define Q-Learning Hyperparameters - Python
DESCRIPTION: Sets the hyperparameters for the Q-learning algorithm: `alpha` (learning rate), `gamma` (discount factor), and `epsilon` (exploration/exploitation rate). These values control the learning process and the trade-off between exploring new actions and exploiting known good actions.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/8-Reinforcement/2-Gym/README.md#_snippet_9

LANGUAGE: Python
CODE:
```
# hyperparameters
alpha = 0.3
gamma = 0.9
epsilon = 0.90
```

----------------------------------------

TITLE: Setting Q-Learning Hyperparameters - Python
DESCRIPTION: Defines the key hyperparameters for the Q-Learning algorithm: `alpha` (learning rate, determining the weight of new information), `gamma` (discount factor, prioritizing future rewards), and `epsilon` (exploration rate, controlling the probability of taking a random action vs. exploiting known Q-values).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/translations/README.ko.md#_snippet_9

LANGUAGE: python
CODE:
```
# hyperparameters\nalpha = 0.3\ngamma = 0.9\nepsilon = 0.90
```

----------------------------------------

TITLE: Installing Required R Packages
DESCRIPTION: Checks if the necessary R packages (`tidyverse`, `tidymodels`, `DataExplorer`, `themis`, `here`) are installed using the `pacman` package. If any package is missing, it automatically installs them. Uses `suppressWarnings` to prevent warning messages during installation.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb#_snippet_0

LANGUAGE: R
CODE:
```
suppressWarnings(if (!require("pacman"))install.packages("pacman"))

pacman::p_load(tidyverse, tidymodels, DataExplorer, themis, here)
```

----------------------------------------

TITLE: Printing Observation Space Limits - Python
DESCRIPTION: Accesses and prints the lower and upper bounds defined by the environment for each value within the observation space. This helps understand the expected range of values for the cart position, cart velocity, pole angle, and pole velocity.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/translations/README.ko.md#_snippet_4

LANGUAGE: python
CODE:
```
print(env.observation_space.low)\nprint(env.observation_space.high)
```

----------------------------------------

TITLE: Printing Top Reviewer Nationalities Python
DESCRIPTION: This snippet identifies and prints the most frequent reviewer nationality and the subsequent top 10 nationalities based on the `nationality_freq` Series previously calculated from the DataFrame's `Reviewer_Nationality` column. It demonstrates accessing elements of a Pandas Series by index and using slicing. Requires the `nationality_freq` Pandas Series to be available. Output consists of formatted strings listing the top nationalities and their respective review counts.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/sw/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_2

LANGUAGE: python
CODE:
```
print("The highest frequency reviewer nationality is " + str(nationality_freq.index[0]).strip() + " with " + str(nationality_freq[0]) + " reviews.")
# Notice there is a leading space on the values, strip() removes that for printing
# What is the top 10 most common nationalities and their frequencies?
print("The next 10 highest frequency reviewer nationalities are:")
print(nationality_freq[1:11].to_string())
```

----------------------------------------

TITLE: Get Observation Space Limits - Python
DESCRIPTION: Prints the minimum and maximum possible values for each dimension in the CartPole observation space. This is useful for understanding the range of the continuous state variables.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/solution/notebook.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
print(env.observation_space.low)
print(env.observation_space.high)
```

----------------------------------------

TITLE: Running CartPole Simulation and Printing Observations/Rewards - Python
DESCRIPTION: Runs a simulation of the CartPole environment until the episode terminates (`done` becomes True). In each step, it renders the environment, takes a random action, and receives the next observation (`obs`), reward (`rew`), termination flag (`done`), and auxiliary info (`info`). It prints the observation vector and the reward at each step.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/translations/README.ko.md#_snippet_3

LANGUAGE: python
CODE:
```
env.reset()\n\ndone = False\nwhile not done:\n   env.render()\n   obs, rew, done, info = env.step(env.action_space.sample())\n   print(f"{obs} -> {rew}")\nenv.close()
```

----------------------------------------

TITLE: Running CartPole Simulation and Retrieving Step Outputs Python
DESCRIPTION: Runs a simulation loop until the episode is done. At each step, it takes a random action and retrieves the observation (new state), reward, done flag (indicating episode end), and info dictionary. It prints the observation vector and the reward received for that step. Rendering is optional.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/README.md#_snippet_3

LANGUAGE: python
CODE:
```
env.reset()

done = False
while not done:
   env.render()
   obs, rew, done, info = env.step(env.action_space.sample())
   print(f"{obs} -> {rew}")
env.close()
```

----------------------------------------

TITLE: Viewing First Rows of Tibble in R
DESCRIPTION: Displays the first 5 rows of the imported data tibble. This provides a quick look at the structure and initial values of the dataset columns.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb#_snippet_2

LANGUAGE: R
CODE:
```
# View the first 5 rows of the data set
df %>%
  slice_head(n = 5)
```

----------------------------------------

TITLE: Setting Q-Learning Hyperparameters Python
DESCRIPTION: Sets the key hyperparameters for the Q-Learning algorithm: learning rate (`alpha`), discount factor (`gamma`), and exploration-exploitation factor (`epsilon`). These values control how the agent learns and explores the environment during training.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/8-Reinforcement/2-Gym/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
# hyperparameters
alpha = 0.3
gamma = 0.9
epsilon = 0.90
```

----------------------------------------

TITLE: Plotting Raw Cumulative Rewards Python
DESCRIPTION: Uses the `matplotlib.pyplot` library (assumed imported as `plt`) to generate a line plot showing the cumulative reward collected during each training epoch. This raw plot typically shows high variability due to the stochastic nature of the training process.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/8-Reinforcement/2-Gym/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
plt.plot(rewards)
```

----------------------------------------

TITLE: Defining Forecasting Horizon Constant Python
DESCRIPTION: Sets the `HORIZON` variable, which determines the number of future time steps the model will predict (e.g., 3 hours ahead). This value is crucial for configuring the forecasting task.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/solution/notebook.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
# Specify the number of steps to forecast ahead
HORIZON = 3
print('Forecasting horizon:', HORIZON, 'hours')
```

----------------------------------------

TITLE: Importing Modules for Polynomial Regression Pipeline - Python
DESCRIPTION: This code imports the `PolynomialFeatures` class, used for generating polynomial and interaction features, and the `make_pipeline` function, a convenient way to create a scikit-learn pipeline, both from the scikit-learn library. These are essential for implementing polynomial regression using a pipeline structure.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/tr/2-Regression/3-Linear/README.md#_snippet_12

LANGUAGE: python
CODE:
```
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
```

----------------------------------------

TITLE: Importing Scikit-learn Modules for Polynomial Regression - Python
DESCRIPTION: Imports the `PolynomialFeatures` transformer, which generates polynomial and interaction features, and the `make_pipeline` utility function from the scikit-learn library. These are used together to create a data processing and modeling pipeline for polynomial regression. Requires the scikit-learn library.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ru/2-Regression/3-Linear/README.md#_snippet_12

LANGUAGE: Python
CODE:
```
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
```

----------------------------------------

TITLE: Plot Raw Training Rewards - Python
DESCRIPTION: Plots the raw list of rewards obtained during each training epoch. This plot often appears noisy and does not clearly show the learning progress due to the stochastic nature of individual episodes.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/solution/notebook.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
plt.plot(rewards)
```

----------------------------------------

TITLE: Prepare ONNX Conversion Parameters Python
DESCRIPTION: Imports necessary components from `skl2onnx` for model conversion. It defines `initial_type` to specify the input tensor format as a Float Tensor with shape [None, 380] (batch size, 380 features). It also defines `options` to disable class information (`nocl`) and zipmap output (`zipmap`) for optimization.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/4-Classification/4-Applied/README.md#_snippet_9

LANGUAGE: python
CODE:
```
from skl2onnx import convert_sklearn
from skl2onnx.common.data_types import FloatTensorType

initial_type = [('float_input', FloatTensorType([None, 380]))]
options = {id(model): {'nocl': True, 'zipmap': False}}
```

----------------------------------------

TITLE: Print Observation Space Bounds - Python
DESCRIPTION: Prints the minimum and maximum values for each component of the observation space as defined by the environment. This helps in understanding the expected range of observation values.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/zh/8-Reinforcement/2-Gym/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
print(env.observation_space.low)
print(env.observation_space.high)
```

----------------------------------------

TITLE: Printing Most Positive Sentences Python
DESCRIPTION: Prints a header indicating the number of most positive sentences found. It then iterates through the positive_sentiment_sentences list, printing each sentence prefixed with "+ ". Newlines and multiple spaces within sentences are removed for cleaner output.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/3-Translation-Sentiment/solution/notebook.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
print("The " + str(len(positive_sentiment_sentences)) + " most positive sentences:")
for sentence in positive_sentiment_sentences:
    print("+ " + str(sentence.replace("\n", "").replace("      ", " ")))

```

----------------------------------------

TITLE: Loading Data and Setting Up for NLP - Python
DESCRIPTION: Imports necessary libraries (time, pandas, nltk), downloads the NLTK VADER lexicon, loads a pre-filtered dataset from a CSV file into a pandas DataFrame, and prepares to save the DataFrame to a new CSV file after further processing.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/6-NLP/5-Hotel-Reviews-2/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
import time
import pandas as pd
import nltk as nltk
from nltk.corpus import stopwords
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

# Load the filtered hotel reviews from CSV
df = pd.read_csv('../../data/Hotel_Reviews_Filtered.csv')

# You code will be added here


# Finally remember to save the hotel reviews with new NLP data added
print("Saving results to Hotel_Reviews_NLP.csv")
df.to_csv(r'../data/Hotel_Reviews_NLP.csv', index = False)
```

----------------------------------------

TITLE: Lint and Fix Files - npm/Node.js
DESCRIPTION: This command runs linting tools to check the code for style and syntax errors. It can also automatically fix some of the issues found, ensuring code quality and consistency.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/quiz-app/README.md#_snippet_3

LANGUAGE: Shell
CODE:
```
npm run lint
```

----------------------------------------

TITLE: Defining Flask App HTML Structure and Form
DESCRIPTION: Creates the main HTML template for the web application's user interface. It includes metadata, links the external CSS stylesheet, defines the page title, and structures the content using divs. A form is included to capture user inputs (seconds, latitude, longitude) which are sent to the `/predict` route via a POST request.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/pt/3-Web-App/1-Web-App/README.md#_snippet_4

LANGUAGE: HTML
CODE:
```
<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8">
    <title>🛸 UFO Appearance Prediction! 👽</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}">
  </head>

  <body>
    <div class="grid">

      <div class="box">

        <p>According to the number of seconds, latitude and longitude, which country is likely to have reported seeing a UFO?</p>

        <form action="{{ url_for('predict')}}" method="post">
          <input type="number" name="seconds" placeholder="Seconds" required="required" min="0" max="60" />
          <input type="text" name="latitude" placeholder="Latitude" required="required" />
          <input type="text" name="longitude" placeholder="Longitude" required="required" />
          <button type="submit" class="btn">Predict country where the UFO is seen</button>
        </form>

        <p>{{ prediction_text }}</p>

      </div>

    </div>

  </body>
</html>
```

----------------------------------------

TITLE: Reading Text File Content Python
DESCRIPTION: Opens the specified text file ("pride.txt") with UTF-8 encoding and reads its entire contents into the file_contents variable. It assumes the file exists in the same directory or path. This prepares the text data for processing.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/3-Translation-Sentiment/solution/notebook.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
# You should download the book text, clean it, and import it here
with open("pride.txt", encoding="utf8") as f:
    file_contents = f.read()

```

----------------------------------------

TITLE: Styling Flask App Frontend (CSS)
DESCRIPTION: Defines basic CSS styles for the Flask web application's frontend. It sets general body styles (font, background, text color), styles for input fields, and creates a simple grid layout with styled boxes for content display.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/3-Web-App/1-Web-App/README.md#_snippet_7

LANGUAGE: css
CODE:
```
body {
	width: 100%;
	height: 100%;
	font-family: 'Helvetica';
	background: black;
	color: #fff;
	text-align: center;
	letter-spacing: 1.4px;
	font-size: 30px;
}

input {
	min-width: 150px;
}

.grid {
	width: 300px;
	border: 1px solid #2d2d2d;
	display: grid;
	justify-content: center;
	margin: 20px auto;
}

.box {
	color: #fff;
	background: #2d2d2d;
	padding: 12px;
	display: inline-block;
}
```

----------------------------------------

TITLE: Install Libraries Python
DESCRIPTION: This snippet installs the necessary `skl2onnx` library, which is required to convert scikit-learn models to ONNX format, and imports the `pandas` library for data manipulation.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/4-Classification/4-Applied/README.md#_snippet_0

LANGUAGE: python
CODE:
```
!pip install skl2onnx
import pandas as pd
```

----------------------------------------

TITLE: Printing DataFrame Shape Python
DESCRIPTION: This snippet prints the dimensions (number of rows and columns) of a Pandas DataFrame named `df`. It helps in understanding the size and structure of the dataset being processed. Requires a loaded Pandas DataFrame named `df`. The expected output is a string showing the shape tuple, e.g., `The shape of the data (rows, cols) is (515738, 17)`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/sw/6-NLP/4-Hotel-Reviews-1/README.md#_snippet_0

LANGUAGE: python
CODE:
```
print("The shape of the data (rows, cols) is " + str(df.shape))
```

----------------------------------------

TITLE: Loading Time Series Data using Python
DESCRIPTION: This code snippet loads time series data, specifically the 'load' column, from a CSV file located at './data' into a pandas DataFrame using a custom utility function `load_data`. It then displays the first 10 rows of the loaded DataFrame.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/README.md#_snippet_1

LANGUAGE: python
CODE:
```
energy = load_data('./data')[['load']]
energy.head(10)
```

----------------------------------------

TITLE: Importing Scikit-learn Modules for Linear Regression
DESCRIPTION: This code imports necessary classes from the Scikit-learn library for performing linear regression. It imports `LinearRegression` for the model, `mean_squared_error` for evaluation, and `train_test_split` for splitting data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/2-Regression/3-Linear/README.md#_snippet_5

LANGUAGE: python
CODE:
```
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

----------------------------------------

TITLE: Styling the Web Page (CSS)
DESCRIPTION: Provides basic CSS rules to style the HTML page elements. Includes styles for the body background and font, input field width, and uses a grid class for basic layout and centering content.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ko/3-Web-App/1-Web-App/README.md#_snippet_3

LANGUAGE: css
CODE:
```
body {
	width: 100%;
	height: 100%;
	font-family: 'Helvetica';
	background: black;
	color: #fff;
	text-align: center;
	letter-spacing: 1.4px;
	font-size: 30px;
}

input {
	min-width: 150px;
}

.grid {
	width: 300px;
	border: 1px solid #2d2d2d;
	display: grid;
	justify-content: center;
	margin: 20px auto;
}

.box {
	color: #fff;
	background: #2d2d2d;
	padding: 12px;
	display: inline-block;
}
```

----------------------------------------

TITLE: Installing Gym and Importing Libraries - Python
DESCRIPTION: This snippet shows how to install the OpenAI Gym library using pip via the Python interpreter and then imports the necessary Python libraries: `sys` (for executing pip), `gym` (for the environment), `matplotlib.pyplot` (for plotting, though not used in provided snippets), `numpy` (for numerical operations), and `random` (for exploration).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/2-Gym/translations/README.ko.md#_snippet_0

LANGUAGE: python
CODE:
```
import sys\n!{sys.executable} -m pip install gym\n\nimport gym\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random
```

----------------------------------------

TITLE: Performing K-Means for Multiple k in R
DESCRIPTION: This snippet runs the K-Means algorithm for a range of cluster numbers (k=1 to 10) and collects performance metrics, specifically the Within Cluster Sum of Squares (WCSS). This process is a prerequisite for applying the elbow method to determine the optimal number of clusters. It leverages tidymodels functions for tidy results.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb#_snippet_10

LANGUAGE: R
CODE:
```
# Create a series of clustering models
kclusts <- tibble(k = 1:10) %>% 
  # Perform kmeans clustering for 1,2,3 ... ,10 clusters
  mutate(model = map(k, ~ kmeans(df_numeric_select, centers = .x, nstart = 25)),
  # Farm out clustering metrics eg WCSS
         glanced = map(model, ~ glance(.x))) %>% 
  unnest(cols = glanced)
```

----------------------------------------

TITLE: Importing Matplotlib Pyplot (Python)
DESCRIPTION: Imports the `pyplot` module from the Matplotlib library under the alias `plt`. This module provides a MATLAB-like interface for generating plots. It is a prerequisite for creating visualizations like scatter plots and bar charts using Matplotlib.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/2-Data/README.md#_snippet_7

LANGUAGE: Python
CODE:
```
import matplotlib.pyplot as plt
```

----------------------------------------

TITLE: Importing Libraries for Data Setup and Visualization (Python)
DESCRIPTION: Imports necessary Python libraries, including `os` for interacting with the operating system, `matplotlib.pyplot` for plotting, and a custom `load_data` function from `common.utils`. It also configures matplotlib for inline plotting within the environment.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/1-Introduction/solution/notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import os
import matplotlib.pyplot as plt
from common.utils import load_data
%matplotlib inline
```

----------------------------------------

TITLE: Installing Required Packages R
DESCRIPTION: Checks if necessary R packages for K-Means clustering (`tidyverse`, `tidymodels`, etc.) are installed and loads them. Uses the `pacman` package to simplify package management. Ensures all prerequisites are available before executing clustering code.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb#_snippet_0

LANGUAGE: R
CODE:
```
suppressWarnings(if(!require("pacman")) install.packages("pacman"))

pacman::p_load('tidyverse', 'tidymodels', 'cluster', 'summarytools', 'plotly', 'paletteer', 'factoextra', 'patchwork')
```

----------------------------------------

TITLE: Displaying First Rows of Data in R
DESCRIPTION: This snippet demonstrates how to display the first few rows of a data frame using the `slice_head` function from the `dplyr` package (often used within `tidymodels`). It helps quickly inspect the structure and content of the data.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb#_snippet_6

LANGUAGE: R
CODE:
```
# Display first 5 rows
new_pumpkins %>%
  slice_head(n = 5)
```

----------------------------------------

TITLE: Installing TextBlob and Corpora - Bash
DESCRIPTION: This bash command snippet provides instructions for installing the TextBlob Python library and downloading its essential linguistic corpora using pip and a Python module command. This is a required prerequisite step before attempting the coding exercises in the lesson that utilize TextBlob.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/1-Introduction-to-NLP/README.md#_snippet_0

LANGUAGE: Bash
CODE:
```
pip install -U textblob
python -m textblob.download_corpora
```

----------------------------------------

TITLE: Adding Ingredient Checkboxes and Button HTML
DESCRIPTION: This HTML code populates the body of the page with a heading and multiple `div` containers holding checkboxes and labels for various ingredients. Each checkbox is assigned a `value` that represents the ingredient's index, crucial for mapping user selections to the model's input vector. A button is included to initiate the recommendation process via the `startInference()` JavaScript function.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/4-Classification/4-Applied/README.md#_snippet_1

LANGUAGE: html
CODE:
```
<h1>Check your refrigerator. What can you create?</h1>
            <div id="wrapper">
                <div class="boxCont">
                    <input type="checkbox" value="4" class="checkbox">
                    <label>apple</label>
                </div>
            
                <div class="boxCont">
                    <input type="checkbox" value="247" class="checkbox">
                    <label>pear</label>
                </div>
            
                <div class="boxCont">
                    <input type="checkbox" value="77" class="checkbox">
                    <label>cherry</label>
                </div>
    
                <div class="boxCont">
                    <input type="checkbox" value="126" class="checkbox">
                    <label>fenugreek</label>
                </div>
    
                <div class="boxCont">
                    <input type="checkbox" value="302" class="checkbox">
                    <label>sake</label>
                </div>
    
                <div class="boxCont">
                    <input type="checkbox" value="327" class="checkbox">
                    <label>soy sauce</label>
                </div>
    
                <div class="boxCont">
                    <input type="checkbox" value="112" class="checkbox">
                    <label>cumin</label>
                </div>
            </div>
            <div style="padding-top:10px">
                <button onClick="startInference()">What kind of cuisine can you make?</button>
            </div>
```

----------------------------------------

TITLE: Suppressing Seaborn User Warnings - Python
DESCRIPTION: This snippet imports the `warnings` module and configures it to ignore `UserWarning` messages originating from the `seaborn` module. This is done specifically to avoid clutter in the output when plotting many data points, although generally not recommended.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/4-Logistic/solution/notebook.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='seaborn')
```

----------------------------------------

TITLE: Installing seaborn Library Python
DESCRIPTION: Installs the seaborn library using pip. Seaborn is a Python data visualization library based on matplotlib, providing a high-level interface for drawing attractive statistical graphics.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/notebook.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
!pip install seaborn
```

----------------------------------------

TITLE: Importing Custom Board Module Python
DESCRIPTION: Imports the custom `rlboard` module, which encapsulates the implementation details of the `Board` class representing the environment. This class likely handles board creation, state, and movement logic for the pathfinding simulation.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/solution/notebook.ipynb#_snippet_1

LANGUAGE: Python
CODE:
```
from rlboard import *
```

----------------------------------------

TITLE: Installing Statsmodels Library Python
DESCRIPTION: This snippet provides the command to install the `statsmodels` library using pip. `statsmodels` is a required dependency for implementing the ARIMA model used in this notebook for time series forecasting.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/working/notebook.ipynb#_snippet_0

LANGUAGE: Python
CODE:
```
pip install statsmodels
```

----------------------------------------

TITLE: Printing Stop Words Removal Duration Python
DESCRIPTION: Calculates the time elapsed since the start variable was set before stop words removal. It then prints the total time taken for the stop word removal process, rounded to two decimal places.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/3-notebook.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
end = time.time()
print("Removing stop words took " + str(round(end - start, 2)) + " seconds")

```

----------------------------------------

TITLE: Importing Matplotlib for Visualization in Python
DESCRIPTION: Imports the `matplotlib.pyplot` module, which is a foundational library for creating static, interactive, and animated visualizations in Python. It is commonly imported with the alias `plt`.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/2-Regression/2-Data/README.md#_snippet_7

LANGUAGE: Python
CODE:
```
import matplotlib.pyplot as plt
```

----------------------------------------

TITLE: Defining Input/Output Variables and Printing Shapes (Placeholder)
DESCRIPTION: Placeholder variables intended to hold the split features (x) and targets (y) for both training and testing sets after time-step transformation. Includes print statements to show the expected shapes once populated.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_13

LANGUAGE: python
CODE:
```
x_train, y_train = None
x_test, y_test = None

print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)
```

----------------------------------------

TITLE: Start Development Server - npm/Node.js
DESCRIPTION: This command compiles the project and starts a local development server. It includes hot-reloading, meaning changes to the code will automatically refresh the browser.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/quiz-app/README.md#_snippet_1

LANGUAGE: Shell
CODE:
```
npm run serve
```

----------------------------------------

TITLE: Installing Data Visualization Library - Seaborn - Python
DESCRIPTION: Installs the required Python package 'seaborn' using the pip package installer. This is a prerequisite step to ensure the seaborn library is available for use in subsequent code snippets for plotting.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/2-K-Means/solution/tester.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
pip install seaborn
```

----------------------------------------

TITLE: Installing skl2onnx Library (Python)
DESCRIPTION: Installs the `skl2onnx` Python package using pip. This library is required for converting scikit-learn models into the ONNX (Open Neural Network Exchange) format, enabling cross-platform compatibility.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/solution/notebook.ipynb#_snippet_0

LANGUAGE: Python
CODE:
```
!pip install skl2onnx
```

----------------------------------------

TITLE: Installing Data Visualization Library - Python
DESCRIPTION: This snippet uses the pip package manager within a Python environment (like a Jupyter notebook) to install the seaborn library, which is required for advanced data visualization functions used in the subsequent steps. This is a prerequisite for running code that depends on seaborn.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/README.md#_snippet_0

LANGUAGE: python
CODE:
```
!pip install seaborn
```

----------------------------------------

TITLE: Install Required R Packages for Clustering
DESCRIPTION: This R snippet checks for and installs necessary packages for the clustering exercise. It first ensures the 'pacman' package is available to simplify loading/installing multiple packages, then uses 'pacman'::p_load to install and attach a list of packages including 'tidyverse', 'tidymodels', and various visualization/utility libraries. This sets up the R environment with all required dependencies.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb#_snippet_0

LANGUAGE: R
CODE:
```
suppressWarnings(if(!require("pacman")) install.packages("pacman"))

pacman::p_load('tidyverse', 'tidymodels', 'DataExplorer', 'summarytools', 'plotly', 'paletteer', 'corrplot', 'patchwork')
```

----------------------------------------

TITLE: Changing Directory to Web App (Bash)
DESCRIPTION: Navigates the terminal into the newly created `web-app` directory. This is a prerequisite step before installing dependencies or running the application from within that directory.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/3-Web-App/1-Web-App/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
cd web-app
```

----------------------------------------

TITLE: Defining Actions and Indices - Python
DESCRIPTION: Defines the set of possible actions the agent can take within the environment: Up (U), Down (D), Left (L), and Right (R). It maps each action string to a tuple representing the corresponding change in (x, y) coordinates on the board. It also creates a dictionary to map action names to numerical indices, which is useful for accessing values in data structures like the Q-Table.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/8-Reinforcement/1-QLearning/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

----------------------------------------

TITLE: Setting Forecasting Horizon in Python
DESCRIPTION: This snippet defines the number of future steps (hours) for which the time series model will generate predictions. This value is crucial for subsequent forecasting and evaluation steps.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/hi/7-TimeSeries/2-ARIMA/README.md#_snippet_0

LANGUAGE: python
CODE:
```
HORIZON = 3
print('Forecasting horizon:', HORIZON, 'hours')
```

----------------------------------------

TITLE: Importing Sys Module and Appending Path (Python)
DESCRIPTION: Imports the standard Python 'sys' module and appends a parent directory to the system's path. This is often necessary to allow the script to import custom modules or utilities located in directories outside the current one, such as a shared 'common' library.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/README.md#_snippet_0

LANGUAGE: python
CODE:
```
import sys
sys.path.append('../../')
```

----------------------------------------

TITLE: Defining Timesteps Variable (Placeholder)
DESCRIPTION: A placeholder variable intended to hold the integer value representing the number of look-back timesteps for creating lagged features. The value is currently set to None.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
# Selecting the timesteps

timesteps=None
```

----------------------------------------

TITLE: Importing Scikit-learn Modules for Polynomial Regression Pipeline in Python
DESCRIPTION: This code imports the necessary components from Scikit-learn to build a polynomial regression model within a pipeline: PolynomialFeatures for transforming data to include polynomial terms and make_pipeline to sequentially apply multiple processing steps.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/2-Regression/3-Linear/README.md#_snippet_13

LANGUAGE: Python
CODE:
```
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
```

----------------------------------------

TITLE: Cloning the Curriculum Repository - Shell
DESCRIPTION: This command clones the Machine Learning for Beginners GitHub repository to your local machine. It is the second step in getting started with the curriculum after forking the repository. Requires Git to be installed on your system.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
git clone https://github.com/microsoft/ML-For-Beginners.git
```

----------------------------------------

TITLE: Define Forecasting Horizon in Python
DESCRIPTION: This snippet defines the number of future steps (horizon) for which the time series model will make predictions. The variable `HORIZON` is set to 3, indicating a 3-hour forecast horizon, and this value is then printed to the console.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/2-ARIMA/README.md#_snippet_9

LANGUAGE: python
CODE:
```
# Specify the number of steps to forecast ahead
HORIZON = 3
print('Forecasting horizon:', HORIZON, 'hours')
```

----------------------------------------

TITLE: Defining SVR Model Variable (Placeholder)
DESCRIPTION: A placeholder variable intended to hold the initialized Support Vector Regressor model instance, typically configured with an RBF kernel for this time series task. The value is currently set to None.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_14

LANGUAGE: python
CODE:
```
# Create model using RBF kernel

model = None
```

----------------------------------------

TITLE: Setting Up Basic HTML Structure - HTML
DESCRIPTION: Defines the basic HTML document structure including DOCTYPE, html, header, and body tags. It sets the page title to 'Cuisine Matcher'. This is the initial boilerplate for the web application's main page.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/4-Classification/4-Applied/README.md#_snippet_11

LANGUAGE: HTML
CODE:
```
<!DOCTYPE html>
<html>
    <header>
        <title>Cuisine Matcher</title>
    </header>
    <body>
        ...
    </body>
</html>
```

----------------------------------------

TITLE: Defining Training Timesteps Data Variable (Placeholder)
DESCRIPTION: A placeholder variable intended to hold the reshaped training data as a 2D tensor with timesteps as a dimension, suitable for the SVR model input. The value is currently set to None.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
# Converting data to 2D tensor

train_data_timesteps=None
```

----------------------------------------

TITLE: Defining Timestamps Variables for Plotting (Placeholder)
DESCRIPTION: Placeholder variables intended to hold the timestamps corresponding to the training and testing data points used for plotting predictions against actuals. The values are currently set to None.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_18

LANGUAGE: python
CODE:
```
# Extract the timesteps for x-axis

train_timestamps = None
test_timestamps = None
```

----------------------------------------

TITLE: Ingredient Checkbox UI HTML
DESCRIPTION: Adds HTML markup within the `body` tag to create a simple user interface. It includes a main heading, a container div with checkboxes representing ingredients (each with a `value` corresponding to its index), and a button to trigger the inference process via an `onClick` event calling `startInference()`. The checkboxes are styled using the `boxCont` and `checkbox` classes (CSS not shown).
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/ja/4-Classification/4-Applied/README.md#_snippet_12

LANGUAGE: html
CODE:
```
<h1>Check your refrigerator. What can you create?</h1>
            <div id="wrapper">
                <div class="boxCont">
                    <input type="checkbox" value="4" class="checkbox">
                    <label>apple</label>
                </div>
            
                <div class="boxCont">
                    <input type="checkbox" value="247" class="checkbox">
                    <label>pear</label>
                </div>
            
                <div class="boxCont">
                    <input type="checkbox" value="77" class="checkbox">
                    <label>cherry</label>
                </div>
    
                <div class="boxCont">
                    <input type="checkbox" value="126" class="checkbox">
                    <label>fenugreek</label>
                </div>
    
                <div class="boxCont">
                    <input type="checkbox" value="302" class="checkbox">
                    <label>sake</label>
                </div>
    
                <div class="boxCont">
                    <input type="checkbox" value="327" class="checkbox">
                    <label>soy sauce</label>
                </div>
    
                <div class="boxCont">
                    <input type="checkbox" value="112" class="checkbox">
                    <label>cumin</label>
                </div>
            </div>
            <div style="padding-top:10px">
                <button onClick="startInference()">What kind of cuisine can you make?</button>
            </div>
```

----------------------------------------

TITLE: Initializing HTML Document Structure
DESCRIPTION: Provides the basic boilerplate for an HTML5 document, including the doctype, html, header, and body tags. It sets the title of the page for the web application.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/sw/4-Classification/4-Applied/README.md#_snippet_0

LANGUAGE: html
CODE:
```
<!DOCTYPE html>
<html>
    <header>
        <title>Cuisine Matcher</title>
    </header>
    <body>
        ...
    </body>
</html>
```

----------------------------------------

TITLE: Build for Production - npm/Node.js
DESCRIPTION: This command compiles and minifies the project's code for production deployment. The output is typically placed in a `dist` or `build` directory, ready to be served by a web server.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/quiz-app/README.md#_snippet_2

LANGUAGE: Shell
CODE:
```
npm run build
```

----------------------------------------

TITLE: Setting up Basic HTML Structure
DESCRIPTION: This snippet initializes the HTML document with essential tags like `<!DOCTYPE html>`, `<html>`, `<header>`, `<title>`, and `<body>`, providing the fundamental layout for the web page. It sets the title of the page to "Cuisine Matcher".
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/es/4-Classification/4-Applied/README.md#_snippet_0

LANGUAGE: html
CODE:
```
<!DOCTYPE html>
<html>
    <header>
        <title>Cuisine Matcher</title>
    </header>
    <body>
        ...
    </body>
</html>
```

----------------------------------------

TITLE: Adding Ingredient Checkboxes and Button HTML
DESCRIPTION: Adds HTML markup within the `<body>` tag for a heading, a container holding a list of ingredient checkboxes, and a button. Each checkbox has a `value` attribute corresponding to the ingredient's index in the training dataset and a class `checkbox` for easy selection in JavaScript. The button triggers the `startInference` JavaScript function when clicked.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/it/4-Classification/4-Applied/README.md#_snippet_1

LANGUAGE: html
CODE:
```
<h1>Check your refrigerator. What can you create?</h1>
            <div id="wrapper">
                <div class="boxCont">
                    <input type="checkbox" value="4" class="checkbox">
                    <label>apple</label>
                </div>
            
                <div class="boxCont">
                    <input type="checkbox" value="247" class="checkbox">
                    <label>pear</label>
                </div>
            
                <div class="boxCont">
                    <input type="checkbox" value="77" class="checkbox">
                    <label>cherry</label>
                </div>
    
                <div class="boxCont">
                    <input type="checkbox" value="126" class="checkbox">
                    <label>fenugreek</label>
                </div>
    
                <div class="boxCont">
                    <input type="checkbox" value="302" class="checkbox">
                    <label>sake</label>
                </div>
    
                <div class="boxCont">
                    <input type="checkbox" value="327" class="checkbox">
                    <label>soy sauce</label>
                </div>
    
                <div class="boxCont">
                    <input type="checkbox" value="112" class="checkbox">
                    <label>cumin</label>
                </div>
            </div>
            <div style="padding-top:10px">
                <button onClick="startInference()">What kind of cuisine can you make?</button>
            </div> 
```

----------------------------------------

TITLE: Defining Python Dependencies (text)
DESCRIPTION: Specifies the required Python packages for the Flask web application. These libraries include scikit-learn for the ML model, pandas and numpy for data handling, and Flask for the web framework. This file is used by `pip` to install dependencies.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/translations/mo/3-Web-App/1-Web-App/README.md#_snippet_0

LANGUAGE: text
CODE:
```
scikit-learn
pandas
numpy
flask
```

----------------------------------------

TITLE: Defining Testing Timesteps Data Variable (Placeholder)
DESCRIPTION: A placeholder variable intended to hold the reshaped testing data as a 2D tensor with timesteps, formatted for the SVR model input. The value is currently set to None.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
# Converting test data to 2D tensor

test_data_timesteps=None
```

----------------------------------------

TITLE: Preparing Full Dataset for Prediction (Placeholder)
DESCRIPTION: Placeholder variables intended to hold the full dataset converted to a numpy array, scaled, and reshaped into a 2D tensor with timesteps for prediction by the trained SVR model. Variables for inputs (X) and outputs (Y) are also defined.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_23

LANGUAGE: python
CODE:
```
# Extracting load values as numpy array
data = None

# Scaling
data = None

# Transforming to 2D tensor as per model input requirement
data_timesteps=None

# Selecting inputs and outputs from data
X, Y = None, None
```

----------------------------------------

TITLE: Defining Prediction Variables (Placeholder)
DESCRIPTION: Placeholder variables intended to store the predictions generated by the SVR model on both the training and testing datasets. The values are currently set to None.
SOURCE: https://github.com/microsoft/ml-for-beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb#_snippet_15

LANGUAGE: python
CODE:
```
# Making predictions

y_train_pred = None
y_test_pred = None
```