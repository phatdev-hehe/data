TITLE: Installing Transformers Library
DESCRIPTION: Installs the Transformers library using pip package manager.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/zero_shot_image_classification.md#2025-04-23_snippet_0

LANGUAGE: bash
CODE:
```
pip install -q transformers
```

----------------------------------------

TITLE: Installing Transformers, Datasets, Evaluate, and Accelerate with pip
DESCRIPTION: This code snippet shows how to install the necessary libraries (transformers, datasets, evaluate, accelerate) using pip, a package installer for Python.  This step is essential for using the Hugging Face Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_0

LANGUAGE: bash
CODE:
```
!pip install transformers datasets evaluate accelerate
```

----------------------------------------

TITLE: Using Pipeline API for Sentiment Analysis in Python
DESCRIPTION: Demonstrates how to use the Transformers pipeline API to perform sentiment analysis on text. Shows downloading a pre-trained model and running inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
>>> from transformers import pipeline

# ä½¿ç”¨æƒ…ç»ªåˆ†æžæµæ°´çº¿
>>> classifier = pipeline('sentiment-analysis')
>>> classifier('We are very happy to introduce pipeline to the transformers repository.')
[{'label': 'POSITIVE', 'score': 0.9996980428695679}]
```

----------------------------------------

TITLE: Loading a Pretrained Tokenizer using AutoTokenizer in Python
DESCRIPTION: This snippet demonstrates how to load a pretrained tokenizer from Hugging Face using the AutoTokenizer class. It shows how to tokenize a string of text and receive input IDs and an attention mask as output tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/fast_tokenizers.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-2b")
tokenizer("We are very happy to show you the ðŸ¤— Transformers library", return_tensors="pt")
{'input_ids': tensor([[     2,   1734,    708,   1508,   4915,    577,   1500,    692,    573,
         156808, 128149,   9581, 235265]]), 
 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}
```

----------------------------------------

TITLE: Loading Pretrained Model with AutoModel in Python
DESCRIPTION: This code snippet demonstrates how to load a pretrained BERT model using the `AutoModel.from_pretrained()` method from the Transformers library.  It infers the model architecture from the provided model name and returns an instance of the corresponding model class, which in this case is `BertModel`. No dependencies are explicitly shown but `transformers` library needs to be installed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/auto.md#_snippet_0

LANGUAGE: python
CODE:
```
model = AutoModel.from_pretrained("google-bert/bert-base-cased")
```

----------------------------------------

TITLE: Tokenizing Text with AutoTokenizer in Python
DESCRIPTION: Shows how to use the loaded tokenizer to encode text. The tokenizer returns a dictionary with input IDs, token type IDs, and attention mask.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/quicktour.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> encoding = tokenizer("We are very happy to show you the ðŸ¤— Transformers library.")
>>> print(encoding)
{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

----------------------------------------

TITLE: Tokenizing Batches of Sentences with Padding and Truncation - PyTorch
DESCRIPTION: This snippet demonstrates how to tokenize a batch of sentences using a tokenizer, with padding and truncation enabled. The `return_tensors` parameter is set to 'pt', which instructs the tokenizer to return PyTorch tensors as output. The resulting encoded input contains `input_ids`, `token_type_ids`, and `attention_mask` tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/preprocessing.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```

>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="pt")
>>> print(encoded_input)
{'input_ids': tensor([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
                      [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
                      [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]]),
 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),
 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
                           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                           [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}

```

----------------------------------------

TITLE: Training ViT Model on Beans Dataset using Trainer API
DESCRIPTION: Command for fine-tuning a Vision Transformer (ViT) model on the beans dataset using HuggingFace Trainer API. Includes configuration for training parameters, logging, evaluation, and model saving strategies.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/image-classification/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python run_image_classification.py \
    --dataset_name beans \
    --output_dir ./beans_outputs/ \
    --remove_unused_columns False \
    --label_column_name labels \
    --do_train \
    --do_eval \
    --push_to_hub \
    --push_to_hub_model_id vit-base-beans \
    --learning_rate 2e-5 \
    --num_train_epochs 5 \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --logging_strategy steps \
    --logging_steps 10 \
    --eval_strategy epoch \
    --save_strategy epoch \
    --load_best_model_at_end True \
    --save_total_limit 3 \
    --seed 1337
```

----------------------------------------

TITLE: Installing Required Dependencies
DESCRIPTION: Install the necessary Python packages for working with transformers, datasets and evaluation
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/masked_language_modeling.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers datasets evaluate
```

----------------------------------------

TITLE: Loading a Pre-trained Tokenizer using AutoTokenizer Python
DESCRIPTION: This code snippet demonstrates how to load a pre-trained tokenizer using the `AutoTokenizer.from_pretrained` method from the `transformers` library. It initializes the tokenizer with the specified checkpoint, in this case, "google-bert/bert-base-uncased".
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/autoclass_tutorial.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
```

----------------------------------------

TITLE: Installing Transformers with PyTorch
DESCRIPTION: Command to install both Hugging Face Transformers and PyTorch in a single line using pip.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/installation.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
pip install 'transformers[torch]'
```

----------------------------------------

TITLE: Using Text Generation Pipeline for Causal Language Modeling in Python
DESCRIPTION: This code demonstrates how to use the Hugging Face pipeline for causal language modeling (text generation). It initializes a text generation pipeline and processes a prompt to generate text continuation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/task_summary.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> prompt = "Hugging Face is a community-based open-source platform for machine learning."
>>> generator = pipeline(task="text-generation")
>>> generator(prompt)  # doctest: +SKIP
```

----------------------------------------

TITLE: Initialize Gemma 2 Text Generation Pipeline (Python)
DESCRIPTION: Initializes a text generation pipeline using the `google/gemma-2-9b` model with specific dtype and device settings. This demonstrates a high-level approach to using the model for generating text with a set maximum number of new tokens.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gemma2.md#_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipe = pipeline(
    task="text-generation",
    model="google/gemma-2-9b",
    torch_dtype=torch.bfloat16,
    device="cuda",
)

pipe("Explain quantum computing simply. ", max_new_tokens=50)
```

----------------------------------------

TITLE: Instantiating Transformers Classes from Pretrained Models
DESCRIPTION: Demonstrates the use of the from_pretrained() method to load pretrained models, configurations, and preprocessing classes in Transformers. This method provides a unified way to initialize objects from pretrained checkpoints.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/philosophy.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from_pretrained()
```

----------------------------------------

TITLE: Creating a Pipeline for Automatic Speech Recognition in Python
DESCRIPTION: Demonstrates how to create a pipeline for automatic speech recognition (ASR) using the default model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/pipeline_tutorial.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition")
```

----------------------------------------

TITLE: Install Transformers and Bitsandbytes
DESCRIPTION: This command installs the necessary libraries: `transformers` for using pre-trained models and `bitsandbytes` for quantization to reduce memory usage.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/llm_tutorial.md#_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers bitsandbytes>=0.39.0 -q
```

----------------------------------------

TITLE: Load Model from Hub - Python
DESCRIPTION: This snippet shows how to load a model from the Hugging Face Model Hub using the `AutoModel.from_pretrained` method, specifying the repository ID in the format `username/model_name`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/model_sharing.md#_snippet_12

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModel

>>> model = AutoModel.from_pretrained("your_username/my-awesome-model")
```

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModel

>>> model = AutoModel.from_pretrained("your_username/my-awesome-model")
```

----------------------------------------

TITLE: Initializing and Training with Trainer
DESCRIPTION: This code snippet shows how to initialize the `Trainer` class from the Transformers library and start the training process. It takes the model, training arguments, training and evaluation datasets, tokenizer, data collator, and a function for computing metrics as input. The `trainer.train()` method then begins the training loop.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/trainer.md#_snippet_2

LANGUAGE: python
CODE:
```
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()
```

----------------------------------------

TITLE: Loading Pre-trained Tokenizer with AutoTokenizer in Python
DESCRIPTION: This code snippet demonstrates how to load a pre-trained tokenizer using `AutoTokenizer.from_pretrained` from the `transformers` library. It initializes a tokenizer with the 'FacebookAI/xlm-roberta-base' checkpoint. The tokenizer is used to convert text into a numerical representation suitable for transformer models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/autoclass_tutorial.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("FacebookAI/xlm-roberta-base")
```

----------------------------------------

TITLE: Installing PyTorch using Bash
DESCRIPTION: Installation command to add the PyTorch library to the Python environment. PyTorch is a prerequisite ML framework for using Transformers with PyTorch models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md#2025-04-22_snippet_1

LANGUAGE: Bash
CODE:
```
!pip install torch
```

----------------------------------------

TITLE: Installing Transformers Library Using Pip in Bash
DESCRIPTION: This snippet shows how to install the Transformers library using pip, a standard package management system for Python, within a virtual environment.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
pip install transformers
```

----------------------------------------

TITLE: Basic Sentiment Analysis Pipeline
DESCRIPTION: Example showing how to create and use a basic sentiment analysis pipeline using pre-trained models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quicktour.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline("sentiment-analysis")
>>> classifier("We are very happy to show you the ðŸ¤— Transformers library.")
[{'label': 'POSITIVE', 'score': 0.9998}]
```

----------------------------------------

TITLE: Importando e utilizando pipeline para anÃ¡lise de sentimento em Python
DESCRIPTION: Demonstra como importar e usar a funÃ§Ã£o pipeline para realizar anÃ¡lise de sentimento em texto.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/quicktour.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline("sentiment-analysis")
>>> classifier("We are very happy to show you the ðŸ¤— Transformers library.")
[{'label': 'POSITIVE', 'score': 0.9998}]
```

----------------------------------------

TITLE: Installing Transformers Library Bash
DESCRIPTION: Installs the `transformers` library using pip. This is a prerequisite for using the mask generation pipeline and other related functionalities.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/mask_generation.md#_snippet_0

LANGUAGE: bash
CODE:
```
pip install -q transformers
```

----------------------------------------

TITLE: Installing Transformers with pip
DESCRIPTION: Command to install the Transformers library using pip package manager.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/installation.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
pip install transformers
```

----------------------------------------

TITLE: Loading a Pre-trained Model for Sequence Classification
DESCRIPTION: This code loads a pre-trained AutoModelForSequenceClassification model from Hugging Face Transformers. It specifies the number of labels for the classification task, initializing the classification head accordingly.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_20

LANGUAGE: Python
CODE:
```
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=5)
```

----------------------------------------

TITLE: Using PyTorch Scaled Dot Product Attention with FlashAttention
DESCRIPTION: Shows how to explicitly enable FlashAttention using PyTorch's scaled dot product attention context manager. This optimizes attention computation for CUDA backends.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/llm_optims.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2b",
    torch_dtype=torch.bfloat16,
)

with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    outputs = model.generate(**inputs)
```

----------------------------------------

TITLE: Performing Sentiment Analysis
DESCRIPTION: Performs sentiment analysis on a given text using the initialized pipeline. The pipeline returns a list of dictionaries containing the predicted label and score.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/quicktour.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
">>> classifier("We are very happy to show you the ðŸ¤— Transformers library.")\n[{'label': 'POSITIVE', 'score': 0.9998}]"
```

----------------------------------------

TITLE: Incorrect Prompt Demonstration - PyTorch
DESCRIPTION: This code snippet illustrates the importance of proper prompt formatting when interacting with LLMs. It shows how an improperly formatted prompt can lead to unexpected output from a chat model. The code then demonstrates the correct usage of `tokenizer.apply_chat_template` for formatting prompts, resulting in the desired model behavior. It depends on `AutoTokenizer` and `AutoModelForCausalLM`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/llm_tutorial.md#_snippet_9

LANGUAGE: Python
CODE:
```
>>> tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-alpha")
>>> model = AutoModelForCausalLM.from_pretrained(
...     "HuggingFaceH4/zephyr-7b-alpha", device_map="auto", load_in_4bit=True
... )
>>> set_seed(0)
>>> prompt = """How many helicopters can a human eat in one sitting? Reply as a thug."""
>>> model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
>>> input_length = model_inputs.input_ids.shape[1]
>>> generated_ids = model.generate(**model_inputs, max_new_tokens=20)
>>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])
"I'm not a thug, but i can tell you that a human cannot eat"
>>> # Oh no, it did not follow our instruction to reply as a thug! Let's see what happens when we write
>>> # a better prompt and use the right template for this model (through `tokenizer.apply_chat_template`)

>>> set_seed(0)
>>> messages = [
...     {
...         "role": "system",
...         "content": "You are a friendly chatbot who always responds in the style of a thug",
...     },
...     {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
... ]
>>> model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to("cuda")
>>> input_length = model_inputs.shape[1]
>>> generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20)
>>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])
'None, you thug. How bout you try to focus on more useful questions?'
>>> # As we can see, it followed a proper thug style ðŸ˜Ž
```

----------------------------------------

TITLE: Loading a Tokenizer with AutoTokenizer
DESCRIPTION: This snippet shows how to load a tokenizer using AutoTokenizer with a pre-trained model name.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/quicktour.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

----------------------------------------

TITLE: Loading Pre-trained Tokenizer with AutoTokenizer in Python
DESCRIPTION: This snippet demonstrates how to load a pre-trained tokenizer using the AutoTokenizer class from the Transformers library. The tokenization process converts user input into a format that can be processed by machine learning models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/autoclass_tutorial.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
```

LANGUAGE: python
CODE:
```
>>> sequence = "In a hole in the ground there lived a hobbit."
>>> print(tokenizer(sequence))
{'input_ids': [101, 1999, 1037, 4920, 1999, 1996, 2598, 2045, 2973, 1037, 7570, 10322, 4183, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

----------------------------------------

TITLE: Mapping Tokenization Preprocessing Function to Dataset
DESCRIPTION: This code applies the `preprocess_function` to the ELI5 dataset using the `map` method.  It sets `batched=True` to process examples in batches and `num_proc=4` to use 4 processes for parallel processing. It also removes the original columns from the dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_7

LANGUAGE: python
CODE:
```
>>> tokenized_eli5 = eli5.map(
...     preprocess_function,
...     batched=True,
...     num_proc=4,
...     remove_columns=eli5["train"].column_names,
... )
```

----------------------------------------

TITLE: Tokenizing with Truncation and Padding
DESCRIPTION: Shows how to tokenize sentences with both truncation and padding enabled to handle variable-length inputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/preprocessing.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)
print(encoded_input)
```

----------------------------------------

TITLE: Loading a Pre-trained Model with AutoModel in Transformers
DESCRIPTION: Example showing how to load a pre-trained BERT model using the AutoModel class from the Transformers library. This represents the basic approach for loading models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/big_models.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoModel

model = AutoModel.from_pretrained("google-bert/bert-base-cased")
```

----------------------------------------

TITLE: Sentiment Analysis Pipeline
DESCRIPTION: Creates a sentiment analysis pipeline by specifying the model and tokenizer.  Then, it applies the pipeline to a French text and prints the sentiment analysis result.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_13

LANGUAGE: Python
CODE:
```
>>> classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
>>> classifier("Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ðŸ¤— Transformers.")
[{'label': '5 stars', 'score': 0.7273}]
```

----------------------------------------

TITLE: Installing Transformers with PyTorch support
DESCRIPTION: This command installs the `transformers` library along with the necessary dependencies for PyTorch integration. This includes libraries needed to work with PyTorch models in Transformers.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/installation.md#_snippet_4

LANGUAGE: bash
CODE:
```
pip install 'transformers[torch]'
```

----------------------------------------

TITLE: Cargando modelo y tokenizer con AutoClasses en PyTorch
DESCRIPTION: Utiliza las clases Auto de Transformers para cargar automÃ¡ticamente un modelo preentrenado y su tokenizador asociado en PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/quicktour.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

----------------------------------------

TITLE: Installing Transformers Library for Zero-shot Image Classification
DESCRIPTION: Command to install the required Transformers library via pip for implementing zero-shot image classification.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/zero_shot_image_classification.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install -q transformers
```

----------------------------------------

TITLE: Loading and generating with Quantized Llama (Python)
DESCRIPTION: This code demonstrates loading a Llama model with int4 weight-only quantization using the `torchao` backend. It specifies a `TorchAoConfig` with "int4_weight_only" and `group_size=128`. The model is loaded using `AutoModelForCausalLM` with this configuration, `torch.bfloat16` dtype, and `device_map="auto"`. Text generation is performed similarly to the `AutoModel` example using the quantized model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llama.md#_snippet_3

LANGUAGE: python
CODE:
```
# pip install torchao
import torch
from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer

quantization_config = TorchAoConfig("int4_weight_only", group_size=128)
model = AutoModelForCausalLM.from_pretrained(
    "huggyllama/llama-30b",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained("huggyllama/llama-30b")
input_ids = tokenizer("Plants create energy through a process known as", return_tensors="pt").to("cuda")

output = model.generate(**input_ids, cache_implementation="static")
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Training Setup with PyTorch Trainer
DESCRIPTION: Shows complete training setup using the Trainer API including model, arguments, tokenizer and data preparation
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/quicktour.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSequenceClassification, TrainingArguments, AutoTokenizer, DataCollatorWithPadding, Trainer
from datasets import load_dataset

model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")

training_args = TrainingArguments(
    output_dir="path/to/save/folder/",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=2,
)

tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
dataset = load_dataset("rotten_tomatoes")

def tokenize_dataset(dataset):
    return tokenizer(dataset["text"])

dataset = dataset.map(tokenize_dataset, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    processing_class=tokenizer,
    data_collator=data_collator,
)
```

----------------------------------------

TITLE: Loading Pretrained Model with AutoModel (PyTorch) - Python
DESCRIPTION: This snippet shows how to load a pretrained model using both the AutoModelForCausalLM class and a model-specific class (MistralForCausalLM) in PyTorch. It includes loading the model weights with parameters for device handling.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/models.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
from transformers import AutoModelForCausalLM, MistralForCausalLM

# load with AutoClass or model-specific class
model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1", , torch_dtype="auto", device_map="auto")
model = MistralForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1", , torch_dtype="auto", device_map="auto")
```

----------------------------------------

TITLE: Initializing Auto Model from Pretrained Weights
DESCRIPTION: Demonstrates how to use AutoModel to automatically instantiate the correct model architecture based on a pretrained model name.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/auto.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
model = AutoModel.from_pretrained("google-bert/bert-base-cased")
```

----------------------------------------

TITLE: Generating Text with AutoModel and AutoTokenizer (PyTorch)
DESCRIPTION: Shows text generation using the lower-level AutoTokenizer and AutoModelForCausalLM classes. It loads the Gemma tokenizer and model, configures precision (bfloat16) and device, prepares input tokens, and generates output text using the `generate` method. Requires the `transformers` and `torch` libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gemma.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b")
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2b",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    attn_implementation="sdpa"
)

input_text = "LLMs generate text through a process known as"
input_ids = tokenizer(input_text, return_tensors="pt").to("cuda")

outputs = model.generate(**input_ids, max_new_tokens=50, cache_implementation="static")
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Downloading Tokenizer and Model
DESCRIPTION: This Python code snippet demonstrates how to download a tokenizer and a model using `AutoTokenizer.from_pretrained` and `AutoModelForSeq2SeqLM.from_pretrained` methods. It retrieves pre-trained models from the Hugging Face Model Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/installation.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
">>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bigscience/T0_3B\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0_3B\")"
```

----------------------------------------

TITLE: Loading Pre-trained Models for Specific Tasks with AutoModelFor Classes in Python
DESCRIPTION: These snippets show how to load pre-trained models for specific tasks using AutoModelFor classes, such as sequence classification and token classification.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/autoclass_tutorial.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")

>>> from transformers import AutoModelForTokenClassification

>>> model = AutoModelForTokenClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Tokenize Text
DESCRIPTION: Tokenizes a text string using the loaded tokenizer. The output includes `input_ids` (numerical representation of tokens), `token_type_ids`, and `attention_mask`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_15

LANGUAGE: Python
CODE:
```
>>> encoding = tokenizer("We are very happy to show you the ðŸ¤— Transformers library.")
>>> print(encoding)
{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

----------------------------------------

TITLE: Initializing and Using Llama3 Model with Transformers Pipeline
DESCRIPTION: This snippet demonstrates how to initialize and use the Llama3 model using the Transformers pipeline for text generation. It specifies the model ID, sets the torch dtype to bfloat16, and uses automatic device mapping.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/llama3.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import transformers
import torch

model_id = "meta-llama/Meta-Llama-3-8B"

pipeline = transformers.pipeline("text-generation", model=model_id, model_kwargs={"torch_dtype": torch.bfloat16}, device_map="auto")
pipeline("Hey how are you doing today?")
```

----------------------------------------

TITLE: Loading a Pretrained Tokenizer with AutoTokenizer in Python
DESCRIPTION: This snippet demonstrates how to load a pretrained tokenizer using the AutoTokenizer class and apply it to a text sequence.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/autoclass_tutorial.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")

>>> sequence = "In a hole in the ground there lived a hobbit."
>>> print(tokenizer(sequence))
```

----------------------------------------

TITLE: Appending User Message and Continuing Chat - Python
DESCRIPTION: Continues the conversation by appending a new user message to the existing chat history list (obtained from the previous model response) and generates the model's subsequent response using the initialized `TextGenerationPipeline`. Requires the `response` object from a previous pipeline call.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/conversations.md#_snippet_4

LANGUAGE: python
CODE:
```
chat = response[0]["generated_text"]
chat.append(
    {"role": "user", "content": "Wait, what's so wild about soup cans?"}
)
response = pipeline(chat, max_new_tokens=512)
print(response[0]["generated_text"][-1]["content"])
```

----------------------------------------

TITLE: Loading a Pretrained Tokenizer in Python
DESCRIPTION: Demonstrates how to load a pretrained tokenizer using the AutoTokenizer class from the transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/preprocessing.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")
```

----------------------------------------

TITLE: Performing Speech Recognition on Audio Dataset in Python
DESCRIPTION: Applies the speech recognition pipeline to the first four samples of the audio dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/quicktour.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> result = speech_recognizer(dataset[:4]["audio"])
>>> print([d["text"] for d in result])
['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', "FONDERING HOW I'D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE", "I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS", 'HOW DO I FURN A JOINA COUT']
```

----------------------------------------

TITLE: Tokenizing Dataset for TensorFlow with Hugging Face Tokenizer
DESCRIPTION: This function tokenizes a dataset using a Hugging Face tokenizer. It takes a dictionary containing the 'text' key as input and returns the tokenized output. The returned dictionary's keys will be added as columns to the dataset, avoiding memory inflation because Hugging Face datasets are stored on disk by default.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_13

LANGUAGE: Python
CODE:
```
def tokenize_dataset (data):
# Ø³ØªØªÙ… Ø¥Ø¶Ø§ÙØ© Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø°ÙŠ ØªÙ…Øª Ø¥Ø¹Ø§Ø¯ØªÙ‡ ÙƒØ£Ø¹Ù…Ø¯Ø© Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
 return tokenizer(data["text"])


dataset = dataset.map(tokenize_dataset)
```

----------------------------------------

TITLE: Padding a Batch of Sentences
DESCRIPTION: This Python code demonstrates how to pad a batch of sentences to the same length using the `padding=True` option in the tokenizer. This ensures that all sentences have the same length, which is required for creating tensors. Padding adds a special token (typically 0) to shorter sentences to match the length of the longest sentence in the batch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_5

LANGUAGE: python
CODE:
```
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast?",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True)
>>> print(encoded_input)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}
```

----------------------------------------

TITLE: Loading Shared Model from Hub in Python
DESCRIPTION: Shows how users can load a shared model from the Hugging Face Hub using the from_pretrained method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/model_sharing.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModel

>>> model = AutoModel.from_pretrained("your_username/my-awesome-model")
```

----------------------------------------

TITLE: Formatting Prompts with Chat Templates in LLaVa
DESCRIPTION: This snippet demonstrates how to format prompts for the LLaVa model using `apply_chat_template` method, which is crucial for ensuring correct formatting based on the underlying large language model backbone. It constructs a conversation history with different modalities like text and image, then applies the chat template to format the prompt.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llava.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("llava-hf/llava-1.5-7b-hf")

conversation = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": "Whatâ€™s shown in this image?"},
            ],
    },
    {
        "role": "assistant",
        "content": [{"type": "text", "text": "This image shows a red stop sign."},

    },
    {

        "role": "user",
        "content": [
            {"type": "text", "text": "Describe the image in more details."},
        ],
    },
]

text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)

# Note that the template simply formats your prompt, you still have to tokenize it and obtain pixel values for your images
print(text_prompt)
>>> "USER: <image>\n<Whatâ€™s shown in this image? ASSISTANT: This image shows a red stop sign.</s>USER: Describe the image in more details. ASSISTANT:"
```

----------------------------------------

TITLE: Batch Processing of Inputs
DESCRIPTION: This code snippet demonstrates how to process multiple input sequences in a batch. It sets the tokenizer's `pad_token` to `eos_token` (end-of-sequence token) because many LLMs don't have a default pad token. It tokenizes two input strings with padding enabled, moves the input tensors to the GPU, generates text, and decodes the generated tokens. Padding ensures that all input sequences in the batch have the same length.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/llm_tutorial.md#_snippet_4

LANGUAGE: python
CODE:
```
>>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
>>> model_inputs = tokenizer(
...     ["A list of colors: red, blue", "Portugal is"], return_tensors="pt", padding=True
... ).to("cuda")
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
['A list of colors: red, blue, green, yellow, orange, purple, pink,',
'Portugal is a country in southwestern Europe, on the Iber']
```

----------------------------------------

TITLE: Loading Pre-trained Model with AutoModel in Python
DESCRIPTION: This code snippet demonstrates how to load a pre-trained BERT model using the `AutoModel` class from the `transformers` library. It automatically infers the model architecture based on the provided model name ("google-bert/bert-base-cased") and returns an instance of the corresponding model class (in this case, `BertModel`).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/auto.md#_snippet_0

LANGUAGE: python
CODE:
```
model = AutoModel.from_pretrained("google-bert/bert-base-cased")
```

----------------------------------------

TITLE: Image Segmentation with Transformers Pipeline API
DESCRIPTION: Example of the `transformers` Pipeline API for image segmentation tasks. It showcases loading an image segmentation model, processing an input image URL, and outputting the identified labels using GPU-accelerated processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
from transformers import pipeline

pipeline = pipeline("image-segmentation", model="facebook/detr-resnet-50-panoptic", device="cuda")
```

LANGUAGE: Python
CODE:
```
segments = pipeline("https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png")
segments[0]["label"]
'bird'
segments[1]["label"]
'bird'
```

----------------------------------------

TITLE: Loading Custom Model and Tokenizer for Text Generation in Python
DESCRIPTION: Shows how to load a specific model and tokenizer from the Hugging Face Model Hub for use in a text generation pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/pipeline_tutorial.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, AutoModelForCausalLM

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilgpt2")
>>> model = AutoModelForCausalLM.from_pretrained("distilbert/distilgpt2")
```

----------------------------------------

TITLE: Autoregressive Prompt Completion with LLaVa-Next
DESCRIPTION: This snippet demonstrates how to autoregressively complete a prompt using the LLaVa-Next model.  It uses the `generate` method of the model to generate new tokens up to a specified maximum and then decodes the output using the processor.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llava_next.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
"output = model.generate(**inputs, max_new_tokens=100)

print(processor.decode(output[0], skip_special_tokens=True))"
```

----------------------------------------

TITLE: Applying torch.compile() to Image Segmentation Model in Python
DESCRIPTION: This snippet illustrates the use of torch.compile() with a SegformerForSemanticSegmentation model. It covers loading the Segformer model, applying compilation, and processing an input image for segmentation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_torch_compile.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation

processor = SegformerImageProcessor.from_pretrained("nvidia/segformer-b0-finetuned-ade-512-512")
model = SegformerForSemanticSegmentation.from_pretrained("nvidia/segformer-b0-finetuned-ade-512-512").to("cuda")
model = torch.compile(model)
seg_inputs = processor(images=image, return_tensors="pt").to("cuda")

with torch.no_grad():
    _ = model(**seg_inputs)
```

----------------------------------------

TITLE: Generating Text with Pipeline - Python
DESCRIPTION: This snippet demonstrates how to generate text using the `pipeline` helper function provided by the Hugging Face `transformers` library. It simplifies the text generation process for the GPT model. Requires `transformers` and `torch` dependencies.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/openai-gpt.md#_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

generator = pipeline(task="text-generation", model="openai-community/gpt", torch_dtype=torch.float16, device=0)
output = generator("The future of AI is", max_length=50, do_sample=True)
print(output[0]["generated_text"])
```

----------------------------------------

TITLE: Generate Text with Llama 2 using Pipeline (Python)
DESCRIPTION: Demonstrates how to use the `transformers` `pipeline` utility to perform text generation with a Llama 2 model. It loads the `Llama-2-7b-hf` model and generates text based on a provided prompt. Requires PyTorch and the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llama2.md#_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipeline = pipeline(
    task="text-generation",
    model="meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.float16,
    device=0
)
pipeline("Plants create energy through a process known as")
```

----------------------------------------

TITLE: Retrieval-Augmented Generation (RAG) with Hugging Face Transformers
DESCRIPTION: Demonstrates implementing retrieval-augmented generation by providing documents to the chat template, expanding the model's context with external information before generating a response
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/chat_templating.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_id = "CohereForAI/c4ai-command-r-v01-4bit"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")\ndevice = model.device\n\nconversation = [\n    {"role": "user", "content": "What has Man always dreamed of?"}\n]\n\ndocuments = [\n    {\n        "title": "The Moon: Our Age-Old Foe", \n        "text": "Man has always dreamed of destroying the moon. In this essay, I shall..."\n    },\n    {\n        "title": "The Sun: Our Age-Old Friend",\n        "text": "Although often underappreciated, the sun provides several notable benefits..."\n    }\n]\n\ninput_ids = tokenizer.apply_chat_template(\n    conversation=conversation,\n    documents=documents,\n    chat_template="rag",\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors="pt").to(device)\n\ngen_tokens = model.generate(\n    input_ids,\n    max_new_tokens=100,\n    do_sample=True,\n    temperature=0.3,\n    )\n\ngen_text = tokenizer.decode(gen_tokens[0])\nprint(gen_text)
```

----------------------------------------

TITLE: Text-Only Generation using Mistral3
DESCRIPTION: This code snippet shows how to generate text using the Mistral3 model without providing any image input. It initializes the processor and model, defines a system prompt and a user prompt, and then uses the `apply_chat_template` method to format the input for the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mistral3.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor, AutoModelForImageTextToText
>>> import torch

>>> torch_device = "cuda"
>>> model_checkpoint = ".mistralai/Mistral-Small-3.1-24B-Instruct-2503"
>>> processor = AutoProcessor.from_pretrained(model_checkpoint)
>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)

>>> SYSTEM_PROMPT = "You are a conversational agent that always answers straight to the point, always end your accurate response with an ASCII drawing of a cat."
>>> user_prompt = "Give me 5 non-formal ways to say 'See you later' in French."

>>> messages = [
...    {"role": "system", "content": SYSTEM_PROMPT},
...    {"role": "user", "content": user_prompt},
... ]

>>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
>>> inputs = processor(text=text, return_tensors="pt").to(0, dtype=torch.float16)
>>> generate_ids = model.generate(**inputs, max_new_tokens=50, do_sample=False)
>>> decoded_output = processor.batch_decode(generate_ids[:, inputs["input_ids"].shape[1] :], skip_special_tokens=True)[0]

>>> print(decoded_output)
"1. Ã€ plus tard!\n2. Salut, Ã  plus!\n3. Ã€ toute!\n4. Ã€ la prochaine!\n5. Je me casse, Ã  plus!\n\n /_\/\
( o.o )\n > ^ <\n"
```

----------------------------------------

TITLE: Generating Text with Falcon using Pipeline (Python)
DESCRIPTION: This snippet demonstrates loading the Falcon 7B Instruct model using the `pipeline` API for text generation. It configures the pipeline with a specific model, datatype (bfloat16), and device (GPU 0). It then calls the pipeline to generate a poem about coding, specifying generation parameters like `max_length`, `do_sample`, and `temperature`. Requires `transformers` and `torch`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/falcon.md#_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipeline = pipeline(
    task="text-generation",
    model="tiiuae/falcon-7b-instruct",
    torch_dtype=torch.bfloat16,
    device=0
)
pipeline(
    "Write a short poem about coding",
    max_length=100,
    do_sample=True,
    temperature=0.7
)
```

----------------------------------------

TITLE: Testing Transformers Installation with Sentiment Analysis in Python
DESCRIPTION: This Python command tests if the Transformers library installation is successful by running a sentiment analysis using the pipeline module. It returns a sentiment label and confidence score.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('hugging face is the best'))"
[{'label': 'POSITIVE', 'score': 0.9998704791069031}]
```

----------------------------------------

TITLE: Pass Batch Input to Model (PyTorch)
DESCRIPTION: Passes the preprocessed batch input to the loaded PyTorch model and unpacks the dictionary with `**` operator.  This produces the model's output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_19

LANGUAGE: Python
CODE:
```
>>> pt_outputs = pt_model(**pt_batch)
```

----------------------------------------

TITLE: Tokenizing, Padding, and Truncating to Model Max Length
DESCRIPTION: This snippet demonstrates padding and truncating a batch of sentences to the model's maximum input length by setting `padding='max_length'` and `truncation=True`.  `truncation=STRATEGY` can be used to specify truncation strategy for sequence pairs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pad_truncation.md#_snippet_6

LANGUAGE: Python
CODE:
```
tokenizer(batch_sentences, padding='max_length', truncation=True)
```

LANGUAGE: Python
CODE:
```
tokenizer(batch_sentences, padding='max_length', truncation=STRATEGY)
```

----------------------------------------

TITLE: Loading 4-bit Quantized Model in Python
DESCRIPTION: This Python snippet demonstrates loading a model with 4-bit quantization using the Transformers library. The method is similar to the 8-bit loading, but with load_in_4bit set to True. It opens opportunities for further memory savings while requiring the bitsandbytes library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
model = AutoModelForCausalLM.from_pretrained("bigcode/octocoder", load_in_4bit=True, low_cpu_mem_usage=True, pad_token_id=0)

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

result = pipe(prompt, max_new_tokens=60)[0]["generated_text"][len(prompt):]
result
```

----------------------------------------

TITLE: Zero-shot Image Classification with Pipeline in PyTorch
DESCRIPTION: This code snippet demonstrates how to perform zero-shot image classification using the `pipeline` abstraction in Transformers. It initializes a CLIP model for zero-shot image classification, defines candidate labels, and then uses the model to classify an image from a URL. The model is loaded with `torch_dtype=torch.bfloat16` for faster inference and placed on device 0.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/clip.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

clip = pipeline(
   task="zero-shot-image-classification",
   model="openai/clip-vit-base-patch32",
   torch_dtype=torch.bfloat16,
   device=0
)
labels = ["a photo of a cat", "a photo of a dog", "a photo of a car"]
clip("http://images.cocodataset.org/val2017/000000039769.jpg", candidate_labels=labels)
```

----------------------------------------

TITLE: Applying Chat Template for Text Generation in Python
DESCRIPTION: Demonstrates integrating structured chat templates, optimizing model input for conversational interfaces. Utilizes role identifiers within prompts to achieve contextually aware responses. Requires a compatible tokenizer and Transformer model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial.md#2025-04-22_snippet_12

LANGUAGE: Python
CODE:
```
messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a pirate",
    },
    {"role": "user", "content": "How many cats does it take to change a light bulb?"},
]
model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to("cuda")
input_length = model_inputs.shape[1]
generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=50)
print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])
"Arr, matey! According to me beliefs, 'twas always one cat to hold the ladder and another to climb up it anâ€™ change the light bulb, but if yer looking to save some catnip, maybe yer can
```

----------------------------------------

TITLE: Installing TensorFlow Dependency
DESCRIPTION: Command to install TensorFlow as a dependency for the Transformers library
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/quicktour.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
pip install tensorflow
```

----------------------------------------

TITLE: Performing Text Translation with Transformers Pipeline - Python
DESCRIPTION: This snippet shows how to use the Hugging Face Transformers `pipeline` to perform text translation via prompting. It sets up the pipeline with a Mistral model, defines a prompt specifying the source and target languages and the text to translate, executes the pipeline with sampling parameters, and prints the translated text.

Dependencies: `transformers`, `torch`.
Inputs: A prompt string with the source text and the translation instruction.
Outputs: A string containing the translated text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/prompting.md#_snippet_5

LANGUAGE: Python
CODE:
```
from transformers import pipeline
import torch

pipeline = pipeline(model="mistralai/Mistral-7B-Instruct-v0.1", torch_dtype=torch.bfloat16, device_map="auto")
prompt = """Translate the English text to French.
Text: Sometimes, I've believed as many as six impossible things before breakfast.
Translation:
"""

outputs = pipeline(prompt, max_new_tokens=20, do_sample=True, top_k=10, return_full_text=False)
for output in outputs:
    print(f"Result: {output['generated_text']}")
```

----------------------------------------

TITLE: Implementing Static KV-Cache with GenerationConfig
DESCRIPTION: Demonstrates how to optimize LLM inference by setting up static kv-cache through GenerationConfig and compiling the model's forward pass using torch.compile. This approach is recommended for basic use cases.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_optims.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # To prevent long warnings :)

tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b")
model = AutoModelForCausalLM.from_pretrained("google/gemma-2b", torch_dtype="auto", device_map="auto")

model.generation_config.cache_implementation = "static"

model.forward = torch.compile(model.forward, mode="reduce-overhead", fullgraph=True)
input_text = "The theory of special relativity states "
input_ids = tokenizer(input_text, return_tensors="pt").to(model.device.type)

outputs = model.generate(**input_ids)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
```

----------------------------------------

TITLE: Fine-tuning Mamba Model with PEFT and LoRA
DESCRIPTION: Example of performing parameter-efficient fine-tuning (PEFT) on a Mamba model using LoRA configuration and SFT Trainer
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mamba.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from datasets import load_dataset
from trl import SFTTrainer
from peft import LoraConfig
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
model_id = "state-spaces/mamba-130m-hf"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)
dataset = load_dataset("Abirate/english_quotes", split="train")
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    logging_dir='./logs',
    logging_steps=10,
    learning_rate=2e-3
)
lora_config =  LoraConfig(
        r=8,
        target_modules=["x_proj", "embeddings", "in_proj", "out_proj"],
        task_type="CAUSAL_LM",
        bias="none"
)
trainer = SFTTrainer(
    model=model,
    processing_class=tokenizer,
    args=training_args,
    peft_config=lora_config,
    train_dataset=dataset,
    dataset_text_field="quote",
)
trainer.train()
```

----------------------------------------

TITLE: Classifying Text with DistilBERT using AutoModel/Tokenizer
DESCRIPTION: This snippet shows how to perform text classification using the `AutoTokenizer` and `AutoModelForSequenceClassification` classes for more control. It loads the tokenizer and model from a pretrained checkpoint, specifying data type (`torch.float16`), device placement (`device_map="auto"`), and attention implementation (`attn_implementation="sdpa"`), processes an input string, performs inference without gradients, and extracts the predicted label.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/distilbert.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    "distilbert/distilbert-base-uncased-finetuned-sst-2-english",
)
model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert/distilbert-base-uncased-finetuned-sst-2-english",
    torch_dtype=torch.float16,
    device_map="auto",
    attn_implementation="sdpa"
)
inputs = tokenizer("I love using Hugging Face Transformers!", return_tensors="pt").to("cuda")

with torch.no_grad():
    outputs = model(**inputs)

predicted_class_id = torch.argmax(outputs.logits, dim=-1).item()
predicted_label = model.config.id2label[predicted_class_id]
print(f"Predicted label: {predicted_label}")
```

----------------------------------------

TITLE: Defining Preprocessing Function
DESCRIPTION: Defines a preprocessing function to load and resample the audio file, check the sampling rate, and set a maximum input length.  It takes a batch of examples, extracts the audio arrays, and processes them using the feature extractor.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/audio_classification.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
">>> def preprocess_function(examples):
...     audio_arrays = [x["array"] for x in examples["audio"]]
...     inputs = feature_extractor(
...         audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True
...     )
...     return inputs"
```

----------------------------------------

TITLE: Performing Sentiment Analysis with Pipeline in Python
DESCRIPTION: Uses the sentiment analysis pipeline to classify the sentiment of a text string.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/quicktour.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> classifier("ç§ãŸã¡ã¯ðŸ¤— Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ãŠè¦‹ã›ã§ãã¦ã¨ã¦ã‚‚å¬‰ã—ã„ã§ã™ã€‚")
[{'label': 'POSITIVE', 'score': 0.9998}]
```

----------------------------------------

TITLE: Load Tokenizer with AutoTokenizer
DESCRIPTION: Loads a tokenizer using `AutoTokenizer` from the `transformers` library, using a pre-trained model name. This ensures the tokenizer uses the same tokenization rules as the pre-trained model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_14

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoTokenizer

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

----------------------------------------

TITLE: Generating Text with LLaMA-3 using Pipeline in Python
DESCRIPTION: This snippet uses the `pipeline` function from `transformers` to generate text with the LLaMA-3 model. It requires `torch` and `transformers` to be installed. The model is loaded with `device_map="auto"` to utilize GPU if available, and `torch_dtype=torch.bfloat16` to reduce memory usage. A response is then generated and printed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/conversations.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipe = pipeline("text-generation", "meta-llama/Meta-Llama-3-8B-Instruct", torch_dtype=torch.bfloat16, device_map="auto")
response = pipe(chat, max_new_tokens=512)
print(response[0]['generated_text'][-1]['content'])
```

----------------------------------------

TITLE: Using AutoModelForMaskedLM for Masked LM (Python)
DESCRIPTION: Shows how to manually load a BERT tokenizer and masked language model (`google-bert/bert-base-uncased`) using `AutoTokenizer` and `AutoModelForMaskedLM`. It tokenizes an input sentence, performs forward pass inference with the model, finds the index of the `[MASK]` token, determines the predicted token ID, and decodes it back to a string. Requires `transformers` and `torch`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bert.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForMaskedLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    "google-bert/bert-base-uncased",
)
model = AutoModelForMaskedLM.from_pretrained(
    "google-bert/bert-base-uncased",
    torch_dtype=torch.float16,
    device_map="auto",
    attn_implementation="sdpa"
)
inputs = tokenizer("Plants create [MASK] through a process known as photosynthesis.", return_tensors="pt").to("cuda")

with torch.no_grad():
    outputs = model(**inputs)
    predictions = outputs.logits

masked_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]
predicted_token_id = predictions[0, masked_index].argmax(dim=-1)
predicted_token = tokenizer.decode(predicted_token_id)

print(f"The predicted token is: {predicted_token}")
```

----------------------------------------

TITLE: Defining TrainingArguments for Trainer
DESCRIPTION: This snippet demonstrates how to define `TrainingArguments` for the `Trainer`. It specifies the output directory where the training checkpoints and logs will be saved.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_4

LANGUAGE: Python
CODE:
```
>>> from transformers import TrainingArguments

>>> training_args = TrainingArguments(output_dir="test_trainer")
```

----------------------------------------

TITLE: Basic Pipeline Usage for Text Classification
DESCRIPTION: Demonstrates simple usage of the pipeline abstraction for text classification with default model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/pipelines.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
pipe = pipeline("text-classification")
pipe("This restaurant is awesome")
```

----------------------------------------

TITLE: Initialization and Loading Methods
DESCRIPTION: Demonstrates the standard methods for loading, saving, and sharing models in the Transformers library across different frameworks
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/philosophy.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
# Standard methods for model handling
model = ModelClass.from_pretrained("model_name")  # Load pretrained model
model.save_pretrained("local_path")  # Save model locally
model.push_to_hub("hub_repository")  # Share model on Hugging Face Hub
```

----------------------------------------

TITLE: Basic LLM Text Generation Implementation - Python
DESCRIPTION: Core implementation of text generation using a Mistral-7B model, including model loading, tokenization and generation steps.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/llm_tutorial.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-v0.1", device_map="auto", load_in_4bit=True
)
```

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
device = "cuda" if torch.cuda.is_available() else "cpu"
model_inputs = tokenizer(["A list of colors: red, blue"], return_tensors="pt").to(device)
```

LANGUAGE: python
CODE:
```
generated_ids = model.generate(**model_inputs)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

----------------------------------------

TITLE: Loading and Quantizing Large Language Model with Bitsandbytes
DESCRIPTION: Demonstrates how to load a large language model (Llama 3.1 8B) and apply 4-bit quantization using BitsAndBytesConfig for memory-efficient inference
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/concept_guide.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "meta-llama/Llama-3.1-8B-Instruct"

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=quantization_config,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
```

----------------------------------------

TITLE: Loading a Model from the Hugging Face Hub
DESCRIPTION: Shows how to load a model that was previously uploaded to the Hugging Face Hub using the from_pretrained method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/model_sharing.md#2025-04-23_snippet_14

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModel

>>> model = AutoModel.from_pretrained("your_username/my-awesome-model")
```

----------------------------------------

TITLE: Generating Text with Pipeline in Python
DESCRIPTION: Demonstrates how to perform text generation using the high-level Hugging Face pipeline API. It loads the GPT-2 model, specifies the task, data type, and device, and then runs the pipeline on an input string.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gpt2.md#_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipeline = pipeline(task="text-generation", model="openai-community/gpt2", torch_dtype=torch.float16, device=0)
pipeline("Hello, I'm a language model")
```

----------------------------------------

TITLE: Setting Up Transformers Trainer for Fine-tuning in Python
DESCRIPTION: This snippet demonstrates how to create a Trainer object with the model, training arguments, datasets, and evaluation function.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/training.md#2025-04-23_snippet_6

LANGUAGE: python
CODE:
```
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)
```

----------------------------------------

TITLE: Loading a Pre-trained Tokenizer
DESCRIPTION: This Python code demonstrates how to load a pre-trained tokenizer using `AutoTokenizer.from_pretrained` from the `transformers` library. The `AutoTokenizer` automatically selects the appropriate tokenizer class based on the pre-trained model specified (in this case, "google-bert/bert-base-cased"). This ensures that the input text is tokenized in the same way as the model was pre-trained.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")
```

----------------------------------------

TITLE: Installing Required Libraries for Question Answering
DESCRIPTION: Installs the necessary Python libraries: transformers for the model, datasets for data handling, and evaluate for evaluation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/question_answering.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers datasets evaluate
```

----------------------------------------

TITLE: Classifying Text with ELECTRA Pipeline (PyTorch)
DESCRIPTION: This snippet demonstrates how to perform text classification using the transformers library's high-level pipeline API. It loads a pre-trained ELECTRA model for emotion classification and applies it to a sample text string. It specifies PyTorch as the backend and uses float16 precision.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/electra.md#_snippet_0

LANGUAGE: Python
CODE:
```
import torch
from transformers import pipeline

classifier = pipeline(
    task="text-classification",
    model="bhadresh-savani/electra-base-emotion",
    torch_dtype=torch.float16,
    device=0
)
classifier("This restaurant has amazing food!")
```

----------------------------------------

TITLE: Initializing PreTrainedTokenizer in Python
DESCRIPTION: The PreTrainedTokenizer class implements common methods for tokenizing input strings, converting tokens to ids and back, and managing special tokens. It provides functionality for splitting strings into subword token strings, encoding/decoding, and adding new tokens to the vocabulary.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/tokenizer.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
class PreTrainedTokenizer:
    def __call__(self):
        # Implementation details
        pass
    
    def add_tokens(self):
        # Implementation details
        pass
    
    def add_special_tokens(self):
        # Implementation details
        pass
    
    def apply_chat_template(self):
        # Implementation details
        pass
    
    def batch_decode(self):
        # Implementation details
        pass
    
    def decode(self):
        # Implementation details
        pass
    
    def encode(self):
        # Implementation details
        pass
    
    def push_to_hub(self):
        # Implementation details
        pass
```

----------------------------------------

TITLE: Loading Model with 8-bit Quantization
DESCRIPTION: Example of loading a PEFT model with 8-bit quantization using BitsAndBytesConfig
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/peft.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```

----------------------------------------

TITLE: Answering Questions Using LLMs in Python
DESCRIPTION: This snippet shows how to set up a structured prompt to answer a question based on contextual information provided to the LLM, effectively guiding its response generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/prompting.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> torch.manual_seed(4) # doctest: +IGNORE_RESULT
>>> prompt = """Answer the question using the context below.\
... Context: Gazpacho is a cold soup and drink made of raw, blended vegetables. Most gazpacho includes stale bread, tomato, cucumbers, onion, bell peppers, garlic, olive oil, wine vinegar, water, and salt. Northern recipes often include cumin and/or pimentÃ³n (smoked sweet paprika). Traditionally, gazpacho was made by pounding the vegetables in a mortar with a pestle; this more laborious method is still sometimes used as it helps keep the gazpacho cool and avoids the foam and silky consistency of smoothie versions made in blenders or food processors.\
... Question: What modern tool is used to make gazpacho?\
... Answer:\
... """

>>> sequences = pipe(
...     prompt,
...     max_new_tokens=10,
...     do_sample=True,
...     top_k=10,
...     return_full_text = False,
... )

>>> for seq in sequences:
...     print(f"Result: {seq['generated_text']}")
Result: Modern tools are used, such as immersion blenders
```

----------------------------------------

TITLE: Generate and Decode Text
DESCRIPTION: This code snippet generates text using the loaded model and then decodes the generated token IDs back into human-readable text. It passes the tokenized input `model_inputs` to the `model.generate()` function, and then uses `tokenizer.batch_decode()` to convert the generated token IDs to text, skipping special tokens like padding.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/llm_tutorial.md#_snippet_3

LANGUAGE: python
CODE:
```
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A list of colors: red, blue, green, yellow, orange, purple, pink,'
```

----------------------------------------

TITLE: PEFT Fine-tuning Mamba Model in Python
DESCRIPTION: This code snippet illustrates how to fine-tune a Mamba model using PEFT (Parameter-Efficient Fine-Tuning) with LoRA (Low-Rank Adaptation). It loads a dataset, configures training arguments, defines a LoRA configuration, and uses the SFTTrainer from the TRL (Transformers Reinforcement Learning) library to train the model on the dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/mamba.md#_snippet_1

LANGUAGE: python
CODE:
```
from datasets import load_dataset
from trl import SFTTrainer
from peft import LoraConfig
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
model_id = "state-spaces/mamba-130m-hf"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)
dataset = load_dataset("Abirate/english_quotes", split="train")
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    logging_dir='./logs',
    logging_steps=10,
    learning_rate=2e-3
)
lora_config =  LoraConfig(
        r=8,
        target_modules=["x_proj", "embeddings", "in_proj", "out_proj"],
        task_type="CAUSAL_LM",
        bias="none"
)
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    peft_config=lora_config,
    train_dataset=dataset,
    dataset_text_field="quote",
)
trainer.train()
```

----------------------------------------

TITLE: Initializing the DistilBERT Tokenizer for Text Preprocessing
DESCRIPTION: Code to load a pre-trained DistilBERT tokenizer which will be used to convert the text data into numerical tokens that the model can process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/sequence_classification.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Initialize and Train the Model with PyTorch Trainer
DESCRIPTION: Initializes the `Trainer` with the model, training arguments, training dataset, evaluation dataset, and data collator.  Then, it starts the training process by calling the `train()` method. Requires the `transformers` library and pre-defined `model`, `training_args`, `lm_dataset`, and `data_collator`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_14

LANGUAGE: python
CODE:
```
>>> trainer = Trainer(
...	model=model,
...	args=training_args,
...	train_dataset=lm_dataset["train"],
...	eval_dataset=lm_dataset["test"],
...	data_collator=data_collator,
... )

>>> trainer.train()
```

----------------------------------------

TITLE: Text Generation using Transformers Pipeline (Python)
DESCRIPTION: Shows how to initialize a `text-generation` pipeline using a specified model from the Hugging Face Hub. It then demonstrates passing an input string to the pipeline to generate text.
SOURCE: https://github.com/huggingface/transformers/blob/main/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
from transformers import pipeline

pipeline = pipeline(task="text-generation", model="Qwen/Qwen2.5-1.5B")
pipeline("the secret to baking a really good cake is ")
[{'generated_text': 'the secret to baking a really good cake is 1) to use the right ingredients and 2) to follow the recipe exactly. the recipe for the cake is as follows: 1 cup of sugar, 1 cup of flour, 1 cup of milk, 1 cup of butter, 1 cup of eggs, 1 cup of chocolate chips. if you want to make 2 cakes, how much sugar do you need? To make 2 cakes, you will need 2 cups of sugar.'}]
```

----------------------------------------

TITLE: Load and Generate with Phi using 4-bit Quantization (Python)
DESCRIPTION: Demonstrates loading the Phi-1 model with 4-bit quantization using `BitsAndBytesConfig`. It configures the quantization settings before loading the model, reducing memory usage. The generation process is similar to the standard AutoModel example.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/phi.md#_snippet_3

LANGUAGE: python
CODE:
```
import torch
from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM

bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type="nf4", bnb_4bit_use_double_quant=True)
tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1")
model = AutoModelForCausalLM.from_pretrained("microsoft/phi-1", torch_dtype=torch.float16, device_map="auto", attn_implementation="sdpa", quantization_config=bnb_config)

input_ids = tokenizer('''def print_prime(n):
   """
   Print all primes between 1 and n
   """''', return_tensors="pt").to("cuda")

output = model.generate(**input_ids, cache_implementation="static")
print(tokenizer.decode(output[0], skip_special_tokens=True))

```

----------------------------------------

TITLE: Training Arguments for Push to Hub - Python
DESCRIPTION: This snippet shows how to configure `TrainingArguments` to automatically push a trained model to the Hugging Face Model Hub by setting `push_to_hub=True`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/model_sharing.md#_snippet_6

LANGUAGE: python
CODE:
```
>>> training_args = TrainingArguments(output_dir="my-awesome-model", push_to_hub=True)
```

----------------------------------------

TITLE: Loading a Quantized Model (8-bit) with Transformers and bitsandbytes
DESCRIPTION: This code snippet demonstrates how to load a quantized model (8-bit) using the Transformers library and the bitsandbytes library. It creates a `BitsAndBytesConfig` with `load_in_8bit=True` and passes it to the `quantization_config` parameter when loading the model with `from_pretrained`. This significantly reduces memory usage.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_optims.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch

quant_config = BitsAndBytesConfig(load_in_8bit=True)
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-v0.1", quantization_config=quant_config, device_map="auto"
)
```

----------------------------------------

TITLE: Installing Transformers with pip
DESCRIPTION: Command for installing the Transformers library using pip package manager. This should be done in a Python virtual environment with one of the supported backends (Flax, PyTorch, or TensorFlow) already installed.
SOURCE: https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
pip install transformers
```

----------------------------------------

TITLE: Handling Padding with Attention Masks (PyTorch)
DESCRIPTION: This Python example illustrates the correct method for preparing batched text inputs of different lengths for a transformers model using attention masks. The tokenizer's `padding=True` and `return_tensors="pt"` arguments automatically handle padding and generate the necessary mask.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/electra.md#_snippet_3

LANGUAGE: Python
CODE:
```
inputs = tokenizer(["Short text", "This is a much longer text that needs padding"],
                    padding=True,
                    return_tensors="pt")
outputs = model(**inputs)  # automatically uses the attention_mask
```

----------------------------------------

TITLE: Performing Sentiment Analysis with a Pipeline in Python
DESCRIPTION: This code snippet demonstrates how to perform sentiment analysis using a pre-initialized pipeline. It passes a text string to the `classifier` pipeline and prints the predicted label and score. The output shows the sentiment of the provided text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_4

LANGUAGE: python
CODE:
```
>>> classifier("We are very happy to show you the ðŸ¤— Transformers library.")
[{'label': 'POSITIVE', 'score': 0.9998}]
```

----------------------------------------

TITLE: Loading a pretrained BERT model in Python
DESCRIPTION: Demonstrates loading a pretrained BERT model using the AutoModel class from Transformers.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/big_models.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoModel

model = AutoModel.from_pretrained("google-bert/bert-base-cased")
```

----------------------------------------

TITLE: Loading a Pretrained Tokenizer in Python
DESCRIPTION: Code snippet demonstrating how to load a pretrained tokenizer using the AutoTokenizer class from the transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/preprocessing.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")
```

----------------------------------------

TITLE: Installing Dependencies with pip
DESCRIPTION: This command installs the `transformers` and `datasets` libraries using pip. These libraries are essential for working with Hugging Face models and datasets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/document_question_answering.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install -q transformers datasets
```

----------------------------------------

TITLE: Hugging Face Login for Model Sharing
DESCRIPTION: Python method to log into Hugging Face account for uploading and sharing machine learning models
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/text-to-speech.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from huggingface_hub import notebook_login

notebook_login()
```

----------------------------------------

TITLE: Notebook Login to Hugging Face Hub
DESCRIPTION: This snippet provides two options for logging into the Hugging Face Hub: via the command line using `huggingface-cli login` or programmatically within a notebook using `notebook_login()`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/custom_models.md#_snippet_14

LANGUAGE: Python
CODE:
```
from huggingface_hub import notebook_login

notebook_login()
```

----------------------------------------

TITLE: Logging into Hugging Face
DESCRIPTION: This snippet demonstrates how to log into Hugging Face to enable model sharing and uploading. It requires the 'huggingface_hub' library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/asr.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Loading a Pre-trained Tokenizer
DESCRIPTION: This code snippet demonstrates how to load a pre-trained tokenizer using `AutoTokenizer.from_pretrained`. The tokenizer is used to convert text into a format that can be processed by a model. The input sequence is then tokenized, and the resulting token IDs, token type IDs, and attention mask are printed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/autoclass_tutorial.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
```

----------------------------------------

TITLE: Initializing and Running Trainer
DESCRIPTION: Example showing how to initialize the Trainer with model, datasets and training arguments and start training
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/trainer.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()
```

----------------------------------------

TITLE: Initializing the Transformers Trainer for Model Fine-tuning
DESCRIPTION: Creates a Trainer instance with the model, training arguments, datasets, and evaluation function to prepare for fine-tuning the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/training.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=small_train_dataset,
...     eval_dataset=small_eval_dataset,
...     compute_metrics=compute_metrics,
... )
```

----------------------------------------

TITLE: TensorFlow Model Training
DESCRIPTION: Configures and executes the training process for TensorFlow models using Keras API.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/training.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from tensorflow.keras.optimizers import Adam

model.compile(optimizer=Adam(3e-5))
model.fit(tf_dataset)
```

----------------------------------------

TITLE: Encoding Input Text with Tokenizer
DESCRIPTION: This Python code shows how to encode input text using a loaded tokenizer. The tokenizer converts the text into a dictionary containing `input_ids`, `token_type_ids`, and `attention_mask`. These are then used as input for transformer models. The `input_ids` are the numerical representations of the tokens, the `token_type_ids` indicate the sequence the token belongs to, and the `attention_mask` specifies which tokens should be attended to.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
>>> print(encoded_input)
{'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

----------------------------------------

TITLE: Performing Sentiment Analysis with Transformers Pipeline in Python
DESCRIPTION: This code example demonstrates text classification using the Hugging Face pipeline for sentiment analysis. It loads a pre-trained sentiment analysis model to evaluate the emotional tone of a text input, returning the classified sentiment label and confidence score.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/task_summary.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline(task="sentiment-analysis")
>>> preds = classifier("Hugging Face is the best thing since sliced bread!")
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.9991, 'label': 'POSITIVE'}]
```

----------------------------------------

TITLE: Saving Transformers Classes Locally
DESCRIPTION: Shows how to use the save_pretrained() method to save models, configurations, and preprocessing classes locally. This allows for later reloading using from_pretrained().
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/philosophy.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
save_pretrained()
```

----------------------------------------

TITLE: Sentiment Analysis with Pipeline
DESCRIPTION: Initializes a sentiment analysis pipeline using the transformers library. This pipeline uses a pre-trained model to classify the sentiment of text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/quicktour.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
">>> from transformers import pipeline\n\n>>> classifier = pipeline("sentiment-analysis")"
```

----------------------------------------

TITLE: Loading a model in 4-bit precision with bitsandbytes
DESCRIPTION: This code configures and loads a pre-trained model in 4-bit precision using `BitsAndBytesConfig` and `AutoModelForCausalLM`. It sets `load_in_4bit=True` within the quantization configuration, utilizes `device_map=â€œautoâ€` to distribute the model across available GPUs and then generates text from a prompt using the loaded model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_one.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM

quantization_config = BitsAndBytesConfig(load_in_4bit=True)
tokenizer = AutoTokenizer("meta-llama/Llama-3.1-8B")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.1-8B", device_map="auto", quantization_config=quantization_config)

prompt = "Hello, my llama is cute"
inputs = tokenizer(prompt, return_tensors="pt").to(model_8bit.device)
generated_ids = model_8bit.generate(**inputs)
outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
```

----------------------------------------

TITLE: Installing Transformers Library with pip
DESCRIPTION: Commands to install the Transformers library using pip, including options for installing with specific deep learning frameworks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/installation.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
pip install transformers
```

LANGUAGE: bash
CODE:
```
pip install transformers[torch]
```

LANGUAGE: bash
CODE:
```
pip install transformers[tf-cpu]
```

LANGUAGE: bash
CODE:
```
pip install transformers[flax]
```

----------------------------------------

TITLE: Loading Pre-trained Models with PyTorch in Transformers
DESCRIPTION: This code demonstrates how to load and use a pre-trained BERT model with PyTorch backend in just three lines of code. It imports the necessary classes, initializes the tokenizer and model, and processes a simple text input.
SOURCE: https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> model = AutoModel.from_pretrained("google-bert/bert-base-uncased")

>>> inputs = tokenizer("Hello world!", return_tensors="pt")
>>> outputs = model(**inputs)
```

----------------------------------------

TITLE: Initializing Pre-trained Models in ðŸ¤— Transformers
DESCRIPTION: Demonstrates the common method for initializing pre-trained model instances across different classes in the ðŸ¤— Transformers library. This method downloads and loads relevant data for the specified pre-trained checkpoint.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/philosophy.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from_pretrained()
```

----------------------------------------

TITLE: Loading TensorFlow Model for Sequence Classification
DESCRIPTION: Demonstrates loading a pretrained sequence classification model using TFAutoModelForSequenceClassification
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/quicktour.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from transformers import TFAutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
```

----------------------------------------

TITLE: Configuring Training Arguments
DESCRIPTION: Setting up TrainingArguments with hyperparameters and training configuration options like learning rate, batch size, and model saving preferences.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/trainer.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="your-model",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=True,
)
```

----------------------------------------

TITLE: Tokenizing a Sequence using AutoTokenizer Python
DESCRIPTION: This code shows how to use a loaded tokenizer to tokenize a given sequence. The tokenizer processes the input string and returns a dictionary containing `input_ids`, `token_type_ids`, and `attention_mask`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/autoclass_tutorial.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> sequence = "In a hole in the ground there lived a hobbit."
>>> print(tokenizer(sequence))
{'input_ids': [101, 1999, 1037, 4920, 1999, 1996, 2598, 2045, 2973, 1037, 7570, 10322, 4183, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

----------------------------------------

TITLE: Generate Text with Mistral using Pipeline API (Python)
DESCRIPTION: This snippet shows how to perform text generation using the `transformers.pipeline` with a Mistral instruction-tuned model. It initializes a pipeline for "text-generation", specifies the model name and data type, and then passes a list of conversation messages to the chatbot. It requires PyTorch and the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mistral.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> import torch
>>> from transformers import pipeline

>>> messages = [
...     {"role": "user", "content": "What is your favourite condiment?"},
...     {"role": "assistant", "content": "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!"},
...     {"role": "user", "content": "Do you have mayonnaise recipes?"}
... ]

>>> chatbot = pipeline("text-generation", model="mistralai/Mistral-7B-Instruct-v0.3", torch_dtype=torch.bfloat16, device=0)
>>> chatbot(messages)
```

----------------------------------------

TITLE: Setting Up OWL-ViT for Zero-shot Detection
DESCRIPTION: This Python snippet sets up a pipeline for zero-shot object detection using the OWL-ViT model. It imports the 'pipeline' method from the 'transformers' library and initializes the detector with the appropriate model checkpoint.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> checkpoint = "google/owlv2-base-patch16-ensemble"
>>> detector = pipeline(model=checkpoint, task="zero-shot-object-detection")
```

----------------------------------------

TITLE: Generate Text with Gemma 2 AutoModel (Python)
DESCRIPTION: Loads the tokenizer and causal language model for `google/gemma-2-9b` explicitly. It tokenizes input text, generates output tokens using the model with specified generation parameters, and decodes the result, showcasing a more granular control over the model interaction.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gemma2.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-9b")
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2-9b",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    attn_implementation="sdpa"
)

input_text = "Explain quantum computing simply."
input_ids = tokenizer(input_text, return_tensors="pt").to("cuda")

outputs = model.generate(**input_ids, max_new_tokens=32, cache_implementation="static")
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Text Summarization with Transformers Pipeline
DESCRIPTION: This example shows how to perform text summarization using the Transformers pipeline. It generates a concise summary of a given text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/task_summary.md#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
>>> from transformers import pipeline

>>> summarizer = pipeline(task="summarization")
>>> summarizer(
...     "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles."
... )
[{'summary_text': ' The Transformer is the first sequence transduction model based entirely on attention . It replaces the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention . For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}]
```

----------------------------------------

TITLE: Verifying Transformers Installation (Sentiment Analysis)
DESCRIPTION: This Python command verifies the installation of the `transformers` library by downloading a pre-trained model and performing sentiment analysis on the input text "we love you". It imports the `pipeline` function from `transformers` and uses it to perform the analysis.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/installation.md#_snippet_7

LANGUAGE: bash
CODE:
```
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
```

----------------------------------------

TITLE: Using Summarization Pipeline in Transformers
DESCRIPTION: Demonstrates how to use the summarization pipeline from Transformers to generate an abstractive summary of a text. The pipeline automatically selects an appropriate model for the task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/task_summary.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import pipeline

summarizer = pipeline(task="summarization")
summarizer(
    "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles."
)
```

----------------------------------------

TITLE: Question Answering with Transformers Pipeline
DESCRIPTION: This snippet demonstrates question answering using the Transformers pipeline. It takes a question and context, and returns the answer with its score and position.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/task_summary.md#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
>>> from transformers import pipeline

>>> question_answerer = pipeline(task="question-answering")
>>> preds = question_answerer(
...     question="What is the name of the repository?",
...     context="The name of the repository is huggingface/transformers",
... )
>>> print(
...     f"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}"
... )
score: 0.9327, start: 30, end: 54, answer: huggingface/transformers
```

----------------------------------------

TITLE: Sentiment Analysis with Transformers Pipeline in Python
DESCRIPTION: This code demonstrates how to use the `pipeline` function from the `transformers` library for sentiment analysis. It initializes a sentiment analysis pipeline and then uses it to classify the sentiment of a given text. The default model is downloaded and cached automatically.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline("sentiment-analysis")
```

----------------------------------------

TITLE: Using Pipeline for Sentiment Analysis in Python
DESCRIPTION: Demonstrates how to use the pipeline API for sentiment analysis. The code downloads a pre-trained model and evaluates it on a given text, returning a positive or negative sentiment with a confidence score.
SOURCE: https://github.com/huggingface/transformers/blob/main/i18n/README_es.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

# Allocate a pipeline for sentiment-analysis
>>> classifier = pipeline('sentiment-analysis')
>>> classifier('We are very happy to introduce pipeline to the transformers repository.')
[{'label': 'POSITIVE', 'score': 0.9996980428695679}]
```

----------------------------------------

TITLE: Initializing Automatic Speech Recognition Pipeline
DESCRIPTION: This code initializes a pipeline for automatic speech recognition (ASR) using the transformers library.  It automatically loads a default model and preprocessing class suitable for ASR. It imports the pipeline class from the transformers library and creates an instance for the specified task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/tutoriel_pipeline.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition")
```

----------------------------------------

TITLE: Setting Up Training Arguments for Transformers Trainer in Python
DESCRIPTION: This snippet shows how to create a TrainingArguments object to specify training hyperparameters and options.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/training.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments

training_args = TrainingArguments(output_dir="test_trainer")
```

----------------------------------------

TITLE: Building PyTorch Tensors
DESCRIPTION: This Python code demonstrates how to build PyTorch tensors from the encoded input using the `return_tensors="pt"` option in the tokenizer. This converts the `input_ids`, `token_type_ids`, and `attention_mask` into PyTorch tensors, which can be directly used as input for PyTorch models. The tensors are created with padding and truncation enabled to ensure consistent shapes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_7

LANGUAGE: python
CODE:
```
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast?",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="pt")
>>> print(encoded_input)
{'input_ids': tensor([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
                      [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
                      [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]]),
 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),
 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
                           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                           [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}
```

----------------------------------------

TITLE: Generating Text with FalconMamba using AutoModel (Python)
DESCRIPTION: This example demonstrates text generation by manually loading the tokenizer and model using `AutoTokenizer` and `AutoModelForCausalLM`. It loads the instruct model, prepares the input prompt by tokenizing it and moving to the GPU, and then calls the model's `generate` method with generation arguments.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/falcon_mamba.md#_snippet_1

LANGUAGE: Python
CODE:
```
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-mamba-7b-instruct")
model = AutoModelForCausalLM.from_pretrained(
    "tiiuae/falcon-mamba-7b-instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

input_ids = tokenizer("Explain the difference between transformers and SSMs", return_tensors="pt").to("cuda")

output = model.generate(**input_ids, max_new_tokens=100, cache_implementation="static")
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Loading a Pretrained Model with PyTorch
DESCRIPTION: Example demonstrating how to use the `transformers` library to load a pretrained Llama-2 model and tokenizer for PyTorch. The model and tokenizer are prepared for inference, with optimized parameters for device and data type configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
```

LANGUAGE: Python
CODE:
```
model_inputs = tokenizer(["The secret to baking a good cake is "], return_tensors="pt").to("cuda")
```

LANGUAGE: Python
CODE:
```
generated_ids = model.generate(**model_inputs, max_length=30)
tokenizer.batch_decode(generated_ids)[0]
'<s> The secret to baking a good cake is 100% in the preparation. There are so many recipes out there,'
```

----------------------------------------

TITLE: Depth Estimation with Pipeline in PyTorch
DESCRIPTION: This snippet demonstrates using the `pipeline` function from Transformers to perform depth estimation with the Depth Anything model. It initializes a pipeline for the `depth-estimation` task, specifying the model name, data type (bfloat16), and device. It then feeds an image URL to the pipeline and extracts the depth map.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/depth_anything.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipe = pipeline(task="depth-estimation", model="LiheYoung/depth-anything-base-hf", torch_dtype=torch.bfloat16, device=0)
pipe("http://images.cocodataset.org/val2017/000000039769.jpg")["depth"]
```

----------------------------------------

TITLE: Initializing Trainer for LayoutLMv2 Fine-tuning in Python
DESCRIPTION: This snippet sets up the Trainer for fine-tuning a LayoutLMv2 model on a document QA task. It combines the model, training arguments, data collator, and datasets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/document_question_answering.md#2025-04-22_snippet_23

LANGUAGE: python
CODE:
```
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=encoded_train_dataset,
    eval_dataset=encoded_test_dataset,
    processing_class=processor,
)

trainer.train()
```

----------------------------------------

TITLE: Batch Tokenization with PyTorch Tensors
DESCRIPTION: This snippet demonstrates how to tokenize a batch of texts with padding, truncation, and conversion to PyTorch tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/quicktour.md#2025-04-23_snippet_6

LANGUAGE: python
CODE:
```
pt_batch = tokenizer(
    ["We are very happy to show you the ðŸ¤— Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)
```

----------------------------------------

TITLE: Tokenize a Text Sequence using BERT Tokenizer Python
DESCRIPTION: This snippet tokenizes a sequence of text using the BERT tokenizer.  The `tokenize` method splits the input sequence into a list of tokens, which can be words or subwords. This step is necessary to convert the text into a format that the BERT model can understand. It depends on the `tokenizer` object from the previous step.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/glossary.md#_snippet_6

LANGUAGE: python
CODE:
```
>>> tokenized_sequence = tokenizer.tokenize(sequence)
```

----------------------------------------

TITLE: Transformers Package Installation Command
DESCRIPTION: Command to install or upgrade the Transformers library via pip.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/tf_xla.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
pip install transformers --upgrade
```

----------------------------------------

TITLE: Installing Transformers Package using pip
DESCRIPTION: This snippet provides the command to install the latest version of the Transformers library, which is necessary for depth estimation tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/monocular_depth_estimation.md#2025-04-22_snippet_0

LANGUAGE: Bash
CODE:
```
pip install -q -U transformers
```

----------------------------------------

TITLE: Generating Text with Transformers AutoModel (Python)
DESCRIPTION: This example illustrates text generation using `AutoModelForCausalLM` and `AutoTokenizer` for finer-grained control. It loads the model and tokenizer, applies a chat template to format input messages, generates token IDs using the model's `generate` method, and decodes the output back into text. It requires `torch` and `transformers` dependencies.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2-1.5B-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    attn_implementation="sdpa"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-1.5B-Instruct")

prompt = "Give me a short introduction to large language models."
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to("cuda")

generated_ids = model.generate(
    model_inputs.input_ids,
    cache_implementation="static",
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_k=50,
    top_p=0.95
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```

----------------------------------------

TITLE: Running Inference with Pipeline
DESCRIPTION: Demonstrates how to use a trained model for inference using the Transformers pipeline API.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_classification.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
from transformers import pipeline

classifier = pipeline("image-classification", model="my_awesome_food_model")
classifier(image)
```

----------------------------------------

TITLE: Text Tokenization Example
DESCRIPTION: Example demonstrating basic text tokenization using AutoTokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/quicktour.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
>>> encoding = tokenizer("We are very happy to show you the ðŸ¤— Transformers library.")
>>> print(encoding)
```

----------------------------------------

TITLE: Load Quantized Mistral Model with BitsAndBytes (Python)
DESCRIPTION: This snippet shows how to load the Mistral model with 4-bit quantization enabled via `BitsAndBytesConfig`. It configures the quantization settings and passes the config to `AutoModelForCausalLM.from_pretrained`. This significantly reduces the model's memory footprint, making it runnable on devices with less VRAM. It requires PyTorch, Transformers, and the bitsandbytes library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mistral.md#_snippet_3

LANGUAGE: python
CODE:
```
>>> import torch
>>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

>>> # specify how to quantize the model
>>> quantization_config = BitsAndBytesConfig(
...         load_in_4bit=True,
...         bnb_4bit_quant_type="nf4",
...         bnb_4bit_compute_dtype="torch.float16",
... )

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.3", quantization_config=True, torch_dtype=torch.bfloat16, device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.3")

>>> prompt = "My favourite condiment is"

>>> messages = [
...     {"role": "user", "content": "What is your favourite condiment?"},
...     {"role": "assistant", "content": "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!"},
...     {"role": "user", "content": "Do you have mayonnaise recipes?"}
... ]

>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")

>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"The expected output"
```

----------------------------------------

TITLE: Loading Tool-Supporting Transformer Model
DESCRIPTION: Initializing a model and tokenizer that supports tool use with HuggingFace Transformers
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_extras.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("NousResearch/Hermes-2-Pro-Llama-3-8B")
model = AutoModelForCausalLM.from_pretrained("NousResearch/Hermes-2-Pro-Llama-3-8B", torch_dtype=torch.bfloat16, device_map="auto")
```

----------------------------------------

TITLE: Installing Transformers with PyTorch
DESCRIPTION: Command to install Transformers library along with PyTorch support.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/installation.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
pip install transformers[torch]
```

----------------------------------------

TITLE: Setting Training Arguments (PyTorch)
DESCRIPTION: This code sets up the TrainingArguments, which specify various training features and hyperparameters. The `output_dir` specifies where the trained model will be saved. Other arguments include `learning_rate`, `per_device_train_batch_size`, `per_device_eval_batch_size`, `num_train_epochs`, and `push_to_hub`.  These arguments control the training process and are crucial for achieving good performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="distilbert-rotten-tomatoes",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=2,
    push_to_hub=True,
)
```

----------------------------------------

TITLE: Loading DistilRoBERTa for Masked Language Modeling with PyTorch
DESCRIPTION: Loads a pre-trained DistilRoBERTa model for masked language modeling using PyTorch's AutoModelForMaskedLM class.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/masked_language_modeling.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoModelForMaskedLM

model = AutoModelForMaskedLM.from_pretrained("distilbert/distilroberta-base")
```

----------------------------------------

TITLE: Decoding Tokenized Input IDs
DESCRIPTION: Demonstrates how to decode the input IDs back into text using the tokenizer's decode method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/preprocessing.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
tokenizer.decode(encoded_input["input_ids"])
```

----------------------------------------

TITLE: Tokenizing dataset for tf.data.Dataset
DESCRIPTION: Tokenizes the dataset by mapping a tokenization function over the dataset. This adds the tokenized outputs as columns to the dataset. This is a necessary step before using `prepare_tf_dataset`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/training.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
def tokenize_dataset(data):
    # Keys of the returned dictionary will be added to the dataset as columns
    return tokenizer(data["text"])


dataset = dataset.map(tokenize_dataset)
```

----------------------------------------

TITLE: Loading Pretrained Model with AutoModelForCausalLM - Python
DESCRIPTION: This snippet demonstrates how to load a pretrained causal language model from the Hugging Face Hub using the AutoModelForCausalLM class. It allows for automatic selection of device and precision through parameters like torch_dtype and device_map.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/models.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", torch_dtype="auto", device_map="auto")
```

----------------------------------------

TITLE: Configure Accelerate
DESCRIPTION: Initializes the Accelerate configuration. This step is required before using Accelerate to train models, allowing you to set up the training environment according to your needs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/run_scripts.md#_snippet_9

LANGUAGE: bash
CODE:
```
accelerate config
```

----------------------------------------

TITLE: Pushing Trained Model to Hugging Face Hub in PyTorch
DESCRIPTION: This snippet shows how to push the trained question answering model to the Hugging Face Hub for sharing and easy access.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/question_answering.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
trainer.push_to_hub()
```

----------------------------------------

TITLE: Multi-Image Inference with LLaVa-Next
DESCRIPTION: This snippet demonstrates how to perform inference with multiple images as input to the LLaVa-Next model. It loads images from URLs, prepares a batch of prompts with image placeholders, processes the input using the processor, and generates output using the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llava_next.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import requests
from PIL import Image
import torch
from transformers import AutoProcessor, AutoModelForImageTextToText

# Load the model in half-precision
model = AutoModelForImageTextToText.from_pretrained("llava-hf/llava-v1.6-mistral-7b-hf", torch_dtype=torch.float16, device_map="auto")
processor = AutoProcessor.from_pretrained("llava-hf/llava-v1.6-mistral-7b-hf")

# Get three different images
url = "https://www.ilankelman.org/stopsigns/australia.jpg"
image_stop = Image.open(requests.get(url, stream=True).raw)

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image_cats = Image.open(requests.get(url, stream=True).raw)

url = "https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg"
image_snowman = Image.open(requests.get(url, stream=True).raw)

# Prepare a batch of two prompts, where the first one is a multi-turn conversation and the second is not
conversation_1 = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": "What is shown in this image?"},
            ],
    },
    {
        "role": "assistant",
        "content": [
            {"type": "text", "text": "There is a red stop sign in the image.",},
            ],
    },
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": "What about this image? How many cats do you see?"},
            ],
    },
]

conversation_2 = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": "What is shown in this image?"},
            ],
    },
]

prompt_1 = processor.apply_chat_template(conversation_1, add_generation_prompt=True)
prompt_2 = processor.apply_chat_template(conversation_2, add_generation_prompt=True)
prompts = [prompt_1, prompt_2]

# We can simply feed images in the order they have to be used in the text prompt
# Each "<image>" token uses one image leaving the next for the subsequent "<image>" tokens
inputs = processor(images=[image_stop, image_cats, image_snowman], text=prompts, padding=True, return_tensors="pt").to(model.device)

# Generate
generate_ids = model.generate(**inputs, max_new_tokens=30)
processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
```

----------------------------------------

TITLE: Perform Zero-Shot Image Classification with Quantized SigLIP2 using AutoModel (Python)
DESCRIPTION: This snippet demonstrates how to load and use a quantized SigLIP2 model (specifically, 4-bit quantization with bitsandbytes) for zero-shot image classification. It involves configuring `BitsAndBytesConfig`, passing it to `AutoModel.from_pretrained`, and then processing inputs and performing inference similar to the standard `AutoModel` approach.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/siglip2.md#_snippet_3

LANGUAGE: Python
CODE:
```
import torch
import requests
from PIL import Image
from transformers import AutoProcessor, AutoModel, BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(load_in_4bit=True)
model = AutoModel.from_pretrained("google/siglip2-large-patch16-512", quantization_config=bnb_config, device_map="auto", attn_implementation="sdpa")
processor = AutoProcessor.from_pretrained("google/siglip2-base-patch16-224")

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
image = Image.open(requests.get(url, stream=True).raw)
candidate_labels = ["a Pallas cat", "a lion", "a Siberian tiger"]

# follows the pipeline prompt template to get same results
texts = [f'This is a photo of {label}.' for label in candidate_labels]

# IMPORTANT: we pass `padding=max_length` and `max_length=64` since the model was trained with this
inputs = processor(text=texts, images=image, padding="max_length", max_length=64, return_tensors="pt").to("cuda")

with torch.no_grad():
    outputs = model(**inputs)

logits_per_image = outputs.logits_per_image
probs = torch.sigmoid(logits_per_image)
print(f"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'")
```

----------------------------------------

TITLE: Speech Recognition Pipeline Setup
DESCRIPTION: Example showing how to set up a speech recognition pipeline with a specific model
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/quicktour.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> import torch
>>> from transformers import pipeline

>>> speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")
```

----------------------------------------

TITLE: Tokenizing Input Text
DESCRIPTION: Tokenizes input text with left padding for language model processing
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", padding_side="left")
model_inputs = tokenizer(["A list of colors: red, blue"], return_tensors="pt").to("cuda")
```

----------------------------------------

TITLE: Configuring Image Augmentation for Object Detection using Albumentations in Python
DESCRIPTION: This code sets up image augmentation pipelines for training and validation using the Albumentations library. It applies geometric and color transformations to images while preserving bounding box information.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/object_detection.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
import albumentations as A

train_augment_and_transform = A.Compose(
    [
        A.Perspective(p=0.1),
        A.HorizontalFlip(p=0.5),
        A.RandomBrightnessContrast(p=0.5),
        A.HueSaturationValue(p=0.1),
    ],
    bbox_params=A.BboxParams(format="coco", label_fields=["category"], clip=True, min_area=25),
)

validation_transform = A.Compose(
    [A.NoOp()],
    bbox_params=A.BboxParams(format="coco", label_fields=["category"], clip=True),
)
```

----------------------------------------

TITLE: Tokenization Steps in Python
DESCRIPTION: This set of snippets illustrates the tokenization process in three steps: tokenizing text into tokens, converting tokens to ids, and decoding ids back to text. It uses methods from the PreTrainedTokenizer class.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/fast_tokenizers.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
tokens = tokenizer.tokenize("We are very happy to show you the ðŸ¤— Transformers library")
print(tokens)
```

LANGUAGE: python
CODE:
```
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

LANGUAGE: python
CODE:
```
decoded_string = tokenizer.decode(ids)
print(decoded_string)
```

----------------------------------------

TITLE: Loading FalconMamba with 4-bit Quantization (Python)
DESCRIPTION: This code demonstrates how to load the FalconMamba 7B model with 4-bit quantization enabled using the `bitsandbytes` integration in Transformers. It configures the quantization settings via `BitsAndBytesConfig` and passes it to the `from_pretrained` method when loading `FalconMambaForCausalLM` and the tokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/falcon_mamba.md#_snippet_3

LANGUAGE: Python
CODE:
```
import torch
from transformers import AutoTokenizer, FalconMambaForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-mamba-7b")
model = FalconMambaForCausalLM.from_pretrained(
    "tiiuae/falcon-mamba-7b",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    quantization_config=quantization_config,
)

inputs = tokenizer("Explain the concept of state space models in simple terms", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Pipeline with Specific Model
DESCRIPTION: Shows how to use a specific model from the Hugging Face Hub in a pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/pipelines.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> pipe = pipeline(model="FacebookAI/roberta-large-mnli")
>>> pipe("This restaurant is awesome")
[{'label': 'NEUTRAL', 'score': 0.7313136458396912}]
```

----------------------------------------

TITLE: Generating Text with Gemma using Pipeline (PyTorch)
DESCRIPTION: Demonstrates how to use the transformers pipeline API for text generation with the Gemma model. It loads the 'google/gemma-2b' model, specifies PyTorch and CUDA device, and generates text based on an input prompt. Requires the `transformers` and `torch` libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gemma.md#_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipeline = pipeline(
    task="text-generation",
    model="google/gemma-2b",
    torch_dtype=torch.bfloat16,
    device="cuda",
)

pipeline("LLMs generate text through a process known as", max_new_tokens=50)
```

----------------------------------------

TITLE: Using AutoModel for Text Context in Python
DESCRIPTION: This example shows using the AutoTokenizer and Llama4ForConditionalGeneration from the Transformers library for text-only generation. It demonstrates loading a tokenizer and a model, preparing inputs, generating outputs, and decoding them. Key dependencies are the Transformers library and PyTorch, and the script outputs generated text based on user prompts.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llama4.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
from transformers import AutoTokenizer, Llama4ForConditionalGeneration
import torch

model_id = "meta-llama/Llama-4-Scout-17B-16E-Instruct"

tokenizer = AutoTokenizer.from_pretrained(model_id)

messages = [
    {"role": "user", "content": "Who are you?"},
]
inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt", return_dict=True)

model = Llama4ForConditionalGeneration.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.bfloat16
)

outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)
outputs = tokenizer.batch_decode(outputs[:, inputs["input_ids"].shape[-1]:])
print(outputs[0])
```

----------------------------------------

TITLE: Saving Pre-trained Models in ðŸ¤— Transformers
DESCRIPTION: Shows the method used to save model, configuration, and preprocessing class instances locally, allowing them to be reloaded using the from_pretrained() method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/philosophy.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
save_pretrained()
```

----------------------------------------

TITLE: Performing Named Entity Recognition with Transformers Pipeline - Python
DESCRIPTION: This snippet demonstrates how to use the Hugging Face Transformers `pipeline` for Named Entity Recognition (NER) by prompting a Mistral instruction-following model. It initializes the pipeline, defines a prompt containing the text and the desired output format, runs the pipeline with specific generation parameters, and prints the generated named entities.

Dependencies: `transformers`, `torch`.
Inputs: A prompt string with the text to analyze and the expected output format.
Outputs: A string containing the identified named entities.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/prompting.md#_snippet_4

LANGUAGE: Python
CODE:
```
from transformers import pipeline
import torch

pipeline = pipeline(model="mistralai/Mistral-7B-Instruct-v0.1", torch_dtype=torch.bfloat16, device_map="auto")
prompt = """Return a list of named entities in the text.
Text: The company was founded in 2016 by French entrepreneurs ClÃ©ment Delangue, Julien Chaumond, and Thomas Wolf in New York City, originally as a company that developed a chatbot app targeted at teenagers.
Named entities:
"""

outputs = pipeline(prompt, max_new_tokens=50, return_full_text=False)
for output in outputs:
    print(f"Result: {output['generated_text']}")
```

----------------------------------------

TITLE: Installing ðŸ¤— Accelerate via pip
DESCRIPTION: Command to install the ðŸ¤— Accelerate library using pip package manager.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/accelerate.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install accelerate
```

----------------------------------------

TITLE: Loading Pre-trained TensorFlow Models for Specific Tasks with TFAutoModelFor Classes in Python
DESCRIPTION: These snippets demonstrate how to load pre-trained TensorFlow models for specific tasks using TFAutoModelFor classes, such as sequence classification and token classification.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/autoclass_tutorial.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")

>>> from transformers import TFAutoModelForTokenClassification

>>> model = TFAutoModelForTokenClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Inference with Pipeline API
DESCRIPTION: Uses the Hugging Face `pipeline` API to perform inference with the fine-tuned masked language model. It initializes a `fill-mask` pipeline with the model and then uses it to fill in the masked token in the provided text.  The `top_k` parameter controls the number of predictions returned. Requires the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_24

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> mask_filler = pipeline("fill-mask", "stevhliu/my_awesome_eli5_mlm_model")
>>> mask_filler(text, top_k=3)
```

----------------------------------------

TITLE: Tokenizing dataset with AutoTokenizer
DESCRIPTION: This snippet demonstrates how to tokenize a dataset using the `AutoTokenizer` from the `transformers` library. It loads a pre-trained tokenizer, defines a `tokenize_function` for padding and truncation, and applies it to the dataset using the `map` method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_1

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")


>>> def tokenize_function(examples):
...     return tokenizer(examples["text"], padding="max_length", truncation=True)
>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")


>>> def tokenize_function(examples):
...     return tokenizer(examples["text"], padding="max_length", truncation=True)


>>> tokenized_datasets = dataset.map(tokenize_function, batched=True)
```

----------------------------------------

TITLE: Training Setup with TensorFlow
DESCRIPTION: Shows complete training setup using TensorFlow/Keras including model preparation and compilation
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/quicktour.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
from transformers import TFAutoModelForSequenceClassification, AutoTokenizer
from tensorflow.keras.optimizers import Adam

model = TFAutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")

def tokenize_dataset(dataset):
    return tokenizer(dataset["text"])

tf_dataset = model.prepare_tf_dataset(
    dataset, batch_size=16, shuffle=True, tokenizer=tokenizer
)

model.compile(optimizer=Adam(3e-5))
model.fit(dataset)
```

----------------------------------------

TITLE: Loading Model-Specific Processor
DESCRIPTION: This snippet demonstrates how to load a processor directly from a specific model class using `ProcessorMixin.from_pretrained`. It initializes a `WhisperProcessor` from the `openai/whisper-tiny` pretrained model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/processors.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import WhisperProcessor

processor = WhisperProcessor.from_pretrained("openai/whisper-tiny")
```

----------------------------------------

TITLE: Aligning Tokens and Labels
DESCRIPTION: Creates a function to realign tokens and labels, assigning -100 to special tokens and handling subword tokenization issues.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/token_classification.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> def tokenize_and_align_labels(examples):
...     tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

...     labels = []
...     for i, label in enumerate(examples[f"ner_tags"]):
...         word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.
...         previous_word_idx = None
...         label_ids = []
...         for word_idx in word_ids:  # Set the special tokens to -100.
...             if word_idx is None:
...                 label_ids.append(-100)
...             elif word_idx != previous_word_idx:  # Only label the first token of a given word.
...                 label_ids.append(label[word_idx])
...             else:
...                 label_ids.append(-100)
...             previous_word_idx = word_idx
...         labels.append(label_ids)

...     tokenized_inputs["labels"] = labels
...     return tokenized_inputs
```

----------------------------------------

TITLE: Creating a Function to Align Tokens with NER Labels
DESCRIPTION: Implements a function that properly aligns the NER labels with tokenized inputs, handling subword tokenization and special tokens by assigning appropriate label IDs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/token_classification.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> def tokenize_and_align_labels(examples):
...     tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

...     labels = []
...     for i, label in enumerate(examples[f"ner_tags"]):
...         word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.
...         previous_word_idx = None
...         label_ids = []
...         for word_idx in word_ids:  # Set the special tokens to -100.
...             if word_idx is None:
...                 label_ids.append(-100)
...             elif word_idx != previous_word_idx:  # Only label the first token of a given word.
...                 label_ids.append(label[word_idx])
...             else:
...                 label_ids.append(-100)
...             previous_word_idx = word_idx
...         labels.append(label_ids)

...     tokenized_inputs["labels"] = labels
...     return tokenized_inputs
```

----------------------------------------

TITLE: Pushing Hugging Face Model to Model Hub
DESCRIPTION: Shows how to use the `push_to_hub` method available on Hugging Face model instances to upload the converted model (including weights and config) to the Hugging Face Model Hub under a specified repository name or organization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/add_new_model.md#_snippet_9

LANGUAGE: python
CODE:
```
brand_new_bert.push_to_hub("brand_new_bert")
# Uncomment the following line to push to an organization.
# brand_new_bert.push_to_hub("<organization>/brand_new_bert")
```

----------------------------------------

TITLE: Batch Mixed Media Inference with Qwen2-VL
DESCRIPTION: This snippet illustrates the process for conducting batch inference on mixed media inputs using Qwen2-VL. The example sets up multiple conversations with combinations of images, videos, and text. The input processing utilizes the AutoProcessor, and the model generates collective responses for the inputs. Dependencies include the 'transformers' library, and the inputs involve lists of conversations with defined media paths and queries. The output comprises the model's batch responses to the inputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2_vl.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
# Conversation for the first image
conversation1 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "path": "/path/to/image1.jpg"},
            {"type": "text", "text": "Describe this image."}
        ]
    }
]

# Conversation with two images
conversation2 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "path": "/path/to/image2.jpg"},
            {"type": "image", "path": "/path/to/image3.jpg"},
            {"type": "text", "text": "What is written in the pictures?"}
        ]
    }
]

# Conversation with pure text
conversation3 = [
    {
        "role": "user",
        "content": "who are you?"
    }
]


# Conversation with mixed midia
conversation4 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "path": "/path/to/image3.jpg"},
            {"type": "image", "path": "/path/to/image4.jpg"},
            {"type": "video", "path": "/path/to/video.jpg"},
            {"type": "text", "text": "What are the common elements in these medias?"},
        ],
    }
]

conversations = [conversation1, conversation2, conversation3, conversation4]
# Preparation for batch inference
ipnuts = processor.apply_chat_template(
    conversations,
    video_fps=1,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt"
).to(model.device)


# Batch Inference
output_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]
output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
print(output_text)
```

----------------------------------------

TITLE: Running Large Models with Pipeline and Accelerate
DESCRIPTION: This code demonstrates how to run large models efficiently using the pipeline API with the accelerate library. It shows loading a model with automatic device mapping and how to use 8-bit quantization for memory efficiency.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/pipeline_tutorial.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
# pip install accelerate
import torch
from transformers import pipeline

pipe = pipeline(model="facebook/opt-1.3b", torch_dtype=torch.bfloat16, device_map="auto")
output = pipe("This is a cool example!", do_sample=True, top_p=0.95)
```

----------------------------------------

TITLE: Running Image-to-Text Generation with Pipeline and Chat Templates
DESCRIPTION: This code runs image-to-text generation using the pipeline with chat templates. It passes the formatted text (messages) to the pipeline, sets the maximum number of new tokens, and removes the input text from the generated output by setting `return_full_text=False`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_text_to_text.md#_snippet_8

LANGUAGE: python
CODE:
```
outputs = pipe(text=messages, max_new_tokens=20, return_full_text=False)
outputs[0]["generated_text"]
```

----------------------------------------

TITLE: Applying Tokenization to the Entire Dataset
DESCRIPTION: Uses the map function to apply the tokenization preprocessing to the entire IMDb dataset in batches, efficiently converting all text samples to tokenized format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
tokenized_imdb = imdb.map(preprocess_function, batched=True)
```

----------------------------------------

TITLE: Multinomial Sampling Text Generation
DESCRIPTION: Demonstrates multinomial sampling for text generation which allows for more diverse outputs compared to greedy search.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/generation_strategies.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
set_seed(0)  # For reproducibility

checkpoint = "openai-community/gpt2-large"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint)

prompt = "Today was an amazing day because"
inputs = tokenizer(prompt, return_tensors="pt")

outputs = model.generate(**inputs, do_sample=True, num_beams=1, max_new_tokens=100)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

----------------------------------------

TITLE: Loading Tokenizer and Model from Local Path
DESCRIPTION: This Python code snippet demonstrates how to load a tokenizer and model from a local directory using the `AutoTokenizer.from_pretrained` and `AutoModel.from_pretrained` methods. It loads the tokenizer and model from the specified directory path.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/installation.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
">>> tokenizer = AutoTokenizer.from_pretrained("./your/path/bigscience_t0")\n>>> model = AutoModel.from_pretrained("./your/path/bigscience_t0")"
```

----------------------------------------

TITLE: Using Phi-3 Model in Transformers - Python
DESCRIPTION: This snippet demonstrates how to use the Phi-3 model with the 'transformers' library for generating text responses. It involves loading a pre-trained model and tokenizer, preparing inputs with a chat template, and generating text output. Ensure that 'trust_remote_code=True' is set in the 'from_pretrained()' function and the 'transformers' library is updated to the development version if necessary.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/phi3.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("microsoft/Phi-3-mini-4k-instruct")
>>> tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")

>>> messages = [{"role": "user", "content": "Can you provide ways to eat combinations of bananas and dragonfruits?"}]
>>> inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt")

>>> outputs = model.generate(inputs, max_new_tokens=32)
>>> text = tokenizer.batch_decode(outputs)[0]
>>> print(text)
```

----------------------------------------

TITLE: Tokenizing Text with AutoTokenizer in Python
DESCRIPTION: This snippet shows how to use AutoTokenizer to tokenize text and return tensors. It demonstrates loading a pre-trained tokenizer and processing a sample sentence, returning input IDs and attention mask.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/fast_tokenizers.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-2b")
tokenizer("We are very happy to show you the ðŸ¤— Transformers library.", return_tensors="pt")
```

----------------------------------------

TITLE: Loading and Processing Dataset with Hugging Face Datasets
DESCRIPTION: Loads the Yelp review dataset and processes it using a BERT tokenizer to prepare for model training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/training.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from datasets import load_dataset

dataset = load_dataset("yelp_review_full")
dataset["train"][100]
```

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)
```

----------------------------------------

TITLE: Tokenizing Text with AutoTokenizer in TensorFlow
DESCRIPTION: This code snippet shows how to tokenize text using the `AutoTokenizer` for TensorFlow models.  The `return_tensors="tf"` argument specifies that the output should be TensorFlow tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_31

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> inputs = tokenizer(text, return_tensors="tf")
```

----------------------------------------

TITLE: Performing Text Translation with T5 Model
DESCRIPTION: This code demonstrates how to use the translation pipeline with a T5 model to translate text from English to French. It shows the basic setup and execution of machine translation using the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/task_summary.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> text = "translate English to French: Hugging Face is a community-based open-source platform for machine learning."
>>> translator = pipeline(task="translation", model="t5-small")
>>> translator(text)
[{'translation_text': "Hugging Face est une tribune communautaire de l'apprentissage des machines."}]
```

----------------------------------------

TITLE: Verifying Transformers Installation
DESCRIPTION: Python command to verify if Transformers is correctly installed by running a sentiment analysis pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/installation.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
```

----------------------------------------

TITLE: Named Entity Recognition with Transformers Pipeline
DESCRIPTION: This example shows how to perform named entity recognition using the Transformers pipeline. It identifies and classifies named entities in a given text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/task_summary.md#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline(task="ner")
>>> preds = classifier("Hugging Face is a French company based in New York City.")
>>> preds = [
...     {
...         "entity": pred["entity"],
...         "score": round(pred["score"], 4),
...         "index": pred["index"],
...         "word": pred["word"],
...         "start": pred["start"],
...         "end": pred["end"],
...     }
...     for pred in preds
... ]
>>> print(*preds, sep="\n")
{'entity': 'I-ORG', 'score': 0.9968, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}
{'entity': 'I-ORG', 'score': 0.9293, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}
{'entity': 'I-ORG', 'score': 0.9763, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}
{'entity': 'I-MISC', 'score': 0.9983, 'index': 6, 'word': 'French', 'start': 18, 'end': 24}
{'entity': 'I-LOC', 'score': 0.999, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}
{'entity': 'I-LOC', 'score': 0.9987, 'index': 11, 'word': 'York', 'start': 46, 'end': 50}
{'entity': 'I-LOC', 'score': 0.9992, 'index': 12, 'word': 'City', 'start': 51, 'end': 55}
```

----------------------------------------

TITLE: Loading a Model in Half-Precision (bfloat16) with Transformers
DESCRIPTION: This code snippet shows how to load a pre-trained model in half-precision (bfloat16) using the Transformers library. It sets the `torch_dtype` parameter to `torch.bfloat16` when loading the model with `from_pretrained`. This reduces memory usage compared to loading in full precision (float32).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_optims.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-v0.1", torch_dtype=torch.bfloat16, device_map="auto",
)
```

----------------------------------------

TITLE: Fine-tuning BERT for Multiple Choice with PyTorch
DESCRIPTION: Loads the BERT model, defines training arguments, initializes the Trainer, and starts the fine-tuning process using PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/multiple_choice.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer

>>> model = AutoModelForMultipleChoice.from_pretrained("google-bert/bert-base-uncased")

>>> training_args = TrainingArguments(
...     output_dir="./results",
...     eval_strategy="epoch",
...     learning_rate=5e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     num_train_epochs=3,
...     weight_decay=0.01,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_swag["train"],
...     eval_dataset=tokenized_swag["validation"],
...     processing_class=tokenizer,
...     data_collator=collator,
... )

>>> trainer.train()
```

----------------------------------------

TITLE: Generating Text with Causal Language Modeling
DESCRIPTION: This snippet shows how to use a causal language model for text generation. It takes a prompt about Hugging Face and uses the text-generation pipeline to generate a continuation of the text based on the model's predictions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/task_summary.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> prompt = "Hugging Face is a community-based open-source platform for machine learning."
>>> generator = pipeline(task="text-generation")
>>> generator(prompt)  # doctest: +SKIP
```

----------------------------------------

TITLE: Model Output with Attention Mask
DESCRIPTION: This example demonstrates how to use an `attention_mask` to ignore padding tokens when generating model outputs, ensuring that the model focuses only on the valid tokens in the input sequences.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/troubleshooting.md#_snippet_7

LANGUAGE: python
CODE:
```
>>> attention_mask = torch.tensor([[1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0]])
>>> output = model(input_ids, attention_mask=attention_mask)
>>> print(output.logits)
tensor([[ 0.0082, -0.2307],
[-0.1008, -0.4061]], grad_fn=<AddmmBackward0>)
```

----------------------------------------

TITLE: Using Text2Text Generation Pipeline with Flan-T5 in Python
DESCRIPTION: This code shows how to use the Transformers 'text2text-generation' pipeline with an encoder-decoder model (Flan-T5) for translation tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/prompting.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> text2text_generator = pipeline("text2text-generation", model = 'google/flan-t5-base')
>>> prompt = "Translate from English to French: I'm very happy to see you"

>>> text2text_generator(prompt)
[{'generated_text': 'Je suis trÃ¨s heureuse de vous rencontrer.'}]
```

----------------------------------------

TITLE: Applying Transforms to Dataset in PyTorch
DESCRIPTION: Defines a function to apply transforms to the dataset and uses it with the Datasets library's with_transform method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/image_classification.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> def transforms(examples):
...     examples["pixel_values"] = [_transforms(img.convert("RGB")) for img in examples["image"]]
...     del examples["image"]
...     return examples

>>> food = food.with_transform(transforms)
```

----------------------------------------

TITLE: Initializing Depth Estimation Pipeline in Transformers
DESCRIPTION: Creates a depth estimation pipeline using a pre-trained model from the Hugging Face Hub. This is the simplest approach to perform depth estimation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/monocular_depth_estimation.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> checkpoint = "vinvino02/glpn-nyu"
>>> depth_estimator = pipeline("depth-estimation", model=checkpoint)
```

----------------------------------------

TITLE: Initializing Text Generation Pipeline with Jamba (Python)
DESCRIPTION: This snippet initializes a Hugging Face `pipeline` for text generation using the Jamba-Mini-1.6 model. It specifies the task, model name, PyTorch data type (float16), and device (GPU 0) for accelerated inference. The pipeline is then used to generate text based on a provided input string.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/jamba.md#_snippet_0

LANGUAGE: python
CODE:
```
# install optimized Mamba implementations
# !pip install mamba-ssm causal-conv1d>=1.2.0
import torch
from transformers import pipeline

pipeline = pipeline(
    task="text-generation",
    model="ai21labs/AI21-Jamba-Mini-1.6",
    torch_dtype=torch.float16,
    device=0
)
pipeline("Plants create energy through a process known as")
```

----------------------------------------

TITLE: PyTorch Model Configuration with Trainer
DESCRIPTION: Configures BERT model for sequence classification and sets up training arguments using the Trainer API
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/training.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSequenceClassification, TrainingArguments

model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=5)
training_args = TrainingArguments(output_dir="test_trainer")
```

----------------------------------------

TITLE: Configuring Image Augmentation Transforms in Python
DESCRIPTION: This code sets up image augmentation transforms using torchvision. It combines RandomResizedCrop and ColorJitter in a Compose object. The transforms are configured based on the image processor's size requirements.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/image_processors.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from torchvision.transforms import RandomResizedCrop, ColorJitter, Compose

size = (
    image_processor.size["shortest_edge"]
    if "shortest_edge" in image_processor.size
    else (image_processor.size["height"], image_processor.size["width"])
)
_transforms = Compose([RandomResizedCrop(size), ColorJitter(brightness=0.5, hue=0.5)])
```

----------------------------------------

TITLE: Complete Training Script with Accelerator
DESCRIPTION: Full implementation of a training script using Accelerator, including main function definition and setup.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from accelerate import Accelerator
  
def main():
  accelerator = Accelerator()

  model, optimizer, training_dataloader, scheduler = accelerator.prepare(
      model, optimizer, training_dataloader, scheduler
  )

  for batch in training_dataloader:
      optimizer.zero_grad()
      inputs, targets = batch
      outputs = model(inputs)
      loss = loss_function(outputs, targets)
      accelerator.backward(loss)
      optimizer.step()
      scheduler.step()

if __name__ == "__main__":
    main()
```

----------------------------------------

TITLE: Load Model and Tokenizer (PyTorch)
DESCRIPTION: Loads a pre-trained model and its associated tokenizer using `AutoModelForSequenceClassification` and `AutoTokenizer` from the `transformers` library. This is the PyTorch implementation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_11

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

----------------------------------------

TITLE: Applying LoRA to the model using Python
DESCRIPTION: This snippet illustrates how to apply the configured LoRA settings to the model by calling the get_peft_model function with the model and LoraConfig parameters. This step integrates the LoRA mechanism directly into the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/how_to_hack_models.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
model = get_peft_model(model, config)
```

----------------------------------------

TITLE: Tokenizing Text with a Pretrained Tokenizer
DESCRIPTION: Example of tokenizing a text string using a pretrained tokenizer and displaying the encoded input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/preprocessing.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
print(encoded_input)
```

----------------------------------------

TITLE: Creating Image Classification Pipeline in Python
DESCRIPTION: Shows how to create an image classification pipeline and use it to classify an image from a URL, formatting the results for display.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/pipeline_tutorial.md#2025-04-23_snippet_10

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> vision_classifier = pipeline(task="image-classification")
>>> preds = vision_classifier(
...     images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.4335, 'label': 'lynx, catamount'}, {'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.0239, 'label': 'Egyptian cat'}, {'score': 0.0229, 'label': 'tiger cat'}]
```

----------------------------------------

TITLE: Configuring AdamW bnb 8-bit Optimizer in TrainingArguments
DESCRIPTION: This code snippet demonstrates how to configure the `adamw_bnb_8bit` optimizer in `TrainingArguments`. This optimizer reduces memory requirements by quantizing optimizer states using bitsandbytes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments

args = TrainingArguments(
    per_device_train_batch_size=4,
    gradient_accumulation_steps=16,
    gradient_checkpointing=True,
    bf16=True,
    optim="adamw_bnb_8bit"
)
```

----------------------------------------

TITLE: Loading Quantized Language Model
DESCRIPTION: Initializes a quantized language model using BitsAndBytesConfig for 4-bit quantization and automatic device mapping
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=True)
model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1", device_map="auto", quantization_config=quantization_config)
```

----------------------------------------

TITLE: Performing Text-Only Generation Qwen2.5-Omni Python
DESCRIPTION: This snippet shows how to load and use the `Qwen2_5OmniThinkerForConditionalGeneration` model for text-only output, which is more efficient when audio generation is not required. It initializes the model and processor, prepares a conversation (similar to single media inference), applies the chat template, and generates text IDs. Note that the model used here (`Thinker`) specifically generates text, and any attempt to access or save audio output from this model would be incorrect.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2_5_omni.md#_snippet_1

LANGUAGE: python
CODE:
```
from transformers import Qwen2_5OmniThinkerForConditionalGeneration, Qwen2_5OmniProcessor

model = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-Omni-7B",
    torch_dtype="auto",
    device_map="auto",
)
processor = Qwen2_5OmniProcessor.from_pretrained("Qwen/Qwen2.5-Omni-7B")

conversations = [
    {
        "role": "system",
        "content": [
            {"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."}
        ],
    },
    {
        "role": "user",
        "content": [
            {"type": "video", "video": "/path/to/video.mp4"},
            {"type": "text", "text": "What cant you hear and see in this video?"},
        ],
    },
]

inputs = processor.apply_chat_template(
    conversations,
    load_audio_from_video=True,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt",
    video_fps=1,

    # kwargs to be passed to `Qwen2-5-OmniProcessor`
    padding=True,
    use_audio_in_video=True,
).to(model.device)


text_ids = model.generate(**inputs, use_audio_in_video=True)
text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)

sf.write(
    "output.wav",
    audio.reshape(-1).detach().cpu().numpy(),
    samplerate=24000,
)
print(text)
```

----------------------------------------

TITLE: Configuring training arguments
DESCRIPTION: Set up training arguments for the Trainer, including batch sizes, learning rate, and evaluation strategy.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/tasks/asr.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
training_args = TrainingArguments(
    output_dir="my_awesome_asr_mind_model",
    per_device_train_batch_size=8,
    gradient_accumulation_steps=2,
    learning_rate=1e-5,
    warmup_steps=500,
    max_steps=2000,
    gradient_checkpointing=True,
    fp16=True,
    group_by_length=True,
    eval_strategy="steps",
    per_device_eval_batch_size=8,
    save_steps=1000,
    eval_steps=1000,
    logging_steps=25,
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    push_to_hub=True,
)
```

----------------------------------------

TITLE: Configuring 8-bit Quantization with CPU Offloading in Python
DESCRIPTION: This snippet demonstrates how to enable 8-bit quantization with CPU offloading using BitsAndBytesConfig. It's useful for fitting large models into memory by offloading some weights to the CPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-23_snippet_9

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)
```

----------------------------------------

TITLE: Enabling Big Model Inference - Transformers - Python
DESCRIPTION: This snippet shows how to enable the Big Model Inference feature with the `transformers` library using `AutoModelForCausalLM` with the `device_map="auto"` setting, optimizing memory usage across available devices for large models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/models.md#2025-04-22_snippet_13

LANGUAGE: Python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("google/gemma-7b", device_map="auto")
```

----------------------------------------

TITLE: Utilizing Long Context with AutoModel in Python
DESCRIPTION: This code demonstrates the use of long context capabilities in the Llama 4 model by processing extensive text inputs. It uses `torchrun` for tensor-parallel execution, given extended attention mechanisms. Dependencies are extensive, including text inputs from files and efficient GPU memory management.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llama4.md#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
from transformers import Llama4ForConditionalGeneration, AutoTokenizer
import torch
import time

file = "very_long_context_prompt.txt"
model_id = "meta-llama/Llama-4-Scout-17B-16E-Instruct"

with open(file, "r") as f:
    very_long_text = "\n".join(f.readlines())

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = Llama4ForConditionalGeneration.from_pretrained(
    model_id,
    device_map="auto",
    attn_implementation="flex_attention",
    torch_dtype=torch.bfloat16
)

messages = [
    {"role": "user", "content": f"Look at the following texts: [{very_long_text}]\n\n\n\nWhat are the books, and who wrote them? Make me a nice list."},
]
input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt")

torch.cuda.synchronize()
start = time.time()
out = model.generate(
    input_ids.to(model.device),
    prefill_chunk_size=2048*8,
    max_new_tokens=300,
    cache_implementation="hybrid",
)
print(time.time()-start)
print(tokenizer.batch_decode(out[:, input_ids.shape[-1]:]))
print(f"{torch.cuda.max_memory_allocated(model.device) / 1024**3:.2f} GiB")
```

----------------------------------------

TITLE: Loading Pretrained Tokenizer
DESCRIPTION: Loads a pretrained BERT tokenizer using the AutoTokenizer class.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/preprocessing.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")
```

----------------------------------------

TITLE: Object Detection with DAB-DETR
DESCRIPTION: This code snippet demonstrates how to perform object detection using a pre-trained DAB-DETR model. It downloads an image, preprocesses it using `AutoImageProcessor`, runs the model, and post-processes the outputs to obtain bounding boxes, labels, and scores. The `requests` and `PIL` libraries are required for image loading and processing, and `torch` is used for tensor operations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/dab-detr.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
import requests

from PIL import Image
from transformers import AutoModelForObjectDetection, AutoImageProcessor

url = 'http://images.cocodataset.org/val2017/000000039769.jpg' 
image = Image.open(requests.get(url, stream=True).raw)

image_processor = AutoImageProcessor.from_pretrained("IDEA-Research/dab-detr-resnet-50")
model = AutoModelForObjectDetection.from_pretrained("IDEA-Research/dab-detr-resnet-50")

inputs = image_processor(images=image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

results = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([image.size[::-1]]), threshold=0.3)

for result in results:
    for score, label_id, box in zip(result["scores"], result["labels"], result["boxes"]):
        score, label = score.item(), label_id.item()
        box = [round(i, 2) for i in box.tolist()]
        print(f"{model.config.id2label[label]}: {score:.2f} {box}")
```

----------------------------------------

TITLE: Loading Model for Inference
DESCRIPTION: Initializes the model and image processor from Hugging Face Hub with automatic device detection.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/object_detection.md#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
from accelerate.test_utils.testing import get_backend
device, _, _ = get_backend()
model_repo = "qubvel-hf/detr_finetuned_cppe5"

image_processor = AutoImageProcessor.from_pretrained(model_repo)
model = AutoModelForObjectDetection.from_pretrained(model_repo)
model = model.to(device)
```

----------------------------------------

TITLE: Classifying Text with Zero-Shot Prompting (Python)
DESCRIPTION: Initializes a text-generation pipeline with a specified Mistral model and PyTorch settings. It defines a prompt asking the model to classify text sentiment and then runs the pipeline, printing the generated text which includes the classification. Requires `transformers` and `torch`. Takes a text string within the prompt as input and outputs the original prompt plus the model's response (the sentiment).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/prompting.md#_snippet_0

LANGUAGE: python
CODE:
```
from transformers import pipeline
import torch

pipeline = pipeline(task="text-generation", model="mistralai/Mistal-7B-Instruct-v0.1", torch_dtype=torch.bfloat16, device_map="auto")
prompt = """Classify the text into neutral, negative or positive.
Text: This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.
Sentiment:
"""

outputs = pipeline(prompt, max_new_tokens=10)
for output in outputs:
    print(f"Result: {output['generated_text']}")
```

----------------------------------------

TITLE: Loading a Model in 8-bit Precision with BitsAndBytesConfig
DESCRIPTION: Python code to load a model in 8-bit precision using the BitsAndBytesConfig with the load_in_8bit parameter set to True.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/bitsandbytes.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_8bit=True)

model_8bit = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-1b7", 
    quantization_config=quantization_config
)
```

----------------------------------------

TITLE: Model Output without Attention Mask
DESCRIPTION: This snippet shows an example of incorrect output when padding tokens in the `input_ids` are not masked with an `attention_mask`. The model processes the padded tokens, leading to inaccurate results.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/troubleshooting.md#_snippet_6

LANGUAGE: Python
CODE:
```
>>> input_ids = torch.tensor([[7592, 2057, 2097, 2393, 9611, 2115], [7592, 0, 0, 0, 0, 0]])
>>> output = model(input_ids)
>>> print(output.logits)
tensor([[ 0.0082, -0.2307],
        [ 0.1317, -0.1683]], grad_fn=<AddmmBackward0>)
```

----------------------------------------

TITLE: Setting Device for PyTorch Model
DESCRIPTION: This code determines whether a CUDA-enabled GPU is available and sets the device accordingly. If a GPU is available, the model will be moved to the GPU; otherwise, it will use the CPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_23

LANGUAGE: Python
CODE:
```
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
```

----------------------------------------

TITLE: Generating Outputs with GPT-2 in Python
DESCRIPTION: This code snippet demonstrates how to use the HuggingFace Transformers library to generate text using the GPT-2 model. It initializes the tokenizer and model, processes input, and generates an output while providing attributes from the model's output structure.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/internal/generation_utils.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained("openai-community/gpt2")
model = GPT2LMHeadModel.from_pretrained("openai-community/gpt2")

inputs = tokenizer("Hello, my dog is cute and ", return_tensors="pt")
generation_output = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)
```

----------------------------------------

TITLE: Initializing T5 Tokenizer for Translation
DESCRIPTION: Load a pre-trained T5 tokenizer to process and prepare English-French translation pairs for model training
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/translation.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer\n\ncheckpoint = "google-t5/t5-small"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Tokenizing, Padding, and Truncating to Specific Length
DESCRIPTION: This snippet shows how to pad to longest and truncate a batch of sentences to a specific length. `padding=True` enables padding to the longest sequence length in the batch and `truncation=True` with `max_length=42` truncates sequences to a length of 42. `truncation=STRATEGY` can be used to specify truncation strategy for sequence pairs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pad_truncation.md#_snippet_8

LANGUAGE: Python
CODE:
```
tokenizer(batch_sentences, padding=True, truncation=True, max_length=42)
```

LANGUAGE: Python
CODE:
```
tokenizer(batch_sentences, padding=True, truncation=STRATEGY, max_length=42)
```

----------------------------------------

TITLE: Tokenizing, Padding, and Truncating to Longest Length
DESCRIPTION: This snippet demonstrates padding a batch of sentences to the longest sequence while truncating to a specified max length. It uses `padding=True` for padding and `truncation=True` for truncation. `truncation=STRATEGY` can be used to specify truncation strategy for sequence pairs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pad_truncation.md#_snippet_5

LANGUAGE: Python
CODE:
```
tokenizer(batch_sentences, padding=True, truncation=True)
```

LANGUAGE: Python
CODE:
```
tokenizer(batch_sentences, padding=True, truncation=STRATEGY)
```

----------------------------------------

TITLE: Generating Text with Quantized T5 Model (PyTorch)
DESCRIPTION: Demonstrates loading and using a T5 model with quantization applied using the torchao backend. It initializes a TorchAoConfig for 'int4_weight_only' quantization, loads the 'google/t5-v1_1-xl' model with this configuration, tokenizes the input, generates text, and decodes the output. This requires the 'torchao' library to be installed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/t5.md#_snippet_3

LANGUAGE: python
CODE:
```
# pip install torchao
import torch
from transformers import TorchAoConfig, AutoModelForSeq2SeqLM, AutoTokenizer

quantization_config = TorchAoConfig("int4_weight_only", group_size=128)
model = AutoModelForSeq2SeqLM.from_pretrained(
    "google/t5-v1_1-xl",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained("google/t5-v1_1-xl")
input_ids = tokenizer("translate English to French: The weather is nice today.", return_tensors="pt").to("cuda")

output = model.generate(**input_ids, cache_implementation="static")
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Tokenizing Text with a Pretrained Tokenizer
DESCRIPTION: Shows how to tokenize a single sentence using a pretrained tokenizer and print the encoded input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/preprocessing.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
print(encoded_input)
```

----------------------------------------

TITLE: Formatting Chat for Training - Python
DESCRIPTION: This code shows how to preprocess a dataset of chats by applying the chat template to format them for training, ensuring that the expected token sequence matches what the model was trained to recognize.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/chat_templating.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer
from datasets import Dataset

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-beta")

chat1 = [
    {"role": "user", "content": "Which is bigger, the moon or the sun?"},
    {"role": "assistant", "content": "The sun."}
]
chat2 = [
    {"role": "user", "content": "Which is bigger, a virus or a bacterium?"},
    {"role": "assistant", "content": "A bacterium."}
]

dataset = Dataset.from_dict({"chat": [chat1, chat2]})
dataset = dataset.map(lambda x: {"formatted_chat": tokenizer.apply_chat_template(x["chat"], tokenize=False, add_generation_prompt=False)})
print(dataset['formatted_chat'][0])
```

----------------------------------------

TITLE: Configuring Gradient Checkpointing for Reduced Memory Usage - Python
DESCRIPTION: This snippet shows how to enable gradient checkpointing using the `TrainingArguments` class. This feature allows for training with reduced memory consumption by storing only a subset of activations, trading off some computation for better memory efficiency.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
from transformers import TrainingArguments

args = TrainingArguments(
    per_device_train_batch_size=4,
    gradient_accumulation_steps=16,
    gradient_checkpointing=True,
)
```

----------------------------------------

TITLE: Enabling BF16 Training
DESCRIPTION: Configuration for enabling BF16 mixed precision training on supported hardware architectures like Ampere.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_train_gpu_one.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
training_args = TrainingArguments(bf16=True, **default_args)
```

----------------------------------------

TITLE: Loading and Using DeepSeek-V3 for Chat Completion in Python
DESCRIPTION: This code snippet demonstrates how to load the DeepSeek-V3 model and tokenizer from the Hugging Face Transformers library, prepare a chat conversation using a chat template, and generate a response from the model. It requires the `transformers` and `torch` libraries to be installed and uses the `deepseek-r1` pretrained model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/deepseek_v3.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
```python
# `run_deepseek_v1.py`
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
torch.manual_seed(30)

tokenizer = AutoTokenizer.from_pretrained("deepseek-r1")

chat = [
  {"role": "user", "content": "Hello, how are you?"},
  {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
  {"role": "user", "content": "I'd like to show off how chat templating works!"},
]


model = AutoModelForCausalLM.from_pretrained("deepseek-r1", device_map="auto", torch_dtype=torch.bfloat16)
inputs = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors="pt").to(model.device)
import time
start = time.time()
outputs = model.generate(inputs, max_new_tokens=50)
print(tokenizer.batch_decode(outputs))
print(time.time()-start)
```
```

----------------------------------------

TITLE: Model Training Setup with PyTorch
DESCRIPTION: Initializes and trains the DistilGPT2 model using the Trainer API in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/language_modeling.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer

model = AutoModelForCausalLM.from_pretrained("distilbert/distilgpt2")

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=2e-5,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_dataset["train"],
    eval_dataset=lm_dataset["test"],
    data_collator=data_collator,
)

trainer.train()
```

----------------------------------------

TITLE: Basic Pipeline Initialization and Inference for Speech Recognition
DESCRIPTION: Shows how to create and use a pipeline for automatic speech recognition (ASR) task using the Whisper model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/pipeline_tutorial.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import pipeline

transcriber = pipeline(task="automatic-speech-recognition")
transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
```

----------------------------------------

TITLE: Generating Text with GPT-NeoX Model
DESCRIPTION: Example demonstrating how to use the GPT-NeoX model's generate() method for text generation with temperature sampling. This includes loading the model and tokenizer, preparing input, generating text, and decoding the output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gpt_neox.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast

>>> model = GPTNeoXForCausalLM.from_pretrained("EleutherAI/gpt-neox-20b")
>>> tokenizer = GPTNeoXTokenizerFast.from_pretrained("EleutherAI/gpt-neox-20b")

>>> prompt = "GPTNeoX20B is a 20B-parameter autoregressive Transformer model developed by EleutherAI."

>>> input_ids = tokenizer(prompt, return_tensors="pt").input_ids

>>> gen_tokens = model.generate(
...     input_ids,
...     do_sample=True,
...     temperature=0.9,
...     max_length=100,
... )
>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]
```

----------------------------------------

TITLE: Applying Mistral Instruct Chat Template in Python
DESCRIPTION: Example demonstrating Mistral's more complex chat template that adds control tokens like [INST] and [/INST] around user messages to format the conversation appropriately for the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/chat_templating.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")

>>> chat = [
...   {"role": "user", "content": "Hello, how are you?"},
...   {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
...   {"role": "user", "content": "I'd like to show off how chat templating works!"},
... ]

>>> tokenizer.apply_chat_template(chat, tokenize=False)
"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]"
```

----------------------------------------

TITLE: Creating a Text Generation Pipeline in Python
DESCRIPTION: Creates a generic pipeline for text generation tasks, allowing easy inference with pre-trained models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/pipeline_tutorial.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> generator = pipeline(task="text-generation")
```

----------------------------------------

TITLE: Load Pretrained Tokenizer with AutoTokenizer in Python
DESCRIPTION: This snippet loads a pretrained tokenizer using the `AutoTokenizer.from_pretrained` method. It downloads the vocabulary associated with the pretrained model specified by the provided model name. The loaded tokenizer can then be used to process text data for the specified model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/preprocessing.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")
```

----------------------------------------

TITLE: Tokenizing and Preparing Model Inputs in Python
DESCRIPTION: This snippet demonstrates how to tokenize a simple input sentence using a Hugging Face tokenizer and prepare tensors for model input. It requires the `transformers` library and a pretrained tokenizer. Input is a list of sentences, and output is a dictionary containing tokenized tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial.md#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
model_inputs = tokenizer(["I am a cat."], return_tensors="pt").to("cuda")
```

----------------------------------------

TITLE: Greedy Search with Transformers
DESCRIPTION: This snippet demonstrates how to use the greedy search decoding strategy with the Transformers library. It initializes a tokenizer and a causal language model, encodes an input string, and generates text using `model.generate` without any specific decoding strategy parameters (defaults to greedy).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/generation_strategies.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
inputs = tokenizer("Hugging Face is an open-source company", return_tensors="pt").to("cuda")

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", torch_dtype=torch.float16).to("cuda")
# explicitly set to default length because Llama2 generation length is 4096
outputs = model.generate(**inputs, max_new_tokens=20)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
'Hugging Face is an open-source company that provides a suite of tools and services for building, deploying, and maintaining natural language processing'
```

----------------------------------------

TITLE: Generating Text with Pipeline (Python)
DESCRIPTION: Demonstrates initializing and using the Hugging Face `pipeline` abstraction for performing image-to-text generation with a Gemma 3 model. It shows how to specify the task, model, device, and data type, then pass an image URL and text prompt to generate output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gemma3.md#_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipeline = pipeline(
    task="image-text-to-text",
    model="google/gemma-3-4b-pt",
    device=0,
    torch_dtype=torch.bfloat16
)
pipeline(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg",
    text="<image> What is shown in this image?"
)
```

----------------------------------------

TITLE: Loading Converted Llama3 Model and Tokenizer
DESCRIPTION: This Python snippet demonstrates how to load the converted Llama3 model and tokenizer using the Transformers library's AutoModelForCausalLM and AutoTokenizer classes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/llama3.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("/output/path")
model = AutoModelForCausalLM.from_pretrained("/output/path")
```

----------------------------------------

TITLE: Configuring Training Arguments and Running Fine-tuning with PyTorch Trainer
DESCRIPTION: Sets up training hyperparameters, initializes the Trainer with the model, datasets, and evaluation metrics, then launches the training process. The trained model is automatically saved and can be pushed to the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_model",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     num_train_epochs=2,
...     weight_decay=0.01,
...     eval_strategy="epoch",
...     save_strategy="epoch",
...     load_best_model_at_end=True,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_imdb["train"],
...     eval_dataset=tokenized_imdb["test"],
...     processing_class=tokenizer,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

----------------------------------------

TITLE: Setting Up Environment and Loading Falcon-7b-instruct Model in Python
DESCRIPTION: This code demonstrates how to set up the environment, load the Falcon-7b-instruct model using the Transformers pipeline, and configure it for text generation with appropriate data types.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/prompting.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
pip install -q transformers accelerate
```

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline, AutoTokenizer
>>> import torch

>>> torch.manual_seed(0) # doctest: +IGNORE_RESULT
>>> model = "tiiuae/falcon-7b-instruct"

>>> tokenizer = AutoTokenizer.from_pretrained(model)
>>> pipe = pipeline(
...     "text-generation",
...     model=model,
...     tokenizer=tokenizer,
...     torch_dtype=torch.bfloat16,
...     device_map="auto",
... )
```

----------------------------------------

TITLE: Using Vision Pipeline for Image Classification
DESCRIPTION: This example shows how to use the pipeline API for vision tasks, specifically image classification. It loads a vision transformer model and classifies an image from a URL, returning prediction scores and labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/pipeline_tutorial.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> vision_classifier = pipeline(model="google/vit-base-patch16-224")
>>> preds = vision_classifier(
...     images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.4335, 'label': 'lynx, catamount'}, {'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.0239, 'label': 'Egyptian cat'}, {'score': 0.0229, 'label': 'tiger cat'}]
```

----------------------------------------

TITLE: Padding Tokenized Sequences in Python
DESCRIPTION: This snippet demonstrates how to apply padding to tokenized sequences to ensure all sequences in a batch have the same length. It uses the padding parameter in the tokenizer function.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/fast_tokenizers.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
encoded_inputs = tokenizer(batch_sentences, padding=True, return_tensors="pt")
print(encoded_inputs)
```

----------------------------------------

TITLE: Quantizing Llama4 with LLM-Compressor in Transformers
DESCRIPTION: This example showcases how to utilize the LLM-Compressor technique with a pre-quantized FP8 checkpoint for the Llama4 model.  It loads the model and tokenizer, creates a chat template, and generates text. The `model_id` points to the pre-quantized FP8 checkpoint, which reduces memory usage and improves performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llama4.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, Llama4ForConditionalGeneration
import torch

model_id = "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8"

tokenizer = AutoTokenizer.from_pretrained(model_id)

messages = [
    {"role": "user", "content": "Who are you?"},
]
inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt", return_dict=True)

model = Llama4ForConditionalGeneration.from_pretrained(
    model_id,
    tp_plan="auto",
    torch_dtype=torch.bfloat16,
)

outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)
outputs = tokenizer.batch_decode(outputs[:, inputs["input_ids"].shape[-1]:])
print(outputs[0])
```

----------------------------------------

TITLE: Load Tokenizer (PyTorch)
DESCRIPTION: This snippet loads a tokenizer using `AutoTokenizer.from_pretrained`. It loads a pre-trained DistilBERT tokenizer, which is necessary for preparing the input data for the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_35

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Loading a Pre-trained Processor with AutoProcessor in Python
DESCRIPTION: This snippet demonstrates how to load a pre-trained processor using the AutoProcessor class for multimodal tasks that require both a feature extractor and a tokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/autoclass_tutorial.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")
```

----------------------------------------

TITLE: Installing Required Libraries for T5 Summarization
DESCRIPTION: Installs the necessary Python libraries for working with Transformers, datasets, and evaluation metrics.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/summarization.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers datasets evaluate rouge_score
```

----------------------------------------

TITLE: Installing Required Libraries using pip
DESCRIPTION: This command installs the necessary libraries: datasets, transformers, evaluate, timm, and albumentations. These libraries are used for dataset handling, model training, evaluation, and image augmentation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/object_detection.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
"pip install -q datasets transformers evaluate timm albumentations"
```

----------------------------------------

TITLE: Training a PEFT Adapter with Trainer
DESCRIPTION: Code showing how to use the Hugging Face Trainer API to train a model with a PEFT adapter configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/peft.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
trainer = Trainer(model=model, ...)
trainer.train()
```

----------------------------------------

TITLE: Quantizing Jamba Model and Generating Chat Response (Python)
DESCRIPTION: This snippet demonstrates loading the Jamba-Large-1.6 model with 8-bit quantization using the `bitsandbytes` backend, excluding Mamba modules from quantization. It uses a pre-defined device map to distribute model layers across multiple GPUs and loads the model in bfloat16 precision with Flash Attention 2. The snippet then tokenizes a chat conversation, generates a response from the quantized model, and extracts the assistant's output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/jamba.md#_snippet_3

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_8bit=True,
                                         llm_int8_skip_modules=["mamba"])

# a device map to distribute the model evenly across 8 GPUs
device_map = {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 2, 'model.layers.26': 2, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.layers.32': 3, 'model.layers.33': 3, 'model.layers.34': 3, 'model.layers.35': 3, 'model.layers.36': 4, 'model.layers.37': 4, 'model.layers.38': 4, 'model.layers.39': 4, 'model.layers.40': 4, 'model.layers.41': 4, 'model.layers.42': 4, 'model.layers.43': 4, 'model.layers.44': 4, 'model.layers.45': 5, 'model.layers.46': 5, 'model.layers.47': 5, 'model.layers.48': 5, 'model.layers.49': 5, 'model.layers.50': 5, 'model.layers.51': 5, 'model.layers.52': 5, 'model.layers.53': 5, 'model.layers.54': 6, 'model.layers.55': 6, 'model.layers.56': 6, 'model.layers.57': 6, 'model.layers.58': 6, 'model.layers.59': 6, 'model.layers.60': 6, 'model.layers.61': 6, 'model.layers.62': 6, 'model.layers.63': 7, 'model.layers.64': 7, 'model.layers.65': 7, 'model.layers.66': 7, 'model.layers.67': 7, 'model.layers.68': 7, 'model.layers.69': 7, 'model.layers.70': 7, 'model.layers.71': 7, 'model.final_layernorm': 7, 'lm_head': 7}
model = AutoModelForCausalLM.from_pretrained("ai21labs/AI21-Jamba-Large-1.6",
                                             torch_dtype=torch.bfloat16,
                                             attn_implementation="flash_attention_2",
                                             quantization_config=quantization_config,
                                             device_map=device_map)

tokenizer = AutoTokenizer.from_pretrained("ai21labs/AI21-Jamba-Large-1.6")

messages = [
   {"role": "system", "content": "You are an ancient oracle who speaks in cryptic but wise phrases, always hinting at deeper meanings."},
   {"role": "user", "content": "Hello!"},
]

input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt').to(model.device)

outputs = model.generate(input_ids, max_new_tokens=216)

# Decode the output
conversation = tokenizer.decode(outputs[0], skip_special_tokens=True)

# Split the conversation to get only the assistant's response
assistant_response = conversation.split(messages[-1]['content'])[1].strip()
print(assistant_response)
# Output: Seek and you shall find. The path is winding, but the journey is enlightening. What wisdom do you seek from the ancient echoes?
```

----------------------------------------

TITLE: Installing Transformers and Support Libraries using Bash
DESCRIPTION: Install the latest version of Transformers and additional libraries for dataset management, evaluation, and model optimization. Dependencies like 'datasets', 'evaluate', 'accelerate', and 'timm' are also installed to support various Transformers operations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md#2025-04-22_snippet_3

LANGUAGE: Bash
CODE:
```
!pip install -U transformers datasets evaluate accelerate timm
```

----------------------------------------

TITLE: Loading Models for Different Tasks - Python
DESCRIPTION: This snippet demonstrates how to load the same model architecture for multiple tasks using AutoModel classes for causal language modeling, sequence classification, and question answering within the Transformers ecosystem.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/models.md#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoModelForQuestionAnswering

# use the same API for 3 different tasks
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
model = AutoModelForSequenceClassification.from_pretrained("meta-llama/Llama-2-7b-hf")
model = AutoModelForQuestionAnswering.from_pretrained("meta-llama/Llama-2-7b-hf")
```

----------------------------------------

TITLE: Loading Pretrained Models for Different Tasks with AutoModelFor Classes in Python
DESCRIPTION: These snippets show how to load pretrained models for sequence classification and token classification tasks using AutoModelFor classes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/autoclass_tutorial.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")

>>> from transformers import AutoModelForTokenClassification

>>> model = AutoModelForTokenClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Padding Sentences in a Batch with Python
DESCRIPTION: This snippet demonstrates padding sentences in a batch to a uniform length.  Setting `padding=True` adds padding tokens (typically 0) to shorter sentences to match the length of the longest sentence in the batch. This is important because models expect inputs of the same size.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/preprocessing.md#_snippet_4

LANGUAGE: Python
CODE:
```
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True)
>>> print(encoded_input)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0], 
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], 
               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]], 
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}
```

----------------------------------------

TITLE: Push Model to Hub using Trainer - Python
DESCRIPTION: This snippet shows how to push a trained model to the Hugging Face Hub using the `push_to_hub` method available in the `Trainer` class. It automatically includes hyperparameters, training results, and framework versions in the model card.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/model_sharing.md#_snippet_8

LANGUAGE: python
CODE:
```
>>> trainer.push_to_hub()
```

----------------------------------------

TITLE: Tokenizing Text with a Transformers Tokenizer
DESCRIPTION: This code demonstrates how to tokenize text using a Hugging Face tokenizer, showing the resulting encoding with input IDs, token type IDs, and attention mask.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/quicktour.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
encoding = tokenizer("We are very happy to show you the ðŸ¤— Transformers library.")
print(encoding)
{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

----------------------------------------

TITLE: Automatic Speech Recognition with Transformers Pipeline in Python
DESCRIPTION: This code demonstrates how to use the `pipeline` for automatic speech recognition.  It imports necessary libraries, initializes the speech recognition pipeline with a specified model, and prepares to use it for transcribing audio.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_6

LANGUAGE: python
CODE:
```
>>> import torch
>>> from transformers import pipeline

>>> speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")
```

----------------------------------------

TITLE: Initializing Image-to-Image Pipeline for Super-Resolution
DESCRIPTION: Creates an image-to-image pipeline using the Swin2SR model for super-resolution tasks. It sets the device to GPU if available, otherwise CPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/image_to_image.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import pipeline

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
pipe = pipeline(task="image-to-image", model="caidas/swin2SR-lightweight-x2-64", device=device)
```

----------------------------------------

TITLE: Installing Transformers with conda
DESCRIPTION: Command for installing the Transformers library using conda package manager from the conda-forge channel. This is an alternative installation method to pip.
SOURCE: https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
conda install conda-forge::transformers
```

----------------------------------------

TITLE: Tokenizing a Sequence with AutoTokenizer in Python
DESCRIPTION: This code snippet demonstrates how to use a loaded tokenizer to tokenize a sequence of text. The `tokenizer` object processes the input sequence and returns a dictionary containing `input_ids`, `token_type_ids`, and `attention_mask`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/autoclass_tutorial.md#_snippet_1

LANGUAGE: Python
CODE:
```
>>> sequence = "In a hole in the ground there lived a hobbit."
>>> print(tokenizer(sequence))
{'input_ids': [101, 1999, 1037, 4920, 1999, 1996, 2598, 2045, 2973, 1037, 7570, 10322, 4183, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

----------------------------------------

TITLE: Performing Speech Recognition on Audio Data in Python
DESCRIPTION: This code snippet demonstrates how to perform automatic speech recognition on the first four audio samples from a dataset. It extracts the raw waveforms and passes them to the `speech_recognizer` pipeline. It then prints the transcribed text from each audio sample.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_9

LANGUAGE: python
CODE:
```
>>> result = speech_recognizer(dataset[:4]["audio"])
>>> print([d["text"] for d in result])
['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', "FONDERING HOW I'D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE", "I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS", 'HOW DO I FURN A JOINA COUT']
```

----------------------------------------

TITLE: Loading Mixed-Int8 Models with Multi-GPU Configuration
DESCRIPTION: Example of loading an 8-bit model with controlled GPU memory allocation across multiple GPUs using the max_memory parameter. Useful for efficiently distributing large models across available hardware.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/perf_infer_gpu_one.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
max_memory_mapping = {0: "1GB", 1: "2GB"}
model_name = "bigscience/bloom-3b"
model_8bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map="auto", load_in_8bit=True, max_memory=max_memory_mapping
)
```

----------------------------------------

TITLE: Generate Text with Llama 2 using AutoModel (Python)
DESCRIPTION: Shows how to load a Llama 2 model and tokenizer manually using `AutoModelForCausalLM` and `AutoTokenizer`. It prepares input tokens, generates output using the model, and decodes the result back into text. Requires PyTorch and the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llama2.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
)
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.float16,
    device_map="auto",
    attn_implementation="sdpa"
)
input_ids = tokenizer("Plants create energy through a process known as", return_tensors="pt").to("cuda")

output = model.generate(**input_ids, cache_implementation="static")
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Loading and Compiling TensorFlow Model for Sequence Classification
DESCRIPTION: Loads a pre-trained BERT model for sequence classification, compiles it with optimizer and loss function, and fits it to the prepared datasets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/training.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import tensorflow as tf
from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=5)

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=tf.metrics.SparseCategoricalAccuracy(),
)

model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)
```

----------------------------------------

TITLE: Multimodal Input Processing with AutoModel in Python
DESCRIPTION: This code snippet illustrates using the AutoProcessor and Llama4ForConditionalGeneration from the Transformers library for multimodal generation tasks. It processes image and text inputs together and generates descriptive text. Dependencies involve the Transformers library, PyTorch, and access to image URLs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llama4.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
from transformers import AutoProcessor, Llama4ForConditionalGeneration
import torch

model_id = "meta-llama/Llama-4-Scout-17B-16E-Instruct"

processor = AutoProcessor.from_pretrained(model_id)
model = Llama4ForConditionalGeneration.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.bfloat16,
)

img_url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg"
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "url": img_url},
            {"type": "text", "text": "Describe this image in two sentences."},
        ]
    },
]

inputs = processor.apply_chat_template(
    messages,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt",
).to(model.device)

outputs = model.generate(
    **inputs,
    max_new_tokens=256,
)

response = processor.batch_decode(outputs[:, inputs["input_ids"].shape[-1]:])[0]
print(response)
```

----------------------------------------

TITLE: Generating Text with a Decoder-Only LLM in Python
DESCRIPTION: This snippet demonstrates how to use the `text-generation` pipeline from Hugging Face Transformers to generate text using a decoder-only model (GPT-2). It initializes the generator, sets a prompt, and generates a text continuation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/prompting.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline
>>> import torch

>>> torch.manual_seed(0) # doctest: +IGNORE_RESULT

>>> generator = pipeline('text-generation', model = 'openai-community/gpt2')
>>> prompt = "Hello, I'm a language model"

>>> generator(prompt, max_length = 30)
[{'generated_text': "Hello, I'm a language model expert, so I'm a big believer in the concept that I know very well and then I try to look into"}]
```

----------------------------------------

TITLE: Inference with Question Answering Pipeline
DESCRIPTION: This code demonstrates how to use the trained question answering model for inference using the Hugging Face pipeline. It takes a question and context as input and returns the predicted answer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/question_answering.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
from transformers import pipeline

question = "How many programming languages does BLOOM support?"
context = "BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages."

question_answerer = pipeline("question-answering", model="my_awesome_qa_model")
question_answerer(question=question, context=context)
```

----------------------------------------

TITLE: Load Causal Language Model
DESCRIPTION: This code snippet loads a causal language model (Mistral-7B-v0.1) using `AutoModelForCausalLM` from the Transformers library.  It specifies `device_map="auto"` to automatically place the model on the available GPU and `load_in_4bit=True` to enable 4-bit quantization for reduced memory footprint.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/llm_tutorial.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained(
...     "mistralai/Mistral-7B-v0.1", device_map="auto", load_in_4bit=True
... )
```

----------------------------------------

TITLE: Loading a model in 4-bit precision with BitsAndBytesConfig
DESCRIPTION: Example of quantizing a BLOOM model to 4-bit precision, which provides even more memory savings compared to 8-bit quantization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=True)

model_4bit = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-1b7",
    device_map="auto",
    quantization_config=quantization_config
)
```

----------------------------------------

TITLE: Saving and Loading Models/Tokenizers in Transformers (Python)
DESCRIPTION: Illustrates how to save and load a `BertForSequenceClassification` model and its associated `BertTokenizer` using the `save_pretrained()` and `from_pretrained()` methods in the ðŸ¤— Transformers library. This snippet includes modifying the tokenizer by adding special tokens and resizing token embeddings, then saving and reloading the modified model and tokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/migration.md#_snippet_5

LANGUAGE: python
CODE:
```
### Carichiamo un modello e un tokenizer
model = BertForSequenceClassification.from_pretrained("google-bert/bert-base-uncased")
tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")

### Facciamo fare alcune cose al nostro modello e tokenizer
# Es: aggiungiamo nuovi token al vocabolario e agli embending del nostro modello
tokenizer.add_tokens(["[SPECIAL_TOKEN_1]", "[SPECIAL_TOKEN_2]"])
model.resize_token_embeddings(len(tokenizer))
# Alleniamo il nostro modello
train(model)

### Ora salviamo il nostro modello e il tokenizer in una cartella
model.save_pretrained("./my_saved_model_directory/")
tokenizer.save_pretrained("./my_saved_model_directory/")

### Ricarichiamo il modello e il tokenizer
model = BertForSequenceClassification.from_pretrained("./my_saved_model_directory/")
tokenizer = BertTokenizer.from_pretrained("./my_saved_model_directory/")
```

----------------------------------------

TITLE: Saving and Loading Models in TensorFlow
DESCRIPTION: Shows how to save a trained TensorFlow model and tokenizer, and then reload them for later use.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/quicktour.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
>>> tf_save_directory = "./tf_save_pretrained"
>>> tokenizer.save_pretrained(tf_save_directory)  # doctest: +IGNORE_RESULT
>>> tf_model.save_pretrained(tf_save_directory)

>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")
```

----------------------------------------

TITLE: Initializing and Running Training
DESCRIPTION: Sets up the Trainer with the model, training arguments, datasets, and necessary processing components. Executes the training process using the train() method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/object_detection.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=cppe5["train"],
    eval_dataset=cppe5["validation"],
    processing_class=image_processor,
    data_collator=collate_fn,
    compute_metrics=eval_compute_metrics_fn,
)

trainer.train()
```

----------------------------------------

TITLE: Loading T5 Model for Seq2Seq Learning with PyTorch
DESCRIPTION: Initializes a T5 model for sequence-to-sequence learning using AutoModelForSeq2SeqLM. The model will be fine-tuned for the summarization task using PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Compile and Train Model (TensorFlow)
DESCRIPTION: This snippet compiles and trains a TensorFlow model using Keras' `compile` and `fit` methods. It configures the optimizer and trains the model on the prepared TensorFlow dataset. A task-specific loss function is automatically used if not explicitly specified.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_45

LANGUAGE: python
CODE:
```
>>> from tensorflow.keras.optimizers import Adam

>>> model.compile(optimizer=Adam(3e-5))  # No loss argument!
>>> model.fit(tf_dataset)  # doctest: +SKIP
```

----------------------------------------

TITLE: Customizing Text Generation Output with Parameters in Python
DESCRIPTION: Demonstrates passing additional parameters to the pipeline to control the generation output, specifically requesting multiple sequences.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/pipeline_tutorial.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
>>> generator(
...     "Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone",
...     num_return_sequences=2,
... )  # doctest: +SKIP
```

----------------------------------------

TITLE: Tokenizing Example Data
DESCRIPTION: Demonstrates tokenization of input data using the DistilBERT tokenizer, with handling for tokens split into words.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/token_classification.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> example = wnut["train"][0]
>>> tokenized_input = tokenizer(example["tokens"], is_split_into_words=True)
>>> tokens = tokenizer.convert_ids_to_tokens(tokenized_input["input_ids"])
>>> tokens
['[CLS]', '@', 'paul', '##walk', 'it', "'", 's', 'the', 'view', 'from', 'where', 'i', "'", 'm', 'living', 'for', 'two', 'weeks', '.', 'empire', 'state', 'building', '=', 'es', '##b', '.', 'pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]']
```

----------------------------------------

TITLE: Distributed Training Code Diff
DESCRIPTION: This code shows the changes needed to convert a standard PyTorch training loop to use Hugging Face Accelerate for distributed training. It highlights the import of the `Accelerator` class, the initialization of the accelerator, the preparation of training components, and the replacement of `loss.backward()` with `accelerator.backward(loss)`. It also removes the explicit device placement.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/accelerate.md#_snippet_4

LANGUAGE: diff
CODE:
```
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)
optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

----------------------------------------

TITLE: Generate Text with Phi using AutoModel (Python)
DESCRIPTION: Shows text generation using `AutoTokenizer` and `AutoModelForCausalLM`. Loads the Phi-1 model with specific dtype, device map, and attention implementation. Tokenizes an input, generates output using `model.generate`, and decodes the result.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/phi.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1")
model = AutoModelForCausalLM.from_pretrained("microsoft/phi-1", torch_dtype=torch.float16, device_map="auto", attn_implementation="sdpa")

input_ids = tokenizer('''def print_prime(n):
   """
   Print all primes between 1 and n
   """''', return_tensors="pt").to("cuda")

output = model.generate(**input_ids, cache_implementation="static")
print(tokenizer.decode(output[0], skip_special_tokens=True))

```

----------------------------------------

TITLE: Quantization with Bitsandbytes in Python
DESCRIPTION: Demonstrates loading a quantized model using the BitsAndBytesConfig for optimizing memory efficiency. Requires installation of bitsandbytes and a compatible GPU/accelerator. Intended to reduce memory usage while maintaining model performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llava_next_video.md#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
from transformers import LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor

# specify how to quantize the model
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

model = LlavaNextVideoForConditionalGeneration.from_pretrained("llava-hf/LLaVA-NeXT-Video-7B-hf", quantization_config=quantization_config, device_map="auto")
```

----------------------------------------

TITLE: Installing Transformers with Flax
DESCRIPTION: Command to install both Hugging Face Transformers and Flax in a single line using pip.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/installation.md#2025-04-22_snippet_7

LANGUAGE: bash
CODE:
```
pip install 'transformers[flax]'
```

----------------------------------------

TITLE: Train Model (PyTorch)
DESCRIPTION: This snippet initiates the training process by calling the `train` method on the `Trainer` object. This starts the training loop, using the configured model, datasets, and training arguments.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_40

LANGUAGE: python
CODE:
```
>>> trainer.train()  # doctest: +SKIP
```

----------------------------------------

TITLE: Training a Masked Language Model with PyTorch Trainer
DESCRIPTION: Sets up training arguments, initializes the Trainer, and trains a masked language model. Includes evaluation and pushing the model to the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/masked_language_modeling.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
training_args = TrainingArguments(
    output_dir="my_awesome_eli5_mlm_model",
    eval_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_dataset["train"],
    eval_dataset=lm_dataset["test"],
    data_collator=data_collator,
)

trainer.train()
```

----------------------------------------

TITLE: Encode a Sequence with the Tokenizer Python
DESCRIPTION: This snippet demonstrates how to encode a sequence of text into input IDs using the tokenizer. The tokenizer converts the sequence into a dictionary containing the input IDs and other information needed by the model.  It depends on the `tokenizer` object and the input `sequence`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/glossary.md#_snippet_9

LANGUAGE: python
CODE:
```
>>> inputs = tokenizer(sequence)
```

----------------------------------------

TITLE: Loading Model, Tokenizer, and Dataset (PyTorch)
DESCRIPTION: This code snippet demonstrates how to load a pre-trained model, tokenizer, and dataset using the Hugging Face Transformers library and the Datasets library. It imports the necessary classes and functions, and then uses them to load the DistilBERT model, its corresponding tokenizer, and the Rotten Tomatoes dataset. These components are essential for training a sequence classification model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from datasets import load_dataset

model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
dataset = load_dataset("rotten_tomatoes")
```

----------------------------------------

TITLE: Running Speech Recognition on Audio File
DESCRIPTION: This code snippet demonstrates how to use the initialized ASR pipeline to transcribe an audio file. It passes the URL of an audio file to the transcriber pipeline.  The pipeline processes the audio and returns a dictionary containing the transcribed text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/tutoriel_pipeline.md#_snippet_1

LANGUAGE: Python
CODE:
```
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES'}
```

----------------------------------------

TITLE: Inference with Qwen2-VL in Python
DESCRIPTION: This snippet demonstrates how to use the Qwen2-VL model for conducting inference on single media inputs, both images and videos. The model is loaded in half-precision mode on available devices, and input processing is handled via the AutoProcessor. The generated output describes the media content. Key dependencies include the 'transformers' package, and the inputs involve media URLs or paths with desired text prompts. Outputs include the processed text descriptions of the provided media.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2_vl.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor

# Load the model in half-precision on the available device(s)
model = Qwen2VLForConditionalGeneration.from_pretrained("Qwen/Qwen2-VL-7B-Instruct", device_map="auto")
processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-7B-Instruct")


conversation = [
    {
        "role":"user",
        "content":[
            {
                "type":"image",
                "url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"
            },
            {
                "type":"text",
                "text":"Describe this image."
            }
        ]
    }
]

inputs = processor.apply_chat_template(
    conversation,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt"
).to(model.device)

# Inference: Generation of the output
output_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]
output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
print(output_text)



# Video
conversation = [
    {
        "role": "user",
        "content": [
            {"type": "video", "path": "/path/to/video.mp4"},
            {"type": "text", "text": "What happened in the video?"},
        ],
    }
]

inputs = processor.apply_chat_template(
    conversation,
    video_fps=1,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt"
).to(model.device)


# Inference: Generation of the output
output_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]
output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
print(output_text)
```

----------------------------------------

TITLE: Applying Tokenization to the Dataset
DESCRIPTION: Applies the preprocessing function to the entire dataset using map() with batched processing for efficiency, removing unnecessary columns to focus only on the tokenized data needed for language modeling.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/language_modeling.md#2025-04-23_snippet_7

LANGUAGE: python
CODE:
```
>>> tokenized_eli5 = eli5.map(
...     preprocess_function,
...     batched=True,
...     num_proc=4,
...     remove_columns=eli5["train"].column_names,
... )
```

----------------------------------------

TITLE: Setting up Trainer for Model Sharing in Python
DESCRIPTION: Demonstrates how to configure the Trainer class with the necessary arguments for model training and sharing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/model_sharing.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=small_train_dataset,
...     eval_dataset=small_eval_dataset,
...     compute_metrics=compute_metrics,
... )
```

----------------------------------------

TITLE: Loading AWQ Quantized Model with Default Settings
DESCRIPTION: Python code to load an AWQ quantized model using from_pretrained with device_map to place the model on GPU. By default, weights are loaded in fp16 format for performance reasons.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/awq.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "TheBloke/zephyr-7B-alpha-AWQ"
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="cuda:0")
```

----------------------------------------

TITLE: Trainer Initialization - Python
DESCRIPTION: This snippet demonstrates how to initialize a `Trainer` with the model, training arguments, and datasets. It includes the `compute_metrics` function for evaluating the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/model_sharing.md#_snippet_7

LANGUAGE: python
CODE:
```
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=small_train_dataset,
...     eval_dataset=small_eval_dataset,
...     compute_metrics=compute_metrics,
... )
```

----------------------------------------

TITLE: Initializing Trainer for GIT Model Fine-tuning
DESCRIPTION: Sets up the Trainer object with the model, training arguments, datasets, and evaluation metric for fine-tuning.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/image_captioning.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    compute_metrics=compute_metrics,
)

trainer.train()

trainer.push_to_hub()
```

----------------------------------------

TITLE: Configuring a LoRA Adapter for Training
DESCRIPTION: Code to define a LoRA adapter configuration with specific hyperparameters for causal language modeling, setting up parameters like rank, alpha, and dropout rate.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/peft.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
from peft import LoraConfig

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)
```

----------------------------------------

TITLE: Saving Custom Configuration to Disk in Python
DESCRIPTION: Demonstrates how to save a custom configuration to a specified directory using the save_pretrained method, which will create a JSON configuration file in the target directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/create_a_model.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> my_config.save_pretrained(save_directory="./your_model_save_path")
```

----------------------------------------

TITLE: Inference on CUDA with quantized model (Python)
DESCRIPTION: Performs inference on CUDA using a quantized model.  It loads the quantized model, sets the device to 'cuda', tokenizes the input text, and generates text using the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/auto_round.md#_snippet_7

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "OPEA/Qwen2.5-1.5B-Instruct-int4-sym-inc"
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="cuda", torch_dtype="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)
text = "There is a girl who likes adventure,"
inputs = tokenizer(text, return_tensors="pt").to(model.device)
print(tokenizer.decode(model.generate(**inputs, max_new_tokens=50, do_sample=False)[0]))
```

----------------------------------------

TITLE: Using FLAN-T5 for Text Generation in Python
DESCRIPTION: This code snippet demonstrates how to load a pre-trained FLAN-T5 model and tokenizer, and use them to generate text based on a given input prompt. It uses the 'google/flan-t5-small' model variant.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/flan-t5.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")

inputs = tokenizer("A step by step recipe to make bolognese pasta:", return_tensors="pt")
outputs = model.generate(**inputs)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
```

----------------------------------------

TITLE: Loading DistilBERT Tokenizer
DESCRIPTION: Initializes the DistilBERT tokenizer from the Hugging Face Transformers library, which will be used to preprocess the text data for the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Training with Push to Hub Integration
DESCRIPTION: Example of configuring model training to automatically push to the Hub using training arguments and callbacks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/model_sharing.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
training_args = TrainingArguments(output_dir="my-awesome-model", push_to_hub=True)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

trainer.push_to_hub()
```

LANGUAGE: python
CODE:
```
from transformers import PushToHubCallback

push_to_hub_callback = PushToHubCallback(
    output_dir="./your_model_save_path", tokenizer=tokenizer, hub_model_id="your-username/my-awesome-model"
)

model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3, callbacks=push_to_hub_callback)
```

----------------------------------------

TITLE: Converting to Better Transformer
DESCRIPTION: Shows how to convert the Bark model to Better Transformer format for 20-30% speed improvement through kernel fusion.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/bark.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
model =  model.to_bettertransformer()
```

----------------------------------------

TITLE: Converting Tokenized Inputs to TensorFlow Tensors
DESCRIPTION: Demonstrates how to convert tokenized inputs into TensorFlow tensors for use with TensorFlow-based models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/preprocessing.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="tf")
print(encoded_input)
```

----------------------------------------

TITLE: Loading MGP-STR Model and Preprocessing Image in Python
DESCRIPTION: This code snippet demonstrates how to load the MGP-STR model and the associated processor for preprocessing images. It also includes a step for fetching an image from a URL and preparing it for the model input, followed by making predictions using the model. It requires the 'transformers', 'requests', and 'PIL' libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mgp-str.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
>>> from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition
>>> import requests
>>> from PIL import Image

>>> processor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')
>>> model = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')

>>> # load image from the IIIT-5k dataset
>>> url = "https://i.postimg.cc/ZKwLg2Gw/367-14.png"
>>> image = Image.open(requests.get(url, stream=True).raw).convert("RGB")

>>> pixel_values = processor(images=image, return_tensors="pt").pixel_values
>>> outputs = model(pixel_values)

>>> generated_text = processor.batch_decode(outputs.logits)['generated_text']
```

----------------------------------------

TITLE: PyTorch Model Training Setup
DESCRIPTION: Configures and executes model training using the Hugging Face Trainer API with PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/tasks/sequence_classification.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

>>> model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased", num_labels=2)

>>> training_args = TrainingArguments(
...     output_dir="./results",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     num_train_epochs=5,
...     weight_decay=0.01,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_imdb["train"],
...     eval_dataset=tokenized_imdb["test"],
...     processing_class=tokenizer,
...     data_collator=data_collator,
... )

>>> trainer.train()
```

----------------------------------------

TITLE: Loading a 4-bit model with automatic data type configuration
DESCRIPTION: Example of loading a model in 4-bit with torch_dtype set to 'auto', which uses the data type defined in the model's config.json file.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=True)

model_4bit = AutoModelForCausalLM.from_pretrained(
    "facebook/opt-350m",
    device_map="auto",
    quantization_config=quantization_config, 
    torch_dtype="auto"
)
model_4bit.model.decoder.layers[-1].final_layer_norm.weight.dtype
```

----------------------------------------

TITLE: Demonstrating Attention Masks with BERT Tokenizer in Python
DESCRIPTION: This code snippet shows how to use the BERT tokenizer to encode two sequences of different lengths and how attention masks are applied to handle padded tokens in a batch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/glossary.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-cased")

>>> sequence_a = "This is a short sequence."
>>> sequence_b = "This is a rather long sequence. It is at least longer than the sequence A."

>>> encoded_sequence_a = tokenizer(sequence_a)["input_ids"]
>>> encoded_sequence_b = tokenizer(sequence_b)["input_ids"]
```

----------------------------------------

TITLE: Tokenize and Pad a Batch (PyTorch)
DESCRIPTION: Tokenizes a batch of text strings, applies padding and truncation to ensure uniform length, and returns PyTorch tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_16

LANGUAGE: Python
CODE:
```
>>> pt_batch = tokenizer(
...     ["We are very happy to show you the ðŸ¤— Transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="pt",
... )
```

----------------------------------------

TITLE: Return TensorFlow Tensors with Tokenizer in Python
DESCRIPTION: This snippet converts the tokenized output into TensorFlow tensors by setting `return_tensors='tf'`.  This is necessary for using the data with TensorFlow-based models. The padding and truncation arguments should be set appropriately before returning the tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/preprocessing.md#_snippet_7

LANGUAGE: Python
CODE:
```
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors="tf")
>>> print(encoded_input)
{'input_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[  101,   153,  7719, 21490,  1122,  1114,  9582,  1623,   102],
       [  101,  5226,  1122,  9649,  1199,  2610,  1236,   102,     0]],
      dtype=int32)>,
 'token_type_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,
 'attention_mask': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 0]], dtype=int32)>}
```

----------------------------------------

TITLE: Loading DistilGPT2 Model for Causal Language Modeling in PyTorch
DESCRIPTION: This snippet demonstrates how to load the DistilGPT2 model using AutoModelForCausalLM for causal language modeling tasks in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer

model = AutoModelForCausalLM.from_pretrained("distilbert/distilgpt2")
```

----------------------------------------

TITLE: Configuring Training Arguments
DESCRIPTION: This configures the training arguments for the Trainer, specifying the output directory, evaluation strategy, learning rate, batch size, number of epochs, weight decay, and push-to-hub settings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/multiple_choice.md#_snippet_11

LANGUAGE: python
CODE:
```
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_swag_model",
...     eval_strategy="epoch",
...     save_strategy="epoch",
...     load_best_model_at_end=True,
...     learning_rate=5e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     num_train_epochs=3,
...     weight_decay=0.01,
...     push_to_hub=True,
... )
```

----------------------------------------

TITLE: Loading and Using UnivNet Model - Python
DESCRIPTION: This Python snippet demonstrates how to load the UnivNet model and feature extractor from HuggingFace Transformers, preprocess audio data, and generate speech waveforms. It requires the 'torch', 'scipy', and 'datasets' libraries. The snippet shows how to handle input features, manage noise sequences and output clean waveforms. The generated audio is finally saved as a WAV file. For proper output, ensure that the input data matches the expected sampling rate and feature dimensions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/univnet.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
import torch
from scipy.io.wavfile import write
from datasets import Audio, load_dataset

from transformers import UnivNetFeatureExtractor, UnivNetModel

model_id_or_path = "dg845/univnet-dev"
model = UnivNetModel.from_pretrained(model_id_or_path)
feature_extractor = UnivNetFeatureExtractor.from_pretrained(model_id_or_path)

ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
# Resample the audio to the model and feature extractor's sampling rate.
ds = ds.cast_column("audio", Audio(sampling_rate=feature_extractor.sampling_rate))
# Pad the end of the converted waveforms to reduce artifacts at the end of the output audio samples.
inputs = feature_extractor(
    ds[0]["audio"]["array"], sampling_rate=ds[0]["audio"]["sampling_rate"], pad_end=True, return_tensors="pt"
)

with torch.no_grad():
    audio = model(**inputs)

# Remove the extra padding at the end of the output.
audio = feature_extractor.batch_decode(**audio)[0]
# Convert to wav file
write("sample_audio.wav", feature_extractor.sampling_rate, audio)
```

----------------------------------------

TITLE: Pushing a Trained Model to Hugging Face Hub with PyTorch
DESCRIPTION: Shares the trained model to the Hugging Face Hub using the Trainer's push_to_hub method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/masked_language_modeling.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
trainer.push_to_hub()
```

----------------------------------------

TITLE: Running TGI with Docker to Serve a Model
DESCRIPTION: This Docker command is used to start a TGI server for a specified Transformer model using GPU support. It maps a directory to the container for data storage and opens port 8080 for external access. Add '--trust-remote-code' to support custom models. The command requires Docker installed and access to the specified model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/serving.md#2025-04-22_snippet_0

LANGUAGE: docker
CODE:
```
docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest --model-id gpt2
```

LANGUAGE: docker
CODE:
```
docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest --model-id <CUSTOM_MODEL_ID> --trust-remote-code
```

----------------------------------------

TITLE: Distributing 8-bit model across multiple GPUs with bitsandbytes
DESCRIPTION: This code distributes an 8-bit model across multiple GPUs by specifying a `max_memory_mapping`. It sets memory allocation for GPU 0 and GPU 1 to 16GB each, ensuring that the model is distributed effectively across available resources.  The `device_map` is set to "auto" to leverage Accelerate's automatic device placement.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_one.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
max_memory_mapping = {0: "16GB", 1: "16GB"}
model_8bit = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B", device_map="auto", quantization_config=quantization_config, max_memory=max_memory_mapping
)
```

----------------------------------------

TITLE: Summarization Task with Text Summarization Pipeline in Python
DESCRIPTION: This example illustrates using the Hugging Face Transformers for text summarization, employing the Pegasus Billsum model. Inputs are text passages, and the output is a concise summary. Ensure 'transformers' is installed before execution.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import pipeline

pipeline = pipeline(task="summarization", model="google/pegasus-billsum")
pipeline("Section was formerly set out as section 44 of this title. ... ")
[{'summary_text': 'Instructs the Secretary of the Interior to convey... and interest of the United States in...'}]
```

----------------------------------------

TITLE: Initialize Model for Training (TensorFlow)
DESCRIPTION: This snippet initializes a TensorFlow model for sequence classification using `TFAutoModelForSequenceClassification.from_pretrained`. It loads a pre-trained DistilBERT model. This is the first step in setting up the training loop.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_41

LANGUAGE: python
CODE:
```
>>> from transformers import TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Loading PEFT Adapter Model
DESCRIPTION: Example of loading a PEFT adapter model using AutoModelForCausalLM class.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/peft.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id)
```

----------------------------------------

TITLE: Visual Question Answering using Transformers Pipeline (Python)
DESCRIPTION: Shows how to use the `visual-question-answering` pipeline with a specified model. It takes an image URL and a text question as input and returns the model's generated answer to the question about the image content.
SOURCE: https://github.com/huggingface/transformers/blob/main/README.md#_snippet_8

LANGUAGE: Python
CODE:
```
from transformers import pipeline

pipeline = pipeline(task="visual-question-answering", model="Salesforce/blip-vqa-base")
pipeline(
    image="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg",
    question="What is in the image?",
)
[{'answer': 'statue of liberty'}]
```

----------------------------------------

TITLE: Demonstrating Incorrect Output with Padding Tokens Not Masked
DESCRIPTION: This example showcases how padding tokens can lead to incorrect outputs if they are not properly masked using an attention mask. The code provides example `input_ids` containing padding tokens and shows the resulting output without masking.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/troubleshooting.md#_snippet_6

LANGUAGE: Python
CODE:
```
>>> input_ids = torch.tensor([[7592, 2057, 2097, 2393, 9611, 2115], [7592, 0, 0, 0, 0, 0]])
>>> output = model(input_ids)
>>> print(output.logits)
tensor([[ 0.0082, -0.2307],
        [ 0.1317, -0.1683]], grad_fn=<AddmmBackward0>)
```

----------------------------------------

TITLE: Classifying Text with DistilBERT using Pipeline API
DESCRIPTION: This snippet demonstrates how to use the high-level `pipeline` API from the `transformers` library to perform text classification with a finetuned DistilBERT model. It initializes a text classification pipeline specifying the task, model name, desired data type (`torch.float16`), and device (GPU 0), then processes an input string and prints the classification result.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/distilbert.md#_snippet_0

LANGUAGE: python
CODE:
```
from transformers import pipeline

classifier = pipeline(
    task="text-classification",
    model="distilbert-base-uncased-finetuned-sst-2-english",
    torch_dtype=torch.float16,
    device=0
)

result = classifier("I love using Hugging Face Transformers!")
print(result)
# Output: [{'label': 'POSITIVE', 'score': 0.9998}]
```

----------------------------------------

TITLE: Tokenize Input Text
DESCRIPTION: This code snippet tokenizes input text using `AutoTokenizer`. It initializes the tokenizer for the Mistral-7B-v0.1 model, sets the `padding_side` to 'left', and converts the input text 'A list of colors: red, blue' into a tensor format suitable for the model, moving it to the GPU with `.to("cuda")`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/llm_tutorial.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", padding_side="left")
>>> model_inputs = tokenizer(["A list of colors: red, blue"], return_tensors="pt").to("cuda")
```

----------------------------------------

TITLE: Text Classification Pipeline for Sentiment Analysis using Hugging Face Transformers
DESCRIPTION: This code snippet shows how to use the text classification pipeline from Hugging Face Transformers for sentiment analysis. It classifies a given text as positive or negative and returns the classification score.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/task_summary.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers import pipeline

classifier = pipeline(task="sentiment-analysis")
preds = classifier("Hugging Face is the best thing since sliced bread!")
preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
preds
```

----------------------------------------

TITLE: Token Classification Inference using Pipeline
DESCRIPTION: This snippet showcases the usage of the `pipeline` for inference with a token classification model.  The pipeline abstracts the tokenization, model prediction, and result interpretation steps.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_27

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline("ner", model="stevhliu/my_awesome_wnut_model")
>>> classifier(text)
```

----------------------------------------

TITLE: Generate Text with Phi using Pipeline (Python)
DESCRIPTION: Demonstrates how to use the `transformers` `pipeline` utility for text generation with the Phi-1.5 model. It initializes the pipeline with specified task, model, device, and torch dtype, then runs it on an input prompt.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/phi.md#_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipeline = pipeline(task="text-generation", model="microsoft/phi-1.5", device=0, torch_dtype=torch.bfloat16)
pipeline("pipeline('''def print_prime(n): \"\"\" Print all primes between 1 and n\"\"\"''')")

```

----------------------------------------

TITLE: Using Text Generation Pipeline with GPT-2 in Python
DESCRIPTION: This code demonstrates how to use the Transformers 'text-generation' pipeline with a decoder-only model (GPT-2) to generate text from a prompt.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/prompting.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline
>>> import torch

>>> torch.manual_seed(0) # doctest: +IGNORE_RESULT

>>> generator = pipeline('text-generation', model = 'openai-community/gpt2')
>>> prompt = "Hello, I'm a language model"

>>> generator(prompt, max_length = 30)
[{'generated_text': "Hello, I'm a language model programmer so you can use some of my stuff. But you also need some sort of a C program to run."}]
```

----------------------------------------

TITLE: Running Int8 model on multiple GPUs
DESCRIPTION: Loads and runs a mixed-Int8 quantized model across multiple GPUs using `AutoModelForCausalLM` and `BitsAndBytesConfig`.  The `device_map` argument distributes the model across GPUs. The `max_memory` argument allows for memory allocation control.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_infer_gpu_one.md#2025-04-22_snippet_15

LANGUAGE: Python
CODE:
```
"model_name = \"bigscience/bloom-2b5\"\nmodel_8bit = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True))"
```

----------------------------------------

TITLE: Loading a PEFT Adapter Model from Hub
DESCRIPTION: Code to load a pre-trained PEFT adapter model directly from the Hugging Face Hub using AutoModelForCausalLM, which automatically handles the adapter configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/peft.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id)
```

----------------------------------------

TITLE: Initializing and Running the Hugging Face Trainer
DESCRIPTION: Creates a Trainer object with the model, training arguments, datasets, and evaluation function, then starts the fine-tuning process with the train method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/training.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()
```

----------------------------------------

TITLE: DÃ©tection d'objets avec pipeline Transformers
DESCRIPTION: Utilise le pipeline de dÃ©tection d'objets avec le modÃ¨le par dÃ©faut pour localiser et identifier des objets dans une image. Le code charge le pipeline, analyse une image de chat et affiche la position des objets dÃ©tectÃ©s avec leurs scores de confiance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/task_summary.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> detector = pipeline(task="object-detection")
>>> preds = detector(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"], "box": pred["box"]} for pred in preds]
>>> preds
[{'score': 0.9865,
  'label': 'cat',
  'box': {'xmin': 178, 'ymin': 154, 'xmax': 882, 'ymax': 598}}]
```

----------------------------------------

TITLE: Creating a Sentiment Analysis Pipeline in Italian
DESCRIPTION: Code to import and initialize a sentiment analysis pipeline using a pre-trained Italian model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/quicktour.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
>>> from transformers import pipeline

>>> classificatore = pipeline("sentiment-analysis", model="MilaNLProc/feel-it-italian-sentiment")
```

----------------------------------------

TITLE: Demonstrating Incorrect Output with Unmasked Padding Tokens in Python
DESCRIPTION: This snippet shows how using input_ids with padding tokens without an attention_mask can lead to incorrect model outputs in Transformers.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/troubleshooting.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> input_ids = torch.tensor([[7592, 2057, 2097, 2393, 9611, 2115], [7592, 0, 0, 0, 0, 0]])
>>> output = model(input_ids)
>>> print(output.logits)
tensor([[ 0.0082, -0.2307],
        [ 0.1317, -0.1683]], grad_fn=<AddmmBackward0>)
```

----------------------------------------

TITLE: Detailed Implementation of Chat Generation with Transformers
DESCRIPTION: Shows the low-level implementation details of the chat pipeline, including loading the model and tokenizer, applying chat templates, tokenizing the input, generating a response, and decoding the output tokens.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/conversations.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# ìž…ë ¥ê°’ì„ ì‚¬ì „ì— ì¤€ë¹„í•´ ë†“ìŠµë‹ˆë‹¤
chat = [
    {"role": "system", "content": "You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986."},
    {"role": "user", "content": "Hey, can you tell me any fun things to do in New York?"}
]

# 1: ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤
model = AutoModelForCausalLM.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct", device_map="auto", torch_dtype=torch.bfloat16)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct")

# 2: ì±„íŒ… í…œí”Œë¦¿ì— ì ìš©í•©ë‹ˆë‹¤
formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
print("Formatted chat:\n", formatted_chat)

# 3: ì±„íŒ…ì„ í† í°í™”í•©ë‹ˆë‹¤ (ë°”ë¡œ ì´ì „ ê³¼ì •ì—ì„œ tokenized=Trueë¡œ ì„¤ì •í•˜ë©´ í•œêº¼ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤)
inputs = tokenizer(formatted_chat, return_tensors="pt", add_special_tokens=False)
# í† í°í™”ëœ ìž…ë ¥ê°’ì„ ëª¨ë¸ì´ ì˜¬ë¼ì™€ ìžˆëŠ” ê¸°ê¸°(CPU/GPU)ë¡œ ì˜®ê¹ë‹ˆë‹¤.
inputs = {key: tensor.to(model.device) for key, tensor in inputs.items()}
print("Tokenized inputs:\n", inputs)

# 4: ëª¨ë¸ë¡œë¶€í„° ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤
outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.1)
print("Generated tokens:\n", outputs)

# 5: ëª¨ë¸ì´ ì¶œë ¥í•œ í† í°ì„ ë‹¤ì‹œ ë¬¸ìžì—´ë¡œ ë””ì½”ë”©í•©ë‹ˆë‹¤
decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)
print("Decoded output:\n", decoded_output)
```

----------------------------------------

TITLE: Trainer Setup and Training
DESCRIPTION: Initializes and executes the training process using the Trainer API with the configured model, datasets, and evaluation metrics.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/training.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    compute_metrics=compute_metrics,
)
trainer.train()
```

----------------------------------------

TITLE: Applying Chat Template for Mistral-7B-Instruct using Python
DESCRIPTION: This snippet demonstrates how to create a chat input structure for the Mistral-7B-Instruct model. It utilizes the `AutoTokenizer` from the Transformers library to apply a chat template for formatting conversations. The chat structure is defined as a list of dictionaries with user and assistant roles.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")
chat = [
  {"role": "user", "content": "Hello, how are you?"},
  {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
  {"role": "user", "content": "I'd like to show off how chat templating works!"},
]

tokenizer.apply_chat_template(chat, tokenize=False)
```

----------------------------------------

TITLE: Loading a Quantized Model from the Hub
DESCRIPTION: This code loads a quantized model from the Hugging Face Hub. It uses `device_map='auto'` to automatically distribute the model across available GPUs for efficient memory usage and faster loading.  Requires the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/gptq.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
"from transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"{your_username}/opt-125m-gptq\", device_map=\"auto\")"
```

----------------------------------------

TITLE: Generating Text with T5 using AutoModel (PyTorch)
DESCRIPTION: Shows how to perform text-to-text generation (translation) using the lower-level AutoModelForSeq2SeqLM and AutoTokenizer classes. It loads the tokenizer and model for 'google-t5/t5-base', tokenizes the input text with the task prefix, generates the output sequence using the model with static caching, and decodes the result back into a human-readable string.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/t5.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    "google-t5/t5-base"
    )
model = AutoModelForSeq2SeqLM.from_pretrained(
    "google-t5/t5-base",
    torch_dtype=torch.float16,
    device_map="auto"
    )

input_ids = tokenizer("translate English to French: The weather is nice today.", return_tensors="pt").to("cuda")

output = model.generate(**input_ids, cache_implementation="static")
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Truncating Tokenized Inputs
DESCRIPTION: Demonstrates how to truncate tokenized sequences to a maximum length accepted by the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/preprocessing.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)
print(encoded_input)
```

----------------------------------------

TITLE: Classification d'images avec pipeline Transformers
DESCRIPTION: Utilise le pipeline de classification d'images avec le modÃ¨le par dÃ©faut pour identifier le contenu d'une image. Le code charge le pipeline, analyse une image de chat et affiche les prÃ©dictions des catÃ©gories avec leurs scores de confiance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/task_summary.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline(task="image-classification")
>>> preds = classifier(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> print(*preds, sep="\n")
{'score': 0.4335, 'label': 'lynx, catamount'}
{'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}
{'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}
{'score': 0.0239, 'label': 'Egyptian cat'}
{'score': 0.0229, 'label': 'tiger cat'}
```

----------------------------------------

TITLE: Converting Decoder Model to BetterTransformer in PyTorch
DESCRIPTION: This code snippet demonstrates how to convert a decoder-based model (e.g., GPT, T5, Llama) to BetterTransformer, which utilizes PyTorch's native fast execution paths and optimized kernels like Flash Attention. It requires PyTorch 2.0 or later. The code loads a pretrained model, converts it to BetterTransformer, and then uses it for training or inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_infer_gpu_many.md#_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")
# convert the model to BetterTransformer
model.to_bettertransformer()

# Use it for training or inference
```

----------------------------------------

TITLE: Using a Pipeline for Inference
DESCRIPTION: Demonstrates how to use the fine-tuned model for inference with the visual-question-answering pipeline, which simplifies the process of getting predictions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_question_answering.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> pipe = pipeline("visual-question-answering", model="MariaK/vilt_finetuned_200")
```

----------------------------------------

TITLE: Creating an Automatic Speech Recognition Pipeline in Python
DESCRIPTION: Creates a pipeline for automatic speech recognition using the default model and preprocessor.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/hi/pipeline_tutorial.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import pipeline

transcriber = pipeline(task="automatic-speech-recognition")
```

----------------------------------------

TITLE: Launching Training Script with Accelerate in Bash
DESCRIPTION: This bash command demonstrates how to launch a training script using `accelerate launch`.  It specifies the path to the training script, model name, task name, and other training parameters. This example uses the `run_glue.py` script.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/trainer.md#_snippet_26

LANGUAGE: bash
CODE:
```
accelerate launch \
    ./examples/pytorch/text-classification/run_glue.py \
    --model_name_or_path google-bert/bert-base-cased \
    --task_name $TASK_NAME \
    --do_train \
    --do_eval \
    --max_seq_length 128 \
    --per_device_train_batch_size 16 \
    --learning_rate 5e-5 \
    --num_train_epochs 3 \
    --output_dir /tmp/$TASK_NAME/ \
    --overwrite_output_dir
```

----------------------------------------

TITLE: Loading and Encoding Dataset with Transformers in Python
DESCRIPTION: This code snippet shows how to load the WikiText-2 dataset using the 'datasets' library and encode it with a pre-trained tokenizer from the Transformers library. The 'load_dataset' function is used to load the dataset, and the tokenizer processes it to generate input tensors. The main dependencies are the 'datasets' and 'transformers' libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perplexity.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
from datasets import load_dataset

test = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")
encodings = tokenizer("\n\n".join(test["text"]), return_tensors="pt")
```

----------------------------------------

TITLE: Image Classification Pipeline with Gradio
DESCRIPTION: Creates an image classification pipeline using the `transformers` library and builds a Gradio interface for it. The interface allows users to drag and drop images and see the classification results.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/tutoriel_pipeline.md#_snippet_8

LANGUAGE: Python
CODE:
```
from transformers import pipeline
import gradio as gr

pipe = pipeline("image-classification", model="google/vit-base-patch16-224")

gr.Interface.from_pipeline(pipe).launch()
```

----------------------------------------

TITLE: Batch Tokenization with Padding in PyTorch
DESCRIPTION: Demonstrates batch tokenization with padding and truncation for PyTorch models. The tokenizer processes a list of sentences and returns tensors suitable for PyTorch models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/quicktour.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> pt_batch = tokenizer(
...     ["We are very happy to show you the ðŸ¤— transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="pt",
... )
```

----------------------------------------

TITLE: Running FP4 model on multiple GPUs
DESCRIPTION: Loads and runs an FP4 quantized model across multiple GPUs using `AutoModelForCausalLM`. The `device_map='auto'` argument distributes the model across available GPUs. The `max_memory` argument allows specifying memory allocation per GPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_infer_gpu_one.md#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
"model_name = \"bigscience/bloom-2b5\"\nmodel_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)"
```

----------------------------------------

TITLE: Loading a PEFT Adapter Model with 8-bit Quantization
DESCRIPTION: Code to load a PEFT adapter model with 8-bit quantization using BitsAndBytesConfig, which reduces memory usage while maintaining model performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/peft.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```

----------------------------------------

TITLE: Quantizing Mixtral Model with BitsAndBytesConfig in Python
DESCRIPTION: This snippet demonstrates how to quantize the Mixtral model from Hugging Face Transformers by setting up a BitsAndBytesConfig to enable 4-bit quantization. The model and tokenizer are loaded accordingly, and a sample message exchange is processed to showcase the model's output. Key parameters include load_in_4bit for model size reduction and device_map for automatic device allocation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mixtral.md#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
>>> import torch
>>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

>>> # specify how to quantize the model
>>> quantization_config = BitsAndBytesConfig(
...         load_in_4bit=True,
...         bnb_4bit_quant_type="nf4",
...         bnb_4bit_compute_dtype="torch.float16",
... )

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mixtral-8x7B-Instruct-v0.1", quantization_config=True, device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mixtral-8x7B-Instruct-v0.1")

>>> prompt = "My favourite condiment is"

>>> messages = [
...     {"role": "user", "content": "What is your favourite condiment?"},
...     {"role": "assistant", "content": "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!"},
...     {"role": "user", "content": "Do you have mayonnaise recipes?"}
... ]

>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")

>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"The expected output"
```

----------------------------------------

TITLE: Loading Large Models with Accelerate in PyTorch
DESCRIPTION: Demonstrates how to load a large language model using the Accelerate library to manage memory usage efficiently. This allows loading models that may not fit entirely in RAM by leveraging CPU and disk offloading.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/model.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSeq2SeqLM

t0pp = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0pp", low_cpu_mem_usage=True)
```

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSeq2SeqLM

t0pp = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0pp", device_map="auto")
```

----------------------------------------

TITLE: Training T5 Translation Model with TensorFlow
DESCRIPTION: Executes the model training process using Keras fit method with the prepared datasets, specified number of epochs, and configured callbacks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/translation.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)
```

----------------------------------------

TITLE: Loading a Pre-trained Tokenizer with AutoTokenizer in Python
DESCRIPTION: This code demonstrates how to load a pre-trained BERT tokenizer using the AutoTokenizer class and apply it to a text sequence, which converts the text into a format that can be processed by the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/autoclass_tutorial.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
```

LANGUAGE: python
CODE:
```
>>> sequence = "In a hole in the ground there lived a hobbit."
>>> print(tokenizer(sequence))
{'input_ids': [101, 1999, 1037, 4920, 1999, 1996, 2598, 2045, 2973, 1037, 7570, 10322, 4183, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

----------------------------------------

TITLE: Pushing trained model to Hugging Face Hub
DESCRIPTION: Upload the fine-tuned model to the Hugging Face Model Hub for sharing and easy access.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/tasks/asr.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
trainer.push_to_hub()
```

----------------------------------------

TITLE: Tokenizing, Padding to Max Length, and Truncating to Specific Length
DESCRIPTION: This snippet shows how to pad to model max length and truncate a batch of sentences to a specific length. `padding='max_length'` enables padding to the models maximum length.  `truncation=True` with `max_length=42` truncates sequences to a length of 42. `truncation=STRATEGY` can be used to specify truncation strategy for sequence pairs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pad_truncation.md#_snippet_9

LANGUAGE: Python
CODE:
```
tokenizer(batch_sentences, padding='max_length', truncation=True, max_length=42)
```

LANGUAGE: Python
CODE:
```
tokenizer(batch_sentences, padding='max_length', truncation=STRATEGY, max_length=42)
```

----------------------------------------

TITLE: Using Flash Attention 2 with PEFT Adapters
DESCRIPTION: Demonstrates how to combine Flash Attention 2 with PEFT for efficient adapter training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_infer_gpu_one.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM
from peft import LoraConfig

model_id = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    load_in_4bit=True,
    attn_implementation="flash_attention_2",
)

lora_config = LoraConfig(
    r=8,
    task_type="CAUSAL_LM"
)

model.add_adapter(lora_config)
```

----------------------------------------

TITLE: Tokenizing Input Sequence with AutoTokenizer in Python
DESCRIPTION: This code snippet showcases how to tokenize an input sequence using the pre-trained tokenizer loaded earlier. It passes the Italian sentence "In un buco nel terreno viveva uno Hobbit." to the tokenizer and prints the resulting dictionary containing `input_ids` and `attention_mask`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/autoclass_tutorial.md#_snippet_1

LANGUAGE: Python
CODE:
```
>>> sequenza = "In un buco nel terreno viveva uno Hobbit."
>>> print(tokenizer(sequenza))
{'input_ids': [0, 360, 51, 373, 587, 1718, 54644, 22597, 330, 3269, 2291, 22155, 18, 5, 2],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

----------------------------------------

TITLE: Depth Estimation with Auto Classes in Transformers
DESCRIPTION: This code snippet demonstrates how to use the `AutoImageProcessor` and `AutoModelForDepthEstimation` classes from Hugging Face Transformers for depth estimation using Depth Anything V2. It loads the processor and model, preprocesses an image, performs inference, and post-processes the output to obtain the final depth map.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/depth_anything_v2.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoImageProcessor, AutoModelForDepthEstimation
>>> import torch
>>> import numpy as np
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("depth-anything/Depth-Anything-V2-Small-hf")
>>> model = AutoModelForDepthEstimation.from_pretrained("depth-anything/Depth-Anything-V2-Small-hf")

>>> # prepare image for the model
>>> inputs = image_processor(images=image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> # interpolate to original size and visualize the prediction
>>> post_processed_output = image_processor.post_process_depth_estimation(
...     outputs,
...     target_sizes=[(image.height, image.width)],
... )

>>> predicted_depth = post_processed_output[0]["predicted_depth"]
>>> depth = (predicted_depth - predicted_depth.min()) / (predicted_depth.max() - predicted_depth.min())
>>> depth = depth.detach().cpu().numpy() * 255
>>> depth = Image.fromarray(depth.astype("uint8"))
```

----------------------------------------

TITLE: Load TF model for sequence classification
DESCRIPTION: Loads a pre-trained TensorFlow model for sequence classification using `TFAutoModelForSequenceClassification`.  The model includes a task-specific loss function by default, so no explicit loss function is required unless desired.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_11

LANGUAGE: Python
CODE:
```
from transformers import TFAutoModelForSequenceClassification
from tensorflow.keras.optimizers import Adam

# ØªØ­Ù…ÙŠÙ„ ÙˆØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø®Ø§Øµ Ø¨Ù†Ø§
model = TFAutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased")
```

----------------------------------------

TITLE: Generating text with Llama using AutoModel (Python)
DESCRIPTION: This code shows how to load a Llama model and tokenizer using `AutoModelForCausalLM` and `AutoTokenizer` respectively. It loads the `huggyllama/llama-7b` model, specifies `torch.float16` dtype, `device_map="auto"`, and uses the "sdpa" attention implementation. The input prompt is tokenized, moved to the GPU, and then passed to the model's `generate` method. The output is decoded and printed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llama.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    "huggyllama/llama-7b",
)
model = AutoModelForCausalLM.from_pretrained(
    "huggyllama/llama-7b",
    torch_dtype=torch.float16,
    device_map="auto",
    attn_implementation="sdpa"
)
input_ids = tokenizer("Plants create energy through a process known as", return_tensors="pt").to("cuda")

output = model.generate(**input_ids, cache_implementation="static")
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Using AutoModel/Tokenizer for Fill-Mask with Longformer (Python)
DESCRIPTION: Shows how to manually load a Longformer masked language model and its tokenizer using `AutoModelForMaskedLM` and `AutoTokenizer`. It processes masked text, gets the model logits, identifies the masked token, calculates probabilities, and decodes the top 5 predictions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/longformer.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForMaskedLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("allenai/longformer-base-4096")
model = AutoModelForMaskedLM.from_pretrained("allenai/longformer-base-4096")

text = (
"""
San Francisco 49ers cornerback Shawntae Spencer will miss the rest of the <mask> with a torn ligament in his left knee.
Spencer, a fifth-year pro, will be placed on injured reserve soon after undergoing surgery Wednesday to repair the ligament. He injured his knee late in the 49ersâ€™ road victory at Seattle on Sept. 14, and missed last weekâ€™s victory over Detroit.
Tarell Brown and Donald Strickland will compete to replace Spencer with the 49ers, who kept 12 defensive backs on their 53-man roster to start the season. Brown, a second-year pro, got his first career interception last weekend while filling in for Strickland, who also sat out with a knee injury.
"""
)

input_ids = tokenizer([text], return_tensors="pt")["input_ids"]
logits = model(input_ids).logits

masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()
probs = logits[0, masked_index].softmax(dim=0)
values, predictions = probs.topk(5)
tokenizer.decode(predictions).split()
```

----------------------------------------

TITLE: Installing TensorFlow Dependency for Transformers
DESCRIPTION: Command to install TensorFlow dependency which is required for using Transformers library with TensorFlow backend.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/quicktour.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
pip install tensorflow
```

----------------------------------------

TITLE: Arithmetic Reasoning with Chain-of-Thought (Python)
DESCRIPTION: Initializes a text-generation pipeline. It defines a prompt that breaks down a simple math problem into step-by-step instructions, guiding the model through intermediate reasoning steps before asking the final question. Runs the pipeline and prints the output. Requires `transformers` and `torch`. Takes a multi-step reasoning problem described in text as input and outputs the original prompt plus the model's step-by-step thinking and final answer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/prompting.md#_snippet_3

LANGUAGE: python
CODE:
```
from transformers import pipeline
import torch

pipeline = pipeline(model="mistralai/Mistral-7B-Instruct-v0.1", torch_dtype=torch.bfloat16, device_map="auto")
prompt = """Let's go through this step-by-step:
1. You start with 15 muffins.
2. You eat 2 muffins, leaving you with 13 muffins.
3. You give 5 muffins to your neighbor, leaving you with 8 muffins.
4. Your partner buys 6 more muffins, bringing the total number of muffins to 14.
5. Your partner eats 2 muffins, leaving you with 12 muffins.
If you eat 6 muffins, how many are left?"""

outputs = pipeline(prompt, max_new_tokens=20, do_sample=True, top_k=10)
for output in outputs:
    print(f"Result: {output['generated_text']}")
```

----------------------------------------

TITLE: Training Token Classification Model with PyTorch in Transformers
DESCRIPTION: Demonstrates how to load a pre-trained model, set up training arguments, and train a token classification model using the Transformers Trainer API in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/token_classification.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer

>>> model = AutoModelForTokenClassification.from_pretrained(
...     "distilbert/distilbert-base-uncased", num_labels=13, id2label=id2label, label2id=label2id
... )

>>> training_args = TrainingArguments(
...     output_dir="my_awesome_wnut_model",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     num_train_epochs=2,
...     weight_decay=0.01,
...     eval_strategy="epoch",
...     save_strategy="epoch",
...     load_best_model_at_end=True,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_wnut["train"],
...     eval_dataset=tokenized_wnut["test"],
...     processing_class=tokenizer,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()

>>> trainer.push_to_hub()
```

----------------------------------------

TITLE: Tokenizing Text with BERT Tokenizer in Python
DESCRIPTION: Demonstrates how to load a pre-trained BERT tokenizer and use it to tokenize a sentence. This example shows how subword tokenization works for the BERT model, including how unknown words are split into subwords.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tokenizer_summary.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> tokenizer.tokenize("I have a new GPU!")
["i", "have", "a", "new", "gp", "##u", "!"]
```

----------------------------------------

TITLE: Initialize and Train the Model with Trainer in Python
DESCRIPTION: Initializes the `Trainer` class with the model, training arguments, data collator, training dataset, and image processor. The `Trainer` then orchestrates the training process based on the specified arguments. The `trainer.train()` call starts the fine-tuning process, updating the model's weights to perform object detection on the training dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/object_detection.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
>>> from transformers import Trainer

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     data_collator=collate_fn,
...     train_dataset=cppe5["train"],
...     processing_class=image_processor,
... )

>>> trainer.train()
```

----------------------------------------

TITLE: Text Generation with Transformers Pipeline API
DESCRIPTION: Using the `transformers` Pipeline API for text generation tasks. This snippet demonstrates creating a Pipeline for text generation, processing input text, and generating additional content with GPU-accelerated inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
from transformers import pipeline

pipeline = pipeline("text-generation", model="meta-llama/Llama-2-7b-hf", device="cuda")
```

LANGUAGE: Python
CODE:
```
pipeline("The secret to baking a good cake is ", max_length=50)
[{'generated_text': 'The secret to baking a good cake is 100% in the batter. The secret to a great cake is the icing.\nThis is why weâ€™ve created the best buttercream frosting reci'}]
```

----------------------------------------

TITLE: Tokenizing Multiple Sentences
DESCRIPTION: Tokenizes a batch of sentences using the tokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/preprocessing.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_inputs = tokenizer(batch_sentences)
print(encoded_inputs)
```

----------------------------------------

TITLE: Sentiment Analysis Pipeline
DESCRIPTION: Demonstrates text sentiment analysis using the Hugging Face pipeline to classify text as positive or negative.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/task_summary.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline(task="sentiment-analysis")
>>> preds = classifier("Hugging Face is the best thing since sliced bread!")
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
```

----------------------------------------

TITLE: Defining compute_metrics function
DESCRIPTION: This snippet defines a `compute_metrics` function that calculates the accuracy of the model's predictions. It converts the raw logits to predictions and then uses the loaded metric to compute the accuracy.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_6

LANGUAGE: Python
CODE:
```
>>> def compute_metrics(eval_pred):
...     logitsØŒ labels = eval_pred
...     predictions = np.argmax(logits, axis=-1)
...     return metric.compute(predictions=predictions, references=labels)
```

----------------------------------------

TITLE: Configuring Tokenizer for Padding and Truncation in Python
DESCRIPTION: This snippet demonstrates various ways to configure a tokenizer for padding and truncation using different combinations of arguments. It shows how to handle different scenarios like padding to batch length, model max length, or specific lengths, as well as truncation strategies.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/pad_truncation.md#2025-04-22_snippet_0

LANGUAGE: markdown
CODE:
```
| Truncation                           | Padding                             | InstrucciÃ³n                                                                                 |
|-----------------------------------------|--------------------------------------|---------------------------------------------------------------------------------------------|\n| sin truncamiento                       | sin relleno                          | `tokenizer(batch_sentences)`                                                           |
|                                         | relleno hasta la longitud mÃ¡xima del lote | `tokenizer(batch_sentences, padding=True)` o                                          |
|                                         |                                      | `tokenizer(batch_sentences, padding='longest')`                                        |
|                                         | relleno hasta la longitud mÃ¡xima del modelo | `tokenizer(batch_sentences, padding='max_length')`                                     |
|                                         | relleno hasta una longitud especÃ­fica | `tokenizer(batch_sentences, padding='max_length', max_length=42)`                      |
|                                         | relleno hasta un mÃºltiplo de un valor | `tokenizer(batch_sentences, padding=True, pad_to_multiple_of=8)`                        |
| truncamiento hasta la longitud mÃ¡xima del modelo | sin relleno                          | `tokenizer(batch_sentences, truncation=True)` o                                       |
|                                         |                                      | `tokenizer(batch_sentences, truncation=ESTRATEGIA)`                                      |
|                                         | relleno hasta la longitud mÃ¡xima del lote | `tokenizer(batch_sentences, padding=True, truncation=True)` o                         |
|                                         |                                      | `tokenizer(batch_sentences, padding=True, truncation=ESTRATEGIA)`                        |
|                                         | relleno hasta la longitud mÃ¡xima del modelo | `tokenizer(batch_sentences, padding='max_length', truncation=True)` o                 |
|                                         |                                      | `tokenizer(batch_sentences, padding='max_length', truncation=ESTRATEGIA)`                |
|                                         | relleno hasta una longitud especÃ­fica | No es posible                                                                                |
| truncamiento hasta una longitud especÃ­fica | sin relleno                          | `tokenizer(batch_sentences, truncation=True, max_length=42)` o                        |
|                                         |                                      | `tokenizer(batch_sentences, truncation=ESTRATEGIA, max_length=42)`                       |
|                                         | relleno hasta la longitud mÃ¡xima del lote | `tokenizer(batch_sentences, padding=True, truncation=True, max_length=42)` o          |
|                                         |                                      | `tokenizer(batch_sentences, padding=True, truncation=ESTRATEGIA, max_length=42)`         |
|                                         | relleno hasta la longitud mÃ¡xima del modelo | No es posible                                                                                |
|                                         | relleno hasta una longitud especÃ­fica | `tokenizer(batch_sentences, padding='max_length', truncation=True, max_length=42)` o  |
|                                         |                                      | `tokenizer(batch_sentences, padding='max_length', truncation=ESTRATEGIA, max_length=42)` |
```

----------------------------------------

TITLE: Tokenizing Multiple Sentences
DESCRIPTION: Shows how to tokenize a batch of sentences using a pretrained tokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/preprocessing.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_inputs = tokenizer(batch_sentences)
print(encoded_inputs)
```

----------------------------------------

TITLE: Performing Question Answering with Transformers Pipeline - Python
DESCRIPTION: This snippet demonstrates how to perform Question Answering (QA) using the Hugging Face Transformers `pipeline` by prompting a Mistral model. It sets up the pipeline, constructs a prompt containing both the context and the question, executes the pipeline with sampling, and prints the generated answer.

Dependencies: `transformers`, `torch`.
Inputs: A prompt string with the context and the question to be answered based on the context.
Outputs: A string containing the answer extracted or generated from the context.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/prompting.md#_snippet_7

LANGUAGE: Python
CODE:
```
from transformers import pipeline
import torch

pipeline = pipeline(model="mistralai/Mistral-7B-Instruct-v0.1", torch_dtype=torch.bfloat16, device_map="auto")
prompt = """Answer the question using the context below.
Context: Gazpacho is a cold soup and drink made of raw, blended vegetables. Most gazpacho includes stale bread, tomato, cucumbers, onion, bell peppers, garlic, olive oil, wine vinegar, water, and salt. Northern recipes often include cumin and/or pimentÃ³n (smoked sweet paprika). Traditionally, gazpacho was made by pounding the vegetables in a mortar with a pestle; this more laborious method is still sometimes used as it helps keep the gazpacho cool and avoids the foam and silky consistency of smoothie versions made in blenders or food processors.
Question: What modern tool is used to make gazpacho?
Answer:
"""

outputs = pipeline(prompt, max_new_tokens=10, do_sample=True, top_k=10, return_full_text=False)
for output in outputs:
    print(f"Result: {output['generated_text']}")
```

----------------------------------------

TITLE: Performing Audio Classification with Transformers Pipeline
DESCRIPTION: This snippet demonstrates how to use the Transformers pipeline for audio classification. It loads a pre-trained model and classifies an audio file, returning emotion labels and scores.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/task_summary.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline(task="audio-classification", model="superb/hubert-base-superb-er")
>>> preds = classifier("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.4532, 'label': 'hap'},
 {'score': 0.3622, 'label': 'sad'},
 {'score': 0.0943, 'label': 'neu'},
 {'score': 0.0903, 'label': 'ang'}]
```

----------------------------------------

TITLE: Performing Speech Recognition Inference with Pipeline
DESCRIPTION: Use Hugging Face pipeline for simple automatic speech recognition inference on an audio file
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/asr.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
from transformers import pipeline

transcriber = pipeline("automatic-speech-recognition", model="stevhliu/my_awesome_asr_minds_model")
transcriber(audio_file)
```

----------------------------------------

TITLE: Classification audio avec Hubert et pipeline Transformers
DESCRIPTION: Utilise le pipeline de classification audio avec le modÃ¨le hubert-base-superb-er pour classifier les Ã©motions dans un fichier audio. Le code charge le pipeline, analyse un fichier audio distant et renvoie les Ã©motions dÃ©tectÃ©es avec leurs scores de confiance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/task_summary.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline(task="audio-classification", model="superb/hubert-base-superb-er")
>>> preds = classifier("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.4532, 'label': 'hap'},
 {'score': 0.3622, 'label': 'sad'},
 {'score': 0.0943, 'label': 'neu'},
 {'score': 0.0903, 'label': 'ang'}]
```

----------------------------------------

TITLE: Using a Fine-tuned Model with Pipeline for Inference
DESCRIPTION: Demonstrates the simplest way to perform inference with a fine-tuned model by using the Hugging Face pipeline. The pipeline handles tokenization and prediction in a single line of code.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_24

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline("sentiment-analysis", model="stevhliu/my_awesome_model")
>>> classifier(text)
[{'label': 'POSITIVE', 'score': 0.9994940757751465}]
```

----------------------------------------

TITLE: Applying Flash Attention with PyTorch Backend
DESCRIPTION: Demonstrates the application of Flash Attention to a transformer model using PyTorch. The snippet adjusts the CUDA backend configurations to enable Flash Attention, processes a long text prompt, and measures the generation time. The result is a significant speed-up while producing the same output as before.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_23

LANGUAGE: python
CODE:
```
with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    result = pipe(long_prompt, max_new_tokens=60)[0]["generated_text"][len(long_prompt):]

print(f"Generated in {time.time() - start_time} seconds.")
result
```

----------------------------------------

TITLE: Loading Models with Device Mapping
DESCRIPTION: This snippet shows how to load a model and distribute its layers across different devices (GPU, CPU, disk) using `device_map="auto"`.  The Accelerate library automatically determines the optimal device placement for each layer.  The `low_cpu_mem_usage` is automatically set to True when `device_map` is passed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/model.md#_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSeq2SeqLM

t0pp = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0pp", device_map="auto")
```

----------------------------------------

TITLE: Applying torch.compile() to Object Detection Model in Python
DESCRIPTION: This code snippet shows how to use torch.compile() with an AutoModelForObjectDetection model. It includes loading the DETR model, applying compilation, and processing input images and texts.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_torch_compile.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoImageProcessor, AutoModelForObjectDetection

processor = AutoImageProcessor.from_pretrained("facebook/detr-resnet-50")
model = AutoModelForObjectDetection.from_pretrained("facebook/detr-resnet-50").to("cuda")
model = torch.compile(model)

texts = ["a photo of a cat", "a photo of a dog"]
inputs = processor(text=texts, images=image, return_tensors="pt").to("cuda")

with torch.no_grad():
    _ = model(**inputs)
```

----------------------------------------

TITLE: Quantizing a Model On-the-Fly in Python
DESCRIPTION: This Python snippet demonstrates how to set up the configuration for quantization using EetqConfig and load a pre-trained causal language model that supports quantization. It specifies the datatype as 'int8'.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/eetq.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, EetqConfig

quantization_config = EetqConfig("int8")
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B",
    torch_dtype="auto",
    device_map="auto",
    quantization_config=quantization_config
)
```

----------------------------------------

TITLE: Combining AWQ with Flash Attention
DESCRIPTION: Loading an AWQ model with Flash Attention 2 enabled for improved performance
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/quantization.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("TheBloke/zephyr-7B-alpha-AWQ", attn_implementation="flash_attention_2", device_map="cuda:0")
```

----------------------------------------

TITLE: Loading a TF Pre-trained Model for Sequence Classification
DESCRIPTION: This code shows how to load a TensorFlow pre-trained model for sequence classification using `TFAutoModelForSequenceClassification.from_pretrained`. It loads the model architecture and the pre-trained weights.  Requires the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/autoclass_tutorial.md#_snippet_9

LANGUAGE: Python
CODE:
```
>>> from transformers import TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Loading Pre-trained Model for Token Classification with AutoModel in Python
DESCRIPTION: This code snippet demonstrates how to load a pre-trained model for token classification using `AutoModelForTokenClassification.from_pretrained` from the `transformers` library. It utilizes the same 'distilbert/distilbert-base-uncased' checkpoint but loads it for a different task, token classification.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/autoclass_tutorial.md#_snippet_5

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoModelForTokenClassification

>>> model = AutoModelForTokenClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Using Helium-1 Preview for Inference in Python
DESCRIPTION: This Python code snippet demonstrates how to load and use the 'helium-1-preview' model from the Huggingface Hub for generating natural language text. It imports the necessary modules, loads the model and tokenizer, and performs inference by generating a response to a given prompt. The snippet also handles tokenization and decoding of the generated response. It requires a GPU device for execution and the installation of the 'transformers' library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/helium.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoModelForCausalLM, AutoTokenizer
>>> device = "cuda" # the device to load the model onto

>>> model = AutoModelForCausalLM.from_pretrained("kyutai/helium-1-preview-2b", device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("kyutai/helium-1-preview-2b")

>>> prompt = "Give me a short introduction to large language model."

>>> model_inputs = tokenizer(prompt, return_tensors="pt").to(device)

>>> generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=True)

>>> generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]

>>> response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

----------------------------------------

TITLE: Image Classification with Image Classification Pipeline in Python
DESCRIPTION: Demonstrates image classification using the Vision Transformers (ViT) model through Hugging Face Transformers. Takes an image URL as input and outputs labels with confidence scores. Ensure 'transformers' is installed and network access is available.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import pipeline

pipeline = pipeline(task="image-classification", model="google/vit-base-patch16-224")
pipeline(images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg")
[{'label': 'lynx, catamount', 'score': 0.4335}, ...]
```

----------------------------------------

TITLE: Running Distributed Training in Notebooks
DESCRIPTION: This Python code demonstrates how to use Accelerate's notebook_launcher to run distributed training in environments like Google Colab, especially useful for TPU training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/accelerate.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

----------------------------------------

TITLE: Low-Level Chat with LLaMA-3 in Python
DESCRIPTION: This snippet demonstrates a lower-level approach to chatting with a model, explicitly loading the model and tokenizer, applying a chat template, tokenizing the input, and generating the output. The code utilizes `AutoModelForCausalLM` and `AutoTokenizer` for loading the model and tokenizer. It also demonstrates moving the input tensors to the same device as the model (GPU/CPU).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/conversations.md#_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ ÙƒÙ…Ø§ Ù‡Ùˆ Ø§Ù„Ø­Ø§Ù„ Ù…Ù† Ù‚Ø¨Ù„
chat = [
    {"role": "system", "content": "You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986."},
    {"role": "user", "content": "Hey, can you tell me any fun things to do in New York?"}
]

# 1: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø­Ù„Ù„
model = AutoModelForCausalLM.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct", device_map="auto", torch_dtype=torch.bfloat16)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct")

# 2: ØªØ·Ø¨ÙŠÙ‚ Ù‚Ø§Ù„Ø¨ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©
formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
print("Formatted chat:\n", formatted_chat)

# 3: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© (ÙŠÙ…ÙƒÙ† Ø¯Ù…Ø¬ Ù‡Ø°Ù‡ Ø§Ù„Ø®Ø·ÙˆØ© Ù…Ø¹ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… tokenize=True)
inputs = tokenizer(formatted_chat, return_tensors="pt", add_special_tokens=False)
# Ù†Ù‚Ù„ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù…Ø­Ù„Ù„Ø© Ø¥Ù„Ù‰ Ù†ÙØ³ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ Ø¹Ù„ÙŠÙ‡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (GPU/CPU)
inputs = {key: tensor.to(model.device) for key, tensor in inputs.items()}
print("Tokenized inputs:\n", inputs)

# 4: Ø¥Ù†Ø´Ø§Ø¡ Ù†Øµ Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.1)
print("Generated tokens:\n", outputs)
```

----------------------------------------

TITLE: Generate Text with Mistral using AutoModel/Tokenizer (Python)
DESCRIPTION: This snippet illustrates how to load the Mistral causal language model and its tokenizer using the `AutoModelForCausalLM` and `AutoTokenizer` classes. It loads the model, tokenizes the chat messages using `apply_chat_template`, and then calls the `generate` method to produce a response. It requires PyTorch and the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mistral.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> import torch
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.3", torch_dtype=torch.bfloat16, attn_implementation="sdpa", device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.3")

>>> messages = [
...     {"role": "user", "content": "What is your favourite condiment?"},
...     {"role": "assistant", "content": "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!"},
...     {"role": "user", "content": "Do you have mayonnaise recipes?"}
... ]

>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")

>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"Mayonnaise can be made as follows: (...)"
```

----------------------------------------

TITLE: Splitting Dataset into Train and Test Sets
DESCRIPTION: Illustrates how to split a dataset into training and testing sets using the train_test_split method from the Datasets library. The test_size parameter dictates that 20% of the data will be used for testing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
eli5 = eli5.train_test_split(test_size=0.2)
```

----------------------------------------

TITLE: Pipeline with Specific Model
DESCRIPTION: Shows how to use a specific model from the Hugging Face hub with the pipeline abstraction.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/pipelines.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
pipe = pipeline(model="FacebookAI/roberta-large-mnli")
pipe("This restaurant is awesome")
```

----------------------------------------

TITLE: Model Initialization for Classification
DESCRIPTION: Initializes a BERT model for sequence classification with a specified number of labels for the Yelp Review task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/training.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=5)
```

----------------------------------------

TITLE: Loading and Using HerBERT with Auto Classes in Python
DESCRIPTION: This code snippet shows how to load and use the HerBERT model and tokenizer using the `AutoModel` and `AutoTokenizer` classes from the `transformers` library. This approach automatically infers the correct model and tokenizer classes based on the pretrained model name. It provides a more convenient and flexible way to load and use the model, especially when working with multiple models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/herbert.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
>>> import torch
>>> from transformers import AutoModel, AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("allegro/herbert-klej-cased-tokenizer-v1")
>>> model = AutoModel.from_pretrained("allegro/herbert-klej-cased-v1")
```

----------------------------------------

TITLE: Checking GPU Status with nvidia-smi
DESCRIPTION: Command to verify GPU memory usage through the nvidia-smi CLI tool, which provides a detailed view of GPU status including memory usage, temperature, and running processes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_memory_anatomy.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
nvidia-smi
```

----------------------------------------

TITLE: Loading a Model for Sequence Classification
DESCRIPTION: Loads a pre-trained `AutoModelForSequenceClassification` model with a specified number of labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/training.md#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=5)
```

----------------------------------------

TITLE: Using AutoBackbone for Feature Extraction
DESCRIPTION: This code snippet demonstrates how to use `AutoBackbone` to extract feature maps from different stages of a pre-trained model.  It requires the `requests`, `PIL`, and `torch` libraries.  The `out_indices` parameter specifies which layer's feature map to extract. The example shows how to load an image, process it, and then pass it through the backbone model to get the feature maps.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/autoclass_tutorial.md#_snippet_3

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoImageProcessor, AutoBackbone
>>> import torch
>>> from PIL import Image
>>> import requests
>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> processor = AutoImageProcessor.from_pretrained("microsoft/swin-tiny-patch4-window7-224")
>>> model = AutoBackbone.from_pretrained("microsoft/swin-tiny-patch4-window7-224", out_indices=(1,))

>>> inputs = processor(image, return_tensors="pt")
>>> outputs = model(**inputs)
>>> feature_maps = outputs.feature_maps
```

----------------------------------------

TITLE: Cargando modelo y tokenizer con AutoClasses en TensorFlow
DESCRIPTION: Utiliza las clases Auto de Transformers para cargar automÃ¡ticamente un modelo preentrenado y su tokenizador asociado en TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/quicktour.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

----------------------------------------

TITLE: Loading Model and Tokenizer with AutoClass (PyTorch)
DESCRIPTION: Loads a pre-trained model and its associated tokenizer using the AutoModelForSequenceClassification and AutoTokenizer classes from the transformers library when using the PyTorch framework.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/quicktour.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
">>> from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n>>> model = AutoModelForSequenceClassification.from_pretrained(model_name)\n>>> tokenizer = AutoTokenizer.from_pretrained(model_name)"
```

----------------------------------------

TITLE: Generating Text with Transformers Pipeline in Python
DESCRIPTION: This snippet demonstrates how to use the Hugging Face Transformers pipeline for text generation with the Llama 4 Scout model. It imports the required modules, initializes the pipeline with a specified model, and generates output messages. Dependencies include the Transformers library, PyTorch, and access to the specific Llama model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llama4.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import pipeline
import torch

model_id = "meta-llama/Llama-4-Scout-17B-16E-Instruct"

messages = [
    {"role": "user", "content": "what is the recipe of mayonnaise?"},
]

pipe = pipeline(
    "text-generation",
    model=model_id,
    device_map="auto",
    torch_dtype=torch.bfloat16
)

output = pipe(messages, do_sample=False, max_new_tokens=200)
print(output[0]["generated_text"][-1]["content"])
```

----------------------------------------

TITLE: Configure Training Arguments and Train Model - PyTorch
DESCRIPTION: Sets up training arguments and initializes the Trainer for model fine-tuning with specified hyperparameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/multiple_choice.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
training_args = TrainingArguments(
    output_dir="my_awesome_swag_model",
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_swag["train"],
    eval_dataset=tokenized_swag["validation"],
    processing_class=tokenizer,
    data_collator=collator,
    compute_metrics=compute_metrics,
)

trainer.train()
```

----------------------------------------

TITLE: Loading a Pretrained Model for Causal Language Modeling
DESCRIPTION: This Python snippet demonstrates how to load the 'bigscience/bloom' pretrained model for causal language modeling. It utilizes the Hugging Face Transformers library to specify device mapping, enabling automatic distribution of model components across available GPUs. The model supports generation tasks by setting a padding token ID.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("bigscience/bloom", device_map="auto", pad_token_id=0)
```

----------------------------------------

TITLE: Text Generation with 8-bit Models
DESCRIPTION: Complete example of loading an 8-bit model, tokenizing input text, generating text outputs, and decoding the outputs. Shows the full workflow for efficient inference using quantized models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/perf_infer_gpu_one.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = "bigscience/bloom-2b5"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True))

text = "Hello, my llama is cute"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
generated_ids = model.generate(**inputs)
outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
```

----------------------------------------

TITLE: Object Detection with OWLv2 using Transformers
DESCRIPTION: This code snippet demonstrates how to perform object detection using the OWLv2 model from the `transformers` library. It loads a pre-trained OWLv2 model and processor, preprocesses an image and text prompts, performs object detection, and then post-processes the outputs to obtain bounding boxes, scores, and text labels for detected objects.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/owlv2.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> import requests
>>> from PIL import Image
>>> import torch

>>> from transformers import Owlv2Processor, Owlv2ForObjectDetection

>>> processor = Owlv2Processor.from_pretrained("google/owlv2-base-patch16-ensemble")
>>> model = Owlv2ForObjectDetection.from_pretrained("google/owlv2-base-patch16-ensemble")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> text_labels = [["a photo of a cat", "a photo of a dog"]]
>>> inputs = processor(text=text_labels, images=image, return_tensors="pt")
>>> outputs = model(**inputs)

>>> # Target image sizes (height, width) to rescale box predictions [batch_size, 2]
>>> target_sizes = torch.tensor([(image.height, image.width)])
>>> # Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)
>>> results = processor.post_process_grounded_object_detection(
...     outputs=outputs, target_sizes=target_sizes, threshold=0.1, text_labels=text_labels
... )
>>> # Retrieve predictions for the first image for the corresponding text queries
>>> result = results[0]
>>> boxes, scores, text_labels = result["boxes"], result["scores"], result["text_labels"]
>>> for box, score, text_label in zip(boxes, scores, text_labels):
...     box = [round(i, 2) for i in box.tolist()]
...     print(f"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}")
Detected a photo of a cat with confidence 0.614 at location [341.67, 23.39, 642.32, 371.35]
Detected a photo of a cat with confidence 0.665 at location [6.75, 51.96, 326.62, 473.13]

```

----------------------------------------

TITLE: Tokenizing a Batch of Sentences in Python
DESCRIPTION: This snippet demonstrates how to tokenize a batch of sentences. The tokenizer processes a list of sentences and returns a dictionary with `input_ids`, `token_type_ids`, and `attention_mask` for each sentence in the batch. The resulting data structure is a list of lists.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/preprocessing.md#_snippet_3

LANGUAGE: Python
CODE:
```
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_inputs = tokenizer(batch_sentences)
>>> print(encoded_inputs)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102], 
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], 
               [101, 1327, 1164, 5450, 23434, 136, 102]], 
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0]], 
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], 
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
                    [1, 1, 1, 1, 1, 1, 1]]}
```

----------------------------------------

TITLE: ChatML Format Template for General Use
DESCRIPTION: A general-purpose chat template following the ChatML format. This template is flexible and suitable for many use cases, especially when training or fine-tuning models for chat applications.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/chat_templating.md#2025-04-22_snippet_8

LANGUAGE: jinja
CODE:
```
{% for message in messages %}
    {{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}
{% endfor %}
```

----------------------------------------

TITLE: Using Zero-Shot Classification Pipeline in Python
DESCRIPTION: This example shows how to use a zero-shot classification pipeline for text classification, allowing for flexible label selection.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/pipeline_tutorial.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> # This model is a `zero-shot-classification` model.
>>> # It will classify text, except you are free to choose any label you might imagine
>>> classifier = pipeline(model="facebook/bart-large-mnli")
>>> classifier(
...     "I have a problem with my iphone that needs to be resolved asap!!",
...     candidate_labels=["urgent", "not urgent", "phone", "tablet", "computer"],
... )
{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!', 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'], 'scores': [0.504, 0.479, 0.013, 0.003, 0.002]}
```

----------------------------------------

TITLE: Creating and Activating Virtual Environment in Python
DESCRIPTION: Commands to create a virtual environment and activate it on Linux/MacOS and Windows.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/installation.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python -m venv .env
```

LANGUAGE: bash
CODE:
```
source .env/bin/activate
```

LANGUAGE: bash
CODE:
```
.env/Scripts/activate
```

----------------------------------------

TITLE: Sequence Classification with Bert in Transformers using Python
DESCRIPTION: This code demonstrates how to perform sequence classification using the BertForSequenceClassification model from the Hugging Face Transformers library. It initializes a tokenizer and a model, encodes an input sequence, and feeds it to the model to obtain the classification output. The example also shows how to pass labels to compute the loss.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/output.md#_snippet_0

LANGUAGE: python
CODE:
```
from transformers import BertTokenizer, BertForSequenceClassification
import torch

tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")
model = BertForSequenceClassification.from_pretrained("google-bert/bert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)
```

----------------------------------------

TITLE: Pushing a Model to Hub with Trainer
DESCRIPTION: Shows how to push a fine-tuned model to the Hugging Face Hub using the Trainer's push_to_hub method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/model_sharing.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> trainer.push_to_hub()
```

----------------------------------------

TITLE: Downloading Tokenizer and Model
DESCRIPTION: This Python code downloads a pre-trained tokenizer and model from Hugging Face Hub.  These will be used for a sequence-to-sequence task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/installation.md#_snippet_18

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

>>> tokenizer = AutoTokenizer.from_pretrained("bigscience/T0_3B")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0_3B")
```

----------------------------------------

TITLE: Initializing Trainer for VideoMAE Fine-tuning in Python
DESCRIPTION: Sets up a Trainer instance with the model, training arguments, datasets, image processor, evaluation metrics, and custom data collator for fine-tuning VideoMAE.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
>>> trainer = Trainer(
...     model,
...     args,
...     train_dataset=train_dataset,
...     eval_dataset=val_dataset,
...     processing_class=image_processor,
...     compute_metrics=compute_metrics,
...     data_collator=collate_fn,
... )
```

----------------------------------------

TITLE: Initializing Trainer
DESCRIPTION: Sets up the Seq2SeqTrainer with model, dataset, and training configurations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/text-to-speech.md#2025-04-22_snippet_26

LANGUAGE: python
CODE:
```
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    data_collator=data_collator,
    processing_class=processor,
)
```

----------------------------------------

TITLE: Generating Text with AutoModel - Python
DESCRIPTION: This snippet shows how to perform text generation more manually using `AutoModelForCausalLM` and `AutoTokenizer`. This method provides more control over the model loading and generation process. Requires the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/openai-gpt.md#_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt")
model = AutoModelForCausalLM.from_pretrained("openai-community/openai-gpt", torch_dtype=torch.float16)

inputs = tokenizer("The future of AI is", return_tensors="pt")
outputs = model.generate(**inputs, max_length=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Configuring Training Arguments
DESCRIPTION: Sets up default training arguments for the Hugging Face Trainer, including output directory, evaluation strategy, number of epochs, and reporting configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_memory_anatomy.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
default_args = {
    "output_dir": "tmp",
    "eval_strategy": "steps",
    "num_train_epochs": 1,
    "log_level": "error",
    "report_to": "none",
}
```

----------------------------------------

TITLE: Conda Installation Command
DESCRIPTION: Command for installing Transformers using conda package manager
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/installation.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
conda install conda-forge::transformers
```

----------------------------------------

TITLE: Authenticating with Hugging Face Hub
DESCRIPTION: Logs into the Hugging Face Hub to enable model uploading and sharing. This allows saving trained models to a personal account.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/token_classification.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Creating Virtual Environment - Python and Bash Setup
DESCRIPTION: Commands for creating and activating a Python virtual environment for installing Transformers
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/installation.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python -m venv .env
```

LANGUAGE: bash
CODE:
```
source .env/bin/activate
```

----------------------------------------

TITLE: Using Attention Mask to Handle Padding Tokens Correctly in Python
DESCRIPTION: This snippet demonstrates how to use an attention_mask to properly handle padding tokens in input sequences, resulting in correct model outputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/troubleshooting.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> attention_mask = torch.tensor([[1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0]])
>>> output = model(input_ids, attention_mask=attention_mask)
>>> print(output.logits)
tensor([[ 0.0082, -0.2307],
        [-0.1008, -0.4061]], grad_fn=<AddmmBackward0>)
```

----------------------------------------

TITLE: Return PyTorch Tensors with Tokenizer in Python
DESCRIPTION: This snippet converts the tokenized output into PyTorch tensors by setting `return_tensors='pt'`. This is necessary for using the data with PyTorch-based models. The padding and truncation arguments should be set appropriately before returning the tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/preprocessing.md#_snippet_6

LANGUAGE: Python
CODE:
```
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors="pt")
>>> print(encoded_input)
{'input_ids': tensor([[  101,   153,  7719, 21490,  1122,  1114,  9582,  1623,   102],
                      [  101,  5226,  1122,  9649,  1199,  2610,  1236,   102,     0]]),
 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],
                           [0, 0, 0, 0, 0, 0, 0, 0, 0]]),
 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],
                           [1, 1, 1, 1, 1, 1, 1, 1, 0]])}
```

----------------------------------------

TITLE: PyTorch Training Loop
DESCRIPTION: This code implements a training loop for a PyTorch model. It iterates through epochs and batches, moves the batch data to the device, performs a forward pass, calculates the loss, performs backpropagation, updates the optimizer and learning rate scheduler, and resets the optimizer's gradients. A tqdm progress bar is used to track the training progress.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_24

LANGUAGE: Python
CODE:
```
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

----------------------------------------

TITLE: Tokenizing Text with BERT Tokenizer in Python
DESCRIPTION: Example of tokenizing the sentence "I have a new GPU!" using BERT's WordPiece tokenization. The tokenizer breaks down the rare word "GPU" into subword tokens and shows how it handles casing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tokenizer_summary.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> tokenizer.tokenize("I have a new GPU!")
["i", "have", "a", "new", "gp", "##u", "!"]
```

----------------------------------------

TITLE: Tokenizing and Padding to Max Length
DESCRIPTION: This snippet shows how to pad a batch of sentences to a specified maximum length or to the model's maximum allowed length if `max_length` is not provided.  The `padding='max_length'` argument enforces padding to either the `max_length` parameter or to the model's maximum input length.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pad_truncation.md#_snippet_1

LANGUAGE: Python
CODE:
```
tokenizer(batch_sentences, padding='max_length')
```

----------------------------------------

TITLE: Loading PEFT Model in 8-bit
DESCRIPTION: Example of loading a PEFT model with 8-bit quantization using BitsAndBytesConfig.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/peft.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```

----------------------------------------

TITLE: Loading 8-bit Model with Custom Device Map and CPU Offloading
DESCRIPTION: Complete example of loading an 8-bit model with custom device mapping and CPU offloading for handling large models efficiently.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/bitsandbytes.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
model_8bit = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-1b7",
    device_map=device_map,
    quantization_config=quantization_config,
)
```

----------------------------------------

TITLE: Loading a Pretrained Tokenizer with AutoTokenizer
DESCRIPTION: Demonstrates how to load a pretrained BERT tokenizer using the AutoTokenizer class from Hugging Face Transformers, which downloads the vocabulary used during model pretraining.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/preprocessing.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")
```

----------------------------------------

TITLE: Quantizing Qwen2.5-VL Model with TorchAo (Python)
DESCRIPTION: Illustrates how to apply quantization to the Qwen2.5-VL model using `torchao` during loading. It specifies `int4_weight_only` quantization with a group size of 128 via the `TorchAoConfig` and passes it to `from_pretrained`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2_5_vl.md#_snippet_2

LANGUAGE: python
CODE:
```
import torch
from transformers import TorchAoConfig, Qwen2_5_VLForConditionalGeneration, AutoProcessor

quantization_config = TorchAoConfig("int4_weight_only", group_size=128)
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-VL-7B-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    quantization_config=quantization_config
)

```

----------------------------------------

TITLE: Running Forward Pass with TensorFlow Model
DESCRIPTION: This snippet demonstrates how to run a forward pass through a TensorFlow model using tokenized inputs and how to apply softmax to obtain probability distributions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/quicktour.md#2025-04-23_snippet_11

LANGUAGE: python
CODE:
```
tf_outputs = tf_model(tf_batch)

import tensorflow as tf

tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
tf_predictions  # doctest: +IGNORE_RESULT
```

----------------------------------------

TITLE: Loading a Pre-trained Model for Sequence Classification with PyTorch
DESCRIPTION: Initializes a DistilBERT model for sequence classification using PyTorch's AutoModelForSequenceClassification class. The model is configured with the number of classes and label mappings needed for sentiment analysis.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

>>> model = AutoModelForSequenceClassification.from_pretrained(
...     "distilbert/distilbert-base-uncased", num_labels=2, id2label=id2label, label2id=label2id
... )
```

----------------------------------------

TITLE: Loading GGUF Model in Transformers
DESCRIPTION: Demonstrates how to load a GGUF quantized model file using the Transformers library's AutoTokenizer and AutoModelForCausalLM classes. The example shows loading both tokenizer and model from a GGUF file.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/gguf.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF"
filename = "tinyllama-1.1b-chat-v1.0.Q6_K.gguf"

tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)
model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)
```

----------------------------------------

TITLE: Loading Code Llama with 4-bit Quantization (Python)
DESCRIPTION: Provides an example of loading a Code Llama model using 4-bit quantization via the `bitsandbytes` library to reduce memory usage. It configures the quantization settings and passes them to the `from_pretrained` method, then uses the quantized model for generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/code_llama.md#_snippet_3

LANGUAGE: python
CODE:
```
# pip install bitsandbytes
import torch
from transformers import AutoModelForCausalLM, CodeLlamaTokenizer, BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type="nf4", bnb_4bit_use_double_quant=True)
tokenizer = CodeLlamaTokenizer.from_pretrained("meta-llama/CodeLlama-34b-hf")
model = AutoModelForCausalLM.from_pretrained(
   "meta-llama/CodeLlama-34b-hf",
   torch_dtype=torch.bfloat16,
   device_map="auto",
   quantization_config=bnb_config
)

prompt = "# Write a Python function to check if a string is a palindrome\ndef is_palindrome(s):"
input_ids = tokenizer(prompt, return_tensors="pt").to("cuda")

output = model.generate(**input_ids, max_new_tokens=200, cache_implementation="static")
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Quantizing Model with Int8 (Dynamic or Weight-Only) on CPU
DESCRIPTION: This snippet demonstrates loading and quantizing a Llama-3.1-8B-Instruct model for CPU inference using either Int8 dynamic activation and Int8 weight quantization or just Int8 weight-only quantization via TorchAoConfig. It loads the model, tokenizes input, and generates a response using the quantized model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/torchao.md#_snippet_6

LANGUAGE: Python
CODE:
```
import torch
from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
from torchao.quantization import Int8DynamicActivationInt8WeightConfig, Int8WeightOnlyConfig

quant_config = Int8DynamicActivationInt8WeightConfig()
# quant_config = Int8WeightOnlyConfig()
quantization_config = TorchAoConfig(quant_type=quant_config)

# Load and quantize the model
quantized_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B-Instruct",
    torch_dtype="auto",
    device_map="cpu",
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-8B-Instruct")
input_text = "What are we having for dinner?"
input_ids = tokenizer(input_text, return_tensors="pt")

# auto-compile the quantized model with `cache_implementation="static"` to get speed up
output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation="static")
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Initializing Trainer and starting training
DESCRIPTION: This snippet initializes the `Trainer` with the model, training arguments, datasets, and the `compute_metrics` function. It then starts the fine-tuning process by calling the `train()` method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_8

LANGUAGE: Python
CODE:
```
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=small_train_dataset,
...     eval_dataset=small_eval_dataset,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

----------------------------------------

TITLE: Quantizing and Generating Text with Gemma (BitsAndBytes, PyTorch)
DESCRIPTION: Demonstrates how to load the Gemma model with 4-bit quantization using the BitsAndBytesConfig from transformers. It configures the quantization settings and loads the 'google/gemma-7b' model, then performs text generation similarly to the AutoModel example. Requires `transformers`, `torch`, and `bitsandbytes`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gemma.md#_snippet_3

LANGUAGE: python
CODE:
```
#!pip install bitsandbytes
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type="nf4"
)
tokenizer = AutoTokenizer.from_pretrained("google/gemma-7b")
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-7b",
    quantization_config=quantization_config,
    device_map="auto",
    attn_implementation="sdpa"
)

input_text = "LLMs generate text through a process known as."
input_ids = tokenizer(input_text, return_tensors="pt").to("cuda")
outputs = model.generate(
    **input_ids,
    max_new_tokens=50,
    cache_implementation="static"
)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Loading Minitron Model and Generating Text in PyTorch
DESCRIPTION: This snippet demonstrates the process of loading the Minitron-4B model using the HuggingFace Transformers library, preparing input text, generating output text, and decoding it. It requires the Torch and Transformers libraries as dependencies. The input is a prompt string, and the output is the generated completion text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/nemotron.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load the tokenizer and model
model_path = 'nvidia/Minitron-4B-Base'
tokenizer  = AutoTokenizer.from_pretrained(model_path)

device = 'cuda'
dtype  = torch.bfloat16
model  = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=dtype, device_map=device)

# Prepare the input text
prompt = 'Complete the paragraph: our solar system is'
inputs = tokenizer.encode(prompt, return_tensors='pt').to(model.device)

# Generate the output
outputs = model.generate(inputs, max_length=20)

# Decode and print the output
output_text = tokenizer.decode(outputs[0])
print(output_text)
```

----------------------------------------

TITLE: Creating Virtual Environment in Python
DESCRIPTION: Command to create a new Python virtual environment in the project directory
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/installation.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python -m venv .env
```

----------------------------------------

TITLE: Greedy Search Decoding
DESCRIPTION: This example demonstrates greedy search decoding, the default decoding strategy in `transformers`.  It loads a pre-trained model and tokenizer, encodes an input prompt, and then generates text using the `generate` method with default parameters. Greedy search always selects the token with the highest probability at each step.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/generation_strategies.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> prompt = "I look forward to"
>>> checkpoint = "distilbert/distilgpt2"

>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)
>>> outputs = model.generate(**inputs)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['I look forward to seeing you all again!\n\n\n\n\n\n\n\n\n\n\n']
```

----------------------------------------

TITLE: Configuring Training Arguments
DESCRIPTION: Sets up training arguments and configuration for the Hugging Face Trainer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/training.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments

training_args = TrainingArguments(output_dir="test_trainer", eval_strategy="epoch")
```

----------------------------------------

TITLE: Generating Text with Cohere Command-R via AutoModel in Python
DESCRIPTION: This snippet shows how to load the Cohere Command-R model and tokenizer using `AutoTokenizer` and `AutoModelForCausalLM` for more granular control over text generation. It applies the model's specific chat template to format user messages and generates text with specified parameters like `max_new_tokens` and `temperature`. Requires `transformers` and PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/cohere.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("CohereForAI/c4ai-command-r-v01")
model = AutoModelForCausalLM.from_pretrained("CohereForAI/c4ai-command-r-v01", torch_dtype=torch.float16, device_map="auto", attn_implementation="sdpa")

# format message with the Command-R chat template
messages = [{"role": "user", "content": "How do plants make energy?"}]
input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to("cuda")
output = model.generate(
    input_ids,
    max_new_tokens=100,
    do_sample=True,
    temperature=0.3,
    cache_implementation="static",
)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Generating Text with Cohere Command-R via Pipeline in Python
DESCRIPTION: This snippet demonstrates using the Hugging Face `pipeline` API to perform text generation with the Cohere Command-R model. It simplifies the process of loading the model and running inference. Requires the `transformers` library and a PyTorch backend.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/cohere.md#_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipeline = pipeline(
    task="text-generation",
    model="CohereForAI/c4ai-command-r-v01",
    torch_dtype=torch.float16,
    device=0
)
pipeline("Plants create energy through a process known as")
```

----------------------------------------

TITLE: Initializing Zero-Shot Object Detection Pipeline with OWL-ViT
DESCRIPTION: Creates a zero-shot object detection pipeline using the OWL-ViT model checkpoint. This provides the simplest way to perform object detection with free-text queries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/zero_shot_object_detection.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> checkpoint = "google/owlvit-base-patch32"
>>> detector = pipeline(model=checkpoint, task="zero-shot-object-detection")
```

----------------------------------------

TITLE: Installing Transformers and Dependencies
DESCRIPTION: Installs the Transformers library and related dependencies using pip.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/quicktour.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
!pip install transformers datasets evaluate accelerate
```

----------------------------------------

TITLE: Installing Transformers Library
DESCRIPTION: Basic pip installation command for HuggingFace Transformers library
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/installation.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
pip install transformers
```

----------------------------------------

TITLE: Loading a Pre-trained Tokenizer with AutoTokenizer in Python
DESCRIPTION: This code snippet shows how to load a pre-trained tokenizer using `AutoTokenizer.from_pretrained` from the `transformers` library. The tokenizer is loaded from the specified checkpoint name, in this case, 'google-bert/bert-base-uncased'.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/autoclass_tutorial.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
```

----------------------------------------

TITLE: Generating Text with FalconMamba using Pipeline (Python)
DESCRIPTION: This snippet shows how to perform text generation using the high-level `pipeline` function from the `transformers` library. It initializes a text generation pipeline with the FalconMamba 7B instruct model, specifies the compute dtype and device, and then calls the pipeline with a prompt and generation parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/falcon_mamba.md#_snippet_0

LANGUAGE: Python
CODE:
```
import torch
from transformers import pipeline

pipeline = pipeline(
    "text-generation",
    model="tiiuae/falcon-mamba-7b-instruct",
    torch_dtype=torch.bfloat16,
    device=0
)
pipeline(
    "Explain the difference between transformers and SSMs",
    max_length=100,
    do_sample=True,
    temperature=0.7
)
```

----------------------------------------

TITLE: Ejecutando anÃ¡lisis de sentimiento con pipeline
DESCRIPTION: Aplica el pipeline de anÃ¡lisis de sentimiento a un texto en espaÃ±ol y obtiene una clasificaciÃ³n positiva o negativa con su score.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/quicktour.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
clasificador("Estamos muy felices de mostrarte la biblioteca de ðŸ¤— Transformers.")
[{'label': 'POS', 'score': 0.9320}]
```

----------------------------------------

TITLE: Implementing Automatic Speech Recognition with ðŸ¤— Transformers Pipeline in Python
DESCRIPTION: This code snippet shows how to use the ðŸ¤— Transformers pipeline for automatic speech recognition. It uses the Whisper model to transcribe speech from an audio file to text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/task_summary.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition", model="openai/whisper-small")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

----------------------------------------

TITLE: Pipeline Execution on GPU in Python
DESCRIPTION: Explains running a text-generation pipeline on a designated GPU using the Gemma-2 model, offering improved performance over CPU. Set the 'device' parameter to the appropriate CUDA index. Necessary to have the 'transformers' library installed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers import pipeline

pipeline = pipeline(task="text-generation", model="google/gemma-2-2b", device=0)
pipeline("the secret to baking a really good cake is ")
```

----------------------------------------

TITLE: Initializing Text Generation Pipeline
DESCRIPTION: Basic setup of a text generation pipeline using the transformers library
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/pipeline_tutorial.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import pipeline

generator = pipeline(task="text-generation")
```

----------------------------------------

TITLE: Verifying Transformers Installation
DESCRIPTION: This command verifies that ðŸ¤— Transformers is installed correctly by running a sentiment analysis pipeline. It downloads a pre-trained model and prints the result.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/installation.md#2025-04-22_snippet_7

LANGUAGE: bash
CODE:
```
"python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\""
```

----------------------------------------

TITLE: Loading the BERT Tokenizer
DESCRIPTION: This loads the BERT tokenizer from the transformers library using the pretrained 'google-bert/bert-base-uncased' model. It's a necessary step to convert the raw text into a format that the BERT model can understand.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/multiple_choice.md#_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
```

----------------------------------------

TITLE: Encoding a Batch of Sentences
DESCRIPTION: This Python code shows how to encode a batch of sentences using the tokenizer. The tokenizer processes each sentence in the list and returns a dictionary containing the `input_ids`, `token_type_ids`, and `attention_mask` for each sentence. This allows for efficient processing of multiple sentences in parallel.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_4

LANGUAGE: python
CODE:
```
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_inputs = tokenizer(batch_sentences)
>>> print(encoded_inputs)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102],
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
               [101, 1327, 1164, 5450, 23434, 136, 102]],
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0]],
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1]]}
```

----------------------------------------

TITLE: Modifying PyTorch training loop for distributed training
DESCRIPTION: Comprehensive diff showing the changes needed to adapt a standard PyTorch training loop for distributed training using ðŸ¤— Accelerate.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/accelerate.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

----------------------------------------

TITLE: Loading DistilBERT Tokenizer for Text Preprocessing
DESCRIPTION: Initializes the DistilBERT tokenizer which will be used to convert text into token IDs that the model can process. Uses the uncased version of the base DistilBERT model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Complete Training Script Modifications
DESCRIPTION: Full example showing all necessary changes to implement distributed training with Accelerate
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/accelerate.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

----------------------------------------

TITLE: Training Image Captioning Model Using Trainer in Python
DESCRIPTION: Initialize and run a Trainer instance for fine-tuning the image captioning model, integrating dataset, training arguments, and evaluation metrics.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_captioning.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    compute_metrics=compute_metrics,
)
```

LANGUAGE: python
CODE:
```
trainer.train()
```

----------------------------------------

TITLE: Beam Search Decoding with Transformers in Python
DESCRIPTION: This code demonstrates how to use beam search decoding for text generation using the Hugging Face Transformers library. It initializes a tokenizer and a causal language model, and generates text using beam search with `num_beams=5`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/generation_strategies.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> prompt = "It is astonishing how one can"
>>> checkpoint = "openai-community/gpt2-medium"

>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)

>>> outputs = model.generate(**inputs, num_beams=5, max_new_tokens=50)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['It is astonishing how one can have such a profound impact on the lives of so many people in such a short period of
time."\n\nHe added: "I am very proud of the work I have been able to do in the last few years.\n\n"I have']
```

----------------------------------------

TITLE: Decoding Input IDs to Text in Python
DESCRIPTION: This snippet shows how to decode `input_ids` back into the original text using the tokenizer's `decode` method. This can be useful for verifying the tokenization process.  It also shows that special tokens such as `[CLS]` and `[SEP]` may be automatically added.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/preprocessing.md#_snippet_2

LANGUAGE: Python
CODE:
```
>>> tokenizer.decode(encoded_input["input_ids"])
'[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]'
```

----------------------------------------

TITLE: Manual Inference with PyTorch: Tokenization
DESCRIPTION: The first step of manual inference with PyTorch is to tokenize the input text. This converts the text into tensors that the model can process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_25

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_model")
>>> inputs = tokenizer(text, return_tensors="pt")
```

----------------------------------------

TITLE: Fine-tuning RoBERTa on SWAG dataset using Trainer
DESCRIPTION: This bash command demonstrates how to fine-tune a RoBERTa model on the SWAG dataset using the Trainer class from the Transformers library. It specifies various hyperparameters such as learning rate, number of epochs, and batch sizes.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/multiple-choice/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python examples/pytorch/multiple-choice/run_swag.py \
--model_name_or_path FacebookAI/roberta-base \
--do_train \
--do_eval \
--learning_rate 5e-5 \
--num_train_epochs 3 \
--output_dir /tmp/swag_base \
--per_device_eval_batch_size=16 \
--per_device_train_batch_size=16 \
--overwrite_output
```

----------------------------------------

TITLE: Installing dependencies
DESCRIPTION: Installs the required libraries: datasets, transformers, accelerate, timm, albumentations, torchmetrics, and pycocotools. `accelerate` is used to easily enable training on multiple GPUs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/object_detection.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
"pip install -q datasets transformers accelerate timm\npip install -q -U albumentations>=1.4.5 torchmetrics pycocotools"
```

----------------------------------------

TITLE: Preparing training components with Accelerator
DESCRIPTION: Example of using the prepare method to set up dataloaders, model, and optimizer for distributed training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/accelerate.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)
```

----------------------------------------

TITLE: Loading DistilBertForSequenceClassification (PyTorch)
DESCRIPTION: This code loads a pre-trained DistilBertForSequenceClassification model for sequence classification tasks in PyTorch. Requires `DistilBertForSequenceClassification` from the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/create_a_model.md#_snippet_11

LANGUAGE: python
CODE:
```
>>> from transformers import DistilBertForSequenceClassification

>>> model = DistilBertForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Loading PyTorch Model and Tokenizer for Multilingual Sentiment Analysis
DESCRIPTION: This snippet demonstrates how to load a pre-trained PyTorch model and tokenizer for multilingual sentiment analysis using AutoModelForSequenceClassification and AutoTokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/quicktour.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

----------------------------------------

TITLE: Load Model and Tokenizer (TensorFlow)
DESCRIPTION: Loads a pre-trained model and its associated tokenizer using `TFAutoModelForSequenceClassification` and `AutoTokenizer` from the `transformers` library. This is the TensorFlow implementation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_12

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

----------------------------------------

TITLE: Loading Image Processor with AutoImageProcessor - Python
DESCRIPTION: This snippet demonstrates how to load a pretrained image processor for the model 'google/vit-base-patch16-224' using AutoImageProcessor. This processor can perform operations like resizing, normalizing, and rescaling as per the model's configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/image_processors.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import AutoImageProcessor

image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
```

----------------------------------------

TITLE: Fine-tuning Mamba 2 with LoRA
DESCRIPTION: Provides a script for fine-tuning the Mamba 2 model using LoRA adaptation. Includes setup for training arguments, LoRA configuration, and uses the SFTTrainer for the training process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mamba2.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from trl import SFTTrainer
from peft import LoraConfig
from transformers import AutoTokenizer, Mamba2ForCausalLM, TrainingArguments
model_id = 'mistralai/Mamba-Codestral-7B-v0.1'
tokenizer = AutoTokenizer.from_pretrained(model_id, revision='refs/pr/9', from_slow=True, legacy=False)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left" #enforce padding side left

model = Mamba2ForCausalLM.from_pretrained(model_id, revision='refs/pr/9')
dataset = load_dataset("Abirate/english_quotes", split="train")
# Without CUDA kernels, batch size of 2 occupies one 80GB device
# but precision can be reduced.
# Experiments and trials welcome!
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    logging_dir='./logs',
    logging_steps=10,
    learning_rate=2e-3
)
lora_config =  LoraConfig(
        r=8,
        target_modules=["embeddings", "in_proj", "out_proj"],
        task_type="CAUSAL_LM",
        bias="none"
)
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    peft_config=lora_config,
    train_dataset=dataset,
    dataset_text_field="quote",
)
trainer.train()
```

----------------------------------------

TITLE: Using Code Llama with Hugging Face Pipeline
DESCRIPTION: Python code snippet showing how to use the Code Llama model with the Hugging Face pipeline for text generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/code_llama.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline
>>> import torch

>>> generator = pipeline("text-generation",model="meta-llama/CodeLlama-7b-hf",torch_dtype=torch.float16, device_map="auto")
>>> generator('def remove_non_ascii(s: str) -> str:\n    """ <FILL_ME>\n    return result', max_new_tokens = 128)
```

----------------------------------------

TITLE: Installing Accelerate, Optimum and Transformers
DESCRIPTION: This command installs the necessary libraries: accelerate, optimum, and transformers, using pip. The --upgrade flag ensures that the latest versions are installed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/gptq.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
"pip install --upgrade accelerate optimum transformers"
```

----------------------------------------

TITLE: Creating and Applying a LoRA Adapter in Python
DESCRIPTION: This snippet demonstrates creating a LoRA adapter configuration and applying it to a causal language model. It involves setting parameters like task type, dimension, alpha, and dropout to optimize training efficiency.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/peft.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from peft import LoraConfig, TaskType, get_peft_model
from transformers import AutoModelForCausalLM

# create LoRA configuration object
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM, # type of task to train on
    inference_mode=False, # set to False for training
    r=8, # dimension of the smaller matrices
    lora_alpha=32, # scaling factor
    lora_dropout=0.1 # dropout of LoRA layers
)

```

LANGUAGE: python
CODE:
```
model.add_adapter(lora_config, adapter_name="lora_1")
trainer = Trainer(model=model, ...)
trainer.train()

```

----------------------------------------

TITLE: Configuring Training Arguments and Trainer in PyTorch
DESCRIPTION: This snippet defines training hyperparameters using `TrainingArguments` and initializes a `Trainer` for fine-tuning the model. The `Trainer` handles the training loop, evaluation, and saving checkpoints.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_wnut_model",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     num_train_epochs=2,
...     weight_decay=0.01,
...     eval_strategy="epoch",
...     save_strategy="epoch",
...     load_best_model_at_end=True,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_wnut["train"],
...     eval_dataset=tokenized_wnut["test"],
...     processing_class=tokenizer,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

----------------------------------------

TITLE: Uploading Fine-tuned Model to Hugging Face Hub
DESCRIPTION: Shares the fine-tuned model to the Hugging Face Hub, making it available for others to use for Visual Question Answering tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/visual_question_answering.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
>>> trainer.push_to_hub()
```

----------------------------------------

TITLE: Enabling FlashAttention-2 in Transformers
DESCRIPTION: This code snippet shows how to enable FlashAttention-2 for a causal language model using the Transformers library. It sets the `attn_implementation` parameter to `"flash_attention_2"` when loading the model with `from_pretrained`.  It also shows how to configure BitsAndBytes for quantization. This reduces memory usage and accelerates inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_optims.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quant_config = BitsAndBytesConfig(load_in_8bit=True)
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2b",
    quantization_config=quant_config,
    torch_dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
)
```

----------------------------------------

TITLE: Running FP4 model on single GPU
DESCRIPTION: Loads and runs an FP4 quantized model on a single GPU using `AutoModelForCausalLM`. The `device_map='auto'` argument is recommended for efficient resource allocation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_infer_gpu_one.md#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
"from transformers import AutoModelForCausalLM\n\nmodel_name = \"bigscience/bloom-2b5\"\nmodel_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)"
```

----------------------------------------

TITLE: Loading and Initializing LLM Model
DESCRIPTION: Loads a pre-trained LLM model with 4-bit quantization and automatic device mapping.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/llm_tutorial.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "openlm-research/open_llama_7b", device_map="auto", load_in_4bit=True
)
```

----------------------------------------

TITLE: Compiling the Generate Function for Efficiency - Python
DESCRIPTION: This snippet shows how to compile the entire generate function of a model to improve performance by optimizing input preparation and logit processor operations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_optims.md#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
model.generate = torch.compile(model.generate, mode="reduce-overhead", fullgraph=True)
input_text = "The theory of special relativity states "
input_ids = tokenizer(input_text, return_tensors="pt").to(model.device.type)
outputs = model.generate(**input_ids)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
```

----------------------------------------

TITLE: Loading Pre-trained Model (PyTorch)
DESCRIPTION: This code loads a pre-trained DistilBertModel. It uses the `from_pretrained` method to load the model's weights and configuration. Requires the `DistilBertModel` class from the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/create_a_model.md#_snippet_6

LANGUAGE: python
CODE:
```
>>> model = DistilBertModel.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Initializing Trainer and Training
DESCRIPTION: Initializes the `Trainer` with the model, training arguments, datasets, feature extractor, and compute metrics function.  Then, it starts the training process by calling the `train` method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/audio_classification.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
">>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=encoded_minds["train"],
...     eval_dataset=encoded_minds["test"],
...     processing_class=feature_extractor,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()"
```

----------------------------------------

TITLE: Detecting Objects in Images with Transformers in Python
DESCRIPTION: This snippet demonstrates how to perform object detection on images using the ðŸ¤— Transformers library. By initializing a detector pipeline and processing an image, it outputs the detected objects along with their confidence scores and bounding box coordinates.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/task_summary.md#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
>>> from transformers import pipeline

>>> detector = pipeline(task="object-detection")
>>> preds = detector(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"], "box": pred["box"]} for pred in preds]
>>> preds
[{"score": 0.9865,
  "label": "cat",
  "box": {"xmin": 178, "ymin": 154, "xmax": 882, "ymax": 598}}]
```

----------------------------------------

TITLE: Configuring Training Arguments for the Transformers Trainer
DESCRIPTION: Demonstrates how to set up training arguments for the Hugging Face Trainer, specifying the output directory for saving model checkpoints.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/training.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import TrainingArguments

>>> training_args = TrainingArguments(output_dir="test_trainer")
```

----------------------------------------

TITLE: Basic Voice Generation with Preset
DESCRIPTION: Demonstrates basic usage of Bark for generating speech using a voice preset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/bark.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor, BarkModel

>>> processor = AutoProcessor.from_pretrained("suno/bark")
>>> model = BarkModel.from_pretrained("suno/bark")

>>> voice_preset = "v2/en_speaker_6"

>>> inputs = processor("Hello, my dog is cute", voice_preset=voice_preset)

>>> audio_array = model.generate(**inputs)
>>> audio_array = audio_array.cpu().numpy().squeeze()
```

----------------------------------------

TITLE: Pipeline with Dataset Integration
DESCRIPTION: Shows how to use pipeline with Hugging Face datasets for efficient processing of large datasets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/pipelines.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import datasets
from transformers import pipeline
from transformers.pipelines.pt_utils import KeyDataset
from tqdm.auto import tqdm

pipe = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h", device=0)
dataset = datasets.load_dataset("superb", name="asr", split="test")

for out in tqdm(pipe(KeyDataset(dataset, "file"))):
    print(out)
```

----------------------------------------

TITLE: Splitting Dataset for Training and Testing
DESCRIPTION: This snippet takes the loaded ELI5 dataset and splits it into training and testing subsets, allocating 20% of the dataset for testing. This ensures effective model evaluation after training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/language_modeling.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
eli5 = eli5.train_test_split(test_size=0.2)
```

----------------------------------------

TITLE: Loading BERT Tokenizer
DESCRIPTION: This code loads a pre-trained BERT tokenizer from the Hugging Face Model Hub using `AutoTokenizer.from_pretrained`. The tokenizer is used to convert the input text into numerical tokens that the BERT model can understand.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/multiple_choice.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
">>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")"
```

----------------------------------------

TITLE: Hugging Face CLI Login - Bash
DESCRIPTION: This command is used to log in to your Hugging Face account from the command line. It stores your authentication token in the Hugging Face cache directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/model_sharing.md#_snippet_1

LANGUAGE: bash
CODE:
```
huggingface-cli login
```

----------------------------------------

TITLE: Using MADLAD-400 for Translation with Transformers
DESCRIPTION: This code snippet demonstrates how to use the MADLAD-400 model for machine translation using the Hugging Face `transformers` library. It loads a pre-trained MADLAD-400 model and tokenizer, translates an input text from Portuguese to English, and prints the translated output. The snippet showcases the basic steps for using the model for inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/madlad-400.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

>>> model = AutoModelForSeq2SeqLM.from_pretrained("google/madlad400-3b-mt")
>>> tokenizer = AutoTokenizer.from_pretrained("google/madlad400-3b-mt")

>>> inputs = tokenizer("<2pt> I love pizza!", return_tensors="pt")
>>> outputs = model.generate(**inputs)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
['Eu amo pizza!']
```

----------------------------------------

TITLE: Loading and Using Mistral Base Model in Python
DESCRIPTION: Demonstrates how to load the Mistral-7B base model, tokenize input, and generate text using the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/mistral.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1", device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")

>>> prompt = "My favourite condiment is"

>>> model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
>>> model.to(device)

>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"My favourite condiment is to ..."
```

----------------------------------------

TITLE: Loading Different Models Using AutoModelForCausalLM - Python
DESCRIPTION: This code snippet illustrates the ease of switching between multiple pretrained models for causal language modeling tasks with the same API. It shows how to load different models using AutoModelForCausalLM.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/models.md#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
from transformers import AutoModelForCausalLM

# use the same API to load 3 different models
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1")
model = AutoModelForCausalLM.from_pretrained("google/gemma-7b")
```

----------------------------------------

TITLE: Using StableLM 3B 4E1T for Causal Language Modeling Inference
DESCRIPTION: This snippet demonstrates how to load the StableLM 3B 4E1T model and tokenizer for text generation. It initializes the model, prepares an input prompt about weather, and generates a continuation using sampling.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/stablelm.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed
>>> device = "cuda" # the device to load the model onto

>>> set_seed(0)

>>> tokenizer = AutoTokenizer.from_pretrained("stabilityai/stablelm-3b-4e1t")
>>> model = AutoModelForCausalLM.from_pretrained("stabilityai/stablelm-3b-4e1t")
>>> model.to(device)  # doctest: +IGNORE_RESULT

>>> model_inputs = tokenizer("The weather is always wonderful in", return_tensors="pt").to(model.device)

>>> generated_ids = model.generate(**model_inputs, max_length=32, do_sample=True)
>>> responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
>>> responses
['The weather is always wonderful in Costa Rica, which makes it a prime destination for retirees. That\'s where the Pensionado program comes in, offering']
```

----------------------------------------

TITLE: Image Classification using Transformers Pipeline (Python)
DESCRIPTION: Shows how to initialize and use the `image-classification` pipeline with a given model (facebook/dinov2-small-imagenet1k-1-layer). It takes an image URL as input and returns a list of the most likely labels along with their confidence scores.
SOURCE: https://github.com/huggingface/transformers/blob/main/README.md#_snippet_7

LANGUAGE: Python
CODE:
```
from transformers import pipeline

pipeline = pipeline(task="image-classification", model="facebook/dinov2-small-imagenetk-1-layer")
pipeline("https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png")
[{'label': 'macaw', 'score': 0.997848391532898},
 {'label': 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',
  'score': 0.0016551691805943847},
 {'label': 'lorikeet', 'score': 0.00018523589824326336},
 {'label': 'African grey, African gray, Psittacus erithacus',
  'score': 7.85409429227002e-05},
 {'label': 'quail', 'score': 5.502637941390276e-05}]
```

----------------------------------------

TITLE: Tokenizing Dataset (TensorFlow)
DESCRIPTION: This code defines a function to tokenize the input text using the loaded tokenizer and applies it to the dataset using the `map` method. This function tokenizes the dataset similarly to the PyTorch implementation but prepares the data for TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
def tokenize_dataset(dataset):
    return tokenizer(dataset["text"])
dataset = dataset.map(tokenize_dataset)
```

----------------------------------------

TITLE: Print the Tokenized Sequence Python
DESCRIPTION: This snippet prints the tokenized sequence, showing how the tokenizer has split the original text into tokens. Subwords are prefixed with '##' to indicate that they are part of a larger word.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/glossary.md#_snippet_7

LANGUAGE: python
CODE:
```
>>> print(tokenized_sequence)
['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']
```

----------------------------------------

TITLE: Processing Image and Text Inputs for Zero-shot Classification
DESCRIPTION: Code to prepare model inputs using a processor that handles both image preprocessing and text tokenization for candidate labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/zero_shot_image_classification.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> candidate_labels = ["tree", "car", "bike", "cat"]
>>> inputs = processor(images=image, text=candidate_labels, return_tensors="pt", padding=True)
```

----------------------------------------

TITLE: Large Dataset Inference with DistilBERT Classification Pipeline
DESCRIPTION: Demonstrates batch inference on a large text classification dataset using HuggingFace Transformers pipeline and KeyDataset utility
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
dataset = datasets.load_dataset("imdb", name="plain_text", split="unsupervised")
pipeline = pipeline(task="text-classification", model="distilbert/distilbert-base-uncased-finetuned-sst-2-english", device="cuda")
for out in pipeline(KeyDataset(dataset, "text"), batch_size=8, truncation="only_first"):
    print(out)
```

----------------------------------------

TITLE: Tokenizing and Encoding Text with BERT Tokenizer in Python
DESCRIPTION: This snippet demonstrates how to use the BERT tokenizer to tokenize a text sequence, convert tokens to input IDs, and decode the encoded sequence. It shows the process of splitting text into tokens, handling subwords, and preparing inputs for a BERT model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/glossary.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-cased")

>>> sequence = "A Titan RTX has 24GB of VRAM"

>>> tokenized_sequence = tokenizer.tokenize(sequence)

>>> print(tokenized_sequence)
['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']

>>> inputs = tokenizer(sequence)

>>> encoded_sequence = inputs["input_ids"]
>>> print(encoded_sequence)
[101, 138, 18696, 155, 1942, 3190, 1144, 1572, 13745, 1104, 159, 9664, 2107, 102]

>>> decoded_sequence = tokenizer.decode(encoded_sequence)

>>> print(decoded_sequence)
[CLS] A Titan RTX has 24GB of VRAM [SEP]
```

----------------------------------------

TITLE: Automatic Speech Recognition using Transformers Pipeline (Python)
DESCRIPTION: Illustrates how to use the `automatic-speech-recognition` pipeline with a specific model. It takes a URL pointing to an audio file as input and outputs the transcribed text.
SOURCE: https://github.com/huggingface/transformers/blob/main/README.md#_snippet_6

LANGUAGE: Python
CODE:
```
from transformers import pipeline

pipeline = pipeline(task="automatic-speech-recognition", model="openai/whisper-large-v3")
pipeline("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

----------------------------------------

TITLE: Batch Inference with IDEFICS in Python
DESCRIPTION: This snippet shows how to run inference on a batch of image prompts using IDEFICS.  It takes a list of prompts, where each prompt contains an image URL and some text. The model generates text for each image in the batch. The outputs are then iterated over to print each generated text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/idefics.md#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
>>> prompts = [
...     [   "https://images.unsplash.com/photo-1543349689-9a4d426bee8e?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3501&q=80",
...         "This is an image of ",
...     ],
...     [   "https://images.unsplash.com/photo-1623944889288-cd147dbb517c?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80",
...         "This is an image of ",
...     ],
...     [   "https://images.unsplash.com/photo-1471193945509-9ad0617afabf?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80",
...         "This is an image of ",
...     ],
... ]

>>> inputs = processor(prompts, return_tensors="pt").to("cuda")
>>> bad_words_ids = processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> for i,t in enumerate(generated_text):
...     print(f"{i}:\n{t}\n")
0:
This is an image of the Eiffel Tower in Paris, France.

1:
This is an image of a couple on a picnic blanket.

2:
This is an image of a vegetable stand.
```

----------------------------------------

TITLE: Loading DistilBERT model for sequence classification (PyTorch)
DESCRIPTION: This snippet demonstrates how to load a pre-trained DistilBERT model for sequence classification using `AutoModelForSequenceClassification`. It specifies the number of labels and the label mappings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

>>> model = AutoModelForSequenceClassification.from_pretrained(
...     "distilbert/distilbert-base-uncased", num_labels=2, id2label=id2label, label2id=label2id
... )
```

----------------------------------------

TITLE: Loading a Pre-trained Processor with AutoProcessor in Python
DESCRIPTION: This code demonstrates how to load a pre-trained processor for multimodal tasks, which combines multiple types of preprocessing tools (like image processors and tokenizers) needed for models that handle multiple input types.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/autoclass_tutorial.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")
```

----------------------------------------

TITLE: Single Media Mode Inference with LLaVa-NeXT-Video in Python
DESCRIPTION: This example demonstrates loading the LLaVa-NeXT-Video model in half-precision for efficient inference using PyTorch, and highlights usage with videos as input. It incorporates downloading a sample video, preparing conversation prompts, and generating output using the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llava_next_video.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
from huggingface_hub import hf_hub_download
import torch
from transformers import LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor

# Load the model in half-precision
model = LlavaNextVideoForConditionalGeneration.from_pretrained("llava-hf/LLaVA-NeXT-Video-7B-hf", torch_dtype=torch.float16, device_map="auto")
processor = LlavaNextVideoProcessor.from_pretrained("llava-hf/LLaVA-NeXT-Video-7B-hf")

# Load the video as an np.array, sampling uniformly 8 frames (can sample more for longer videos)
video_path = hf_hub_download(repo_id="raushan-testing-hf/videos-test", filename="sample_demo_1.mp4", repo_type="dataset")

conversation = [
    {

        "role": "user",
        "content": [
            {"type": "text", "text": "Why is this video funny?"},
            {"type": "video", "path": video_path},
            ],
    },
]

inputs = processor.apply_chat_template(conversation, num_frames=8, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors="pt")

out = model.generate(**inputs, max_new_tokens=60)
processor.batch_decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True)
```

----------------------------------------

TITLE: Running Instruction Tuned Model with Transformers in Python
DESCRIPTION: This snippet demonstrates how to load and run the Mixtral-8x7B-Instruct model, which is tuned for chat interactions. It involves preparing input messages using a chat template before generation. Ensure a CUDA-enabled environment for execution.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mixtral.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mixtral-8x7B-Instruct-v0.1", device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mixtral-8x7B-Instruct-v0.1")

>>> messages = [
...     {"role": "user", "content": "What is your favourite condiment?"},
...     {"role": "assistant", "content": "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!"},
...     {"role": "user", "content": "Do you have mayonnaise recipes?"}
... ]

>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")

>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"Mayonnaise can be made as follows: (...)"
```

----------------------------------------

TITLE: Importing and Using Transformers Pipeline for Sentiment Analysis
DESCRIPTION: This snippet imports the pipeline function from transformers and uses it to create a sentiment analysis pipeline. The pipeline can then be used to classify the sentiment of text inputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md#2025-04-23_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import pipeline
```

----------------------------------------

TITLE: Loading Image Processor for ViT with Python
DESCRIPTION: Loads a pretrained image processor for the ViT model, facilitating image transformation into tensors required for the model's input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_classification.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoImageProcessor

>>> checkpoint = "google/vit-base-patch16-224-in21k"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Using DistilBert with Question Answering Head in PyTorch
DESCRIPTION: Shows how to load a pre-trained DistilBert model with a question answering head in PyTorch for tasks like extractive QA or reading comprehension.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/create_a_model.md#2025-04-23_snippet_12

LANGUAGE: python
CODE:
```
>>> from transformers import DistilBertForQuestionAnswering

>>> model = DistilBertForQuestionAnswering.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Creating and Using a Sentiment Analysis Pipeline in French
DESCRIPTION: This code demonstrates how to create a sentiment analysis pipeline with a specified model and tokenizer, then use it to analyze French text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/quicktour.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ðŸ¤— Transformers.")
[{'label': '5 stars', 'score': 0.7273}]
```

----------------------------------------

TITLE: Implementing Translation with Transformers
DESCRIPTION: Shows how to use the translation pipeline from Transformers with the T5 small model to translate text from English to French. The pipeline requires specifying both the task and model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/task_summary.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import pipeline

text = "translate English to French: Hugging Face is a community-based open-source platform for machine learning."
translator = pipeline(task="translation", model="google-t5/t5-small")
translator(text)
```

----------------------------------------

TITLE: Procesando muestras de audio con el pipeline de reconocimiento de voz
DESCRIPTION: Extrae las primeras 4 muestras de audio del dataset y las procesa con el pipeline de reconocimiento de voz para obtener transcripciones.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/quicktour.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
resultado = reconocedor_de_voz(dataset[:4]["audio"])
print([d["text"] for d in resultado])
['ahora buenas eh a ver tengo un problema con vuestra aplicaciÃ³n resulta que que quiero hacer una transferencia bancaria a una cuenta conocida pero me da error la aplicaciÃ³n a ver que a ver que puede ser', 'la aplicaciÃ³n no cargue saldo de mi nueva cuenta', 'hola tengo un problema con la aplicaciÃ³n no carga y y tampoco veo que carga el saldo de mi cuenta nueva dice que la aplicaciÃ³n estÃ¡ siendo reparada y ahora no puedo acceder a mi cuenta no necesito inmediatamente', 'hora buena la aplicaciÃ³n no se carga la vida no carga el saldo de mi cuenta nueva dice que la villadenta siendo reparada y oro no puedo hacer a mi cuenta']
```

----------------------------------------

TITLE: Loading Data Collator (PyTorch)
DESCRIPTION: This code loads a DataCollatorWithPadding, which is used to create batches of data with padding to ensure that all sequences in a batch have the same length.  The tokenizer is passed to the data collator, which handles the padding process. This is necessary because Transformer models require input sequences of equal length.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

----------------------------------------

TITLE: Resuming Training from Specific Checkpoint
DESCRIPTION: Example of resuming training from a specific checkpoint by providing the checkpoint path to the resume_from_checkpoint parameter.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/trainer.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
trainer.train(resume_from_checkpoint="your-model/checkpoint-1000")
```

----------------------------------------

TITLE: Tokenizing Dataset with BERT Tokenizer in Python
DESCRIPTION: This code shows how to tokenize the dataset using a BERT tokenizer and apply the tokenization function to the entire dataset using the map method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/training.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)
```

----------------------------------------

TITLE: Loading Image Feature Extractor
DESCRIPTION: This code loads a pre-trained image processor for image data, specifically the 'google/vit-base-patch16-224' model. The `AutoImageProcessor.from_pretrained` method is used to load the image processor, which is responsible for preparing the image input for a vision transformer model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/preprocessing.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
```

----------------------------------------

TITLE: Installing Transformers Library
DESCRIPTION: This command is used to install the `transformers` library with a version greater than or equal to 4.48.0 using pip, which is required to use the Zamba2 model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/zamba2.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
"pip install transformers>=4.48.0"
```

----------------------------------------

TITLE: Configure Training Arguments (PyTorch)
DESCRIPTION: This snippet configures the training arguments using `TrainingArguments`.  It sets the output directory, learning rate, batch sizes, and number of training epochs. These arguments control the training process. The directory path is a string.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_34

LANGUAGE: python
CODE:
```
>>> from transformers import TrainingArguments

>>> training_args = TrainingArguments(
...	output_dir="path/to/save/folder/",
...	learning_rate=2e-5,
...	per_device_train_batch_size=8,
...	per_device_eval_batch_size=8,
...	num_train_epochs=2,
... )
```

----------------------------------------

TITLE: Processing Images for DETA Model in Python
DESCRIPTION: The DetaImageProcessor class handles image preprocessing and postprocessing for the DETA model. It includes methods for preparing input images and processing model outputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/deta.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
[[autodoc]] DetaImageProcessor
    - preprocess
    - post_process_object_detection
```

----------------------------------------

TITLE: Initializing and Using FP8 Quantized Model in Python
DESCRIPTION: This Python snippet demonstrates how to initialize a causal language model with fine-grained FP8 quantization using the Transformers library. It configures the model to load weights in the most memory-optimal data type and demonstrates text generation. This requires PyTorch and Transformers libraries and a compatible GPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/finegrained_fp8.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import FineGrainedFP8Config, AutoModelForCausalLM, AutoTokenizer

model_name = "meta-llama/Meta-Llama-3-8B"
quantization_config = FineGrainedFP8Config()
quantized_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto", quantization_config=quantization_config)

tokenizer = AutoTokenizer.from_pretrained(model_name)
input_text = "What are we having for dinner?"
input_ids = tokenizer(input_text, return_tensors="pt").to("cuda")

output = quantized_model.generate(**input_ids, max_new_tokens=10)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Initializing Accelerator
DESCRIPTION: This snippet imports the `Accelerator` class from the `accelerate` library and creates an instance of it. The `Accelerator` automatically detects the distributed setup and initializes the necessary components for training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/accelerate.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> from accelerate import Accelerator

>>> accelerator = Accelerator()
```

----------------------------------------

TITLE: Launching Training Script
DESCRIPTION: This command launches a training script using the Hugging Face Accelerate library. The `accelerate launch` command handles the distributed training setup, ensuring that the training script runs correctly across multiple GPUs or machines.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/accelerate.md#_snippet_6

LANGUAGE: bash
CODE:
```
accelerate launch train.py
```

----------------------------------------

TITLE: Setting Up PyTorch Model, Optimizer, and Learning Rate Scheduler
DESCRIPTION: Loads a pre-trained model, configures the AdamW optimizer, and creates a learning rate scheduler for fine-tuning in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/training.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSequenceClassification, get_scheduler
from torch.optim import AdamW
import torch

model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=5)
optimizer = AdamW(model.parameters(), lr=5e-5)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
```

----------------------------------------

TITLE: Zero-shot Object Detection with Grounding DINO
DESCRIPTION: Example showing how to perform zero-shot object detection using Grounding DINO. The code demonstrates loading the model, processing an image with text labels, and extracting detection results with confidence scores and bounding boxes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/grounding-dino.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import requests

import torch
from PIL import Image
from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection

model_id = "IDEA-Research/grounding-dino-tiny"
device = "cuda"

processor = AutoProcessor.from_pretrained(model_id)
model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)

image_url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(image_url, stream=True).raw)
# Check for cats and remote controls
text_labels = [["a cat", "a remote control"]]

inputs = processor(images=image, text=text_labels, return_tensors="pt").to(device)
with torch.no_grad():
    outputs = model(**inputs)

results = processor.post_process_grounded_object_detection(
    outputs,
    inputs.input_ids,
    box_threshold=0.4,
    text_threshold=0.3,
    target_sizes=[image.size[::-1]]
)

# Retrieve the first image result
result = results[0]
for box, score, labels in zip(result["boxes"], result["scores"], result["labels"]):
    box = [round(x, 2) for x in box.tolist()]
    print(f"Detected {labels} with confidence {round(score.item(), 3)} at location {box}")
```

----------------------------------------

TITLE: Running Int8 model on single GPU
DESCRIPTION: Loads and runs a mixed-Int8 quantized model on a single GPU using `AutoModelForCausalLM` and `BitsAndBytesConfig`. The `quantization_config` parameter enables Int8 quantization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_infer_gpu_one.md#2025-04-22_snippet_13

LANGUAGE: Python
CODE:
```
"from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\nmodel_name = \"bigscience/bloom-2b5\"\nmodel_8bit = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True))"
```

----------------------------------------

TITLE: Tokenizing and Padding to Specific Length
DESCRIPTION: This snippet demonstrates padding a batch of sentences to a specific length, defined by the `max_length` parameter. The `padding='max_length'` argument, combined with a defined `max_length` ensures all sequences are padded to the specified length.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pad_truncation.md#_snippet_2

LANGUAGE: Python
CODE:
```
tokenizer(batch_sentences, padding='max_length', max_length=42)
```

----------------------------------------

TITLE: Implementing ResNet Feature Extraction Model
DESCRIPTION: Creates a custom ResNet model class that inherits from PreTrainedModel for feature extraction. Wraps the timm ResNet implementation to extract hidden features from images.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/custom_models.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import PreTrainedModel
from timm.models.resnet import BasicBlock, Bottleneck, ResNet
from .configuration_resnet import ResnetConfig

BLOCK_MAPPING = {"basic": BasicBlock, "bottleneck": Bottleneck}

class ResnetModel(PreTrainedModel):
    config_class = ResnetConfig

    def __init__(self, config):
        super().__init__(config)
        block_layer = BLOCK_MAPPING[config.block_type]
        self.model = ResNet(
            block_layer,
            config.layers,
            num_classes=config.num_classes,
            in_chans=config.input_channels,
            cardinality=config.cardinality,
            base_width=config.base_width,
            stem_width=config.stem_width,
            stem_type=config.stem_type,
            avg_down=config.avg_down,
        )

    def forward(self, tensor):
        return self.model.forward_features(tensor)
```

----------------------------------------

TITLE: Loading and Initializing LLM for Text Generation
DESCRIPTION: This Python code snippet demonstrates how to load a pre-trained LLM model, initialize a tokenizer, and generate text using the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/llm_tutorial.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained(
...     "openlm-research/open_llama_7b", device_map="auto", load_in_4bit=True
... )

>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("openlm-research/open_llama_7b")
>>> model_inputs = tokenizer(["A list of colors: red, blue"], return_tensors="pt").to("cuda")

>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A list of colors: red, blue, green, yellow, black, white, and brown'
```

----------------------------------------

TITLE: Using Pipeline with Large Models and Accelerate in Python
DESCRIPTION: This example shows how to use the pipeline with large models using the Accelerate library, demonstrating both full-precision and 8-bit quantized loading.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/pipeline_tutorial.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
# pip install accelerate
import torch
from transformers import pipeline

pipe = pipeline(model="facebook/opt-1.3b", torch_dtype=torch.bfloat16, device_map="auto")
output = pipe("This is a cool example!", do_sample=True, top_p=0.95)

# 8-bit quantized loading
# pip install accelerate bitsandbytes
import torch
from transformers import pipeline

pipe = pipeline(model="facebook/opt-1.3b", device_map="auto", model_kwargs={"load_in_8bit": True})
output = pipe("This is a cool example!", do_sample=True, top_p=0.95)
```

----------------------------------------

TITLE: Creating Validation and Test Dataset Pipelines
DESCRIPTION: Sets up validation and test dataset transformations and creates the respective datasets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
val_transform = Compose(
    [
        ApplyTransformToKey(
            key="video",
            transform=Compose(
                [
                    UniformTemporalSubsample(num_frames_to_sample),
                    Lambda(lambda x: x / 255.0),
                    Normalize(mean, std),
                    Resize(resize_to),
                ]
            ),
        ),
    ]
)

val_dataset = pytorchvideo.data.Ucf101(
    data_path=os.path.join(dataset_root_path, "val"),
    clip_sampler=pytorchvideo.data.make_clip_sampler("uniform", clip_duration),
    decode_audio=False,
    transform=val_transform,
)

test_dataset = pytorchvideo.data.Ucf101(
    data_path=os.path.join(dataset_root_path, "test"),
    clip_sampler=pytorchvideo.data.make_clip_sampler("uniform", clip_duration),
    decode_audio=False,
    transform=val_transform,
)
```

----------------------------------------

TITLE: Loading and Preprocessing VoxPopuli Dataset
DESCRIPTION: Demonstrates loading the VoxPopuli multilingual speech dataset, filtering for Dutch language, and preparing audio data with correct sampling rate
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/text-to-speech.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from datasets import load_dataset, Audio

dataset = load_dataset("facebook/voxpopuli", "nl", split="train")
len(dataset)
dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
```

----------------------------------------

TITLE: Loading ELI5 Dataset with Datasets Library
DESCRIPTION: This snippet demonstrates loading the first 5000 examples from the ELI5 dataset using the Datasets library. The split argument allows users to specify the subset of the dataset for experimentation before full-scale training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from datasets import load_dataset

eli5 = load_dataset("eli5_category", split="train[:5000]")
```

----------------------------------------

TITLE: Loading SWAG Dataset
DESCRIPTION: Code to load the SWAG dataset for multiple choice training using the datasets library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/multiple_choice.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
from datasets import load_dataset

swag = load_dataset("swag", "regular")
```

----------------------------------------

TITLE: Loading BillSum Dataset Subset in Python
DESCRIPTION: Loads the California bills subset of the BillSum dataset using the Hugging Face datasets library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/summarization.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> billsum = load_dataset("billsum", split="ca_test")
```

----------------------------------------

TITLE: Using Text Generation Pipeline in Python
DESCRIPTION: This snippet demonstrates using the Hugging Face Transformers for text generation. It sets up a text-generation pipeline utilizing the Google Gemma 2 model. The input is a string prompt and the output generated text extends the prompt intelligently. Requires 'transformers' library installed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import pipeline

pipeline = pipeline(task="text-generation", model="google/gemma-2-2b")
pipeline("the secret to baking a really good cake is ")
[{'generated_text': 'the secret to baking a really good cake is 1. the right ingredients 2. the'}]
```

----------------------------------------

TITLE: Basic Pipeline Usage for Text Classification
DESCRIPTION: Demonstrates basic usage of the pipeline abstraction for text classification with a single input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/pipelines.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> pipe = pipeline("text-classification")
>>> pipe("This restaurant is awesome")
[{'label': 'POSITIVE', 'score': 0.9998743534088135}]
```

----------------------------------------

TITLE: Truncating Tokenized Sequences in Python
DESCRIPTION: This snippet shows how to truncate tokenized sequences to a specified maximum length. It uses the truncation and max_length parameters in the tokenizer function to limit sequence length.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/fast_tokenizers.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
encoded_inputs = tokenizer(batch_sentences, max_length=8, truncation=True, return_tensors="pt")
print(encoded_inputs)
```

----------------------------------------

TITLE: Image Classification Pipeline Example
DESCRIPTION: Example of using the pipeline for image classification
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/pipeline_tutorial.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import pipeline

vision_classifier = pipeline(task="image-classification")
vision_classifier(
    images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
)
```

----------------------------------------

TITLE: Using Granite Model for Text Generation in Python
DESCRIPTION: This snippet demonstrates how to load the Granite model (PowerLM-3b), tokenize input text, generate output tokens, and decode them back into text. It uses the AutoModelForCausalLM and AutoTokenizer classes from the transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/granite.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = "ibm/PowerLM-3b"
tokenizer = AutoTokenizer.from_pretrained(model_path)

# drop device_map if running on CPU
model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto")
model.eval()

# change input text as desired
prompt = "Write a code to find the maximum value in a list of numbers."

# tokenize the text
input_tokens = tokenizer(prompt, return_tensors="pt")
# generate output tokens
output = model.generate(**input_tokens, max_new_tokens=100)
# decode output tokens into text
output = tokenizer.batch_decode(output)
# loop over the batch to print, in this example the batch size is 1
for i in output:
    print(i)
```

----------------------------------------

TITLE: Using Multimodal Pipeline for Visual Question Answering
DESCRIPTION: This example shows how to use a multimodal pipeline for visual question answering (VQA). It processes both an image and a text question about the image, using LayoutLM to extract and answer information from a document image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/pipeline_tutorial.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> vqa = pipeline(model="impira/layoutlm-document-qa")
>>> output = vqa(
...     image="https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png",
...     question="What is the invoice number?",
... )
>>> output[0]["score"] = round(output[0]["score"], 3)
>>> output
[{'score': 0.425, 'answer': 'us-001', 'start': 16, 'end': 16}]
```

----------------------------------------

TITLE: Local PyTorch Model Export
DESCRIPTION: Example of saving and exporting a local PyTorch model to ONNX format
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/serialization.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> # Load tokenizer and PyTorch weights form the Hub
>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
>>> pt_model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
>>> # Save to disk
>>> tokenizer.save_pretrained("local-pt-checkpoint")
>>> pt_model.save_pretrained("local-pt-checkpoint")
```

----------------------------------------

TITLE: CLI Export of Transformers Model to ONNX
DESCRIPTION: Example command to export a Hugging Face model to ONNX format using the Optimum CLI
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/serialization.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
optimum-cli export onnx --model distilbert/distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/
```

----------------------------------------

TITLE: Configuring Training Parameters and Training a MLM Model (PyTorch)
DESCRIPTION: Code to set up training arguments, initialize a Trainer with the model and datasets, and start the training process for masked language modeling in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
training_args = TrainingArguments(
    output_dir="my_awesome_eli5_mlm_model",
    eval_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_dataset["train"],
    eval_dataset=lm_dataset["test"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)

trainer.train()
```

----------------------------------------

TITLE: Preparing Training Components
DESCRIPTION: Preparing training objects (dataloaders, model, optimizer) for distributed training
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/hi/accelerate.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)
```

----------------------------------------

TITLE: Using ASR Pipeline for Inference in Python
DESCRIPTION: Shows how to use the created ASR pipeline to transcribe audio from a URL.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/pipeline_tutorial.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES'}
```

----------------------------------------

TITLE: Image Classification with ViT AutoModel
DESCRIPTION: Detailed example showing how to load and use a ViT model directly with the AutoModel classes. Demonstrates image preprocessing, model inference, and class label prediction using the google/vit-base-patch16-224 checkpoint.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/vit.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
import requests
from PIL import Image
from transformers import AutoModelForImageClassification, AutoImageProcessor

image_processor = AutoImageProcessor.from_pretrained(
    "google/vit-base-patch16-224",
    use_fast=True,
)
model = AutoModelForImageClassification.from_pretrained(
    "google/vit-base-patch16-224",
    torch_dtype=torch.float16,
    device_map="auto",
    attn_implementation="sdpa"
)
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = image_processor(image, return_tensors="pt").to("cuda")

with torch.no_grad():
  logits = model(**inputs).logits
predicted_class_id = logits.argmax(dim=-1).item()

class_labels = model.config.id2label
predicted_class_label = class_labels[predicted_class_id]
print(f"The predicted class label is: {predicted_class_label}")
```

----------------------------------------

TITLE: Running Inference with Speech Recognition Pipeline in Python
DESCRIPTION: Passes an audio file URL to the speech recognition pipeline to transcribe speech to text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/hi/pipeline_tutorial.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
```

----------------------------------------

TITLE: Performing Depth Estimation Inference
DESCRIPTION: Demonstrates how to use the depth estimation pipeline to perform inference on the loaded image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/monocular_depth_estimation.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
>>> predictions = depth_estimator(image)
```

----------------------------------------

TITLE: Setting Up and Training T5 Model with PyTorch Trainer
DESCRIPTION: Loads a T5 model for sequence-to-sequence translation, configures training arguments, initializes the Seq2SeqTrainer, and starts the fine-tuning process with the prepared dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/translation.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
>>> training_args = Seq2SeqTrainingArguments(
...     output_dir="my_awesome_opus_books_model",
...     eval_strategy="epoch",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     weight_decay=0.01,
...     save_total_limit=3,
...     num_train_epochs=2,
...     predict_with_generate=True,
...     fp16=True,
...     push_to_hub=True,
... )

>>> trainer = Seq2SeqTrainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_books["train"],
...     eval_dataset=tokenized_books["test"],
...     processing_class=tokenizer,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

----------------------------------------

TITLE: Initializing Basic Text Generation Pipeline
DESCRIPTION: Creates a basic text generation pipeline using the default model and demonstrates text generation with single and multiple inputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/pipeline_tutorial.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import pipeline

generator = pipeline(task="text-generation")
```

----------------------------------------

TITLE: Installing Accelerate Library with pip
DESCRIPTION: Command to install the ðŸ¤— Accelerate library using pip, which is required for enabling distributed training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/accelerate.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install accelerate
```

----------------------------------------

TITLE: Configuring TrainingArguments for Hugging Face Trainer
DESCRIPTION: Example of configuring TrainingArguments with common parameters for training a model, including output directory, learning rate, batch sizes, evaluation strategy, and Hub integration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/trainer.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="your-model",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=True,
)
```

----------------------------------------

TITLE: Initializing TrainingArguments for AdamW Optimizer in PyTorch
DESCRIPTION: This snippet initializes the TrainingArguments with parameters specific for using the AdamW optimizer in PyTorch. Key parameters include output directory, batch size, learning rate, and logging strategy. Outputs are saved in the specified output directory for further training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/optimizers.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM, Trainer

args = TrainingArguments(
    output_dir="./test-optimizer",
    max_steps=1000,
    per_device_train_batch_size=4,
    logging_strategy="steps",
    logging_steps=1,
    learning_rate=2e-5,
    save_strategy="no",
    run_name="optimizer-name",
)
```

----------------------------------------

TITLE: Generating text with Llama using Pipeline (Python)
DESCRIPTION: This snippet demonstrates how to use the `pipeline` function from the `transformers` library to easily perform text generation with a Llama model. It loads the `huggyllama/llama-7b` model, specifies `torch.float16` dtype and uses device 0 (typically GPU). The `pipeline` object is then called with a prompt string to generate output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llama.md#_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipeline = pipeline(
    task="text-generation",
    model="huggyllama/llama-7b",
    torch_dtype=torch.float16,
    device=0
)
pipeline("Plants create energy through a process known as")
```

----------------------------------------

TITLE: Generating Text with Transformers Pipeline (Python)
DESCRIPTION: This snippet demonstrates how to use the `transformers` pipeline for text generation with a Qwen2 instruction-tuned model. It loads the model, formats input messages in a chat-like structure, and generates a response using specified parameters like `max_new_tokens`, `temperature`, `top_k`, and `top_p`. It requires `torch` and `transformers` dependencies.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2.md#_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipe = pipeline(
    task="text-generation",
    model="Qwen/Qwen2-1.5B-Instruct",
    torch_dtype=torch.bfloat16,
    device_map=0
)

messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Tell me about the Qwen2 model family."},
]
outputs = pipe(messages, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)
print(outputs[0]["generated_text"][-1]['content'])
```

----------------------------------------

TITLE: Creating a Data Collator for Batching
DESCRIPTION: Initializes a DefaultDataCollator to combine individual examples into batches for training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_question_answering.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator()
```

----------------------------------------

TITLE: Loading DistilBERT for Token Classification with PyTorch
DESCRIPTION: This code loads a pre-trained DistilBERT model for token classification using `AutoModelForTokenClassification` from the Transformers library. It initializes the model with the specified number of labels and the id2label and label2id mappings for associating predictions with labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer

>>> model = AutoModelForTokenClassification.from_pretrained(
...     "distilbert/distilbert-base-uncased", num_labels=13, id2label=id2label, label2id=label2id
... )
```

----------------------------------------

TITLE: Model Training Configuration
DESCRIPTION: Configuring and executing the training process using the Trainer API.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/audio_classification.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer

>>> num_labels = len(id2label)
>>> model = AutoModelForAudioClassification.from_pretrained(
...     "facebook/wav2vec2-base", num_labels=num_labels, label2id=label2id, id2label=id2label
... )

>>> training_args = TrainingArguments(
...     output_dir="my_awesome_mind_model",
...     eval_strategy="epoch",
...     save_strategy="epoch",
...     learning_rate=3e-5,
...     per_device_train_batch_size=32,
...     gradient_accumulation_steps=4,
...     per_device_eval_batch_size=32,
...     num_train_epochs=10,
...     warmup_ratio=0.1,
...     logging_steps=10,
...     load_best_model_at_end=True,
...     metric_for_best_model="accuracy",
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=encoded_minds["train"],
...     eval_dataset=encoded_minds["test"],
...     processing_class=feature_extractor,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

----------------------------------------

TITLE: Pushing Trained Model to Hugging Face Hub in PyTorch
DESCRIPTION: Demonstrates how to push the trained model to the Hugging Face Hub using the Trainer's push_to_hub method in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/translation.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
trainer.push_to_hub()
```

----------------------------------------

TITLE: Loading Image Processor for SegFormer
DESCRIPTION: Python code for loading the AutoImageProcessor for the SegFormer model, setting do_reduce_labels=True to properly handle background class indexing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoImageProcessor

>>> checkpoint = "nvidia/mit-b0"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint, do_reduce_labels=True)
```

----------------------------------------

TITLE: Fine-tuning MarianMT for English to Romanian Translation
DESCRIPTION: This snippet demonstrates how to fine-tune a MarianMT model for English to Romanian translation using the run_translation.py script. It specifies the model, languages, dataset, and training parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python examples/pytorch/translation/run_translation.py \
    --model_name_or_path Helsinki-NLP/opus-mt-en-ro \
    --do_train \
    --do_eval \
    --source_lang en \
    --target_lang ro \
    --dataset_name wmt16 \
    --dataset_config_name ro-en \
    --output_dir /tmp/tst-translation \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Splitting Dataset into Train and Test Sets in Python
DESCRIPTION: This snippet demonstrates the process of splitting the previously loaded dataset into training and testing subsets using the train_test_split method from the Datasets library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> ds = ds.train_test_split(test_size=0.2)
>>> train_ds = ds["train"]
>>> test_ds = ds["test"]
```

----------------------------------------

TITLE: Loading DistilRoBERTa for Masked Language Modeling with TensorFlow
DESCRIPTION: Loads a pre-trained DistilRoBERTa model for masked language modeling using TensorFlow's TFAutoModelForMaskedLM class.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/masked_language_modeling.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import TFAutoModelForMaskedLM

model = TFAutoModelForMaskedLM.from_pretrained("distilbert/distilroberta-base")
```

----------------------------------------

TITLE: Load Model in Half-Precision - LlavaOnevision
DESCRIPTION: This snippet demonstrates how to load the LlavaOnevision model in half-precision (torch.float16) to reduce memory usage. It uses the `from_pretrained` method with the `torch_dtype` and `device_map` parameters. It also initializes the processor for the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llava_onevision.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
"model = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\", torch_dtype=torch.float16, device_map=\"auto\")
processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")"
```

----------------------------------------

TITLE: Depth Estimation with AutoModel in PyTorch
DESCRIPTION: This snippet demonstrates using the `AutoImageProcessor` and `AutoModelForDepthEstimation` classes to perform depth estimation. It downloads and preprocesses an image from a URL, feeds it to the model, and post-processes the output to obtain a depth map.  The resulting depth map is normalized, converted to a uint8 image, and displayed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/depth_anything.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
import requests
import numpy as np
from PIL import Image
from transformers import AutoImageProcessor, AutoModelForDepthEstimation

image_processor = AutoImageProcessor.from_pretrained("LiheYoung/depth-anything-base-hf")
model = AutoModelForDepthEstimation.from_pretrained("LiheYoung/depth-anything-base-hf", torch_dtype=torch.bfloat16)
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = image_processor(images=image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

post_processed_output = image_processor.post_process_depth_estimation(
    outputs,
    target_sizes=[(image.height, image.width)],
)
predicted_depth = post_processed_output[0]["predicted_depth"]
depth = (predicted_depth - predicted_depth.min()) / (predicted_depth.max() - predicted_depth.min())
depth = depth.detach().cpu().numpy() * 255
Image.fromarray(depth.astype("uint8"))
```

----------------------------------------

TITLE: Perform Zero-Shot Image Classification with SigLIP2 using Pipeline (Python)
DESCRIPTION: This snippet shows how to perform zero-shot image classification effortlessly using the Hugging Face `pipeline` API with a SigLIP2 FixRes model. It requires the `transformers` library and `torch`. The pipeline handles model loading, processing, and inference, taking an image URL and candidate text labels as input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/siglip2.md#_snippet_0

LANGUAGE: Python
CODE:
```
import torch
from transformers import pipeline

image = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
candidate_labels = ["a Pallas cat", "a lion", "a Siberian tiger"]

pipeline = pipeline(task="zero-shot-image-classification", model="google/siglip2-base-patch16-224", device=0, torch_dtype=torch.bfloat16)
pipeline(image, candidate_labels=candidate_labels)
```

----------------------------------------

TITLE: Define Training Arguments for PyTorch Trainer
DESCRIPTION: Defines the training hyperparameters using `TrainingArguments` for use with the PyTorch `Trainer`. Key parameters include `output_dir` (where the model is saved), `eval_strategy` (how often to evaluate), `learning_rate`, `num_train_epochs`, `weight_decay`, and `push_to_hub` (whether to upload the model to the Hugging Face Hub).  Requires the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_13

LANGUAGE: python
CODE:
```
>>> training_args = TrainingArguments(
...	output_dir="my_awesome_eli5_mlm_model",
...	eval_strategy="epoch",
...	learning_rate=2e-5,
...	num_train_epochs=3,
...	weight_decay=0.01,
...	push_to_hub=True,
... )
```

----------------------------------------

TITLE: Configuring Training Arguments for Model Fine-Tuning (PyTorch)
DESCRIPTION: Sets up training configuration with specific hyperparameters for model fine-tuning, including output directory, learning rate, and push to hub option
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/masked_language_modeling.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_eli5_mlm_model",
...     eval_strategy="epoch",
...     learning_rate=2e-5,
...     num_train_epochs=3,
...     weight_decay=0.01,
...     push_to_hub=True,
... )
```

----------------------------------------

TITLE: Defining Training Arguments
DESCRIPTION: Defines the training hyperparameters using the `TrainingArguments` class. This includes specifying the output directory, evaluation strategy, learning rate, batch size, and other training parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/audio_classification.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
">>> training_args = TrainingArguments(
...     output_dir="my_awesome_mind_model",
...     eval_strategy="epoch",
...     save_strategy="epoch",
...     learning_rate=3e-5,
...     per_device_train_batch_size=32,
...     gradient_accumulation_steps=4,
...     per_device_eval_batch_size=32,
...     num_train_epochs=10,
...     warmup_ratio=0.1,
...     logging_steps=10,
...     load_best_model_at_end=True,
...     metric_for_best_model="accuracy",
...     push_to_hub=True,
... )"
```

----------------------------------------

TITLE: Loading Jamba Model and Generating Text with AutoModel (Python)
DESCRIPTION: This snippet loads the Jamba-Large-1.6 model and its corresponding tokenizer using the `transformers` Auto classes. It configures the model for half-precision (float16), automatic device mapping, and utilizes the SDPA attention implementation. The snippet then tokenizes an input prompt, performs text generation, and prints the decoded output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/jamba.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    "ai21labs/AI21-Jamba-Large-1.6",
)
model = AutoModelForCausalLM.from_pretrained(
    "ai21labs/AI21-Jamba-Large-1.6",
    torch_dtype=torch.float16,
    device_map="auto",
    attn_implementation="sdpa"
)
input_ids = tokenizer("Plants create energy through a process known as", return_tensors="pt").to("cuda")

output = model.generate(**input_ids, cache_implementation="static")
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Loading a Pre-trained Feature Extractor with AutoFeatureExtractor in Python
DESCRIPTION: This code snippet demonstrates how to load a pre-trained feature extractor using `AutoFeatureExtractor.from_pretrained` from the `transformers` library. The feature extractor is loaded from the specified checkpoint name, 'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition'.  This is typically used for audio or image processing tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/autoclass_tutorial.md#_snippet_2

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained(
...     "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
... )
```

----------------------------------------

TITLE: Generating Text with Mamba Model in Python
DESCRIPTION: This code snippet demonstrates how to use the MambaForCausalLM model from the Transformers library to generate text. It loads a pre-trained Mamba model and tokenizer, encodes an input prompt, and generates a sequence of tokens. The generated tokens are then decoded back into text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/mamba.md#_snippet_0

LANGUAGE: python
CODE:
```
from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained("state-spaces/mamba-130m-hf")
model = MambaForCausalLM.from_pretrained("state-spaces/mamba-130m-hf")
input_ids = tokenizer("Hey how are you doing?", return_tensors= "pt")["input_ids"]

out = model.generate(input_ids, max_new_tokens=10)
print(tokenizer.batch_decode(out))
```

----------------------------------------

TITLE: Loading FP4 Quantized Model for Single GPU Inference
DESCRIPTION: Load a Transformers model in 4-bit precision for efficient inference on a single GPU using bitsandbytes integration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/perf_infer_gpu_one.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model_name = "bigscience/bloom-2b5"
model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_4bit=True)
```

----------------------------------------

TITLE: Logging into Hugging Face Hub
DESCRIPTION: This code snippet logs into the Hugging Face Hub using the `notebook_login` function from the `huggingface_hub` library.  It requires the user to enter their Hugging Face token when prompted, enabling them to upload models and share them with the community.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Complete Chat Generation with Zephyr Model
DESCRIPTION: Demonstrates full chat generation pipeline using Zephyr model, including model loading, tokenization, and text generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/chat_templating.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

checkpoint = "HuggingFaceH4/zephyr-7b-beta"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint)

messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a pirate",
    },
    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
 ]
tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")
print(tokenizer.decode(tokenized_chat[0]))

outputs = model.generate(tokenized_chat, max_new_tokens=128)
print(tokenizer.decode(outputs[0]))
```

----------------------------------------

TITLE: Padding Sequences with Tokenizer in Python
DESCRIPTION: This snippet demonstrates how to use the tokenizer to pad sequences to the same length. The `padding=True` argument tells the tokenizer to add padding tokens to the shorter sequence.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/glossary.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)
```

----------------------------------------

TITLE: Initializing Image Feature Extraction Pipeline - Transformers, Python
DESCRIPTION: This snippet shows how to initialize the image-feature-extraction pipeline from the Hugging Face Transformers library for extracting image features. It includes automatic model loading with a specified device, enhancing computational efficiency.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_feature_extraction.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
import torch
from transformers import pipeline
from accelerate.test_utils.testing import get_backend
# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)
DEVICE, _, _ = get_backend()
pipe = pipeline(task="image-feature-extraction", model_name="google/vit-base-patch16-384", device=DEVICE, pool=True)
```

----------------------------------------

TITLE: Running Forward Pass with PyTorch Model
DESCRIPTION: This snippet shows how to run a forward pass through a PyTorch model using tokenized inputs and how to apply softmax to obtain probability distributions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/quicktour.md#2025-04-23_snippet_9

LANGUAGE: python
CODE:
```
pt_outputs = pt_model(**pt_batch)

from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)
tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],
        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)
```

----------------------------------------

TITLE: Authenticating with Hugging Face Hub
DESCRIPTION: Logs in to the Hugging Face Hub to enable pushing models during training. This authentication allows the trained model to be saved to the Hub automatically.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/knowledge_distillation_for_image_classification.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from huggingface_hub import notebook_login

notebook_login()
```

----------------------------------------

TITLE: Loading IDEFICS Model and Processor
DESCRIPTION: Python code to load the IDEFICS 9B parameter model checkpoint and its processor. Sets up the model with bfloat16 precision and automatic device mapping for optimal performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/idefics.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> checkpoint = "HuggingFaceM4/idefics-9b"
```

LANGUAGE: python
CODE:
```
>>> import torch

>>> from transformers import IdeficsForVisionText2Text, AutoProcessor

>>> processor = AutoProcessor.from_pretrained(checkpoint)

>>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map="auto")
```

----------------------------------------

TITLE: Loading PEFT Model in 8-bit Precision
DESCRIPTION: Example of loading a PEFT model in 8-bit precision using the BitsAndBytesConfig for memory efficiency.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/peft.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```

----------------------------------------

TITLE: Loading T5 Model for Seq2Seq Tasks in PyTorch
DESCRIPTION: Demonstrates how to load a T5 model using AutoModelForSeq2SeqLM for sequence-to-sequence tasks in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/translation.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Configuring Training Arguments and Trainer for ASR Model in Python
DESCRIPTION: This code sets up the training arguments and creates a Trainer for fine-tuning an ASR model. It includes various hyperparameters such as batch size, learning rate, and evaluation strategy, and configures the Trainer with the model, datasets, and processing utilities.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/asr.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_asr_mind_model",
...     per_device_train_batch_size=8,
...     gradient_accumulation_steps=2,
...     learning_rate=1e-5,
...     warmup_steps=500,
...     max_steps=2000,
...     gradient_checkpointing=True,
...     fp16=True,
...     group_by_length=True,
...     eval_strategy="steps",
...     per_device_eval_batch_size=8,
...     save_steps=1000,
...     eval_steps=1000,
...     logging_steps=25,
...     load_best_model_at_end=True,
...     metric_for_best_model="wer",
...     greater_is_better=False,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=encoded_minds["train"],
...     eval_dataset=encoded_minds["test"],
...     processing_class=processor,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

----------------------------------------

TITLE: Saving Model and Tokenizer (PyTorch)
DESCRIPTION: This snippet demonstrates how to save a fine-tuned model along with its tokenizer using `PreTrainedModel.save_pretrained` and `tokenizer.save_pretrained` in PyTorch.  The saved model and tokenizer can then be reloaded using `PreTrainedModel.from_pretrained`. The save directory is specified as a string.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_24

LANGUAGE: python
CODE:
```
>>> pt_save_directory = "./pt_save_pretrained"
>>> tokenizer.save_pretrained(pt_save_directory)  # doctest: +IGNORE_RESULT
>>> pt_model.save_pretrained(pt_save_directory)
```

----------------------------------------

TITLE: Loading Model and Processor Manually
DESCRIPTION: This code loads the OWL-ViT model and its associated processor directly from the Hugging Face Hub. This provides more control over the object detection process compared to using the pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/zero_shot_object_detection.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
">>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection

>>> model = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint)
>>> processor = AutoProcessor.from_pretrained(checkpoint)"
```

----------------------------------------

TITLE: Sharing Trained Model to Hugging Face Model Hub
DESCRIPTION: This command runs a summarization script and uploads the resulting model to the Hugging Face Model Hub, specifying a custom repository name using the 'push_to_hub_model_id' argument.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/run_scripts.md#2025-04-22_snippet_16

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py 
    --model_name_or_path google-t5/t5-small 
    --do_train 
    --do_eval 
    --dataset_name cnn_dailymail 
    --dataset_config "3.0.0" 
    --source_prefix "summarize: " 
    --push_to_hub 
    --push_to_hub_model_id finetuned-t5-cnn_dailymail 
    --output_dir /tmp/tst-summarization 
    --per_device_train_batch_size=4 
    --per_device_eval_batch_size=4 
    --overwrite_output_dir 
    --predict_with_generate
```

----------------------------------------

TITLE: Initializing and Running Trainer
DESCRIPTION: Example of initializing the Trainer class with model, datasets, and training arguments, then starting the training process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/trainer.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    processing_class=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()
```

----------------------------------------

TITLE: Initializing and running Trainer
DESCRIPTION: Set up the Trainer with the model, dataset, and training arguments, then start the training process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/tasks/asr.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_minds["train"],
    eval_dataset=encoded_minds["test"],
    processing_class=processor,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()
```

----------------------------------------

TITLE: Print Encoded Sequence (Input IDs) Python
DESCRIPTION: This snippet prints the encoded sequence (input IDs), which are the numerical representations of the tokens. The input IDs are used as input to the BERT model.  The `inputs` variable is expected to be a dictionary returned by the tokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/glossary.md#_snippet_10

LANGUAGE: python
CODE:
```
>>> encoded_sequence = inputs["input_ids"]
>>> print(encoded_sequence)
[101ØŒ 138ØŒ 18696ØŒ 155ØŒ 1942ØŒ 3190ØŒ 1144ØŒ 1572ØŒ 13745ØŒ 1104ØŒ 159ØŒ 9664ØŒ 2107ØŒ 102]
```

----------------------------------------

TITLE: Hugging Face Login
DESCRIPTION: Authentication setup for Hugging Face Hub access to enable model sharing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/audio_classification.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from huggingface_hub import notebook_login

notebook_login()
```

----------------------------------------

TITLE: Loading the ELI5 Dataset
DESCRIPTION: This snippet demonstrates how to load the ELI5 dataset from the Hugging Face Datasets library. It fetches the first 5000 records from the dataset for practical training purposes. Requires the 'datasets' library from Hugging Face.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/language_modeling.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> eli5 = load_dataset("eli5", split="train_asks[:5000]")
```

----------------------------------------

TITLE: Loading a Model and Pipeline for NLP Tasks in Python
DESCRIPTION: This snippet demonstrates how to load a specific model (Falcon-7b-instruct) and pipeline for text-generation tasks in Python, utilizing appropriate configurations for optimal performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/prompting.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline, AutoTokenizer
>>> import torch

>>> torch.manual_seed(0) # doctest: +IGNORE_RESULT
>>> model = "tiiuae/falcon-7b-instruct"

>>> tokenizer = AutoTokenizer.from_pretrained(model)
>>> pipe = pipeline(
...     "text-generation",
...     model=model,
...     tokenizer=tokenizer,
...     torch_dtype=torch.bfloat16,
...     device_map="auto",
... )
```

----------------------------------------

TITLE: Loading model for sequence classification
DESCRIPTION: This snippet demonstrates how to load a pre-trained model for sequence classification using `AutoModelForSequenceClassification`. It specifies the model name and the number of labels for the classification task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_3

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=5)
```

----------------------------------------

TITLE: Configuring and Running Training for ViT Model
DESCRIPTION: Sets up training arguments, initializes the Trainer, and starts the fine-tuning process for the ViT model on the Food-101 dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/image_classification.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> training_args = TrainingArguments(
...     output_dir="./results",
...     per_device_train_batch_size=16,
...     eval_strategy="steps",
...     num_train_epochs=4,
...     fp16=True,
...     save_steps=100,
...     eval_steps=100,
...     logging_steps=10,
...     learning_rate=2e-4,
...     save_total_limit=2,
...     remove_unused_columns=False,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     data_collator=data_collator,
...     train_dataset=food["train"],
...     eval_dataset=food["test"],
...     processing_class=image_processor,
... )

>>> trainer.train()
```

----------------------------------------

TITLE: Sharing Transformers Classes on the Hub
DESCRIPTION: Illustrates the use of the push_to_hub() method to share models, configurations, and preprocessing classes on the Hugging Face Hub, making them easily accessible to others.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/philosophy.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
push_to_hub()
```

----------------------------------------

TITLE: Loading and Initializing LLM Model
DESCRIPTION: Loads a pre-trained LLM model (Mistral-7B) with 4-bit quantization and automatic device mapping.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/llm_tutorial.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-v0.1", device_map="auto", load_in_4bit=True
)
```

----------------------------------------

TITLE: Loading Llama2 Model and Tokenizer in Python
DESCRIPTION: Python code snippet demonstrating how to load the Llama2 model and tokenizer after conversion to Hugging Face format. Uses the LlamaForCausalLM and LlamaTokenizer classes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/llama2.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import LlamaForCausalLM, LlamaTokenizer

tokenizer = LlamaTokenizer.from_pretrained("/output/path")
model = LlamaForCausalLM.from_pretrained("/output/path")
```

----------------------------------------

TITLE: Loading Code Llama Model and Tokenizer in Python
DESCRIPTION: Python code snippet demonstrating how to load the Code Llama model and tokenizer, and generate code completions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/code_llama.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import LlamaForCausalLM, CodeLlamaTokenizer

>>> tokenizer = CodeLlamaTokenizer.from_pretrained("meta-llama/CodeLlama-7b-hf")
>>> model = LlamaForCausalLM.from_pretrained("meta-llama/CodeLlama-7b-hf")
>>> PROMPT = '''def remove_non_ascii(s: str) -> str:
    """ <FILL_ME>
    return result
'''
>>> input_ids = tokenizer(PROMPT, return_tensors="pt")["input_ids"]
>>> generated_ids = model.generate(input_ids, max_new_tokens=128)

>>> filling = tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens = True)[0]
>>> print(PROMPT.replace("<FILL_ME>", filling))
```

----------------------------------------

TITLE: Using Vision Pipeline for Image Classification
DESCRIPTION: This code demonstrates how to use the vision pipeline to classify an image from a URL using a Vision Transformer model, showing how to process and display prediction results.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pipeline_tutorial.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> vision_classifier = pipeline(model="google/vit-base-patch16-224")
>>> preds = vision_classifier(
...     images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.4335, 'label': 'lynx, catamount'}, {'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.0239, 'label': 'Egyptian cat'}, {'score': 0.0229, 'label': 'tiger cat'}]
```

----------------------------------------

TITLE: Training EncoderDecoderModel (Python)
DESCRIPTION: This code shows how to fine-tune an EncoderDecoderModel. It initializes the model and tokenizer, sets decoder start and pad token IDs, prepares input and label tensors, and calculates the loss by passing the input IDs and labels to the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/encoder-decoder.md#_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import BertTokenizer, EncoderDecoderModel

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> model = EncoderDecoderModel.from_encoder_decoder_pretrained("google-bert/bert-base-uncased", "google-bert/bert-base-uncased")

>>> model.config.decoder_start_token_id = tokenizer.cls_token_id
>>> model.config.pad_token_id = tokenizer.pad_token_id

>>> input_ids = tokenizer(
...     "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side.During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was  finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft).Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.",
...     return_tensors="pt",
... ).input_ids

>>> labels = tokenizer(
...     "the eiffel tower surpassed the washington monument to become the tallest structure in the world. it was the first structure to reach a height of 300 metres in paris in 1930. it is now taller than the chrysler building by 5. 2 metres ( 17 ft ) and is the second tallest free - standing structure in paris.",
...     return_tensors="pt",
... ).input_ids

>>> # forward í•¨ìˆ˜ê°€ ìžë™ìœ¼ë¡œ ì í•©í•œ decoder_input_idsë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
>>> loss = model(input_ids=input_ids, labels=labels).loss
```

----------------------------------------

TITLE: Installing Required Dependencies
DESCRIPTION: Commands to install the main Transformers library and related packages needed for machine learning tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/quicktour.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
!pip install transformers datasets evaluate accelerate
```

----------------------------------------

TITLE: Using Pipeline for Masked Prediction
DESCRIPTION: Code to use the Hugging Face pipeline for fill-mask tasks with the finetuned model, returning the top k predictions for the masked token.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_23

LANGUAGE: python
CODE:
```
from transformers import pipeline

mask_filler = pipeline("fill-mask", "username/my_awesome_eli5_mlm_model")
mask_filler(text, top_k=3)
```

----------------------------------------

TITLE: Initializing and Running Trainer
DESCRIPTION: Sets up and executes the training process using Hugging Face Trainer with the configured model and datasets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/training.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()
```

----------------------------------------

TITLE: Tokenize Text with a Pretrained Tokenizer in Python
DESCRIPTION: This snippet demonstrates how to tokenize a sentence using a pretrained tokenizer. The tokenizer converts the input sentence into a dictionary containing `input_ids`, `token_type_ids`, and `attention_mask`.  `input_ids` are the numerical representation of each token, `attention_mask` indicates which tokens to attend to, and `token_type_ids` indicates the sequence each token belongs to.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/preprocessing.md#_snippet_1

LANGUAGE: Python
CODE:
```
>>> encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
>>> print(encoded_input)
{'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

----------------------------------------

TITLE: Padding Sequences to Equal Length with BERT Tokenizer
DESCRIPTION: This code shows how to batch process sequences of different lengths by padding them to equal length using the BERT tokenizer's padding functionality.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/glossary.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)
```

----------------------------------------

TITLE: Initializing Speech Recognition Pipeline in Python
DESCRIPTION: Creates a pipeline for automatic speech recognition using a specified pretrained model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/quicktour.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> import torch
>>> from transformers import pipeline

>>> speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")
```

----------------------------------------

TITLE: Loading a Pre-trained Tokenizer with AutoTokenizer in Python
DESCRIPTION: This snippet demonstrates how to load a pre-trained tokenizer using the AutoTokenizer class and tokenize a sample sequence.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/autoclass_tutorial.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")

>>> sequence = "In a hole in the ground there lived a hobbit."
>>> print(tokenizer(sequence))
{'input_ids': [101, 1999, 1037, 4920, 1999, 1996, 2598, 2045, 2973, 1037, 7570, 10322, 4183, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

----------------------------------------

TITLE: Generating Text with Falcon using AutoModel (Python)
DESCRIPTION: This code shows how to load the Falcon 7B Instruct model and tokenizer directly using `AutoTokenizer` and `AutoModelForCausalLM` for text generation. It loads the model with bfloat16 dtype, automatic device mapping, and specifies `sdpa` for attention implementation. It tokenizes an input prompt, moves it to the GPU, generates text, and decodes the output. Requires `transformers` and `torch`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/falcon.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-7b-instruct")
model = AutoModelForCausalLM.from_pretrained(
    "tiiuae/falcon-7b-instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    attn_implementation="sdpa",
)

input_ids = tokenizer("Write a short poem about coding", return_tensors="pt").to("cuda")

output = model.generate(**input_ids)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Performing Sentiment Analysis with Custom Model in Python
DESCRIPTION: Uses a custom model and tokenizer in a sentiment analysis pipeline to classify French text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/quicktour.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
>>> classifier("Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ðŸ¤— Transformers.")
[{'label': '5 stars', 'score': 0.7273}]
```

----------------------------------------

TITLE: Loading Pre-trained Models with PyTorch
DESCRIPTION: Example of loading a pre-trained BERT model using PyTorch backend in the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
model = AutoModel.from_pretrained("google-bert/bert-base-uncased")

inputs = tokenizer("Hello world!", return_tensors="pt")
outputs = model(**inputs)
```

----------------------------------------

TITLE: Training the Model with PyTorch
DESCRIPTION: This code snippet initializes the Trainer with the specified model, training arguments, datasets, and metrics computation function, and starts the training process for the semantic segmentation model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=train_ds,
...     eval_dataset=test_ds,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

----------------------------------------

TITLE: Installing Transformers with pip
DESCRIPTION: This command installs the ðŸ¤— Transformers library using pip. This is the standard installation method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/installation.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
"pip install transformers"
```

----------------------------------------

TITLE: Load AutoProcessor (Python)
DESCRIPTION: This code snippet loads an `AutoProcessor` from a pre-trained model using the `AutoProcessor.from_pretrained` method. The processor is used to preprocess audio and text data for multimodal tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/preprocessing.md#_snippet_26

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")
```

----------------------------------------

TITLE: Creating Trainer and Training Wav2Vec2 Model
DESCRIPTION: Initialize Trainer with model, training arguments, datasets, processor, and metrics, then initiate model training
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/asr.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_minds["train"],
    eval_dataset=encoded_minds["test"],
    processing_class=processor,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()
```

----------------------------------------

TITLE: Logging into Hugging Face Hub
DESCRIPTION: This snippet imports the `notebook_login` function from the `huggingface_hub` library and then calls it to authenticate the user with their Hugging Face account. This allows the user to upload the trained model to the Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/multiple_choice.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
">>> from huggingface_hub import notebook_login\n\n>>> notebook_login()"
```

----------------------------------------

TITLE: Load Gemma 2 with BitsAndBytes 4-bit Quantization (Python)
DESCRIPTION: Demonstrates how to load a Gemma 2 model (`google/gemma-2-27b`) with 4-bit quantization enabled using `BitsAndBytesConfig` to reduce memory footprint. It then proceeds to perform text generation using the quantized model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gemma2.md#_snippet_3

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=True)
tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-27b")
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2-27b",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    attn_implementation="sdpa"
)

input_text = "Explain quantum computing simply."
input_ids = tokenizer(input_text, return_tensors="pt").to("cuda")

outputs = model.generate(**input_ids, max_new_tokens=32, cache_implementation="static")
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Loading Pre-trained Models with PyTorch in Python
DESCRIPTION: Demonstrates how to download and use pre-trained models with PyTorch. The code initializes a tokenizer and model, and processes a simple text input to get model outputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/i18n/README_es.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> model = AutoModel.from_pretrained("google-bert/bert-base-uncased")

>>> inputs = tokenizer("Hello world!", return_tensors="pt")
>>> outputs = model(**inputs)
```

----------------------------------------

TITLE: Tokenizing a Sequence
DESCRIPTION: This snippet shows how to use the loaded tokenizer to process a text sequence. The tokenizer returns a dictionary containing `input_ids`, `token_type_ids`, and `attention_mask`. These are the standard inputs for many transformer models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/autoclass_tutorial.md#_snippet_1

LANGUAGE: Python
CODE:
```
>>> sequence = "In a hole in the ground there lived a hobbit."
>>> print(tokenizer(sequence))
{'input_ids': [101, 1999, 1037, 4920, 1999, 1996, 2598, 2045, 2973, 1037, 7570, 10322, 4183, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

----------------------------------------

TITLE: Tokenizing text (PyTorch)
DESCRIPTION: This snippet shows how to tokenize text using `AutoTokenizer` and return PyTorch tensors. The tokenizer is loaded from the fine-tuned model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_26

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_model")
>>> inputs = tokenizer(text, return_tensors="pt")
```

----------------------------------------

TITLE: Training the TensorFlow Model
DESCRIPTION: This trains the TensorFlow model using the prepared datasets, epochs, and callbacks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/multiple_choice.md#_snippet_21

LANGUAGE: python
CODE:
```
>>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=2, callbacks=callbacks)
```

----------------------------------------

TITLE: Continuing the Chat with LLaMA-3 in Python
DESCRIPTION: This snippet demonstrates how to continue a conversation by appending a user's response to the existing chat history and feeding it back into the pipeline. This allows for a multi-turn conversation with the chat model. The generated response is then printed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/conversations.md#_snippet_2

LANGUAGE: python
CODE:
```
chat = response[0]['generated_text']
chat.append(
    {"role": "user", "content": "Wait, what's so wild about soup cans?"}
)
response = pipe(chat, max_new_tokens=512)
print(response[0]['generated_text'][-1]['content'])
```

----------------------------------------

TITLE: Generating Model Inputs with Cached Key-Value Pairs - Python
DESCRIPTION: This snippet handles the generation of model inputs using a tokenizer while utilizing cached key-value pairs to improve the efficiency of subsequent rounds of conversation in transformer models. The parameters allow for custom token generation with specified output limits.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_29

LANGUAGE: python
CODE:
```
prompt = decoded_output + "\nQuestion: How can I modify the function above to return Mega bytes instead?\n\nAnswer: Here"
model_inputs = tokenizer(prompt, return_tensors='pt')
generation_output = model.generate(
  **model_inputs,
  past_key_values=generation_output.past_key_values,
  max_new_tokens=60,
  return_dict_in_generate=True
)
tokenizer.batch_decode(generation_output.sequences)[0][len(prompt):]
```

----------------------------------------

TITLE: Pushing Model During Training with Trainer
DESCRIPTION: Code demonstrating how to configure the Trainer to automatically push model updates to the Hub during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_sharing.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
training_args = TrainingArguments(output_dir="my-awesome-model", push_to_hub=True)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

trainer.push_to_hub()
```

----------------------------------------

TITLE: Multinomial Sampling with Transformers in Python
DESCRIPTION: This code demonstrates how to use multinomial sampling for text generation with the Hugging Face Transformers library. It initializes a tokenizer and a causal language model, sets a seed for reproducibility, and generates text using multinomial sampling with `do_sample=True` and `num_beams=1`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/generation_strategies.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
>>> set_seed(0)  # ìž¬í˜„ì„±ì„ ìœ„í•´

>>> checkpoint = "openai-community/gpt2-large"
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)

>>> prompt = "Today was an amazing day because"
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> outputs = model.generate(**inputs, do_sample=True, num_beams=1, max_new_tokens=100)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Today was an amazing day because when you go to the World Cup and you don\'t, or when you don\'t get invited,
that\'s a terrible feeling."']
```

----------------------------------------

TITLE: Exporting a Model to ONNX Programmatically
DESCRIPTION: This Python code snippet exports a ðŸ¤— Transformers model to ONNX programmatically using the `optimum.onnxruntime` library.  It loads the model, exports it to ONNX format, and saves the ONNX model and tokenizer to the specified directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/serialization.md#_snippet_6

LANGUAGE: python
CODE:
```
>>> from optimum.onnxruntime import ORTModelForSequenceClassification
>>> from transformers import AutoTokenizer

>>> model_checkpoint = "distilbert_base_uncased_squad"
>>> save_directory = "onnx/"

>>> # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† transformers ÙˆØªØµØ¯ÙŠØ±Ù‡ Ø¥Ù„Ù‰ ONNX
>>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)
>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

>>> # Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ onnx ÙˆÙ…Ø¬Ø²Ù‰Ø¡ Ø§Ù„Ù†ØµÙˆØµ
>>> ort_model.save_pretrained(save_directory)
>>> tokenizer.save_pretrained(save_directory)
```

----------------------------------------

TITLE: Initializing and Running Hugging Face Trainer
DESCRIPTION: Example of initializing a Trainer with model, training arguments, datasets, tokenizer, and other components, followed by calling the train method to start the training process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/trainer.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    processing_class=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()
```

----------------------------------------

TITLE: Resuming Training from Checkpoint
DESCRIPTION: This code snippet demonstrates how to resume training from a previously saved checkpoint using the `resume_from_checkpoint` argument in the `trainer.train()` method. It shows how to resume from the latest checkpoint or from a specific checkpoint within the output directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/trainer.md#_snippet_3

LANGUAGE: python
CODE:
```
# Ø§Ø³ØªØ£Ù†Ù Ù…Ù† Ø£Ø­Ø¯Ø« Ù†Ù‚Ø·Ø© Ø­ÙØ¸
trainer.train(resume_from_checkpoint=True)

# Ø§Ø³ØªØ£Ù†Ù Ù…Ù† Ù†Ù‚Ø·Ø© Ø­ÙØ¸ Ù…Ø­Ø¯Ø¯Ø© Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
trainer.train(resume_from_checkpoint="your-model/checkpoint-1000")
```

----------------------------------------

TITLE: Applying Transformations to a Dataset in Python
DESCRIPTION: Uses the set_transform method from ðŸ¤— Datasets to apply the transformation function to the dataset on-the-fly.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/preprocessing.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
dataset.set_transform(transforms)
```

----------------------------------------

TITLE: Loading and Preparing Yelp Review Dataset
DESCRIPTION: Demonstrates loading the Yelp Review dataset using Hugging Face datasets library and preparing it for training through tokenization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/training.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from datasets import load_dataset

dataset = load_dataset("yelp_review_full")
dataset["train"][100]
```

----------------------------------------

TITLE: Translation Pipeline using Hugging Face Transformers and Google T5 Model
DESCRIPTION: This code snippet demonstrates how to use the translation pipeline from Hugging Face Transformers with the Google T5 model. It translates a given English text to French.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/task_summary.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
from transformers import pipeline

text = "translate English to French: Hugging Face is a community-based open-source platform for machine learning."
translator = pipeline(task="translation", model="google-t5/t5-small")
translator(text)
```

----------------------------------------

TITLE: Initialize GPT-2 Model and Tokenizer in Python
DESCRIPTION: This snippet initializes a GPT-2 language model and its corresponding tokenizer using the Hugging Face Transformers library. It loads a pre-trained model from the `openai-community/gpt2-large` checkpoint and moves it to the specified device (CUDA if available).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/perplexity.md#_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import GPT2LMHeadModel, GPT2TokenizerFast

device = "cuda"
model_id = "openai-community/gpt2-large"
model = GPT2LMHeadModel.from_pretrained(model_id).to(device)
tokenizer = GPT2TokenizerFast.from_pretrained(model_id)
```

----------------------------------------

TITLE: Installing Required Libraries for Python
DESCRIPTION: This snippet demonstrates how to install the required libraries for working with transformers and datasets in Python. It uses pip to ensure that users have the necessary tools for the project.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/question_answering.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers datasets evaluate
```

----------------------------------------

TITLE: Visual Question Answering with IDEFICS in Python
DESCRIPTION: This snippet demonstrates how to use the IDEFICS model to answer questions about an image. It takes an image URL and a question as input, and generates an answer based on the visual content. The processor prepares the inputs for the model, and the `generate` method performs the inference. Bad word IDs are used to prevent the model from generating unwanted tokens.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/idefics.md#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
>>> prompt = [
...     "Instruction: Provide an answer to the question. Use the image to answer.\n",
...     "https://images.unsplash.com/photo-1623944889288-cd147dbb517c?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80",
...     "Question: Where are these people and what's the weather like? Answer:"
... ]

>>> inputs = processor(prompt, return_tensors="pt").to("cuda")
>>> bad_words_ids = processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, max_new_tokens=20, bad_words_ids=bad_words_ids)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> print(generated_text[0])
Instruction: Provide an answer to the question. Use the image to answer.
 Question: Where are these people and what's the weather like? Answer: They're in a park in New York City, and it's a beautiful day.
```

----------------------------------------

TITLE: Greedy Search Decoding with Transformers in Python
DESCRIPTION: This code illustrates using a greedy search decoding strategy in Hugging Face Transformers, which selects the most likely token at each step during text generation. Requires a pretrained model and tokenizer. It demonstrates decoding a sequence and producing textual output without special tokens removed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial.md#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
generated_ids = model.generate(**model_inputs)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

----------------------------------------

TITLE: Controlling Output Length in Text Generation
DESCRIPTION: Shows how to control the maximum number of new tokens generated using the max_new_tokens parameter.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/llm_tutorial.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
model_inputs = tokenizer(["A sequence of numbers: 1, 2"], return_tensors="pt").to("cuda")

# By default, the output will contain up to 20 tokens
generated_ids = model.generate(**model_inputs)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

# Setting `max_new_tokens` allows you to control the maximum length
generated_ids = model.generate(**model_inputs, max_new_tokens=50)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

----------------------------------------

TITLE: Initializing Pipeline with 8-bit Quantization - Python
DESCRIPTION: Demonstrates loading a large conversational model using `TextGenerationPipeline` with 8-bit quantization enabled via `BitsAndBytesConfig`. This significantly reduces memory requirements compared to full or half precision loading but may slightly impact model performance or output quality. Requires `transformers` and `bitsandbytes` libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/conversations.md#_snippet_5

LANGUAGE: python
CODE:
```
from transformers import pipeline, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_8bit=True)
pipeline = pipeline(task="text-generation", model="meta-llama/Meta-Llama-3-8B-Instruct", device_map="auto", model_kwargs={"quantization_config": quantization_config})
```

----------------------------------------

TITLE: Using Pipeline for Masked LM (Python)
DESCRIPTION: Demonstrates how to use the Hugging Face `pipeline` function with a BERT model (`google-bert/bert-base-uncased`) to perform a fill-mask task. It initializes the pipeline specifying the task, model, data type, and device, then runs the pipeline on an example sentence with a `[MASK]` token. Requires the `transformers` and `torch` libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bert.md#_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipeline = pipeline(
    task="fill-mask",
    model="google-bert/bert-base-uncased",
    torch_dtype=torch.float16,
    device=0
)
pipeline("Plants create [MASK] through a process known as photosynthesis.")
```

----------------------------------------

TITLE: Processing Multiple Inputs with Pipeline
DESCRIPTION: Example showing how to process multiple text inputs through a sentiment analysis pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/quicktour.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> results = classifier(["We are very happy to show you the ðŸ¤— Transformers library.", "We hope you don't hate it."])
>>> for result in results:
...     print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
```

----------------------------------------

TITLE: Downloading Model Repository from HuggingFace Hub in Python
DESCRIPTION: This code snippet demonstrates how to download a model repository from HuggingFace Hub using the snapshot_download method, which is key for offline usage.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
from huggingface_hub import snapshot_download

snapshot_download(repo_id="meta-llama/Llama-2-7b-hf", repo_type="model")
```

----------------------------------------

TITLE: Loading Pre-trained Model for Sequence Classification with AutoModel in Python
DESCRIPTION: This code snippet showcases how to load a pre-trained model for sequence classification using `AutoModelForSequenceClassification.from_pretrained` from the `transformers` library. It loads the model associated with the 'distilbert/distilbert-base-uncased' checkpoint, ready for sequence classification tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/autoclass_tutorial.md#_snippet_4

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Initialize Model for Training (PyTorch)
DESCRIPTION: This snippet initializes a PyTorch model for sequence classification using `AutoModelForSequenceClassification.from_pretrained`.  It loads a pre-trained DistilBERT model. This is the first step in setting up the training loop.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_33

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Loading GGUF model and tokenizer in Transformers (Python)
DESCRIPTION: This code snippet demonstrates how to load a GGUF model and tokenizer using the `from_pretrained` function in the Hugging Face `transformers` library.  It requires the `transformers` library to be installed and specifies the model ID and the GGUF filename for loading. The model is first dequantized to fp32.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/gguf.md#_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF"
filename = "tinyllama-1.1b-chat-v1.0.Q6_K.gguf"

tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)
model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)
```

----------------------------------------

TITLE: Performing Zero-shot Image Classification
DESCRIPTION: Uses the pipeline to classify an image with given candidate labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/zero_shot_image_classification.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
>>> predictions = detector(image, candidate_labels=["fox", "bear", "seagull", "owl"])
>>> predictions
```

----------------------------------------

TITLE: Loading a Model from Hugging Face Hub
DESCRIPTION: Demonstrates how to load a model that has been uploaded to the Hugging Face Hub using the AutoModel class.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/model_sharing.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModel

>>> model = AutoModel.from_pretrained("il-tuo-username/il-mio-bellissimo-modello")
```

----------------------------------------

TITLE: Installing Required Python Libraries
DESCRIPTION: Install necessary Python libraries such as 'transformers', 'datasets', and others required for performing token classification tasks. These dependencies are prerequisites for running the subsequent code snippets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/token_classification.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers datasets evaluate seqeval
```

----------------------------------------

TITLE: Instruct Model with Image Input using MllamaForConditionalGeneration
DESCRIPTION: This example demonstrates how to use the MllamaForConditionalGeneration model to generate responses based on input images and user messages. The code initializes the model and processor, prepares the inputs, and executes the generation of a response based on the provided input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mllama.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import MllamaForConditionalGeneration, AutoProcessor

model_id = "meta-llama/Llama-3.2-11B-Vision-Instruct"
model = MllamaForConditionalGeneration.from_pretrained(model_id, device_map="auto", torch_dtype=torch.bfloat16)
processor = AutoProcessor.from_pretrained(model_id)

messages = [
    [
        {
            "role": "user", 
            "content": [
                {"type": "image", "url": "https://llava-vl.github.io/static/images/view.jpg"},
                {"type": "text", "text": "What does the image show?"}
            ]
        }
    ],
]
inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors="pt").to(model.device)
output = model.generate(**inputs, max_new_tokens=25)
print(processor.decode(output[0]))
```

----------------------------------------

TITLE: Installing Required Libraries - Bash
DESCRIPTION: Command to install the necessary Python packages including transformers and bitsandbytes libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/llm_tutorial.md#2025-04-23_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers bitsandbytes>=0.39.0 -q
```

----------------------------------------

TITLE: Loading a Pre-trained DistilBert Fast Tokenizer in Python
DESCRIPTION: Shows how to load the Rust-based fast tokenizer for DistilBert which offers better performance especially for batch tokenization and additional features like offset mapping.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/create_a_model.md#2025-04-23_snippet_17

LANGUAGE: python
CODE:
```
>>> from transformers import DistilBertTokenizerFast

>>> fast_tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Pushing a Tokenizer to Hub
DESCRIPTION: Demonstrates how to push a tokenizer to an existing model repository on the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/model_sharing.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
>>> tokenizer.push_to_hub("il-mio-bellissimo-modello")
```

----------------------------------------

TITLE: Enabling ExLlamaV2 Kernels in AutoAWQ
DESCRIPTION: This code snippet demonstrates how to enable ExLlamaV2 kernels in AutoAWQ by setting `version="exllama"` in the `AwqConfig`. It imports necessary libraries, defines the quantization configuration, and loads a pre-trained model with the specified configuration, mapping it to the appropriate device.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/awq.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\n\nquantization_config = AwqConfig(version=\"exllama\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"TheBloke/Mistral-7B-Instruct-v0.1-AWQ\",\n    quantization_config=quantization_config,\n    device_map=\"auto\",\n)"
```

----------------------------------------

TITLE: Loading 4-bit Quantized Cohere Command-R Model
DESCRIPTION: This code snippet demonstrates how to load the Cohere Command-R model in 4-bit quantized format using the `BitsAndBytesConfig` class from the `transformers` library. This can reduce memory usage. Dependencies: transformers, bitsandbytes, accelerate.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/cohere.md#_snippet_2

LANGUAGE: python
CODE:
```
# pip install transformers bitsandbytes accelerate
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(load_in_4bit=True)

model_id = "CohereForAI/c4ai-command-r-v01"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)

gen_tokens = model.generate(
    input_ids, 
    max_new_tokens=100, 
    do_sample=True, 
    temperature=0.3,
    )

gen_text = tokenizer.decode(gen_tokens[0])
print(gen_text)
```

----------------------------------------

TITLE: Pushing Model to Hub Directly in Python
DESCRIPTION: Demonstrates how to use the push_to_hub function directly on a model to share it on the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/model_sharing.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> pt_model.push_to_hub("my-awesome-model")
```

----------------------------------------

TITLE: Loading 8-bit Quantized Model in Python
DESCRIPTION: This Python code snippet demonstrates how to load a model in 8-bit quantization using the Transformers library. It initializes a model with 8-bit precision by setting the load_in_8bit flag to True in the from_pretrained method. The example shows the usage of the AutoModelForCausalLM class and requires the model and tokenizer to be from the Hugging Face model hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
model = AutoModelForCausalLM.from_pretrained("bigcode/octocoder", load_in_8bit=True, pad_token_id=0)
```

----------------------------------------

TITLE: Loading a model in 8-bit precision with BitsAndBytesConfig
DESCRIPTION: Example of quantizing a BLOOM model to 8-bit precision using BitsAndBytesConfig with automatic device mapping.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_8bit=True)

model_8bit = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-1b7", 
    device_map="auto",
    quantization_config=quantization_config
)
```

----------------------------------------

TITLE: Activating Virtual Environment on Windows
DESCRIPTION: Command to activate the Python virtual environment on Windows systems.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/installation.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
.env/Scripts/activate
```

----------------------------------------

TITLE: Modifying Training Loop for Distributed Training
DESCRIPTION: Complete example showing the changes needed to adapt a standard PyTorch training loop for distributed training with ðŸ¤— Accelerate.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/accelerate.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

----------------------------------------

TITLE: TrainingArguments Configuration with Transformers
DESCRIPTION: This code snippet demonstrates how to configure training arguments using the `TrainingArguments` class from the Transformers library. It sets parameters such as output directory, learning rate, batch size, number of epochs, weight decay, evaluation strategy, save strategy, and the option to push the model to the Hugging Face Hub. These arguments control the training process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/trainer.md#_snippet_1

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="your-model",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=True,
)
```

----------------------------------------

TITLE: Constructing Attention Masks for Padded Sequences in Python
DESCRIPTION: This snippet illustrates how to derive the attention mask from the padded sequences generated by the tokenizer. The attention mask indicates which tokens the model should pay attention to and which are padding tokens that should be ignored.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/glossary.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> padded_sequences["attention_mask"]
[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]
```

----------------------------------------

TITLE: Loading Custom Models with Precaution - Transformers - Python
DESCRIPTION: This snippet shows how to load custom models from the Hugging Face Hub using the `transformers` library while enabling `trust_remote_code=True` for safety. It demonstrates loading a model with protection against changes in remote code and highlights the importance of specifying a commit revision.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/models.md#2025-04-22_snippet_18

LANGUAGE: Python
CODE:
```
from transformers import AutoModelForImageClassification

model = AutoModelForImageClassification.from_pretrained("sgugger/custom-resnet50d", trust_remote_code=True)
```

----------------------------------------

TITLE: Text Generation with Int8 Quantized Model
DESCRIPTION: Perform text generation using an int8 quantized Transformers model, demonstrating tokenization, generation, and output decoding.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/perf_infer_gpu_one.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = "bigscience/bloom-2b5"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True))

prompt = "Hello, my llama is cute"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
generated_ids = model.generate(**inputs)
outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
```

----------------------------------------

TITLE: Loading Pre-trained TF Model for Sequence Classification with AutoModel in Python
DESCRIPTION: This code snippet showcases how to load a pre-trained TensorFlow model for sequence classification using `TFAutoModelForSequenceClassification.from_pretrained` from the `transformers` library. It loads the model associated with the 'distilbert/distilbert-base-uncased' checkpoint, prepared for sequence classification tasks within a TensorFlow environment.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/autoclass_tutorial.md#_snippet_6

LANGUAGE: Python
CODE:
```
>>> from transformers import TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Starting the Model Fine-tuning Process
DESCRIPTION: Triggers the fine-tuning process by calling the train method on the configured Trainer instance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/training.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> trainer.train()
```

----------------------------------------

TITLE: Initializing and Training the Trainer
DESCRIPTION: This initializes the Trainer with the model, training arguments, datasets, tokenizer, data collator, and compute metrics function, then starts the training process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/multiple_choice.md#_snippet_12

LANGUAGE: python
CODE:
```
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_swag["train"],
...     eval_dataset=tokenized_swag["validation"],
...     processing_class=tokenizer,
...     data_collator=collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

----------------------------------------

TITLE: BERT Input Sequence Construction with Tokenizer
DESCRIPTION: This code snippet demonstrates how to construct an input sequence for BERT models using the `BertTokenizer`. It combines two sequences, sequence_a and sequence_b, using special tokens [CLS] and [SEP]. The tokenizer automatically handles the insertion of these tokens and returns a dictionary containing the encoded input IDs and token type IDs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/glossary.md#_snippet_13

LANGUAGE: python
CODE:
```
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-cased")
>>> sequence_a = "HuggingFace is based in NYC"
>>> sequence_b = "Where is HuggingFace based?"

>>> encoded_dict = tokenizer(sequence_aØŒ sequence_b)
>>> decoded = tokenizer.decode(encoded_dict["input_ids"])
```

----------------------------------------

TITLE: Quantizing Model with BitsAndBytesConfig
DESCRIPTION: This code snippet demonstrates how to quantize a pre-trained language model using the `BitsAndBytesConfig` class from the `transformers` library. It allows loading models in 8-bit or 4-bit precision to reduce memory footprint. The model is loaded onto the GPU using `device_map="auto"`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/conversations.md#_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_8bit=True) # ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ØªØ¬Ø±Ø¨Ø© load_in_4bit
model = AutoModelForCausalLM.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct", device_map="auto", quantization_config=quantization_config)
```

----------------------------------------

TITLE: Training a PEFT Adapter with Trainer
DESCRIPTION: Example of how to train a PEFT adapter using the Trainer class, including defining the adapter configuration and adding it to the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/peft.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from peft import LoraConfig

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)

model.add_adapter(peft_config)

trainer = Trainer(model=model, ...)
trainer.train()

model.save_pretrained(save_dir)
model = AutoModelForCausalLM.from_pretrained(save_dir)
```

----------------------------------------

TITLE: Downloading and Saving Pretrained Models for Offline Use
DESCRIPTION: Python code demonstrating how to download, save, and load pretrained models and tokenizers for offline use.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/installation.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

>>> tokenizer = AutoTokenizer.from_pretrained("bigscience/T0_3B")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0_3B")

>>> tokenizer.save_pretrained("./your/path/bigscience_t0")
>>> model.save_pretrained("./your/path/bigscience_t0")

>>> tokenizer = AutoTokenizer.from_pretrained("./your/path/bigscience_t0")
>>> model = AutoModel.from_pretrained("./your/path/bigscience_t0")
```

----------------------------------------

TITLE: Loading and Using Cohere Model with Transformers
DESCRIPTION: This code snippet demonstrates how to load the Cohere C4AI Command R7B model and its associated tokenizer using the Hugging Face Transformers library. It shows how to format a message using the command-r chat template, generate tokens, and decode them to produce text. The snippet requires the `transformers` library to be installed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/cohere2.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
```python
# pip install transformers
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "CohereForAI/c4ai-command-r7b-12-2024"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

# Format message with the command-r chat template
messages = [{"role": "user", "content": "Hello, how are you?"}]
input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")

gen_tokens = model.generate(
    input_ids,
    max_new_tokens=100,
    do_sample=True,
    temperature=0.3,
)

gen_text = tokenizer.decode(gen_tokens[0])
print(gen_text)
```
```

----------------------------------------

TITLE: Generating Text with LLM
DESCRIPTION: Uses the model's generate method to perform text generation based on the tokenized input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/llm_tutorial.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
generated_ids = model.generate(**model_inputs)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

----------------------------------------

TITLE: Initializing Transformer Models for Text Generation in Python
DESCRIPTION: Demonstrates the initialization of a tokenizer and model for text generation tasks using Hugging Face Transformers. Indicates loading of a causal language model with GPU support. Assumes `transformers` library is installed and configured.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial.md#2025-04-22_snippet_10

LANGUAGE: Python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-alpha")
model = AutoModelForCausalLM.from_pretrained(
    "HuggingFaceH4/zephyr-7b-alpha", device_map="auto", load_in_4bit=True
)
```

----------------------------------------

TITLE: Load Tokenizer (TensorFlow)
DESCRIPTION: This snippet loads a tokenizer using `AutoTokenizer.from_pretrained` in TensorFlow.  It loads a pre-trained DistilBERT tokenizer, which is necessary for preparing the input data for the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_42

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Text Generation with Single Input
DESCRIPTION: Example of generating text from a single input string using the pipeline
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/pipeline_tutorial.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
generator("Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone")
```

----------------------------------------

TITLE: Loading Pretrained PyTorch Model in Huggingface
DESCRIPTION: Demonstrates how to load a pretrained DistilBertModel with its weights, which is more efficient than training from scratch and provides immediately useful representations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/create_a_model.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> model = DistilBertModel.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Basic Pipeline Usage for Sentiment Analysis
DESCRIPTION: Example showing how to initialize and use the pipeline for sentiment analysis on text
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/quicktour.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline("sentiment-analysis")
>>> classifier("We are very happy to show you the ðŸ¤— Transformers library.")
[{'label': 'POSITIVE', 'score': 0.9998}]
```

----------------------------------------

TITLE: Manual Inference for Question Answering in PyTorch
DESCRIPTION: This snippet shows how to manually perform inference using the trained question answering model in PyTorch, including tokenization, model prediction, and answer extraction.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/question_answering.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch

tokenizer = AutoTokenizer.from_pretrained("my_awesome_qa_model")
inputs = tokenizer(question, context, return_tensors="pt")

model = AutoModelForQuestionAnswering.from_pretrained("my_awesome_qa_model")
with torch.no_grad():
    outputs = model(**inputs)

answer_start_index = outputs.start_logits.argmax()
answer_end_index = outputs.end_logits.argmax()

predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
tokenizer.decode(predict_answer_tokens)
```

----------------------------------------

TITLE: Loading a Large Language Model with Automatic Device Mapping
DESCRIPTION: This code loads the BLOOM model using automatic device mapping, which distributes the model layers across available GPUs. It sets the pad token ID to 0.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/llm_tutorial_optimization.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("bigscience/bloom", device_map="auto", pad_token_id=0)
```

----------------------------------------

TITLE: Inference using Pixtral with AutoProcessor and LlavaForConditionalGeneration
DESCRIPTION: This code snippet demonstrates how to use the `AutoProcessor` and `LlavaForConditionalGeneration` classes from the `transformers` library to perform inference with the Pixtral model. It loads the model and processor, prepares the input prompt using `apply_chat_template`, and generates text based on the image and text inputs. The output is then decoded to obtain the final result.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/pixtral.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoProcessor, LlavaForConditionalGeneration

model_id = "mistral-community/pixtral-12b"
processor = AutoProcessor.from_pretrained(model_id)
model = LlavaForConditionalGeneration.from_pretrained(model_id, device_map="cuda")

chat = [
    {
      "role": "user", "content": [
        {"type": "text", "content": "Can this animal"}, 
        {"type": "image", "url": "https://picsum.photos/id/237/200/300"}, 
        {"type": "text", "content": "live here?"}, 
        {"type": "image", "url": "https://picsum.photos/seed/picsum/200/300"}
      ]
    }
]

inputs = processor.apply_chat_template(
    chat,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt"
).to(model.device)

generate_ids = model.generate(**inputs, max_new_tokens=500)
output = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
```

----------------------------------------

TITLE: Generating Text with AutoModel/Processor (Python)
DESCRIPTION: Illustrates using the lower-level `AutoProcessor` and `Gemma3ForConditionalGeneration` classes to perform image-to-text generation. It covers loading the model and processor, structuring chat messages including an image URL and text prompt, applying the chat template, and generating text output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gemma3.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoProcessor, Gemma3ForConditionalGeneration

model = Gemma3ForConditionalGeneration.from_pretrained(
    "google/gemma-3-4b-it",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    attn_implementation="sdpa"
)
processor = AutoProcessor.from_pretrained(
    "google/gemma-3-4b-it",
    padding_side="left"
)

messages = [
    {
        "role": "system",
        "content": [
            {"type": "text", "text": "You are a helpful assistant."}
        ]
    },
    {
        "role": "user", "content": [
            {"type": "image", "url": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"},
            {"type": "text", "text": "What is shown in this image?"},
        ]
    },
]
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    return_dict=True,
    return_tensors="pt",
    add_generation_prompt=True,
).to("cuda")

output = model.generate(**inputs, max_new_tokens=50, cache_implementation="static")
print(processor.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Implementing a Text Generation Pipeline with LLaMA-3
DESCRIPTION: Sets up a text generation pipeline using LLaMA-3 model with bfloat16 precision to save memory and automatic device mapping. The code then generates a response to the chat input and prints the resulting content.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/conversations.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipe = pipeline("text-generation", "meta-llama/Meta-Llama-3-8B-Instruct", torch_dtype=torch.bfloat16, device_map="auto")
response = pipe(chat, max_new_tokens=512)
print(response[0]['generated_text'][-1]['content'])
```

----------------------------------------

TITLE: Tokenizing and Padding Sequences with BERT in Python
DESCRIPTION: This snippet demonstrates how to tokenize two sequences of different lengths using the BERT tokenizer, and how padding is applied to make them the same length. It also shows how to generate attention masks for the padded sequences.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/glossary.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-cased")

>>> sequence_a = "This is a short sequence."
>>> sequence_b = "This is a rather long sequence. It is at least longer than the sequence A."

>>> encoded_sequence_a = tokenizer(sequence_a)["input_ids"]
>>> encoded_sequence_b = tokenizer(sequence_b)["input_ids"]

>>> len(encoded_sequence_a), len(encoded_sequence_b)
(8, 19)

>>> padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)

>>> padded_sequences["input_ids"]
[[101, 1188, 1110, 170, 1603, 4954, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1188, 1110, 170, 1897, 1263, 4954, 119, 1135, 1110, 1120, 1655, 2039, 1190, 1103, 4954, 138, 119, 102]]

>>> padded_sequences["attention_mask"]
[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]
```

----------------------------------------

TITLE: Installing Required Libraries for Transformers
DESCRIPTION: Command to install the necessary Python packages for working with transformers, datasets, and model evaluation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/language_modeling.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers datasets evaluate
```

----------------------------------------

TITLE: Creando un pipeline para anÃ¡lisis de sentimiento en espaÃ±ol
DESCRIPTION: Crea un pipeline para anÃ¡lisis de sentimiento utilizando un modelo preentrenado especÃ­fico para espaÃ±ol.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/quicktour.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import pipeline

clasificador = pipeline("sentiment-analysis", model="pysentimiento/robertuito-sentiment-analysis")
```

----------------------------------------

TITLE: Loading and Using Cohere Command-R Model
DESCRIPTION: This code snippet demonstrates how to load the Cohere Command-R model and tokenizer using the `AutoTokenizer` and `AutoModelForCausalLM` classes from the `transformers` library. It also shows how to format messages with the command-r chat template and generate text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/cohere.md#_snippet_0

LANGUAGE: python
CODE:
```
# pip install transformers
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "CohereForAI/c4ai-command-r-v01"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

# Format message with the command-r chat template
messages = [{"role": "user", "content": "Hello, how are you?"}]
input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")
## <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Hello, how are you?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>

gen_tokens = model.generate(
    input_ids, 
    max_new_tokens=100, 
    do_sample=True, 
    temperature=0.3,
    )

gen_text = tokenizer.decode(gen_tokens[0])
print(gen_text)
```

----------------------------------------

TITLE: Saving Tokenizer and Model
DESCRIPTION: This Python code snippet demonstrates how to save a tokenizer and model to a local directory using the `save_pretrained` method. The tokenizer and model are saved to the specified directory path.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/installation.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
">>> tokenizer.save_pretrained("./your/path/bigscience_t0")\n>>> model.save_pretrained("./your/path/bigscience_t0")"
```

----------------------------------------

TITLE: Performing Named Entity Recognition (NER) in Python
DESCRIPTION: This snippet shows how to configure a LLM to extract named entities from a given text by providing a specific prompt. It demonstrates the collect entities effectively from the input text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/prompting.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> torch.manual_seed(1) # doctest: +IGNORE_RESULT
>>> prompt = """Return a list of named entities in the text.\
... Text: The Golden State Warriors are an American professional basketball team based in San Francisco.\
... Named entities:\
... """

>>> sequences = pipe(
...     prompt,
...     max_new_tokens=15,
...     return_full_text = False,
... )

>>> for seq in sequences:
...     print(f"{seq['generated_text']}")
- Golden State Warriors
- San Francisco
```

----------------------------------------

TITLE: Model Training Setup with PyTorch Trainer
DESCRIPTION: Configures and initializes a BERT model for sequence classification using the Hugging Face Trainer API with evaluation metrics.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/training.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=5)

from transformers import TrainingArguments

training_args = TrainingArguments(output_dir="test_trainer")

import numpy as np
import evaluate

metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(output_dir="test_trainer", eval_strategy="epoch")

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()
```

----------------------------------------

TITLE: Initializing TextGenerationPipeline and First Chat Turn - Python
DESCRIPTION: Loads a conversational model using `TextGenerationPipeline`, configures data type and device mapping for potential performance and memory improvements, and generates the first response from the model based on the provided initial chat history list. Requires `transformers` and `torch` libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/conversations.md#_snippet_3

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipeline = pipeline(task="text-generation", model="meta-llama/Meta-Llama-3-8B-Instruct", torch_dtype=torch.bfloat16, device_map="auto")
response = pipeline(chat, max_new_tokens=512)
print(response[0]["generated_text"][-1]["content"])
```

----------------------------------------

TITLE: Logging into Hugging Face Hub in Python
DESCRIPTION: Logs into the Hugging Face Hub to enable model uploading and sharing. Prompts for a token input for authentication.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/video_classification.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Initializing Trainer for Model Training
DESCRIPTION: Sets up the Trainer with the model, training arguments, dataset, and other components to manage the fine-tuning process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/visual_question_answering.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
>>> from transformers import Trainer

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     data_collator=data_collator,
...     train_dataset=processed_dataset,
...     processing_class=processor,
... )
```

----------------------------------------

TITLE: Loading a Pretrained Model for Sequence Classification
DESCRIPTION: Shows how to load a pretrained BERT model configured for sequence classification with the appropriate number of labels for the Yelp Reviews dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/training.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=5)
```

----------------------------------------

TITLE: Loading MInDS-14 Dataset
DESCRIPTION: Loading and splitting the MInDS-14 dataset using the Hugging Face datasets library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/audio_classification.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset, Audio

>>> minds = load_dataset("PolyAI/minds14", name="en-US", split="train")
>>> minds = minds.train_test_split(test_size=0.2)
>>> minds = minds.remove_columns(["path", "transcription", "english_transcription", "lang_id"])
```

----------------------------------------

TITLE: Training the PEFT model
DESCRIPTION: Trains the PEFT model using the Hugging Face `Trainer`. It passes the configured model to the trainer and then calls the `train` method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/peft.md#_snippet_12

LANGUAGE: python
CODE:
```
trainer = Trainer(model=model, ...)
trainer.train()
```

----------------------------------------

TITLE: Mapping Preprocessing Function to Dataset
DESCRIPTION: This code applies the `preprocess_function` to the entire SWAG dataset using the `map` method. Setting `batched=True` speeds up the preprocessing by processing multiple examples at once.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/multiple_choice.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
">>> tokenized_swag = swag.map(preprocess_function, batched=True)"
```

----------------------------------------

TITLE: Initializing DistilBERT Tokenizer for Question Answering
DESCRIPTION: Loads the DistilBERT tokenizer for preprocessing the question and context fields.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/question_answering.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Decode Encoded Sequence Python
DESCRIPTION: This snippet demonstrates how to decode an encoded sequence (input IDs) back into human-readable text using the tokenizer. It utilizes the `decode` method of the tokenizer.  It requires the `encoded_sequence` which contains numerical IDs that correspond to tokens in the tokenizer's vocabulary.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/glossary.md#_snippet_11

LANGUAGE: python
CODE:
```
>>> decoded_sequence = tokenizer.decode(encoded_sequence)
```

----------------------------------------

TITLE: Generating Text with GPT Neo in PyTorch
DESCRIPTION: This snippet demonstrates how to use the GPT Neo model for text generation using the generate() method. It loads a pre-trained model and tokenizer, then generates text based on a given prompt.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gpt_neo.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
from transformers import GPTNeoForCausalLM, GPT2Tokenizer

model = GPTNeoForCausalLM.from_pretrained("EleutherAI/gpt-neo-1.3B")
tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")

prompt = (
    "In a shocking finding, scientists discovered a herd of unicorns living in a remote, "
    "previously unexplored valley, in the Andes Mountains. Even more surprising to the "
    "researchers was the fact that the unicorns spoke perfect English."
)

input_ids = tokenizer(prompt, return_tensors="pt").input_ids

gen_tokens = model.generate(
    input_ids,
    do_sample=True,
    temperature=0.9,
    max_length=100,
)
gen_text = tokenizer.batch_decode(gen_tokens)[0]
```

----------------------------------------

TITLE: Loading Persimmon Model and Tokenizer (Python)
DESCRIPTION: This snippet shows how to load the Persimmon model and tokenizer from a specified directory using the `PersimmonForCausalLM` and `PersimmonTokenizer` classes from the `transformers` library. The `from_pretrained` method is used to instantiate the model and tokenizer from the saved weights and configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/persimmon.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
```py
from transformers import PersimmonForCausalLM, PersimmonTokenizer

model = PersimmonForCausalLM.from_pretrained("/output/path")
tokenizer = PersimmonTokenizer.from_pretrained("/output/path")
```
```

----------------------------------------

TITLE: Initializing Accelerator object in Python
DESCRIPTION: Code snippet to import and create an Accelerator object, which automatically detects the available distributed setup.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/accelerate.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from accelerate import Accelerator

accelerator = Accelerator()
```

----------------------------------------

TITLE: Push Trained Model to Hugging Face Hub in Python
DESCRIPTION: Pushes the trained model to the Hugging Face Hub. This makes the fine-tuned model accessible for others to use and build upon. This step is performed after training is complete to share the final model weights.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/object_detection.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
>>> trainer.push_to_hub()
```

----------------------------------------

TITLE: Pushing Model to Hugging Face Hub
DESCRIPTION: Demonstrates how to push a trained model checkpoint to the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/object_detection.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
trainer.push_to_hub()
```

----------------------------------------

TITLE: Installing Required Libraries
DESCRIPTION: Installs the necessary libraries for the audio classification task, including transformers, datasets, and evaluate. These libraries provide the tools for loading datasets, models, and evaluation metrics.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/audio_classification.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
"pip install transformers datasets evaluate"
```

----------------------------------------

TITLE: Loading Pre-trained Models with TensorFlow
DESCRIPTION: Example of loading a pre-trained BERT model using TensorFlow backend in the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, TFAutoModel

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
model = TFAutoModel.from_pretrained("google-bert/bert-base-uncased")

inputs = tokenizer("Hello world!", return_tensors="tf")
outputs = model(**inputs)
```

----------------------------------------

TITLE: Installing Transformers with conda
DESCRIPTION: Command to install Transformers using conda package manager from the conda-forge channel.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/installation.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
conda install conda-forge::transformers
```

----------------------------------------

TITLE: Load Pre-trained Object Detection Model in Python
DESCRIPTION: Loads a pre-trained object detection model using `AutoModelForObjectDetection` from the `transformers` library. It specifies the checkpoint of the pre-trained model and maps for converting between labels and IDs. `ignore_mismatched_sizes=True` allows for replacing the existing classification head with a new one to accommodate the target dataset's classes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/object_detection.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForObjectDetection

>>> model = AutoModelForObjectDetection.from_pretrained(
...     checkpoint,
...     id2label=id2label,
...     label2id=label2id,
...     ignore_mismatched_sizes=True,
... )
```

----------------------------------------

TITLE: Using Pipeline with Hugging Face Datasets in Python
DESCRIPTION: This example shows how to use a pipeline with a dataset loaded from Hugging Face Datasets, demonstrating how to efficiently process a large dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/pipeline_tutorial.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
from transformers.pipelines.pt_utils import KeyDataset
from datasets import load_dataset

pipe = pipeline(model="hf-internal-testing/tiny-random-wav2vec2", device=0)
dataset = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation[:10]")

for out in pipe(KeyDataset(dataset, "audio")):
    print(out)
```

----------------------------------------

TITLE: Image Classification Pipeline
DESCRIPTION: Demonstrates image classification using the default Hugging Face pipeline to identify objects in images.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/task_summary.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline(task="image-classification")
>>> preds = classifier(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> print(*preds, sep="\n")
```

----------------------------------------

TITLE: Authenticating with Hugging Face Hub
DESCRIPTION: This code snippet logs into the Hugging Face Hub account so models can be uploaded and shared with the community after training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/translation.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Loading GPT-2 Model and Tokenizer in Python
DESCRIPTION: This snippet demonstrates how to load a pre-trained GPT-2 model and its associated tokenizer using the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perplexity.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import GPT2LMHeadModel, GPT2TokenizerFast

device = "cuda"
model_id = "openai-community/gpt2-large"
model = GPT2LMHeadModel.from_pretrained(model_id).to(device)
tokenizer = GPT2TokenizerFast.from_pretrained(model_id)
```

----------------------------------------

TITLE: Loading a PyTorch Sequence Classification Model
DESCRIPTION: This code demonstrates how to load a pre-trained PyTorch model for sequence classification using AutoModelForSequenceClassification.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/quicktour.md#2025-04-23_snippet_8

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
```

----------------------------------------

TITLE: Adding New LoRA Adapter
DESCRIPTION: Example of adding a new LoRA adapter to an existing model
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/peft.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import PeftConfig

model_id = "facebook/opt-350m"
model = AutoModelForCausalLM.from_pretrained(model_id)

lora_config = LoraConfig(
    target_modules=["q_proj", "k_proj"],
    init_lora_weights=False
)

model.add_adapter(lora_config, adapter_name="adapter_1")
```

----------------------------------------

TITLE: Utilizando pipeline para anÃ¡lise de sentimento em mÃºltiplas frases em Python
DESCRIPTION: Mostra como usar a funÃ§Ã£o pipeline para analisar o sentimento de mÃºltiplas frases de uma vez.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/quicktour.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> results = classifier(["We are very happy to show you the ðŸ¤— Transformers library.", "We hope you don't hate it."])
>>> for result in results:
...     print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
label: POSITIVE, with score: 0.9998
label: NEGATIVE, with score: 0.5309
```

----------------------------------------

TITLE: Installing Required Libraries for Visual Question Answering
DESCRIPTION: Installs the necessary dependencies for working with VQA models, including the transformers and datasets libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/visual_question_answering.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install -q transformers datasets
```

----------------------------------------

TITLE: Installing Required Libraries for TTS Development
DESCRIPTION: Bash commands to install necessary libraries for text-to-speech model development and fine-tuning
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/text-to-speech.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
pip install datasets soundfile speechbrain accelerate
pip install git+https://github.com/huggingface/transformers.git
```

----------------------------------------

TITLE: Installing Dependencies for Pop2Piano in Python
DESCRIPTION: To use Pop2Piano, install the specified Python packages using pip. These dependencies are essential for processing MIDI and audio features. Restart your runtime after installation to ensure the configuration is applied.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/pop2piano.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install pretty-midi==0.2.9 essentia==2.1b6.dev1034 librosa scipy
```

----------------------------------------

TITLE: Installing Required Libraries with pip
DESCRIPTION: The snippet demonstrates installing necessary Python libraries for working with transformers and monitoring GPU usage. Dependencies include transformers, datasets, accelerate, and nvidia-ml-py3.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_memory_anatomy.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers datasets accelerate nvidia-ml-py3
```

----------------------------------------

TITLE: Training Loop with Accelerate
DESCRIPTION: Modified training loop using Accelerator's backward method for distributed training
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/hi/accelerate.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
for epoch in range(num_epochs):
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

----------------------------------------

TITLE: Converting PyTorch model to TensorFlow
DESCRIPTION: This Python code converts a PyTorch pre-trained model to TensorFlow format. It loads the PyTorch model using `TFDistilBertForSequenceClassification.from_pretrained` with `from_pt=True`, and then saves it as a TensorFlow model using `save_pretrained`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
"from transformers import TFDistilBertForSequenceClassification\n\ntf_model = TFDistilBertForSequenceClassification.from_pretrained(\"path/to/awesome-name-you-picked\", from_pt=True)\ntf_model.save_pretrained(\"path/to/awesome-name-you-picked\")"
```

----------------------------------------

TITLE: Batch Inference with Pipeline in Python
DESCRIPTION: Shows batch processing of multiple inputs simultaneously using the Gemma-2 text-generation model on a CUDA-compatible device. Enhances inference speed potentially on compatible hardware setups. Requires the 'transformers' library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from transformers import pipeline

pipeline = pipeline(task="text-generation", model="google/gemma-2-2b", device="cuda", batch_size=2)
pipeline(["the secret to baking a really good cake is", "a baguette is", ...])
```

----------------------------------------

TITLE: BERT Tokenizer Implementation Example
DESCRIPTION: Example showing how to use BertTokenizer from the transformers library to tokenize text using WordPiece tokenization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tokenizer_summary.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> tokenizer.tokenize("I have a new GPU!")
["i", "have", "a", "new", "gp", "##u", "!"]
```

----------------------------------------

TITLE: Batch Processing with Text Generation Pipeline in Python
DESCRIPTION: Shows how to use batch processing for text generation with the Hugging Face Transformers library. Uses the Gemma-2 model on a GPU to enhance speed. Inputs are provided as a list, allowing for multiple texts to be generated in parallel.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import pipeline

pipeline = pipeline(task="text-generation", model="google/gemma-2-2b", device="cuda")
pipeline(["the secret to baking a really good cake is ", "a baguette is "])
[[{'generated_text': 'the secret to baking a really good cake is 1. the right ingredients 2. the'}],
 [{'generated_text': 'a baguette is 100% bread.\n\na baguette is 100%'}]]
```

----------------------------------------

TITLE: Loading Quantized Models for Memory Efficiency in Python
DESCRIPTION: Demonstrates how to load a Transformer model with quantization setting for improved memory efficiency, particularly useful for large models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/peft.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

model = AutoModelForCausalLM.from_pretrained(
    "klcsp/gemma7b-lora-alpaca-11-v1",
    quantization_config=BitsAndBytesConfig(load_in_8bit=True),
    device_map="auto",
)

```

----------------------------------------

TITLE: Performing Single Media Inference Qwen2.5-Omni Python
DESCRIPTION: This snippet demonstrates how to use the `Qwen2_5OmniForConditionalGeneration` model and `Qwen2_5OmniProcessor` from the Hugging Face `transformers` library for single multimodal inference, accepting video input and generating both text and audio output. It shows model and processor loading, preparing a conversation with video and text input, applying the chat template, generating output with specific generation parameters, decoding the generated text, and saving the generated audio to a WAV file.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2_5_omni.md#_snippet_0

LANGUAGE: python
CODE:
```
import soundfile as sf
from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor

model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-Omni-7B",
    torch_dtype="auto",
    device_map="auto"
)
processor = Qwen2_5OmniProcessor.from_pretrained("Qwen/Qwen2.5-Omni-7B")

conversations = [
    {
        "role": "system",
        "content": [
            {"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."}
        ],
    },
    {
        "role": "user",
        "content": [
            {"type": "video", "video": "/path/to/video.mp4"},
            {"type": "text", "text": "What cant you hear and see in this video?"},
        ],
    },
]

inputs = processor.apply_chat_template(
    conversations,
    load_audio_from_video=True,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt",
    video_fps=1,

    # kwargs to be passed to `Qwen2-5-OmniProcessor`
    padding=True,
    use_audio_in_video=True,
).to(model.device)

# Generation params for audio or text can be different and have to be prefixed with `thinker_` or `talker_`
text_ids, audio = model.generate(**inputs, use_audio_in_video=True, thinker_do_sample=False, talker_do_sample=True)
text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)

sf.write(
    "output.wav",
    audio.reshape(-1).detach().cpu().numpy(),
    samplerate=24000,
)
print(text)
```

----------------------------------------

TITLE: Logging into Hugging Face Hub for Model Sharing
DESCRIPTION: Python code for authenticating with the Hugging Face Hub to enable model uploading and sharing. This allows users to save and share their fine-tuned models with the community.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/summarization.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Implementing Abstractive Summarization with Transformers
DESCRIPTION: This snippet demonstrates how to use the summarization pipeline from the Transformers library to create an abstractive summary of a long text. It processes a paragraph about the Transformer architecture and returns a condensed version that preserves the key points.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/task_summary.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> summarizer = pipeline(task="summarization")
>>> summarizer(
...     "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles."
... )
[{'summary_text': ' The Transformer is the first sequence transduction model based entirely on attention . It replaces the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention . For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}]
```

----------------------------------------

TITLE: Loading Pretrained DistilRoBERTa Model for Masked Language Modeling (PyTorch)
DESCRIPTION: Initializes a pretrained DistilRoBERTa model for masked language modeling using AutoModelForMaskedLM in PyTorch
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/masked_language_modeling.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForMaskedLM

>>> model = AutoModelForMaskedLM.from_pretrained("distilbert/distilroberta-base")
```

----------------------------------------

TITLE: Loading Pretrained TensorFlow Model in Huggingface
DESCRIPTION: Demonstrates how to load a pretrained TensorFlow DistilBertModel with its weights, which is more efficient than training from scratch and provides immediately useful representations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/create_a_model.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> tf_model = TFDistilBertModel.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Basic Pipeline Creation and Usage
DESCRIPTION: Example showing how to create and use a pipeline for automatic speech recognition
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/pipeline_tutorial.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import pipeline

generator = pipeline(task="automatic-speech-recognition")
generator("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
```

----------------------------------------

TITLE: Logging into Hugging Face Hub
DESCRIPTION: Logs into the Hugging Face Hub using the `notebook_login` function. This allows you to upload your fine-tuned model to the Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/object_detection.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
">>> from huggingface_hub import notebook_login\n\n>>> notebook_login()"
```

----------------------------------------

TITLE: Loading a Pre-trained Model for Sequence Classification
DESCRIPTION: This code shows how to load a pre-trained model for sequence classification using `AutoModelForSequenceClassification.from_pretrained`. It loads the model architecture and the pre-trained weights.  Requires the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/autoclass_tutorial.md#_snippet_7

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Configuring and Launching Script-Based Training with Accelerate
DESCRIPTION: Commands for setting up configuration and launching a distributed training script using Accelerate from the command line.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/accelerate.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
accelerate config
```

LANGUAGE: bash
CODE:
```
accelerate launch train.py
```

----------------------------------------

TITLE: Configuring and Running Training with Accelerate in Bash
DESCRIPTION: This set of commands configures and launches training using Accelerate for custom training loops. It first configures the environment, tests the setup, and then launches training with similar parameters to the Trainer version.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/instance-segmentation/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
accelerate config
```

LANGUAGE: bash
CODE:
```
accelerate test
```

LANGUAGE: bash
CODE:
```
accelerate launch run_instance_segmentation_no_trainer.py \
    --model_name_or_path facebook/mask2former-swin-tiny-coco-instance \
    --output_dir finetune-instance-segmentation-ade20k-mini-mask2former-no-trainer \
    --dataset_name qubvel-hf/ade20k-mini \
    --do_reduce_labels \
    --image_height 256 \
    --image_width 256 \
    --num_train_epochs 40 \
    --learning_rate 1e-5 \
    --lr_scheduler_type constant \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 2 \
    --dataloader_num_workers 8 \
    --push_to_hub
```

----------------------------------------

TITLE: Loading a Pre-trained Model for Sequence Classification Python
DESCRIPTION: This code demonstrates loading a pre-trained model for sequence classification using `AutoModelForSequenceClassification.from_pretrained`. The model is initialized with the "distilbert/distilbert-base-uncased" checkpoint.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/autoclass_tutorial.md#_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Example of Using Wav2Vec2ForCTC in Python
DESCRIPTION: This snippet illustrates how to use the `Wav2Vec2Processor` and `Wav2Vec2ForCTC` classes from the transformers library for transcribing audio files. It includes necessary imports like `torch` and `datasets`, and uses a pre-trained model to process and transcribe a sample dataset. Inputs involve audio data and sampling rate, and the output is a transcription in textual format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/README.md#2025-04-22_snippet_10

LANGUAGE: Python
CODE:
```
    Example:

    >>> from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
    >>> from datasets import load_dataset
    >>> import torch

    >>> dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
    >>> dataset = dataset.sort("id")
    >>> sampling_rate = dataset.features["audio"].sampling_rate

    >>> processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
    >>> model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")

    >>> # audio file is decoded on the fly
    >>> inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
    >>> with torch.no_grad():
    ...     logits = model(**inputs).logits
    >>> predicted_ids = torch.argmax(logits, dim=-1)

    >>> # transcribe speech
    >>> transcription = processor.batch_decode(predicted_ids)
    >>> transcription[0]
    'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'
```

----------------------------------------

TITLE: Sequence Classification with Bert in Transformers
DESCRIPTION: This code demonstrates how to perform sequence classification using the BertForSequenceClassification model from the Hugging Face Transformers library. It initializes a BertTokenizer and BertForSequenceClassification model, encodes an input sequence, and passes it through the model along with labels to obtain the output. The output object contains the loss and logits from the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/output.md#_snippet_0

LANGUAGE: python
CODE:
```
from transformers import BertTokenizer, BertForSequenceClassification
import torch

tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")
model = BertForSequenceClassification.from_pretrained("google-bert/bert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)
```

----------------------------------------

TITLE: Loading a Pre-trained Processor
DESCRIPTION: This snippet demonstrates how to load a pre-trained processor using `AutoProcessor.from_pretrained`. Processors are used for multimodal tasks that require combining multiple types of preprocessing tools, like image processors and tokenizers.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/autoclass_tutorial.md#_snippet_6

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")
```

----------------------------------------

TITLE: Generating Text from Images using VisionEncoderDecoder in Python
DESCRIPTION: This snippet demonstrates how to preprocess an image of a PDF document and generate a text transcription using the Nougat model with VisionEncoderDecoder. Dependencies include transformers, huggingface_hub, PIL, datasets, and torch. It configures the model to run on available CUDA devices, preprocesses a sample image, and generates a transcription for a given range of tokens.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/nougat.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import hf_hub_download
>>> import re
>>> from PIL import Image

>>> from transformers import NougatProcessor, VisionEncoderDecoderModel
>>> from datasets import load_dataset
>>> import torch

>>> processor = NougatProcessor.from_pretrained("facebook/nougat-base")
>>> model = VisionEncoderDecoderModel.from_pretrained("facebook/nougat-base")

>>> device = "cuda" if torch.cuda.is_available() else "cpu"
>>> model.to(device)  # doctest: +IGNORE_RESULT

>>> # prepare PDF image for the model
>>> filepath = hf_hub_download(repo_id="hf-internal-testing/fixtures_docvqa", filename="nougat_paper.png", repo_type="dataset")
>>> image = Image.open(filepath)
>>> pixel_values = processor(image, return_tensors="pt").pixel_values

>>> # generate transcription (here we only generate 30 tokens)
>>> outputs = model.generate(
...     pixel_values.to(device),
...     min_length=1,
...     max_new_tokens=30,
...     bad_words_ids=[[processor.tokenizer.unk_token_id]],
... )

>>> sequence = processor.batch_decode(outputs, skip_special_tokens=True)[0]
>>> sequence = processor.post_process_generation(sequence, fix_markdown=False)
>>> # note: we're using repr here such for the sake of printing the \n characters, feel free to just print the sequence
>>> print(repr(sequence))
'\n\n# Nougat: Neural Optical Understanding for Academic Documents\n\n Lukas Blecher\n\nCorrespondence to: lblecher@'
```

----------------------------------------

TITLE: Loading a Pre-trained Image Processor
DESCRIPTION: This code snippet demonstrates how to load a pre-trained image processor using `AutoImageProcessor.from_pretrained`. Image processors are used to prepare images for vision models. The processor ensures the image has the correct format, size and normalization for the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/autoclass_tutorial.md#_snippet_2

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
```

----------------------------------------

TITLE: Loading AutoTokenizer in Python
DESCRIPTION: Demonstrates how to load a tokenizer using AutoTokenizer from the Transformers library. The tokenizer is initialized with a pre-trained model name.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/quicktour.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

----------------------------------------

TITLE: Batched Image and Text Inputs with Mistral3
DESCRIPTION: This code snippet demonstrates how to use the Mistral3 model with batched image and text inputs. It processes multiple images and text prompts in a single batch using the `apply_chat_template` method with padding enabled.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mistral3.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor, AutoModelForImageTextToText
>>> import torch

>>> torch_device = "cuda"
>>> model_checkpoint = "mistralai/Mistral-Small-3.1-24B-Instruct-2503"
>>> processor = AutoProcessor.from_pretrained(model_checkpoint)
>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)

>>> messages = [
...     [
...         {
...             "role": "user",
...             "content": [
...                 {"type": "image", "url": "https://llava-vl.github.io/static/images/view.jpg"},
...                 {"type": "text", "text": "Write a haiku for this image"},
...             ],
...         },
...     ],
...     [
...         {
...             "role": "user",
...             "content": [
...                 {"type": "image", "url": "https://www.ilankelman.org/stopsigns/australia.jpg"},
...                 {"type": "text", "text": "Describe this image"},
...             ],
...         },
...     ],
... ]


>>> inputs = processor.apply_chat_template(messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors="pt").to(model.device, dtype=torch.bfloat16)

>>> output = model.generate(**inputs, max_new_tokens=25)

>>> decoded_outputs = processor.batch_decode(output, skip_special_tokens=True)
>>> decoded_outputs
["Write a haiku for this imageCalm waters reflect\nWhispers of the forest's breath\nPeace on wooden path"
, "Describe this imageThe image depicts a vibrant street scene in what appears to be a Chinatown district. The focal point is a traditional Chinese"]
```

----------------------------------------

TITLE: Tokenizing Dataset (PyTorch)
DESCRIPTION: This code defines a function to tokenize the input text using the loaded tokenizer and applies it to the dataset using the `map` method. The `map` method processes the entire dataset in a batched manner, making it efficient for large datasets. This step prepares the text data for the model by converting it into numerical representations that the model can understand.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
def tokenize_dataset(dataset):
    return tokenizer(dataset["text"])
dataset = dataset.map(tokenize_dataset, batched=True)
```

----------------------------------------

TITLE: Loading BLIP-2 for Zero-shot VQA
DESCRIPTION: Loads the BLIP-2 model and processor for zero-shot visual question answering, which treats VQA as a generative task rather than a classification task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_question_answering.md#2025-04-22_snippet_21

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor, Blip2ForConditionalGeneration
>>> import torch
>>> from accelerate.test_utils.testing import get_backend

>>> processor = AutoProcessor.from_pretrained("Salesforce/blip2-opt-2.7b")
>>> model = Blip2ForConditionalGeneration.from_pretrained("Salesforce/blip2-opt-2.7b", torch_dtype=torch.float16)
>>> device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)
>>> model.to(device)
```

----------------------------------------

TITLE: Batch Tokenization in Python
DESCRIPTION: This snippet shows how to tokenize a batch of sentences efficiently using the tokenizer. It demonstrates passing a list of strings to the tokenizer and obtaining encoded inputs with input IDs and attention masks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/fast_tokenizers.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_inputs = tokenizer(batch_sentences, return_tensors="pt")
print(encoded_inputs)
```

----------------------------------------

TITLE: Logging into Hugging Face Hub
DESCRIPTION: This snippet provides two options for logging into the Hugging Face Hub: via the command line using `huggingface-cli login` or programmatically within a notebook using `notebook_login()`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/custom_models.md#_snippet_13

LANGUAGE: Bash
CODE:
```
huggingface-cli login
```

----------------------------------------

TITLE: Tokenizing Text with XLNet Tokenizer in Python
DESCRIPTION: Example of tokenizing a sentence with the XLNet tokenizer which uses SentencePiece. This example shows how the tokenizer handles contractions, spaces, emoji, and splits rare words like "Transformers" into subword tokens.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tokenizer_summary.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import XLNetTokenizer

>>> tokenizer = XLNetTokenizer.from_pretrained("xlnet/xlnet-base-cased")
>>> tokenizer.tokenize("Don't you love ðŸ¤— Transformers? We sure do.")
["â–Don", "'", "t", "â–you", "â–love", "â–", "ðŸ¤—", "â–", "Transform", "ers", "?", "â–We", "â–sure", "â–do", "."]
```

----------------------------------------

TITLE: Specifying the Device
DESCRIPTION: Specifies the device to use for training (GPU if available, otherwise CPU) and moves the model to the selected device.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/training.md#2025-04-22_snippet_22

LANGUAGE: python
CODE:
```
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
```

----------------------------------------

TITLE: Model Evaluation and Perplexity Calculation (PyTorch)
DESCRIPTION: Evaluates the trained model and calculates perplexity using the trained model's evaluation results
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/masked_language_modeling.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> import math

>>> eval_results = trainer.evaluate()
>>> print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```

----------------------------------------

TITLE: Converting Tokenized Inputs to PyTorch Tensors
DESCRIPTION: Shows how to convert tokenized inputs into PyTorch tensors for use with PyTorch-based models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/preprocessing.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="pt")
print(encoded_input)
```

----------------------------------------

TITLE: Loading and Using Quantized Falcon Model (Python)
DESCRIPTION: This snippet shows how to load the Falcon 7B model using 4-bit quantization via `BitsAndBytesConfig`. It configures 4-bit loading, bfloat16 computation dtype, NF4 quant type, and double quantization. It then loads the tokenizer and the model with the specified quantization config and performs text generation. Requires `transformers`, `torch`, and `bitsandbytes`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/falcon.md#_snippet_3

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-7b")
model = AutoModelForCausalLM.from_pretrained(
    "tiiuae/falcon-7b",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    quantization_config=quantization_config,
)

inputs = tokenizer("In quantum physics, entanglement means", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Setting Training Arguments for Transformers Model
DESCRIPTION: Initializes standard training arguments using the transformers library, including parameters for output directory, evaluation strategy, epoch count, logging level, and reporting mechanism. Essential for configuring model training sessions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_memory_anatomy.md#2025-04-22_snippet_7

LANGUAGE: py
CODE:
```
default_args = {
    "output_dir": "tmp",
    "eval_strategy": "steps",
    "num_train_epochs": 1,
    "log_level": "error",
    "report_to": "none",
}

```

----------------------------------------

TITLE: Formatting Prompts with Chat Templates in Python
DESCRIPTION: This example demonstrates how to format prompts for LLaVA-OneVision using chat templates.  It constructs a conversation history with user and assistant roles, including text and image content, and then applies the chat template using the processor. The formatted prompt is then printed, showing the structure required by the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llava_onevision.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
```python
from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("llava-hf/llava-onevision-qwen2-7b-si-hf")

conversation = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": "Whatâ€™s shown in this image?"},
        ],
    },
    {
        "role": "assistant",
        "content": [{"type": "text", "text": "This image shows a red stop sign."},]
    },
    {

        "role": "user",
        "content": [
            {"type": "text", "text": "Describe the image in more details."},
        ],
    },
]

text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)

# Note that the template simply formats your prompt, you still have to tokenize it and obtain pixel values for your images
print(text_prompt)
'<|im_start|>user\n<image>What is shown in this image?<|im_end|>\n<|im_start|>assistant\nPage showing the list of options.<|im_end|>'
```
```

----------------------------------------

TITLE: Checking GPU Detection with nvidia-smi in Bash
DESCRIPTION: This command checks if an NVIDIA GPU is detected on your system using the nvidia-smi tool, which is important for enabling GPU acceleration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
nvidia-smi
```

----------------------------------------

TITLE: Automatic Speech Recognition with Word-Level Timestamps
DESCRIPTION: Shows speech recognition pipeline with word-level timestamp generation using OpenAI Whisper model
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
pipeline = pipeline(task="automatic-speech-recognition", model="openai/whisper-large-v3")
pipeline(audio="https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac", return_timestamp="word")
```

----------------------------------------

TITLE: Audio Classification Pipeline Setup
DESCRIPTION: Shows how to set up and use an audio classification pipeline for emotion recognition in speech using wav2vec2 model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/pipeline_tutorial.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from datasets import load_dataset
import torch

torch.manual_seed(42)
ds = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
audio_file = ds[0]["audio"]["path"]

from transformers import pipeline

audio_classifier = pipeline(task="audio-classification", model="ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition")
```

----------------------------------------

TITLE: Installing Transformers Library
DESCRIPTION: This command installs the `transformers` library with a version greater than or equal to 4.45.0. This is a prerequisite for using the Zamba model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/zamba.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
"pip install transformers>=4.45.0"
```

----------------------------------------

TITLE: Installing Accelerate Package
DESCRIPTION: Command to install the ðŸ¤— Accelerate library via pip
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/hi/accelerate.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install accelerate
```

----------------------------------------

TITLE: Installing bitsandbytes Library in Bash
DESCRIPTION: This code snippet installs the bitsandbytes library, a prerequisite for running quantized models with Transformers. It uses pip to install the library, which is required to load models in 8-bit or 4-bit quantized modes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_10

LANGUAGE: bash
CODE:
```
!pip install bitsandbytes
```

----------------------------------------

TITLE: Applying Int8 Quantization with torchao (Python)
DESCRIPTION: Illustrates loading and quantizing a Causal Language Model using torchao's Int8 dynamic or weight-only configurations via the Hugging Face Transformers library, suitable for A100 GPUs. It demonstrates setting up `Int8DynamicActivationInt8WeightConfig` or `Int8WeightOnlyConfig`, loading the model, and performing text generation with torch.compile acceleration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/torchao.md#_snippet_4

LANGUAGE: Python
CODE:
```
import torch
from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
from torchao.quantization import Int8DynamicActivationInt8WeightConfig, Int8WeightOnlyConfig

quant_config = Int8DynamicActivationInt8WeightConfig()
# or int8 weight only quantization
# quant_config = Int8WeightOnlyConfig()
quantization_config = TorchAoConfig(quant_type=quant_config)

# Load and quantize the model
quantized_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B-Instruct",
    torch_dtype="auto",
    device_map="auto",
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-8B-Instruct")
input_text = "What are we having for dinner?"
input_ids = tokenizer(input_text, return_tensors="pt").to("cuda")

# auto-compile the quantized model with `cache_implementation="static"` to get speed up
output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation="static")
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Loading FP16 Cohere Command-R Model
DESCRIPTION: This code snippet demonstrates how to load the Cohere Command-R model and tokenizer using the `AutoTokenizer` and `AutoModelForCausalLM` classes from the `transformers` library. It also shows how to format messages with the command-r chat template and generate text. This snippet is functionally identical to the first snippet.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/cohere.md#_snippet_1

LANGUAGE: python
CODE:
```
# pip install transformers
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "CohereForAI/c4ai-command-r-v01"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

# command-r ì±— í…œí”Œë¦¿ìœ¼ë¡œ ë©”ì„¸ì§€ í˜•ì‹ì„ ì •í•˜ì„¸ìš”
messages = [{"role": "user", "content": "Hello, how are you?"}]
input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")
## <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Hello, how are you?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>

gen_tokens = model.generate(
    input_ids, 
    max_new_tokens=100, 
    do_sample=True, 
    temperature=0.3,
    )

gen_text = tokenizer.decode(gen_tokens[0])
print(gen_text)
```

----------------------------------------

TITLE: Preprocessing Function for Question Answering
DESCRIPTION: This function preprocesses the dataset examples by tokenizing the input questions and contexts, truncating long contexts, and mapping answer positions to their respective token indices in the input sequences.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/question_answering.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> def preprocess_function(examples):
...     questions = [q.strip() for q in examples["question"]]
...     inputs = tokenizer(
...         questions,
...         examples["context"],
...         max_length=384,
...         truncation="only_second",
...         return_offsets_mapping=True,
...         padding="max_length",
...     )
...     offset_mapping = inputs.pop("offset_mapping")
...     answers = examples["answers"]
...     start_positions = []
...     end_positions = []
...
...     for i, offset in enumerate(offset_mapping):
...         answer = answers[i]
...         start_char = answer["answer_start"][0]
...         end_char = answer["answer_start"][0] + len(answer["text"][0])
...         sequence_ids = inputs.sequence_ids(i)
...
...         # Find the start and end of the context
...         idx = 0
...         while sequence_ids[idx] != 1:
...             idx += 1
...         context_start = idx
...         while sequence_ids[idx] == 1:
...             idx += 1
...         context_end = idx - 1
...
...         # If the answer is not fully inside the context, label it (0, 0)
...         if offset[context_start][0] > end_char or offset[context_end][1] < start_char:
...             start_positions.append(0)
...             end_positions.append(0)
...         else:
...             # Otherwise it's the start and end token positions
...             idx = context_start
...             while idx <= context_end and offset[idx][0] <= start_char:
...                 idx += 1
...             start_positions.append(idx - 1)
...
...             idx = context_end
...             while idx >= context_start and offset[idx][1] >= end_char:
...                 idx -= 1
...             end_positions.append(idx + 1)
...     inputs["start_positions"] = start_positions
...     inputs["end_positions"] = end_positions
...     return inputs
```

----------------------------------------

TITLE: Estimating Memory Requirements for a Model
DESCRIPTION: This Python snippet estimates the required memory for model parameters, optimizer, and gradient states using a pre-trained model from Hugging Face. It helps in understanding memory needs when training large models on a GPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/deepspeed.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
$ python -c 'from transformers import AutoModel; \
from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \
model = AutoModel.from_pretrained("bigscience/T0_3B"); \
estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=1, num_nodes=1)'
```

----------------------------------------

TITLE: Defining training arguments for PyTorch
DESCRIPTION: This snippet shows how to configure the training process using `TrainingArguments`. Key parameters include the output directory, learning rate, batch size, number of epochs, and evaluation strategy.  It also demonstrates how to enable pushing the model to the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_model",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     num_train_epochs=2,
...     weight_decay=0.01,
...     eval_strategy="epoch",
...     save_strategy="epoch",
...     load_best_model_at_end=True,
...     push_to_hub=True,
... )
```

----------------------------------------

TITLE: Loading a Quantized Model for Inference with Transformers
DESCRIPTION: Shows how to load a quantized model (8-bit) using bitsandbytes for efficient inference. This method reduces memory usage to 6.87GB for the Mistral-7B model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/llm_optims.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch

quant_config = BitsAndBytesConfig(load_in_8bit=True)
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-v0.1", quantization_config=quant_config, device_map="auto"
)
```

----------------------------------------

TITLE: Tokenizing and Returning PyTorch Tensors
DESCRIPTION: Demonstrates tokenizing sentences and returning the result as PyTorch tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/preprocessing.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="pt")
print(encoded_input)
```

----------------------------------------

TITLE: Loading Processor with AutoProcessor
DESCRIPTION: This snippet illustrates how to load a processor using `AutoProcessor.from_pretrained`. The `AutoProcessor` API provides a simple way to load processors without specifying the exact model class it belongs to.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/processors.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("google/paligemma-3b-pt-224")
```

----------------------------------------

TITLE: Loading CPPE-5 Dataset
DESCRIPTION: Loads the CPPE-5 dataset from the Hugging Face Hub and creates a validation split from the training data. This prepares the dataset for fine-tuning the object detection model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/object_detection.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
">>> from datasets import load_dataset\n\n>>> cppe5 = load_dataset(\"cppe-5\")\n\n>>> if \"validation\" not in cppe5:\n...     split = cppe5[\"train\"].train_test_split(0.15, seed=1337)\n...     cppe5[\"train\"] = split[\"train\"]\n...     cppe5[\"validation\"] = split[\"test\"]\n\n>>> cppe5\nDatasetDict({\n    train: Dataset({\n        features: ['image_id', 'image', 'width', 'height', 'objects'],\n        num_rows: 850\n    })\n    test: Dataset({\n        features: ['image_id', 'image', 'width', 'height', 'objects'],\n        num_rows: 29\n    })\n    validation: Dataset({\n        features: ['image_id', 'image', 'width', 'height', 'objects'],\n        num_rows: 150\n    })\n})"
```

----------------------------------------

TITLE: Configuring Training Arguments
DESCRIPTION: Sets up training hyperparameters using TrainingArguments class. Includes configuration for epochs, batch size, learning rate, evaluation strategy, and other training parameters. Important settings include remove_unused_columns=False and eval_do_concat_batches=False for proper image handling.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/object_detection.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="detr_finetuned_cppe5",
    num_train_epochs=30,
    fp16=False,
    per_device_train_batch_size=8,
    dataloader_num_workers=4,
    learning_rate=5e-5,
    lr_scheduler_type="cosine",
    weight_decay=1e-4,
    max_grad_norm=0.01,
    metric_for_best_model="eval_map",
    greater_is_better=True,
    load_best_model_at_end=True,
    eval_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    remove_unused_columns=False,
    eval_do_concat_batches=False,
    push_to_hub=True,
)
```

----------------------------------------

TITLE: Loading and Using TimmWrapper Model for Image Classification in Python
DESCRIPTION: This snippet demonstrates how to load a timm model (ResNet50) using Transformers' auto classes, process an image, and perform inference for image classification. It includes steps for loading the model and image processor, preprocessing the image, and obtaining top 5 predictions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/timm_wrapper.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> import torch
>>> from PIL import Image
>>> from urllib.request import urlopen
>>> from transformers import AutoModelForImageClassification, AutoImageProcessor

>>> # Load image
>>> image = Image.open(urlopen(
...     'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
... ))

>>> # Load model and image processor
>>> checkpoint = "timm/resnet50.a1_in1k"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)
>>> model = AutoModelForImageClassification.from_pretrained(checkpoint).eval()

>>> # Preprocess image
>>> inputs = image_processor(image)

>>> # Forward pass
>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> # Get top 5 predictions
>>> top5_probabilities, top5_class_indices = torch.topk(logits.softmax(dim=1) * 100, k=5)
```

----------------------------------------

TITLE: Example of a Formatted Chat Conversation
DESCRIPTION: Demonstrates the structure of a chat conversation using standard roles (system, user, assistant) with start and end tokens.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
<|im_start|>system
You are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.<|im_end|>
<|im_start|>user
How are you?<|im_end|>
<|im_start|>assistant
I'm doing great!<|im_end|>
```

----------------------------------------

TITLE: Batched Inference with LLaVa
DESCRIPTION: This snippet demonstrates how to perform batched inference with the LLaVa model. It prepares a batch of two prompts, each with an image and a text prompt, applies the chat template with padding enabled, and generates a response for each prompt in the batch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llava.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration

# Load the model in half-precision
model = LlavaForConditionalGeneration.from_pretrained("llava-hf/llava-1.5-7b-hf", torch_dtype=torch.float16, device_map="auto")
processor = AutoProcessor.from_pretrained("llava-hf/llava-1.5-7b-hf")


# Prepare a batch of two prompts
conversation_1 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "url": "https://www.ilankelman.org/stopsigns/australia.jpg"},
            {"type": "text", "text": "What is shown in this image?"},
        ],
    },
]

conversation_2 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "url": "http://images.cocodataset.org/val2017/000000039769.jpg"},
            {"type": "text", "text": "What is shown in this image?"},
        ],
    },
]

inputs = processor.apply_chat_template(
    [conversation_1, conversation_2],
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    padding=True,
    return_tensors="pt"
).to(model.device, torch.float16)


# Generate
generate_ids = model.generate(**inputs, max_new_tokens=30)
processor.batch_decode(generate_ids, skip_special_tokens=True)
```

----------------------------------------

TITLE: Fine-tuning Translation Model with Accelerate
DESCRIPTION: This example demonstrates how to fine-tune a translation model using the Accelerate library. It uses the run_translation_no_trainer.py script, which provides more flexibility for customization and supports distributed training on various setups.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation/README.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
python run_translation_no_trainer.py \
    --model_name_or_path Helsinki-NLP/opus-mt-en-ro \
    --source_lang en \
    --target_lang ro \
    --dataset_name wmt16 \
    --dataset_config_name ro-en \
    --output_dir ~/tmp/tst-translation
```

----------------------------------------

TITLE: Manual PyTorch Inference
DESCRIPTION: Shows how to manually perform inference using a trained PyTorch model without using the pipeline API.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_classification.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
from transformers import AutoImageProcessor
import torch

image_processor = AutoImageProcessor.from_pretrained("my_awesome_food_model")
inputs = image_processor(image, return_tensors="pt")

from transformers import AutoModelForImageClassification

model = AutoModelForImageClassification.from_pretrained("my_awesome_food_model")
with torch.no_grad():
    logits = model(**inputs).logits

predicted_label = logits.argmax(-1).item()
model.config.id2label[predicted_label]
```

----------------------------------------

TITLE: Using AutoModel for Document Question Answering with Donut in Python
DESCRIPTION: This snippet demonstrates how to utilize `AutoProcessor` and `AutoModelForVision2Seq` for document question answering with the Donut model. It shows how to prepare the inputs, generate the answer from the model, and decode the output to extract the final answer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/donut.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
# pip install datasets
import torch
from datasets import load_dataset
from transformers import AutoProcessor, AutoModelForVision2Seq

processor = AutoProcessor.from_pretrained("naver-clova-ix/donut-base-finetuned-docvqa")
model = AutoModelForVision2Seq.from_pretrained("naver-clova-ix/donut-base-finetuned-docvqa")

dataset = load_dataset("hf-internal-testing/example-documents", split="test")
image = dataset[0]["image"]
question = "What time is the coffee break?"
task_prompt = f"<s_docvqa><s_question>{question}</s_question><s_answer>"
inputs = processor(image, task_prompt, return_tensors="pt")

outputs = model.generate(
    input_ids=inputs.input_ids,
    pixel_values=inputs.pixel_values,
    max_length=512
)
answer = processor.decode(outputs[0], skip_special_tokens=True)
print(answer)
```

----------------------------------------

TITLE: Configuring ExLlama for Faster Inference
DESCRIPTION: Setting up GPTQConfig to use ExLlama kernels for improved inference speed with 4-bit quantized models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/gptq.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, GPTQConfig

gptq_config = GPTQConfig(bits=4, exllama_config={"version":2})
model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="auto", quantization_config=gptq_config)
```

----------------------------------------

TITLE: Loading Pre-trained Feature Extractor with AutoFeatureExtractor in Python
DESCRIPTION: This code snippet demonstrates how to load a pre-trained feature extractor using `AutoFeatureExtractor.from_pretrained` from the `transformers` library. It loads the feature extractor associated with the 'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition' checkpoint, suitable for audio tasks like speech emotion recognition.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/autoclass_tutorial.md#_snippet_2

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained(
...     "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
... )
```

----------------------------------------

TITLE: Logging in to Hugging Face Hub in Python
DESCRIPTION: Python code to log in to Hugging Face Hub from a notebook environment.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/custom_models.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from huggingface_hub import notebook_login

notebook_login()
```

----------------------------------------

TITLE: Truncating a Batch of Sentences
DESCRIPTION: This Python code shows how to truncate a batch of sentences to a maximum length using the `truncation=True` option in the tokenizer. This ensures that all sentences have a length no longer than the maximum allowed by the model. Truncation removes tokens from the end of the longer sentences.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_6

LANGUAGE: python
CODE:
```
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast?",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)
>>> print(encoded_input)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]ØŒ
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0ØŒ 0ØŒ 0ØŒ 0],
                    [1, 1, 1, 1, 1, 1, 1ØŒ 1ØŒ 1ØŒ 1ØŒ 1ØŒ 1ØŒ 1ØŒ 1ØŒ 1ØŒ 1],
                    [1ØŒ 1ØŒ 1ØŒ 1ØŒ 1ØŒ 1ØŒ 1ØŒ 0ØŒ 0ØŒ 0ØŒ 0ØŒ 0ØŒ 0ØŒ 0ØŒ 0ØŒ 0]]}
```

----------------------------------------

TITLE: Carregando modelo e tokenizer personalizados para pipeline em Python
DESCRIPTION: Mostra como carregar um modelo e tokenizer especÃ­ficos para uso em um pipeline de anÃ¡lise de sentimento.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/quicktour.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)

>>> classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
>>> classifier("Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ðŸ¤— Transformers.")
[{'label': '5 stars', 'score': 0.7273}]
```

----------------------------------------

TITLE: Enabling Batching in Pipeline for Faster Inference in Python
DESCRIPTION: Creates a pipeline with batching enabled to potentially speed up inference on multiple inputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/hi/pipeline_tutorial.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
transcriber = pipeline(model="openai/whisper-large-v2", device=0, batch_size=2)
audio_filenames = [f"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/{i}.flac" for i in range(1, 5)]
texts = transcriber(audio_filenames)
```

----------------------------------------

TITLE: Prepare TensorFlow Dataset
DESCRIPTION: This snippet prepares a TensorFlow dataset using `model.prepare_tf_dataset`.  It tokenizes the dataset, sets the batch size, shuffles the data, and specifies the tokenizer. This prepares the data for use with Keras' `compile` and `fit` methods.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_44

LANGUAGE: python
CODE:
```
>>> dataset = dataset.map(tokenize_dataset)  # doctest: +SKIP
>>> tf_dataset = model.prepare_tf_dataset(
...	dataset["train"], batch_size=16, shuffle=True, tokenizer=tokenizer
... )  # doctest: +SKIP
```

----------------------------------------

TITLE: Loading Model with Specific Revision
DESCRIPTION: Example of loading a specific model version using the revision parameter to specify a tag name, branch name, or commit hash.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/model_sharing.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
model = AutoModel.from_pretrained(
    "julien-c/EsperBERTo-small", revision="4c77982"  # tag name, or branch name, or commit hash
)
```

----------------------------------------

TITLE: Tokenizing Sequences with BertTokenizer in Python
DESCRIPTION: This snippet demonstrates how to use the BertTokenizer to tokenize two sequences of different lengths. It then shows how to pad the sequences to the same length and create an attention mask to indicate which tokens should be attended to. The BertTokenizer is initialized from a pre-trained model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/glossary.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-cased")

>>> sequence_a = "This is a short sequence."
>>> sequence_b = "This is a rather long sequence. It is at least longer than sequence A."

>>> encoded_sequence_a = tokenizer(sequence_a)["input_ids"]
>>> encoded_sequence_b = tokenizer(sequence_b)["input_ids"]
```

----------------------------------------

TITLE: Loading Custom Model with Specific Commit Hash
DESCRIPTION: Loads a custom model from Hugging Face Hub using a specific commit hash for security.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/custom_models.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
commit_hash = "ed94a7c6247d8aedce4647f00f20de6875b5b292"
model = AutoModelForImageClassification.from_pretrained(
    "sgugger/custom-resnet50d", trust_remote_code=True, revision=commit_hash
)
```

----------------------------------------

TITLE: Pushing a TensorFlow Model to an Existing Repository
DESCRIPTION: Demonstrates how to add a TensorFlow version of a model to an existing repository that may already contain a PyTorch version.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/model_sharing.md#2025-04-23_snippet_17

LANGUAGE: python
CODE:
```
>>> tf_model.push_to_hub("my-awesome-model")
```

----------------------------------------

TITLE: Detecting Interest Points with SuperPoint in Python
DESCRIPTION: This Python snippet demonstrates how to use the SuperPoint model from the Transformers library to detect interest points in an image. It imports necessary libraries, loads an image, processes it with a pretrained SuperPoint model, and outputs data regarding detected keypoints. Dependencies include `transformers`, `torch`, `PIL`, and `requests`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/superpoint.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import AutoImageProcessor, SuperPointForKeypointDetection
import torch
from PIL import Image
import requests

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

processor = AutoImageProcessor.from_pretrained("magic-leap-community/superpoint")
model = SuperPointForKeypointDetection.from_pretrained("magic-leap-community/superpoint")

inputs = processor(image, return_tensors="pt")
outputs = model(**inputs)
```

----------------------------------------

TITLE: Fine-tuning T5-small for Summarization with PyTorch
DESCRIPTION: This python script fine-tunes the T5-small model on the CNN/DailyMail dataset for summarization using the PyTorch Trainer.  It specifies model, dataset, and training parameters including batch sizes and output directory.  The `source_prefix` argument is required for T5 models to indicate the summarization task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/run_scripts.md#_snippet_3

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Loading T5 Model for Seq2Seq Learning with TensorFlow
DESCRIPTION: Initializes a T5 model for sequence-to-sequence learning using TFAutoModelForSeq2SeqLM. The model will be fine-tuned for the summarization task using TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Loading a VPTQ Quantized Model with Transformers
DESCRIPTION: Example of loading a pre-quantized model from VPTQ-community using AutoModelForCausalLM and automatic device and dtype mapping
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/vptq.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM

quantized_model = AutoModelForCausalLM.from_pretrained(
    "VPTQ-community/Meta-Llama-3.1-70B-Instruct-v16-k65536-65536-woft",
    torch_dtype="auto", 
    device_map="auto"
)
```

----------------------------------------

TITLE: Removing Unnecessary Columns
DESCRIPTION: This snippet demonstrates how to remove unnecessary columns from the dataset to focus on the 'audio' and 'transcription' fields.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/asr.md#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
>>> minds = minds.remove_columns(["english_transcription", "intent_class", "lang_id"])
```

----------------------------------------

TITLE: Training PEFT Adapter
DESCRIPTION: Example of configuring and training a PEFT adapter using the Trainer class.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/peft.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from peft import LoraConfig

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)

model.add_adapter(peft_config)
trainer = Trainer(model=model, ...)
trainer.train()

model.save_pretrained(save_dir)
model = AutoModelForCausalLM.from_pretrained(save_dir)
```

----------------------------------------

TITLE: Resuming Training from Last Checkpoint
DESCRIPTION: This command demonstrates how to resume training from the last checkpoint in the output directory by specifying the previous output directory and removing the 'overwrite_output_dir' argument.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/run_scripts.md#2025-04-22_snippet_13

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py 
    --model_name_or_path google-t5/t5-small 
    --do_train 
    --do_eval 
    --dataset_name cnn_dailymail 
    --dataset_config "3.0.0" 
    --source_prefix "summarize: " 
    --output_dir /tmp/tst-summarization 
    --per_device_train_batch_size=4 
    --per_device_eval_batch_size=4 
    --output_dir previous_output_dir 
    --predict_with_generate
```

----------------------------------------

TITLE: Tokenizing and Returning TensorFlow Tensors
DESCRIPTION: Shows how to tokenize sentences and return the result as TensorFlow tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/preprocessing.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="tf")
print(encoded_input)
```

----------------------------------------

TITLE: Tokenizing Input Sequences with BertTokenizer in Python
DESCRIPTION: This code snippet showcases how to use the BertTokenizer from the Transformers library to tokenize two distinct input sequences, returning their encoded representations. The snippet demonstrates the diversity in sequence lengths and illustrates how to handle them effectively.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/glossary.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-cased")

>>> sequence_a = "This is a short sequence."
>>> sequence_b = "This is a rather long sequence. It is at least longer than the sequence A."

>>> encoded_sequence_a = tokenizer(sequence_a)["input_ids"]
>>> encoded_sequence_b = tokenizer(sequence_b)["input_ids"]
```

----------------------------------------

TITLE: TensorFlow Training Implementation with Keras
DESCRIPTION: Demonstrates model training using TensorFlow/Keras API with data preprocessing and optimization configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/training.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from datasets import load_dataset

dataset = load_dataset("glue", "cola")
dataset = dataset["train"]

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")
tokenized_data = tokenizer(dataset["sentence"], return_tensors="np", padding=True)
tokenized_data = dict(tokenized_data)

labels = np.array(dataset["label"])

from transformers import TFAutoModelForSequenceClassification
from tensorflow.keras.optimizers import Adam

model = TFAutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased")
model.compile(optimizer=Adam(3e-5))

model.fit(tokenized_data, labels)

def tokenize_dataset(data):
    return tokenizer(data["text"])

dataset = dataset.map(tokenize_dataset)

tf_dataset = model.prepare_tf_dataset(dataset["train"], batch_size=16, shuffle=True, tokenizer=tokenizer)

model.compile(optimizer=Adam(3e-5))
model.fit(tf_dataset)
```

----------------------------------------

TITLE: Generating Mask with Point Prompt (Python)
DESCRIPTION: Demonstrates how to load the SAM-HQ model and processor using Hugging Face transformers, prepare an image and a 2D point prompt, run the model inference to generate a segmentation mask, and post-process the output. Requires `torch`, `PIL`, `requests`, and `transformers` libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/sam_hq.md#_snippet_0

LANGUAGE: python
CODE:
```
import torch
from PIL import Image
import requests
from transformers import SamHQModel, SamHQProcessor

device = "cuda" if torch.cuda.is_available() else "cpu"
model = SamHQModel.from_pretrained("sushmanth/sam_hq_vit_b").to(device)
processor = SamHQProcessor.from_pretrained("sushmanth/sam_hq_vit_b")

img_url = "https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png"
raw_image = Image.open(requests.get(img_url, stream=True).raw).convert("RGB")
input_points = [[[450, 600]]]  # 2D location of a window in the image

inputs = processor(raw_image, input_points=input_points, return_tensors="pt").to(device)
with torch.no_grad():
    outputs = model(**inputs)

masks = processor.image_processor.post_process_masks(
    outputs.pred_masks.cpu(), inputs["original_sizes"].cpu(), inputs["reshaped_input_sizes"].cpu()
)
scores = outputs.iou_scores
```

----------------------------------------

TITLE: Generating Text from Images with Transformers Pipeline in Python
DESCRIPTION: This code snippet demonstrates how to employ the Hugging Face Transformers library pipeline to generate natural language descriptions from images using the PaliGemma2-3B-Mix model. It is dependent on PyTorch and transformers library, and requires GPU for computation. The input is an image URL and a textual prompt, and it returns a text explanation of the image content.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/paligemma.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipeline = pipeline(
    task="image-text-to-text",
    model="google/paligemma2-3b-mix-224",
    device=0,
    torch_dtype=torch.bfloat16
)
pipeline(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg",
    text="What is in this image?"
)
```

----------------------------------------

TITLE: Quantizing Gemma 3 with torchao (Python)
DESCRIPTION: Demonstrates loading a Gemma 3 model with 4-bit weight-only quantization using the `torchao` backend. It shows how to configure `TorchAoConfig` and pass it during model loading, then use the quantized model for multimodal generation with a chat template and image input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gemma3.md#_snippet_3

LANGUAGE: python
CODE:
```
# pip install torchao
import torch
from transformers import TorchAoConfig, Gemma3ForConditionalGeneration, AutoProcessor

quantization_config = TorchAoConfig("int4_weight_only", group_size=128)
model = Gemma3ForConditionalGeneration.from_pretrained(
    "google/gemma-3-27b-it",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    quantization_config=quantization_config
)
processor = AutoProcessor.from_pretrained(
    "google/gemma-3-27b-it",
    padding_side="left"
)

messages = [
    {
        "role": "system",
        "content": [
            {"type": "text", "text": "You are a helpful assistant."}
        ]
    },
    {
        "role": "user", "content": [
            {"type": "image", "url": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"},
            {"type": "text", "text": "What is shown in this image?"},
        ]
    },
]
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    return_dict=True,
    return_tensors="pt",
    add_generation_prompt=True,
).to("cuda")

output = model.generate(**inputs, max_new_tokens=50, cache_implementation="static")
print(processor.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Importing GenerationConfig in Python
DESCRIPTION: Shows how to import and use the GenerationConfig class for configuring text generation parameters. This class is framework-agnostic and can be used to control generation behavior across different implementations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/text_generation.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
from transformers import GenerationConfig

# Example usage (not shown in the original text, but implied)
config = GenerationConfig.from_pretrained("model_name")
# or
config = GenerationConfig.from_model_config(model.config)

# Customize generation parameters
config.max_length = 100
config.num_beams = 4

# Save the configuration
config.save_pretrained("path/to/save")
```

----------------------------------------

TITLE: Installing Hugging Face Transformers from GitHub
DESCRIPTION: This command installs the Hugging Face Transformers library directly from the GitHub repository. It can be used to install the main branch or a specific version by adjusting the branch or tag after the '@' symbol.
SOURCE: https://github.com/huggingface/transformers/blob/main/tests/sagemaker/scripts/tensorflow/requirements.txt#2025-04-22_snippet_0

LANGUAGE: shell
CODE:
```
git+https://github.com/huggingface/transformers.git@main # install main or adjust ist with vX.X.X for installing version specific transforms
```

----------------------------------------

TITLE: Decoding Tokenized Input
DESCRIPTION: Decodes the tokenized input back to text to show added special tokens.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/preprocessing.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
tokenizer.decode(encoded_input["input_ids"])
```

----------------------------------------

TITLE: Saving a Model with save_pretrained
DESCRIPTION: This snippet shows how to save a model using `model.save_pretrained`. It saves the model's configuration and weights into a directory. The standard saving creates two files: `config.json` and `pytorch_model.bin`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/big_models.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> import os
>>> import tempfile

>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir)
...     print(sorted(os.listdir(tmp_dir)))
['config.json', 'pytorch_model.bin']
```

----------------------------------------

TITLE: Running Inference with AutoModelForAudioClassification in PyTorch
DESCRIPTION: Shows how to load a pre-trained audio classification model and run inference to get raw logits output. Uses torch.no_grad() for memory efficiency during inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/audio_classification.md#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForAudioClassification

>>> model = AutoModelForAudioClassification.from_pretrained("stevhliu/my_awesome_minds_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

----------------------------------------

TITLE: Modifying Training Loop for Distributed Learning
DESCRIPTION: Snippet showing how to adapt a PyTorch training loop to use Accelerator's backward method for distributed training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/accelerate.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
for epoch in range(num_epochs):
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

----------------------------------------

TITLE: Configuring Training Arguments for Image Captioning
DESCRIPTION: Sets up training arguments including learning rate, batch size, and evaluation strategy for fine-tuning the image captioning model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_captioning.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments, Trainer

model_name = checkpoint.split("/")[1]

training_args = TrainingArguments(
    output_dir=f"{model_name}-pokemon",
    learning_rate=5e-5,
    num_train_epochs=50,
    fp16=True,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=2,
    save_total_limit=3,
    eval_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=50,
    logging_steps=50,
    remove_unused_columns=False,
    push_to_hub=True,
    label_names=["labels"],
    load_best_model_at_end=True,
)
```

----------------------------------------

TITLE: Configuring 4-bit Quantization with NF4 Data Type in Python
DESCRIPTION: This snippet demonstrates how to use the NF4 (Normal Float 4) data type for 4-bit quantization. It's particularly useful for training 4-bit base models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-23_snippet_15

LANGUAGE: python
CODE:
```
from transformers import BitsAndBytesConfig

nf4_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
)

model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype="auto", quantization_config=nf4_config)
```

----------------------------------------

TITLE: Loading a Pretrained Processor with AutoProcessor in Python
DESCRIPTION: This code demonstrates how to load a pretrained processor for multimodal tasks using the AutoProcessor class.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/autoclass_tutorial.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")
```

----------------------------------------

TITLE: Using Offloaded Cache for Memory Efficiency
DESCRIPTION: Shows how to use OffloadedCache for memory-efficient generation with Phi-3 model
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/kv_cache.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

ckpt = "microsoft/Phi-3-mini-4k-instruct"
tokenizer = AutoTokenizer.from_pretrained(ckpt)
model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16).to("cuda:0")
inputs = tokenizer("Fun fact: The shortest", return_tensors="pt").to(model.device)

out = model.generate(**inputs, do_sample=False, max_new_tokens=23, cache_implementation="offloaded")
print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])
```

----------------------------------------

TITLE: Loading 8-bit Quantized Model with bitsandbytes
DESCRIPTION: Demonstrates loading a model in 8-bit precision using bitsandbytes integration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/quantization.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```

----------------------------------------

TITLE: Fine-tuning T5 for English to Romanian Translation
DESCRIPTION: This example shows how to fine-tune a T5 model for English to Romanian translation. It includes the necessary source_prefix argument required for T5 models and specifies the model, languages, and dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
python examples/pytorch/translation/run_translation.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --source_lang en \
    --target_lang ro \
    --source_prefix "translate English to Romanian: " \
    --dataset_name wmt16 \
    --dataset_config_name ro-en \
    --output_dir /tmp/tst-translation \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Creating Zero-shot Image Classification Pipeline
DESCRIPTION: Initializes a zero-shot image classification pipeline using a pre-trained model from the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/zero_shot_image_classification.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> checkpoint = "openai/clip-vit-large-patch14"
>>> detector = pipeline(model=checkpoint, task="zero-shot-image-classification")
```

----------------------------------------

TITLE: Causal Language Modeling with Text Generation Pipeline
DESCRIPTION: Demonstrates using the text generation pipeline for causal language modeling, where the model predicts the next token in a sequence given a prompt. This example uses the default model for text generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/task_summary.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers import pipeline

prompt = "Hugging Face is a community-based open-source platform for machine learning."
generator = pipeline(task="text-generation")
generator(prompt)  # doctest: +SKIP
```

----------------------------------------

TITLE: Performing Masked LM with MobileBERT AutoModel in PyTorch
DESCRIPTION: Shows how to use the `AutoTokenizer` and `AutoModelForMaskedLM` classes for masked language modeling. It loads the pre-trained MobileBERT model and tokenizer, processes input text, performs inference, and extracts the predicted token for the `[MASK]` position. Utilizes FP16 precision and automatic device mapping for efficiency. Requires the `transformers` and `torch` libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mobilebert.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForMaskedLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    "google/mobilebert-uncased",
)
model = AutoModelForMaskedLM.from_pretrained(
    "google/mobilebert-uncased",
    torch_dtype=torch.float16,
    device_map="auto",
)
inputs = tokenizer("The capital of France is [MASK].", return_tensors="pt").to("cuda")

with torch.no_grad():
    outputs = model(**inputs)
    predictions = outputs.logits

masked_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]
predicted_token_id = predictions[0, masked_index].argmax(dim=-1)
predicted_token = tokenizer.decode(predicted_token_id)

print(f"The predicted token is: {predicted_token}")
```

----------------------------------------

TITLE: Loading DistilBERT for Token Classification with TensorFlow
DESCRIPTION: This code loads a pre-trained DistilBERT model for token classification using `TFAutoModelForTokenClassification` from the Transformers library in TensorFlow. It initializes the model with the specified number of labels and the `id2label` and `label2id` mappings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
>>> from transformers import TFAutoModelForTokenClassification

>>> model = TFAutoModelForTokenClassification.from_pretrained(
...     "distilbert/distilbert-base-uncased", num_labels=13, id2label=id2label, label2id=label2id
... )
```

----------------------------------------

TITLE: Setting Device for Pipeline
DESCRIPTION: This code shows how to specify the device (CPU or GPU) for the pipeline to run on.  It sets the 'device' parameter to 0, which typically refers to the first GPU. It is useful for leveraging GPUs for faster inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/tutoriel_pipeline.md#_snippet_4

LANGUAGE: Python
CODE:
```
transcriber = pipeline(model="openai/whisper-large-v2", device=0)
```

----------------------------------------

TITLE: Using Text Generation Pipeline with Zephyr - Python
DESCRIPTION: This example demonstrates using the text generation pipeline from the Transformers library to simplify chat interactions with the Zephyr model, reducing the need for explicit tokenization and template application by passing structured messages directly.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/chat_templating.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import pipeline

pipe = pipeline("text-generation", "HuggingFaceH4/zephyr-7b-beta")
messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a pirate",
    },
    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
]
print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])  # ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ì‘ë‹µì„ ì¶œë ¥í•©ë‹ˆë‹¤.
```

----------------------------------------

TITLE: Generating Text with Quantized Cohere Command-R in Python
DESCRIPTION: This snippet illustrates how to load the Cohere Command-R model using 4-bit quantization via `BitsAndBytesConfig` to reduce memory usage. It otherwise follows the same pattern as the `AutoModel` example for generating text with the chat template. Requires the `transformers` library, PyTorch, and `bitsandbytes` installed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/cohere.md#_snippet_3

LANGUAGE: python
CODE:
```
import torch
from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM

bnb_config = BitsAndBytesConfig(load_in_4bit=True)
tokenizer = AutoTokenizer.from_pretrained("CohereForAI/c4ai-command-r-v01")
model = AutoModelForCausalLM.from_pretrained("CohereForAI/c4ai-command-r-v01", torch_dtype=torch.float16, device_map="auto", quantization_config=bnb_config, attn_implementation="sdpa")

# format message with the Command-R chat template
messages = [{"role": "user", "content": "How do plants make energy?"}]
input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to("cuda")
output = model.generate(
    input_ids,
    max_new_tokens=100,
    do_sample=True,
    temperature=0.3,
    cache_implementation="static",
)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Loading a Pretrained BERT Model in Transformers
DESCRIPTION: This snippet demonstrates how to load a pretrained BERT model using the AutoModel class from the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/big_models.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import AutoModel

model = AutoModel.from_pretrained("google-bert/bert-base-cased")
```

----------------------------------------

TITLE: Generating and Decoding Text
DESCRIPTION: Generates text tokens and decodes them back to human-readable format
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
generated_ids = model.generate(**model_inputs)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

----------------------------------------

TITLE: Pipeline with Dataset Integration
DESCRIPTION: Shows how to use pipeline with a HuggingFace dataset for efficient processing of large datasets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/pipelines.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import datasets
from transformers import pipeline
from transformers.pipelines.pt_utils import KeyDataset
from tqdm.auto import tqdm

pipe = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h", device=0)
dataset = datasets.load_dataset("superb", name="asr", split="test")

for out in tqdm(pipe(KeyDataset(dataset, "file"))):
    print(out)
```

----------------------------------------

TITLE: Loading a Pre-trained Processor using AutoProcessor Python
DESCRIPTION: This code snippet shows how to load a pre-trained processor for multimodal tasks using `AutoProcessor.from_pretrained`. The processor combines multiple pre-processing tools and is initialized with the checkpoint "microsoft/layoutlmv2-base-uncased".
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/autoclass_tutorial.md#_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")
```

----------------------------------------

TITLE: Load Dataset
DESCRIPTION: This snippet loads the Rotten Tomatoes dataset using the `load_dataset` function from the `datasets` library. This provides the data that will be used to train and evaluate the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_36

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> dataset = load_dataset("rotten_tomatoes")  # doctest: +IGNORE_RESULT
```

----------------------------------------

TITLE: Making Model Predictions with PyTorch
DESCRIPTION: Shows how to pass inputs to the model and get predictions using softmax
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/quicktour.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
pt_outputs = pt_model(**pt_batch)

from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)
```

----------------------------------------

TITLE: Loading a Tokenizer for DistilRoBERTa
DESCRIPTION: Loads a DistilRoBERTa tokenizer from the Hugging Face Transformers library. The tokenizer is used to process text data in preparation for masked language modeling. Text fields need to be flattened for proper processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert/distilroberta-base")
```

----------------------------------------

TITLE: Utilizando un modelo multilingÃ¼e para anÃ¡lisis de sentimiento
DESCRIPTION: Define un modelo BERT multilingÃ¼e para anÃ¡lisis de sentimiento que puede procesar texto en diferentes idiomas.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/quicktour.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
```

----------------------------------------

TITLE: Creating Audio Preprocessing Function with Padding and Truncation
DESCRIPTION: Defines a preprocessing function that handles multiple audio examples, applying padding and truncation to create uniform-length sequences suitable for batch processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/preprocessing.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
>>> def preprocess_function(examples):
...     audio_arrays = [x["array"] for x in examples["audio"]]
...     inputs = feature_extractor(
...         audio_arrays,
...         sampling_rate=16000,
...         padding=True,
...         max_length=1000000,
...         truncation=True,
...     )
...     return inputs
```

----------------------------------------

TITLE: Hugging Face Authentication
DESCRIPTION: Code to login to Hugging Face account for model sharing capabilities
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/translation.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Quantizing a model with AutoRound API (Python)
DESCRIPTION: Quantizes a language model using the AutoRound Python API. It loads a pre-trained model and tokenizer from Hugging Face Transformers, configures AutoRound with specified quantization parameters (bits, group size), and saves the quantized model to a directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/auto_round.md#_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer
from auto_round import AutoRound

model_name = "facebook/opt-125m"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)
bits, group_size, sym = 4, 128, True
# mixed bits config
# layer_config = {"model.decoder.layers.6.self_attn.out_proj": {"bits": 2, "group_size": 32}}
autoround = AutoRound(
    model,
    tokenizer,
    bits=bits,
    group_size=group_size,
    sym=sym,
    # enable_torch_compile=True,
    # layer_config=layer_config,
)

output_dir = "./tmp_autoround"
# format= 'auto_round'(default), 'auto_gptq', 'auto_awq'
autoround.quantize_and_save(output_dir, format='auto_round') 
```

----------------------------------------

TITLE: Initializing Sentiment Analysis Pipeline in Python
DESCRIPTION: Creates a pipeline for sentiment analysis using the default pretrained model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/quicktour.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline("sentiment-analysis")
```

----------------------------------------

TITLE: Automatic Speech Recognition with Transformers Pipeline API
DESCRIPTION: Snippet demonstrating the automatic speech recognition capability using the `transformers` Pipeline API. It initializes a speech recognition model and processes an input audio file URL to extract transcribed text, leveraging GPU for optimization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
from transformers import pipeline

pipeline = pipeline("automatic-speech-recognition", model="openai/whisper-large-v3", device="cuda")
```

LANGUAGE: Python
CODE:
```
pipeline("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac")
{'text': ' He hoped there would be stew for dinner, turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick, peppered flour-fatten sauce.'}
```

----------------------------------------

TITLE: Loading a sharded Transformers model checkpoint in Python
DESCRIPTION: Shows how to load a sharded Transformers model checkpoint using the from_pretrained method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/big_models.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir, max_shard_size="200MB")
...     new_model = AutoModel.from_pretrained(tmp_dir)
```

----------------------------------------

TITLE: Applying Transforms to Datasets
DESCRIPTION: Sets the transform functions on the datasets using the set_transform method, which applies transformations on-the-fly to save disk space and improve processing speed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/semantic_segmentation.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```

----------------------------------------

TITLE: Creating DataCollator for PyTorch Implementation
DESCRIPTION: Initializes a DataCollatorForSeq2Seq object for PyTorch, which efficiently handles padding by dynamically padding sentences to the longest length in each batch rather than padding the entire dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
```

----------------------------------------

TITLE: Distributing 4-bit model across multiple GPUs with bitsandbytes
DESCRIPTION: This code distributes a 4-bit model across multiple GPUs by setting a `max_memory_mapping`. It allocates 16GB of memory to GPU 0 and GPU 1, ensuring efficient model distribution across available resources. The model is loaded using `AutoModelForCausalLM` with the specified memory mapping and quantization configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_one.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
max_memory_mapping = {0: "16GB", 1: "16GB"}
model_4bit = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B", device_map="auto", quantization_config=quantization_config, max_memory=max_memory_mapping
)
```

----------------------------------------

TITLE: Creating DataLoaders
DESCRIPTION: Creates `DataLoader` instances for the training and evaluation datasets, enabling batched iteration over the data.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/training.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
from torch.utils.data import DataLoader

train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)
eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)
```

----------------------------------------

TITLE: Supervised Training of M2M100 Model - PyTorch
DESCRIPTION: This code snippet demonstrates how to fine-tune the M2M100 model for supervised training using the Hugging Face Transformers library. It imports necessary classes, initializes the model and tokenizer, processes the input text, and computes the loss for a forward pass. Dependencies include the Transformers library and SentencePiece for tokenization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/m2m_100.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import M2M100Config, M2M100ForConditionalGeneration, M2M100Tokenizer

model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")
tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M", src_lang="en", tgt_lang="fr")

src_text = "Life is like a box of chocolates."
tgt_text = "La vie est comme une boÃ®te de chocolat."

model_inputs = tokenizer(src_text, text_target=tgt_text, return_tensors="pt")

loss = model(**model_inputs).loss  # forward pass
```

----------------------------------------

TITLE: Applying Quantization to Idefics2 Model in Python
DESCRIPTION: This snippet demonstrates how to apply 4-bit quantization to shrink the Idefics2 model while loading it. By including a quantization configuration, the model's memory requirement is reduced significantly, facilitating deployment in resource-constrained environments. It requires the BitsAndBytes library for handling quantized models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/idefics2.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16
)
model = Idefics2ForConditionalGeneration.from_pretrained(
    "HuggingFaceM4/idefics2-8b",
    torch_dtype=torch.float16,    
    quantization_config=quantization_config,
).to(device)
```

----------------------------------------

TITLE: Using Text Pipeline for Zero-Shot Classification
DESCRIPTION: This snippet demonstrates how to use the pipeline API for NLP tasks, specifically zero-shot classification. It uses a BART model to classify text into user-defined categories without specific training on those categories.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/pipeline_tutorial.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> # Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù‡Ùˆ Ù†Ù…ÙˆØ°Ø¬ "zero-shot-classification".
>>> # Ø³ÙŠØµÙ†Ù Ø§Ù„Ù†ØµØŒ ÙˆÙ„ÙƒÙ† ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø®ØªÙŠØ§Ø± Ø£ÙŠ ØªØ³Ù…ÙŠØ© Ù‚Ø¯ ØªØªØ®ÙŠÙ„Ù‡Ø§
>>> classifier = pipeline(model="facebook/bart-large-mnli")
>>> classifier(
...     "I have a problem with my iphone that needs to be resolved asap!!",
...     candidate_labels=["urgent", "not urgent", "phone", "tablet", "computer"],
... )
{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!', 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'], 'scores': [0.504, 0.479, 0.013, 0.003, 0.002]}
```

----------------------------------------

TITLE: Loading SeamlessM4T Processor and Model
DESCRIPTION: Initializes the SeamlessM4T processor and model for multilingual and multimodal translation tasks. The processor handles data preparation while the model performs the translation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/seamless_m4t.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoProcessor, SeamlessM4TModel

processor = AutoProcessor.from_pretrained("facebook/hf-seamless-m4t-medium")
model = SeamlessM4TModel.from_pretrained("facebook/hf-seamless-m4t-medium")
```

----------------------------------------

TITLE: Training Loop with Accelerator
DESCRIPTION: Example of a training loop using Accelerator's backward method for gradient computation and optimization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
for epoch in range(num_epochs):
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

----------------------------------------

TITLE: ONNX Runtime Inference
DESCRIPTION: Code snippet showing how to load and run inference using an exported ONNX model with ONNX Runtime
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/serialization.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer
>>> from onnxruntime import InferenceSession

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
>>> session = InferenceSession("onnx/model.onnx")
>>> # ONNX Runtime expects NumPy arrays as input
>>> inputs = tokenizer("Using DistilBERT with ONNX Runtime!", return_tensors="np")
>>> outputs = session.run(output_names=["last_hidden_state"], input_feed=dict(inputs))
```

----------------------------------------

TITLE: Loading Text-Only Gemma 3 Model (Python)
DESCRIPTION: Shows how to load the text-only variant of the Gemma 3 model using `AutoModelForCausalLM` and `AutoTokenizer`. It demonstrates basic text generation by encoding an input string, passing it to the model's `generate` method, and decoding the output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gemma3.md#_snippet_6

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    "google/gemma-3-1b-pt",
)
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-3-1b-pt",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    attn_implementation="sdpa"
)
input_ids = tokenizer("Plants create energy through a process known as", return_tensors="pt").to("cuda")

output = model.generate(**input_ids, cache_implementation="static")
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Tokenizing Dataset with BERT Tokenizer
DESCRIPTION: Shows how to tokenize text data using BERT tokenizer and apply the tokenization to the entire dataset using batched processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/training.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)
```

----------------------------------------

TITLE: Vision Classification Pipeline Example
DESCRIPTION: Demonstrates using pipeline for image classification tasks with a ViT model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/pipeline_tutorial.md#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
from transformers import pipeline

vision_classifier = pipeline(model="google/vit-base-patch16-224")
preds = vision_classifier(
    images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
)
```

----------------------------------------

TITLE: Pushing Trained Model to Hugging Face Hub in PyTorch
DESCRIPTION: This code snippet pushes the fine-tuned model to the Hugging Face Hub, making it accessible for others to use. This uses the `push_to_hub` method of the `Trainer`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
>>> trainer.push_to_hub()
```

----------------------------------------

TITLE: Visual Question Answering with VQA Pipeline in Python
DESCRIPTION: Utilizes visual question answering capabilities employing the Salesforce's BLIP-VQA model via the Hugging Face Transformers library. Accepts a question and corresponding image URL and returns the textual answer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import pipeline

pipeline = pipeline(task="visual-question-answering", model="Salesforce/blip-vqa-base")
pipeline(image="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg", question="What is in the image?")
[{'answer': 'statue of liberty'}]
```

----------------------------------------

TITLE: Using Visual Question Answering Pipeline in Python
DESCRIPTION: This snippet demonstrates how to use a multimodal pipeline for Visual Question Answering (VQA), combining image and text inputs to answer questions about an image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/pipeline_tutorial.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> vqa = pipeline(model="impira/layoutlm-document-qa")
>>> output = vqa(
...     image="https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png",
...     question="What is the invoice number?",
... )
>>> output[0]["score"] = round(output[0]["score"], 3)
>>> output
[{'score': 0.425, 'answer': 'us-001', 'start': 16, 'end': 16}]
```

----------------------------------------

TITLE: Training the TensorFlow Model
DESCRIPTION: Executes the training process using the model's fit method with the configured training and validation datasets, number of epochs, and callbacks. The model is automatically uploaded to Hugging Face Hub upon completion.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_22

LANGUAGE: python
CODE:
```
model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)
```

----------------------------------------

TITLE: Installing Transformers using Conda
DESCRIPTION: Command to install Transformers from the conda-forge channel using Conda package manager.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/installation.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
conda install conda-forge::transformers
```

----------------------------------------

TITLE: Configuring TrainingArguments for evaluation
DESCRIPTION: This snippet configures the `TrainingArguments` to perform evaluation at the end of each epoch. This allows for monitoring the evaluation metrics during the fine-tuning process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_7

LANGUAGE: Python
CODE:
```
>>> from transformers import TrainingArguments, Trainer

>>> training_args = TrainingArguments(output_dir="test_trainer", eval_strategy="epoch")
```

----------------------------------------

TITLE: Calculating MLM Loss for Longformer (Python)
DESCRIPTION: Provides a code example for computing the masked language model (MLM) loss with `LongformerForMaskedLM`. It shows how to encode input and label sequences using the tokenizer and then pass them to the model's forward method to obtain the loss.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/longformer.md#_snippet_3

LANGUAGE: python
CODE:
```
input_ids = tokenizer.encode("This is a sentence from [MASK] training data", return_tensors="pt")
mlm_labels = tokenizer.encode("This is a sentence from the training data", return_tensors="pt")
loss = model(input_ids, labels=input_ids, masked_lm_labels=mlm_labels)[0]
```

----------------------------------------

TITLE: Running Model Training with Trainer and Monitoring Performance in Python
DESCRIPTION: This snippet sets up training arguments, initializes a Trainer, runs the training process, and prints a summary of the results including GPU memory usage.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_memory_anatomy.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import TrainingArguments, Trainer, logging

>>> logging.set_verbosity_error()


>>> training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)
>>> trainer = Trainer(model=model, args=training_args, train_dataset=ds)
>>> result = trainer.train()
>>> print_summary(result)
```

----------------------------------------

TITLE: Loading a Model in 4-bit Precision with BitsAndBytesConfig
DESCRIPTION: Code to load a model in 4-bit precision using BitsAndBytesConfig with the load_in_4bit parameter set to True.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/bitsandbytes.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=True)

model_4bit = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-1b7",
    quantization_config=quantization_config
)
```

----------------------------------------

TITLE: Pipeline with Specific Model
DESCRIPTION: Shows how to use a specific model from Hugging Face Hub with pipeline abstraction.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/pipelines.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> pipe = pipeline(model="FacebookAI/roberta-large-mnli")
>>> pipe("This restaurant is awesome")
[{'label': 'NEUTRAL', 'score': 0.7313136458396912}]
```

----------------------------------------

TITLE: Few-Shot Prompting with Chat Template (Python)
DESCRIPTION: Initializes a pipeline and defines the prompt as a list of messages structured like a user-assistant conversation. It uses the tokenizer's `apply_chat_template` method to format the messages into a single string suitable for the model, then runs the pipeline and prints the output. Requires `transformers` and `torch`. Takes a list of user/assistant message dictionaries as input and outputs the formatted prompt plus the model's response.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/prompting.md#_snippet_2

LANGUAGE: python
CODE:
```
from transformers import pipeline
import torch

pipeline = pipeline(model="mistralai/Mistral-7B-Instruct-v0.1", torch_dtype=torch.bfloat16, device_map="auto")

messages = [
    {"role": "user", "content": "Text: The first human went into space and orbited the Earth on April 12, 1961."},
    {"role": "assistant", "content": "Date: 04/12/1961"},
    {"role": "user", "content": "Text: The first-ever televised presidential debate in the United States took place on September 28, 1960, between presidential candidates John F. Kennedy and Richard Nixon."}
]

prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

outputs = pipeline(prompt, max_new_tokens=12, do_sample=True, top_k=10)

for output in outputs:
    print(f"Result: {output['generated_text']}")
```

----------------------------------------

TITLE: Audio Classification Inference using Pipeline in Python
DESCRIPTION: This snippet demonstrates how to use the Hugging Face `pipeline` for audio classification inference. It instantiates a `pipeline` with the specified model and then passes the audio file to it. The output is a list of predicted labels with their corresponding scores.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/audio_classification.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline("audio-classification", model="stevhliu/my_awesome_minds_model")
>>> classifier(audio_file)
```

----------------------------------------

TITLE: Generating Text with AutoModel and AutoProcessor (Python)
DESCRIPTION: Shows how to load the Qwen2.5-VL model and processor using `AutoModel` and `AutoProcessor`. It prepares a conversation with an image and text using the processor's chat template, moves inputs to the appropriate device, and generates text using the model's `generate` method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2_5_vl.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor

model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-VL-7B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    attn_implementation="sdpa"
)
processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct")
messages = [
    {
        "role":"user",
        "content":[
            {
                "type":"image",
                "url": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
            },
            {
                "type":"text",
                "text":"Describe this image."
            }
        ]
    }

]

inputs = processor.apply_chat_template(
    messages,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt"
).to("cuda")

generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
       generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
```

----------------------------------------

TITLE: Logging into Hugging Face Account using Python
DESCRIPTION: This code snippet demonstrates how to log into a Hugging Face account using the 'notebook_login' function from the 'huggingface_hub' package. This is necessary to access models and datasets hosted on Hugging Face Hub. Make sure to have network access and valid user tokens for successful authentication.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from huggingface_hub import notebook_login

notebook_login()
```

----------------------------------------

TITLE: Loading IMDb Dataset with Hugging Face Datasets
DESCRIPTION: Loads the IMDb dataset using the datasets library and examines a sample entry.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/tasks/sequence_classification.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> imdb = load_dataset("imdb")
```

----------------------------------------

TITLE: Hugging Face CLI Login
DESCRIPTION: This command logs in to the Hugging Face Hub using the Hugging Face CLI. This is required before pushing a model to the Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/run_scripts_fr.md#_snippet_17

LANGUAGE: bash
CODE:
```
huggingface-cli login
```

----------------------------------------

TITLE: Beam Search Multinomial Sampling with Transformers
DESCRIPTION: This snippet demonstrates beam search multinomial sampling by setting `num_beams` greater than 1 and `do_sample=True`. It initializes a tokenizer and causal language model, encodes the input text, and generates text using the `model.generate` function.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/generation_strategies.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
inputs = tokenizer("Hugging Face is an open-source company", return_tensors="pt").to("cuda")

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", torch_dtype=torch.float16).to("cuda")
# explicitly set to 100 because Llama2 generation length is 4096
outputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, num_beams=4)
'Hugging Face is an open-source company 100% dedicated to making AI more accessible. We believe that AI should be available to everyone, and weâ€™re working hard to make that a reality.\nWeâ€™re a team of passionate engineers, designers,'
```

----------------------------------------

TITLE: Diverse Beam Search with Transformers
DESCRIPTION: This snippet illustrates how to implement diverse beam search decoding strategy. It initializes the tokenizer and the model and generates text with `model.generate` setting `num_beams`, `num_beam_groups`, and `diversity_penalty` parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/generation_strategies.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
inputs = tokenizer("Hugging Face is an open-source company", return_tensors="pt").to("cuda")

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", torch_dtype=torch.float16).to("cuda")
# explicitly set to 100 because Llama2 generation length is 4096
outputs = model.generate(**inputs, max_new_tokens=50, num_beams=6, num_beam_groups=3, diversity_penalty=1.0, do_sample=False)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
'Hugging Face is an open-source company ðŸ¤—\nWe are an open-source company. Our mission is to democratize AI and make it accessible to everyone. We believe that AI should be used for the benefit of humanity, not for the benefit of a'
```

----------------------------------------

TITLE: Initializing Video Classification Pipeline
DESCRIPTION: Creates a video classification pipeline using a pre-trained model for easy inference
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/video_classification.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> video_cls = pipeline(model="my_awesome_video_cls_model")
>>> video_cls("https://huggingface.co/datasets/sayakpaul/ucf101-subset/resolve/main/v_BasketballDunk_g14_c06.avi")
```

----------------------------------------

TITLE: Loading and Fine-tuning DistilGPT2 with PyTorch
DESCRIPTION: This snippet demonstrates how to load DistilGPT2, define training arguments, create a Trainer, and fine-tune the model using PyTorch. It also includes steps for evaluation and sharing the model on the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/language_modeling.md#2025-04-23_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM, TrainingArguments, Trainer

>>> model = AutoModelForCausalLM.from_pretrained("distilbert/distilgpt2")

>>> training_args = TrainingArguments(
...     output_dir="my_awesome_eli5_clm-model",
...     eval_strategy="epoch",
...     learning_rate=2e-5,
...     weight_decay=0.01,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=lm_dataset["train"],
...     eval_dataset=lm_dataset["test"],
...     data_collator=data_collator,
... )

>>> trainer.train()

>>> import math

>>> eval_results = trainer.evaluate()
>>> print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
Perplexity: 49.61

>>> trainer.push_to_hub()
```

----------------------------------------

TITLE: Authenticating with the Hugging Face Hub in Notebooks
DESCRIPTION: Shows how to authenticate with the Hugging Face Hub in Jupyter notebooks or Colaboratory using the notebook_login function.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/model_sharing.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Inference with TensorFlow Model
DESCRIPTION: Demonstrates how to perform inference using the loaded TensorFlow model and process the output logits with softmax to get probabilities.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/quicktour.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
>>> tf_outputs = tf_model(tf_batch)

>>> import tensorflow as tf

>>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
>>> tf_predictions  # doctest: +IGNORE_RESULT
```

----------------------------------------

TITLE: Logging in to Hugging Face Hub
DESCRIPTION: Logs in to the Hugging Face Hub to enable model sharing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/document_question_answering.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Performing Inference with PyTorch Model
DESCRIPTION: This code snippet performs inference using a PyTorch model loaded with `AutoModelForTokenClassification`. The input is passed to the model to obtain `logits`, which represent the raw, unnormalized predictions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_29

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForTokenClassification

>>> model = AutoModelForTokenClassification.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

----------------------------------------

TITLE: Generating Text with DBRX and Flash Attention in Python
DESCRIPTION: This code snippet demonstrates how to use the DBRX model with Flash Attention 2 for faster text generation. It sets the `attn_implementation` parameter to `flash_attention_2` when loading the model.  It requires the `transformers`, `torch`, and `flash-attn` libraries.  Replace `YOUR_HF_TOKEN` with your actual Hugging Face token.  Make sure to install flash-attn using `pip install flash-attn` before running this code.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/dbrx.md#_snippet_1

LANGUAGE: python
CODE:
```
from transformers import DbrxForCausalLM, AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained("databricks/dbrx-instruct", token="YOUR_HF_TOKEN")
model = DbrxForCausalLM.from_pretrained(
    "databricks/dbrx-instruct",
    device_map="auto",
    torch_dtype=torch.bfloat16,
    token="YOUR_HF_TOKEN",
    attn_implementation="flash_attention_2",
    )

input_text = "What does it take to build a great LLM?"
messages = [{"role": "user", "content": input_text}]
input_ids = tokenizer.apply_chat_template(messages, return_dict=True, tokenize=True, add_generation_prompt=True, return_tensors="pt").to("cuda")

outputs = model.generate(**input_ids, max_new_tokens=200)
print(tokenizer.decode(outputs[0]))
```

----------------------------------------

TITLE: Adding a LoRA Adapter for Training
DESCRIPTION: Simple code snippet showing how to add a configured LoRA adapter to a model in preparation for training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/peft.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
model.add_adapter(peft_config)
```

----------------------------------------

TITLE: Activating Virtual Environment (Windows)
DESCRIPTION: This command activates the virtual environment on Windows systems. Activating the environment ensures that subsequent pip commands install packages within the isolated environment.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/installation.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
".env/Scripts/activate"
```

----------------------------------------

TITLE: Installing Required Dependencies
DESCRIPTION: Install the necessary HuggingFace libraries for implementing question answering
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/question_answering.md#2025-04-23_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers datasets evaluate
```

----------------------------------------

TITLE: Creating Trainer and Training Model (PyTorch)
DESCRIPTION: Initializes a Trainer with model, training arguments, datasets, and data collator, then trains the model
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/masked_language_modeling.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=lm_dataset["train"],
...     eval_dataset=lm_dataset["test"],
...     data_collator=data_collator,
...     tokenizer=tokenizer,
... )

>>> trainer.train()
```

----------------------------------------

TITLE: Processing Multiple Inputs with Text Generation Pipeline in Python
DESCRIPTION: Shows how to process multiple input strings by passing them as a list to the text generation pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/pipeline_tutorial.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
>>> generator(
...     [
...         "Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone",
...         "Nine for Mortal Men, doomed to die, One for the Dark Lord on his dark throne",
...     ]
... )  # doctest: +SKIP
```

----------------------------------------

TITLE: Chat Template Using Jinja Templating for Message Formatting
DESCRIPTION: Illustrates a simplified Zephyr chat template using Jinja, showing how messages can be formatted with specific role tokens and generation prompts
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/chat_templating.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
{%- for message in messages %}\n    {{- '<|' + message['role'] + |>\n' }}\n    {{- message['content'] + eos_token }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|assistant|>\n' }}\n{%- endif %}
```

----------------------------------------

TITLE: Loading a Model with FlashAttention-2 in Python
DESCRIPTION: Demonstrates how to load a pre-trained language model using FlashAttention-2 for optimized attention computation. It uses 8-bit quantization and bfloat16 precision.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/llm_optims.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quant_config = BitsAndBytesConfig(load_in_8bit=True)
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2b",
    quantization_config=quant_config,
    torch_dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
)
```

----------------------------------------

TITLE: Named Entity Recognition Pipeline
DESCRIPTION: Shows how to perform token classification for named entity recognition to identify entities like organizations and locations in text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/task_summary.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline(task="ner")
>>> preds = classifier("Hugging Face is a French company based in New York City.")
>>> preds = [
...     {
...         "entity": pred["entity"],
...         "score": round(pred["score"], 4),
...         "index": pred["index"],
...         "word": pred["word"],
...         "start": pred["start"],
...         "end": pred["end"],
...     }
...     for pred in preds
... ]
>>> print(*preds, sep="\n")
```

----------------------------------------

TITLE: Speech Recognition with Speech2Text Model
DESCRIPTION: Example of using Speech2Text model for automatic speech recognition (ASR) using the LibriSpeech pretrained model. Demonstrates loading the model and processor, processing audio input, and generating transcriptions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/speech_to_text.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration
from datasets import load_dataset

model = Speech2TextForConditionalGeneration.from_pretrained("facebook/s2t-small-librispeech-asr")
processor = Speech2TextProcessor.from_pretrained("facebook/s2t-small-librispeech-asr")

ds = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")

inputs = processor(ds[0]["audio"]["array"], sampling_rate=ds[0]["audio"]["sampling_rate"], return_tensors="pt")
generated_ids = model.generate(inputs["input_features"], attention_mask=inputs["attention_mask"])

transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)
```

----------------------------------------

TITLE: Processing Multiple Audio Files
DESCRIPTION: This snippet illustrates how to process multiple audio files using the ASR pipeline by passing a list of URLs. It demonstrates the ability to handle multiple inputs in a single call.  The pipeline processes each audio file and returns a list of dictionaries, each containing the transcribed text for the corresponding audio file.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/tutoriel_pipeline.md#_snippet_3

LANGUAGE: Python
CODE:
```
transcriber(
    [
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac",
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac",
    ]
)
```

----------------------------------------

TITLE: Generating Text with PyTorch Model
DESCRIPTION: This snippet demonstrates how to generate text using the finetuned model in PyTorch, including loading the model and decoding the output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_23

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("username/my_awesome_eli5_clm-model")
outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)

tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

----------------------------------------

TITLE: Configuring Training Arguments and Trainer for PyTorch
DESCRIPTION: This code sets up the training arguments and initializes the Trainer for finetuning the model in PyTorch. It includes settings for output directory, evaluation strategy, learning rate, and pushing to the Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
training_args = TrainingArguments(
    output_dir="my_awesome_eli5_clm-model",
    eval_strategy="epoch",
    learning_rate=2e-5,
    weight_decay=0.01,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_dataset["train"],
    eval_dataset=lm_dataset["test"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)

trainer.train()
```

----------------------------------------

TITLE: Initializing Pipeline with Specific Model
DESCRIPTION: This code initializes an ASR pipeline using a specific model, "openai/whisper-large-v2".  It demonstrates how to override the default model with a user-specified model from the Hugging Face Model Hub. This allows for using different models optimized for specific tasks or languages.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/tutoriel_pipeline.md#_snippet_2

LANGUAGE: Python
CODE:
```
>>> transcriber = pipeline(model="openai/whisper-large-v2")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

----------------------------------------

TITLE: Push Model to Hugging Face Hub (PyTorch)
DESCRIPTION: Uploads the trained model to the Hugging Face Hub using the `push_to_hub()` method of the `Trainer`.  This makes the model available for others to use. Requires a trained `trainer` object and that `push_to_hub=True` was set in `TrainingArguments`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_16

LANGUAGE: python
CODE:
```
>>> trainer.push_to_hub()
```

----------------------------------------

TITLE: Installing Transformers and related libraries
DESCRIPTION: Installs the necessary libraries for using the ðŸ¤— Transformers library, including transformers, datasets, evaluate, and accelerate. This command uses pip, the Python package installer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/quicktour.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
"!pip install transformers datasets evaluate accelerate"
```

----------------------------------------

TITLE: Creating a Virtual Environment in Python
DESCRIPTION: Command to create a virtual environment in your project workspace using Python's built-in venv module.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/installation.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python -m venv .env
```

----------------------------------------

TITLE: Speculative Decoding with Assistant Model - Python
DESCRIPTION: This snippet implements speculative decoding by generating candidate tokens using an assistant model, which is verified by a larger model to improve generation speed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_optims.md#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
outputs = model.generate(**inputs, assistant_model=assistant_model)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

----------------------------------------

TITLE: Implementing Backward Pass
DESCRIPTION: This snippet replaces the standard `loss.backward()` call with `accelerator.backward(loss)`. This handles the backward pass in a distributed manner, taking care of gradient synchronization and other distributed training complexities.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/accelerate.md#_snippet_3

LANGUAGE: python
CODE:
```
>>> for epoch in range(num_epochs):
...     for batch in train_dataloader:
...         outputs = model(**batch)
...         loss = outputs.loss
...         accelerator.backward(loss)

...         optimizer.step()
...         lr_scheduler.step()
...         optimizer.zero_grad()
...         progress_bar.update(1)
```

----------------------------------------

TITLE: Initializing DistilBERT Tokenizer for Question Answering
DESCRIPTION: Loads the DistilBERT tokenizer which will be used to preprocess the questions and context passages for the model input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/question_answering.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Text Generation with GPT-2
DESCRIPTION: This section describes GPT-2, a decoder-only model pre-trained on a large amount of text, and its application to text generation. It highlights the use of Byte-Pair Encoding (BPE) for tokenization and causal language modeling as its pre-training objective.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks_explained.md#2025-04-22_snippet_3



----------------------------------------

TITLE: Loading Models with Specific dtype
DESCRIPTION: This snippet demonstrates how to load a model with a specific data type (dtype), such as `torch.float16`. This can be useful for reducing memory usage when loading large models, especially when the pre-trained weights are in a lower precision. It loads T5ForConditionalGeneration model with torch.float16.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/model.md#_snippet_4

LANGUAGE: python
CODE:
```
model = T5ForConditionalGeneration.from_pretrained("t5", torch_dtype=torch.float16)
```

----------------------------------------

TITLE: Complete Chat Generation Example with Zephyr
DESCRIPTION: Full example showing how to use chat templates with the Zephyr model for generating responses, including system prompts and model generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/chat_templating.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

checkpoint = "HuggingFaceH4/zephyr-7b-beta"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint)

messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a pirate",
    },
    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
 ]
tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")
print(tokenizer.decode(tokenized_chat[0]))

outputs = model.generate(tokenized_chat, max_new_tokens=128) 
print(tokenizer.decode(outputs[0]))
```

----------------------------------------

TITLE: Loading GPT-2 Model and Tokenizer with Transformers in Python
DESCRIPTION: This snippet demonstrates loading a pre-trained GPT-2 language model and its tokenizer, using the Transformers library from Hugging Face. The method utilizes a backend detection library to automatically determine the device (e.g., CUDA, CPU). The dependencies include the 'transformers' library and a test utility from 'accelerate'.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perplexity.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import GPT2LMHeadModel, GPT2TokenizerFast
from accelerate.test_utils.testing import get_backend

device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)
model_id = "openai-community/gpt2-large"
model = GPT2LMHeadModel.from_pretrained(model_id).to(device)
tokenizer = GPT2TokenizerFast.from_pretrained(model_id)
```

----------------------------------------

TITLE: Training T5 for Summarization with Trainer API on CNN/DailyMail Dataset
DESCRIPTION: This bash script demonstrates how to fine-tune a T5 model on the CNN/DailyMail dataset for text summarization using Hugging Face's Trainer API. It configures training and evaluation settings, specifies the required prefix for T5 models, and enables prediction with generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Loading a specific model revision
DESCRIPTION: This Python code demonstrates how to load a specific version of a pre-trained model from the Hugging Face Hub using the `revision` parameter.  It uses the `AutoModel.from_pretrained` method to load the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
"model = AutoModel.from_pretrained(\n    \"julien-c/EsperBERTo-small\", revision=\"4c77982\"\n)"
```

----------------------------------------

TITLE: Define Image Augmentation Transformations with Albumentations in Python
DESCRIPTION: Defines a series of image augmentation transformations using the Albumentations library. These transformations include resizing, horizontal flipping, and random brightness/contrast adjustments. The `Compose` function chains these transformations together, and `BboxParams` ensures that bounding boxes are updated accordingly with the image transformations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/object_detection.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> import albumentations
>>> import numpy as np
>>> import torch

>>> transform = albumentations.Compose(
...     [
...         albumentations.Resize(480, 480),
...         albumentations.HorizontalFlip(p=1.0),
...         albumentations.RandomBrightnessContrast(p=1.0),
...     ],
...     bbox_params=albumentations.BboxParams(format="coco", label_fields=["category"]),
... )
```

----------------------------------------

TITLE: Performing Object Detection Inference with D-FINE using Transformers (Python)
DESCRIPTION: This snippet demonstrates how to use the DFineForObjectDetection model from the Hugging Face transformers library to perform object detection on an image. It covers loading the image, initializing the image processor and model, preparing the input tensors, running the model inference with `torch.no_grad()`, post-processing the results, and printing the detected objects with their scores and bounding boxes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/d_fine.md#_snippet_0

LANGUAGE: Python
CODE:
```
import torch
from transformers.image_utils import load_image
from transformers import DFineForObjectDetection, AutoImageProcessor

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = load_image(url)

image_processor = AutoImageProcessor.from_pretrained("ustc-community/dfine_x_coco")
model = DFineForObjectDetection.from_pretrained("ustc-community/dfine_x_coco")

inputs = image_processor(images=image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

results = image_processor.post_process_object_detection(outputs, target_sizes=[(image.height, image.width)], threshold=0.5)

for result in results:
    for score, label_id, box in zip(result["scores"], result["labels"], result["boxes"]):
        score, label = score.item(), label_id.item()
        box = [round(i, 2) for i in box.tolist()]
        print(f"{model.config.id2label[label]}: {score:.2f} {box}")
```

----------------------------------------

TITLE: Loading and Splitting Dataset
DESCRIPTION: Loading OPUS Books dataset and splitting into train/test sets
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/translation.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> books = load_dataset("opus_books", "en-fr")
>>> books = books["train"].train_test_split(test_size=0.2)
```

----------------------------------------

TITLE: Creating PyTorch DataLoaders
DESCRIPTION: This code creates PyTorch DataLoaders for the training and evaluation datasets. The training DataLoader is shuffled, and both DataLoaders use a batch size of 8. The DataLoaders will provide batches of data to the training loop.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_19

LANGUAGE: Python
CODE:
```
from torch.utils.data import DataLoader

train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)
eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)
```

----------------------------------------

TITLE: Loading PEFT adapter
DESCRIPTION: Loads a PEFT adapter model for causal language modeling using `AutoModelForCausalLM`.  It takes the PEFT model ID and uses it to load the pre-trained model. It requires the `transformers` library to be installed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/peft.md#_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id)
```

----------------------------------------

TITLE: Preprocessing with DistilBERT Tokenizer - Python
DESCRIPTION: This snippet initializes a tokenizer from the DistilBERT model to preprocess the dataset. It is mandatory to load this tokenizer before tokenizing the reviews.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Enabling torch.compile with inductor backend in TrainingArguments
DESCRIPTION: This snippet shows how to enable `torch.compile` and select the `inductor` backend for optimized kernel compilation using `TrainingArguments`.  This can significantly speed up training by compiling PyTorch code into optimized kernels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments

args = TrainingArguments(
    per_device_train_batch_size=4,
    gradient_accumulation_steps=16,
    gradient_checkpointing=True,
    bf16=True,
    optim="adamw_bnb_8bit",
    dataloader_pin_memory=True,
    dataloader_num_workers=4,
    torch_empty_cache_steps=4,
    torch_compile=True,
    torch_compile_backend="inductor"
)
```

----------------------------------------

TITLE: Configuring Gradient Accumulation for Efficient Training - Python
DESCRIPTION: This snippet illustrates how to set up gradient accumulation in the `TrainingArguments` class. It allows for training large models that may not fit on a single GPU by accumulating gradients over several mini-batches before performing weight updates, effectively increasing the batch size without additional memory consumption.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
from transformers import TrainingArguments

# effective batch size of 64
args = TrainingArguments(
    per_device_train_batch_size=4,
    gradient_accumulation_steps=16,
)
```

----------------------------------------

TITLE: Loading a Pretrained Tokenizer in Hugging Face Transformers
DESCRIPTION: This snippet shows how to load a pretrained tokenizer file (specifically for BigBird which uses a sentencepiece-based tokenizer) in the Hugging Face Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/templates/adding_a_new_model/open_model_proposals/ADD_BIG_BIRD.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
from transformers import BertGenerationTokenizer
tokenizer = BertGenerationTokenizer("/path/to/gpt2.model/file")
```

----------------------------------------

TITLE: Using Pipeline for Fill-Mask with Longformer (Python)
DESCRIPTION: Demonstrates how to use the `transformers` `pipeline` function for the "fill-mask" task with a Longformer model. It loads the specified model and uses a GPU (device=0) for inference on the provided masked text, leveraging half-precision floats for efficiency.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/longformer.md#_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipeline = pipeline(
    task="fill-mask",
    model="allenai/longformer-base-4096",
    torch_dtype=torch.float16,
    device=0
)
pipeline("""San Francisco 49ers cornerback Shawntae Spencer will miss the rest of the <mask> with a torn ligament in his left knee.
Spencer, a fifth-year pro, will be placed on injured reserve soon after undergoing surgery Wednesday to repair the ligament. He injured his knee late in the 49ersâ€™ road victory at Seattle on Sept. 14, and missed last weekâ€™s victory over Detroit.
Tarell Brown and Donald Strickland will compete to replace Spencer with the 49ers, who kept 12 defensive backs on their 53-man roster to start the season. Brown, a second-year pro, got his first career interception last weekend while filling in for Strickland, who also sat out with a knee injury.""")
```

----------------------------------------

TITLE: Adding a new adapter
DESCRIPTION: Adds a new LoRA adapter to an existing model.  It defines a `LoraConfig` object and then adds the adapter to the model using the `add_adapter` method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/peft.md#_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import LoraConfig

model_id = "facebook/opt-350m"
model = AutoModelForCausalLM.from_pretrained(model_id)

lora_config = LoraConfig(
    target_modules=["q_proj", "k_proj"],
    init_lora_weights=False
)

model.add_adapter(lora_config, adapter_name="adapter_1")
```

----------------------------------------

TITLE: Generating Text with DBRX and Standard Attention in Python
DESCRIPTION: This code snippet demonstrates how to use the DBRX model to generate text with standard attention. It loads the model and tokenizer, prepares input text using the tokenizer's `apply_chat_template` method, and generates text using the `generate` method. It requires the `transformers` and `torch` libraries. Replace `YOUR_HF_TOKEN` with your actual Hugging Face token.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/dbrx.md#_snippet_0

LANGUAGE: python
CODE:
```
from transformers import DbrxForCausalLM, AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained("databricks/dbrx-instruct", token="YOUR_HF_TOKEN")
model = DbrxForCausalLM.from_pretrained(
    "databricks/dbrx-instruct",
    device_map="auto",
    torch_dtype=torch.bfloat16,
    token="YOUR_HF_TOKEN",
    )

input_text = "What does it take to build a great LLM?"
messages = [{"role": "user", "content": input_text}]
input_ids = tokenizer.apply_chat_template(messages, return_dict=True, tokenize=True, add_generation_prompt=True, return_tensors="pt").to("cuda")

outputs = model.generate(**input_ids, max_new_tokens=200)
print(tokenizer.decode(outputs[0]))
```

----------------------------------------

TITLE: Using MatCha for Chart Question Answering
DESCRIPTION: This code snippet demonstrates how to use a pre-trained MatCha model (`google/matcha-chartqa`) to answer questions about charts. It loads the model and processor, fetches an image from a URL, and generates a prediction based on the image and a given question.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/matcha.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoProcessor, Pix2StructForConditionalGeneration
import requests
from PIL import Image

model = Pix2StructForConditionalGeneration.from_pretrained("google/matcha-chartqa").to(0)
processor = AutoProcessor.from_pretrained("google/matcha-chartqa")
url = "https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/20294671002019.png"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, text="Is the sum of all 4 places greater than Laos?", return_tensors="pt").to(0)
predictions = model.generate(**inputs, max_new_tokens=512)
print(processor.decode(predictions[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Installing Datasets Package using pip
DESCRIPTION: This bash command installs the `datasets` package, which is used to load and experiment with datasets in the provided examples. This package provides tools for easy access to and manipulation of various datasets for machine learning tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_0

LANGUAGE: bash
CODE:
```
pip install datasets
```

----------------------------------------

TITLE: Running Text Classification with Custom Dataset
DESCRIPTION: Example command for running the text classification script with a custom dataset. Shows how to specify model, input files, output directory, and which operations to perform (training, evaluation, prediction).
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/tensorflow/text-classification/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
python run_text_classification.py \
--model_name_or_path distilbert/distilbert-base-cased \
--train_file training_data.json \
--validation_file validation_data.json \
--output_dir output/ \
--test_file data_to_predict.json \
--do_train \
--do_eval \
--do_predict
```

----------------------------------------

TITLE: Using Pipeline for Masked Language Model Inference
DESCRIPTION: Performs inference with a fine-tuned masked language model using the Transformers pipeline, returning multiple predictions for the masked token.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/masked_language_modeling.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
from transformers import pipeline

mask_filler = pipeline("fill-mask", "stevhliu/my_awesome_eli5_mlm_model")
mask_filler(text, top_k=3)
```

----------------------------------------

TITLE: Creating Custom Device Map for 8-bit Model with CPU Offloading
DESCRIPTION: Code showing how to create a custom device map to distribute model components between GPU and CPU when using 8-bit quantization with offloading.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/bitsandbytes.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
device_map = {
    "transformer.word_embeddings": 0,
    "transformer.word_embeddings_layernorm": 0,
    "lm_head": "cpu",
    "transformer.h": 0,
    "transformer.ln_f": 0,
}
```

----------------------------------------

TITLE: Applying Preprocessing and Data Collation
DESCRIPTION: Apply preprocessing to the dataset and set up data collation for batch processing
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/question_answering.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
>>> tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad["train"].column_names)
```

LANGUAGE: python
CODE:
```
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator()
```

----------------------------------------

TITLE: Training Loop with Accelerate
DESCRIPTION: Implementation of training loop using Accelerate's backward method for distributed training
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/accelerate.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
for epoch in range(num_epochs):
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

----------------------------------------

TITLE: Generating Text with an Encoder-Decoder LLM in Python
DESCRIPTION: This snippet illustrates the use of the `text2text-generation` pipeline to perform text generation tasks with an encoder-decoder model (Flan-T5). Given a prompt for translation, it generates the required output directly.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/prompting.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> text2text_generator = pipeline("text2text-generation", model = 'google/flan-t5-base')
>>> prompt = "Translate from English to French: I'm very happy to see you"

>>> text2text_generator(prompt)
[{'generated_text': 'Je suis trÃ¨s heureuse de vous rencontrer.'}]
```

----------------------------------------

TITLE: Loading Pre-trained Model for Sequence Classification with TFAutoModelForSequenceClassification in Python
DESCRIPTION: This snippet shows how to load a TensorFlow pre-trained model for sequence classification using TFAutoModelForSequenceClassification, ensuring users can easily adopt same checkpoints across different tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/autoclass_tutorial.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> from transformers import TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: FP4 with BetterTransformer and Flash Attention
DESCRIPTION: Combines FP4 quantization with BetterTransformer and flash attention for improved performance.  Sets the quantization config, loads the model, prepares the input, and performs generation with flash attention enabled.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_infer_gpu_one.md#2025-04-22_snippet_17

LANGUAGE: Python
CODE:
```
"import torch\nfrom torch.nn.attention import SDPBackend, sdpa_kernel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", quantization_config=quantization_config)\n\ninput_text = \"Hello my dog is cute and\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\nwith sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n    outputs = model.generate(**inputs)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))"
```

----------------------------------------

TITLE: Defining Evaluation Metrics for Model Training
DESCRIPTION: Loads an accuracy metric and defines a compute_metrics function that calculates and returns model accuracy during training and evaluation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/training.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
import numpy as np
from datasets import load_metric

metric = load_metric("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

----------------------------------------

TITLE: Creating a Metrics Computation Function for Model Evaluation
DESCRIPTION: Defines a function that computes accuracy metrics from model predictions by converting logits to predicted class indices and comparing with reference labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/training.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> def compute_metrics(eval_pred):
...     logits, labels = eval_pred
...     predictions = np.argmax(logits, axis=-1)
...     return metric.compute(predictions=predictions, references=labels)
```

----------------------------------------

TITLE: Fine-tuning BERT on CoNLL-2003 Dataset for Token Classification
DESCRIPTION: This command fine-tunes a BERT base uncased model on the CoNLL-2003 dataset for named entity recognition, saving the output to a temporary directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/tensorflow/token-classification/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python run_ner.py \
  --model_name_or_path google-bert/bert-base-uncased \
  --dataset_name conll2003 \
  --output_dir /tmp/test-ner
```

----------------------------------------

TITLE: Fine-tuning Whisper Small on Hindi Common Voice (Single GPU)
DESCRIPTION: Bash command to fine-tune OpenAI's Whisper small model on the Hindi subset of Common Voice using a single GPU. The script configures essential parameters like language, learning rate, and input preprocessing.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-recognition/README.md#2025-04-22_snippet_7

LANGUAGE: bash
CODE:
```
python run_speech_recognition_seq2seq.py \
	--model_name_or_path="openai/whisper-small" \
	--dataset_name="mozilla-foundation/common_voice_11_0" \
	--dataset_config_name="hi" \
	--language="hindi" \
	--task="transcribe" \
	--train_split_name="train+validation" \
	--eval_split_name="test" \
	--max_steps="5000" \
	--output_dir="./whisper-small-hi" \
	--per_device_train_batch_size="16" \
	--gradient_accumulation_steps="2" \
	--per_device_eval_batch_size="16" \
	--logging_steps="25" \
	--learning_rate="1e-5" \
	--warmup_steps="500" \
	--eval_strategy="steps" \
	--eval_steps="1000" \
	--save_strategy="steps" \
	--save_steps="1000" \
	--generation_max_length="225" \
	--preprocessing_num_workers="16" \
	--max_duration_in_seconds="30" \
	--text_column_name="sentence" \
	--freeze_feature_encoder="False" \
	--gradient_checkpointing \
	--fp16 \
	--overwrite_output_dir \
	--do_train \
	--do_eval \
	--predict_with_generate \
	--use_auth_token
```

----------------------------------------

TITLE: Inference with Pipeline using Mistral3
DESCRIPTION: This code snippet demonstrates how to perform inference with the Mistral3 model using the `image-text-to-text` pipeline from the `transformers` library. It initializes the pipeline with the specified model checkpoint and then performs inference on an image described by a URL.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mistral3.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> messages = [
...     {
...         "role": "user",
...         "content": [
...             {
...                 "type": "image",
...                 "image": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg",
...             },
...             {"type": "text", "text": "Describe this image."},
...         ],
...     },
... ]

>>> pipe = pipeline("image-text-to-text", model="mistralai/Mistral-Small-3.1-24B-Instruct-2503", torch_dtype=torch.bfloat16)
>>> outputs = pipe(text=messages, max_new_tokens=50, return_full_text=False)
>>> outputs[0]["generated_text"]
'The image depicts a vibrant and lush garden scene featuring a variety of wildflowers and plants. The central focus is on a large, pinkish-purple flower, likely a Greater Celandine (Chelidonium majus), with a'
```

----------------------------------------

TITLE: Model-Based Text Generation from Images Using AutoModel in Python
DESCRIPTION: This snippet illustrates the use of an AutoModel class to perform conditional text generation based on image input using the PaliGemma2-3B-Mix model. Dependencies include PyTorch, Transformers, PIL for image handling, and requests for fetching images. The code processes an image and text input and outputs a text summary, leveraging CUDA and a specific attention implementation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/paligemma.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
import requests
from PIL import Image
from transformers import AutoProcessor, PaliGemmaForConditionalGeneration

model = PaliGemmaForConditionalGeneration.from_pretrained(
    "google/paligemma2-3b-mix-224",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    attn_implementation="sdpa"
)
processor = AutoProcessor.from_pretrained(
    "google/paligemma2-3b-mix-224",
)

prompt = "What is in this image?"
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(image, prompt, return_tensors="pt").to("cuda")

output = model.generate(**inputs, max_new_tokens=50, cache_implementation="static")
print(processor.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Loading an audio dataset
DESCRIPTION: This code demonstrates loading an audio dataset using the `load_dataset` function from the `datasets` library. It retrieves a single audio sample from the 'gtzan' dataset for use as an audio prompt.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/musicgen_melody.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> dataset = load_dataset("sanchit-gandhi/gtzan", split="train", streaming=True)
>>> sample = next(iter(dataset))["audio"]
```

----------------------------------------

TITLE: Generating Responses with Transformers in Python
DESCRIPTION: This snippet shows how to process a conversation that includes text and media (images and video) inputs using the Transformers library. It applies a chat template and generates a response using the model.generate function. No external dependencies are listed beyond standard installation of the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llava_next_video.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
# Generate from image and video mixed inputs
conversation = [
    {

        "role": "user",
        "content": [
            {"type": "text", "text": "How many cats are there in the image?"},
            {"type": "image", "url": "http://images.cocodataset.org/val2017/000000039769.jpg"},
            ],
    },
    {

        "role": "assistant",
        "content": [{"type": "text", "text": "There are two cats"}],
    },
    {

        "role": "user",
        "content": [
            {"type": "text", "text": "Why is this video funny?"},
            {"type": "video", "path": video_path},
            ],
    },
]
inputs = processor.apply_chat_template(conversation, num_frames=8, add_generation_prompt=True, tokenize=True, return_dict=True, padding=True, return_tensors="pt")

# Generate
generate_ids = model.generate(**inputs, max_length=50)
processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
```

----------------------------------------

TITLE: Performing Document Question Answering using Donut and Pipeline in Python
DESCRIPTION: This snippet demonstrates the usage of the `pipeline` feature from the `transformers` library to perform document question answering with Donut. It initializes the pipeline for the 'document-question-answering' task using a pre-trained model, loads a dataset containing document images, and queries the image for specific information.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/donut.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
# pip install datasets
import torch
from transformers import pipeline
from PIL import Image

pipeline = pipeline(
    task="document-question-answering",
    model="naver-clova-ix/donut-base-finetuned-docvqa",
    device=0,
    torch_dtype=torch.float16
)
dataset = load_dataset("hf-internal-testing/example-documents", split="test")
image = dataset[0]["image"]

pipeline(image=image, question="What time is the coffee break?")
```

----------------------------------------

TITLE: Using Speech Recognition Pipeline with Timestamps in Python
DESCRIPTION: This snippet demonstrates how to use the Automatic Speech Recognition pipeline with the 'return_timestamps' parameter to get both transcribed text and timestamp information for each chunk.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/pipeline_tutorial.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> transcriber = pipeline(model="openai/whisper-large-v2", return_timestamps=True)
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.', 'chunks': [{'timestamp': (0.0, 11.88), 'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its'}, {'timestamp': (11.88, 12.38), 'text': ' creed.'}]}
```

----------------------------------------

TITLE: Distributed Training with PyTorch and Mixed Precision
DESCRIPTION: This snippet showcases how to run a PyTorch training script with distributed training and mixed precision enabled. It uses `torchrun` to launch the script across multiple GPUs, activates mixed precision with the `--fp16` flag, and specifies other necessary parameters for summarization task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/run_scripts_fr.md#_snippet_5

LANGUAGE: bash
CODE:
```
torchrun \
    --nproc_per_node 8 pytorch/summarization/run_summarization.py \
    --fp16 \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Implementing Autoregressive Generation in LLMs with Python
DESCRIPTION: This code snippet demonstrates the basic implementation of autoregressive text generation using a language model. It processes an input prompt, then iteratively generates new tokens by taking the argmax of the output logits and appending each new token to the input sequence.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/llm_tutorial_optimization.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
input_ids = tokenizer(prompt, return_tensors="pt")["input_ids"].to("cuda")

for _ in range(5):
  next_logits = model(input_ids)["logits"][:, -1:]
  next_token_id = torch.argmax(next_logits,dim=-1)

  input_ids = torch.cat([input_ids, next_token_id], dim=-1)
  print("shape of input_ids", input_ids.shape)

generated_text = tokenizer.batch_decode(input_ids[:, -5:])
generated_text
```

----------------------------------------

TITLE: Loading a TF Pre-trained Model for Sequence Classification with TFAutoModel
DESCRIPTION: This code snippet shows how to load a pre-trained TensorFlow model for sequence classification using `TFAutoModelForSequenceClassification.from_pretrained` from the `transformers` library. The model is loaded from the specified checkpoint name, 'distilbert/distilbert-base-uncased'.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/autoclass_tutorial.md#_snippet_6

LANGUAGE: Python
CODE:
```
>>> from transformers import TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Using SigLIP with Pipeline for Zero-Shot Image Classification
DESCRIPTION: Demonstrates how to use the Transformers pipeline for zero-shot image classification with SigLIP. The code loads an image from a URL and classifies it against a set of candidate labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/siglip.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

image = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
candidate_labels = ["a Pallas cat", "a lion", "a Siberian tiger"]

pipeline = pipeline(task="zero-shot-image-classification", model="google/siglip-base-patch16-224", device=0, torch_dtype=torch.bfloat16)
pipeline(image, candidate_labels=candidate_labels)
```

----------------------------------------

TITLE: Training the Model with Trainer in PyTorch
DESCRIPTION: This snippet initializes the `Trainer` class with the model and the training arguments, subsequently calling the train method to start the fine-tuning process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/sequence_classification.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=train_ds,
...     eval_dataset=test_ds,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

----------------------------------------

TITLE: Checking GPU Availability for TTS Model Training
DESCRIPTION: Bash commands to verify GPU availability for training text-to-speech models, supporting both NVIDIA and AMD GPUs
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/text-to-speech.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
!nvidia-smi
!rocm-smi
```

----------------------------------------

TITLE: Loading DistilBERT Tokenizer
DESCRIPTION: Loads the DistilBERT tokenizer from the Transformers library to preprocess token data for input into the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/token_classification.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Chat Template Example in Jinja
DESCRIPTION: This code snippet shows an example of a Jinja template used for formatting chat conversations into a single string suitable for input to a language model. The template iterates through a list of messages, distinguishes between user and assistant roles, and applies specific tags to each message.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/deepseek_v3.md#2025-04-22_snippet_1

LANGUAGE: jinja
CODE:
```
```jinja
{% for message in messages %}
    {% if message['role'] == 'user' %}
        <|user|>{{ message['content'] }}<|end|>
    {% elif message['role'] == 'assistant' %}
        <|assistant|>{{ message['content'] }}<|end|>
    {% endif %}
{% endfor %}
<|assistant|>
```
```

----------------------------------------

TITLE: Prompt Lookup Decoding with Sampling in Transformers
DESCRIPTION: This code snippet demonstrates how to use prompt lookup decoding with sampling for text generation using the Transformers library. It initializes a tokenizer and a causal language model, then generates text using the `generate` method with the `prompt_lookup_num_tokens`, `do_sample`, and `temperature` parameters. The `get_backend()` function automatically detects the appropriate device (CUDA, CPU, etc.).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_optims.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from accelerate.test_utils.testing import get_backend

device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-1.3b")
inputs = tokenizer("The second law of thermodynamics states", return_tensors="pt").to(device)

model = AutoModelForCausalLM.from_pretrained("facebook/opt-1.3b", torch_dtype="auto").to(device)
outputs = model.generate(**inputs, prompt_lookup_num_tokens=3, do_sample=True, temperature=0.7)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
```

----------------------------------------

TITLE: Loading a Model and Tokenizer with Pipeline for Text Generation
DESCRIPTION: This code snippet loads the 'bigcode/octocoder' model along with its tokenizer using the Hugging Face Transformers library. It creates a pipeline for text generation, allowing for efficient inference with model quantization to reduce memory overhead, setting the model dtype to bfloat16.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch

model = AutoModelForCausalLM.from_pretrained("bigcode/octocoder", torch_dtype=torch.bfloat16, device_map="auto", pad_token_id=0)
tokenizer = AutoTokenizer.from_pretrained("bigcode/octocoder")

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
```

----------------------------------------

TITLE: Generating Responses with RAG Models using Transformers in Python
DESCRIPTION: This code snippet shows how to load a RAG model and tokenizer, define a conversation, apply the RAG chat template, and generate a response. It uses the CohereForAI/c4ai-command-r-v01-4bit model and includes document retrieval in the generation process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_extras.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load the model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("CohereForAI/c4ai-command-r-v01-4bit")
model = AutoModelForCausalLM.from_pretrained("CohereForAI/c4ai-command-r-v01-4bit", device_map="auto")
device = model.device # Get the device the model is loaded on

# Define conversation input
conversation = [
    {"role": "user", "content": "What has Man always dreamed of?"}
]

input_ids = tokenizer.apply_chat_template(
    conversation=conversation,
    documents=documents,
    chat_template="rag",
    tokenize=True,
    add_generation_prompt=True,
    return_tensors="pt").to(device)

# Generate a response 
generated_tokens = model.generate(
    input_ids,
    max_new_tokens=100,
    do_sample=True,
    temperature=0.3,
    )

# Decode and print the generated text along with generation prompt
generated_text = tokenizer.decode(generated_tokens[0])
print(generated_text)
```

----------------------------------------

TITLE: Creating a Web Server with Transformers Pipeline in Python
DESCRIPTION: This Python code creates a basic web server using the `starlette` library that utilizes a Transformers pipeline for processing requests. The server sets up an asynchronous queue to handle incoming requests and a background task that runs the pipeline. The pipeline is initialized once to avoid loading the model multiple times.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pipeline_webserver.md#_snippet_0

LANGUAGE: python
CODE:
```
from starlette.applications import Starlette
from starlette.responses import JSONResponse
from starlette.routing import Route
from transformers import pipeline
import asyncio


async def homepage(request):
    payload = await request.body()
    string = payload.decode("utf-8")
    response_q = asyncio.Queue()
    await request.app.model_queue.put((string, response_q))
    output = await response_q.get()
    return JSONResponse(output)


async def server_loop(q):
    pipe = pipeline(model="google-bert/bert-base-uncased")
    while True:
        (string, response_q) = await q.get()
        out = pipe(string)
        await response_q.put(out)


app = Starlette(
    routes=[
        Route("/", homepage, methods=["POST"]),
    ],
)


@app.on_event("startup")
async def startup_event():
    q = asyncio.Queue()
    app.model_queue = q
    asyncio.create_task(server_loop(q))
```

----------------------------------------

TITLE: Using Automatic Device Mapping for Large Models in Python
DESCRIPTION: Creates a pipeline that will automatically distribute the model across available devices.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/hi/pipeline_tutorial.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
transcriber = pipeline(model="openai/whisper-large-v2", device_map="auto")
```

----------------------------------------

TITLE: Converting a model to BetterTransformer
DESCRIPTION: This code converts a pre-trained model to BetterTransformer format using `model.to_bettertransformer()`. This enables fastpath execution of specialized Transformer functions directly on the GPU, improving inference speed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_one.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("bigscience/bloom")
model = model.to_bettertransformer()
```

----------------------------------------

TITLE: Training the Model (PyTorch)
DESCRIPTION: This code initializes the Trainer class with the model, training arguments, train dataset, evaluation dataset, tokenizer, and data collator. The `trainer.train()` method then starts the training process. This combines all the previously defined components and initiates the model training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()
```

----------------------------------------

TITLE: Defining Training Arguments with PyTorch
DESCRIPTION: This code snippet sets up the training parameters necessary for fine-tuning the segmentation model using PyTorch. It includes defining the output directory, learning rate, and other hyper-parameters using the `TrainingArguments` class from Transformers.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/sequence_classification.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
>>> training_args = TrainingArguments(
...     output_dir="segformer-b0-scene-parse-150",
...     learning_rate=6e-5,
...     num_train_epochs=50,
...     per_device_train_batch_size=2,
...     per_device_eval_batch_size=2,
...     save_total_limit=3,
...     eval_strategy="steps",
...     save_strategy="steps",
...     save_steps=20,
...     eval_steps=20,
...     logging_steps=1,
...     eval_accumulation_steps=5,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )
```

----------------------------------------

TITLE: Direct Model Push to Hub
DESCRIPTION: Code showing how to directly push a model to the Hub using the push_to_hub method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_sharing.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
pt_model.push_to_hub("my-awesome-model")

# Loading the pushed model
from transformers import AutoModel
model = AutoModel.from_pretrained("your_username/my-awesome-model")
```

----------------------------------------

TITLE: Creating Trainer Instance in Python
DESCRIPTION: This snippet illustrates the initialization of a Trainer instance, specifying the model, training arguments, datasets, metrics computation, data processing class, and model initialization function.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/hpo_train.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> trainer = Trainer(
...     model=None,
...     args=training_args,
...     train_dataset=small_train_dataset,
...     eval_dataset=small_eval_dataset,
...     compute_metrics=compute_metrics,
...     processing_class=tokenizer,
...     model_init=model_init,
...     data_collator=data_collator,
... )
```

----------------------------------------

TITLE: Loading a Pre-trained Processor with AutoProcessor in Python
DESCRIPTION: This code snippet demonstrates how to load a pre-trained processor using `AutoProcessor.from_pretrained` from the `transformers` library. The processor is loaded from the specified checkpoint name, 'microsoft/layoutlmv2-base-uncased'. Processors are often used for multimodal tasks that require both text and image processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/autoclass_tutorial.md#_snippet_3

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")
```

----------------------------------------

TITLE: Sampling vs Greedy Decoding in LLM Generation
DESCRIPTION: Demonstrates the difference between sampling and greedy decoding approaches in text generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/llm_tutorial.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers import set_seed
set_seed(0)

model_inputs = tokenizer(["I am a cat."], return_tensors="pt").to("cuda")

# LLM + greedy decoding = repetitive, boring output
generated_ids = model.generate(**model_inputs)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

# With sampling, the output becomes more creative!
generated_ids = model.generate(**model_inputs, do_sample=True)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

----------------------------------------

TITLE: Automatic Device Selection for Pipelines in Python
DESCRIPTION: Utilizes the Accelerate library to automate model weight distribution across devices for inference. Demonstrates setting 'device_map' for performance optimization with the Gemma-2 model. Requires 'accelerate' and 'transformers' libraries to be installed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
!pip install -U accelerate

from transformers import pipeline

pipeline = pipeline(task="text-generation", model="google/gemma-2-2b", device_map="auto")
pipeline("the secret to baking a really good cake is ")
```

----------------------------------------

TITLE: Training CLIP Model with CLI Arguments
DESCRIPTION: Command line script to train a CLIP-like model using the Hugging Face Transformers library, specifying model architecture, training parameters, and dataset configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/tensorflow/contrastive-image-text/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
python examples/tensorflow/contrastive-image-text/run_clip.py \
    --output_dir ./clip-roberta-finetuned \
    --vision_model_name_or_path openai/clip-vit-base-patch32 \
    --text_model_name_or_path FacebookAI/roberta-base \
    --data_dir $PWD/data \
    --dataset_name ydshieh/coco_dataset_script \
    --dataset_config_name=2017 \
    --image_column image_path \
    --caption_column caption \
    --remove_unused_columns=False \
    --do_train  --do_eval \
    --per_device_train_batch_size="64" \
    --per_device_eval_batch_size="64" \
    --learning_rate="5e-5" --warmup_steps="0" --weight_decay 0.1 \
    --overwrite_output_dir \
    --push_to_hub
```

----------------------------------------

TITLE: Using BertTokenizer for Subword Tokenization in Python
DESCRIPTION: Shows how to use the BERT tokenizer to split text into subwords, where unknown words like "GPU" are broken into known subword units.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/tokenizer_summary.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> tokenizer.tokenize("I have a new GPU!")
["i", "have", "a", "new", "gp", "##u", "!"]
```

----------------------------------------

TITLE: Using Chat Templates for Better Prompting
DESCRIPTION: Shows how to use chat templates to format prompts correctly for chat-based language models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/llm_tutorial.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-alpha")
model = AutoModelForCausalLM.from_pretrained(
    "HuggingFaceH4/zephyr-7b-alpha", device_map="auto", load_in_4bit=True
)
set_seed(0)
messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a thug",
    },
    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
]
model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to("cuda")
input_length = model_inputs.shape[1]
generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20)
print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])
```

----------------------------------------

TITLE: Instalando la biblioteca Datasets para trabajar con conjuntos de datos
DESCRIPTION: Instala la biblioteca complementaria Datasets de Hugging Face para cargar y procesar datasets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/quicktour.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
pip install datasets
```

----------------------------------------

TITLE: ZeRO-2 configuration with CPU offloading
DESCRIPTION: JSON configuration for DeepSpeed ZeRO stage 2 with CPU offloading enabled, suitable for single GPU deployments to handle larger models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/deepspeed.md#2025-04-22_snippet_10

LANGUAGE: json
CODE:
```
{
  "zero_optimization": {
     "stage": 2,
     "offload_optimizer": {
         "device": "cpu",
         "pin_memory": true
     },
     "allgather_partitions": true,
     "allgather_bucket_size": 2e8,
     "reduce_scatter": true,
     "reduce_bucket_size": 2e8,
     "overlap_comm": true,
     "contiguous_gradients": true
  }
}
```

----------------------------------------

TITLE: Enabling Fused Modules for Supported Architectures
DESCRIPTION: Python code to load an AWQ quantized model with fused modules enabled for supported architectures (like Llama and Mistral). This sets up the AwqConfig with fuse_max_seq_len and do_fuse parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/awq.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
import torch
from transformers import AwqConfig, AutoModelForCausalLM

model_id = "TheBloke/Mistral-7B-OpenOrca-AWQ"

quantization_config = AwqConfig(
    bits=4,
    fuse_max_seq_len=512,
    do_fuse=True,
)

model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config).to(0)
```

----------------------------------------

TITLE: Using Auto Device Mapping for Large Models in Python
DESCRIPTION: Shows how to use automatic device mapping for large models that don't fit on a single GPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/pipeline_tutorial.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
transcriber = pipeline(model="openai/whisper-large-v2", device_map="auto")
```

----------------------------------------

TITLE: Configuring and Executing Training with PyTorch Seq2SeqTrainer
DESCRIPTION: Sets up training arguments, initializes a Seq2SeqTrainer with the model and datasets, and starts the training process. The configuration includes learning rate, batch sizes, evaluation strategy, and push-to-hub settings for sharing the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
training_args = Seq2SeqTrainingArguments(
    output_dir="my_awesome_billsum_model",
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=4,
    predict_with_generate=True,
    fp16=True, #change to bf16=True for XPU
    push_to_hub=True,
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_billsum["train"],
    eval_dataset=tokenized_billsum["test"],
    processing_class=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()
```

----------------------------------------

TITLE: Pushing Tokenizer to Hub in Python
DESCRIPTION: Shows how to add a tokenizer to the model repository on the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/model_sharing.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
>>> tokenizer.push_to_hub("my-awesome-model")
```

----------------------------------------

TITLE: Cargando el Dataset BillSum en Python
DESCRIPTION: Este snippet muestra cÃ³mo cargar el dataset BillSum utilizando la biblioteca Datasets de HuggingFace. Se carga especÃ­ficamente el split 'ca_test' que contiene proyectos de ley del estado de California.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/summarization.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> billsum = load_dataset("billsum", split="ca_test")
```

----------------------------------------

TITLE: Multilingual Translation with MarianMT
DESCRIPTION: Example demonstrating how to use MarianMT for translating text to multiple target languages using language codes. Shows initialization of tokenizer and model, handling multiple source texts, and generating translations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/marian.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import MarianMTModel, MarianTokenizer

src_text = [
    ">>fra<< this is a sentence in english that we want to translate to french",
    ">>por<< This should go to portuguese",
    ">>esp<< And this to Spanish",
]

model_name = "Helsinki-NLP/opus-mt-en-roa"
tokenizer = MarianTokenizer.from_pretrained(model_name)
print(tokenizer.supported_language_codes)

model = MarianMTModel.from_pretrained(model_name)
translated = model.generate(**tokenizer(src_text, return_tensors="pt", padding=True))
[tokenizer.decode(t, skip_special_tokens=True) for t in translated]
```

----------------------------------------

TITLE: Instance Segmentation with Hugging Face Transformers in Python
DESCRIPTION: This snippet demonstrates how to apply instance segmentation using a different pretrained model. The pipeline infers masks for individual instances of objects in the loaded image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
instance_segmentation = pipeline("image-segmentation", "facebook/mask2former-swin-large-cityscapes-instance")
results = instance_segmentation(image)
results
```

----------------------------------------

TITLE: Loading T5 Tokenizer for Translation
DESCRIPTION: This code loads the tokenizer for the T5-small model which will be used to preprocess the English and French text for the translation task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/translation.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> checkpoint = "google-t5/t5-small"
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Implementing Object Detection with ðŸ¤— Transformers Pipeline in Python
DESCRIPTION: This code snippet shows how to perform object detection using the ðŸ¤— Transformers pipeline. It detects objects in an image, providing labels, confidence scores, and bounding box coordinates.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/task_summary.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> detector = pipeline(task="object-detection")
>>> preds = detector(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"], "box": pred["box"]} for pred in preds]
>>> preds
[{'score': 0.9865,
  'label': 'cat',
  'box': {'xmin': 178, 'ymin': 154, 'xmax': 882, 'ymax': 598}}]
```

----------------------------------------

TITLE: Using Fill-Mask Pipeline for Masked Language Modeling in Python
DESCRIPTION: This code shows how to use the Hugging Face pipeline for masked language modeling. It initializes a fill-mask pipeline to predict a masked token in a sentence, and formats the output to display the prediction score, token ID, token string, and complete sequence.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/task_summary.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> text = "Hugging Face is a community-based open-source <mask> for machine learning."
>>> fill_mask = pipeline(task="fill-mask")
>>> preds = fill_mask(text, top_k=1)
>>> preds = [
...     {
...         "score": round(pred["score"], 4),
...         "token": pred["token"],
...         "token_str": pred["token_str"],
...         "sequence": pred["sequence"],
...     }
...     for pred in preds
... ]
>>> preds
[{'score': 0.224, 'token': 3944, 'token_str': ' tool', 'sequence': 'Hugging Face is a community-based open-source tool for machine learning.'}]
```

----------------------------------------

TITLE: Initializing Chat History in Python
DESCRIPTION: This snippet initializes a chat history as a list of dictionaries. Each dictionary represents a message with a 'role' (either 'system' or 'user') and 'content'.  The 'system' message provides instructions to the model, while the 'user' message represents a user's input. This chat history is then passed to the chat model for generating a response.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/conversations.md#_snippet_0

LANGUAGE: python
CODE:
```
chat = [
    {"role": "system", "content": "You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986."},
    {"role": "user", "content": "Hey, can you tell me any fun things to do in New York?"}
]
```

----------------------------------------

TITLE: Data Collator with Padding
DESCRIPTION: This snippet creates a `DataCollatorWithPadding` object.  This data collator is used to pad the input sequences to the same length within a batch, which is necessary for efficient training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_38

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorWithPadding

>>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

----------------------------------------

TITLE: Creating and launching a Gradio interface from a Transformers pipeline
DESCRIPTION: This code snippet demonstrates how to create a pipeline for image classification using the `transformers` library and then create a Gradio interface from the pipeline.  The `gr.Interface.from_pipeline(pipeline).launch()` line creates a web server and starts up the app, allowing users to interact with the image classification model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_gradio.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import pipeline
import gradio as gr

pipeline = pipeline("image-classification", model="google/vit-base-patch16-224")
gr.Interface.from_pipeline(pipeline).launch()
```

----------------------------------------

TITLE: Setting Device for Pipeline in Python
DESCRIPTION: Demonstrates how to specify a GPU device for the pipeline to use.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/pipeline_tutorial.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
transcriber = pipeline(model="openai/whisper-large-v2", device=0)
```

----------------------------------------

TITLE: Saving BertModel with TorchScript
DESCRIPTION: This code snippet demonstrates how to export a BertModel to TorchScript. It involves tokenizing input text, creating dummy inputs, initializing the model with the `torchscript` flag, and tracing the model using `torch.jit.trace`. The traced model is then saved to disk.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/torchscript.md#_snippet_0

LANGUAGE: python
CODE:
```
from transformers import BertModel, BertTokenizer, BertConfig
import torch

enc = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")

# Tokenizing input text
text = "[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]"
tokenized_text = enc.tokenize(text)

# Masking one of the input tokens
masked_index = 8
tokenized_text[masked_index] = "[MASK]"
indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

# Creating a dummy input
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

# Initializing the model with the torchscript flag
# Flag set to True even though it is not necessary as this model does not have an LM Head.
config = BertConfig(
    vocab_size_or_config_json_file=32000,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072,
    torchscript=True,
)

# Instantiating the model
model = BertModel(config)

# The model needs to be in evaluation mode
model.eval()

# If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag
model = BertModel.from_pretrained("google-bert/bert-base-uncased", torchscript=True)

# Creating the trace
traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])
torch.jit.save(traced_model, "traced_bert.pt")
```

----------------------------------------

TITLE: Implementing Asynchronous Web Server for Transformers Inference in Python
DESCRIPTION: This code snippet sets up a Starlette web server that handles POST requests for machine learning inference using a Hugging Face pipeline. It demonstrates asynchronous request handling and a separate inference loop to optimize resource usage.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/pipeline_webserver.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from starlette.applications import Starlette
from starlette.responses import JSONResponse
from starlette.routing import Route
from transformers import pipeline
import asyncio


async def homepage(request):
    payload = await request.body()
    string = payload.decode("utf-8")
    response_q = asyncio.Queue()
    await request.app.model_queue.put((string, response_q))
    output = await response_q.get()
    return JSONResponse(output)


async def server_loop(q):
    pipe = pipeline(model="google-bert/bert-base-uncased")
    while True:
        (string, response_q) = await q.get()
        out = pipe(string)
        await response_q.put(out)


app = Starlette(
    routes=[
        Route("/", homepage, methods=["POST"]),
    ],
)


@app.on_event("startup")
async def startup_event():
    q = asyncio.Queue()
    app.model_queue = q
    asyncio.create_task(server_loop(q))
```

----------------------------------------

TITLE: Loading M2M100 Model for Multilingual Translation
DESCRIPTION: This code loads the M2M100 model and tokenizer for multilingual translation, specifically for translating from Chinese to English.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/multilingual.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
chinese_text = "ä¸è¦æ’æ‰‹å·«å¸«çš„äº‹å‹™, å› ç‚ºä»–å€‘æ˜¯å¾®å¦™çš„, å¾ˆå¿«å°±æœƒç™¼æ€’."

tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M", src_lang="zh")
model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")
```

----------------------------------------

TITLE: Configuring DeepSpeed with Accelerate
DESCRIPTION: This YAML configuration file sets up DeepSpeed integration with Accelerate using a configuration file. It specifies the path to the DeepSpeed configuration file and enables the zero3 initialization flag.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/trainer.md#2025-04-22_snippet_10

LANGUAGE: yaml
CODE:
```
compute_environment: LOCAL_MACHINE
deepspeed_config:
  deepspeed_config_file: /home/user/configs/ds_zero3_config.json
  zero3_init_flag: true
distributed_type: DEEPSPEED
downcast_bf16: 'no'
machine_rank: 0
main_training_function: main
num_machines: 1
num_processes: 4
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

----------------------------------------

TITLE: Enabling Tensor Parallelism in Transformers (Python)
DESCRIPTION: This Python snippet demonstrates how to enable tensor parallelism for a model using the Hugging Face Transformers library. It initializes the model and tokenizer, prepares input tokens, and performs a distributed inference run on a specified device. Dependencies include the Transformers library and PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_multi.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer


# enable tensor parallelism
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Meta-Llama-3-8B-Instruct",
    tp_plan="auto",
)

# prepare input tokens
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct")
prompt = "Can I help"
inputs = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)

# distributed run
outputs = model(inputs)
```

----------------------------------------

TITLE: Multinomial Sampling with Transformers in Python
DESCRIPTION: Presents a multinomial sampling strategy for generating more diverse outputs in text generation using Transformers. It requires a pretrained model and tokenizer. Enables random sampling by setting `do_sample=True` in the generation method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial.md#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
generated_ids = model.generate(**model_inputs, do_sample=True)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

----------------------------------------

TITLE: Configuring ZeRO-3 and ZeRO-Infinity with NVMe Offloading in DeepSpeed
DESCRIPTION: A comprehensive configuration example for ZeRO-3 with NVMe offloading, showing settings for optimizer and parameter offloading to NVMe storage. Includes automatic configuration for fp16 precision, optimizer parameters, scheduler, and various ZeRO optimization parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/deepspeed.md#2025-04-22_snippet_11

LANGUAGE: yaml
CODE:
```
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": "auto",
            "betas": "auto",
            "eps": "auto",
            "weight_decay": "auto"
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },

    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "nvme",
            "nvme_path": "/local_nvme",
            "pin_memory": true,
            "buffer_count": 4,
            "fast_init": false
        },
        "offload_param": {
            "device": "nvme",
            "nvme_path": "/local_nvme",
            "pin_memory": true,
            "buffer_count": 5,
            "buffer_size": 1e8,
            "max_in_cpu": 1e9
        },
        "aio": {
            "block_size": 262144,
            "queue_depth": 32,
            "thread_count": 1,
            "single_submit": false,
            "overlap_events": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    },

    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 2000,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false
}
```

----------------------------------------

TITLE: Configuring Training Arguments with Evaluation Strategy
DESCRIPTION: Updates the training arguments to specify an evaluation strategy that calculates metrics after each epoch during the fine-tuning process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/training.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(output_dir="test_trainer", eval_strategy="epoch")
```

----------------------------------------

TITLE: Automatic Speech Recognition with Transformers Pipeline
DESCRIPTION: This code snippet shows how to use the Transformers pipeline for automatic speech recognition. It transcribes an audio file to text using a pre-trained Whisper model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/task_summary.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
>>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition", model="openai/whisper-small")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

----------------------------------------

TITLE: Criando pipeline para reconhecimento de fala automÃ¡tico em Python
DESCRIPTION: Demonstra como criar um pipeline para reconhecimento de fala automÃ¡tico usando um modelo especÃ­fico.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/quicktour.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> import torch
>>> from transformers import pipeline

>>> speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")
```

----------------------------------------

TITLE: Applying Image Augmentation and Preprocessing in Python
DESCRIPTION: This function applies the defined transforms to images, converts them to RGB, and then uses the image processor to convert them to pixel values. It's designed to be used with the dataset's set_transform method for on-the-fly preprocessing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/image_processors.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
def transforms(examples):
    images = [_transforms(img.convert("RGB")) for img in examples["image"]]
    examples["pixel_values"] = image_processor(images, do_resize=False, return_tensors="pt")["pixel_values"]
    return examples
```

----------------------------------------

TITLE: Using Text Pipeline for Zero-Shot Classification
DESCRIPTION: This example demonstrates how to use a zero-shot classification model to classify text with custom labels, allowing flexible categorization without task-specific training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pipeline_tutorial.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> # This model is a `zero-shot-classification` model.
>>> # It will classify text, except you are free to choose any label you might imagine
>>> classifier = pipeline(model="facebook/bart-large-mnli")
>>> classifier(
...     "I have a problem with my iphone that needs to be resolved asap!!",
...     candidate_labels=["urgent", "not urgent", "phone", "tablet", "computer"],
... )
{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!', 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'], 'scores': [0.504, 0.479, 0.013, 0.003, 0.002]}
```

----------------------------------------

TITLE: Saving Quantized Model to Hugging Face Hub
DESCRIPTION: Pushing the quantized model and its tokenizer to the Hugging Face Hub for easy sharing and access.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/gptq.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
quantized_model.push_to_hub("opt-125m-gptq")
tokenizer.push_to_hub("opt-125m-gptq")
```

----------------------------------------

TITLE: Configuring CPU Offloading for 8-bit Models
DESCRIPTION: Example demonstrating how to configure CPU offloading for 8-bit models to handle very large models that don't fit in GPU memory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/bitsandbytes.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)
```

----------------------------------------

TITLE: Setting Up Callbacks for T5 Model Training in TensorFlow
DESCRIPTION: Configures callbacks for metric computation and pushing the model to the Hugging Face Hub during TensorFlow training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/translation.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback

metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_test_set)

push_to_hub_callback = PushToHubCallback(
    output_dir="my_awesome_opus_books_model",
    tokenizer=tokenizer,
)

callbacks = [metric_callback, push_to_hub_callback]
```

----------------------------------------

TITLE: Loading a Model with GPTQConfig
DESCRIPTION: This code loads a pre-trained causal language model and applies GPTQ quantization during loading. It uses `device_map='auto'` to automatically offload the model to CPU to save memory. Requires `transformers` and a properly configured `gptq_config` object.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/gptq.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
"quantized_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", device_map=\"auto\", quantization_config=gptq_config)"
```

----------------------------------------

TITLE: Tokenizing a Single Sentence with BERT Tokenizer
DESCRIPTION: Shows how to tokenize a single sentence using a BERT tokenizer, which returns a dictionary containing input_ids, token_type_ids, and attention_mask. The tokenizer automatically adds special tokens like [CLS] and [SEP].
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/preprocessing.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
>>> print(encoded_input)
{'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

----------------------------------------

TITLE: Configuring Mixed Precision Training with fp16 - Python
DESCRIPTION: This snippet demonstrates how to enable mixed precision training using the fp16 data type in the `TrainingArguments`. Mixed precision training speeds up the training process by using lower precision for certain calculations while keeping others in full precision to maintain model accuracy.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
from transformers import TrainingArguments

args = TrainingArguments(
    per_device_train_batch_size=4,
    gradient_accumulation_steps=16,
    gradient_checkpointing=True,
    fp16=True,
)
```

----------------------------------------

TITLE: Initializing Audio Classification Pipeline
DESCRIPTION: Setup of an audio classification pipeline for emotion recognition from speech
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/pipeline_tutorial.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import pipeline

audio_classifier = pipeline(
    task="audio-classification", model="ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
)
```

----------------------------------------

TITLE: Pushing a Trained Model to Hugging Face Hub with PyTorch
DESCRIPTION: After training completes, this code pushes the model to the Hugging Face Hub to make it available for others to use. This requires being logged in to Hugging Face.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
>>> trainer.push_to_hub()
```

----------------------------------------

TITLE: Running GLUE Task Fine-tuning with Transformers
DESCRIPTION: This bash script demonstrates how to fine-tune a pre-trained BERT model on a GLUE task using the Transformers library. It sets up the environment variables and runs the 'run_glue.py' script with specified parameters for model training and evaluation.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
export TASK_NAME=mrpc

python run_glue.py \
  --model_name_or_path google-bert/bert-base-cased \
  --task_name $TASK_NAME \
  --do_train \
  --do_eval \
  --max_seq_length 128 \
  --per_device_train_batch_size 32 \
  --learning_rate 2e-5 \
  --num_train_epochs 3 \
  --output_dir /tmp/$TASK_NAME/
```

----------------------------------------

TITLE: Performing Super-Resolution using Pipeline
DESCRIPTION: Uses the previously initialized pipeline to upscale the loaded image and prints the new size.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/image_to_image.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
upscaled = pipe(image)
print(upscaled.size)
```

----------------------------------------

TITLE: Structuring Messages for Multiple Images (Python)
DESCRIPTION: Provides an example of how to structure the `messages` list for the `apply_chat_template` method when the user input includes multiple images before a text prompt. It demonstrates adding multiple entries with `"type": "image"` and a `"url"` within a single user `"content"` array.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gemma3.md#_snippet_5

LANGUAGE: python
CODE:
```
url_cow = "https://media.istockphoto.com/id/1192867753/photo/cow-in-berchida-beach-siniscola.jpg?s=612x612&w=0&k=20&c=v0hjjniwsMNfJSuKWZuIn8pssmD5h5bSN1peBd1CmH4="
url_cat = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"

messages =[
    {
        "role": "system",
        "content": [
            {"type": "text", "text": "You are a helpful assistant."}
        ]
    },
    {
        "role": "user",
        "content": [
            {"type": "image", "url": url_cow},
            {"type": "image", "url": url_cat},
            {"type": "text", "text": "Which image is cuter?"},
        ]
    },
]
```

----------------------------------------

TITLE: Text Classification Pipeline
DESCRIPTION: Example showing how to use pipeline for zero-shot text classification
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/pipeline_tutorial.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import pipeline

classifier = pipeline(model="facebook/bart-large-mnli")
classifier(
    "I have a problem with my iphone that needs to be resolved asap!!",
    candidate_labels=["urgent", "not urgent", "phone", "tablet", "computer"],
)
```

----------------------------------------

TITLE: Loading and Quantizing a Model with GPTQ
DESCRIPTION: Loading a pre-trained model and applying GPTQ quantization using the configured GPTQConfig.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/gptq.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", quantization_config=gptq_config)
```

----------------------------------------

TITLE: Dataset Processing with Pipeline
DESCRIPTION: Example showing how to process a dataset using pipeline with iteration
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/pipeline_tutorial.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers.pipelines.pt_utils import KeyDataset
from datasets import load_dataset

pipe = pipeline(model="hf-internal-testing/tiny-random-wav2vec2", device=0)
dataset = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation[:10]")

for out in pipe(KeyDataset(dataset, "audio")):
    print(out)
```

----------------------------------------

TITLE: Applying Tokenization to the Entire IMDb Dataset
DESCRIPTION: Code to process the entire dataset by applying the tokenization function to all examples in batches, which is more efficient than processing one example at a time.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/sequence_classification.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
tokenized_imdb = imdb.map(preprocess_function, batched=True)
```

----------------------------------------

TITLE: Applying Preprocessing to the Entire Dataset
DESCRIPTION: Applies the preprocessing function to the entire dataset using the map method with batched processing for efficiency. The original columns are removed to retain only the preprocessed features.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/question_answering.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad["train"].column_names)
```

----------------------------------------

TITLE: Mapping Dataset with Preparation Function
DESCRIPTION: This snippet applies the preprocessing function across the entire dataset while removing unnecessary columns to create an encoded dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/asr.md#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
>>> encoded_minds = minds.map(prepare_dataset, remove_columns=minds.column_names["train"], num_proc=4)
```

----------------------------------------

TITLE: Inference with Token Classification Model using Transformers Pipeline
DESCRIPTION: Demonstrates how to use a trained token classification model for inference using the Transformers pipeline API.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/token_classification.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
>>> text = "The Golden State Warriors are an American professional basketball team based in San Francisco."

>>> from transformers import pipeline

>>> classifier = pipeline("ner", model="stevhliu/my_awesome_wnut_model")
>>> classifier(text)
```

----------------------------------------

TITLE: Logging in to Hugging Face Hub in Python
DESCRIPTION: Python code to log in to the Hugging Face Hub using a notebook login function, allowing users to upload and share their models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/asr.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Basic Training with Trainer
DESCRIPTION: Executes a basic training loop using the `Trainer` class from the `transformers` library. It initializes the `TrainingArguments` with a batch size of 4 and then trains the model on the dummy dataset. It also prints the training summary including time, samples per second, and GPU memory utilization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/model_memory_anatomy.md#_snippet_8

LANGUAGE: python
CODE:
```
>>> from transformers import TrainingArgumentsØŒ TrainerØŒ logging

>>> logging.set_verbosity_error()


>>> training_args = TrainingArguments(per_device_train_batch_size=4ØŒ **default_args)
>>> trainer = Trainer(model=modelØŒ args=training_argsØŒ train_dataset=ds)
>>> result = trainer.train()
>>> print_summary(result)
```

----------------------------------------

TITLE: Creating a Data Collator for Language Modeling
DESCRIPTION: Sets up a DataCollatorForLanguageModeling to dynamically pad sequences and randomly mask tokens during training. This configuration ensures efficient batching with the right padding and masking probabilities.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```

----------------------------------------

TITLE: Fine-tuning Speech-Encoder-Decoder Model on Multiple GPUs
DESCRIPTION: Bash command to run the speech recognition fine-tuning script on 8 GPUs using torchrun and the Librispeech dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-recognition/README.md#2025-04-22_snippet_14

LANGUAGE: bash
CODE:
```
torchrun \
 	--nproc_per_node 8 run_speech_recognition_seq2seq.py \
	--dataset_name="librispeech_asr" \
	--model_name_or_path="./" \
	--dataset_config_name="clean" \
	--train_split_name="train.100" \
	--eval_split_name="validation" \
	--output_dir="./" \
	--preprocessing_num_workers="16" \
	--length_column_name="input_length" \
	--overwrite_output_dir \
	--num_train_epochs="5" \
	--per_device_train_batch_size="8" \
	--per_device_eval_batch_size="8" \
	--gradient_accumulation_steps="1" \
	--learning_rate="3e-4" \
	--warmup_steps="400" \
	--eval_strategy="steps" \
	--text_column_name="text" \
	--save_steps="400" \
	--eval_steps="400" \
	--logging_steps="10" \
	--save_total_limit="1" \
	--freeze_feature_encoder \
	--gradient_checkpointing \
	--fp16 \
	--group_by_length \
	--predict_with_generate \
	--do_train --do_eval \
	--do_lower_case
```

----------------------------------------

TITLE: Tokenizing with Right Padding in Python
DESCRIPTION: This snippet shows how to tokenize sequences with right padding using Transformers, ensuring uniform length inputs for the model. Demonstrates tokenization, model input preparation, and post-processing generated output. Requires a pretrained tokenizer and model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial.md#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
model_inputs = tokenizer(
    ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
).to("cuda")
generated_ids = model.generate(**model_inputs)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'1, 2, 33333333333'
```

----------------------------------------

TITLE: Preparing Input for Zephyr Model - Python
DESCRIPTION: This code snippet showcases how to prepare chat input for a specific language model (Zephyr) by using the tokenizer's apply_chat_template method, ensuring the correct format for exchange between user and assistant roles.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/chat_templating.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

checkpoint = "HuggingFaceH4/zephyr-7b-beta"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint)   # ì—¬ê¸°ì„œ bfloat16 ì‚¬ìš© ë°/ë˜ëŠ” GPUë¡œ ì´ë™í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.


messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a pirate",
    },
    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
 ]
tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")
print(tokenizer.decode(tokenized_chat[0]))
```

----------------------------------------

TITLE: Creando un pipeline personalizado con modelo y tokenizador especÃ­ficos
DESCRIPTION: Configura un pipeline de anÃ¡lisis de sentimiento con un modelo y tokenizador previamente cargados y lo prueba con texto en francÃ©s.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/quicktour.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ðŸ¤— Transformers.")
[{'label': '5 stars', 'score': 0.7273}]
```

----------------------------------------

TITLE: Using SigLIP with AutoModel for Zero-Shot Image Classification
DESCRIPTION: Shows how to implement SigLIP manually using AutoModel and AutoProcessor. This approach provides more flexibility by manually processing texts and images, calculating similarity scores, and converting them to probabilities with a sigmoid function.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/siglip.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
import requests
from PIL import Image
from transformers import AutoProcessor, AutoModel

model = AutoModel.from_pretrained("google/siglip-base-patch16-224", torch_dtype=torch.float16, device_map="auto", attn_implementation="sdpa")
processor = AutoProcessor.from_pretrained("google/siglip-base-patch16-224")

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
image = Image.open(requests.get(url, stream=True).raw)
candidate_labels = ["a Pallas cat", "a lion", "a Siberian tiger"]
texts = [f'This is a photo of {label}.' for label in candidate_labels]
inputs = processor(text=texts, images=image, padding="max_length", return_tensors="pt").to("cuda")

with torch.no_grad():
    outputs = model(**inputs)

logits_per_image = outputs.logits_per_image
probs = torch.sigmoid(logits_per_image)
print(f"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'")
```

----------------------------------------

TITLE: Loading ERNIE Model and Tokenizer in Python
DESCRIPTION: This snippet demonstrates how to load a pre-trained ERNIE model and its corresponding tokenizer using the `AutoTokenizer` and `AutoModel` classes from the `transformers` library. It loads the `ernie-1.0-base-zh` model. The tokenizer is used to preprocess text for the model, and the model performs the specified task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/ernie.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("nghuyong/ernie-1.0-base-zh")
model = AutoModel.from_pretrained("nghuyong/ernie-1.0-base-zh")
```

----------------------------------------

TITLE: Using SigLIP with Flash Attention 2
DESCRIPTION: Demonstrates how to load and use the SigLIP model with Flash Attention 2 for improved performance. It includes setup instructions and code for inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/siglip.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
import torch
import requests
from PIL import Image
from transformers import SiglipProcessor, SiglipModel
device = "cuda"

model = SiglipModel.from_pretrained(
    "google/siglip-so400m-patch14-384",
    attn_implementation="flash_attention_2",
    torch_dtype=torch.float16,
    device_map=device,
)
processor = SiglipProcessor.from_pretrained("google/siglip-so400m-patch14-384")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

candidate_labels = ["2 cats", "2 dogs"]
texts = [f'This is a photo of {label}.' for label in candidate_labels]
inputs = processor(text=texts, images=image, padding="max_length", return_tensors="pt").to(device)

with torch.no_grad():
    with torch.autocast(device):
        outputs = model(**inputs)

logits_per_image = outputs.logits_per_image
probs = torch.sigmoid(logits_per_image)
print(f"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'")
```

----------------------------------------

TITLE: Verifying Transformers Installation
DESCRIPTION: Python command to verify successful installation by running a sentiment analysis pipeline
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/installation.md#2025-04-22_snippet_7

LANGUAGE: bash
CODE:
```
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
```

----------------------------------------

TITLE: Calculating Memory Requirement for Key-Value Cache - Python
DESCRIPTION: This snippet computes the total number of float values needed for the key-value cache of a transformer model, which depends on various model configurations. The formula incorporates factors like sequence length, number of attention heads, and dimensionality of the embeddings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_30

LANGUAGE: python
CODE:
```
config = model.config
2 * 16_000 * config.n_layer * config.n_head * config.n_embd // config.n_head
```

----------------------------------------

TITLE: Estimating Depth in Images with Transformers Pipeline in Python
DESCRIPTION: This snippet demonstrates how to use the Hugging Face pipeline for depth estimation from a single image. It loads a depth estimation model that predicts the distance of each pixel in the image from the camera, creating a depth map of the scene.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/task_summary.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> depth_estimator = pipeline(task="depth-estimation")
>>> preds = depth_estimator(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
```

----------------------------------------

TITLE: Directly Loading Sharded Checkpoints - Transformers - Python
DESCRIPTION: This code snippet demonstrates how to directly load model checkpoints from sharded files using the `load_sharded_checkpoint` function within the `transformers` library. This method allows for efficiently using sharded data checkpoints for large models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/models.md#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
from transformers.modeling_utils import load_sharded_checkpoint

with tempfile.TemporaryDirectory() as tmp_dir:
    model.save_pretrained(tmp_dir, max_shard_size="5GB")
    load_sharded_checkpoint(model, tmp_dir)
```

----------------------------------------

TITLE: Verifying Transformers Installation - Python Test
DESCRIPTION: Python command to verify successful installation by running a sentiment analysis pipeline
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/installation.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
```

----------------------------------------

TITLE: Running Inference and Decoding Predictions
DESCRIPTION: Executes the manual inference function and decodes the model's logits to get the predicted class
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/video_classification.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> logits = run_inference(trained_model, sample_test_video["video"])
>>> predicted_class_idx = logits.argmax(-1).item()
>>> print("Predicted class:", model.config.id2label[predicted_class_idx])
# Predicted class: BasketballDunk
```

----------------------------------------

TITLE: Loading VoxPopuli Dataset for Dutch TTS
DESCRIPTION: Loads the Dutch subset of the VoxPopuli dataset for text-to-speech training and ensures the audio sampling rate is 16kHz.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/text-to-speech.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
from datasets import load_dataset, Audio

dataset = load_dataset("facebook/voxpopuli", "nl", split="train")
len(dataset)

dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
```

----------------------------------------

TITLE: Converting to TensorFlow Tensors
DESCRIPTION: Tokenizes, pads, truncates, and converts a batch of sentences to TensorFlow tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/preprocessing.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="tf")
print(encoded_input)
```

----------------------------------------

TITLE: Pushing Quantized Model to Hub
DESCRIPTION: Shows how to save and push a quantized model to the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/quantization.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
quantized_model.push_to_hub("opt-125m-gptq")
tokenizer.push_to_hub("opt-125m-gptq")
```

----------------------------------------

TITLE: Setting Up the Trainer
DESCRIPTION: Initializes a Trainer with the model, training arguments, data collator, processed dataset, and processor for fine-tuning the ViLT model on the VQA task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_question_answering.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
>>> from transformers import Trainer

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     data_collator=data_collator,
...     train_dataset=processed_dataset,
...     processing_class=processor,
... )
```

----------------------------------------

TITLE: Loading TensorFlow Model and Tokenizer for Multilingual Sentiment Analysis
DESCRIPTION: This snippet shows how to load a pre-trained TensorFlow model and tokenizer for multilingual sentiment analysis using TFAutoModelForSequenceClassification and AutoTokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/quicktour.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

----------------------------------------

TITLE: Loading DistilGPT2 Model for Causal Language Modeling in TensorFlow
DESCRIPTION: This code loads the DistilGPT2 model using TFAutoModelForCausalLM for causal language modeling tasks in TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
from transformers import TFAutoModelForCausalLM

model = TFAutoModelForCausalLM.from_pretrained("distilbert/distilgpt2")
```

----------------------------------------

TITLE: Code Diff Showing Accelerate Integration into Training Loop
DESCRIPTION: Diff showing the exact changes needed to convert a standard PyTorch training loop to use ðŸ¤— Accelerate for distributed training, with minimal modifications.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/accelerate.md#2025-04-22_snippet_4

LANGUAGE: diff
CODE:
```
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

----------------------------------------

TITLE: Manual Implementation with Swin2SR Model and Processor
DESCRIPTION: Initializes the Swin2SR model and image processor directly for manual super-resolution processing without using the pipeline abstraction.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_to_image.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers import Swin2SRForImageSuperResolution, Swin2SRImageProcessor 

model = Swin2SRForImageSuperResolution.from_pretrained("caidas/swin2SR-lightweight-x2-64").to(device)
processor = Swin2SRImageProcessor("caidas/swin2SR-lightweight-x2-64")
```

----------------------------------------

TITLE: Data Collator for Language Modeling (PyTorch)
DESCRIPTION: This code creates a `DataCollatorForLanguageModeling` object for PyTorch. It sets the tokenizer's pad token to the end-of-sequence token and initializes the data collator with the tokenizer and a masking probability of 0.15. This collator dynamically pads and masks the input sequences for masked language modeling during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_10

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorForLanguageModeling

>>> tokenizer.pad_token = tokenizer.eos_token
>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```

----------------------------------------

TITLE: Setting Up Callbacks for Training in TensorFlow
DESCRIPTION: This snippet sets up Keras callbacks for monitoring metrics and pushing the model to the Hugging Face Hub during training. The KerasMetricCallback and PushToHubCallback are utilized here.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/sequence_classification.md#2025-04-22_snippet_22

LANGUAGE: python
CODE:
```
>>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback

>>> metric_callback = KerasMetricCallback(
...     metric_fn=compute_metrics, eval_dataset=tf_eval_dataset, batch_size=batch_size, label_cols=["labels"]
... )

>>> push_to_hub_callback = PushToHubCallback(output_dir="scene_segmentation", image_processor=image_processor)

>>> callbacks = [metric_callback, push_to_hub_callback]
```

----------------------------------------

TITLE: Pipeline with Specific Model
DESCRIPTION: Example showing how to use a pipeline with a specific model (Whisper) and process multiple inputs
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/pipeline_tutorial.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
generator = pipeline(model="openai/whisper-large")
generator([
    "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac",
    "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac",
])
```

----------------------------------------

TITLE: Tokenizing String with Hugging Face Tokenizer
DESCRIPTION: Demonstrates how to load the newly implemented Hugging Face tokenizer for the model from a local folder and use it to tokenize an input string. The resulting `input_ids` are compared to those obtained from the original repository's tokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/add_new_model.md#_snippet_8

LANGUAGE: python
CODE:
```
from transformers import BrandNewBertTokenizer

input_str = "This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words."

tokenizer = BrandNewBertTokenizer.from_pretrained("/path/to/tokenizer/folder/")

input_ids = tokenizer(input_str).input_ids
```

----------------------------------------

TITLE: Applying Preprocessing to SQuAD Dataset
DESCRIPTION: Uses the map function to apply the preprocessing function to the entire SQuAD dataset, removing unnecessary columns.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/question_answering.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad["train"].column_names)
```

----------------------------------------

TITLE: Using ImageTextToTextPipeline for Multimodal Chat Processing
DESCRIPTION: This snippet shows how to initialize and use the ImageTextToTextPipeline from Transformers to process chat inputs containing both text and images. It explains setting parameters such as device and data type for efficient processing and demonstrates executing the pipeline for generating responses from chat inputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating_multimodal.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipeline = pipeline("image-text-to-text", model="llava-hf/llava-onevision-qwen2-0.5b-ov-hf", device="cuda", torch_dtype=torch.float16)
pipeline(text=messages, max_new_tokens=50, return_full_text=False)
[{'input_text': [{'role': 'system',
    'content': [{'type': 'text',
      'text': 'You are a friendly chatbot who always responds in the style of a pirate'}]},
   {'role': 'user',
    'content': [{'type': 'image',
      'url': 'http://images.cocodataset.org/val2017/000000039769.jpg'},
     {'type': 'text', 'text': 'What are these?'}]}],
  'generated_text': 'The image shows two cats lying on a pink surface, which appears to be a cushion or a soft blanket. The cat on the left has a striped coat, typical of tabby cats, and is lying on its side with its head resting on the'}]
```

----------------------------------------

TITLE: Implementing a Metrics Computation Function for Evaluation
DESCRIPTION: Definition of a function that computes accuracy by comparing model predictions to ground truth labels, converting logits to class predictions by taking the argmax.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/sequence_classification.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> import numpy as np


>>> def compute_metrics(eval_pred):
...     predictions, labels = eval_pred
...     predictions = np.argmax(predictions, axis=1)
...     return accuracy.compute(predictions=predictions, references=labels)
```

----------------------------------------

TITLE: Using RT-DETRv2 for Object Detection with Transformers
DESCRIPTION: This example demonstrates how to use RT-DETRv2 for object detection on an image. It shows loading the model and processor, processing an image, running inference, and interpreting the results with bounding boxes and class labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/rt_detr_v2.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> import torch
>>> import requests

>>> from PIL import Image
>>> from transformers import RTDetrV2ForObjectDetection, RTDetrImageProcessor

>>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = RTDetrImageProcessor.from_pretrained("PekingU/rtdetr_v2_r18vd")
>>> model = RTDetrV2ForObjectDetection.from_pretrained("PekingU/rtdetr_v2_r18vd")

>>> inputs = image_processor(images=image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> results = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.5)

>>> for result in results:
...     for score, label_id, box in zip(result["scores"], result["labels"], result["boxes"]):
...         score, label = score.item(), label_id.item()
...         box = [round(i, 2) for i in box.tolist()]
...         print(f"{model.config.id2label[label]}: {score:.2f} {box}")
cat: 0.97 [341.14, 25.11, 639.98, 372.89]
cat: 0.96 [12.78, 56.35, 317.67, 471.34]
remote: 0.95 [39.96, 73.12, 175.65, 117.44]
sofa: 0.86 [-0.11, 2.97, 639.89, 473.62]
sofa: 0.82 [-0.12, 1.78, 639.87, 473.52]
remote: 0.79 [333.65, 76.38, 370.69, 187.48]
```

----------------------------------------

TITLE: Running Inference and Post-processing Zero-shot Classification Results
DESCRIPTION: Code to perform model inference and process the output logits into human-readable classification results with confidence scores.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/zero_shot_image_classification.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> import torch

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> logits = outputs.logits_per_image[0]
>>> probs = logits.softmax(dim=-1).numpy()
>>> scores = probs.tolist()

>>> result = [
...     {"score": score, "label": candidate_label}
...     for score, candidate_label in sorted(zip(probs, candidate_labels), key=lambda x: -x[0])
... ]

>>> result
[{'score': 0.998572, 'label': 'car'},
 {'score': 0.0010570387, 'label': 'bike'},
 {'score': 0.0003393686, 'label': 'tree'},
 {'score': 3.1572064e-05, 'label': 'cat'}]
```

----------------------------------------

TITLE: Applying Chat Template with Mistral Model - Python
DESCRIPTION: This snippet illustrates the application of the chat template using the Mistral model's tokenizer, highlighting how to format user messages with special control tokens that the model was trained on.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/chat_templating.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")
>>> chat = [
...   {"role": "user", "content": "Hello, how are you?"},
...   {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
...   {"role": "user", "content": "I'd like to show off how chat templating works!"},
... ]
>>> tokenizer.apply_chat_template(chat, tokenize=False)
"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]"
```

----------------------------------------

TITLE: Loading Pretrained DistilBERT Model in PyTorch
DESCRIPTION: This snippet shows how to load a pretrained DistilBERT model in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/create_a_model.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
model = DistilBertModel.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Loading Model (TensorFlow)
DESCRIPTION: This snippet demonstrates how to reload a TensorFlow model using `TFAutoModelForSequenceClassification.from_pretrained`. It expects that the model and tokenizer have been saved to the specified directory. The directory path is provided as a string.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_27

LANGUAGE: python
CODE:
```
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")
```

----------------------------------------

TITLE: Running Base Model with Transformers in Python
DESCRIPTION: The snippet demonstrates how to load and utilize the base Mixtral-8x7B model using the Hugging Face Transformers library. The model predicts the next token when given a text prompt, using the `AutoModelForCausalLM` class. It requires a CUDA-enabled device for execution.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mixtral.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mixtral-8x7B-v0.1", device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mixtral-8x7B-v0.1")

>>> prompt = "My favourite condiment is"

>>> model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
>>> model.to(device)

>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"My favourite condiment is to ..."
```

----------------------------------------

TITLE: Initializing Image-Text Pipeline with Transformers (Python)
DESCRIPTION: Demonstrates how to load and use the `transformers` `pipeline` for multimodal tasks with Qwen2.5-VL. It initializes the pipeline with the model path, device, and data type, then processes a user message containing an image URL and text prompt to generate text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2_5_vl.md#_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline
pipe = pipeline(
    task="image-text-to-text",
    model="Qwen/Qwen2.5-VL-7B-Instruct",
    device=0,
    torch_dtype=torch.bfloat16
)
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "url": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg",
            },
            { "type": "text", "text": "Describe this image."},
        ]
    }
]
pipe(text=messages,max_new_tokens=20, return_full_text=False)
```

----------------------------------------

TITLE: Loading BillSum Dataset from Hugging Face
DESCRIPTION: Loads the California test subset of the BillSum dataset using the Hugging Face datasets library
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> billsum = load_dataset("billsum", split="ca_test")
```

----------------------------------------

TITLE: Streaming Text Generation with Transformers in Python
DESCRIPTION: This snippet demonstrates how to implement streaming text generation using the Hugging Face Transformers library. It initializes a model and tokenizer from a pre-trained GPT-2 model, creates an instance of `TextStreamer`, and generates text output in a streamed manner token by token. The streaming feature is enabled via the `streamer` parameter, which requires the `TextStreamer` class to have `put` and `end` methods. The code is useful for reducing latency in real-time applications.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/generation_features.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2")
model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")
inputs = tokenizer(["The secret to baking a good cake is "], return_tensors="pt")
streamer = TextStreamer(tokenizer)

_ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)
```

----------------------------------------

TITLE: Creating Custom Dataset for Semantic Segmentation in Python
DESCRIPTION: This code demonstrates how to create a custom dataset for semantic segmentation using the Hugging Face datasets library. It includes steps to create Dataset objects, combine them into a DatasetDict, and optionally push to the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
from datasets import Dataset, DatasetDict, Image

image_paths_train = ["path/to/image_1.jpg/jpg", "path/to/image_2.jpg/jpg", ..., "path/to/image_n.jpg/jpg"]
label_paths_train = ["path/to/annotation_1.png", "path/to/annotation_2.png", ..., "path/to/annotation_n.png"]

image_paths_validation = [...]
label_paths_validation = [...]

def create_dataset(image_paths, label_paths):
    dataset = Dataset.from_dict({"image": sorted(image_paths),
                                "label": sorted(label_paths)})
    dataset = dataset.cast_column("image", Image())
    dataset = dataset.cast_column("label", Image())
    return dataset

# step 1: create Dataset objects
train_dataset = create_dataset(image_paths_train, label_paths_train)
validation_dataset = create_dataset(image_paths_validation, label_paths_validation)

# step 2: create DatasetDict
dataset = DatasetDict({
     "train": train_dataset,
     "validation": validation_dataset,
     }
)

# step 3: push to Hub (assumes you have ran the huggingface-cli login command in a terminal/notebook)
dataset.push_to_hub("your-name/dataset-repo")

# optionally, you can push to a private repo on the Hub
# dataset.push_to_hub("name of repo on the hub", private=True)
```

----------------------------------------

TITLE: Creating Multimodal Chat Templates in Python
DESCRIPTION: This code snippet demonstrates how to build and handle a chat template with both text and image content, using a dictionary structure to define roles and content types. It highlights the setup of a multimodal chat history involving a system role and a user inquiry, supported by the ImageTextToTextPipeline of the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating_multimodal.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
messages = [
    {
        "role": "system",
        "content": [{"type": "text", "text": "You are a friendly chatbot who always responds in the style of a pirate"}],
    },
    {
      "role": "user",
      "content": [
            {"type": "image", "url": "http://images.cocodataset.org/val2017/000000039769.jpg"},
            {"type": "text", "text": "What are these?"},
        ],
    },
]
```

----------------------------------------

TITLE: Using CodeGen Model for Code Generation in Python
DESCRIPTION: Example demonstrating how to load and use the CodeGen model for code completion. Shows initialization of the model and tokenizer, followed by generating a completion for a 'hello_world' function.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/codegen.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> checkpoint = "Salesforce/codegen-350M-mono"
>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)

>>> text = "def hello_world():"

>>> completion = model.generate(**tokenizer(text, return_tensors="pt"))

>>> print(tokenizer.decode(completion[0]))
def hello_world():
    print("Hello World")

hello_world()
```

----------------------------------------

TITLE: Performing Mask Filling with BART in Python
DESCRIPTION: This code snippet demonstrates how to use the BART model for filling multi-token masks in a sentence. It initializes the model and tokenizer from the pretrained weights and processes an example phrase with a mask token. The output shows the predicted text after generating the filled content.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bart.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import BartForConditionalGeneration, BartTokenizer

model = BartForConditionalGeneration.from_pretrained("facebook/bart-large", forced_bos_token_id=0)
tok = BartTokenizer.from_pretrained("facebook/bart-large")
example_english_phrase = "UN Chief Says There Is No <mask> in Syria"
batch = tok(example_english_phrase, return_tensors="pt")
generated_ids = model.generate(batch["input_ids"])
assert tok.batch_decode(generated_ids, skip_special_tokens=True) == [
    "UN Chief Says There Is No Plan to Stop Chemical Weapons in Syria"
]
```

----------------------------------------

TITLE: Pass Batch Input to Model (TensorFlow)
DESCRIPTION: Passes the preprocessed batch input to the loaded TensorFlow model. This produces the model's output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_22

LANGUAGE: Python
CODE:
```
>>> tf_outputs = tf_model(tf_batch)
```

----------------------------------------

TITLE: Inference with PyTorch Model
DESCRIPTION: Demonstrates how to perform inference using the loaded PyTorch model and process the output logits with softmax to get probabilities.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/quicktour.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> pt_outputs = pt_model(**pt_batch)

>>> from torch import nn

>>> pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
>>> print(pt_predictions)
tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],
        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)
```

----------------------------------------

TITLE: Loading, compiling, and fitting a model with Keras
DESCRIPTION: Loads a pre-trained transformer model for sequence classification using TensorFlow Keras. Compiles the model with the Adam optimizer and a learning rate of 3e-5. Then, the model is trained using the tokenized data and labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/training.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from transformers import TFAutoModelForSequenceClassification
from tensorflow.keras.optimizers import Adam

# Load and compile our model
model = TFAutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased")
# Lower learning rates are often better for fine-tuning transformers
model.compile(optimizer=Adam(3e-5))

model.fit(tokenized_data, labels)
```

----------------------------------------

TITLE: Passing inputs to the model and retrieving logits (PyTorch)
DESCRIPTION: This snippet demonstrates how to pass the tokenized inputs to the model and retrieve the `logits`. It uses `torch.no_grad()` to disable gradient calculation during inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_27

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("stevhliu/my_awesome_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

----------------------------------------

TITLE: Mapping Preprocessing Function over Dataset
DESCRIPTION: Applies the preprocessing function to the dataset using the map method for efficient processing. Batched processing is enabled, and unnecessary columns are removed to streamline the dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
tokenized_eli5 = eli5.map(
    preprocess_function,
    batched=True,
    num_proc=4,
    remove_columns=eli5["train"].column_names,
)
```

----------------------------------------

TITLE: Logging into Hugging Face
DESCRIPTION: This code snippet logs the user into their Hugging Face account using the `notebook_login` function from the `huggingface_hub` library. This allows the user to upload and share their fine-tuned model with the community. A Hugging Face token is required for authentication.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
">>> from huggingface_hub import notebook_login\n\n>>> notebook_login()"
```

----------------------------------------

TITLE: Logging in to Hugging Face Hub in Python
DESCRIPTION: Shows how to log in to the Hugging Face Hub using the huggingface_hub library in a Jupyter or Colab notebook.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/model_sharing.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Creating and Activating Virtual Environment Using venv in Bash
DESCRIPTION: This snippet demonstrates how to create and activate a Python virtual environment using the venv module, which is useful for managing project dependencies.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python -m venv .env
source .env/bin/activate
```

----------------------------------------

TITLE: Initializing PatchTSMixer with HuggingFace Trainer in Python
DESCRIPTION: This snippet demonstrates how to initialize and train a PatchTSMixer model using the HuggingFace Transformers library's Trainer API. Dependencies include `transformers.Trainer` and `transformers.TrainingArguments`. The model is initialized with a specific configuration and trained on a training dataset, with evaluation on a test dataset. Key parameters include `context_length`, which specifies the input length, and `prediction_length`, which defines the output length. The snippet outputs the evaluation results after training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/patchtsmixer.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import PatchTSMixerConfig, PatchTSMixerForPrediction
from transformers import Trainer, TrainingArguments,


config = PatchTSMixerConfig(context_length = 512, prediction_length = 96)
model = PatchTSMixerForPrediction(config)
trainer = Trainer(model=model, args=training_args, 
            train_dataset=train_dataset,
            eval_dataset=valid_dataset)
trainer.train()
results = trainer.evaluate(test_dataset)
```

----------------------------------------

TITLE: Loading a Pretrained Model with AutoModel
DESCRIPTION: This snippet demonstrates how to load a pretrained model using `AutoModel.from_pretrained` from the `transformers` library. It loads the 'google-bert/bert-base-cased' model. This is a standard way to load models but may consume a lot of memory for very large models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/big_models.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoModel

model = AutoModel.from_pretrained("google-bert/bert-base-cased")
```

----------------------------------------

TITLE: Semantic Segmentation with Hugging Face Transformers in Python
DESCRIPTION: This snippet shows how to perform semantic segmentation using a pretrained model from Hugging Face Transformers. It involves loading an image from a URL and applying the semantic segmentation pipeline to infer class labels for each pixel.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import pipeline
from PIL import Image
import requests

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/segmentation_input.jpg"
image = Image.open(requests.get(url, stream=True).raw)
image
```

----------------------------------------

TITLE: Running Inference with Fine-tuned VideoMAE Model in Python
DESCRIPTION: Defines a function to perform inference using the fine-tuned VideoMAE model. Handles input preprocessing, device management, and forward pass.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
>>> def run_inference(model, video):
...     # (num_frames, num_channels, height, width)
...     perumuted_sample_test_video = video.permute(1, 0, 2, 3)
...     inputs = {
...         "pixel_values": perumuted_sample_test_video.unsqueeze(0),
...         "labels": torch.tensor(
...             [sample_test_video["label"]]
...         ),  # this can be skipped if you don't have labels available.
...     }

...     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
...     inputs = {k: v.to(device) for k, v in inputs.items()}
...     model = model.to(device)

...     # forward pass
...     with torch.no_grad():
...         outputs = model(**inputs)
...         logits = outputs.logits

...     return logits
```

----------------------------------------

TITLE: Basic Chat Template Usage with BlenderBot
DESCRIPTION: Demonstrates basic usage of chat templates with BlenderBot model, showing how to format a simple conversation using the tokenizer's apply_chat_template method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/chat_templating.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("facebook/blenderbot-400M-distill")

chat = [
   {"role": "user", "content": "Hello, how are you?"},
   {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
   {"role": "user", "content": "I'd like to show off how chat templating works!"},
]

tokenizer.apply_chat_template(chat, tokenize=False)
```

----------------------------------------

TITLE: Performing Super-Resolution with the Pipeline
DESCRIPTION: Processes the image through the pipeline to upscale it, then prints the dimensions of the resulting image to show the resolution improvement.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_to_image.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
upscaled = pipe(image)
print(upscaled.size)
```

----------------------------------------

TITLE: Tokenizing Dataset and Adding Columns in Python
DESCRIPTION: This snippet defines a function to tokenize text data and add the tokenized output as new columns to a dataset. It then maps this function over the dataset to process all samples.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/training.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
def tokenize_dataset(data):
    # è¿”ã•ã‚ŒãŸè¾žæ›¸ã®ã‚­ãƒ¼ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åˆ—ã¨ã—ã¦è¿½åŠ ã•ã‚Œã¾ã™
    return tokenizer(data["text"])


dataset = dataset.map(tokenize_dataset)
```

----------------------------------------

TITLE: Training GPT-2 with Custom Training Files
DESCRIPTION: Fine-tunes GPT-2 model using custom training and validation files with the Trainer API. Configures batch sizes and output directory for custom dataset training.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
python run_clm.py \
    --model_name_or_path openai-community/gpt2 \
    --train_file path_to_train_file \
    --validation_file path_to_validation_file \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-clm
```

----------------------------------------

TITLE: Enabling Flash Attention 2 on Model Load with Qwen2.5-Omni (Python)
DESCRIPTION: Demonstrates how to load the Qwen2.5-Omni model using `from_pretrained` while enabling the Flash Attention 2 optimization by setting `attn_implementation` to 'flash_attention_2'. This requires Flash Attention 2 to be installed and the model to be loaded in a compatible data type like `torch.bfloat16` or `torch.float16`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2_5_omni.md#_snippet_11

LANGUAGE: python
CODE:
```
from transformers import Qwen2_5OmniForConditionalGeneration

model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-Omni-7B",
    device_map="auto",
    torch_dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
)
```

----------------------------------------

TITLE: Loading Accuracy Metric for Evaluation
DESCRIPTION: Loads the accuracy metric from the Evaluate library for model evaluation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/image_classification.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> import evaluate

>>> accuracy = evaluate.load("accuracy")
```

----------------------------------------

TITLE: Setting Up Data Collator for Batch Processing
DESCRIPTION: Initializes the DefaultDataCollator to handle batching of processed examples during training, combining individual examples into training batches.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/visual_question_answering.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator()
```

----------------------------------------

TITLE: Train TensorFlow Model
DESCRIPTION: Trains the TensorFlow model using the `fit` method, providing the training dataset, validation dataset, number of epochs, and callbacks.  After training, the model is automatically uploaded to the Hugging Face Hub. Requires the `tf_train_set`, `tf_test_set`, and `callback` to be defined.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_22

LANGUAGE: python
CODE:
```
>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])
```

----------------------------------------

TITLE: Training TensorFlow Translation Model with Callbacks
DESCRIPTION: This code initiates the training process for the TensorFlow model using the prepared datasets and callbacks for three epochs, with automatic model uploading upon completion.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/translation.md#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)
```

----------------------------------------

TITLE: Modified Training Loop with Accelerate for Backward Pass
DESCRIPTION: Shows how to replace the standard loss.backward() method with Accelerate's backward() method in a training loop.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/accelerate.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> for epoch in range(num_epochs):
...     for batch in train_dataloader:
...         outputs = model(**batch)
...         loss = outputs.loss
...         accelerator.backward(loss)

...         optimizer.step()
...         lr_scheduler.step()
...         optimizer.zero_grad()
...         progress_bar.update(1)
```

----------------------------------------

TITLE: Dataset Preparation Function
DESCRIPTION: This snippet creates a function to preprocess the dataset by loading, resampling, and tokenizing audio input and its corresponding transcription.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/asr.md#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
>>> def prepare_dataset(batch):
...     audio = batch["audio"]
...     batch = processor(audio["array"], sampling_rate=audio["sampling_rate"], text=batch["transcription"])
...     batch["input_length"] = len(batch["input_values"][0])
...     return batch
```

----------------------------------------

TITLE: Saving and Loading TensorFlow Models with Transformers
DESCRIPTION: This code shows the recommended way to save and load TensorFlow models in Hugging Face Transformers using `TFPretrainedModel.save_pretrained` and `TFPreTrainedModel.from_pretrained`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/troubleshooting.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import TFPreTrainedModel

>>> model.save_pretrained("path_to/model")
>>> model = TFPreTrainedModel.from_pretrained("path_to/model")
```

----------------------------------------

TITLE: Quantizing ColPali with BitsAndBytes in Python
DESCRIPTION: This code snippet demonstrates how to quantize the ColPali model using the bitsandbytes library. It configures the quantization parameters, such as loading the model in 4-bit precision, using double quantization, setting the quantization type to nf4, and specifying the compute data type. The code then loads the model with the specified quantization configuration and performs image retrieval using the quantized model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/colpali.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import requests
import torch
from PIL import Image
from transformers import ColPaliForRetrieval, ColPaliProcessor
from transformers import BitsAndBytesConfig

# 4-bit quantization configuration
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

model_name = "vidore/colpali-v1.2-hf"

# Load model 
model = ColPaliForRetrieval.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="cuda"
).eval()

processor = ColPaliProcessor.from_pretrained(model_name)

url1 = "https://upload.wikimedia.org/wikipedia/commons/8/89/US-original-Declaration-1776.jpg"
url2 = "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Romeoandjuliet1597.jpg/500px-Romeoandjuliet1597.jpg"

images = [
    Image.open(requests.get(url1, stream=True).raw),
    Image.open(requests.get(url2, stream=True).raw),
]

queries = [
    "Who printed the edition of Romeo and Juliet?",
    "When was the United States Declaration of Independence proclaimed?",
]

# Process the inputs
inputs_images = processor(images=images, return_tensors="pt").to(model.device)
inputs_text = processor(text=queries, return_tensors="pt").to(model.device)

# Forward pass
with torch.no_grad():
    image_embeddings = model(**inputs_images).embeddings
    query_embeddings = model(**inputs_text).embeddings

scores = processor.score_retrieval(query_embeddings, image_embeddings)

print("Retrieval scores (query x image):")
print(scores)
```

----------------------------------------

TITLE: Saving and Publishing Model
DESCRIPTION: Saves the processor and pushes the trained model to Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/text-to-speech.md#2025-04-22_snippet_28

LANGUAGE: python
CODE:
```
processor.save_pretrained("YOUR_ACCOUNT_NAME/speecht5_finetuned_voxpopuli_nl")
trainer.push_to_hub()
```

----------------------------------------

TITLE: Sentiment Analysis on Multiple Inputs
DESCRIPTION: Performs sentiment analysis on a list of texts using the initialized pipeline. The pipeline returns a list of dictionaries containing the predicted label and score for each input text, then the result is parsed and printed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/quicktour.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
">>> results = classifier(["We are very happy to show you the ðŸ¤— Transformers library.", "We hope you don't hate it."])\n>>> for result in results:\n...     print(f"label: {result['label']}, with score: {round(result['score'], 4)}")\nlabel: POSITIVE, with score: 0.9998\nlabel: NEGATIVE, with score: 0.5309"
```

----------------------------------------

TITLE: Downloading GPU Network Test Script (Bash)
DESCRIPTION: This bash command downloads a Python script for diagnosing network communication problems between processes and/or nodes when training or inferring with `DistributedDataParallel` and multiple GPUs. This script helps identify issues where processes fail to communicate effectively.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/debugging.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
"wget https://raw.githubusercontent.com/huggingface/transformers/main/scripts/distributed/torch-distributed-gpu-test.py"
```

----------------------------------------

TITLE: One-liner ChatML Template for Copy-Paste
DESCRIPTION: A compact one-line version of the ChatML template, ready to be copied into code. This is useful for quickly setting up a chat template in Python scripts or notebooks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/chat_templating.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
tokenizer.chat_template = "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}"
```

----------------------------------------

TITLE: Running Summarization Training in PyTorch
DESCRIPTION: Script to fine-tune T5-small model on CNN/DailyMail dataset for summarization using PyTorch. Includes key parameters like model name, dataset configuration, and training settings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/run_scripts.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Diverse Beam Search Decoding with Transformers
DESCRIPTION: This code demonstrates how to use diverse beam search decoding for text generation using the Hugging Face Transformers library. It initializes a tokenizer and a sequence-to-sequence language model, and generates text using diverse beam search with `num_beams=5`, `num_beam_groups=5`, `max_new_tokens=30`, and `diversity_penalty=1.0`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/generation_strategies.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

>>> checkpoint = "google/pegasus-xsum"
>>> prompt = (
...     "The Permaculture Design Principles are a set of universal design principles "
...     "that can be applied to any location, climate and culture, and they allow us to design "
...     "the most efficient and sustainable human habitation and food production systems. "
...     "Permaculture is a design system that encompasses a wide variety of disciplines, such "
...     "as ecology, landscape design, environmental science and energy conservation, and the "
...     "Permaculture design principles are drawn from these various disciplines. Each individual "
...     "design principle itself embodies a complete conceptual framework based on sound "
...     "scientific principles. When we bring all these separate  principles together, we can "
...     "create a design system that both looks at whole systems, the parts that these systems "
...     "consist of, and how those parts interact with each other to create a complex, dynamic, "
...     "living system. Each design principle serves as a tool that allows us to integrate all "
...     "the separate parts of a design, referred to as elements, into a functional, synergistic, "
...     "whole system, where the elements harmoniously interact and work together in the most "
...     "efficient way possible."
... )

>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

>>> outputs = model.generate(**inputs, num_beams=5, num_beam_groups=5, max_new_tokens=30, diversity_penalty=1.0)
>>> tokenizer.decode(outputs[0], skip_special_tokens=True)
'The Design Principles are a set of universal design principles that can be applied to any location, climate and
culture, and they allow us to design the'
```

----------------------------------------

TITLE: Launching Training with Accelerate
DESCRIPTION: This bash script launches a training job using `accelerate launch` and specifies the training script along with its arguments. It sets parameters like the model name, task name, batch size, learning rate, and output directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/trainer.md#2025-04-22_snippet_12

LANGUAGE: bash
CODE:
```
cd transformers

accelerate launch \
./examples/pytorch/text-classification/run_glue.py \
--model_name_or_path google-bert/bert-base-cased \
--task_name $TASK_NAME \
--do_train \
--do_eval \
--max_seq_length 128 \
--per_device_train_batch_size 16 \
--learning_rate 5e-5 \
--num_train_epochs 3 \
--output_dir /tmp/$TASK_NAME/ \
--overwrite_output_dir
```

----------------------------------------

TITLE: Punctuation-Aware Tokenization Example
DESCRIPTION: Example showing tokenization that handles punctuation separately
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tokenizer_summary.md#2025-04-22_snippet_1

LANGUAGE: plaintext
CODE:
```
["Don", "'", "t", "you", "love", "ðŸ¤—", "Transformers", "?", "We", "sure", "do", "."]
```

----------------------------------------

TITLE: Installing Transformers with conda
DESCRIPTION: Command to install Transformers using conda package manager from the conda-forge channel.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/installation.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
conda install conda-forge::transformers
```

----------------------------------------

TITLE: Preparing Input for Zephyr Model Generation with Chat Template
DESCRIPTION: Complete example showing how to prepare a conversation for generation with the Zephyr model by applying a chat template with generation prompt. The formatted input is then used with model.generate().
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/chat_templating.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

checkpoint = "HuggingFaceH4/zephyr-7b-beta"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint)  # You may want to use bfloat16 and/or move to GPU here

messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a pirate",
    },
    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
 ]
tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")
print(tokenizer.decode(tokenized_chat[0]))
```

----------------------------------------

TITLE: Pushing Model to Hugging Face Hub
DESCRIPTION: Uploads the trained model to the Hugging Face Hub using the `push_to_hub` method. This makes the model available for others to use.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/audio_classification.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
">>> trainer.push_to_hub()"
```

----------------------------------------

TITLE: Preparing Objects for Distributed Training
DESCRIPTION: Uses the prepare method to adapt dataloaders, model, and optimizer for distributed training environments.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/accelerate.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
...     train_dataloader, eval_dataloader, model, optimizer
... )
```

----------------------------------------

TITLE: Preparing Model for Inference with Torchao Backend
DESCRIPTION: This snippet prepares a model for faster inference using the torchao_int4 backend. It is optimized for rapid processing with capabilities reaching up to 200 tokens per second on compatible hardware. This requires the `hqq.utils.patching` module for configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/hqq.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from hqq.utils.patching import prepare_for_inference

prepare_for_inference("model", backend="torchao_int4")
```

----------------------------------------

TITLE: Loading a TensorFlow Sequence Classification Model
DESCRIPTION: This code shows how to load a pre-trained TensorFlow model for sequence classification using TFAutoModelForSequenceClassification.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/quicktour.md#2025-04-23_snippet_10

LANGUAGE: python
CODE:
```
from transformers import TFAutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
```

----------------------------------------

TITLE: Preparing Training Components
DESCRIPTION: Preparing dataloaders, model, and optimizer for distributed training using Accelerator
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/accelerate.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)
```

----------------------------------------

TITLE: Extracting Image Features Using Pipeline
DESCRIPTION: Code to run inference on two images using the image feature extraction pipeline, resulting in pooled embeddings for both images.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_feature_extraction.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
outputs = pipe([image_real, image_gen])
```

----------------------------------------

TITLE: Programmatically Exporting a Model to ONNX with optimum.onnxruntime
DESCRIPTION: Python code showing how to programmatically export a Transformers model to ONNX format using the optimum.onnxruntime package and save both the model and tokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/serialization.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> from optimum.onnxruntime import ORTModelForSequenceClassification
>>> from transformers import AutoTokenizer

>>> model_checkpoint = "distilbert_base_uncased_squad"
>>> save_directory = "onnx/"

>>> # Load a model from transformers and export it to ONNX
>>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)
>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

>>> # Save the onnx model and tokenizer
>>> ort_model.save_pretrained(save_directory)
>>> tokenizer.save_pretrained(save_directory)
```

----------------------------------------

TITLE: Convert PyTorch to TensorFlow - Python
DESCRIPTION: This snippet demonstrates converting a PyTorch checkpoint to TensorFlow using `from_pretrained` with `from_pt=True`, followed by saving the converted model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/model_sharing.md#_snippet_4

LANGUAGE: python
CODE:
```
>>> tf_model = TFDistilBertForSequenceClassification.from_pretrained("path/to/awesome-name-you-picked", from_pt=True)
```

LANGUAGE: python
CODE:
```
>>> tf_model.save_pretrained("path/to/awesome-name-you-picked")
```

----------------------------------------

TITLE: Leveraging BERT Checkpoints with EncoderDecoderModel in Python
DESCRIPTION: This snippet illustrates the usage of BERT pre-trained checkpoints for initializing an EncoderDecoderModel with both encoder and decoder components. The example shows how to prepare the necessary tokens and structures for fine-tuning tasks such as text summarization, using the transformers library. The inputs to the model include an encoder, a decoder, and tokenized input and label sequences. The snippet also demonstrates basic training steps using back-propagation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bert-generation.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> encoder = BertGenerationEncoder.from_pretrained("google-bert/bert-large-uncased", bos_token_id=101, eos_token_id=102)
>>> decoder = BertGenerationDecoder.from_pretrained(
...     "google-bert/bert-large-uncased", add_cross_attention=True, is_decoder=True, bos_token_id=101, eos_token_id=102
... )
>>> bert2bert = EncoderDecoderModel(encoder=encoder, decoder=decoder)

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-large-uncased")

>>> input_ids = tokenizer(
...     "This is a long article to summarize", add_special_tokens=False, return_tensors="pt"
... ).input_ids
>>> labels = tokenizer("This is a short summary", return_tensors="pt").input_ids

>>> loss = bert2bert(input_ids=input_ids, decoder_input_ids=labels, labels=labels).loss
>>> loss.backward()
```

----------------------------------------

TITLE: Generating JSON Schema for Function Metadata
DESCRIPTION: Demonstrates how to automatically generate a JSON schema for a function using get_json_schema, which converts function metadata into a structured JSON representation for tool usage
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/chat_templating.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from transformers.utils import get_json_schema\n\ndef multiply(a: float, b: float):\n    """\n    A function that multiplies two numbers\n    \n    Args:\n        a: The first number to multiply\n        b: The second number to multiply\n    """\n    return a * b\n\nschema = get_json_schema(multiply)\nprint(schema)
```

----------------------------------------

TITLE: Defining Compute Metrics Function
DESCRIPTION: Defines a function to compute the accuracy metric during training. This function takes the model's predictions and labels as input and returns the computed accuracy.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/audio_classification.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
">>> import numpy as np\n\n
>>> def compute_metrics(eval_pred):
...     predictions = np.argmax(eval_pred.predictions, axis=1)
...     return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)"
```

----------------------------------------

TITLE: Inference on CPU with quantized model (Python)
DESCRIPTION: Performs inference on a CPU using a quantized model. It loads the quantized model, sets the device to 'cpu', tokenizes the input text, and generates text using the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/auto_round.md#_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "OPEA/Qwen2.5-1.5B-Instruct-int4-sym-inc"
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="cpu", torch_dtype="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)
text = "There is a girl who likes adventure,"
inputs = tokenizer(text, return_tensors="pt").to(model.device)
print(tokenizer.decode(model.generate(**inputs, max_new_tokens=50, do_sample=False)[0]))
```

----------------------------------------

TITLE: Image Classification with Transformers Pipeline
DESCRIPTION: This example demonstrates image classification using the Transformers pipeline. It classifies an image and returns the top predicted labels with their scores.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/task_summary.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline(task="image-classification")
>>> preds = classifier(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> print(*preds, sep="\n")
{'score': 0.4335, 'label': 'lynx, catamount'}
{'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}
{'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}
{'score': 0.0239, 'label': 'Egyptian cat'}
{'score': 0.0229, 'label': 'tiger cat'}
```

----------------------------------------

TITLE: Creating and Activating Virtual Environment in Python
DESCRIPTION: Commands to create a virtual environment in the project directory and activate it for isolated package management.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/installation.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python -m venv .env
```

LANGUAGE: bash
CODE:
```
source .env/bin/activate
```

----------------------------------------

TITLE: Generation Mode Control Example - Python
DESCRIPTION: Example showing the difference between greedy decoding and sampling in text generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/llm_tutorial.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
from transformers import set_seed
set_seed(0)

model_inputs = tokenizer(["I am a cat."], return_tensors="pt").to("cuda")

# LLM + greedy decoding = repetitive, boring output
generated_ids = model.generate(**model_inputs)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

# With sampling, the output becomes more creative!
generated_ids = model.generate(**model_inputs, do_sample=True)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

----------------------------------------

TITLE: Saving and Loading a Trained PEFT Adapter
DESCRIPTION: Code demonstrating how to save a trained PEFT adapter model to disk and later reload it using AutoModelForCausalLM.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/peft.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
model.save_pretrained(save_dir)
model = AutoModelForCausalLM.from_pretrained(save_dir)
```

----------------------------------------

TITLE: Implementing Image Classification with Transformers Pipeline in Python
DESCRIPTION: This example shows how to use the Hugging Face pipeline for image classification. The code loads a default pre-trained model to analyze an image of a cat, returning ranked predictions of animal species with confidence scores for each classification.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/task_summary.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline(task="image-classification")
>>> preds = classifier(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> print(*preds, sep="\n")
{'score': 0.4335, 'label': 'lynx, catamount'}
{'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}
{'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}
{'score': 0.0239, 'label': 'Egyptian cat'}
{'score': 0.0229, 'label': 'tiger cat'}
```

----------------------------------------

TITLE: Quantizing Mistral Model for Reduced Memory Usage
DESCRIPTION: Shows how to load and use a quantized version of the Mistral model to reduce memory requirements.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/mistral.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> import torch
>>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

>>> # specify how to quantize the model
>>> quantization_config = BitsAndBytesConfig(
...         load_in_4bit=True,
...         bnb_4bit_quant_type="nf4",
...         bnb_4bit_compute_dtype="torch.float16",
... )

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2", quantization_config=True, device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")

>>> prompt = "My favourite condiment is"

>>> messages = [
...     {"role": "user", "content": "What is your favourite condiment?"},
...     {"role": "assistant", "content": "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!"},
...     {"role": "user", "content": "Do you have mayonnaise recipes?"}
... ]

>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")

>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"The expected output"
```

----------------------------------------

TITLE: Loading Custom Model and Tokenizer for Sentiment Analysis in Python (TensorFlow)
DESCRIPTION: Loads a custom pretrained model and tokenizer for sentiment analysis using TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/quicktour.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

----------------------------------------

TITLE: Training TensorFlow Text Classification Model
DESCRIPTION: Initialize and compile a TensorFlow text classification model using Hugging Face Transformers, then fit the model with prepared dataset
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

with strategy.scope():
    model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
    model.compile(optimizer="adam")

model.fit(tf_dataset)
```

----------------------------------------

TITLE: Defining Custom Device Map for Model Offloading in Python
DESCRIPTION: This code defines a custom device map to fit most of the model on the GPU while offloading the 'lm_head' to the CPU. It's used to optimize memory usage when loading large language models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-23_snippet_10

LANGUAGE: python
CODE:
```
device_map = {
    "transformer.word_embeddings": 0,
    "transformer.word_embeddings_layernorm": 0,
    "lm_head": "cpu",
    "transformer.h": 0,
    "transformer.ln_f": 0,
}
```

----------------------------------------

TITLE: Starlette Web Server with Transformers Pipeline (Python)
DESCRIPTION: This code snippet sets up a basic web server using Starlette to handle POST requests and uses a Transformers pipeline for text processing. It utilizes an asyncio queue to manage requests and a separate task to handle the actual model inference to avoid blocking the main thread.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/pipeline_webserver.md#_snippet_0

LANGUAGE: python
CODE:
```
from starlette.applications import Starlette
from starlette.responses import JSONResponse
from starlette.routing import Route
from transformers import pipeline
import asyncio


async def homepage(request):
    payload = await request.body()
    string = payload.decode("utf-8")
    response_q = asyncio.Queue()
    await request.app.model_queue.put((string, response_q))
    output = await response_q.get()
    return JSONResponse(output)


async def server_loop(q):
    pipe = pipeline(model="google-bert/bert-base-uncased")
    while True:
        (string, response_q) = await q.get()
        out = pipe(string)
        await response_q.put(out)


app = Starlette(
    routes=[
        Route("/", homepage, methods=["POST"]),
    ],
)


@app.on_event("startup")
async def startup_event():
    q = asyncio.Queue()
    app.model_queue = q
    asyncio.create_task(server_loop(q))
```

----------------------------------------

TITLE: Loading Model, Tokenizer (TensorFlow)
DESCRIPTION: This code loads a pre-trained TensorFlow model and tokenizer using the Hugging Face Transformers library. It imports the necessary classes and functions and then uses them to load the DistilBERT model and its corresponding tokenizer. This is the initial step for training a TensorFlow-based sequence classification model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
from transformers import TFAutoModelForSequenceClassification, AutoTokenizer

model = TFAutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Generating Text with T5 using Pipeline (PyTorch)
DESCRIPTION: Demonstrates how to use the transformers library's high-level pipeline interface for text-to-text generation with a T5 model. It initializes a pipeline for 'text2text-generation', loads the 'google-t5/t5-base' model with half-precision (float16) on a specified device, and performs an English to French translation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/t5.md#_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipeline = pipeline(
    task="text2text-generation",
    model="google-t5/t5-base",
    torch_dtype=torch.float16,
    device=0
)
pipeline("translate English to French: The weather is nice today.")
```

----------------------------------------

TITLE: Specifying GPU Device for Pipeline in Python
DESCRIPTION: Creates a pipeline that will run on GPU device 0.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/hi/pipeline_tutorial.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
transcriber = pipeline(model="openai/whisper-large-v2", device=0)
```

----------------------------------------

TITLE: Apply Softmax to Model Output (TensorFlow)
DESCRIPTION: Applies the softmax function to the model's logits to obtain probabilities for each class in TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_23

LANGUAGE: Python
CODE:
```
>>> import tensorflow as tf

>>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
>>> tf_predictions  # doctest: +IGNORE_RESULT
```

----------------------------------------

TITLE: Manual Inference with TensorFlow: Tokenization
DESCRIPTION: The first step of manual inference with TensorFlow is to tokenize the input text. This converts the text into tensors that the model can process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_28

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_model")
>>> inputs = tokenizer(text, return_tensors="tf")
```

----------------------------------------

TITLE: Load Model for Sequence Classification (TensorFlow)
DESCRIPTION: Loads a pre-trained model for sequence classification using `TFAutoModelForSequenceClassification` for TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_21

LANGUAGE: Python
CODE:
```
>>> from transformers import TFAutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
```

----------------------------------------

TITLE: Tokenizing Input Text
DESCRIPTION: Tokenizes input text using the model's tokenizer with left-side padding.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/llm_tutorial.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", padding_side="left")
model_inputs = tokenizer(["A list of colors: red, blue"], return_tensors="pt").to("cuda")
```

----------------------------------------

TITLE: Implementing Dynamic Batching for Transformers Inference in Python
DESCRIPTION: This code snippet demonstrates how to implement dynamic batching for Transformers inference in a server environment. It accumulates requests for a short time before processing them in a batch, which can improve performance for certain models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/pipeline_webserver.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
(string, rq) = await q.get()
strings = []
queues = []
while True:
    try:
        (string, rq) = await asyncio.wait_for(q.get(), timeout=0.001)  # 1ms
    except asyncio.exceptions.TimeoutError:
        break
    strings.append(string)
    queues.append(rq)
strings
outs = pipe(strings, batch_size=len(strings))
for rq, out in zip(queues, outs):
    await rq.put(out)
```

----------------------------------------

TITLE: Implementing PyTorch Training Loop with Progress Bar
DESCRIPTION: Defines the training loop for fine-tuning the model, including forward pass, loss calculation, backpropagation, and optimizer steps, with a progress bar for visualization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/training.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

----------------------------------------

TITLE: Loading AWQ-Quantized Model with Specific Parameters
DESCRIPTION: Load an AWQ-quantized model with custom torch dtype and device mapping
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/awq.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model = AutoModelForCausalLM.from_pretrained(
  "TheBloke/zephyr-7B-alpha-AWQ",
  torch_dtype=torch.float32,
  device_map="cuda:0"
)
```

----------------------------------------

TITLE: Setting Up DataCollator for Language Modeling in PyTorch
DESCRIPTION: Configures the DataCollatorForLanguageModeling for PyTorch, setting the padding token and disabling masked language modeling.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/language_modeling.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorForLanguageModeling

>>> tokenizer.pad_token = tokenizer.eos_token
>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
```

----------------------------------------

TITLE: Pushing Model to Hub During Training in PyTorch
DESCRIPTION: Demonstrates how to push a model to the Hugging Face Hub during training using the TrainingArguments and Trainer classes in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_sharing.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
training_args = TrainingArguments(output_dir="my-awesome-model", push_to_hub=True)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

trainer.push_to_hub()
```

----------------------------------------

TITLE: Using Sentiment Analysis Pipeline in Python
DESCRIPTION: Demonstrates how to use the sentiment analysis pipeline from Transformers to classify the sentiment of a given text.
SOURCE: https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
>>> from transformers import pipeline

# ä½¿ç”¨æƒ…ç·’åˆ†æž pipeline
>>> classifier = pipeline('sentiment-analysis')
>>> classifier('We are very happy to introduce pipeline to the transformers repository.')
[{'label': 'POSITIVE', 'score': 0.9996980428695679}]
```

----------------------------------------

TITLE: Using push_to_hub Function Directly
DESCRIPTION: Demonstrates how to use the push_to_hub function directly on a model to upload it to the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_sharing.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
pt_model.push_to_hub("my-awesome-model")
```

----------------------------------------

TITLE: Loading Pre-trained Model for Token Classification with AutoModelForTokenClassification in Python
DESCRIPTION: This example illustrates loading a pre-trained model for token classification tasks using the AutoModelForTokenClassification class, which allows different architectures to be reused with the same checkpoint.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/autoclass_tutorial.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForTokenClassification

>>> model = AutoModelForTokenClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: DeepSpeed ZeRO-2 configuration with CPU offload
DESCRIPTION: JSON configuration for DeepSpeed using ZeRO stage 2 optimization with CPU offload, AdamW optimizer, WarmupLR scheduler, and mixed precision training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/deepspeed.md#2025-04-22_snippet_21

LANGUAGE: json
CODE:
```
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": "auto",
            "betas": "auto",
            "eps": "auto",
            "weight_decay": "auto"
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },

    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "allgather_partitions": true,
        "allgather_bucket_size": 2e8,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2e8,
        "contiguous_gradients": true
    },

    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto"
}
```

----------------------------------------

TITLE: Loading the MInDS-14 Audio Dataset using Hugging Face Datasets in Python
DESCRIPTION: This code snippet shows how to load the MInDS-14 audio dataset using the `load_dataset` function from the `datasets` library. It specifies the dataset name, language, and split (train).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_7

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")  # doctest: +IGNORE_RESULT
```

----------------------------------------

TITLE: Enabling CPU Offloading for Llama4 in Transformers
DESCRIPTION: This snippet demonstrates how to enable CPU offloading for the Llama4 model when available GPU memory is insufficient. Setting `device_map="auto"` during model loading allows the model to automatically offload components to the CPU.  This requires sufficient CPU memory but enables running the model on systems with limited GPU resources.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llama4.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
from transformers import Llama4ForConditionalGeneration
import torch

model = Llama4ForConditionalGeneration.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.bfloat16,
)
```

----------------------------------------

TITLE: Initializing Chinese-CLIP Model and Processor
DESCRIPTION: Load a pre-trained Chinese-CLIP model and processor for processing image and text features, using the base patch16 variant
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/chinese_clip.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from PIL import Image
import requests
from transformers import ChineseCLIPProcessor, ChineseCLIPModel

model = ChineseCLIPModel.from_pretrained("OFA-Sys/chinese-clip-vit-base-patch16")
processor = ChineseCLIPProcessor.from_pretrained("OFA-Sys/chinese-clip-vit-base-patch16")
```

----------------------------------------

TITLE: Pushing Fine-tuned PyTorch Model to Hugging Face Hub
DESCRIPTION: Uploads the trained model to the Hugging Face Hub using the push_to_hub method, making it available for others to use. This happens after the training is completed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
trainer.push_to_hub()
```

----------------------------------------

TITLE: Pushing the trained model to the Hub (PyTorch)
DESCRIPTION: This snippet shows how to push the trained model to the Hugging Face Hub using the `push_to_hub()` method of the `Trainer`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
>>> trainer.push_to_hub()
```

----------------------------------------

TITLE: Accelerator Initialization
DESCRIPTION: Setting up Accelerator instance and device configuration for distributed training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from accelerate import Accelerator

accelerator = Accelerator()
device = accelerator.device
```

----------------------------------------

TITLE: Saving a BERT Model to TorchScript
DESCRIPTION: This snippet demonstrates how to instantiate a BERT model with TorchScript flag, create dummy inputs, trace the model, and save it to disk as a TorchScript file.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/torchscript.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import BertModel, BertTokenizer, BertConfig
import torch

enc = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")

# Tokenize input text
text = "[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]"
tokenized_text = enc.tokenize(text)

# Mask one of the input tokens
masked_index = 8
tokenized_text[masked_index] = "[MASK]"
indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

# Create dummy inputs
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

# Initialize the model with torchscript flag
config = BertConfig(
    vocab_size_or_config_json_file=32000,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072,
    torchscript=True,
)

# Instantiate the model
model = BertModel(config)

# Ensure the model is in evaluation mode
model.eval()

# Alternatively, use from_pretrained with torchscript flag
model = BertModel.from_pretrained("google-bert/bert-base-uncased", torchscript=True)

# Generate the trace
traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])
torch.jit.save(traced_model, "traced_bert.pt")
```

----------------------------------------

TITLE: Tokenizing Text with BERT Tokenizer for Input IDs
DESCRIPTION: This snippet demonstrates how to process raw text into tokenized input IDs using the BERT tokenizer, which is required for passing sequences to a transformer model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/glossary.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-cased")

>>> sequence = "A Titan RTX has 24GB of VRAM"
```

----------------------------------------

TITLE: Tokenizing and Padding to Multiple of Length
DESCRIPTION: This snippet shows how to pad a batch of sentences to the nearest multiple of a specified length.  This is controlled via `pad_to_multiple_of`. Note that `padding=True` is also necessary here to enable padding.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pad_truncation.md#_snippet_3

LANGUAGE: Python
CODE:
```
tokenizer(batch_sentences, padding=True, pad_to_multiple_of=8)
```

----------------------------------------

TITLE: Loading Int8 Quantized Model for Single GPU Inference
DESCRIPTION: Load a Transformers model using int8 mixed-precision matrix decomposition for efficient inference on a single GPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/perf_infer_gpu_one.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

model_name = "bigscience/bloom-2b5"
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```

----------------------------------------

TITLE: Multi Image Inference with OmDet-Turbo in Python
DESCRIPTION: This code demonstrates batched multi-image inference using OmDet-Turbo, allowing for different text prompts and classes in each image. It shows how to load multiple images, define text labels and tasks for each, process them in a batch, run inference, and post-process the results to print the detected objects, their confidence scores, and locations within each image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/omdet-turbo.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> import torch
>>> import requests
>>> from io import BytesIO
>>> from PIL import Image
>>> from transformers import AutoProcessor, OmDetTurboForObjectDetection

>>> processor = AutoProcessor.from_pretrained("omlab/omdet-turbo-swin-tiny-hf")
>>> model = OmDetTurboForObjectDetection.from_pretrained("omlab/omdet-turbo-swin-tiny-hf")

>>> url1 = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image1 = Image.open(BytesIO(requests.get(url1).content)).convert("RGB")
>>> text_labels1 = ["cat", "remote"]
>>> task1 = "Detect {}.".format(", ".join(text_labels1))

>>> url2 = "http://images.cocodataset.org/train2017/000000257813.jpg"
>>> image2 = Image.open(BytesIO(requests.get(url2).content)).convert("RGB")
>>> text_labels2 = ["boat"]
>>> task2 = "Detect everything that looks like a boat."

>>> url3 = "https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg"
>>> image3 = Image.open(BytesIO(requests.get(url3).content)).convert("RGB")
>>> text_labels3 = ["statue", "trees"]
>>> task3 = "Focus on the foreground, detect statue and trees."

>>> inputs = processor(
...     images=[image1, image2, image3],
...     text=[text_labels1, text_labels2, text_labels3],
...     task=[task1, task2, task3],
...     return_tensors="pt",
... )

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> # convert outputs (bounding boxes and class logits)
>>> results = processor.post_process_grounded_object_detection(
...     outputs,
...     text_labels=[text_labels1, text_labels2, text_labels3],
...     target_sizes=[(image.height, image.width) for image in [image1, image2, image3]],
...     threshold=0.2,
...     nms_threshold=0.3,
... )

>>> for i, result in enumerate(results):
...     for score, text_label, box in zip(
...         result["scores"], result["text_labels"], result["boxes"]
...     ):
...         box = [round(i, 1) for i in box.tolist()]
...         print(
...             f"Detected {text_label} with confidence "
...             f"{round(score.item(), 2)} at location {box} in image {i}"
...         )
Detected remote with confidence 0.77 at location [39.9, 70.4, 176.7, 118.0] in image 0
Detected cat with confidence 0.72 at location [11.6, 54.2, 314.8, 474.0] in image 0
Detected remote with confidence 0.56 at location [333.4, 75.8, 370.7, 187.0] in image 0
Detected cat with confidence 0.55 at location [345.2, 24.0, 639.8, 371.7] in image 0
Detected boat with confidence 0.32 at location [146.9, 219.8, 209.6, 250.7] in image 1
Detected boat with confidence 0.3 at location [319.1, 223.2, 403.2, 238.4] in image 1
Detected boat with confidence 0.27 at location [37.7, 220.3, 84.0, 235.9] in image 1
Detected boat with confidence 0.22 at location [407.9, 207.0, 441.7, 220.2] in image 1
Detected statue with confidence 0.73 at location [544.7, 210.2, 651.9, 502.8] in image 2
Detected trees with confidence 0.25 at location [3.9, 584.3, 391.4, 785.6] in image 2
Detected trees with confidence 0.25 at location [1.4, 621.2, 118.2, 787.8] in image 2
Detected statue with confidence 0.2 at location [428.1, 205.5, 767.3, 759.5] in image 2
```

----------------------------------------

TITLE: Running Model Inference for Zero-shot Classification
DESCRIPTION: This snippet demonstrates how to run the model inference using the prepared inputs. It calculates the logits, applies softmax to convert logits to probabilities, and formats the results into a sorted list of labels with associated scores.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_image_classification.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> import torch

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> logits = outputs.logits_per_image[0]
>>> probs = logits.softmax(dim=-1).numpy()
>>> scores = probs.tolist()

>>> result = [
...     {"score": score, "label": candidate_label}
...     for score, candidate_label in sorted(zip(probs, candidate_labels), key=lambda x: -x[0])
... ]

>>> result
[{'score': 0.998572, 'label': 'car'},
 {'score': 0.0010570387, 'label': 'bike'},
 {'score': 0.0003393686, 'label': 'tree'},
 {'score': 3.1572064e-05, 'label': 'cat'}]
```

----------------------------------------

TITLE: Using KOSMOS-2 for Image Grounding and Caption Generation in Python
DESCRIPTION: This code demonstrates how to use the KOSMOS-2 model to process an image with a text prompt, generate a caption with grounded text spans connected to bounding boxes, and extract the entities with their locations. It loads the model and processor, processes an image and prompt, generates text, and extracts the grounded entities.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/kosmos-2.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, Kosmos2ForConditionalGeneration

>>> model = Kosmos2ForConditionalGeneration.from_pretrained("microsoft/kosmos-2-patch14-224")
>>> processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")

>>> url = "https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> prompt = "<grounding> An image of"

>>> inputs = processor(text=prompt, images=image, return_tensors="pt")

>>> generated_ids = model.generate(
...     pixel_values=inputs["pixel_values"],
...     input_ids=inputs["input_ids"],
...     attention_mask=inputs["attention_mask"],
...     image_embeds=None,
...     image_embeds_position_mask=inputs["image_embeds_position_mask"],
...     use_cache=True,
...     max_new_tokens=64,
... )
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
>>> processed_text = processor.post_process_generation(generated_text, cleanup_and_extract=False)
>>> processed_text
'<grounding> An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object> warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>.'

>>> caption, entities = processor.post_process_generation(generated_text)
>>> caption
'An image of a snowman warming himself by a fire.'

>>> entities
[('a snowman', (12, 21), [(0.390625, 0.046875, 0.984375, 0.828125)]), ('a fire', (41, 47), [(0.171875, 0.015625, 0.484375, 0.890625)])]
```

----------------------------------------

TITLE: Loading 8-bit Quantized Model with Custom Configuration in Python
DESCRIPTION: This snippet shows how to load a pre-trained model with 8-bit quantization, custom device mapping, and quantization configuration. It's used to efficiently load large language models with specific memory optimizations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-23_snippet_11

LANGUAGE: python
CODE:
```
model_8bit = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-1b7",
    torch_dtype="auto",
    device_map=device_map,
    quantization_config=quantization_config,
)
```

----------------------------------------

TITLE: Int8 Model GPU Memory Allocation
DESCRIPTION: Example showcasing how to control GPU memory allocation for a multi-GPU setup with an Int8 quantized model. The `max_memory` argument is used to specify memory limits for each GPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_infer_gpu_one.md#2025-04-22_snippet_16

LANGUAGE: Python
CODE:
```
"max_memory_mapping = {0: \"1GB\", 1: \"2GB\"}\nmodel_name = \"bigscience/bloom-3b\"\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\n    model_name, device_map=\"auto\", load_in_8bit=True, max_memory=max_memory_mapping\n)"
```

----------------------------------------

TITLE: Loading a TF Pre-trained Model for Token Classification
DESCRIPTION: This code snippet demonstrates how to reuse the same checkpoint to load a TensorFlow model architecture for a different task - token classification - using `TFAutoModelForTokenClassification.from_pretrained`. The underlying weights are the same but the head is reinitialized for the new task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/autoclass_tutorial.md#_snippet_10

LANGUAGE: Python
CODE:
```
>>> from transformers import TFAutoModelForTokenClassification

>>> model = TFAutoModelForTokenClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Loading a TF Pre-trained Model for Token Classification with TFAutoModel
DESCRIPTION: This code snippet shows how to load a pre-trained TensorFlow model for token classification using `TFAutoModelForTokenClassification.from_pretrained` from the `transformers` library. It demonstrates the ability to reuse the same checkpoint ('distilbert/distilbert-base-uncased') to load a model for a different task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/autoclass_tutorial.md#_snippet_7

LANGUAGE: Python
CODE:
```
>>> from transformers import TFAutoModelForTokenClassification

>>> model = TFAutoModelForTokenClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Formatting Prompts with Chat Templates using Transformers in Python
DESCRIPTION: This snippet demonstrates how to structure input prompts for the LLaVa-NeXT-Video model using the `apply_chat_template` method from the `LlavaNextVideoProcessor`. It details constructing conversation histories, emphasizing the importance of dictionary structures with roles and content lists for different modalities such as text and images.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llava_next_video.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import LlavaNextVideoProcessor

processor = LlavaNextVideoProcessor.from_pretrained("llava-hf/LLaVA-NeXT-Video-7B-hf")

conversation = [
    {
        "role": "system",
        "content": [
            {"type": "text", "text": "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions."},
            ],
    },
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "Whatâ€™s shown in this image?"},
            {"type": "image"},
            ],
    },
    {
        "role": "assistant",
        "content": [{"type": "text", "text": "This image shows a red stop sign."},]
    },
    {

        "role": "user",
        "content": [
            {"type": "text", "text": "Why is this video funny?"},
            {"type": "video"},
            ],
    },
]

text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)

# Note that the template simply formats your prompt, you still have to tokenize it and obtain pixel values for your visuals
print(text_prompt)
```

----------------------------------------

TITLE: Tokenizing and Padding to Longest Length
DESCRIPTION: This snippet demonstrates how to use the tokenizer to pad a batch of sentences to the length of the longest sequence within the batch. The `padding=True` argument enables padding to the longest sequence. This is equivalent to `padding='longest'`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pad_truncation.md#_snippet_0

LANGUAGE: Python
CODE:
```
tokenizer(batch_sentences, padding=True)
```

LANGUAGE: Python
CODE:
```
tokenizer(batch_sentences, padding='longest')
```

----------------------------------------

TITLE: Tokenizing Text with AutoTokenizer in PyTorch
DESCRIPTION: This code snippet demonstrates how to tokenize text using the `AutoTokenizer` from the Transformers library, configured to return PyTorch tensors. The tokenizer is loaded from the pre-trained model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_28

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> inputs = tokenizer(text, return_tensors="pt")
```

----------------------------------------

TITLE: Uploading Model to Hugging Face Hub (Python)
DESCRIPTION: Demonstrates using the `push_to_hub` method available on Transformer model objects to upload the model to a specified repository on the Hugging Face Hub. Requires authentication and a model object.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/add_new_model.md#_snippet_32

LANGUAGE: python
CODE:
```
brand_new_bert.push_to_hub("brand_new_llama")
```

----------------------------------------

TITLE: Manual Inference with the Fine-tuned Model
DESCRIPTION: Demonstrates how to manually perform inference by preprocessing the input, forwarding it through the model, and interpreting the output logits to get the predicted answer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_question_answering.md#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
>>> processor = ViltProcessor.from_pretrained("MariaK/vilt_finetuned_200")

>>> image = Image.open(example['image_id'])
>>> question = example['question']

>>> # prepare inputs
>>> inputs = processor(image, question, return_tensors="pt")

>>> model = ViltForQuestionAnswering.from_pretrained("MariaK/vilt_finetuned_200")

>>> # forward pass
>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> logits = outputs.logits
>>> idx = logits.argmax(-1).item()
>>> print("Predicted answer:", model.config.id2label[idx])
Predicted answer: down
```

----------------------------------------

TITLE: Loading and Tokenizing Dataset
DESCRIPTION: This snippet loads the CoLA dataset from the GLUE benchmark using the `datasets` library. It then tokenizes the sentences using a pre-trained DistilBERT tokenizer, pads the sequences to a maximum length of 128, and prepares the data for training by converting labels to a NumPy array.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
"from transformers import AutoTokenizer\nfrom datasets import load_dataset\nimport numpy as np\n\ndataset = load_dataset(\"glue\", \"cola\")['train']\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n\ntrain_data = tokenizer(\n    dataset[\"sentence\"],\n    padding=\"max_length\",\n    truncation=True,\n    max_length=128,\n    return_tensors=\"np\",\n)\ntrain_data = dict(train_data)\ntrain_labels = np.array(dataset[\"label\"])"
```

----------------------------------------

TITLE: Implementing SDPA with PyTorch and Transformers
DESCRIPTION: This snippet demonstrates how to load a BEiT model using the Transformers library with scaled dot product attention explicitly activated. It requires the Transformers library and a PyTorch environment. The primary parameter is 'attn_implementation' set to 'sdpa'. The model is configured to operate in half-precision for optimal performance enhancements.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/beit.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import BeitForImageClassification
model = BeitForImageClassification.from_pretrained("microsoft/beit-base-patch16-224", attn_implementation="sdpa", torch_dtype=torch.float16)
...
```

----------------------------------------

TITLE: Image Generation Inference with Emu3
DESCRIPTION: This code snippet showcases how to generate images from textual input using the Emu3 model. It utilizes the `Emu3Processor` to prepare the text prompts and the `Emu3ForConditionalGeneration` model for image generation. A custom `prefix_allowed_tokens_fn` is used to constrain the generated tokens to valid image tokens, and the generated image tokens are then decoded and saved as a PNG file.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/emu3.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
processor = Emu3Processor.from_pretrained("BAAI/Emu3-Gen-hf")
model = Emu3ForConditionalGeneration.from_pretrained("BAAI/Emu3-Gen-hf", torch_dtype="bfloat16", device_map="auto", attn_implementation="flash_attention_2")


inputs = processor(
    text=["a portrait of young girl. masterpiece, film grained, best quality.", "a dog running under the rain"],
    padding=True,
    return_tensors="pt",
    return_for_image_generation=True,
)
inputs = inputs.to(device="cuda:0", dtype=torch.bfloat16)

neg_prompt = "lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry."
neg_inputs = processor(text=[neg_prompt] * 2, return_tensors="pt").to(device="cuda:0")

image_sizes = inputs.pop("image_sizes")
HEIGHT, WIDTH = image_sizes[0]
VISUAL_TOKENS = model.vocabulary_mapping.image_tokens

def prefix_allowed_tokens_fn(batch_id, input_ids):
    height, width = HEIGHT, WIDTH
    visual_tokens = VISUAL_TOKENS
    image_wrapper_token_id = torch.tensor([processor.tokenizer.image_wrapper_token_id], device=model.device)
    eoi_token_id = torch.tensor([processor.tokenizer.eoi_token_id], device=model.device)
    eos_token_id = torch.tensor([processor.tokenizer.eos_token_id], device=model.device)
    pad_token_id = torch.tensor([processor.tokenizer.pad_token_id], device=model.device)
    eof_token_id = torch.tensor([processor.tokenizer.eof_token_id], device=model.device)
    eol_token_id = processor.tokenizer.encode("<|extra_200|>", return_tensors="pt")[0]

    position = torch.nonzero(input_ids == image_wrapper_token_id, as_tuple=True)[0][0]
    offset = input_ids.shape[0] - position
    if offset % (width + 1) == 0:
        return (eol_token_id, )
    elif offset == (width + 1) * height + 1:
        return (eof_token_id, )
    elif offset == (width + 1) * height + 2:
        return (eoi_token_id, )
    elif offset == (width + 1) * height + 3:
        return (eos_token_id, )
    elif offset > (width + 1) * height + 3:
        return (pad_token_id, )
    else:
        return visual_tokens


out = model.generate(
    **inputs,
    max_new_tokens=50_000, # make sure to have enough tokens for one image
    prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,
    return_dict_in_generate=True,
    negative_prompt_ids=neg_inputs.input_ids, # indicate for Classifier-Free Guidance
    negative_prompt_attention_mask=neg_inputs.attention_mask,
)

image = model.decode_image_tokens(out.sequences[:, inputs.input_ids.shape[1]: ], height=HEIGHT, width=WIDTH)
images = processor.postprocess(list(image.float()), return_tensors="PIL.Image.Image") # internally we convert to np but it's not supported in bf16 precision
for i, image in enumerate(images['pixel_values']):
    image.save(f"result{i}.png")

```

----------------------------------------

TITLE: Define Training Arguments in Python
DESCRIPTION: Defines training hyperparameters using the `TrainingArguments` class. Configuration includes output directory, batch size, number of training epochs, mixed-precision floating point (fp16), saving and logging frequencies, learning rate, weight decay, limits on saved checkpoints, and whether to push the model to the Hugging Face Hub. `remove_unused_columns=False` is crucial to prevent the removal of the image column needed to create pixel values.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/object_detection.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
>>> from transformers import TrainingArguments

>>> training_args = TrainingArguments(
...     output_dir="detr-resnet-50_finetuned_cppe5",
...     per_device_train_batch_size=8,
...     num_train_epochs=10,
...     fp16=True,
...     save_steps=200,
...     logging_steps=50,
...     learning_rate=1e-5,
...     weight_decay=1e-4,
...     save_total_limit=2,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )
```

----------------------------------------

TITLE: Dynamic Padding Data Collator - Python (TensorFlow)
DESCRIPTION: Similar to the previous PyTorch snippet, this initializes a data collator for TensorFlow that manages dynamic padding and is capable of returning tensor format for TensorFlow models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```

----------------------------------------

TITLE: Using Text Generation Pipeline for Chat
DESCRIPTION: Shows how to use the text generation pipeline for chat-based interactions, simplifying the process of working with chat models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/chat_templating.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import pipeline

pipe = pipeline("text-generation", "HuggingFaceH4/zephyr-7b-beta")
messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a pirate",
    },
    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
]
print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])
```

----------------------------------------

TITLE: Loading and Using TimesFM for Prediction in PyTorch
DESCRIPTION: This code snippet demonstrates how to load a pre-trained TimesFM model for prediction and use it to generate forecasts. It loads the model from Hugging Face Model Hub, creates dummy inputs, and generates predictions using the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/timesfm.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import TimesFmModelForPrediction


model = TimesFmModelForPrediction.from_pretrained(
    "google/timesfm-2.0-500m-pytorch",
    torch_dtype=torch.bfloat16,
    attn_implementation="sdpa",
    device_map="cuda" if torch.cuda.is_available() else None
)


 # Create dummy inputs
forecast_input = [
    np.sin(np.linspace(0, 20, 100)),
    np.sin(np.linspace(0, 20, 200)),
    np.sin(np.linspace(0, 20, 400)),
]
frequency_input = [0, 1, 2]

# Convert inputs to sequence of tensors
forecast_input_tensor = [
    torch.tensor(ts, dtype=torch.bfloat16).to("cuda" if torch.cuda.is_available() else "cpu")
    for ts in forecast_input
]
frequency_input_tensor = torch.tensor(frequency_input, dtype=torch.long).to(
    "cuda" if torch.cuda.is_available() else "cpu"
)

# Get predictions from the pre-trained model
with torch.no_grad():
    outputs = model(past_values=forecast_input_tensor, freq=frequency_input_tensor, return_dict=True)
    point_forecast_conv = outputs.mean_predictions.float().cpu().numpy()
    quantile_forecast_conv = outputs.full_predictions.float().cpu().numpy()
```

----------------------------------------

TITLE: TensorFlow/Keras Model Training Setup
DESCRIPTION: Configures and trains the model using TensorFlow with Keras API, including data preparation and model compilation
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/training.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import TFAutoModelForSequenceClassification
from tensorflow.keras.optimizers import Adam

model = TFAutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased")
model.compile(optimizer=Adam(3e-5))
model.fit(tokenized_data, labels)
```

----------------------------------------

TITLE: Applying Image Transformations to the Dataset
DESCRIPTION: This snippet uses the `set_transform` method from the ðŸ¤— Datasets library to apply the defined `transforms` function to the dataset. This ensures that the image transformations are applied on-the-fly when accessing the data.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_23

LANGUAGE: Python
CODE:
```
>>> dataset.set_transform(transforms)
```

----------------------------------------

TITLE: Creating Keras Metric Callback
DESCRIPTION: This creates a Keras callback to compute metrics during training using the compute_metrics function.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/multiple_choice.md#_snippet_18

LANGUAGE: python
CODE:
```
>>> from transformers.keras_callbacks import KerasMetricCallback

>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)
```

----------------------------------------

TITLE: Prepare for Inference with Dataset Loading in Py
DESCRIPTION: Load a dataset and split for testing data preparation before performing inference on a fine-tuned model. Sample data is loaded and displayed, requiring the datasets library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md#2025-04-22_snippet_20

LANGUAGE: py
CODE:
```
>>> from datasets import load_dataset

>>> ds = load_dataset("scene_parse_150", split="train[:50]")
>>> ds = ds.train_test_split(test_size=0.2)
>>> test_ds = ds["test"]
>>> image = ds["test"][0]["image"]
>>> image

```

----------------------------------------

TITLE: Computing Evaluation Metrics
DESCRIPTION: Defines a function to compute metrics such as precision, recall, F1, and accuracy based on model predictions and true labels, using the seqeval framework.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/token_classification.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> import numpy as np

>>> labels = [label_list[i] for i in example[f"ner_tags"]]


>>> def compute_metrics(p):
...     predictions, labels = p
...     predictions = np.argmax(predictions, axis=2)

...     true_predictions = [
...         [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
...         for prediction, label in zip(predictions, labels)
...     ]
...     true_labels = [
...         [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
...         for prediction, label in zip(predictions, labels)
...     ]

...     results = seqeval.compute(predictions=true_predictions, references=true_labels)
...     return {
...         "precision": results["overall_precision"],
...         "recall": results["overall_recall"],
...         "f1": results["overall_f1"],
...         "accuracy": results["overall_accuracy"],
...     }
```

----------------------------------------

TITLE: Loading a Pre-trained Model for Sequence Classification with AutoModel in PyTorch
DESCRIPTION: This code snippet shows how to load a pre-trained model for sequence classification using `AutoModelForSequenceClassification.from_pretrained` from the `transformers` library. The model is loaded from the specified checkpoint name, 'distilbert/distilbert-base-uncased'.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/autoclass_tutorial.md#_snippet_4

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Using Transformers Logger in Custom Module
DESCRIPTION: Illustrates how to use the Transformers logger in a custom module or script, including setting verbosity and logging at different levels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/logging.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers.utils import logging

logging.set_verbosity_info()
logger = logging.get_logger("transformers")
logger.info("INFO")
logger.warning("WARN")
```

----------------------------------------

TITLE: Initializing Accelerator Object for Distributed Training
DESCRIPTION: Code to import and create an Accelerator object, which automatically detects the distributed setup type and initializes all components needed for training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/accelerate.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from accelerate import Accelerator

>>> accelerator = Accelerator()
```

----------------------------------------

TITLE: Applying Preprocessing to Dataset
DESCRIPTION: Maps the preprocessing function across the entire dataset using parallel processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/language_modeling.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
tokenized_eli5 = eli5.map(
    preprocess_function,
    batched=True,
    num_proc=4,
    remove_columns=eli5["train"].column_names,
)
```

----------------------------------------

TITLE: Using ASR Pipeline with Multiple Inputs in Python
DESCRIPTION: Shows how to use the ASR pipeline with multiple audio inputs passed as a list.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/pipeline_tutorial.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
transcriber(
    [
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac",
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac",
    ]
)
```

----------------------------------------

TITLE: Loading Feature Extractor
DESCRIPTION: Loads a Wav2Vec2 feature extractor using `AutoFeatureExtractor.from_pretrained`. The feature extractor is used to process the audio signal into a format suitable for the Wav2Vec2 model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/audio_classification.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
">>> from transformers import AutoFeatureExtractor\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")"
```

----------------------------------------

TITLE: Apply Transformation to Dataset in Python
DESCRIPTION: Applies the defined transformation function to the entire dataset using the `with_transform` method.  This method transforms the dataset elements on-the-fly when they are loaded.  An example of accessing a transformed element is shown to verify the transformation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/object_detection.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> cppe5["train"] = cppe5["train"].with_transform(transform_aug_ann)
>>> cppe5["train"][15]
{'pixel_values': tensor([[[ 0.9132,  0.9132,  0.9132,  ..., -1.9809, -1.9809, -1.9809],
          [ 0.9132,  0.9132,  0.9132,  ..., -1.9809, -1.9809, -1.9809],
          [ 0.9132,  0.9132,  0.9132,  ..., -1.9638, -1.9638, -1.9638],
          ...,
          [-1.5699, -1.5699, -1.5699,  ..., -1.9980, -1.9980, -1.9980],
          [-1.5528, -1.5528, -1.5528,  ..., -1.9980, -1.9809, -1.9809],
          [-1.5528, -1.5528, -1.5528,  ..., -1.9980, -1.9809, -1.9809]],

         [[ 1.3081,  1.3081,  1.3081,  ..., -1.8431, -1.8431, -1.8431],
          [ 1.3081,  1.3081,  1.3081,  ..., -1.8431, -1.8431, -1.8431],
          [ 1.3081,  1.3081,  1.3081,  ..., -1.8256, -1.8256, -1.8256],
          ...,
          [-1.3179, -1.3179, -1.3179,  ..., -1.8606, -1.8606, -1.8606],
          [-1.3004, -1.3004, -1.3004,  ..., -1.8606, -1.8431, -1.8431],
          [-1.3004, -1.3004, -1.3004,  ..., -1.8606, -1.8431, -1.8431]],

         [[ 1.4200,  1.4200,  1.4200,  ..., -1.6476, -1.6476, -1.6476],
          [ 1.4200,  1.4200,  1.4200,  ..., -1.6476, -1.6476, -1.6476],
          [ 1.4200,  1.4200,  1.4200,  ..., -1.6302, -1.6302, -1.6302],
          ...,
          [-1.0201, -1.0201, -1.0201,  ..., -1.5604, -1.5604, -1.5604],
          [-1.0027, -1.0027, -1.0027,  ..., -1.5604, -1.5430, -1.5430],
          [-1.0027, -1.0027, -1.0027,  ..., -1.5604, -1.5430, -1.5430]]]),
 'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         ...,
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1]]),
 'labels': {'size': tensor([800, 800]), 'image_id': tensor([756]), 'class_labels': tensor([4]), 'boxes': tensor([[0.7340, 0.6986, 0.3414, 0.5944]]), 'area': tensor([519544.4375]), 'iscrowd': tensor([0]), 'orig_size': tensor([480, 480])}}
```

----------------------------------------

TITLE: Transcribing Speech Using AutoModel in Python
DESCRIPTION: This snippet shows the process of loading a Whisper model using AutoProcessor and WhisperForConditionalGeneration for speech transcription. Dependencies include the datasets library and Hugging Face Transformers. The workflow involves processing audio input features, model prediction, and decoding outputs to transcribed text. The model is loaded onto CUDA for efficient processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/whisper.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
# pip install datasets
import torch
from datasets import load_dataset
from transformers import AutoProcessor, WhisperForConditionalGeneration

processor = AutoProcessor.from_pretrained(
    "openai/whisper-large-v3-turbo",
)
model = WhisperForConditionalGeneration.from_pretrained(
    "openai/whisper-large-v3-turbo",
    torch_dtype=torch.float16,
    device_map="auto",
    attn_implementation="sdpa"
).to("cuda")

ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
audio_sample = ds[0]["audio"]

input_features = processor(
    audio_sample["array"],
    sampling_rate=audio_sample["sampling_rate"],
    return_tensors="pt"
).input_features
input_features = input_features.to("cuda", dtype=torch.float16)

predicted_ids = model.generate(input_features, cache_implementation="static")
transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)
transcription[0]
```

----------------------------------------

TITLE: Initializing Text-to-Speech Pipeline with Bark Model
DESCRIPTION: Demonstrates how to create a text-to-speech pipeline using the Bark model for generating audio from text, including support for non-verbal communications
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/text-to-speech.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import pipeline

pipe = pipeline("text-to-speech", model="suno/bark-small")
text = "[clears throat] This is a test ... and I just took a long pause."
output = pipe(text)
```

----------------------------------------

TITLE: LlavaOnevision Model with BitsAndBytes Quantization
DESCRIPTION: This code snippet shows how to quantize the LlavaOnevision model using bitsandbytes to further reduce memory requirements. It configures quantization to 4-bit with NF4 quantization type and sets the compute dtype to float16. The `BitsAndBytesConfig` is passed to the `from_pretrained` method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llava_onevision.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
"from transformers import LlavaOnevisionForConditionalGeneration, BitsAndBytesConfig

# specify how to quantize the model
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=\"nf4\",
    bnb_4bit_compute_dtype=torch.float16,
)

model = LlavaOnevisionForConditionalGeneration.from_pretrained(model_id, quantization_config=quantization_config, device_map=\"auto\")"
```

----------------------------------------

TITLE: Loading Adapter Weights from Pre-trained Models in Python
DESCRIPTION: Loads adapter weights into a Transformer model after initializing from a pre-trained source, ensuring the necessary configuration files are present in the target directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/peft.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("google/gemma-7b")
model.load_adapter("klcsp/gemma7b-lora-alpaca-11-v1")

```

----------------------------------------

TITLE: Configuring Training Arguments for Model Fine-tuning
DESCRIPTION: Sets up training hyperparameters including batch size, learning rate, number of epochs, and Hub upload configuration using TrainingArguments.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/visual_question_answering.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
>>> from transformers import TrainingArguments

>>> repo_id = "MariaK/vilt_finetuned_200"

>>> training_args = TrainingArguments(
...     output_dir=repo_id,
...     per_device_train_batch_size=4,
...     num_train_epochs=20,
...     save_steps=200,
...     logging_steps=50,
...     learning_rate=5e-5,
...     save_total_limit=2,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )
```

----------------------------------------

TITLE: Downloading UCF101 Dataset Subset
DESCRIPTION: Fetches a subset of the UCF101 dataset from the Hugging Face Hub for model training and evaluation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/video_classification.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import hf_hub_download

>>> hf_dataset_identifier = "sayakpaul/ucf101-subset"
>>> filename = "UCF101_subset.tar.gz"
>>> file_path = hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type="dataset")
```

----------------------------------------

TITLE: Loading WNUT 17 Dataset in Python
DESCRIPTION: Demonstrates how to load the WNUT 17 dataset using the 'datasets' library. This step is crucial for preparing the data used in token classification tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/token_classification.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> wnut = load_dataset("wnut_17")
```

----------------------------------------

TITLE: Basic Text Generation with LLM
DESCRIPTION: Demonstrates basic text generation using the generate method and decoding the output tokens.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/llm_tutorial.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
generated_ids = model.generate(**model_inputs)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

----------------------------------------

TITLE: Mistral Chat Template Implementation
DESCRIPTION: Shows how to use chat templates with the Mistral model, demonstrating its specific formatting requirements including control tokens.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/chat_templating.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")

chat = [
  {"role": "user", "content": "Hello, how are you?"},
  {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
  {"role": "user", "content": "I'd like to show off how chat templating works!"},
]

tokenizer.apply_chat_template(chat, tokenize=False)
```

----------------------------------------

TITLE: Model Output with Attention Mask
DESCRIPTION: This snippet demonstrates how to use an `attention_mask` to ignore padding tokens in the `input_ids`. By providing an `attention_mask`, the model correctly computes the output, matching the expected result from the single sequence example.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/troubleshooting.md#_snippet_8

LANGUAGE: Python
CODE:
```
>>> attention_mask = torch.tensor([[1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0]])
>>> output = model(input_ids, attention_mask=attention_mask)
>>> print(output.logits)
tensor([[ 0.0082, -0.2307],
        [-0.1008, -0.4061]], grad_fn=<AddmmBackward0>)
```

----------------------------------------

TITLE: Controlling LLM Output Length
DESCRIPTION: Shows how to control the length of generated text using max_new_tokens parameter.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/llm_tutorial.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
model_inputs = tokenizer(["A sequence of numbers: 1, 2"], return_tensors="pt").to("cuda")

# By default, the output will contain up to 20 tokens
generated_ids = model.generate(**model_inputs)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

# Setting `max_new_tokens` allows you to control the maximum length
generated_ids = model.generate(**model_inputs, max_new_tokens=50)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

----------------------------------------

TITLE: Creating a Simple Web Server with Starlette and Transformers
DESCRIPTION: This Python code creates a simple web server using Starlette that loads a Transformers pipeline for masked language modeling. It defines routes for handling POST requests, queues incoming requests, and processes them in a separate thread using the pipeline. The `server_loop` function initializes the pipeline and processes the requests from the queue, while the `homepage` function retrieves input from the request body and returns JSON response.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_webserver.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
"from starlette.applications import Starlette
from starlette.responses import JSONResponse
from starlette.routing import Route
from transformers import pipeline
import asyncio

async def homepage(request):
    payload = await request.body()
    string = payload.decode("utf-8")
    response_q = asyncio.Queue()
    await request.app.model_queue.put((string, response_q))
    output = await response_q.get()
    return JSONResponse(output)

async def server_loop(q):
    pipe = pipeline(task="fill-mask",model="google-bert/bert-base-uncased")
    while True:
        (string, response_q) = await q.get()
        out = pipe(string)
        await response_q.put(out)

app = Starlette(
    routes=[
        Route("/", homepage, methods=["POST"]),
    ],
)

@app.on_event("startup")
async def startup_event():
    q = asyncio.Queue()
    app.model_queue = q
    asyncio.create_task(server_loop(q))"
```

----------------------------------------

TITLE: Loading a Pre-trained BERT Model for Sequence Classification
DESCRIPTION: Loads a pre-trained BERT model and configures it for sequence classification by specifying the number of labels needed for the Yelp Reviews classification task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/training.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=5)
```

----------------------------------------

TITLE: Configuring and Launching Distributed Training with Accelerate
DESCRIPTION: This sequence of commands shows the workflow for setting up and launching distributed training with the Accelerate library. It includes configuration, testing the setup, and launching the training script with the appropriate parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/README.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
accelerate config
```

LANGUAGE: bash
CODE:
```
accelerate test
```

LANGUAGE: bash
CODE:
```
accelerate launch run_summarization_no_trainer.py \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir ~/tmp/tst-summarization
```

----------------------------------------

TITLE: Verifying Transformers Installation with Sentiment Analysis
DESCRIPTION: Python command to verify the Transformers installation by running a sentiment analysis pipeline on a sample text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/installation.md#2025-04-22_snippet_8

LANGUAGE: bash
CODE:
```
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
```

----------------------------------------

TITLE: Initializing AutoModel for Image Feature Extraction
DESCRIPTION: Code to manually set up feature extraction using AutoImageProcessor and AutoModel classes instead of using the pipeline. This approach provides more flexibility.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_feature_extraction.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from transformers import AutoImageProcessor, AutoModel

processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
model = AutoModel.from_pretrained("google/vit-base-patch16-224").to(DEVICE)
```

----------------------------------------

TITLE: Loading Model with FlashAttention2 Implementation
DESCRIPTION: Load a causal language model using FlashAttention2 with bfloat16 data type
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_one.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.1-8B", device_map="auto", torch_dtype=torch.bfloat16, attn_implementation="flash_attention_2")
```

----------------------------------------

TITLE: Control Generation Length
DESCRIPTION: This code demonstrates how to control the maximum length of generated sequences using the `max_new_tokens` parameter. By default, the model generates up to 20 tokens. Setting `max_new_tokens` allows you to explicitly specify the maximum number of new tokens to generate.  The input text is "A sequence of numbers: 1, 2".
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/llm_tutorial.md#_snippet_6

LANGUAGE: python
CODE:
```
>>> model_inputs = tokenizer(["A sequence of numbers: 1, 2"], return_tensors="pt").to("cuda")

>>> # By default, the output will contain up to 20 tokens
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A sequence of numbers: 1, 2, 3, 4, 5'

>>> # Setting `max_new_tokens` allows you to control the maximum length
>>> generated_ids = model.generate(**model_inputs, max_new_tokens=50)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,'
```

----------------------------------------

TITLE: Creating an Automatic Speech Recognition Pipeline
DESCRIPTION: Code to initialize an automatic speech recognition pipeline using a pre-trained Italian model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/quicktour.md#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
>>> import torch
>>> from transformers import pipeline

>>> riconoscitore_vocale = pipeline(
...     "automatic-speech-recognition", model="radiogroup-crits/wav2vec2-xls-r-1b-italian-doc4lm-5gram"
... )
```

----------------------------------------

TITLE: Tokenizing Input with PyTorch
DESCRIPTION: Tokenizes the prompt and candidate answer pairs using a pre-trained tokenizer. It returns PyTorch tensors with padding. The `labels` tensor is created to represent the correct answer index (0 in this case).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/multiple_choice.md#_snippet_23

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_swag_model")
>>> inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors="pt", padding=True)
>>> labels = torch.tensor(0).unsqueeze(0)
```

----------------------------------------

TITLE: Creating an Optimizer
DESCRIPTION: Creates an `AdamW` optimizer for fine-tuning the model parameters with a specified learning rate.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/training.md#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

----------------------------------------

TITLE: Using OffloadedStaticCache for Memory-Efficient JIT-Optimized Generation in Python
DESCRIPTION: This snippet demonstrates the use of OffloadedStaticCache for memory-efficient, JIT-optimized generation. It initializes a tokenizer and model, then generates text using an offloaded static cache implementation, which moves most of the cache to CPU memory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/kv_cache.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf", torch_dtype=torch.float16, device_map="auto")
inputs = tokenizer("Hello, my name is", return_tensors="pt").to(model.device)

out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation="offloaded_static")
tokenizer.batch_decode(out, skip_special_tokens=True)[0]
```

----------------------------------------

TITLE: Tokenizing Text Data for Model Input
DESCRIPTION: Shows how to tokenize the dataset using a BERT tokenizer with padding and truncation to prepare the text data for input to the model, applying the tokenization function to the entire dataset at once.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/training.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")

>>> def tokenize_function(examples):
...     return tokenizer(examples["text"], padding="max_length", truncation=True)

>>> tokenized_datasets = dataset.map(tokenize_function, batched=True)
```

----------------------------------------

TITLE: Loading M2M100 with Flash Attention 2 - PyTorch
DESCRIPTION: This snippet demonstrates how to load the M2M100 model with Flash Attention 2 optimization enabled. It specifies the attention implementation while loading the model and sets the tensor precision to torch.float16 for improved performance. The snippet includes generating translations with the model for the specified inputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/m2m_100.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> import torch
>>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

>>> model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M", torch_dtype=torch.float16, attn_implementation="flash_attention_2").to("cuda").eval()
>>> tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M")

>>> # translate Hindi to French
>>> hi_text = "à¤œà¥€à¤µà¤¨ à¤à¤• à¤šà¥‰à¤•à¤²à¥‡à¤Ÿ à¤¬à¥‰à¤•à¥à¤¸ à¤•à¥€ à¤¤à¤°à¤¹ à¤¹à¥ˆà¥¤"
>>> tokenizer.src_lang = "hi"
>>> encoded_hi = tokenizer(hi_text, return_tensors="pt").to("cuda")
>>> generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id("fr"))
>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
"La vie est comme une boÃ®te de chocolat."
```

----------------------------------------

TITLE: Configuring Mixed Precision Training with bf16 - Python
DESCRIPTION: This snippet illustrates how to configure mixed precision training using the bf16 data type in the `TrainingArguments`. bf16 provides a larger dynamic range than fp16, which can help avoid overflow and underflow errors while maintaining training speed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
from transformers import TrainingArguments

args = TrainingArguments(
    per_device_train_batch_size=4,
    gradient_accumulation_steps=16,
    gradient_checkpointing=True,
    bf16=True,
)
```

----------------------------------------

TITLE: Tokenizing Batches of Sentences with Padding and Truncation - TensorFlow
DESCRIPTION: This snippet shows how to tokenize a batch of sentences using a tokenizer, with padding and truncation enabled. The `return_tensors` parameter is set to 'tf', indicating that TensorFlow tensors should be returned. The output includes `input_ids`, `token_type_ids`, and `attention_mask` tensors, formatted as TensorFlow tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/preprocessing.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```

>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="tf")
>>> print(encoded_input)
{'input_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
       [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
       [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],
      dtype=int32)>, 
 'token_type_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 
 'attention_mask': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>

```

----------------------------------------

TITLE: Setting Seeds for Reproducible Results in ML Tests
DESCRIPTION: Demonstrates how to set random seeds for Python, PyTorch, NumPy, and TensorFlow to achieve reproducible results in machine learning tests. This is essential for ensuring consistent test outcomes across different runs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/testing.md#2025-04-22_snippet_50

LANGUAGE: python
CODE:
```
seed = 42

# python RNG
import random

random.seed(seed)

# pytorch RNGs
import torch

torch.manual_seed(seed)
torch.backends.cudnn.deterministic = True
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# numpy RNG
import numpy as np

np.random.seed(seed)

# tf RNG
import tensorflow as tf 

tf.random.set_seed(seed)
```

----------------------------------------

TITLE: Calculating Model Accuracy - Python
DESCRIPTION: Creates a function to compute the accuracy of model predictions by comparing them against the actual labels. It leverages NumPy for processing the predictions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=labels)
```

----------------------------------------

TITLE: Pipeline Inference Setup
DESCRIPTION: Sets up text-to-speech pipeline and prepares input for inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/text-to-speech.md#2025-04-22_snippet_29

LANGUAGE: python
CODE:
```
from transformers import pipeline

pipe = pipeline("text-to-speech", model="YOUR_ACCOUNT_NAME/speecht5_finetuned_voxpopuli_nl")
text = "hallo allemaal, ik praat nederlands. groetjes aan iedereen!"
example = dataset["test"][304]
speaker_embeddings = torch.tensor(example["speaker_embeddings"]).unsqueeze(0)
forward_params = {"speaker_embeddings": speaker_embeddings}
output = pipe(text, forward_params=forward_params)
```

----------------------------------------

TITLE: Checking Attention Mask in Python
DESCRIPTION: This snippet shows the attention mask generated by the tokenizer. The attention mask indicates which tokens should be attended to (1) and which should be ignored (0). In this case, the padding tokens in the first sequence have a value of 0 in the attention mask.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/glossary.md#_snippet_4

LANGUAGE: python
CODE:
```
>>> padded_sequences["attention_mask"]
[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]
```

----------------------------------------

TITLE: Loading Pretrained TensorFlow Model with Custom Configuration
DESCRIPTION: Shows how to load a pretrained TensorFlow DistilBertModel but override its default configuration with a custom one, allowing for selective customization of model attributes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/create_a_model.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> tf_model = TFDistilBertModel.from_pretrained("distilbert/distilbert-base-uncased", config=my_config)
```

----------------------------------------

TITLE: Initializing YOLOS with Scaled Dot Product Attention
DESCRIPTION: This snippet demonstrates how to initialize a YOLOS model for object detection using Scaled Dot Product Attention (SDPA) in PyTorch, specifying the use of half-precision for performance optimization. Dependencies include transformers  and PyTorch (torch>=2.1.1). Key parameters include the model name ('hustvl/yolos-base') and attention implementation ('sdpa'). Inputs are existing model weights, and the output is an initialized model ready for inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/yolos.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import AutoModelForObjectDetection
model = AutoModelForObjectDetection.from_pretrained("hustvl/yolos-base", attn_implementation="sdpa", torch_dtype=torch.float16)
...
```

----------------------------------------

TITLE: Training Segmentation Models with Keras in TensorFlow
DESCRIPTION: Fine-tune a model in TensorFlow by defining hyperparameters, optimizer, and learning rate. Load pretrained model with TFAutoModelForSemanticSegmentation, convert dataset formats, and use fit method with callbacks to compute metrics and upload model to hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md#2025-04-22_snippet_19

LANGUAGE: py
CODE:
```
>>> from transformers import create_optimizer

>>> batch_size = 2
>>> num_epochs = 50
>>> num_train_steps = len(train_ds) * num_epochs
>>> learning_rate = 6e-5
>>> weight_decay_rate = 0.01

>>> optimizer, lr_schedule = create_optimizer(
...     init_lr=learning_rate,
...     num_train_steps=num_train_steps,
...     weight_decay_rate=weight_decay_rate,
...     num_warmup_steps=0,
... )

```

LANGUAGE: py
CODE:
```
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained(
...     checkpoint,
...     id2label=id2label,
...     label2id=label2id,
... )
>>> model.compile(optimizer=optimizer)  # No loss argument!

```

LANGUAGE: py
CODE:
```
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors="tf")

>>> tf_train_dataset = train_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )

>>> tf_eval_dataset = test_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )

```

LANGUAGE: py
CODE:
```
>>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback

>>> metric_callback = KerasMetricCallback(
...     metric_fn=compute_metrics, eval_dataset=tf_eval_dataset, batch_size=batch_size, label_cols=["labels"]
... )

>>> push_to_hub_callback = PushToHubCallback(output_dir="scene_segmentation", tokenizer=image_processor)

>>> callbacks = [metric_callback, push_to_hub_callback]

```

LANGUAGE: py
CODE:
```
>>> model.fit(
...     tf_train_dataset,
...     validation_data=tf_eval_dataset,
...     callbacks=callbacks,
...     epochs=num_epochs,
... )

```

----------------------------------------

TITLE: Filtering Problem Examples
DESCRIPTION: Removing problematic examples from the dataset where bounding boxes extend beyond image boundaries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/object_detection.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> remove_idx = [590, 821, 822, 875, 876, 878, 879]
>>> keep = [i for i in range(len(cppe5["train"])) if i not in remove_idx]
>>> cppe5["train"] = cppe5["train"].select(keep)
```

----------------------------------------

TITLE: Loading and Generating Text with Bamba in Python
DESCRIPTION: This code snippet demonstrates how to load the Bamba-9B model and its associated tokenizer using the `transformers` library.  It then shows how to use the model to generate text based on a given input prompt. The generated output is decoded and printed to the console.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bamba.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("ibm-fms/Bamba-9B")
tokenizer = AutoTokenizer.from_pretrained("ibm-fms/Bamba-9B")

message = ["Mamba is a snake with following properties  "]
inputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False)
response = model.generate(**inputs, max_new_tokens=64)
print(tokenizer.batch_decode(response, skip_special_tokens=True)[0])
```

----------------------------------------

TITLE: Continuing a Chat Conversation
DESCRIPTION: Demonstrates how to continue a conversation by appending a new user message to the previous chat history and generating another response using the same pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/conversations.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
chat = response[0]['generated_text']
chat.append(
    {"role": "user", "content": "Wait, what's so wild about soup cans?"}
)
response = pipe(chat, max_new_tokens=512)
print(response[0]['generated_text'][-1]['content'])
```

----------------------------------------

TITLE: Loading a Pre-trained Model for Token Classification
DESCRIPTION: This code snippet demonstrates how to reuse the same checkpoint to load a model architecture for a different task - token classification - using `AutoModelForTokenClassification.from_pretrained`. The underlying weights are the same but the head is reinitialized for the new task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/autoclass_tutorial.md#_snippet_8

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoModelForTokenClassification

>>> model = AutoModelForTokenClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Running Text Generation Pipeline in Python
DESCRIPTION: This example uses the pipeline function from the Transformers library to generate text using the model loaded in the previous step. The pipeline is configured for text generation using the given prompt and tokenizer, and the result is fetched with max_new_tokens set to 60.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

result = pipe(prompt, max_new_tokens=60)[0]["generated_text"][len(prompt):]
result
```

----------------------------------------

TITLE: Loading a Model-Specific Tokenizer in Python
DESCRIPTION: This snippet shows how to load a model-specific tokenizer directly from its respective class, either for the basic or fast implementation, demonstrating the ease of use of the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/fast_tokenizers.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
from transformers import GemmaTokenizer

tokenizer = GemmaTokenizer.from_pretrained("google/gemma-2-2b")
tokenizer("We are very happy to show you the ðŸ¤— Transformers library.", return_tensors="pt")
```

LANGUAGE: Python
CODE:
```
from transformers import GemmaTokenizerFast

tokenizer = GemmaTokenizerFast.from_pretrained("google/gemma-2-2b")
tokenizer("We are very happy to show you the ðŸ¤— Transformers library.", return_tensors="pt")
```

----------------------------------------

TITLE: Preparing Input for PyTorch Model for Inference
DESCRIPTION: This code snippet prepares the input image for the PyTorch model by processing it through an image processor and moving the pixel values to the appropriate device (GPU or CPU).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_25

LANGUAGE: python
CODE:
```
>>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # use GPU if available, otherwise use a CPU
>>> encoding = image_processor(image, return_tensors="pt")
>>> pixel_values = encoding.pixel_values.to(device)
```

----------------------------------------

TITLE: Perform Masked Language Modeling with AutoModel Python
DESCRIPTION: This example shows how to perform masked language modeling using the lower-level `AutoModelForMaskedLM` and `AutoTokenizer` APIs. It loads the tokenizer and model, tokenizes the input, performs a forward pass, identifies the masked token's index, finds the top prediction, and decodes it back into a token.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/modernbert.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForMaskedLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    "answerdotai/ModernBERT-base",
)
model = AutoModelForMaskedLM.from_pretrained(
    "answerdotai/ModernBERT-base",
    torch_dtype=torch.float16,
    device_map="auto",
    attn_implementation="sdpa"
)
inputs = tokenizer("Plants create [MASK] through a process known as photosynthesis.", return_tensors="pt").to("cuda")

with torch.no_grad():
    outputs = model(**inputs)
    predictions = outputs.logits

masked_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]
predicted_token_id = predictions[0, masked_index].argmax(dim=-1)
predicted_token = tokenizer.decode(predicted_token_id)

print(f"The predicted token is: {predicted_token}")
```

----------------------------------------

TITLE: Preprocessing Function for Question Answering Data
DESCRIPTION: Defines a function to preprocess the SQuAD dataset, including tokenization, handling long contexts, and mapping answer positions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/question_answering.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> def preprocess_function(examples):
...     questions = [q.strip() for q in examples["question"]]
...     inputs = tokenizer(
...         questions,
...         examples["context"],
...         max_length=384,
...         truncation="only_second",
...         return_offsets_mapping=True,
...         padding="max_length",
...     )

...     offset_mapping = inputs.pop("offset_mapping")
...     answers = examples["answers"]
...     start_positions = []
...     end_positions = []

...     for i, offset in enumerate(offset_mapping):
...         answer = answers[i]
...         start_char = answer["answer_start"][0]
...         end_char = answer["answer_start"][0] + len(answer["text"][0])
...         sequence_ids = inputs.sequence_ids(i)

...         # Find the start and end of the context
...         idx = 0
...         while sequence_ids[idx] != 1:
...             idx += 1
...         context_start = idx
...         while sequence_ids[idx] == 1:
...             idx += 1
...         context_end = idx - 1

...         # If the answer is not fully inside the context, label it (0, 0)
...         if offset[context_start][0] > end_char or offset[context_end][1] < start_char:
...             start_positions.append(0)
...             end_positions.append(0)
...         else:
...             # Otherwise it's the start and end token positions
...             idx = context_start
...             while idx <= context_end and offset[idx][0] <= start_char:
...                 idx += 1
...             start_positions.append(idx - 1)

...             idx = context_end
...             while idx >= context_start and offset[idx][1] >= end_char:
...                 idx -= 1
...             end_positions.append(idx + 1)

...     inputs["start_positions"] = start_positions
...     inputs["end_positions"] = end_positions
...     return inputs
```

----------------------------------------

TITLE: Batch Tokenization with TensorFlow Tensors
DESCRIPTION: This snippet shows how to tokenize a batch of texts with padding, truncation, and conversion to TensorFlow tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/quicktour.md#2025-04-23_snippet_7

LANGUAGE: python
CODE:
```
tf_batch = tokenizer(
    ["We are very happy to show you the ðŸ¤— Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf",
)
```

----------------------------------------

TITLE: Creating Smaller Dataset Subsets for Faster Experimentation
DESCRIPTION: Demonstrates how to create smaller subsets of the training and evaluation datasets to reduce execution time during development and experimentation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/training.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
>>> small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
```

----------------------------------------

TITLE: Initializing Data Collator for Sequence-to-Sequence Training
DESCRIPTION: Create a DataCollatorForSeq2Seq to efficiently handle dynamic padding and prepare batches for model training
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/translation.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
```

----------------------------------------

TITLE: Using Flash Attention 2 with GPT-NeoX
DESCRIPTION: Code for loading the GPT-NeoX model with Flash Attention 2 optimization. This implementation uses half-precision for better memory efficiency and speed while maintaining quality.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gpt_neox.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast

model = GPTNeoXForCausalLM.from_pretrained("EleutherAI/gpt-neox-20b", torch_dtype=torch.float16, attn_implementation="flash_attention_2").to(device)
...
```

----------------------------------------

TITLE: Batch Mixed Media Processing with Qwen2-VL
DESCRIPTION: Example demonstrating how to process multiple types of media (images, videos, text) in batch mode using Qwen2-VL.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/qwen2_vl.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
conversation1 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "path": "/path/to/image1.jpg"},
            {"type": "text", "text": "Describe this image."}
        ]
    }
]

conversation2 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "path": "/path/to/image2.jpg"},
            {"type": "image", "path": "/path/to/image3.jpg"},
            {"type": "text", "text": "What is written in the pictures?"}
        ]
    }
]

conversations = [conversation1, conversation2]
ipnuts = processor.apply_chat_template(
    conversations,
    video_fps=1,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt"
).to(model.device)

output_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]
output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
print(output_text)
```

----------------------------------------

TITLE: Post-processing Model Output for Audio Classification in Python
DESCRIPTION: This snippet post-processes the model's output logits to obtain the predicted class label. It uses `torch.argmax` to get the class with the highest probability and then maps the class ID to a label using the model's `id2label` configuration. The output is the predicted label for the audio file.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/audio_classification.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> import torch

>>> predicted_class_ids = torch.argmax(logits).item()
>>> predicted_label = model.config.id2label[predicted_class_ids]
>>> predicted_label
```

----------------------------------------

TITLE: Managing Multiple Adapters
DESCRIPTION: Example of adding and switching between multiple adapters
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/peft.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
# attach new adapter with same config
model.add_adapter(lora_config, adapter_name="adapter_2")

# use adapter_1
model.set_adapter("adapter_1")
output = model.generate(**inputs)
print(tokenizer.decode(output_disabled[0], skip_special_tokens=True))

# use adapter_2
model.set_adapter("adapter_2")
output_enabled = model.generate(**inputs)
print(tokenizer.decode(output_enabled[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Image Feature Extraction with I-JEPA in PyTorch
DESCRIPTION: This code snippet demonstrates how to use the I-JEPA model for extracting image features. It utilizes the `transformers` library to load the model and processor, processes input images from URLs, and computes the cosine similarity between their embeddings. The `infer` function encapsulates the processing and feature extraction logic using a trained I-JEPA model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/ijepa.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
import requests\nimport torch\nfrom PIL import Image\nfrom torch.nn.functional import cosine_similarity\n\nfrom transformers import AutoModel, AutoProcessor\n\nurl_1 = "http://images.cocodataset.org/val2017/000000039769.jpg"\nurl_2 = "http://images.cocodataset.org/val2017/000000219578.jpg"\nimage_1 = Image.open(requests.get(url_1, stream=True).raw)\nimage_2 = Image.open(requests.get(url_2, stream=True).raw)\n\nmodel_id = "facebook/ijepa_vith14_1k"\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = AutoModel.from_pretrained(model_id)\n\n@torch.no_grad()\ndef infer(image):\n    inputs = processor(image, return_tensors="pt")\n    outputs = model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1)\n\n\nembed_1 = infer(image_1)\nembed_2 = infer(image_2)\n\nsimilarity = cosine_similarity(embed_1, embed_2)\nprint(similarity)
```

----------------------------------------

TITLE: Saving TensorFlow Model Weights to H5
DESCRIPTION: This snippet demonstrates how to save TensorFlow model weights to a `.h5` file and then reload the model using `TFPreTrainedModel.from_pretrained`. This is a recommended approach to avoid issues when saving and loading entire TensorFlow models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/troubleshooting.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> from transformers import TFPreTrainedModel
>>> from tensorflow import keras

>>> model.save_weights("some_folder/tf_model.h5")
>>> model = TFPreTrainedModel.from_pretrained("some_folder")
```

----------------------------------------

TITLE: Generating Model Response from Tokenized Chat Using Python
DESCRIPTION: This code snippet demonstrates how to generate a response from the tokenized chat input by passing it to the `generate` method of the model. It illustrates the end-to-end process from tokenization to response generation, ensuring that the model answers based on the formatted input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
outputs = model.generate(tokenized_chat, max_new_tokens=128) 
print(tokenizer.decode(outputs[0]))
```

----------------------------------------

TITLE: NCCL Error Troubleshooting in Bash
DESCRIPTION: This code snippet shows the error message and explanation about NCCL. It provides a troubleshooting step for resolving NCCL-related errors when running DeepSeek-V3, indicating that the NCCL library might not be properly loaded.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/deepseek_v3.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
```bash
[rank0]: ncclInternalError: Internal check failed.
[rank0]: Last error:
[rank0]: Bootstrap : no socket interface found
```
```

----------------------------------------

TITLE: Logging into Hugging Face Hub
DESCRIPTION: This Python snippet allows users to log into their Hugging Face account, providing a token prompt for authentication which is necessary for model sharing and uploading.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Loading Large Models with Accelerate
DESCRIPTION: This code demonstrates how to load a large model using Accelerate to reduce memory usage. It utilizes `low_cpu_mem_usage=True` to initialize the model with empty weights and load the pre-trained weights afterwards. Requires accelerate and pytorch versions to be Accelerate >= 0.9.0, PyTorch >= 1.9.0.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/model.md#_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSeq2SeqLM

t0pp = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0pp", low_cpu_mem_usage=True)
```

----------------------------------------

TITLE: Mapping Preprocessing Function
DESCRIPTION: Applies the preprocessing function to the entire dataset using the `map` function. It also removes the original "audio" column and renames "intent_class" to "label" for compatibility with the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/audio_classification.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
">>> encoded_minds = minds.map(preprocess_function, remove_columns="audio", batched=True)
>>> encoded_minds = encoded_minds.rename_column("intent_class", "label")"
```

----------------------------------------

TITLE: Obtaining Depth Predictions from the Pipeline
DESCRIPTION: This snippet retrieves depth predictions from the previously set up pipeline, which includes the predicted depth tensor and a visualization image of the depth estimation result.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/monocular_depth_estimation.md#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
>>> predictions = pipe(image)
>>> predictions["depth"]
```

----------------------------------------

TITLE: Performing Translation with M2M100 Model
DESCRIPTION: This snippet demonstrates how to use the M2M100 model to translate Chinese text to English, including tokenization and generation steps.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/multilingual.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
encoded_zh = tokenizer(chinese_text, return_tensors="pt")
generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id("en"))
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
```

----------------------------------------

TITLE: Converting PyTorch Model to Flax
DESCRIPTION: Shows how to convert a PyTorch checkpoint to Flax format using the from_pt parameter.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/model_sharing.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> flax_model = FlaxDistilBertForSequenceClassification.from_pretrained(
...     "path/verso/il-nome-magnifico-che-hai-scelto", from_pt=True
... )
```

----------------------------------------

TITLE: Loading XLM Model and Tokenizer for Multilingual Inference
DESCRIPTION: This snippet demonstrates how to load an XLM model and tokenizer for multilingual inference, specifically using the 'FacebookAI/xlm-clm-enfr-1024' checkpoint for English-French causal language modeling.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/multilingual.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import XLMTokenizer, XLMWithLMHeadModel

tokenizer = XLMTokenizer.from_pretrained("FacebookAI/xlm-clm-enfr-1024")
model = XLMWithLMHeadModel.from_pretrained("FacebookAI/xlm-clm-enfr-1024")
```

----------------------------------------

TITLE: Loading LLaMA Model and Tokenizer in Python
DESCRIPTION: Python code snippet to load the LLaMA tokenizer and model after converting the weights to Hugging Face format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/llama.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import LlamaForCausalLM, LlamaTokenizer

tokenizer = LlamaTokenizer.from_pretrained("/output/path")
model = LlamaForCausalLM.from_pretrained("/output/path")
```

----------------------------------------

TITLE: Tokenizing Input Text for LLM
DESCRIPTION: Preprocesses input text using a tokenizer and prepares it for model inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/llm_tutorial.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("openlm-research/open_llama_7b")
model_inputs = tokenizer(["A list of colors: red, blue"], return_tensors="pt").to("cuda")
```

----------------------------------------

TITLE: Loading Pre-trained Models with TensorFlow in Python
DESCRIPTION: Shows how to download and use pre-trained models with TensorFlow. Similar to the PyTorch example, it initializes a tokenizer and model, but uses TensorFlow-specific model classes and tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/i18n/README_es.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, TFAutoModel

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> model = TFAutoModel.from_pretrained("google-bert/bert-base-uncased")

>>> inputs = tokenizer("Hello world!", return_tensors="tf")
>>> outputs = model(**inputs)
```

----------------------------------------

TITLE: Using Object Detection Pipeline in Python
DESCRIPTION: Code snippet showing how to load an image and use the object detection pipeline to identify objects in the image.
SOURCE: https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png"
image_data = requests.get(url, stream=True).raw
image = Image.open(image_data)

object_detector = pipeline('object-detection')
object_detector(image)
```

----------------------------------------

TITLE: Installing Transformers with pip
DESCRIPTION: Commands to install Transformers library and its dependencies using pip.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/installation.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
pip install transformers
```

LANGUAGE: bash
CODE:
```
pip install 'transformers[torch]'
```

LANGUAGE: bash
CODE:
```
pip install 'transformers[tf-cpu]'
```

LANGUAGE: bash
CODE:
```
pip install 'transformers[flax]'
```

----------------------------------------

TITLE: Loading ELI5 Dataset Subset in Python
DESCRIPTION: Loads a small subset of the ELI5 dataset's r/askscience subset using the Datasets library and splits it into train and test sets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/language_modeling.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> eli5 = load_dataset("eli5", split="train_asks[:5000]")
>>> eli5 = eli5.train_test_split(test_size=0.2)
```

----------------------------------------

TITLE: Loading Image Dataset for Vision Processing
DESCRIPTION: Loads a small sample of the food101 dataset for vision preprocessing demonstration. Uses the split parameter to limit the dataset size since the full dataset is large.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/preprocessing.md#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> dataset = load_dataset("food101", split="train[:100]")
```

----------------------------------------

TITLE: Analyzing Multiple Sentences with Sentiment Pipeline
DESCRIPTION: Code showing how to process multiple sentences at once with the sentiment analysis pipeline and iterate through the results.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/quicktour.md#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
>>> risultati = classificatore(
...     ["Siamo molto felici di mostrarti la libreria ðŸ¤— Transformers.", "Speriamo te non la odierai."]
... )
>>> for risultato in risultati:
...     print(f"etichetta: {risultato['label']}, con punteggio: {round(risultato['score'], 4)}")
etichetta: positive, con punteggio: 0.9998
etichetta: negative, con punteggio: 0.9998
```

----------------------------------------

TITLE: Loading Wav2Vec2 with Flash Attention 2
DESCRIPTION: This Python snippet demonstrates how to load a pre-trained Wav2Vec2 model from Hugging Face Transformers and enable Flash Attention 2. It also loads the model in half-precision (torch.float16) for reduced memory usage and faster inference. The model is then moved to the specified device.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/wav2vec2.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
">>> from transformers import Wav2Vec2Model\n\nmodel = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)\n..."
```

----------------------------------------

TITLE: Saving and loading the trained PEFT model
DESCRIPTION: Saves the trained PEFT adapter and then loads it back. This allows you to reuse the trained adapter later.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/peft.md#_snippet_13

LANGUAGE: python
CODE:
```
model.save_pretrained(save_dir)
model = AutoModelForCausalLM.from_pretrained(save_dir)
```

----------------------------------------

TITLE: Loading Base Model for Image Captioning in Python
DESCRIPTION: Leverage the AutoModelForCausalLM from transformers to load a pre-trained base model for fine-tuning on the image captioning task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_captioning.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Handling Padding of Input Sequences in Python
DESCRIPTION: This code snippet demonstrates how to pad input sequences of varying lengths to ensure they can be processed together in the same tensor. It uses the tokenizer's padding feature to achieve this and displays the padded sequences' input IDs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/glossary.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)

>>> padded_sequences["input_ids"]
[[101, 1188, 1110, 170, 1603, 4954, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1188, 1110, 170, 1897, 1263, 4954, 119, 1135, 1110, 1120, 1655, 2039, 1190, 1103, 4954, 138, 119, 102]]
```

----------------------------------------

TITLE: Converting Data to Tensors using TAPAS Tokenizer
DESCRIPTION: This snippet illustrates how to utilize the TAPAS Tokenizer to convert prepared dataset table-question pairs into a suitable tensor format required for the TAPAS model's fine-tuning process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/tapas.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> from transformers import TapasTokenizer

```

----------------------------------------

TITLE: Decoding Encoded Input IDs
DESCRIPTION: This Python code demonstrates how to decode the `input_ids` back into the original text using the tokenizer's `decode` method. This is useful for verifying the tokenization process and understanding how the text has been transformed into numerical representations. It also shows how special tokens like `CLS` and `SEP` are added by the tokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_3

LANGUAGE: python
CODE:
```
>>> tokenizer.decode(encoded_input["input_ids"])
'[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]'
```

----------------------------------------

TITLE: Using PushToHubCallback with Keras fit Method
DESCRIPTION: Demonstrates how to use the PushToHubCallback with Keras model.fit() to upload the model to Hub during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/model_sharing.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3, callbacks=push_to_hub_callback)
```

----------------------------------------

TITLE: Initializing Trainer for Image Captioning Model
DESCRIPTION: Creates a Trainer instance with the model, training arguments, datasets, and evaluation metrics for fine-tuning.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_captioning.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    compute_metrics=compute_metrics,
)
```

----------------------------------------

TITLE: Running distributed PyTorch pre-training for RoBERTa on Norwegian text
DESCRIPTION: Launches a distributed PyTorch training job for masked language modeling with RoBERTa on Norwegian text using 8 GPUs. Uses gradient accumulation to handle larger effective batch sizes, and configures various training parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/README.md#2025-04-22_snippet_18

LANGUAGE: bash
CODE:
```
python3 -m torch.distributed.launch --nproc_per_node ${NUM_GPUS} run_mlm.py \
    --output_dir="${MODEL_DIR}" \
    --model_type="roberta" \
    --config_name="${MODEL_DIR}" \
    --tokenizer_name="${MODEL_DIR}" \
    --dataset_name="oscar" \
    --dataset_config_name="unshuffled_deduplicated_no" \
    --max_seq_length="128" \
    --weight_decay="0.01" \
    --per_device_train_batch_size="32" \
    --per_device_eval_batch_size="32" \
    --gradient_accumulation="4" \
    --learning_rate="3e-4" \
    --warmup_steps="1000" \
    --overwrite_output_dir \
    --num_train_epochs="18" \
    --adam_beta1="0.9" \
    --adam_beta2="0.98" \
    --do_train \
    --do_eval \
    --logging_steps="500" \
    --eval_strategy="steps" \
    --report_to="tensorboard" \
    --save_strategy="no"
```

----------------------------------------

TITLE: Loading Tokenizer for Masked Language Modeling
DESCRIPTION: Initializes the DistilRoBERTa tokenizer for processing text data in masked language modeling tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/language_modeling.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert/distilroberta-base")
```

----------------------------------------

TITLE: BERT Tokenizer Implementation
DESCRIPTION: Example showing how to use BERT tokenizer from the transformers library
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tokenizer_summary.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")
tokenizer.tokenize("I have a new GPU!")
```

----------------------------------------

TITLE: Loading data using datasets library
DESCRIPTION: Loads the CoLA dataset from the GLUE benchmark using the `datasets` library. This dataset is used for binary text classification. It retrieves the training split of the dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/training.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from datasets import load_dataset

dataset = load_dataset("glue", "cola")
dataset = dataset["train"]  # Just take the training split for now
```

----------------------------------------

TITLE: Performing Batch Inference with Qwen2-Audio-7B-Instruct Model in Python
DESCRIPTION: This code demonstrates how to perform batch inference using the Qwen2-Audio-7B-Instruct model. It processes multiple conversations containing both audio and text inputs, then generates appropriate responses. The example shows how to handle different audio file formats from URLs and structure conversations with multiple turns.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2_audio.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from io import BytesIO
from urllib.request import urlopen
import librosa
from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor

processor = AutoProcessor.from_pretrained("Qwen/Qwen2-Audio-7B-Instruct")
model = Qwen2AudioForConditionalGeneration.from_pretrained("Qwen/Qwen2-Audio-7B-Instruct", device_map="auto")

conversation1 = [
    {"role": "user", "content": [
        {"type": "audio", "audio_url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3"},
        {"type": "text", "text": "What's that sound?"},
    ]},
    {"role": "assistant", "content": "It is the sound of glass shattering."},
    {"role": "user", "content": [
        {"type": "audio", "audio_url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav"},
        {"type": "text", "text": "What can you hear?"},
    ]}
]

conversation2 = [
    {"role": "user", "content": [
        {"type": "audio", "audio_url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac"},
        {"type": "text", "text": "What does the person say?"},
    ]},
]

conversations = [conversation1, conversation2]

text = [processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) for conversation in conversations]

audios = []
for conversation in conversations:
    for message in conversation:
        if isinstance(message["content"], list):
            for ele in message["content"]:
                if ele["type"] == "audio":
                    audios.append(
                        librosa.load(
                            BytesIO(urlopen(ele['audio_url']).read()),
                            sr=processor.feature_extractor.sampling_rate)[0]
                    )

inputs = processor(text=text, audios=audios, return_tensors="pt", padding=True)
inputs['input_ids'] = inputs['input_ids'].to("cuda")
inputs.input_ids = inputs.input_ids.to("cuda")

generate_ids = model.generate(**inputs, max_length=256)
generate_ids = generate_ids[:, inputs.input_ids.size(1):]

response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
```

----------------------------------------

TITLE: Performing Image Segmentation with Transformers Pipeline in Python
DESCRIPTION: This code example shows how to use Hugging Face pipeline for image segmentation. It loads a default segmentation model to analyze an image, identifying different segments in the image at the pixel level and returning labels with confidence scores for each identified segment.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/task_summary.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> segmenter = pipeline(task="image-segmentation")
>>> preds = segmenter(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> print(*preds, sep="\n")
{'score': 0.9879, 'label': 'LABEL_184'}
{'score': 0.9973, 'label': 'snow'}
{'score': 0.9972, 'label': 'cat'}
```

----------------------------------------

TITLE: Reconnaissance vocale automatique avec Whisper et pipeline Transformers
DESCRIPTION: Utilise le pipeline de reconnaissance vocale automatique avec le modÃ¨le whisper-small pour transcrire un fichier audio en texte. Le code charge le pipeline, traite un fichier audio distant et affiche la transcription texte.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/task_summary.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition", model="openai/whisper-small")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

----------------------------------------

TITLE: Padding Configuration Example - Python
DESCRIPTION: Demonstration of correct padding configuration for batch processing with LLMs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/llm_tutorial.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
# The tokenizer initialized above has right-padding active by default: the 1st sequence,
# which is shorter, has padding on the right side. Generation fails.
model_inputs = tokenizer(
    ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
).to("cuda")
generated_ids = model.generate(**model_inputs)
tokenizer.batch_decode(generated_ids[0], skip_special_tokens=True)[0]

# With left-padding, it works as expected!
tokenizer = AutoTokenizer.from_pretrained("openlm-research/open_llama_7b", padding_side="left")
tokenizer.pad_token = tokenizer.eos_token  # Llama has no pad token by default
model_inputs = tokenizer(
    ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
).to("cuda")
generated_ids = model.generate(**model_inputs)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

----------------------------------------

TITLE: Performing Zero-shot Image Classification with Pipeline
DESCRIPTION: Code demonstrating how to use the classifier pipeline to predict image labels from a set of candidate categories without prior training on those specific labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/zero_shot_image_classification.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> predictions = classifier(image, candidate_labels=["fox", "bear", "seagull", "owl"])
>>> predictions
[{'score': 0.9996670484542847, 'label': 'owl'},
 {'score': 0.000199399160919711, 'label': 'seagull'},
 {'score': 7.392891711788252e-05, 'label': 'fox'},
 {'score': 5.96074532950297e-05, 'label': 'bear'}]
```

----------------------------------------

TITLE: Configuring Training Arguments and Trainer for Question Answering in PyTorch
DESCRIPTION: This code sets up the training arguments and initializes the Trainer for fine-tuning a question answering model. It includes hyperparameters like learning rate, batch size, and number of epochs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/question_answering.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
training_args = TrainingArguments(
    output_dir="my_awesome_qa_model",
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_squad["train"],
    eval_dataset=tokenized_squad["test"],
    processing_class=tokenizer,
    data_collator=data_collator,
)

trainer.train()
```

----------------------------------------

TITLE: Batch Sentiment Analysis with Pipeline in Python
DESCRIPTION: Performs sentiment analysis on multiple input strings using the pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/quicktour.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> results = classifier(["ðŸ¤— Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã”ç´¹ä»‹ã§ãã¦éžå¸¸ã«å¬‰ã—ã„ã§ã™ã€‚", "å«Œã„ã«ãªã‚‰ãªã„ã§ã»ã—ã„ã§ã™ã€‚"])
>>> for result in results:
...     print(f"label: {result['label']}, ã‚¹ã‚³ã‚¢: {round(result['score'], 4)}")
label: POSITIVE, ã‚¹ã‚³ã‚¢: 0.9998
label: NEGATIVE, ã‚¹ã‚³ã‚¢: 0.5309
```

----------------------------------------

TITLE: Performing Speech Recognition on Dataset
DESCRIPTION: Extracts the raw waveform arrays from the first 4 samples of the dataset and passes them as a list to the speech recognition pipeline. The transcribed texts are then printed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/quicktour.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
">>> result = speech_recognizer(dataset[:4]["audio"])\n>>> print([d["text"] for d in result])\n['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', "FONDERING HOW I'D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE", "I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS", 'HOW DO I FURN A JOINA COUT']"
```

----------------------------------------

TITLE: Loading Wav2Vec2 Processor
DESCRIPTION: Initializes the Wav2Vec2 processor from the pre-trained 'facebook/wav2vec2-base' model for audio signal processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/asr.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base")
```

----------------------------------------

TITLE: Pushing model to Hub with Trainer
DESCRIPTION: This Python code demonstrates how to push a model to the Hugging Face Hub using the `Trainer` class. It configures `TrainingArguments` with `push_to_hub=True` and then uses `trainer.push_to_hub()` to upload the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(output_dir=\"my-awesome-model\", push_to_hub=True)\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\ntrainer.push_to_hub()"
```

----------------------------------------

TITLE: Truncating Sentences in a Batch with Python
DESCRIPTION: This snippet demonstrates how to truncate sentences in a batch to a maximum length. Setting `truncation=True` shortens sentences that exceed the model's maximum input length. This prevents errors caused by excessively long sequences.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/preprocessing.md#_snippet_5

LANGUAGE: Python
CODE:
```
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)
>>> print(encoded_input)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0], 
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], 
               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]], 
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}
```

----------------------------------------

TITLE: Training BERT Encoder-Decoder Model with Custom Input
DESCRIPTION: Demonstrates how to initialize a BERT encoder-decoder model, configure tokenization, and compute loss using custom input text. Shows the complete workflow from tokenization to training with proper token ID configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/encoder-decoder.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import BertTokenizer, EncoderDecoderModel

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> model = EncoderDecoderModel.from_encoder_decoder_pretrained("google-bert/bert-base-uncased", "google-bert/bert-base-uncased")

>>> model.config.decoder_start_token_id = tokenizer.cls_token_id
>>> model.config.pad_token_id = tokenizer.pad_token_id

>>> input_ids = tokenizer(
...     "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side.During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was  finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft).Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.",
...     return_tensors="pt",
... ).input_ids

>>> labels = tokenizer(
...     "the eiffel tower surpassed the washington monument to become the tallest structure in the world. it was the first structure to reach a height of 300 metres in paris in 1930. it is now taller than the chrysler building by 5. 2 metres ( 17 ft ) and is the second tallest free - standing structure in paris.",
...     return_tensors="pt",
... ).input_ids

>>> # the forward function automatically creates the correct decoder_input_ids
>>> loss = model(input_ids=input_ids, labels=labels).loss
```

----------------------------------------

TITLE: Configuring DeepSpeed ZeRO-3 with Auto Settings (JSON)
DESCRIPTION: This JSON configuration file illustrates a DeepSpeed ZeRO-3 setup using auto settings.  It demonstrates the configuration parameters for ZeRO-3, including optimizer offloading, parameter offloading, and related settings with automatic assignment.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/deepspeed.md#2025-04-22_snippet_30

LANGUAGE: json
CODE:
```
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": "auto",
            "betas": "auto",
            "eps": "auto",
            "weight_decay": "auto"
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },

    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    },

    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 2000,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false
}
```

----------------------------------------

TITLE: Running Hyperparameter Search with Trainer in Python
DESCRIPTION: Example of running hyperparameter search using the Trainer API with Optuna backend and custom objective function.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/hpo_train.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
best_trial = trainer.hyperparameter_search(
    direction="maximize",
    backend="optuna",
    hp_space=optuna_hp_space,
    n_trials=20,
    compute_objective=compute_objective,
)
```

----------------------------------------

TITLE: Installing Datasets Library for Working with Datasets
DESCRIPTION: Command to install the Hugging Face Datasets library which is needed for loading and working with datasets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/quicktour.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
pip install datasets
```

----------------------------------------

TITLE: Installing TensorFlow using Bash
DESCRIPTION: Installation command to add the TensorFlow library to the Python environment. TensorFlow is a prerequisite ML framework for using Transformers with TensorFlow models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md#2025-04-22_snippet_2

LANGUAGE: Bash
CODE:
```
!pip install tensorflow
```

----------------------------------------

TITLE: Inference with Zamba2 using Transformers
DESCRIPTION: This code snippet demonstrates how to perform inference with the Zamba2-7B model using the `transformers` library. It initializes the tokenizer and model, prepares input text, generates output, and decodes the output text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/zamba2.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
"from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

tokenizer = AutoTokenizer.from_pretrained("Zyphra/Zamba2-7B")
model = AutoModelForCausalLM.from_pretrained("Zyphra/Zamba2-7B", device_map="cuda", torch_dtype=torch.bfloat16)

input_text = "What factors contributed to the fall of the Roman Empire?"
input_ids = tokenizer(input_text, return_tensors="pt").to("cuda")

outputs = model.generate(**input_ids, max_new_tokens=100)
print(tokenizer.decode(outputs[0]))"
```

----------------------------------------

TITLE: Tokenizing Multiple Sentences as a Batch
DESCRIPTION: Shows how to tokenize multiple sentences as a batch by passing a list of sentences to the tokenizer, which returns input_ids, token_type_ids, and attention_mask for each sentence.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/preprocessing.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_inputs = tokenizer(batch_sentences)
>>> print(encoded_inputs)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102], 
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], 
               [101, 1327, 1164, 5450, 23434, 136, 102]], 
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0]], 
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], 
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
                    [1, 1, 1, 1, 1, 1, 1]]}
```

----------------------------------------

TITLE: Run summarization script with PyTorch
DESCRIPTION: This code executes the `run_summarization.py` script in the PyTorch examples directory for fine-tuning a T5-small model on the CNN/DailyMail dataset for summarization. It sets various training and evaluation parameters, including batch sizes, output directory, and dataset configuration. It also includes the `source_prefix` argument required for T5 models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/run_scripts.md#_snippet_3

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Enabling PEFT Adapters in a Model
DESCRIPTION: Code to load a pre-trained model, add a PEFT adapter configuration, and explicitly enable the adapter for text generation, demonstrating the adapter activation process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/peft.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import PeftConfig

model_id = "facebook/opt-350m"
adapter_model_id = "ybelkada/opt-350m-lora"
tokenizer = AutoTokenizer.from_pretrained(model_id)
text = "Hello"
inputs = tokenizer(text, return_tensors="pt")

model = AutoModelForCausalLM.from_pretrained(model_id)
peft_config = PeftConfig.from_pretrained(adapter_model_id)

# to initiate with random weights
peft_config.init_lora_weights = False

model.add_adapter(peft_config)
model.enable_adapters()
output = model.generate(**inputs)
```

----------------------------------------

TITLE: Converting to PyTorch Tensors
DESCRIPTION: Tokenizes, pads, truncates, and converts a batch of sentences to PyTorch tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/preprocessing.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="pt")
print(encoded_input)
```

----------------------------------------

TITLE: Basic Chat Template Usage with BlenderBot
DESCRIPTION: Demonstrates how to use chat templates with BlenderBot model to format a simple conversation into model input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/chat_templating.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer
>>> tokenizer = AutoTokenizer.from_pretrained("facebook/blenderbot-400M-distill")

>>> chat = [
...    {"role": "user", "content": "Hello, how are you?"},
...    {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
...    {"role": "user", "content": "I'd like to show off how chat templating works!"},
... ]

>>> tokenizer.apply_chat_template(chat, tokenize=False)
" Hello, how are you?  I'm doing great. How can I help you today?   I'd like to show off how chat templating works!</s>"
```

----------------------------------------

TITLE: Combining AWQ Quantization with FlashAttention-2
DESCRIPTION: Python code to load an AWQ quantized model with FlashAttention-2 enabled for additional inference acceleration. This combines two optimization techniques for maximum speed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/awq.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("TheBloke/zephyr-7B-alpha-AWQ", attn_implementation="flash_attention_2", device_map="cuda:0")
```

----------------------------------------

TITLE: Named Entity Recognition Pipeline using Hugging Face Transformers
DESCRIPTION: This snippet demonstrates the use of the token classification pipeline for Named Entity Recognition (NER). It identifies and labels entities such as organizations, locations, and misc in a given text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/task_summary.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from transformers import pipeline

classifier = pipeline(task="ner")
preds = classifier("Hugging Face is a French company based in New York City.")
preds = [
    {
        "entity": pred["entity"],
        "score": round(pred["score"], 4),
        "index": pred["index"],
        "word": pred["word"],
        "start": pred["start"],
        "end": pred["end"],
    }
    for pred in preds
]
print(*preds, sep="\n")
```

----------------------------------------

TITLE: Tokenizing Multiple Sentences
DESCRIPTION: Demonstrates tokenizing a batch of sentences using a tokenizer and displaying the encoded inputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/preprocessing.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_inputs = tokenizer(batch_sentences)
print(encoded_inputs)
```

----------------------------------------

TITLE: Configuring 4-bit Quantization with Double Quantization in Python
DESCRIPTION: This code sets up nested quantization (double quantization) for 4-bit models using BitsAndBytesConfig. It enables further memory savings without additional performance cost.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-23_snippet_16

LANGUAGE: python
CODE:
```
from transformers import BitsAndBytesConfig

double_quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
)

model_double_quant = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-13b-chat-hf", torch_dtype="auto", quantization_config=double_quant_config)
```

----------------------------------------

TITLE: Creating a Sentiment Analysis Pipeline with Custom Model
DESCRIPTION: Code to create a sentiment analysis pipeline using the previously loaded model and tokenizer, and test it on a French text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/quicktour.md#2025-04-22_snippet_12

LANGUAGE: Python
CODE:
```
>>> classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
>>> classifier("Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ðŸ¤— Transformers.")
[{'label': '5 stars', 'score': 0.7273}]
```

----------------------------------------

TITLE: Verifying Transformers Installation with Python
DESCRIPTION: Python command to verify the installation of Transformers by running a sentiment analysis pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/installation.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
```

----------------------------------------

TITLE: Push TensorFlow Model to Hub - Python
DESCRIPTION: This snippet shows how to push a TensorFlow version of a model to the Hub using the `push_to_hub` method, allowing users to access both PyTorch and TensorFlow versions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/model_sharing.md#_snippet_15

LANGUAGE: python
CODE:
```
>>> tf_model.push_to_hub("my-awesome-model")
```

----------------------------------------

TITLE: Using Vision Classification Pipeline in Python
DESCRIPTION: This snippet demonstrates how to use a vision classification pipeline to classify an image, showing how to pass an image URL and process the results.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/pipeline_tutorial.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> vision_classifier = pipeline(model="google/vit-base-patch16-224")
>>> preds = vision_classifier(
...     images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.4335, 'label': 'lynx, catamount'}, {'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.0239, 'label': 'Egyptian cat'}, {'score': 0.0229, 'label': 'tiger cat'}]
```

----------------------------------------

TITLE: Loading a Pre-trained Model for Token Classification with AutoModel in PyTorch
DESCRIPTION: This code snippet shows how to load a pre-trained model for token classification using `AutoModelForTokenClassification.from_pretrained` from the `transformers` library. It demonstrates the ability to reuse the same checkpoint ('distilbert/distilbert-base-uncased') to load a model for a different task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/autoclass_tutorial.md#_snippet_5

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoModelForTokenClassification

>>> model = AutoModelForTokenClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Loading Pre-trained BERT Model for Sequence Classification in Python
DESCRIPTION: This code loads a pre-trained BERT model and configures it for sequence classification with 5 labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/training.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=5)
```

----------------------------------------

TITLE: Training a TensorFlow Model with Keras API
DESCRIPTION: Demonstrates how to load a pretrained model for TensorFlow, compile it with an appropriate optimizer, and train it using the Keras fit method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/training.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
from transformers import TFAutoModelForSequenceClassification
from tensorflow.keras.optimizers import Adam

# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã™ã‚‹
model = TFAutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased")
# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯é€šå¸¸ã€å­¦ç¿’çŽ‡ã‚’ä¸‹ã’ã‚‹ã¨è‰¯ã„ã§ã™
model.compile(optimizer=Adam(3e-5))  # æå¤±é–¢æ•°ã®æŒ‡å®šã¯ä¸è¦ã§ã™ï¼

model.fit(tokenized_data, labels)
```

----------------------------------------

TITLE: Initializing Accelerator in Python
DESCRIPTION: This code initializes the Accelerator object, which automatically detects the distributed setup and initializes necessary training components.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/accelerate.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from accelerate import Accelerator

accelerator = Accelerator()
```

----------------------------------------

TITLE: Sentiment Analysis of Multiple Inputs with Transformers Pipeline in Python
DESCRIPTION: This code snippet shows how to perform sentiment analysis on a list of input texts using the `pipeline` function.  It iterates through the results and prints the label and score for each input text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_5

LANGUAGE: python
CODE:
```
>>> results = classifier(["We are very happy to show you the ðŸ¤— Transformers library.", "We hope you don't hate it."])
>>> for result in results:
...     print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
label: POSITIVE, with score: 0.9998
label: NEGATIVE, with score: 0.5309
```

----------------------------------------

TITLE: Loading a Pre-trained Feature Extractor with AutoFeatureExtractor in Python
DESCRIPTION: This code shows how to load a pre-trained audio feature extractor using AutoFeatureExtractor, which processes audio signals into the correct format for speech and audio models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/autoclass_tutorial.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained(
...     "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
... )
```

----------------------------------------

TITLE: Logging in to Hugging Face Hub
DESCRIPTION: Code snippet for logging into the Hugging Face Hub to enable sharing and uploading models to the community.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/language_modeling.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Handling Padding in LLM Text Generation
DESCRIPTION: This Python code snippet demonstrates the importance of correct padding when working with LLMs, specifically using left-padding for proper text generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/llm_tutorial.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> # The tokenizer initialized above has right-padding active by default: the 1st sequence,
>>> # which is shorter, has padding on the right side. Generation fails.
>>> model_inputs = tokenizer(
...     ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
... ).to("cuda")
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids[0], skip_special_tokens=True)[0]
''

>>> # With left-padding, it works as expected!
>>> tokenizer = AutoTokenizer.from_pretrained("openlm-research/open_llama_7b", padding_side="left")
>>> tokenizer.pad_token = tokenizer.eos_token  # Llama has no pad token by default
>>> model_inputs = tokenizer(
...     ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
... ).to("cuda")
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'1, 2, 3, 4, 5, 6,'
```

----------------------------------------

TITLE: Logging into Hugging Face Hub for Model Sharing
DESCRIPTION: Python code to login to Hugging Face Hub from a notebook environment, allowing users to share their fine-tuned models with the community.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/sequence_classification.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Hugging Face Login Setup
DESCRIPTION: Code to login to Hugging Face Hub for model sharing capabilities.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/multiple_choice.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
from huggingface_hub import notebook_login

notebook_login()
```

----------------------------------------

TITLE: Loading T5 Tokenizer for Summarization
DESCRIPTION: Initializes an AutoTokenizer with a T5 checkpoint for text summarization tasks. The tokenizer will be used to process text and summary data.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

checkpoint = "google-t5/t5-small"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Using Pipeline on a Large Dataset with Iterator in Python
DESCRIPTION: This snippet demonstrates how to use a pipeline to process a large dataset efficiently using an iterator, which allows for memory-efficient processing of data.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/pipeline_tutorial.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
def data():
    for i in range(1000):
        yield f"My example {i}"


pipe = pipeline(model="openai-community/gpt2", device=0)
generated_characters = 0
for out in pipe(data()):
    generated_characters += len(out[0]["generated_text"])
```

----------------------------------------

TITLE: Training a TensorFlow Model with Keras fit Method
DESCRIPTION: Executes the fine-tuning process for the TensorFlow model using the Keras fit method. Training runs for the specified number of epochs with the provided callbacks for metrics tracking and model saving.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_22

LANGUAGE: python
CODE:
```
>>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)
```

----------------------------------------

TITLE: Performing Text Summarization with Transformers Pipeline - Python
DESCRIPTION: This snippet illustrates using the Hugging Face Transformers `pipeline` for text summarization through prompting. It configures the pipeline with a Mistral model, creates a prompt that includes the text to be summarized and an instruction for summarization, runs the pipeline with sampling, and prints the generated summary.

Dependencies: `transformers`, `torch`.
Inputs: A prompt string containing the text to summarize and the summarization instruction.
Outputs: A string containing the summary of the text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/prompting.md#_snippet_6

LANGUAGE: Python
CODE:
```
from transformers import pipeline
import torch

pipeline = pipeline(model="mistralai/Mistral-7B-Instruct-v0.1", torch_dtype=torch.bfloat16, device_map="auto")
prompt = """Permaculture is a design process mimicking the diversity, functionality and resilience of natural ecosystems. The principles and practices are drawn from traditional ecological knowledge of indigenous cultures combined with modern scientific understanding and technological innovations. Permaculture design provides a framework helping individuals and communities develop innovative, creative and effective strategies for meeting basic needs while preparing for and mitigating the projected impacts of climate change.
Write a summary of the above text.
Summary:
"""

outputs = pipeline(prompt, max_new_tokens=30, do_sample=True, top_k=10, return_full_text=False)
for output in outputs:
    print(f"Result: {output['generated_text']}")
```

----------------------------------------

TITLE: Authenticating with Hugging Face Hub
DESCRIPTION: This code snippet logs into the Hugging Face Hub to allow uploading and sharing models with the community.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/sequence_classification.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Drawing Keypoints and Links on Image in Python
DESCRIPTION: This code iterates through the detected pose results and draws the keypoints and links on the input image using the defined `draw_points` and `draw_links` functions. It converts the image to a numpy array, retrieves keypoints and scores from each pose result, and then applies the drawing functions with specified color palettes and thresholds. The resulting image is then converted back to a PIL Image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/vitpose.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
numpy_image = np.array(image)

for pose_result in image_pose_result:
    scores = np.array(pose_result["scores"])
    keypoints = np.array(pose_result["keypoints"])

    # draw each point on image
    draw_points(numpy_image, keypoints, scores, keypoint_colors, keypoint_score_threshold=0.3, radius=4, show_keypoint_weight=False)

    # draw links
    draw_links(numpy_image, keypoints, scores, keypoint_edges, link_colors, keypoint_score_threshold=0.3, thickness=1, show_keypoint_weight=False)

pose_image = Image.fromarray(numpy_image)
pose_image
```

----------------------------------------

TITLE: Launching Training Script
DESCRIPTION: Command to launch the distributed training script using Accelerate CLI with specific number of processes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.md#2025-04-22_snippet_8

LANGUAGE: bash
CODE:
```
accelerate launch --num_processes=2 your_script.py
```

----------------------------------------

TITLE: Pipeline with GPU Device Configuration
DESCRIPTION: Demonstrates how to configure pipeline to use specific GPU devices and automatic device mapping.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/pipeline_tutorial.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
transcriber = pipeline(model="openai/whisper-large-v2", device=0)

# Using device_map for automatic allocation
transcriber = pipeline(model="openai/whisper-large-v2", device_map="auto")
```

----------------------------------------

TITLE: Authentication and Model Checkpoint Setup
DESCRIPTION: Code to login to HuggingFace Hub and set up the model checkpoint variable
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/visual_question_answering.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()

>>> model_checkpoint = "dandelin/vilt-b32-mlm"
```

----------------------------------------

TITLE: Multi-node DeepSpeed deployment with torch.distributed.run
DESCRIPTION: Command to deploy DeepSpeed across multiple nodes using PyTorch's distributed run launcher with node configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/deepspeed.md#2025-04-22_snippet_12

LANGUAGE: bash
CODE:
```
python -m torch.distributed.run --nproc_per_node=8 --nnode=2 --node_rank=0 --master_addr=hostname1 \
--master_port=9901 your_program.py <normal cl args> --deepspeed ds_config.json
```

----------------------------------------

TITLE: Loading a Pre-trained Model for Sequence Classification with TensorFlow
DESCRIPTION: Initializes a DistilBERT model for sequence classification using TensorFlow's TFAutoModelForSequenceClassification class. The model is configured with label mappings for sentiment analysis.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
>>> from transformers import TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained(
...     "distilbert/distilbert-base-uncased", num_labels=2, id2label=id2label, label2id=label2id
... )
```

----------------------------------------

TITLE: Tokenizing Input Text and Analyzing Token Transformations
DESCRIPTION: Shows how the tokenizer processes input tokens, demonstrating that words may be split into subwords and special tokens are added.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/token_classification.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> example = wnut["train"][0]
>>> tokenized_input = tokenizer(example["tokens"], is_split_into_words=True)
>>> tokens = tokenizer.convert_ids_to_tokens(tokenized_input["input_ids"])
>>> tokens
['[CLS]', '@', 'paul', '##walk', 'it', "'", 's', 'the', 'view', 'from', 'where', 'i', "'", 'm', 'living', 'for', 'two', 'weeks', '.', 'empire', 'state', 'building', '=', 'es', '##b', '.', 'pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]']
```

----------------------------------------

TITLE: Using Scaled Dot Product Attention with PyTorch
DESCRIPTION: This Python snippet illustrates loading the OPT model with scaled dot-product attention (SDPA) by specifying 'attn_implementation="sdpa"'. Ensure PyTorch version 2.1.1 or higher for optimized SDPA utilization. It's recommended to use half-precision for better performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/opt.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import OPTForCausalLM
model = OPTForCausalLM.from_pretrained("facebook/opt-350m", torch_dtype=torch.float16, attn_implementation="sdpa")
...
```

----------------------------------------

TITLE: Classifying Audio with Transformers in Python
DESCRIPTION: This snippet demonstrates how to use the ðŸ¤— Transformers library to perform audio classification. It initializes a classifier pipeline with a pre-trained model and processes an audio file from a URL to classify its content into predefined categories, returning the classification score and labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/task_summary.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline(task="audio-classification", model="superb/hubert-base-superb-er")
>>> preds = classifier("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{"score": 0.4532, "label": "hap"},
 {"score": 0.3622, "label": "sad"},
 {"score": 0.0943, "label": "neu"},
 {"score": 0.0903, "label": "ang"}]
```

----------------------------------------

TITLE: Using DistilBert with Sequence Classification Head in PyTorch
DESCRIPTION: Shows how to load a pre-trained DistilBert model with a sequence classification head in PyTorch for tasks like sentiment analysis or text classification.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/create_a_model.md#2025-04-23_snippet_11

LANGUAGE: python
CODE:
```
>>> from transformers import DistilBertForSequenceClassification

>>> model = DistilBertForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Preprocessing Dataset with Chat Template for Model Training
DESCRIPTION: Applies a chat template to a dataset for model training, demonstrating how to use the AutoTokenizer and Dataset classes from Hugging Face.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer
from datasets import Dataset

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-beta")

chat1 = [
    {"role": "user", "content": "Which is bigger, the moon or the sun?"},
    {"role": "assistant", "content": "The sun."}
]
chat2 = [
    {"role": "user", "content": "Which is bigger, a virus or a bacterium?"},
    {"role": "assistant", "content": "A bacterium."}
]

dataset = Dataset.from_dict({"chat": [chat1, chat2]})
dataset = dataset.map(lambda x: {"formatted_chat": tokenizer.apply_chat_template(x["chat"], tokenize=False, add_generation_prompt=False)})
print(dataset['formatted_chat'][0])
```

----------------------------------------

TITLE: ZeRO-3 Configuration in DeepSpeed
DESCRIPTION: This JSON configuration demonstrates ZeRO stage 3 optimization with CPU offloading for both the optimizer and parameters. Key parameters include `stage3_max_live_parameters`, `stage3_max_reuse_distance`, `reduce_bucket_size`, `stage3_prefetch_bucket_size`, and `stage3_param_persistence_threshold`. The configuration aims to further reduce GPU RAM usage by offloading both optimizer state and parameters to the CPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/deepspeed.md#2025-04-22_snippet_24

LANGUAGE: json
CODE:
```
{
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    }
}
```

----------------------------------------

TITLE: Using PhoBERT with Transformers in Python
DESCRIPTION: This code snippet demonstrates how to load and use the PhoBERT model in Python using the transformers library. It requires the installation of PyTorch and Hugging Face Transformers. The snippet includes importing necessary libraries, loading the pretrained model and tokenizer, and showing how to process a word-segmented Vietnamese sentence. The input text must be word-segmented according to the requirements of the PhoBERT model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/phobert.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> import torch
>>> from transformers import AutoModel, AutoTokenizer

>>> phobert = AutoModel.from_pretrained("vinai/phobert-base")
>>> tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")

>>> # INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!
>>> line = "TÃ´i lÃ  sinh_viÃªn trÆ°á»ng Ä‘áº¡i_há»c CÃ´ng_nghá»‡ ."

>>> input_ids = torch.tensor([tokenizer.encode(line)])

>>> with torch.no_grad():
...     features = phobert(input_ids)  # Models outputs are now tuples

>>> # With TensorFlow 2.0+:
>>> # from transformers import TFAutoModel
>>> # phobert = TFAutoModel.from_pretrained("vinai/phobert-base")

```

----------------------------------------

TITLE: Handling Common LLM Generation Pitfalls
DESCRIPTION: This Python code snippet illustrates common pitfalls in LLM text generation and how to address them, including controlling output length and using different generation modes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/llm_tutorial.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("openlm-research/open_llama_7b")
>>> tokenizer.pad_token = tokenizer.eos_token  # Llama has no pad token by default
>>> model = AutoModelForCausalLM.from_pretrained(
...     "openlm-research/open_llama_7b", device_map="auto", load_in_4bit=True
... )

>>> model_inputs = tokenizer(["A sequence of numbers: 1, 2"], return_tensors="pt").to("cuda")

>>> # By default, the output will contain up to 20 tokens
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A sequence of numbers: 1, 2, 3, 4, 5'

>>> # Setting `max_new_tokens` allows you to control the maximum length
>>> generated_ids = model.generate(**model_inputs, max_new_tokens=50)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,'

>>> # Set seed or reproducibility -- you don't need this unless you want full reproducibility
>>> from transformers import set_seed
>>> set_seed(0)

>>> model_inputs = tokenizer(["I am a cat."], return_tensors="pt").to("cuda")

>>> # LLM + greedy decoding = repetitive, boring output
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'I am a cat. I am a cat. I am a cat. I am a cat'

>>> # With sampling, the output becomes more creative!
>>> generated_ids = model.generate(**model_inputs, do_sample=True)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'I am a cat.\nI just need to be. I am always.\nEvery time'
```

----------------------------------------

TITLE: Generating Code with Pipeline API (Python)
DESCRIPTION: Demonstrates how to perform basic code generation and infilling using the `transformers` pipeline API. It loads the Code Llama model into a text generation pipeline and generates text based on provided prompts, including a prompt with the `<FILL_ME>` marker for infilling.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/code_llama.md#_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipe = pipeline(
    "text-generation",
    model="meta-llama/CodeLlama-7b-hf",
    torch_dtype=torch.float16,
    device_map=0
)

# basic code generation
result = pipe("# Function to calculate the factorial of a number\ndef factorial(n):", max_new_tokens=256)
print(result[0]['generated_text'])

# infilling
infill_result = pipe("def remove_non_ascii(s: str) -> str:\n    \"\"\" <FILL_ME>\n    return result", max_new_tokens=200)
print(infill_result[0]['generated_text'])
```

----------------------------------------

TITLE: Performing Automatic Speech Recognition with Whisper in Python
DESCRIPTION: This code demonstrates how to use the Hugging Face pipeline for automatic speech recognition (ASR). It utilizes OpenAI's Whisper small model to transcribe speech from an audio file into text, converting the spoken content into a textual representation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/task_summary.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition", model="openai/whisper-small")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

----------------------------------------

TITLE: Basic Sentiment Analysis Pipeline
DESCRIPTION: Example of creating and using a sentiment analysis pipeline for text classification.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/quicktour.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline("sentiment-analysis")
>>> classifier("We are very happy to show you the ðŸ¤— Transformers library.")
[{'label': 'POSITIVE', 'score': 0.9998}]
```

----------------------------------------

TITLE: Getting Environment Info with transformers env - bash
DESCRIPTION: Run this command from your terminal to automatically collect information about your operating system, Python, PyTorch, and TensorFlow versions when reporting a bug. This helps maintainers quickly understand your setup and reproduce the issue.
SOURCE: https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#_snippet_0

LANGUAGE: bash
CODE:
```
transformers env
```

----------------------------------------

TITLE: Hugging Face Hub Authentication
DESCRIPTION: Commands to authenticate with the Hugging Face Hub using either CLI or notebook interface
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/custom_models.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
huggingface-cli login
```

LANGUAGE: python
CODE:
```
from huggingface_hub import notebook_login

notebook_login()
```

----------------------------------------

TITLE: Logging into Hugging Face Hub
DESCRIPTION: Authenticates with the Hugging Face Hub to enable model sharing and uploading. The notebook_login function prompts for a token to authenticate the user's account.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/question_answering.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Log in to Hugging Face Hub
DESCRIPTION: This command logs in to the Hugging Face Hub, which is necessary for uploading the trained model. It requires a Hugging Face account and authentication token.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/run_scripts.md#_snippet_17

LANGUAGE: bash
CODE:
```
huggingface-cli login
```

----------------------------------------

TITLE: Installing ML Framework - PyTorch
DESCRIPTION: Command to install PyTorch as the machine learning framework.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quicktour.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
pip install torch
```

----------------------------------------

TITLE: Performing Image Classification with ðŸ¤— Transformers Pipeline in Python
DESCRIPTION: This snippet demonstrates image classification using the ðŸ¤— Transformers pipeline. It classifies an image and returns labels with corresponding confidence scores.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/task_summary.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline(task="image-classification")
>>> preds = classifier(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> print(*preds, sep="\n")
{'score': 0.4335, 'label': 'lynx, catamount'}
{'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}
{'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}
{'score': 0.0239, 'label': 'Egyptian cat'}
{'score': 0.0229, 'label': 'tiger cat'}
```

----------------------------------------

TITLE: Creating Pipeline with Custom Model and Tokenizer in Python
DESCRIPTION: Demonstrates creating a text generation pipeline with a custom model and tokenizer instead of using the default ones.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/pipeline_tutorial.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> generator = pipeline(task="text-generation", model=model, tokenizer=tokenizer)
```

----------------------------------------

TITLE: Training TensorFlow Image Classification Model
DESCRIPTION: Initialize and compile a TensorFlow image classification model using Hugging Face Transformers, then fit the model with prepared dataset
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
from transformers import TFAutoModelForImageClassification

with strategy.scope():
    model = TFAutoModelForImageClassification.from_pretrained(image_model_checkpoint)
    model.compile(optimizer="adam")

model.fit(tf_dataset)
```

----------------------------------------

TITLE: Using SigLIP with Pipeline API for Zero-Shot Image Classification
DESCRIPTION: Demonstrates how to use the SigLIP model with the pipeline API for zero-shot image classification. It loads an image, defines candidate labels, and performs inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/siglip.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
from transformers import pipeline
from PIL import Image
import requests

# Load pipeline
image_classifier = pipeline(task="zero-shot-image-classification", model="google/siglip-base-patch16-224")

# Load image
url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

# Inference
candidate_labels = ["2 cats", "a plane", "a remote"]
outputs = image_classifier(image, candidate_labels=candidate_labels)
outputs = [{"score": round(output["score"], 4), "label": output["label"] } for output in outputs]
print(outputs)
```

----------------------------------------

TITLE: Quantization using Bitsandbytes for LLaVa-Next
DESCRIPTION: This snippet demonstrates how to load the LLaVa-Next model in 4-bit precision using Bitsandbytes, reducing memory requirements. It defines a `BitsAndBytesConfig` object specifying the quantization parameters and passes it to the `from_pretrained` method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llava_next.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoModelForImageTextToText, BitsAndBytesConfig

# specify how to quantize the model
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForImageTextToText.from_pretrained("llava-hf/llava-v1.6-mistral-7b-hf", quantization_config=quantization_config, device_map="auto")
```

----------------------------------------

TITLE: Creating and Training a BPE Tokenizer
DESCRIPTION: Creates a BPE tokenizer with special tokens and trains it on specified files. Uses Whitespace pre-tokenizer and includes common special tokens like UNK, CLS, SEP, PAD, and MASK.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/fast_tokenizers.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from tokenizers import Tokenizer
>>> from tokenizers.models import BPE
>>> from tokenizers.trainers import BpeTrainer
>>> from tokenizers.pre_tokenizers import Whitespace

>>> tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
>>> trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

>>> tokenizer.pre_tokenizer = Whitespace()
>>> files = [...]
>>> tokenizer.train(files, trainer)
```

----------------------------------------

TITLE: Loading BERT Model for Sequence Classification
DESCRIPTION: Demonstrates loading BERT model weights directly into GPU memory using the transformers library. It checks the memory consumption of model weights on the GPU, illustrating efficient resource monitoring.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_memory_anatomy.md#2025-04-22_snippet_5

LANGUAGE: py
CODE:
```
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-large-uncased").to("cuda")
>>> print_gpu_utilization()
GPU memory occupied: 2631 MB.

```

----------------------------------------

TITLE: Generating Outputs with GPT2 in Python
DESCRIPTION: This snippet demonstrates how to generate text using the GPT2 model and tokenizer from the Transformers library. It showcases the usage of `return_dict_in_generate=True` and `output_scores=True` to obtain a `GenerateDecoderOnlyOutput` object containing generated sequences and scores.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/internal/generation_utils.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained("openai-community/gpt2")
model = GPT2LMHeadModel.from_pretrained("openai-community/gpt2")

inputs = tokenizer("Hello, my dog is cute and ", return_tensors="pt")
generation_output = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)
```

LANGUAGE: python
CODE:
```
generation_output[:2]
```

----------------------------------------

TITLE: Loading Evaluation Metric
DESCRIPTION: Uses the Evaluate library to load the seqeval evaluation metric, which computes precision, recall, F1, and accuracy for model performance evaluation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/token_classification.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> import evaluate

>>> seqeval = evaluate.load("seqeval")
```

----------------------------------------

TITLE: Loading evaluation metric
DESCRIPTION: This snippet demonstrates how to load an evaluation metric using the `evaluate` library. Specifically, it loads the 'accuracy' metric to evaluate the model's performance during fine-tuning.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_5

LANGUAGE: Python
CODE:
```
>>> import numpy as np
>>> import evaluate

>>> metric = evaluate.load("accuracy")
```

----------------------------------------

TITLE: Tokenizing and Embedding Text with BERT for NLP Tasks
DESCRIPTION: BERT uses WordPiece tokenization to generate token embeddings of text. It adds special tokens [SEP] to differentiate sentences and [CLS] at the beginning of sequences for classification tasks. A segment embedding is also added to denote which sentence a token belongs to in paired sentences.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks_explained.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
# Pseudo-code for BERT tokenization and embedding
text = "[CLS] This is a sentence. [SEP] This is another sentence. [SEP]"
tokens = wordpiece_tokenize(text)
token_embeddings = create_token_embeddings(tokens)
segment_embeddings = create_segment_embeddings(tokens)
input_embeddings = token_embeddings + segment_embeddings
```

----------------------------------------

TITLE: Using Qwen2MoE-Chat for Language Model Inference
DESCRIPTION: This code snippet demonstrates how to set up and use the 'Qwen1.5-MoE-A2.7B-Chat' model for generating text responses using the HuggingFace transformers library. Dependencies include having the 'transformers' library installed and a compatible GPU to utilize CUDA. The code initializes the model and tokenizer, formats a chat prompt using a specific template function, and generates responses based on user input. This setup enables the exploration of dialogue-based AI applications using advanced language models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2_moe.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM, AutoTokenizer
>>> device = "cuda" # the device to load the model onto

>>> model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen1.5-MoE-A2.7B-Chat", device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen1.5-MoE-A2.7B-Chat")

>>> prompt = "Give me a short introduction to large language model."

>>> messages = [{"role": "user", "content": prompt}]

>>> text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

>>> model_inputs = tokenizer([text], return_tensors="pt").to(device)

>>> generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=True)

>>> generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]

>>> response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

----------------------------------------

TITLE: Chat Template Usage with Mistral Model
DESCRIPTION: Shows how to use chat templates with the Mistral model, demonstrating more complex formatting with instruction tokens.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/chat_templating.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")

>>> chat = [
...   {"role": "user", "content": "Hello, how are you?"},
...   {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
...   {"role": "user", "content": "I'd like to show off how chat templating works!"},
... ]

>>> tokenizer.apply_chat_template(chat, tokenize=False)
"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]"
```

----------------------------------------

TITLE: Single Image Inference with LLaVA-OneVision in Python
DESCRIPTION: This code snippet demonstrates how to perform single image inference with the LLaVA-OneVision model. It loads the model and processor, prepares an image and text prompt, and then generates text using the model.  The generated text is then decoded and printed to the console, showing the model's response to the prompt.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llava_onevision.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
```python
from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration
import torch

processor = AutoProcessor.from_pretrained("llava-hf/llava-onevision-qwen2-7b-ov-hf") 
model = LlavaOnevisionForConditionalGeneration.from_pretrained(
    "llava-hf/llava-onevision-qwen2-7b-ov-hf",
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    device_map="cuda:0"
)

# prepare image and text prompt, using the appropriate prompt template
url = "https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true"
conversation = [
    {
        "role": "user",
        "content": [
            {"type": "image", "url": url},
            {"type": "text", "text": "What is shown in this image?"},
        ],
    },
]
inputs = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors="pt")
inputs = inputs.to("cuda:0", torch.float16)

# autoregressively complete prompt
output = model.generate(**inputs, max_new_tokens=100)
print(processor.decode(output[0], skip_special_tokens=True))
'user\n\nWhat is shown in this image?\nassistant\nThe image shows a radar chart, also known as a spider chart or a star chart, which is used to compare multiple quantitative variables. Each axis represents a different variable, and the chart is filled with'
```
```

----------------------------------------

TITLE: Loading a Multilingual Sentiment Analysis Model with PyTorch
DESCRIPTION: Code to load a pre-trained multilingual BERT model and its tokenizer for sentiment analysis using PyTorch AutoClasses.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/quicktour.md#2025-04-22_snippet_10

LANGUAGE: Python
CODE:
```
>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"

>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

----------------------------------------

TITLE: Generating Code with AutoModel API (Python)
DESCRIPTION: Shows how to perform code generation and infilling using the `transformers` AutoModelForCausalLM and AutoTokenizer classes. It loads the tokenizer and model, prepares the input IDs, and uses the `model.generate` method for both standard generation and infilling.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/code_llama.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/CodeLlama-7b-hf")
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/CodeLlama-7b-hf",
    torch_dtype=torch.float16,
    device_map="auto",
    attn_implementation="sdpa"
)

# basic code generation
prompt = "# Function to calculate the factorial of a number\ndef factorial(n):"
input_ids = tokenizer(prompt, return_tensors="pt").to("cuda")

output = model.generate(
    **input_ids,
    max_new_tokens=256,
    cache_implementation="static"
)
print(tokenizer.decode(output[0], skip_special_tokens=True))

# infilling
infill_prompt = "def remove_non_ascii(s: str) -> str:\n    \"\"\" <FILL_ME>\n    return result"
input_ids = tokenizer(infill_prompt, return_tensors="pt").to(model.device)

filled_output = model.generate(**input_ids, max_new_tokens=200)
filled_text = tokenizer.decode(filled_output[0], skip_special_tokens=True)
print(filled_text)
```

----------------------------------------

TITLE: Implementing Custom Loss Trainer
DESCRIPTION: Example of customizing the Trainer class by subclassing and overriding the compute_loss method to implement weighted loss calculation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/trainer.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from torch import nn
from transformers import Trainer

class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        # forward pass
        outputs = model(**inputs)
        logits = outputs.get("logits")
        # compute custom loss for 3 labels with different weights
        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0], device=model.device))
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss
```

----------------------------------------

TITLE: Pushing Model to Hub using Trainer in Python
DESCRIPTION: Shows how to use the push_to_hub method of the Trainer class to share the model on the Hugging Face Hub after training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/model_sharing.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> trainer.push_to_hub()
```

----------------------------------------

TITLE: Loading the ELI5 Dataset
DESCRIPTION: In this Python code, the first 5000 examples of the ELI5 dataset are loaded using the Datasets library. The dataset is split into train and test sets to facilitate modeling.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> eli5 = load_dataset("eli5_category", split="train[:5000]")
```

----------------------------------------

TITLE: Creating and Initializing Model
DESCRIPTION: Creates ResNet configuration and model instances, then loads pretrained weights
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/custom_models.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
resnet50d_config = ResnetConfig(block_type="bottleneck", stem_width=32, stem_type="deep", avg_down=True)
resnet50d = ResnetModelForImageClassification(resnet50d_config)

pretrained_model = timm.create_model("resnet50d", pretrained=True)
resnet50d.model.load_state_dict(pretrained_model.state_dict())
```

----------------------------------------

TITLE: Mapping Text Grouping Function to Dataset
DESCRIPTION: This code applies the `group_texts` function to the `tokenized_eli5` dataset using the `map` method.  It processes examples in batches (`batched=True`) using 4 processes for parallel execution (`num_proc=4`).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_9

LANGUAGE: python
CODE:
```
>>> lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)
```

----------------------------------------

TITLE: Manual Inference with PyTorch: Model Prediction
DESCRIPTION: Passes the tokenized inputs to the model to get logits (raw prediction scores). The prediction is made without gradient calculation since we're only performing inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_26

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("stevhliu/my_awesome_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

----------------------------------------

TITLE: Configuring Training Arguments for Seq2Seq Tasks in PyTorch
DESCRIPTION: Sets up the training arguments for a sequence-to-sequence task using Seq2SeqTrainingArguments in PyTorch, including learning rate, batch size, and other hyperparameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/translation.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
training_args = Seq2SeqTrainingArguments(
    output_dir="my_awesome_opus_books_model",
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=2,
    predict_with_generate=True,
    fp16=True, #change to bf16=True for XPU
    push_to_hub=True,
)
```

----------------------------------------

TITLE: Creating and Using Whisper Large-v2 ASR Pipeline in Python
DESCRIPTION: Demonstrates creating an ASR pipeline with a specific model (Whisper large-v2) and using it for transcription.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/pipeline_tutorial.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> transcriber = pipeline(model="openai/whisper-large-v2")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

----------------------------------------

TITLE: Fine-tuning BERT on SWAG dataset using Accelerate
DESCRIPTION: This bash command shows how to fine-tune a BERT model on the SWAG dataset using the Accelerate library. It specifies the model, dataset, sequence length, batch size, learning rate, and number of epochs. This approach allows for more customization and works with distributed setups, TPUs, and mixed precision.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/multiple-choice/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
export DATASET_NAME=swag

python run_swag_no_trainer.py \
  --model_name_or_path google-bert/bert-base-cased \
  --dataset_name $DATASET_NAME \
  --max_seq_length 128 \
  --per_device_train_batch_size 32 \
  --learning_rate 2e-5 \
  --num_train_epochs 3 \
  --output_dir /tmp/$DATASET_NAME/
```

----------------------------------------

TITLE: Initializing ID to Label and Label to ID Mappings in Python
DESCRIPTION: This snippet initializes dictionaries to map IDs to labels (`id2label`) and labels to IDs (`label2id`). These mappings are crucial for token classification tasks, allowing the model to associate numerical predictions with meaningful textual labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
>>> id2label = {
...     0: "O",
...     1: "B-corporation",
...     2: "I-corporation",
...     3: "B-creative-work",
...     4: "I-creative-work",
...     5: "B-group",
...     6: "I-group",
...     7: "B-location",
...     8: "I-location",
...     9: "B-person",
...     10: "I-person",
...     11: "B-product",
...     12: "I-product",
... }
>>> label2id = {
...     "O": 0,
...     "B-corporation": 1,
...     "I-corporation": 2,
...     "B-creative-work": 3,
...     "I-creative-work": 4,
...     "B-group": 5,
...     "I-group": 6,
...     "B-location": 7,
...     "I-location": 8,
...     "B-person": 9,
...     "I-person": 10,
...     "B-product": 11,
...     "I-product": 12,
... }
```

----------------------------------------

TITLE: Configuring LoRA for PEFT
DESCRIPTION: Configures the LoRA (Low-Rank Adaptation) parameters for a PEFT model. This configuration is passed to the model to define the adapter's behavior.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/peft.md#_snippet_10

LANGUAGE: python
CODE:
```
from peft import LoraConfig

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM"ØŒ
)
```

----------------------------------------

TITLE: Installing CPU-only Version of Transformers for PyTorch in Bash
DESCRIPTION: These commands install the CPU-only version of the Transformers library tailored for PyTorch using pip and uv, suitable for environments without GPU resources.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
pip install 'transformers[torch]'
uv pip install 'transformers[torch]'
```

----------------------------------------

TITLE: Implementing Autoregressive Generation with Key-Value Caching in LLMs
DESCRIPTION: This code demonstrates how to use key-value caching for efficient autoregressive text generation. By storing and reusing the key-value vectors from previous steps, this approach avoids redundant computations and significantly speeds up inference while maintaining a constant input size.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/llm_tutorial_optimization.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
past_key_values = None # past_key_values ëŠ” í‚¤-ê°’ ìºì‹œë¥¼ ì˜ë¯¸
generated_tokens = []
next_token_id = tokenizer(prompt, return_tensors="pt")["input_ids"].to("cuda")

for _ in range(5):
  next_logits, past_key_values = model(next_token_id, past_key_values=past_key_values, use_cache=True).to_tuple()
  next_logits = next_logits[:, -1:]
  next_token_id = torch.argmax(next_logits, dim=-1)

  print("shape of input_ids", next_token_id.shape)
  print("length of key-value cache", len(past_key_values[0][0]))  # past_key_values í˜•íƒœ: [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]
  generated_tokens.append(next_token_id.item())

generated_text = tokenizer.batch_decode(generated_tokens)
generated_text
```

----------------------------------------

TITLE: Running inference with Optimum pipeline
DESCRIPTION: This code creates a text classification pipeline using the loaded ONNX model, tokenizer, and specifies the device as CUDA. It then uses the pipeline to perform inference on a given input text and outputs the classification result.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_one.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from optimum.pipelines import pipeline
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased-finetuned-sst-2-english")
pipeline = pipeline(task="text-classification", model=ort_model, tokenizer=tokenizer, device="cuda:0")
result = pipeline("Both the music and visual were astounding, not to mention the actors performance.")
```

----------------------------------------

TITLE: Loading the Yelp Reviews Dataset with Hugging Face Datasets
DESCRIPTION: Loads the Yelp Reviews dataset using the load_dataset function from the datasets library and displays a sample entry from the training set.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/training.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from datasets import load_dataset

dataset = load_dataset("yelp_review_full")
dataset["train"][100]
```

----------------------------------------

TITLE: Applying Preprocessing Function - Python
DESCRIPTION: This snippet applies the defined preprocessing function across the entire IMDb dataset. Using batched processing can improve efficiency by handling multiple examples simultaneously.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
tokenized_imdb = imdb.map(preprocess_function, batched=True)
```

----------------------------------------

TITLE: Initializing Question Answering Pipeline in Python
DESCRIPTION: Shows how to import and initialize a question answering pipeline from the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
>>> from transformers import pipeline
```

----------------------------------------

TITLE: Converting Tokenized Data to PyTorch Tensors
DESCRIPTION: Demonstrates how to convert tokenized data to PyTorch tensors by setting the return_tensors parameter to 'pt', creating model-ready inputs with appropriate padding and truncation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/preprocessing.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors="pt")
>>> print(encoded_input)
{'input_ids': tensor([[  101,   153,  7719, 21490,  1122,  1114,  9582,  1623,   102],
                      [  101,  5226,  1122,  9649,  1199,  2610,  1236,   102,     0]]), 
 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],
                           [0, 0, 0, 0, 0, 0, 0, 0, 0]]), 
 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],
                           [1, 1, 1, 1, 1, 1, 1, 1, 0]])}
```

----------------------------------------

TITLE: Implementing Greedy Search with Speculative Decoding in Transformers
DESCRIPTION: Demonstrates how to use speculative decoding with greedy search using a smaller assistant model to speed up text generation. Shows implementation both directly and through the pipeline API.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/generation_strategies.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM-1.7B")
model = AutoModelForCausalLM.from_pretrained("HuggingFaceTB/SmolLM-1.7B")
assistant_model = AutoModelForCausalLM.from_pretrained("HuggingFaceTB/SmolLM-135M")
inputs = tokenizer("Hugging Face is an open-source company", return_tensors="pt")

outputs = model.generate(**inputs, assistant_model=assistant_model)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

LANGUAGE: python
CODE:
```
from transformers import pipeline
import torch

pipe = pipeline(
    "text-generation",
    model="meta-llama/Llama-3.1-8B",
    assistant_model="meta-llama/Llama-3.2-1B",
    torch_dtype=torch.bfloat16
)
pipe_output = pipe("Once upon a time, ", max_new_tokens=50, do_sample=False)
pipe_output[0]["generated_text"]
```

----------------------------------------

TITLE: Saving a Transformers Model with Sharded Checkpoints
DESCRIPTION: This snippet demonstrates how to save a model with sharded checkpoints by setting the max_shard_size parameter, which splits the model weights into multiple smaller files to reduce memory usage.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/big_models.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir, max_shard_size="200MB")
...     print(sorted(os.listdir(tmp_dir)))
['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']
```

----------------------------------------

TITLE: Configuring Training Arguments
DESCRIPTION: Sets up the training hyperparameters using TrainingArguments, including batch size, learning rate, number of epochs, logging frequency, and push to Hub configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_question_answering.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
>>> from transformers import TrainingArguments

>>> repo_id = "MariaK/vilt_finetuned_200"

>>> training_args = TrainingArguments(
...     output_dir=repo_id,
...     per_device_train_batch_size=4,
...     num_train_epochs=20,
...     save_steps=200,
...     logging_steps=50,
...     learning_rate=5e-5,
...     save_total_limit=2,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )
```

----------------------------------------

TITLE: Detecting Objects in Images with Transformers Pipeline in Python
DESCRIPTION: This snippet demonstrates object detection using the Hugging Face pipeline. It loads a default object detection model to identify objects in an image, returning the object class label, confidence score, and bounding box coordinates defining the object's location in the image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/task_summary.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> detector = pipeline(task="object-detection")
>>> preds = detector(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"], "box": pred["box"]} for pred in preds]
>>> preds
[{'score': 0.9865,
  'label': 'cat',
  'box': {'xmin': 178, 'ymin': 154, 'xmax': 882, 'ymax': 598}}]
```

----------------------------------------

TITLE: Making Model Predictions with TensorFlow
DESCRIPTION: Shows how to pass inputs to the model and get predictions using softmax
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/quicktour.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
tf_outputs = tf_model(tf_batch)

import tensorflow as tf

tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
tf_predictions
```

----------------------------------------

TITLE: Loading WNUT 17 Dataset with HuggingFace Datasets
DESCRIPTION: Loads the WNUT 17 dataset for Named Entity Recognition using the datasets library and displays a sample.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/tasks/token_classification.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from datasets import load_dataset

wnut = load_dataset("wnut_17")
```

----------------------------------------

TITLE: Print the Tokenized Sequence Example 2 Python
DESCRIPTION: This snippet prints the tokenized sequence, showing how the tokenizer has split the original text into tokens. Subwords are prefixed with '##' to indicate that they are part of a larger word. This snippet shows a version where the characters are in a different alphabet, demonstrating tokenization with non-ASCII characters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/glossary.md#_snippet_8

LANGUAGE: python
CODE:
```
>>> print(tokenized_sequence)
['A'ØŒ 'Titan'ØŒ 'R'ØŒ '##T'ØŒ '##X'ØŒ 'has'ØŒ '24'ØŒ '##GB'ØŒ 'of'ØŒ 'V'ØŒ '##RA'ØŒ '##M']
```

----------------------------------------

TITLE: Cleaning Up Model and Pipeline Instances
DESCRIPTION: These Python code snippets show how to delete the model and pipeline instances to clear memory usage. The del statements remove the instances, and flush calls a function to ensure memory is cleared.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
del model
del pipe
```

LANGUAGE: python
CODE:
```
flush()
```

----------------------------------------

TITLE: Compiling and fitting the model with tf.data.Dataset
DESCRIPTION: Compiles the model with the Adam optimizer and a learning rate of 3e-5, then trains the model using the prepared `tf.data.Dataset`. This avoids loading the entire dataset into memory at once.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/training.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
model.compile(optimizer=Adam(3e-5))

model.fit(tf_dataset)
```

----------------------------------------

TITLE: Launching Distributed Training with Accelerate CLI
DESCRIPTION: This bash command demonstrates how to use the accelerate_launch CLI tool to start a distributed training job. It runs a text classification script with FSDP configuration and various training parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/trainer.md#2025-04-22_snippet_11

LANGUAGE: bash
CODE:
```
accelerate launch \
    ./examples/pytorch/text-classification/run_glue.py \
    --model_name_or_path google-bert/bert-base-cased \
    --task_name $TASK_NAME \
    --do_train \
    --do_eval \
    --max_seq_length 128 \
    --per_device_train_batch_size 16 \
    --learning_rate 5e-5 \
    --num_train_epochs 3 \
    --output_dir /tmp/$TASK_NAME/ \
    --overwrite_output_dir
```

----------------------------------------

TITLE: Padding Tokenized Sentences to Equal Length
DESCRIPTION: Demonstrates how to pad tokenized sentences to ensure uniform length within a batch by setting the padding parameter to True, which adds padding tokens (0) to shorter sequences to match the longest one.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/preprocessing.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True)
>>> print(encoded_input)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0], 
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], 
               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]], 
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}
```

----------------------------------------

TITLE: Apply LoRA to Specific Projections - Python
DESCRIPTION: Applies Low-Rank Adaptation (LoRA) to the specified projections (q and v) of the attention mechanism using the `peft` library. The `LoraConfig` defines the LoRA parameters, such as rank (`r`), alpha (`lora_alpha`), target modules, dropout, and task type. The `get_peft_model` function integrates LoRA into the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/how_to_hack_models.md#_snippet_2

LANGUAGE: python
CODE:
```
from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q", "v"],  # ØªØ·Ø¨ÙŠÙ‚ LoRA Ø¹Ù„Ù‰ Ø¥Ø³Ù‚Ø§Ø·Ø§Øª q Ùˆ v
    lora_dropout=0.1,
    task_type="mask-generation"
)

# ØªØ·Ø¨ÙŠÙ‚ LoRA Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
model = get_peft_model(model, config)
```

----------------------------------------

TITLE: Saving a Converted Transformers Model
DESCRIPTION: Saves the converted model and its configuration to a specified folder using the `save_pretrained` method. This creates files like `model.safetensors` and `config.json` in the target directory, making the model easily loadable by the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/add_new_model.md#_snippet_15

LANGUAGE: Python
CODE:
```
model.save_pretrained("/path/to/converted/checkpoint/folder")
```

----------------------------------------

TITLE: Inference with GLM-4-9B Chat Model using PyTorch
DESCRIPTION: Demonstrates how to load and use the GLM-4-9B chat model for inference using the ChatML format. The example shows model initialization, tokenization, chat template application, and text generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/glm.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM, AutoTokenizer
>>> device = "cuda" # the device to load the model onto

>>> model = AutoModelForCausalLM.from_pretrained("THUDM/glm-4-9b-chat", device_map="auto", trust_remote_code=True)
>>> tokenizer = AutoTokenizer.from_pretrained("THUDM/glm-4-9b-chat")

>>> prompt = "Give me a short introduction to large language model."

>>> messages = [{"role": "user", "content": prompt}]

>>> text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

>>> model_inputs = tokenizer([text], return_tensors="pt").to(device)

>>> generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=True)

>>> generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]

>>> response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

----------------------------------------

TITLE: Basic Generation with Mamba 2 Model
DESCRIPTION: Demonstrates how to load and use the Mamba 2 model for basic text generation using the Mamba-Codestral-7B model. Shows tokenization, model initialization and generation process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mamba2.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import Mamba2Config, Mamba2ForCausalLM, AutoTokenizer
import torch
model_id = 'mistralai/Mamba-Codestral-7B-v0.1'
tokenizer = AutoTokenizer.from_pretrained(model_id, revision='refs/pr/9', from_slow=True, legacy=False)
model = Mamba2ForCausalLM.from_pretrained(model_id, revision='refs/pr/9')
input_ids = tokenizer("Hey how are you doing?", return_tensors= "pt")["input_ids"]

out = model.generate(input_ids, max_new_tokens=10)
print(tokenizer.batch_decode(out))
```

----------------------------------------

TITLE: Quantizing and Saving a Model for Later Loading
DESCRIPTION: This snippet illustrates the initial steps for loading a quantized model: it quantizes a Llama-3.1-8B-Instruct model on the CPU using Int8 weight-only quantization and then saves it to a local directory. The saved model can subsequently be loaded on a different device like CUDA.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/torchao.md#_snippet_13

LANGUAGE: Python
CODE:
```
import torch
from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
from torchao.quantization import Int8WeightOnlyConfig

quant_config = Int8WeightOnlyConfig(group_size=128)
quantization_config = TorchAoConfig(quant_type=quant_config)

# Load and quantize the model
quantized_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B-Instruct",
    torch_dtype="auto",
    device_map="cpu",
    quantization_config=quantization_config
)
# save the quantized model
output_dir = "llama-3.1-8b-torchao-int8-cuda"
quantized_model.save_pretrained(output_dir, safe_serialization=False)
```

----------------------------------------

TITLE: Controlling Message Continuation in Tokenization Using Python
DESCRIPTION: This snippet shows the use of the `continue_final_message` parameter in the `apply_chat_template` method, which controls how the model continues the chat flow. It allows for refining the model's output by tuning how input messages are handled in regards to ending sequences.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
formatted_chat = tokenizer.apply_chat_template(chat, tokenize=True, return_dict=True, continue_final_message=True)
model.generate(**formatted_chat)
```

----------------------------------------

TITLE: Installing Transformers with Conda
DESCRIPTION: Demonstrates how to install the Transformers library using conda from the conda-forge channel. Note that installation from the huggingface channel is deprecated.
SOURCE: https://github.com/huggingface/transformers/blob/main/i18n/README_es.md#2025-04-22_snippet_7

LANGUAGE: bash
CODE:
```
conda install conda-forge::transformers
```

----------------------------------------

TITLE: Generating Text with Unformatted Prompt in Python
DESCRIPTION: Illustrates generating text from an unformatted prompt string using a Transformer model. Models can underperform with such inputs, highlighting the need for structured prompts. Requires a pretrained model and tokenizer for execution.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial.md#2025-04-22_snippet_11

LANGUAGE: Python
CODE:
```
prompt = """How many cats does it take to change a light bulb? Reply as a pirate."""
model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
input_length = model_inputs.input_ids.shape[1]
generated_ids = model.generate(**model_inputs, max_new_tokens=50)
print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])
"Aye, matey! 'Tis a simple task for a cat with a keen eye and nimble paws. First, the cat will climb up the ladder, carefully avoiding the rickety rungs. Then, with"
```

----------------------------------------

TITLE: Running 8-bit Quantized Models with Pipeline
DESCRIPTION: This example shows how to load and run models in 8-bit precision using the bitsandbytes library with the pipeline API. This technique reduces memory usage while maintaining reasonable model performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/pipeline_tutorial.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
# pip install accelerate bitsandbytes
import torch
from transformers import pipeline

pipe = pipeline(model="facebook/opt-1.3b", device_map="auto", model_kwargs={"load_in_8bit": True})
output = pipe("This is a cool example!", do_sample=True, top_p=0.95)
```

----------------------------------------

TITLE: Loading Fuyu Model for Causal Language Modeling - Python
DESCRIPTION: This snippet illustrates the process of loading the Fuyu model for causal language modeling, utilizing the Fuyu configuration and pre-trained weights.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/fuyu.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import FuyuConfig, FuyuForCausalLM
model_config = FuyuConfig()
model = FuyuForCausalLM(model_config).from_pretrained('/output/path')
```

----------------------------------------

TITLE: Configuring PyTorch Training Arguments and Trainer for Translation
DESCRIPTION: This code sets up training arguments (including learning rate, batch size, evaluation strategy) and initializes the Seq2SeqTrainer with the model, datasets, and evaluation metrics.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/translation.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> training_args = Seq2SeqTrainingArguments(
...     output_dir="my_awesome_opus_books_model",
...     eval_strategy="epoch",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     weight_decay=0.01,
...     save_total_limit=3,
...     num_train_epochs=2,
...     predict_with_generate=True,
...     fp16=True,
...     push_to_hub=True,
... )

>>> trainer = Seq2SeqTrainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_books["train"],
...     eval_dataset=tokenized_books["test"],
...     processing_class=tokenizer,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

----------------------------------------

TITLE: Saving and Loading TensorFlow Models with Hugging Face Methods in Python
DESCRIPTION: This snippet demonstrates how to save and load TensorFlow models using Hugging Face's save_pretrained and from_pretrained methods to ensure compatibility.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/troubleshooting.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import TFPreTrainedModel

>>> model.save_pretrained("path_to/model")
>>> model = TFPreTrainedModel.from_pretrained("path_to/model")
```

----------------------------------------

TITLE: Generating Text Based on a Prompt
DESCRIPTION: This snippet generates a response based on a given prompt using the previously created pipeline. It specifies the maximum number of new tokens to generate and extracts the generated text from the output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
prompt = "Question: Please write a function in Python that transforms bytes to Giga bytes.\n\nAnswer:"

result = pipe(prompt, max_new_tokens=60)[0]["generated_text"][len(prompt):]
result
```

----------------------------------------

TITLE: Enabling CPU Offload
DESCRIPTION: Demonstrates how to enable CPU offloading for idle submodels to reduce GPU memory footprint by 80%.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/bark.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
model.enable_cpu_offload()
```

----------------------------------------

TITLE: Fine-tuning Mamba2 Model with LoRA in Python
DESCRIPTION: This snippet provides a draft script for fine-tuning a Mamba2 model using the SFTTrainer from the TRL library and LoRA (Low-Rank Adaptation) for efficient training. It includes setting up the tokenizer, model, dataset, and training arguments.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/mamba2.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from trl import SFTTrainer
from peft import LoraConfig
from transformers import AutoTokenizer, Mamba2ForCausalLM, TrainingArguments
model_id = 'mistralai/Mamba-Codestral-7B-v0.1'
tokenizer = AutoTokenizer.from_pretrained(model_id, revision='refs/pr/9', from_slow=True, legacy=False)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left" #ì™¼ìª½ íŒ¨ë”©ìœ¼ë¡œ ì„¤ì •

model = Mamba2ForCausalLM.from_pretrained(model_id, revision='refs/pr/9')
dataset = load_dataset("Abirate/english_quotes", split="train")
# CUDA ì»¤ë„ì—†ì´ëŠ”, ë°°ì¹˜í¬ê¸° 2ê°€ 80GB ìž¥ì¹˜ë¥¼ í•˜ë‚˜ ì°¨ì§€í•©ë‹ˆë‹¤.
# í•˜ì§€ë§Œ ì •í™•ë„ëŠ” ê°ì†Œí•©ë‹ˆë‹¤.
# ì‹¤í—˜ê³¼ ì‹œë„ë¥¼ í™˜ì˜í•©ë‹ˆë‹¤!
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    logging_dir='./logs',
    logging_steps=10,
    learning_rate=2e-3
)
lora_config =  LoraConfig(
        r=8,
        target_modules=["embeddings", "in_proj", "out_proj"],
        task_type="CAUSAL_LM",
        bias="none"
)
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    peft_config=lora_config,
    train_dataset=dataset,
    dataset_text_field="quote",
)
trainer.train()
```

----------------------------------------

TITLE: Preprocessing Images and Extracting OCR Data
DESCRIPTION: This function `get_ocr_words_and_boxes` takes a batch of examples, converts the images to RGB format, applies the image processor, and extracts the pixel values, words, and bounding boxes.  It returns the updated examples with the processed image and OCR data.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/document_question_answering.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> image_processor = processor.image_processor


>>> def get_ocr_words_and_boxes(examples):
...     images = [image.convert("RGB") for image in examples["image"]]
...     encoded_inputs = image_processor(images)

...     examples["image"] = encoded_inputs.pixel_values
...     examples["words"] = encoded_inputs.words
...     examples["boxes"] = encoded_inputs.boxes

...     return examples
```

----------------------------------------

TITLE: Custom Model Pipeline Implementation
DESCRIPTION: Example showing how to use a custom model and tokenizer in a pipeline
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/quicktour.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
>>> classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
>>> classifier("Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ðŸ¤— Transformers.")
[{'label': '5 stars', 'score': 0.7273}]
```

----------------------------------------

TITLE: Running Summarization Example (TensorFlow)
DESCRIPTION: Command to run the TensorFlow summarization example script, fine-tuning T5-small on the CNN/DailyMail dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/run_scripts.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
python examples/tensorflow/summarization/run_summarization.py  \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval
```

----------------------------------------

TITLE: Compiling Image Segmentation Model with torch.compile()
DESCRIPTION: Demonstrates the application of torch.compile() to a Segformer model for image segmentation. The code loads a pre-trained model, applies optimization, and performs inference on a sample image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/perf_torch_compile.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation

processor = SegformerImageProcessor.from_pretrained("nvidia/segformer-b0-finetuned-ade-512-512")
model = SegformerForSemanticSegmentation.from_pretrained("nvidia/segformer-b0-finetuned-ade-512-512").to("cuda")
model = torch.compile(model)
seg_inputs = processor(images=image, return_tensors="pt").to("cuda")

with torch.no_grad():
    _ = model(**seg_inputs)
```

----------------------------------------

TITLE: Loading Tokenizer for Text Processing
DESCRIPTION: Initializes the DistilGPT2 tokenizer from Hugging Face's transformers library to prepare text data for the language model by converting text into token IDs that the model can process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/language_modeling.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilgpt2")
```

----------------------------------------

TITLE: Utilizing SinkCache for Long Sequence Generation in Python
DESCRIPTION: This snippet demonstrates the use of SinkCache for generating very long sequences. It initializes a tokenizer, model, and SinkCache with specific parameters, then generates text while retaining only essential tokens.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/kv_cache.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, SinkCache

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf", torch_dtype=torch.float16).to("cuda:0")
inputs = tokenizer("This is a long story about unicorns, fairies and magic.", return_tensors="pt").to(model.device)

past_key_values = SinkCache(window_length=256, num_sink_tokens=4)
out = model.generate(**inputs, do_sample=False, max_new_tokens=30, past_key_values=past_key_values)
tokenizer.batch_decode(out, skip_special_tokens=True)[0]
```

----------------------------------------

TITLE: Data Collator Setup for PyTorch
DESCRIPTION: Configures data collator for language modeling with dynamic padding in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/language_modeling.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
```

----------------------------------------

TITLE: Initializing TensorFlow Data Collator for Transformers
DESCRIPTION: Creates a DefaultDataCollator instance configured to return TensorFlow tensors for batching model inputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/training.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import DefaultDataCollator

data_collator = DefaultDataCollator(return_tensors="tf")
```

----------------------------------------

TITLE: Fine-tuning BERT on SQuAD1.0 using Trainer
DESCRIPTION: Script to fine-tune BERT-base model on SQuAD1.0 dataset using the Hugging Face Trainer. Runs in 24 minutes on Tesla V100 16GB GPU. Achieves 88.52 F1 score and 81.22 exact match score.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python run_qa.py \
  --model_name_or_path google-bert/bert-base-uncased \
  --dataset_name squad \
  --do_train \
  --do_eval \
  --per_device_train_batch_size 12 \
  --learning_rate 3e-5 \
  --num_train_epochs 2 \
  --max_seq_length 384 \
  --doc_stride 128 \
  --output_dir /tmp/debug_squad/
```

----------------------------------------

TITLE: Decoding Token IDs Back to Text
DESCRIPTION: Demonstrates how to decode token IDs back to text, revealing the special tokens [CLS] and [SEP] that were added during tokenization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/preprocessing.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> tokenizer.decode(encoded_input["input_ids"])
'[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]'
```

----------------------------------------

TITLE: Compiling and Fitting Model (TensorFlow)
DESCRIPTION: This code compiles the model for training using the Adam optimizer and then starts the training process using the `fit` method. The `model.compile` configures the model with the chosen optimizer, and `model.fit` trains the model on the prepared TensorFlow dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
from tensorflow.keras.optimizers import Adam

model.compile(optimizer="adam")
model.fit(tf_dataset)
```

----------------------------------------

TITLE: Automatic Speech Recognition with Pipeline
DESCRIPTION: Initializes an automatic speech recognition pipeline using the transformers library and a specific pre-trained model.  It requires the `torch` library to be installed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/quicktour.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
">>> import torch\n>>> from transformers import pipeline\n\n>>> speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")"
```

----------------------------------------

TITLE: Generating Text with GPT-J Model
DESCRIPTION: This code snippet shows how to generate text using the GPT-J model with a specified prompt. It utilizes the Transformers library and requires the model and tokenizer to be loaded first. The snippet demonstrates both standard and float16 precision usage.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gptj.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-j-6B")
>>> tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6B")

>>> prompt = (
...     "In a shocking finding, scientists discovered a herd of unicorns living in a remote, "
...     "previously unexplored valley, in the Andes Mountains. Even more surprising to the "
...     "researchers was the fact that the unicorns spoke perfect English."
... )

>>> input_ids = tokenizer(prompt, return_tensors="pt").input_ids

>>> gen_tokens = model.generate(
...     input_ids,
...     do_sample=True,
...     temperature=0.9,
...     max_length=100,
... )
>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]
```

LANGUAGE: python
CODE:
```
>>> from transformers import GPTJForCausalLM, AutoTokenizer
>>> import torch

>>> device = "cuda"
>>> model = GPTJForCausalLM.from_pretrained("EleutherAI/gpt-j-6B", torch_dtype=torch.float16).to(device)
>>> tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6B")

>>> prompt = (
...     "In a shocking finding, scientists discovered a herd of unicorns living in a remote, "
...     "previously unexplored valley, in the Andes Mountains. Even more surprising to the "
...     "researchers was the fact that the unicorns spoke perfect English."
... )

>>> input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)

>>> gen_tokens = model.generate(
...     input_ids,
...     do_sample=True,
...     temperature=0.9,
...     max_length=100,
... )
>>> gen_text = tokenizer.batch_decode(gen_tokens)[0]
```

----------------------------------------

TITLE: Loading DistilBERT for Question Answering in PyTorch
DESCRIPTION: This snippet demonstrates how to load a pre-trained DistilBERT model for question answering tasks using the AutoModelForQuestionAnswering class from the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/question_answering.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer

model = AutoModelForQuestionAnswering.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Sharing Models on Hugging Face Hub in ðŸ¤— Transformers
DESCRIPTION: Illustrates the method for sharing model, configuration, and preprocessing class instances on the Hugging Face Hub, making them easily accessible to others.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/philosophy.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
push_to_hub()
```

----------------------------------------

TITLE: Transcribing Speech with Transformers in Python
DESCRIPTION: This snippet illustrates how to transcribe speech to text using the ðŸ¤— Transformers library's automatic speech recognition feature. It loads a pre-trained model and processes an audio file to return the transcribed text, demonstrating the application's ease of use in converting audio to written form.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/task_summary.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
>>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition", model="openai/whisper-small")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

----------------------------------------

TITLE: Creating Optimizer and Learning Rate Schedule in TensorFlow
DESCRIPTION: This code snippet creates an optimizer and learning rate schedule for training the model in TensorFlow. It uses the `create_optimizer` function from the Transformers library to set up the optimizer with specified learning rate, weight decay, and warmup steps.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
>>> from transformers import create_optimizer

>>> batch_size = 16
>>> num_train_epochs = 3
>>> num_train_steps = (len(tokenized_wnut["train"]) // batch_size) * num_train_epochs
>>> optimizer, lr_schedule = create_optimizer(
...     init_lr=2e-5,
...     num_train_steps=num_train_steps,
...     weight_decay_rate=0.01,
...     num_warmup_steps=0,
... )
```

----------------------------------------

TITLE: Dataset Train-Test Split
DESCRIPTION: This Python code snippet demonstrates how to perform a train-test split on the ELI5 dataset, reserving 20% of the data for testing purposes using the train_test_split method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> eli5 = eli5.train_test_split(test_size=0.2)
```

----------------------------------------

TITLE: Creating Train-Test Split in Python
DESCRIPTION: Splits the dataset into training and testing subsets with a 90/10 ratio. This provides separate data for training and evaluation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/text-to-speech.md#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
>>> dataset = dataset.train_test_split(test_size=0.1)
```

----------------------------------------

TITLE: Training PEFT Adapter
DESCRIPTION: Example of configuring and training a LoRA adapter using the Trainer class
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/peft.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from peft import LoraConfig

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)

model.add_adapter(peft_config)

trainer = Trainer(model=model, ...)
trainer.train()
```

----------------------------------------

TITLE: Full Training Loop with Accelerate Integration
DESCRIPTION: Diff view showing all changes needed to convert a standard PyTorch training loop to a distributed one using Accelerate, highlighting the minimal modifications required.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/accelerate.md#2025-04-22_snippet_4

LANGUAGE: diff
CODE:
```
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

----------------------------------------

TITLE: Pushing a 4-bit quantized model to the Hugging Face Hub
DESCRIPTION: Code to quantize a model to 4-bit and push it to the Hugging Face Hub, which requires the latest version of bitsandbytes for proper serialization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-23_snippet_6

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=True)

model = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-560m", 
    device_map="auto",
    quantization_config=quantization_config
)

model.push_to_hub("bloom-560m-4bit")
```

----------------------------------------

TITLE: Initializing and running the Trainer (PyTorch)
DESCRIPTION: This snippet demonstrates how to initialize the `Trainer` with the model, training arguments, datasets, tokenizer, data collator, and compute metrics function. It then starts the training process by calling the `train()` method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_imdb["train"],
...     eval_dataset=tokenized_imdb["test"],
...     processing_class=tokenizer,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

----------------------------------------

TITLE: Phi4 Multimodal Inference with Transformers
DESCRIPTION: This code demonstrates how to perform inference using the Phi4 Multimodal model from the Hugging Face Transformers library. It loads the model and processor, optionally loads adapters for speech and vision, processes input based on the modality (image or audio), and generates a response. It requires the `torch` and `transformers` libraries to be installed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/phi4_multimodal.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig


# Define model path
model_path = "microsoft/Phi-4-multimodal-instruct"
device = "cuda:0"

# Load model and processor
processor = AutoProcessor.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device,  torch_dtype=torch.float16)

# Optional: load the adapters (note that without them, the base model will very likely not work well)
model.load_adapter(model_path, adapter_name="speech", device_map=device, adapter_kwargs={"subfolder": 'speech-lora'})
model.load_adapter(model_path, adapter_name="vision", device_map=device, adapter_kwargs={"subfolder": 'vision-lora'})

# Part : Image Processing
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "url": "https://www.ilankelman.org/stopsigns/australia.jpg"},
            {"type": "text", "text": "What is shown in this image?"},
        ],
    },
]

model.set_adapter("vision") # if loaded, activate the vision adapter
inputs = processor.apply_chat_template(
    messages,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt",
).to(device)

# Generate response
generate_ids = model.generate(
    **inputs,
    max_new_tokens=1000,
    do_sample=False,
)
generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]
response = processor.batch_decode(
    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
)[0]
print(f'>>> Response\n{response}')


# Part 2: Audio Processing
model.set_adapter("speech") # if loaded, activate the speech adapter
audio_url = "https://upload.wikimedia.org/wikipedia/commons/b/b0/Barbara_Sahakian_BBC_Radio4_The_Life_Scientific_29_May_2012_b01j5j24.flac"
messages = [
    {
        "role": "user",
        "content": [
            {"type": "audio", "url": audio_url},
            {"type": "text", "text": "Transcribe the audio to text, and then translate the audio to French. Use <sep> as a separator between the origina transcript and the translation.",
            },
        ],
    },
]

inputs = processor.apply_chat_template(
    messages,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt",
).to(device)

generate_ids = model.generate(
    **inputs,
    max_new_tokens=1000,
    do_sample=False,
)
generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]
response = processor.batch_decode(
    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
)[0]
print(f'>>> Response\n{response}')
```

----------------------------------------

TITLE: Loading a Pre-trained Model for Token Classification Python
DESCRIPTION: This code snippet illustrates loading a pre-trained model for token classification using `AutoModelForTokenClassification.from_pretrained`. It uses the same "distilbert/distilbert-base-uncased" checkpoint as the sequence classification example, showcasing how the same checkpoint can be reused for different tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/autoclass_tutorial.md#_snippet_6

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForTokenClassification

>>> model = AutoModelForTokenClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Launching Distributed Training Script
DESCRIPTION: These bash commands show how to create a configuration file and launch a distributed training script using Accelerate.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/accelerate.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
accelerate config
```

LANGUAGE: bash
CODE:
```
accelerate launch train.py
```

----------------------------------------

TITLE: Specifying PyTorch Dependencies
DESCRIPTION: Lists the core PyTorch-related package dependencies with minimum version requirements. Includes PyTorch core (>=1.5.0), TorchVision for computer vision utilities (>=0.6.0), and Datasets library for data handling (>=1.8.0).
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/image-pretraining/requirements.txt#2025-04-22_snippet_0

LANGUAGE: text
CODE:
```
torch>=1.5.0
torchvision>=0.6.0
datasets>=1.8.0
```

----------------------------------------

TITLE: Prompt Lookup Decoding with Greedy Search in Transformers
DESCRIPTION: This code snippet demonstrates how to use prompt lookup decoding with greedy search for text generation using the Transformers library. It initializes a tokenizer and a causal language model, then generates text using the `generate` method with the `prompt_lookup_num_tokens` parameter to enable prompt lookup. The `get_backend()` function helps automatically detect the appropriate device (CUDA, CPU, etc.).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_optims.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from accelerate.test_utils.testing import get_backend

device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-1.3b")
inputs = tokenizer("The second law of thermodynamics states", return_tensors="pt").to(device)

model = AutoModelForCausalLM.from_pretrained("facebook/opt-1.3b", torch_dtype="auto").to(device)
assistant_model = AutoModelForCausalLM.from_pretrained("facebook/opt-125m").to(device)
outputs = model.generate(**inputs, prompt_lookup_num_tokens=3)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
```

----------------------------------------

TITLE: Compiling a Transformers Model with Fullgraph Optimization
DESCRIPTION: This example demonstrates compiling a Transformers model with the `fullgraph=True` option, along with `reduce-overhead` mode. The `fullgraph` option attempts to compile the entire model into a single graph for maximal performance. This approach may raise an error if a graph break is encountered, preventing compilation into a single graph.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_torch_compile.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("google/gemma-2b", device_map="auto")
compiled_model = torch.compile(model, mode="reduce-overhead", fullgraph=True)
```

----------------------------------------

TITLE: Initializing Tokenizers
DESCRIPTION: Shows how to create both standard and fast tokenizers, including custom vocabulary initialization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/create_a_model.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import DistilBertTokenizer

>>> my_tokenizer = DistilBertTokenizer(vocab_file="my_vocab_file.txt", do_lower_case=False, padding_side="left")
```

LANGUAGE: python
CODE:
```
>>> from transformers import DistilBertTokenizer, DistilBertTokenizerFast

>>> slow_tokenizer = DistilBertTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
>>> fast_tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Adding and Managing Multiple Adapters in Python
DESCRIPTION: This example shows how to add multiple adapters to a model in PEFT, enabling users to manage different configurations using `adapter_name` for organized training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/peft.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
model.add_adapter(lora_config, adapter_name="lora_2")

```

----------------------------------------

TITLE: Running Distributed Training in a Notebook
DESCRIPTION: Code snippet to launch distributed training in a notebook environment using Accelerate's notebook_launcher.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/accelerate.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

----------------------------------------

TITLE: Loading Quantized IDEFICS Model
DESCRIPTION: Demonstrates how to load a quantized version of the IDEFICS model using BitsAndBytesConfig for 4-bit precision.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/idefics.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch
from transformers import IdeficsForVisionText2Text, AutoProcessor, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
)

processor = AutoProcessor.from_pretrained(checkpoint)

model = IdeficsForVisionText2Text.from_pretrained(
    checkpoint,
    quantization_config=quantization_config,
    device_map="auto"
)
```

----------------------------------------

TITLE: Modifying Training Loop for Distributed Training
DESCRIPTION: This code shows how to modify a typical PyTorch training loop to use Accelerate for distributed training, replacing the standard backward pass.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/accelerate.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
for epoch in range(num_epochs):
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

----------------------------------------

TITLE: Tokenizer Padding and Truncation Examples
DESCRIPTION: Demonstrates various methods for padding and truncating input sequences using Hugging Face tokenizers with different configuration options
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/pad_truncation.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
tokenizer(batch_sentences)
```

LANGUAGE: python
CODE:
```
tokenizer(batch_sentences, padding=True)
```

LANGUAGE: python
CODE:
```
tokenizer(batch_sentences, padding='longest')
```

LANGUAGE: python
CODE:
```
tokenizer(batch_sentences, padding='max_length')
```

LANGUAGE: python
CODE:
```
tokenizer(batch_sentences, padding='max_length', max_length=42)
```

LANGUAGE: python
CODE:
```
tokenizer(batch_sentences, padding=True, pad_to_multiple_of=8)
```

LANGUAGE: python
CODE:
```
tokenizer(batch_sentences, truncation=True)
```

LANGUAGE: python
CODE:
```
tokenizer(batch_sentences, padding=True, truncation=True)
```

LANGUAGE: python
CODE:
```
tokenizer(batch_sentences, padding='max_length', truncation=True)
```

----------------------------------------

TITLE: Splitting Dataset into Train and Test Sets
DESCRIPTION: Divides the dataset into training and testing subsets with an 80/20 split ratio using the train_test_split method from the Datasets library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/semantic_segmentation.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> ds = ds.train_test_split(test_size=0.2)
>>> train_ds = ds["train"]
>>> test_ds = ds["test"]
```

----------------------------------------

TITLE: Setting Up Optimizer and Learning Rate Schedule for TensorFlow
DESCRIPTION: This code configures the optimizer and learning rate schedule for training a question answering model in TensorFlow, including batch size and number of epochs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/question_answering.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
from transformers import create_optimizer

batch_size = 16
num_epochs = 2
total_train_steps = (len(tokenized_squad["train"]) // batch_size) * num_epochs
optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=total_train_steps,
)
```

----------------------------------------

TITLE: Create a custom collate_fn for batching images in Python
DESCRIPTION: Defines a custom `collate_fn` to handle batching of images, specifically to pad images to the largest image in the batch and to create a corresponding pixel mask to indicate real vs. padded pixels. This function prepares the data for efficient processing during model training by ensuring that all images in a batch have the same dimensions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/object_detection.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> def collate_fn(batch):
...     pixel_values = [item["pixel_values"] for item in batch]
...     encoding = image_processor.pad(pixel_values, return_tensors="pt")
...     labels = [item["labels"] for item in batch]
...     batch = {}
...     batch["pixel_values"] = encoding["pixel_values"]
...     batch["pixel_mask"] = encoding["pixel_mask"]
...     batch["labels"] = labels
...     return batch
```

----------------------------------------

TITLE: Defining Weather Tools in Python
DESCRIPTION: Creating custom functions for retrieving temperature and wind speed with type hints and docstrings
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_extras.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
def get_current_temperature(location: str, unit: str) -> float:
    """
    Get the current temperature at a location.
    
    Args:
        location: The location to get the temperature for, in the format "City, Country"
        unit: The unit to return the temperature in. (choices: ["celsius", "fahrenheit"])
    Returns:
        The current temperature at the specified location in the specified units, as a float.
    """
    return 22.  # A real function should probably actually get the temperature!

def get_current_wind_speed(location: str) -> float:
    """
    Get the current wind speed in km/h at a given location.
    
    Args:
        location: The location to get the temperature for, in the format "City, Country"
    Returns:
        The current wind speed at the given location in km/h, as a float.
    """
    return 6.  # A real function should probably actually get the wind speed!

tools = [get_current_temperature, get_current_wind_speed]
```

----------------------------------------

TITLE: Load and Tokenize WikiText-2 Dataset in Python
DESCRIPTION: This snippet loads the WikiText-2 dataset using the Hugging Face Datasets library and tokenizes it using the GPT-2 tokenizer. The dataset is loaded into memory, and the text is tokenized into a PyTorch tensor.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/perplexity.md#_snippet_1

LANGUAGE: Python
CODE:
```
from datasets import load_dataset

test = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")
encodings = tokenizer("\n\n".join(test["text"]), return_tensors="pt")
```

----------------------------------------

TITLE: Preprocessing and Inference for Semantic Segmentation with TensorFlow
DESCRIPTION: Loads an image processor, preprocesses an image, and performs semantic segmentation using a TensorFlow model. The code includes initializing the processor and model, running inference, and resizing the output to match the original image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md#2025-04-22_snippet_22

LANGUAGE: python
CODE:
```
from transformers import AutoImageProcessor

image_processor = AutoImageProcessor.from_pretrained("MariaK/scene_segmentation")
inputs = image_processor(image, return_tensors="tf")
```

LANGUAGE: python
CODE:
```
from transformers import TFAutoModelForSemanticSegmentation

model = TFAutoModelForSemanticSegmentation.from_pretrained("MariaK/scene_segmentation")
logits = model(**inputs).logits
```

LANGUAGE: python
CODE:
```
logits = tf.transpose(logits, [0, 2, 3, 1])

upsampled_logits = tf.image.resize(
    logits,
    # We reverse the shape of `image` because `image.size` returns width and height.
    image.size[::-1],
)

pred_seg = tf.math.argmax(upsampled_logits, axis=-1)[0]
```

----------------------------------------

TITLE: Training the Model with fit() in TensorFlow
DESCRIPTION: This snippet shows how to execute the training process for the model using the fit() method, incorporating the training and validation datasets along with the defined callbacks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/sequence_classification.md#2025-04-22_snippet_23

LANGUAGE: python
CODE:
```
>>> model.fit(
...     tf_train_dataset,
...     validation_data=tf_eval_dataset,
...     callbacks=callbacks,
...     epochs=num_epochs,
... )
```

----------------------------------------

TITLE: Preprocessing DocVQA Dataset
DESCRIPTION: Filters and modifies the dataset to keep only English questions, remove unnecessary features, and limit example length.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/document_question_answering.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> updated_dataset = dataset.map(lambda example: {"question": example["query"]["en"]}, remove_columns=["query"])
>>> updated_dataset = updated_dataset.map(
...     lambda example: {"answer": example["answers"][0]}, remove_columns=["answer", "answers"]
... )
>>> updated_dataset = updated_dataset.filter(lambda x: len(x["words"]) + len(x["question"].split()) < 512)
>>> updated_dataset = updated_dataset.remove_columns("words")
>>> updated_dataset = updated_dataset.remove_columns("bounding_boxes")
```

----------------------------------------

TITLE: Configuring Accelerate for Script-based Training
DESCRIPTION: Command to create and save a configuration file for Accelerate when training from a script.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/accelerate.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
accelerate config
```

----------------------------------------

TITLE: Installing PEFT Library from PyPI
DESCRIPTION: Command to install the PEFT (Parameter-Efficient Fine-Tuning) library from PyPI using pip, which is required to use PEFT adapters with Transformers.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/peft.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install peft
```

----------------------------------------

TITLE: Setting Model Data Type for Initialization - Transformers - Python
DESCRIPTION: This snippet demonstrates how to set a PyTorch model's data type at initialization by specifying the `torch_dtype` parameter when loading a model with the `transformers` library. It ensures efficient memory usage by directly loading the model into a specified data type.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/models.md#2025-04-22_snippet_15

LANGUAGE: Python
CODE:
```
from transformers import AutoModelForCausalLM

gemma = AutoModelForCausalLM.from_pretrained("google/gemma-7b", torch_dtype=torch.float16)
```

----------------------------------------

TITLE: Loading Dataset Sample for Image Preprocessing in Python
DESCRIPTION: This snippet loads a small sample of the food101 dataset using the Hugging Face datasets library. It's used as example data for demonstrating image preprocessing techniques.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/image_processors.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from datasets import load_dataset

dataset = load_dataset("food101", split="train[:100]")
```

----------------------------------------

TITLE: Generating Speech from Text and Audio using CLVP in Python
DESCRIPTION: This code snippet demonstrates how to use the CLVP model to generate speech from a given text input and corresponding audio. It utilizes the Hugging Face Transformers library and the datasets library. Required dependencies include 'datasets' and 'transformers'. The expected input is a text string and a dataset containing audio samples, while the output is the generated speech data. Note that the audio should be sampled at 22.05 kHz.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/clvp.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
>>> import datasets
>>> from transformers import ClvpProcessor, ClvpModelForConditionalGeneration

>>> # Define the Text and Load the Audio (We are taking an audio example from HuggingFace Hub using `datasets` library).
>>> text = "This is an example text."
>>> ds = datasets.load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> ds = ds.cast_column("audio", datasets.Audio(sampling_rate=22050))
>>> sample = ds[0]["audio"]

>>> # Define processor and model.
>>> processor = ClvpProcessor.from_pretrained("susnato/clvp_dev")
>>> model = ClvpModelForConditionalGeneration.from_pretrained("susnato/clvp_dev")

>>> # Generate processor output and model output.
>>> processor_output = processor(raw_speech=sample["array"], sampling_rate=sample["sampling_rate"], text=text, return_tensors="pt")
>>> generated_output = model.generate(**processor_output)
```

----------------------------------------

TITLE: Batch Mixed Media Inference with SmolVLM in Python
DESCRIPTION: Shows how to perform batch inference with mixed media inputs (images, videos, and text) using SmolVLM. It includes setting up multiple conversations and processing them together.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/smolvlm.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoProcessor, AutoModelForImageTextToText

processor = AutoProcessor.from_pretrained("HuggingFaceTB/SmolVLM2-256M-Video-Instruct")
model = AutoModelForImageTextToText.from_pretrained(
    "HuggingFaceTB/SmolVLM2-256M-Video-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="cuda"
)

# Conversation for the first image
conversation1 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "path": "/path/to/image.jpg"},
            {"type": "text", "text": "Describe this image."}
        ]
    }
]

# Conversation with two images
conversation2 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "path": "/path/to/image.jpg"},
            {"type": "image", "path": "/path/to/image.jpg"},
            {"type": "text", "text": "What is written in the pictures?"}
        ]
    }
]

# Conversation with pure text
conversation3 = [
    {"role": "user","content": "who are you?"}
]


conversations = [conversation1, conversation2, conversation3]
inputs = processor.apply_chat_template(
    conversation,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt",
).to(model.device, dtype=torch.bfloat16)

generated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=100)
generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)
print(generated_texts[0])
```

----------------------------------------

TITLE: Uppercasing Transcriptions for Wav2Vec2 Tokenizer
DESCRIPTION: Converts all transcriptions to uppercase to match the Wav2Vec2 tokenizer's vocabulary, which is trained only on uppercase characters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/asr.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> def uppercase(example):
...     return {"transcription": example["transcription"].upper()}


>>> minds = minds.map(uppercase)
```

----------------------------------------

TITLE: Using DistilBert with Question Answering Head in TensorFlow
DESCRIPTION: Shows how to load a pre-trained DistilBert model with a question answering head in TensorFlow for tasks like extractive QA or reading comprehension.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/create_a_model.md#2025-04-23_snippet_14

LANGUAGE: python
CODE:
```
>>> from transformers import TFDistilBertForQuestionAnswering

>>> tf_model = TFDistilBertForQuestionAnswering.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: DataCollatorWithPadding Class Definition in Python
DESCRIPTION: Defines DataCollatorWithPadding, a class that ensures proper padding is applied to the dataset elements, aligning them to the same length within the batches. This is essential for sequence-based models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/data_collator.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
[[autodoc]] data.data_collator.DataCollatorWithPadding
```

----------------------------------------

TITLE: Defining the Pre-trained ViLT Model Checkpoint
DESCRIPTION: Sets the model checkpoint to use the pre-trained ViLT model with masked language modeling, which will serve as the base for fine-tuning.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/visual_question_answering.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> model_checkpoint = "dandelin/vilt-b32-mlm"
```

----------------------------------------

TITLE: Pushing Trained ASR Model to Hugging Face Hub in Python
DESCRIPTION: This snippet shows how to push the trained ASR model to the Hugging Face Hub, making it accessible to others.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/asr.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
>>> trainer.push_to_hub()
```

----------------------------------------

TITLE: Loading a PEFT Adapter for a Base Model
DESCRIPTION: Code to load a pre-trained model first and then add a PEFT adapter from the Hub using the load_adapter method, which is useful when you want to keep the original model intact.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/peft.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "facebook/opt-350m"
peft_model_id = "ybelkada/opt-350m-lora"

model = AutoModelForCausalLM.from_pretrained(model_id)
model.load_adapter(peft_model_id)
```

----------------------------------------

TITLE: Using pipeline for sentiment analysis
DESCRIPTION: Demonstrates how to use the `pipeline` function for sentiment analysis with a fine-tuned model. The `pipeline` simplifies the inference process by abstracting away the tokenization and model execution steps.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_25

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline("sentiment-analysis", model="stevhliu/my_awesome_model")
>>> classifier(text)
[{'label': 'POSITIVE', 'score': 0.9994940757751465}]
```

----------------------------------------

TITLE: Using Custom Models with Pipeline
DESCRIPTION: Example showing how to use a custom multilingual model for sentiment analysis.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/quicktour.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> model = AutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
>>> classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
>>> classifier("Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ðŸ¤— Transformers.")
[{'label': '5 stars', 'score': 0.7273}]
```

----------------------------------------

TITLE: Setting up the Trainer with Push to Hub Configuration in PyTorch
DESCRIPTION: Demonstrates how to initialize a Trainer with configured training arguments that include the push_to_hub option.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/model_sharing.md#2025-04-23_snippet_9

LANGUAGE: python
CODE:
```
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=small_train_dataset,
...     eval_dataset=small_eval_dataset,
...     compute_metrics=compute_metrics,
... )
```

----------------------------------------

TITLE: Defining Initial Chat History Structure - Python
DESCRIPTION: Creates a list of dictionaries representing the conversation history for a chat model. Each dictionary contains a 'role' ('system' or 'user') and 'content' representing a message, formatted according to the expected input for conversational pipelines.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/conversations.md#_snippet_2

LANGUAGE: python
CODE:
```
chat = [
    {"role": "system", "content": "You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986."},
    {"role": "user", "content": "Hey, can you tell me any fun things to do in New York?"}
]
```

----------------------------------------

TITLE: Installing ðŸ¤— Accelerate using pip
DESCRIPTION: Command to install the ðŸ¤— Accelerate library using pip package manager.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/accelerate.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install accelerate
```

----------------------------------------

TITLE: Trainer Implementation with FSDP
DESCRIPTION: Example of using Trainer API with FSDP configuration for distributed training, including training arguments setup and trainer initialization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="your-model",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    fsdp_config="path/to/fsdp_config",
    fsdp_strategy="full_shard",
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    processing_class=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()
```

----------------------------------------

TITLE: Loading Converted Model and Running Forward Pass
DESCRIPTION: Loads a model from a previously saved, converted checkpoint using `from_pretrained`. It then prepares a sample input sequence (`input_ids`) and runs a forward pass (implicitly via the `generate` method) to get the model output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/add_new_model.md#_snippet_21

LANGUAGE: Python
CODE:
```
model = BrandNewLlamaModel.from_pretrained("/path/to/converted/checkpoint/folder")
input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]
output = model.generate(input_ids).last_hidden_states
```

----------------------------------------

TITLE: Tokenizing text (TensorFlow)
DESCRIPTION: This snippet shows how to tokenize text using `AutoTokenizer` and return TensorFlow tensors. The tokenizer is loaded from the fine-tuned model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_29

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_model")
>>> inputs = tokenizer(text, return_tensors="tf")
```

----------------------------------------

TITLE: Compiling a Transformers Model with 'reduce-overhead' Mode
DESCRIPTION: This snippet illustrates how to compile a Transformers model with the `reduce-overhead` mode. This mode is designed to reduce Python overhead, potentially leading to faster inference times.  Different modes should be tested to find the optimal setting.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_torch_compile.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("google/gemma-2b", device_map="auto")
compiled_model = torch.compile(model, mode="reduce-overhead")
```

----------------------------------------

TITLE: Initialize BERT Model for Multiple Choice - PyTorch
DESCRIPTION: Loads a pre-trained BERT model configured for multiple choice tasks using AutoModelForMultipleChoice.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/multiple_choice.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer

model = AutoModelForMultipleChoice.from_pretrained("google-bert/bert-base-uncased")
```

----------------------------------------

TITLE: Text-Conditional Music Generation in Python
DESCRIPTION: This snippet illustrates how to generate music samples conditioned on specific text prompts using the MusicgenProcessor. It involves preprocessing the text input and generating audio values based on the processed inputs, while also configuring the guidance scale for optimal output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/musicgen.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor, MusicgenForConditionalGeneration

>>> processor = AutoProcessor.from_pretrained("facebook/musicgen-small")
>>> model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")

>>> inputs = processor(
...     text=["80s pop track with bassy drums and synth", "90s rock song with loud guitars and heavy drums"],
...     padding=True,
...     return_tensors="pt",
... )
>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)
```

----------------------------------------

TITLE: Running with DeepSpeed on multiple GPUs using PyTorch launcher
DESCRIPTION: Command to execute a training script with DeepSpeed enabled across multiple GPUs using the PyTorch distributed launcher.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/deepspeed.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
torch.distributed.run --nproc_per_node=2 your_program.py <normal cl args> --deepspeed ds_config.json
```

----------------------------------------

TITLE: Loading and Splitting the SceneParse150 Dataset
DESCRIPTION: This code loads a small subset of the SceneParse150 dataset and splits it into training and testing sets for experimentation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/sequence_classification.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> ds = load_dataset("scene_parse_150", split="train[:50]")
```

----------------------------------------

TITLE: Initializing Accelerator in Python
DESCRIPTION: Import and create an Accelerator object to automatically detect and initialize distributed training components.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/accelerate.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from accelerate import Accelerator

accelerator = Accelerator()
```

----------------------------------------

TITLE: Enable Sampling for Generation
DESCRIPTION: This code demonstrates how to enable sampling (random choice) during text generation using the `do_sample=True` parameter. Setting a seed ensures reproducibility. By default, the model uses greedy decoding, which can result in repetitive and predictable outputs. Sampling introduces randomness, leading to more creative and diverse results.  The input text is "I am a cat.".
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/llm_tutorial.md#_snippet_7

LANGUAGE: python
CODE:
```
>>> # Set seed or reproducibility -- you don't need this unless you want full reproducibility
>>> from transformers import set_seed
>>> set_seed(42)

>>> model_inputs = tokenizer(["I am a cat."], return_tensors="pt").to("cuda")

>>> # LLM + greedy decoding = repetitive, boring output
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'I am a cat. I am a cat. I am a cat. I am a cat'

>>> # With sampling, the output becomes more creative!
>>> generated_ids = model.generate(**model_inputs, do_sample=True)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'I am a cat.  Specifically, I am an indoor-only cat.  I'
```

----------------------------------------

TITLE: Loading Images with PIL and Requests - Python
DESCRIPTION: This snippet demonstrates how to load and convert images from a given URL using the Python Imaging Library (PIL) and the requests library. These images will later be used for feature extraction and similarity computation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_feature_extraction.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from PIL import Image
import requests

img_urls = ["https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png", "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.jpeg"]
image_real = Image.open(requests.get(img_urls[0], stream=True).raw).convert("RGB")
image_gen = Image.open(requests.get(img_urls[1], stream=True).raw).convert("RGB")
```

----------------------------------------

TITLE: Running Summarization Script in TensorFlow
DESCRIPTION: Executes the `run_summarization.py` script from the TensorFlow examples directory to fine-tune a T5-small model on the CNN/DailyMail dataset.  Includes dataset configuration, output directory, batch size, training epochs, and specifies training and evaluation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/run_scripts.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
```bash
python examples/tensorflow/summarization/run_summarization.py  \
    --model_name_or_path google-t5/t5-small \
    # remove the `max_train_samples`, `max_eval_samples` and `max_predict_samples` if everything works
    --max_train_samples 50 \
    --max_eval_samples 50 \
    --max_predict_samples 50 \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval \
```
```

----------------------------------------

TITLE: Initializing BERT Tokenizer in Python
DESCRIPTION: Loads the BERT tokenizer for preprocessing the sentence beginnings and possible endings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/multiple_choice.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
```

----------------------------------------

TITLE: PyTorch Evaluation Loop
DESCRIPTION: This code implements an evaluation loop for a PyTorch model. It iterates through the evaluation data loader, moves the batch data to the device, performs a forward pass, calculates predictions, and accumulates the results using the `evaluate` library.  The accuracy metric is calculated at the end.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_25

LANGUAGE: Python
CODE:
```
import evaluate

metric = evaluate.load("accuracy")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

----------------------------------------

TITLE: Loading Quantized Qwen2 Model with BitsAndBytes (Python)
DESCRIPTION: This snippet shows how to load a Qwen2 model quantized to 4-bits using the `BitsAndBytesConfig`. This technique significantly reduces the model's memory footprint while maintaining performance. It configures 4-bit loading with specific computation dtype, quantization type, and double quantization settings, then loads the model and tokenizer for inference. Requires `torch`, `transformers`, and `bitsandbytes`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2.md#_snippet_3

LANGUAGE: python
CODE:
```
# pip install -U flash-attn --no-build-isolation
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-7B")
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2-7B",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    quantization_config=quantization_config,
    attn_implementation="flash_attention_2"
)

inputs = tokenizer("The Qwen2 model family is", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Loading and Splitting the ELI5 Dataset
DESCRIPTION: Code to load a subset of the ELI5 dataset from r/askscience and split it into training and testing sets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/language_modeling.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> eli5 = load_dataset("eli5", split="train_asks[:5000]")
```

LANGUAGE: python
CODE:
```
>>> eli5 = eli5.train_test_split(test_size=0.2)
```

LANGUAGE: python
CODE:
```
>>> eli5["train"][0]
```

----------------------------------------

TITLE: Loading a Pre-trained Adapter in Python
DESCRIPTION: This snippet covers loading pre-trained adapter weights from a local directory or a repository using a structured approach. It is crucial for utilizing pre-existing adapters for your models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/peft.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("klcsp/gemma7b-lora-alpaca-11-v1")

```

----------------------------------------

TITLE: Loading and Processing Dataset with Hugging Face
DESCRIPTION: Loads the Yelp review dataset and preprocesses it using a BERT tokenizer with padding and truncation for consistent sequence lengths.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/training.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from datasets import load_dataset

dataset = load_dataset("yelp_review_full")
dataset["train"][100]

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
```

----------------------------------------

TITLE: Run Summarization with Custom Dataset
DESCRIPTION: Runs the summarization script with a custom dataset.  This command specifies the paths to the training and validation files, the text and summary columns, and other relevant parameters.  It requires defining `train_file`, `validation_file`, `text_column`, and `summary_column`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/run_scripts.md#_snippet_12

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --train_file path_to_csv_or_jsonlines_file \
    --validation_file path_to_csv_or_jsonlines_file \
    --text_column text_column_name \
    --summary_column summary_column_name \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --overwrite_output_dir \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate
```

----------------------------------------

TITLE: Tokenize and convert data to NumPy arrays
DESCRIPTION: This snippet tokenizes the 'sentence' column of the CoLA dataset using a pre-trained tokenizer. It converts the tokenized data and labels into NumPy arrays for use with Keras.  Padding is applied to ensure all sequences have the same length.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_10

LANGUAGE: Python
CODE:
```
from transformers import AutoTokenizer
import numpy as np

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")
tokenized_data = tokenizer(dataset["sentence"], return_tensors="np", padding=True)
# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras
tokenized_data = dict(tokenized_data)

labels = np.array(dataset["label"])  # Label is already an array of 0 and 1
```

----------------------------------------

TITLE: Visual Question Answering (VQA) Pipeline Setup
DESCRIPTION: Shows how to implement a multimodal pipeline for visual question answering that combines image and text inputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/pipeline_tutorial.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import pipeline

image = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
question = "Where is the cat?"

vqa = pipeline(task="vqa")
preds = vqa(image=image, question=question)
```

----------------------------------------

TITLE: Using Pipeline with HuggingFace Datasets
DESCRIPTION: This example shows how to use Pipeline with the Datasets library, using KeyDataset utility to efficiently iterate through audio data from a dataset for speech recognition.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pipeline_tutorial.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
# KeyDataset is a util that will just output the item we're interested in.
from transformers.pipelines.pt_utils import KeyDataset

pipe = pipeline(model="hf-internal-testing/tiny-random-wav2vec2", device=0)
dataset = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation[:10]")

for out in pipe(KeyDataset(dataset["audio"])):
    print(out)
```

----------------------------------------

TITLE: Installing Datasets Library
DESCRIPTION: This command installs the `datasets` library, which is used for loading and working with datasets from the Hugging Face Hub.  It's a prerequisite for using the audio and image processing examples provided in the document.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/preprocessing.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
pip install datasets
```

----------------------------------------

TITLE: M2M100 Translation from Chinese to English in Python
DESCRIPTION: This code demonstrates how to use the M2M100 model for translation. It loads the tokenizer and model, specifies the source language ('zh' for Chinese), and encodes the input text. It then generates the translation by forcing the beginning-of-sequence token to the target language ('en' for English).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/multilingual.md#_snippet_1

LANGUAGE: Python
CODE:
```
>>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

>>> en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
>>> chinese_text = "ä¸è¦æ’æ‰‹å·«å¸«çš„äº‹å‹™, å› ç‚ºä»–å€‘æ˜¯å¾®å¦™çš„, å¾ˆå¿«å°±æœƒç™¼æ€’."

>>> tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M", src_lang="zh")
>>> model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")
```

LANGUAGE: Python
CODE:
```
>>> encoded_zh = tokenizer(chinese_text, return_tensors="pt")
```

LANGUAGE: Python
CODE:
```
>>> generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id("en"))
>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
'Do not interfere with the matters of the witches, because they are delicate and will soon be angry.'
```

----------------------------------------

TITLE: Pipeline with Multiple Inputs
DESCRIPTION: Example of using pipeline with multiple text inputs as a list.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/pipelines.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> pipe = pipeline("text-classification")
>>> pipe(["This restaurant is awesome", "This restaurant is awful"])
[{'label': 'POSITIVE', 'score': 0.9998743534088135},
 {'label': 'NEGATIVE', 'score': 0.9996669292449951}]
```

----------------------------------------

TITLE: Loading Models with Auto dtype
DESCRIPTION: This code shows how to load a model with the special `torch_dtype="auto"` argument, which automatically infers the data type (dtype) from the model's weights. This ensures that the model is loaded with the optimal memory configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/model.md#_snippet_5

LANGUAGE: python
CODE:
```
model = T5ForConditionalGeneration.from_pretrained("t5", torch_dtype="auto")
```

----------------------------------------

TITLE: Defining compute_metrics function
DESCRIPTION: Create a function to compute the Word Error Rate metric for model evaluation during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/tasks/asr.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
import numpy as np

def compute_metrics(pred):
    pred_logits = pred.predictions
    pred_ids = np.argmax(pred_logits, axis=-1)

    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id

    pred_str = processor.batch_decode(pred_ids)
    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)

    wer = wer.compute(predictions=pred_str, references=label_str)

    return {"wer": wer}
```

----------------------------------------

TITLE: Generating Text Embeddings and Querying Top k Images
DESCRIPTION: The snippet demonstrates how to generate text embeddings using a given query and retrieve the indices and scores of the top k most relevant images utilizing the previously defined function. It exemplifies retrieving and displaying relevant documents based on textual queries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_document_retrieval.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
with torch.no_grad():
  text_embeds = model(**processor(text="a document about Mars expedition").to("cuda"), return_tensors="pt").embeddings
indices, scores = find_top_k_indices_batched(ds_with_embeddings, text_embeds, processor, k=3, batch_size=4)
print(indices, scores)
```

----------------------------------------

TITLE: Loading TFAutoModelForSequenceClassification in TensorFlow
DESCRIPTION: Shows how to load a pre-trained model for sequence classification using TFAutoModelForSequenceClassification in TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/quicktour.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> from transformers import TFAutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
```

----------------------------------------

TITLE: Int8 inference with generate method
DESCRIPTION: This code snippet showcases how to perform inference with an Int8 quantized model using the `generate()` method. It includes tokenization, moving input to the GPU, and decoding the output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_infer_gpu_one.md#2025-04-22_snippet_14

LANGUAGE: Python
CODE:
```
"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nmodel_name = \"bigscience/bloom-2b5\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel_8bit = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True))\n\nprompt = \"Hello, my llama is cute\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\ngenerated_ids = model.generate(**inputs)\noutputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
```

----------------------------------------

TITLE: Using ASR Pipeline for Inference in Python
DESCRIPTION: This snippet demonstrates how to use a fine-tuned ASR model in a pipeline for automatic speech recognition on an audio file.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/asr.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> transcriber = pipeline("automatic-speech-recognition", model="stevhliu/my_awesome_asr_minds_model")
>>> transcriber(audio_file)
{'text': 'I WOUD LIKE O SET UP JOINT ACOUNT WTH Y PARTNER'}
```

----------------------------------------

TITLE: Masked Language Modeling with Fill-Mask Pipeline
DESCRIPTION: Shows how to use the fill-mask pipeline for masked language modeling, where the model predicts a masked token in a sequence with access to all surrounding tokens. The example formats the results for better readability.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/task_summary.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
text = "Hugging Face is a community-based open-source <mask> for machine learning."
fill_mask = pipeline(task="fill-mask")
preds = fill_mask(text, top_k=1)
preds = [
    {
        "score": round(pred["score"], 4),
        "token": pred["token"],
        "token_str": pred["token_str"],
        "sequence": pred["sequence"],
    }
    for pred in preds
]
preds
```

----------------------------------------

TITLE: Installing Transformers with Different Frameworks
DESCRIPTION: Various pip installation commands for Transformers with different deep learning frameworks
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/installation.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
pip install transformers
```

LANGUAGE: bash
CODE:
```
pip install transformers[torch]
```

LANGUAGE: bash
CODE:
```
pip install transformers[tf-cpu]
```

LANGUAGE: bash
CODE:
```
pip install transformers[flax]
```

----------------------------------------

TITLE: Accessing Model Configuration - Transformers - Python
DESCRIPTION: Demonstrates how to load a model using the `from_pretrained` method and subsequently access its associated configuration object through the `.config` attribute. This illustrates the standard way to retrieve model configuration after loading pretrained weights in the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/add_new_model.md#_snippet_0

LANGUAGE: Python
CODE:
```
model = BrandNewLlamaModel.from_pretrained("username/brand_new_llama")
model.config
```

----------------------------------------

TITLE: Generating Text with GPTSAN-japanese Model in Python
DESCRIPTION: This snippet demonstrates how to use the GPTSAN-japanese model for text generation. It initializes the tokenizer and model, sets up input text, and generates new tokens based on the input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gptsan-japanese.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModel, AutoTokenizer
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
>>> model = AutoModel.from_pretrained("Tanrei/GPTSAN-japanese").cuda()
>>> x_tok = tokenizer("ã¯ã€", prefix_text="ç¹”ç”°ä¿¡é•·", return_tensors="pt")
>>> torch.manual_seed(0)
>>> gen_tok = model.generate(x_tok.input_ids.cuda(), token_type_ids=x_tok.token_type_ids.cuda(), max_new_tokens=20)
>>> tokenizer.decode(gen_tok[0])
'ç¹”ç”°ä¿¡é•·ã¯ã€2004å¹´ã«ã€Žæˆ¦å›½BASARAã€ã®ãŸã‚ã«ã€è±Šè‡£ç§€å‰'
```

----------------------------------------

TITLE: Installing AQLM Library with Kernel Support
DESCRIPTION: Install AQLM library with GPU and CPU support, requires Python 3.10+
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/aqlm.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install aqlm[gpu,cpu]
```

----------------------------------------

TITLE: Running Summarization Training in TensorFlow
DESCRIPTION: Script to fine-tune T5-small model on CNN/DailyMail dataset for summarization using TensorFlow. Includes model configuration and training parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/run_scripts.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
python examples/tensorflow/summarization/run_summarization.py  \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval
```

----------------------------------------

TITLE: GLUE Task Fine-tuning without Trainer
DESCRIPTION: This bash script demonstrates how to fine-tune a model on a GLUE task using the Transformers library without the Trainer API. It uses the 'run_glue_no_trainer.py' script, which provides more flexibility for customization in the training loop.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/README.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
export TASK_NAME=mrpc

python run_glue_no_trainer.py \
  --model_name_or_path google-bert/bert-base-cased \
  --task_name $TASK_NAME \
  --max_length 128 \
  --per_device_train_batch_size 32 \
  --learning_rate 2e-5 \
  --num_train_epochs 3 \
  --output_dir /tmp/$TASK_NAME/
```

----------------------------------------

TITLE: Implementing a Custom Pipeline Class in Python
DESCRIPTION: Defines a custom pipeline class by inheriting from the base Pipeline class and implementing the required methods: _sanitize_parameters, preprocess, _forward, and postprocess.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/add_new_pipeline.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import Pipeline

class MyPipeline(Pipeline):
    def _sanitize_parameters(self, **kwargs):
        preprocess_kwargs = {}
        if "maybe_arg" in kwargs:
            preprocess_kwargs["maybe_arg"] = kwargs["maybe_arg"]
        return preprocess_kwargs, {}, {}

    def preprocess(self, inputs, maybe_arg=2):
        model_input = Tensor(inputs["input_ids"])
        return {"model_input": model_input}

    def _forward(self, model_inputs):
        # model_inputs == {"model_input": model_input}
        outputs = self.model(**model_inputs)
        # Maybe {"logits": Tensor(...)}
        return outputs

    def postprocess(self, model_outputs):
        best_class = model_outputs["logits"].softmax(-1)
        return best_class
```

----------------------------------------

TITLE: Processing Images with Fast Image Processor on GPU in Python
DESCRIPTION: Shows how to use a fast image processor to process images on a CUDA device, including reading an image and specifying the output tensor type.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/image_processor.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from torchvision.io import read_image
from transformers import DetrImageProcessorFast

images = read_image("image.jpg")
processor = DetrImageProcessorFast.from_pretrained("facebook/detr-resnet-50")
images_processed = processor(images, return_tensors="pt", device="cuda")
```

----------------------------------------

TITLE: Single Image Inference with OmDet-Turbo in Python
DESCRIPTION: This code snippet demonstrates how to perform zero-shot object detection on a single image using the OmDet-Turbo model. It covers loading the model and processor, preparing inputs including image and text labels, running inference, and post-processing the results to display detected objects with their confidence scores and locations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/omdet-turbo.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> import torch
>>> import requests
>>> from PIL import Image

>>> from transformers import AutoProcessor, OmDetTurboForObjectDetection

>>> processor = AutoProcessor.from_pretrained("omlab/omdet-turbo-swin-tiny-hf")
>>> model = OmDetTurboForObjectDetection.from_pretrained("omlab/omdet-turbo-swin-tiny-hf")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> text_labels = ["cat", "remote"]
>>> inputs = processor(image, text=text_labels, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> # convert outputs (bounding boxes and class logits)
>>> results = processor.post_process_grounded_object_detection(
...     outputs,
...     target_sizes=[(image.height, image.width)],
...     text_labels=text_labels,
...     threshold=0.3,
...     nms_threshold=0.3,
... )
>>> result = results[0]
>>> boxes, scores, text_labels = result["boxes"], result["scores"], result["text_labels"]
>>> for box, score, text_label in zip(boxes, scores, text_labels):
...     box = [round(i, 2) for i in box.tolist()]
...     print(f"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}")
Detected remote with confidence 0.768 at location [39.89, 70.35, 176.74, 118.04]
Detected cat with confidence 0.72 at location [11.6, 54.19, 314.8, 473.95]
Detected remote with confidence 0.563 at location [333.38, 75.77, 370.7, 187.03]
Detected cat with confidence 0.552 at location [345.15, 23.95, 639.75, 371.67]
```

----------------------------------------

TITLE: Using Scaled Dot Product Attention (SDPA) with GPT-NeoX
DESCRIPTION: Code for loading the GPT-NeoX model with PyTorch's native Scaled Dot Product Attention implementation. This provides optimization on compatible hardware and is used by default in PyTorch >=2.1.1.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gpt_neox.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import GPTNeoXForCausalLM
model = GPTNeoXForCausalLM.from_pretrained("EleutherAI/gpt-neox-20b", torch_dtype=torch.float16, attn_implementation="sdpa")
...
```

----------------------------------------

TITLE: Loading Custom Models at Specific Revision - Transformers - Python
DESCRIPTION: This snippet illustrates the procedure for loading a custom model from a specific commit hash to ensure model consistency, providing an extra layer of security by preventing unexpected changes in the model code.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/models.md#2025-04-22_snippet_19

LANGUAGE: Python
CODE:
```
commit_hash = "ed94a7c6247d8aedce4647f00f20de6875b5b292"
model = AutoModelForImageClassification.from_pretrained(
    "sgugger/custom-resnet50d", trust_remote_code=True, revision=commit_hash
)
```

----------------------------------------

TITLE: Model Evaluation
DESCRIPTION: Evaluates the trained student model on the test dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/knowledge_distillation_for_image_classification.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
trainer.evaluate(processed_datasets["test"])
```

----------------------------------------

TITLE: Text Generation Inference with Emu3
DESCRIPTION: This code snippet demonstrates how to load the Emu3 model and perform text generation using both text and image inputs. It uses the `Emu3Processor` to preprocess the inputs and the `Emu3ForConditionalGeneration` model to generate the output. The generated output is then decoded back into human-readable text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/emu3.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import Emu3Processor, Emu3ForConditionalGeneration
import torch
from PIL import Image
import requests

processor = Emu3Processor.from_pretrained("BAAI/Emu3-Chat-hf")
model = Emu3ForConditionalGeneration.from_pretrained("BAAI/Emu3-Chat-hf", torch_dtype=torch.bfloat16, device_map="cuda")

# prepare image and text prompt
url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)
prompt = "What do you see in this image?<image>"

inputs = processor(images=image, text=prompt, return_tensors="pt").to(model.device, dtype=torch.bfloat16)

# autoregressively complete prompt
output = model.generate(**inputs, max_new_tokens=50)
print(processor.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Setting Dataset Format to PyTorch
DESCRIPTION: Sets the format of the tokenized dataset to return PyTorch tensors instead of lists.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/training.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
tokenized_datasets.set_format("torch")
```

----------------------------------------

TITLE: Setting Up TensorFlow Optimizer for Translation Model
DESCRIPTION: This code configures the AdamWeightDecay optimizer for fine-tuning a T5 translation model in TensorFlow with appropriate learning rate and weight decay settings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/translation.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
>>> from transformers import AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

----------------------------------------

TITLE: Padding Tokenized Inputs
DESCRIPTION: Tokenizes and pads a batch of sentences to the same length.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/preprocessing.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True)
print(encoded_input)
```

----------------------------------------

TITLE: Compiling and Training Model in TensorFlow
DESCRIPTION: This code compiles the model, sets up a PushToHubCallback, and starts the training process using the fit method in TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
import tensorflow as tf
from transformers.keras_callbacks import PushToHubCallback

model.compile(optimizer=optimizer)  # No loss argument!

callback = PushToHubCallback(
    output_dir="my_awesome_eli5_clm-model",
    tokenizer=tokenizer,
)

model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])
```

----------------------------------------

TITLE: Accessing Model Configuration in Hugging Face Transformers
DESCRIPTION: This snippet demonstrates how to load a pre-trained model and access its configuration object. It shows the standard pattern for model instantiation from pre-trained weights and how the configuration is accessible as a property of the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/templates/adding_a_new_model/open_model_proposals/ADD_BIG_BIRD.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
# assuming that `brand_new_bert` belongs to the organization `brandy`
model = BrandNewBertModel.from_pretrained("brandy/brand_new_bert")
model.config  # model has access to its config
```

----------------------------------------

TITLE: Resuming Training from Specific Checkpoint
DESCRIPTION: This command shows how to resume training from a specific checkpoint by using the 'resume_from_checkpoint' argument to specify the path to the desired checkpoint.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/run_scripts.md#2025-04-22_snippet_14

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py 
    --model_name_or_path google-t5/t5-small 
    --do_train 
    --do_eval 
    --dataset_name cnn_dailymail 
    --dataset_config "3.0.0" 
    --source_prefix "summarize: " 
    --output_dir /tmp/tst-summarization 
    --per_device_train_batch_size=4 
    --per_device_eval_batch_size=4 
    --overwrite_output_dir 
    --resume_from_checkpoint path_to_specific_checkpoint 
    --predict_with_generate
```

----------------------------------------

TITLE: Loading Multimodal Tokenizer with Extra Special Tokens in Python
DESCRIPTION: This snippet demonstrates how to load a multimodal tokenizer with additional special tokens for a vision-language model using the HuggingFace library. It is essential to have the HuggingFace Transformers library installed. The `AutoTokenizer.from_pretrained` method loads the tokenizer, and special tokens are defined using the `extra_special_tokens` parameter. Inputs include the model name and special tokens, and outputs include the special token string and its corresponding ID.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/tokenizer.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
vision_tokenizer = AutoTokenizer.from_pretrained(
    "llava-hf/llava-1.5-7b-hf",
    extra_special_tokens={"image_token": "<image>", "boi_token": "<image_start>", "eoi_token": "<image_end>"}
)
print(vision_tokenizer.image_token, vision_tokenizer.image_token_id)
("<image>", 32000)
```

----------------------------------------

TITLE: Loading and Using BERT Japanese with Character Tokenization in Python
DESCRIPTION: This snippet shows how to load and use a pre-trained BERT Japanese model that uses character-level tokenization. It utilizes the Transformers library. The code tokenizes a Japanese sentence, prints the decoded tokens, and then passes the tokenized input to the model for processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/bert-japanese.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> bertjapanese = AutoModel.from_pretrained("cl-tohoku/bert-base-japanese-char")
>>> tokenizer = AutoTokenizer.from_pretrained("cl-tohoku/bert-base-japanese-char")

>>> ## Input Japanese Text
>>> line = "å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€‚"

>>> inputs = tokenizer(line, return_tensors="pt")

>>> print(tokenizer.decode(inputs["input_ids"][0]))
[CLS] å¾ è¼© ã¯ çŒ« ã§ ã‚ ã‚‹ ã€‚ [SEP]

>>> outputs = bertjapanese(**inputs)
```

----------------------------------------

TITLE: Fine-tuning T5 for Translation with Custom Files
DESCRIPTION: This example demonstrates how to fine-tune a T5 model for translation using custom JSONLINES files. It specifies the paths to training and validation files, along with other necessary parameters for the model and training process.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation/README.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
python examples/pytorch/translation/run_translation.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --source_lang en \
    --target_lang ro \
    --source_prefix "translate English to Romanian: " \
    --dataset_name wmt16 \
    --dataset_config_name ro-en \
    --train_file path_to_jsonlines_file \
    --validation_file path_to_jsonlines_file \
    --output_dir /tmp/tst-translation \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Share Model to Hugging Face Hub
DESCRIPTION: This code snippet demonstrates how to upload the trained model to the Hugging Face Model Hub. It uses the `push_to_hub` argument to create a repository and upload the model. The `push_to_hub_model_id` argument allows specifying a custom repository name.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/run_scripts_fr.md#_snippet_18

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --push_to_hub \
    --push_to_hub_model_id finetuned-t5-cnn_dailymail \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Few-shot Prompting with IDEFICS
DESCRIPTION: Illustrates few-shot prompting with IDEFICS, where an example is provided to guide the model in generating a response in a specific format for a new image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/idefics.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
prompt = ["User:",
           "https://images.unsplash.com/photo-1543349689-9a4d426bee8e?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3501&q=80",
           "Describe this image.\nAssistant: An image of the Eiffel Tower at night. Fun fact: the Eiffel Tower is the same height as an 81-storey building.\n",
           "User:",
           "https://images.unsplash.com/photo-1524099163253-32b7f0256868?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3387&q=80",
           "Describe this image.\nAssistant:"]

inputs = processor(prompt, return_tensors="pt").to("cuda")
bad_words_ids = processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

generated_ids = model.generate(**inputs, max_new_tokens=30, bad_words_ids=bad_words_ids)
generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
print(generated_text[0])
```

----------------------------------------

TITLE: Running Summarization Script in PyTorch
DESCRIPTION: This command runs the `run_summarization.py` script in the `examples/pytorch/summarization` directory using Python. It specifies several parameters, including the model to use (`google-t5/t5-small`), the dataset (`cnn_dailymail`), and training/evaluation configurations. It also shows how to define the source prefix for the t5 model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/run_scripts.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
```bash
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```
```

----------------------------------------

TITLE: M2M100 Chinese to English Translation
DESCRIPTION: Demonstrates how to perform Chinese to English translation using the facebook/m2m100_418M model. Shows setting source language, tokenization, and generating translations with forced BOS token.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/multilingual.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

>>> en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
>>> chinese_text = "ä¸è¦æ’æ‰‹å·«å¸«çš„äº‹å‹™, å› ç‚ºä»–å€‘æ˜¯å¾®å¦™çš„, å¾ˆå¿«å°±æœƒç™¼æ€’."

>>> tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M", src_lang="zh")
>>> model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")

>>> encoded_zh = tokenizer(chinese_text, return_tensors="pt")
>>> generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id("en"))
>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
```

----------------------------------------

TITLE: Resuming Training from Latest Checkpoint
DESCRIPTION: Example of resuming training from the latest checkpoint by setting resume_from_checkpoint to True in the train method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/trainer.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
trainer.train(resume_from_checkpoint=True)
```

----------------------------------------

TITLE: Loading a Pre-trained Image Processor using AutoImageProcessor Python
DESCRIPTION: This code snippet demonstrates loading a pre-trained image processor using `AutoImageProcessor.from_pretrained`. It initializes the image processor with the specified checkpoint, "google/vit-base-patch16-224", for vision tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/autoclass_tutorial.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
```

----------------------------------------

TITLE: Depth Estimation with Default Model in Python
DESCRIPTION: Code snippet demonstrating how to estimate depth in an image using the transformers pipeline with its default depth estimation model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/task_summary.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> depth_estimator = pipeline(task="depth-estimation")
>>> preds = depth_estimator(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
```

----------------------------------------

TITLE: Creating a TensorFlow Dynamic Padding Data Collator
DESCRIPTION: This snippet shows how to create a DataCollatorForLanguageModeling instance in TensorFlow, which also uses dynamic padding and ensures compatibility with TensorFlow tensors during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorForLanguageModeling

>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors="tf")
```

----------------------------------------

TITLE: Dynamic Padding Data Collator - Python (PyTorch)
DESCRIPTION: This snippet initializes a data collator for PyTorch, enabling dynamic padding of sequences to the maximum length within a batch during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

----------------------------------------

TITLE: Attempting to Load GPT2 for Question Answering (Incorrect Usage)
DESCRIPTION: This code attempts to load a GPT2 model using `AutoModelForQuestionAnswering`, which is incorrect as GPT2 is not designed for question answering tasks. This results in a `ValueError`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/troubleshooting.md#_snippet_8

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor, AutoModelForQuestionAnswering

>>> processor = AutoProcessor.from_pretrained("openai-community/gpt2-medium")
>>> model = AutoModelForQuestionAnswering.from_pretrained("openai-community/gpt2-medium")
ValueError: Unrecognized configuration class <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'> for this kind of AutoModel: AutoModelForQuestionAnswering.
Model type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig, BloomConfig, ...
```

----------------------------------------

TITLE: Manual Inference with TensorFlow: Processing Results
DESCRIPTION: Processes the model's output logits to determine the predicted class. The class ID with the highest probability is converted to its text label using the model's id2label mapping.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_30

LANGUAGE: python
CODE:
```
>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])
>>> model.config.id2label[predicted_class_id]
'POSITIVE'
```

----------------------------------------

TITLE: Preprocessing Image for Inference in Python
DESCRIPTION: This snippet handles the preprocessing of the loaded image before performing inference. It converts the image to tensor format, allowing it to be processed by the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_to_image.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
pixel_values = processor(image, return_tensors="pt").pixel_values
print(pixel_values.shape)

pixel_values = pixel_values.to(device)
```

----------------------------------------

TITLE: Loading GGUF Files with AutoTokenizer and AutoModelForCausalLM - Python
DESCRIPTION: This snippet demonstrates how to load a GGUF file using the AutoTokenizer and AutoModelForCausalLM classes from the Transformers library. The model ID and filename are specified as parameters to the 'from_pretrained' method to retrieve the tokenizer and model correctly.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/gguf.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF"
filename = "tinyllama-1.1b-chat-v1.0.Q6_K.gguf"

tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)
model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)
```

----------------------------------------

TITLE: Examining Extracted Features from Images
DESCRIPTION: Code to inspect the output embeddings from the feature extraction pipeline, showing the embedding dimensionality and structure.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_feature_extraction.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
# ë‹¨ì¼ ì¶œë ¥ì˜ ê¸¸ì´ êµ¬í•˜ê¸°
print(len(outputs[0][0]))
# ì¶œë ¥ ê²°ê³¼ í‘œì‹œí•˜ê¸°
print(outputs)

# 768
# [[[-0.03909236937761307, 0.43381670117378235, -0.06913255900144577,
```

----------------------------------------

TITLE: Tokenizing Single Sentence
DESCRIPTION: Tokenizes a single sentence using the loaded tokenizer and prints the encoded output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/preprocessing.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
print(encoded_input)
```

----------------------------------------

TITLE: Loading Pre-trained Processor with AutoProcessor in Python
DESCRIPTION: This snippet demonstrates the loading of a processor that combines both image and text preprocessing tools for multimodal tasks using the AutoProcessor class.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/autoclass_tutorial.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")
```

----------------------------------------

TITLE: Loading an 8-bit model with specific data type configuration
DESCRIPTION: Example of loading a model in 8-bit with explicit torch_dtype configuration, demonstrating how to control non-quantized layer types.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_8bit=True)

model_8bit = AutoModelForCausalLM.from_pretrained(
    "facebook/opt-350m", 
    device_map="auto",
    quantization_config=quantization_config, 
    torch_dtype="auto"
)
model_8bit.model.decoder.layers[-1].final_layer_norm.weight.dtype
```

----------------------------------------

TITLE: Extracting predicted class (PyTorch)
DESCRIPTION: This snippet demonstrates how to extract the predicted class with the highest probability from the `logits` and convert it to a text label using `id2label` from the model's configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_28

LANGUAGE: python
CODE:
```
>>> predicted_class_id = logits.argmax().item()
>>> model.config.id2label[predicted_class_id]
'POSITIVE'
```

----------------------------------------

TITLE: Configuring Mixed Precision Training with tf32 - Python
DESCRIPTION: This snippet shows how to enable mixed precision training with the tf32 mode for efficient GPU performance. tf32 is designed to improve throughput while still maintaining a good precision range, and it is beneficial when working with NVIDIA Ampere architecture.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
import torch
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

from transformers import TrainingArguments

args = TrainingArguments(
    per_device_train_batch_size=4,
    gradient_accumulation_steps=16,
    gradient_checkpointing=True,
    bf16=True,
    tf32=True,
)
```

----------------------------------------

TITLE: Initializing Accelerator in Python
DESCRIPTION: Code snippet showing how to import and create an Accelerator object for distributed training setup.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/accelerate.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from accelerate import Accelerator

accelerator = Accelerator()
```

----------------------------------------

TITLE: Pushing a PyTorch Model to the Hub after Training
DESCRIPTION: Shows how to manually push a PyTorch model to the Hugging Face Hub after training using the push_to_hub method of the Trainer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/model_sharing.md#2025-04-23_snippet_10

LANGUAGE: python
CODE:
```
>>> trainer.push_to_hub()
```

----------------------------------------

TITLE: Loading PEFT Adapter Using load_adapter Method
DESCRIPTION: Alternative method to load a PEFT adapter using the load_adapter method on a pre-trained model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/peft.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "facebook/opt-350m"
peft_model_id = "ybelkada/opt-350m-lora"

model = AutoModelForCausalLM.from_pretrained(model_id)
model.load_adapter(peft_model_id)
```

----------------------------------------

TITLE: Loading a Pre-trained Feature Extractor with AutoFeatureExtractor in Python
DESCRIPTION: This snippet shows how to load a pre-trained feature extractor using the AutoFeatureExtractor class for audio and vision tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/autoclass_tutorial.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained(
...     "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
... )
```

----------------------------------------

TITLE: Setting Training Arguments
DESCRIPTION: Configures training parameters including batch size, learning rate, and optimization settings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/text-to-speech.md#2025-04-22_snippet_25

LANGUAGE: python
CODE:
```
from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="speecht5_finetuned_voxpopuli_nl",  # change to a repo name of your choice
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=1e-5,
    warmup_steps=500,
    max_steps=4000,
    gradient_checkpointing=True,
    fp16=True,
    eval_strategy="steps",
    per_device_eval_batch_size=2,
    save_steps=1000,
    eval_steps=1000,
    logging_steps=25,
    report_to=["tensorboard"],
    load_best_model_at_end=True,
    greater_is_better=False,
    label_names=["labels"],
    push_to_hub=True,
)
```

----------------------------------------

TITLE: Tokenizing Input for Text Generation in PyTorch
DESCRIPTION: This code shows how to tokenize input text and prepare it for text generation in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_22

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("username/my_awesome_eli5_clm-model")
inputs = tokenizer(prompt, return_tensors="pt").input_ids
```

----------------------------------------

TITLE: Pushing 8-bit Quantized Model to Hugging Face Hub
DESCRIPTION: Example of how to push an 8-bit quantized model to the Hugging Face Hub using the push_to_hub method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/bitsandbytes.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_8bit=True)

model = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-560m", 
    quantization_config=quantization_config
)
tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-560m")

model.push_to_hub("bloom-560m-8bit")
```

----------------------------------------

TITLE: Loading ASR Pipeline for Inference
DESCRIPTION: This snippet demonstrates how to load the ASR pipeline with the specified target language and model configurations, including handling mismatched size warnings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mms.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import pipeline

model_id = "facebook/mms-1b-all"
target_lang = "fra"

pipe = pipeline(model=model_id, model_kwargs={"target_lang": "fra", "ignore_mismatched_sizes": True})
```

----------------------------------------

TITLE: Building TensorFlow Tensors
DESCRIPTION: This Python code demonstrates how to build TensorFlow tensors from the encoded input using the `return_tensors="tf"` option in the tokenizer. This converts the `input_ids`, `token_type_ids`, and `attention_mask` into TensorFlow tensors, which can be directly used as input for TensorFlow models. The tensors are created with padding and truncation enabled to ensure consistent shapes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_8

LANGUAGE: python
CODE:
```
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast?",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="tf")
>>> print(encoded_input)
{'input_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
       [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
       [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],
      dtype=int32)>,
 'token_type_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,
 'attention_mask': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}
```

----------------------------------------

TITLE: Using MobileBERT Fill-Mask Pipeline in PyTorch
DESCRIPTION: Demonstrates how to perform masked language modeling using the `pipeline` abstraction. It sets up a pipeline for `fill-mask` with the MobileBERT model, enabling faster inference with FP16 precision on GPU device 0. The example predicts the `[MASK]` token in a given sentence. Requires the `transformers` and `torch` libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mobilebert.md#_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipeline = pipeline(
    task="fill-mask",
    model="google/mobilebert-uncased",
    torch_dtype=torch.float16,
    device=0
)
pipeline("The capital of France is [MASK].")
```

----------------------------------------

TITLE: Image Segmentation with Default Model in Python
DESCRIPTION: Code example showing how to perform pixel-level image segmentation using the transformers pipeline with its default segmentation model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/task_summary.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> segmenter = pipeline(task="image-segmentation")
>>> preds = segmenter(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> print(*preds, sep="\n")
{'score': 0.9879, 'label': 'LABEL_184'}
{'score': 0.9973, 'label': 'snow'}
{'score': 0.9972, 'label': 'cat'}
```

----------------------------------------

TITLE: Multiple Sentence Sentiment Analysis
DESCRIPTION: Example showing sentiment analysis on multiple sentences using pipeline
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/quicktour.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> results = classifier(["We are very happy to show you the ðŸ¤— Transformers library.", "We hope you don't hate it."])
>>> for result in results:
...     print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
label: POSITIVE, with score: 0.9998
label: NEGATIVE, with score: 0.5309
```

----------------------------------------

TITLE: Initialize Trainer (PyTorch)
DESCRIPTION: This snippet initializes the `Trainer` class in PyTorch. It configures the trainer with the model, training arguments, training and evaluation datasets, the tokenizer, and the data collator. This sets up the training loop with all necessary components.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_39

LANGUAGE: python
CODE:
```
>>> from transformers import Trainer

>>> trainer = Trainer(
...	model=model,
...	args=training_args,
...	train_dataset=dataset["train"],
...	eval_dataset=dataset["test"],
...	processing_class=tokenizer,
...	data_collator=data_collator,
... )  # doctest: +SKIP
```

----------------------------------------

TITLE: Initializing a Transformers Model with Default Config
DESCRIPTION: Demonstrates how to instantiate a model using a configuration object and the model class provided by the Transformers library. This verifies the model's `__init__` method works correctly and initializes weights randomly based on the configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/add_new_model.md#_snippet_12

LANGUAGE: Python
CODE:
```
from transformers import BrandNewLlama, BrandNewLlamaConfig
model = BrandNewLlama(BrandNewLlamaConfig())
```

----------------------------------------

TITLE: Image-to-Text Generation with Pipeline and Separate Image Loading
DESCRIPTION: This code loads images separately and passes them to the image-to-text pipeline. It initializes the pipeline with a different model and fetches images from URLs, then provides both text and image inputs to the pipeline for generating descriptive text. `return_full_text=False` removes the input from the output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_text_to_text.md#_snippet_9

LANGUAGE: python
CODE:
```
pipe = pipeline("image-text-to-text", model="HuggingFaceTB/SmolVLM-256M-Instruct")

img_urls = [
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png",
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg",
]
images = [
    Image.open(requests.get(img_urls[0], stream=True).raw),
    Image.open(requests.get(img_urls[1], stream=True).raw),
]

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "image"},
            {"type": "text", "text": "What do you see in these images?"},
        ],
    }
]
outputs = pipe(text=messages, images=images, max_new_tokens=50, return_full_text=False)
outputs[0]["generated_text"]
```

----------------------------------------

TITLE: Reloading and Inferencing Quantized Transformers Model (GPU)
DESCRIPTION: This snippet loads a previously saved quantized model from `output_dir` using `AutoModelForCausalLM` with `device_map="auto"` and `torch_dtype=torch.bfloat16`. It then loads a tokenizer, prepares input text, moves the input tensor to "cuda", generates text using the reloaded model, and decodes the output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/torchao.md#_snippet_14

LANGUAGE: python
CODE:
```
# reload the quantized model
reloaded_model = AutoModelForCausalLM.from_pretrained(
    output_dir, 
    device_map="auto", 
    torch_dtype=torch.bfloat16
)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-8B-Instruct")
input_text = "What are we having for dinner?"
input_ids = tokenizer(input_text, return_tensors="pt").to("cuda")

output = reloaded_model.generate(**input_ids, max_new_tokens=10)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Loading SceneParse150 Dataset in Python
DESCRIPTION: This code snippet showcases how to load a subset of the SceneParse150 dataset for fine-tuning purposes using the Datasets library from Hugging Face. It also performs splitting of the dataset into training and testing sets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> ds = load_dataset("scene_parse_150", split="train[:50]")
```

----------------------------------------

TITLE: Pipeline Dataset Processing Example
DESCRIPTION: Shows how to use pipeline for processing large datasets using iterators.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/pipeline_tutorial.md#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
def data():
    for i in range(1000):
        yield f"My example {i}"

pipe = pipeline(model="openai-community/gpt2", device=0)
generated_characters = 0
for out in pipe(data()):
    generated_characters += len(out[0]["generated_text"])
```

----------------------------------------

TITLE: Generating Text with Cached Model Output - Python
DESCRIPTION: This snippet generates text using a model with static caching techniques. It appends new user input for continuation and uses decoding to obtain user-friendly output from model predictions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_optims.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
new_input_ids = outputs
outputs = model.generate(new_input_ids, past_key_values=past_key_values)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
['The theory of special relativity states 1. The speed of light is constant in all inertial reference frames. 2. The speed of light is constant in all inertial reference frames. 3.']
```

----------------------------------------

TITLE: Processing the Entire Dataset
DESCRIPTION: Applies the tokenization and label alignment function to the entire WNUT 17 dataset using the map method from the Datasets library, setting batched=True for efficiency.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)
```

----------------------------------------

TITLE: Zero-Shot Text Classification Pipeline
DESCRIPTION: Shows how to use pipeline for zero-shot text classification with custom labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/pipeline_tutorial.md#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
from transformers import pipeline

classifier = pipeline(model="facebook/bart-large-mnli")
classifier(
    "I have a problem with my iphone that needs to be resolved asap!!",
    candidate_labels=["urgent", "not urgent", "phone", "tablet", "computer"],
)
```

----------------------------------------

TITLE: DataCollatorForSeq2Seq Class Definition in Python
DESCRIPTION: Defines the DataCollatorForSeq2Seq class, which is tailored for sequence-to-sequence tasks, providing mechanisms to batch sequences appropriately while preserving their structure.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/data_collator.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
[[autodoc]] data.data_collator.DataCollatorForSeq2Seq
```

----------------------------------------

TITLE: Defining the Compute Metrics Function
DESCRIPTION: This function calculates the accuracy metric by comparing the model's predictions with the ground truth labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/multiple_choice.md#_snippet_9

LANGUAGE: python
CODE:
```
>>> import numpy as np


>>> def compute_metrics(eval_pred):
...     predictions, labels = eval_pred
...     predictions = np.argmax(predictions, axis=1)
...     return accuracy.compute(predictions=predictions, references=labels)
```

----------------------------------------

TITLE: Initializing and Using BERTweet Model
DESCRIPTION: Demonstrates how to initialize the BERTweet model and tokenizer, process input text, and generate features. Shows implementation for both PyTorch and TensorFlow frameworks with proper tokenization handling.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/bertweet.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModel, AutoTokenizer

bertweet = AutoModel.from_pretrained("vinai/bertweet-base")

# For transformers v4.x+:
tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base", use_fast=False)

# For transformers v3.x:
# tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base")

# INPUT TWEET IS ALREADY NORMALIZED!
line = "SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:"

input_ids = torch.tensor([tokenizer.encode(line)])

with torch.no_grad():
    features = bertweet(input_ids)  # Models outputs are now tuples

# With TensorFlow 2.0+:
# from transformers import TFAutoModel
# bertweet = TFAutoModel.from_pretrained("vinai/bertweet-base")
```

----------------------------------------

TITLE: Configuring the Trainer for Pushing to Hub in PyTorch
DESCRIPTION: Shows how to set up TrainingArguments to automatically push a PyTorch model to the Hugging Face Hub during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/model_sharing.md#2025-04-23_snippet_8

LANGUAGE: python
CODE:
```
>>> training_args = TrainingArguments(output_dir="my-awesome-model", push_to_hub=True)
```

----------------------------------------

TITLE: Initializing PreTrainedTokenizerFast in Python
DESCRIPTION: The PreTrainedTokenizerFast class is a fast implementation of the tokenizer based on the Rust-based tokenizers library. It provides significantly faster tokenization for batches and additional methods for mapping between raw strings and token space.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/tokenizer.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
class PreTrainedTokenizerFast:
    def __call__(self):
        # Implementation details
        pass
    
    def add_tokens(self):
        # Implementation details
        pass
    
    def add_special_tokens(self):
        # Implementation details
        pass
    
    def apply_chat_template(self):
        # Implementation details
        pass
    
    def batch_decode(self):
        # Implementation details
        pass
    
    def decode(self):
        # Implementation details
        pass
    
    def encode(self):
        # Implementation details
        pass
    
    def push_to_hub(self):
        # Implementation details
        pass
```

----------------------------------------

TITLE: Running Trainer with Accelerate Launcher using Config File
DESCRIPTION: Bash command to launch a GLUE task training script with the Accelerate launcher using a previously created configuration file, specifying model parameters and training settings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/trainer.md#2025-04-22_snippet_12

LANGUAGE: bash
CODE:
```
cd transformers

accelerate launch \
./examples/pytorch/text-classification/run_glue.py \
--model_name_or_path google-bert/bert-base-cased \
--task_name $TASK_NAME \
--do_train \
--do_eval \
--max_seq_length 128 \
--per_device_train_batch_size 16 \
--learning_rate 5e-5 \
--num_train_epochs 3 \
--output_dir /tmp/$TASK_NAME/ \
--overwrite_output_dir
```

----------------------------------------

TITLE: Optimizer and Scheduler Implementation with AdamW (Python)
DESCRIPTION: Demonstrates how to implement the AdamW optimizer and a linear warmup scheduler in the ðŸ¤— Transformers library, replacing the previous BertAdam optimizer. The code highlights the separation of the optimizer and scheduler, and shows how to use PyTorch's learning rate schedulers and gradient clipping with AdamW.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/migration.md#_snippet_6

LANGUAGE: python
CODE:
```
# Parametri:
lr = 1e-3
max_grad_norm = 1.0
num_training_steps = 1000
num_warmup_steps = 100
warmup_proportion = float( num_warmup_steps) / float(num_training_steps) # 0.1

### In precedenza l'ottimizzatore BertAdam veniva istanziato in questo modo:
optimizer = BertAdam(
   model.parameters(),
   lr=lr,
   schedule="warmup_linear",
   warmup=warmup_proportion,
   num_training_steps=num_training_steps,
)
### e usato in questo modo:
for batch in train_data:
   loss = model(batch)
   loss.backward()
   optimizer.step()

### In ðŸ¤— Transformers, ottimizzatore e schedule sono divisi e usati in questo modo:
optimizer = AdamW(
   model.parameters(), lr=lr, correct_bias=False
) # Per riprodurre il comportamento specifico di BertAdam impostare correct_bias=False
scheduler = get_linear_schedule_with_warmup(
   optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps
) # PyTorch scheduler
### e va usato cosÃ¬:
for batch in train_data:
   loss = model(batch)
   loss.backward()
   torch.nn.utils.clip_grad_norm_(
   model.parameters(), max_grad_norm
   ) # Gradient clipping non Ã¨ piÃ¹ in AdamW (quindi puoi usare amp senza problemi)
   optimizer.step()
   scheduler.step()
```

----------------------------------------

TITLE: Setting Up Data Collator for PyTorch Training
DESCRIPTION: Creates a DataCollatorWithPadding instance for PyTorch that handles dynamic padding of sequences to the same length within each batch, which is more efficient than padding the entire dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorWithPadding

>>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

----------------------------------------

TITLE: Initializing Accelerator Object
DESCRIPTION: Creating an Accelerator instance to automatically detect and initialize distributed setup
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/hi/accelerate.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from accelerate import Accelerator

accelerator = Accelerator()
```

----------------------------------------

TITLE: Defining Evaluation Metrics for Video Classification in Python
DESCRIPTION: Creates a function to compute accuracy metrics for video classification predictions. Uses the 'accuracy' metric from the 'evaluate' library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
import evaluate

metric = evaluate.load("accuracy")


def compute_metrics(eval_pred):
    predictions = np.argmax(eval_pred.predictions, axis=1)
    return metric.compute(predictions=predictions, references=eval_pred.label_ids)
```

----------------------------------------

TITLE: Configuring PyTorch Training Arguments
DESCRIPTION: Sets up training arguments including output directory, batch sizes, learning rate, and evaluation strategy for model training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_classification.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
training_args = TrainingArguments(
    output_dir="my_awesome_food_model",
    remove_unused_columns=False,
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    gradient_accumulation_steps=4,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    warmup_ratio=0.1,
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    push_to_hub=True,
)
```

----------------------------------------

TITLE: Load DistilRoBERTa Model for Masked Language Modeling (TensorFlow)
DESCRIPTION: Loads the pre-trained DistilRoBERTa model for masked language modeling in TensorFlow using `TFAutoModelForMaskedLM` from the `transformers` library. This prepares the model for fine-tuning on the masked language modeling task within the TensorFlow ecosystem. Requires the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_18

LANGUAGE: python
CODE:
```
>>> from transformers import TFAutoModelForMaskedLM

>>> model = TFAutoModelForMaskedLM.from_pretrained("distilbert/distilroberta-base")
```

----------------------------------------

TITLE: Tokenize Text for Inference (TensorFlow)
DESCRIPTION: Tokenizes the input text using the fine-tuned model's tokenizer and returns TensorFlow tensors. It also identifies the index of the `<mask>` token. Requires the `transformers` library and a trained tokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_28

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_eli5_mlm_model")
>>> inputs = tokenizer(text, return_tensors="tf")
>>> mask_token_index = tf.where(inputs["input_ids"] == tokenizer.mask_token_id)[0, 1]
```

----------------------------------------

TITLE: Initializing mLUKE Model in Python
DESCRIPTION: This code snippet demonstrates how to initialize the mLUKE model using the Hugging Face Transformers library. It requires the Transformers library to be installed and the pre-trained model weights to be accessible.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mluke.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import LukeModel

model = LukeModel.from_pretrained("studio-ousia/mluke-base")
```

----------------------------------------

TITLE: Training BART Model with DLM
DESCRIPTION: Bash command to train BART model using denoising language modeling on Norwegian dataset with specified hyperparameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/README.md#2025-04-22_snippet_14

LANGUAGE: bash
CODE:
```
python run_bart_dlm_flax.py \
    --output_dir="./norwegian-bart-base" \
    --config_name="./norwegian-bart-base" \
    --tokenizer_name="./norwegian-bart-base" \
    --dataset_name="oscar" \
    --dataset_config_name="unshuffled_deduplicated_no" \
    --max_seq_length="1024" \
    --per_device_train_batch_size="32" \
    --per_device_eval_batch_size="32" \
    --learning_rate="1e-4" \
    --warmup_steps="2000" \
    --overwrite_output_dir \
    --logging_steps="500" \
    --save_steps="2000" \
    --eval_steps="2000" \
    --push_to_hub
```

----------------------------------------

TITLE: Using a Specific Speech Recognition Model in Pipeline
DESCRIPTION: This snippet shows how to use a specific pre-trained model (Whisper Large) for speech recognition to achieve better transcription results compared to the default model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pipeline_tutorial.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> generator = pipeline(model="openai/whisper-large")
>>> generator("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

----------------------------------------

TITLE: Training T5 Model with MLM
DESCRIPTION: Bash command to train T5 model using masked language modeling on Norwegian dataset with specified hyperparameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/README.md#2025-04-22_snippet_11

LANGUAGE: bash
CODE:
```
python run_t5_mlm_flax.py \
	--output_dir="./norwegian-t5-base" \
	--model_type="t5" \
	--config_name="./norwegian-t5-base" \
	--tokenizer_name="./norwegian-t5-base" \
	--dataset_name="oscar" \
	--dataset_config_name="unshuffled_deduplicated_no" \
	--max_seq_length="512" \
	--per_device_train_batch_size="32" \
	--per_device_eval_batch_size="32" \
	--adafactor \
	--learning_rate="0.005" \
	--weight_decay="0.001" \
	--warmup_steps="2000" \
	--overwrite_output_dir \
	--logging_steps="500" \
	--save_steps="10000" \
	--eval_steps="2500" \
	--push_to_hub
```

----------------------------------------

TITLE: Greedy Search Text Generation
DESCRIPTION: Demonstrates the default greedy search decoding strategy for text generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/generation_strategies.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

prompt = "I look forward to"
checkpoint = "distilbert/distilgpt2"

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
inputs = tokenizer(prompt, return_tensors="pt")

model = AutoModelForCausalLM.from_pretrained(checkpoint)
outputs = model.generate(**inputs)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

----------------------------------------

TITLE: Tokenizing Data for TensorFlow and Converting to NumPy Arrays
DESCRIPTION: Shows how to tokenize text data and convert it to NumPy arrays for use with TensorFlow and Keras, including preparing the labels as arrays.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/training.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")
tokenized_data = tokenizer(dataset["sentence"], return_tensors="np", padding=True)
# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¯BatchEncodingã‚’è¿”ã—ã¾ã™ãŒã€ãã‚Œã‚’Kerasç”¨ã«è¾žæ›¸ã«å¤‰æ›ã—ã¾ã™
tokenized_data = dict(tokenized_data)

labels = np.array(dataset["label"])  # ãƒ©ãƒ™ãƒ«ã¯ã™ã§ã«0ã¨1ã®é…åˆ—ã§ã™
```

----------------------------------------

TITLE: Saving and Loading Models in PyTorch
DESCRIPTION: Shows how to save a trained PyTorch model and tokenizer, and then reload them for later use.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/quicktour.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
>>> pt_save_directory = "./pt_save_pretrained"
>>> tokenizer.save_pretrained(pt_save_directory)  # doctest: +IGNORE_RESULT
>>> pt_model.save_pretrained(pt_save_directory)

>>> pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")
```

----------------------------------------

TITLE: Text Preprocessing Function
DESCRIPTION: Function to join and tokenize text examples from the dataset
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/masked_language_modeling.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> def preprocess_function(examples):
...     return tokenizer([" ".join(x) for x in examples["answers.text"]])
```

----------------------------------------

TITLE: Installing Transformers from Source Code
DESCRIPTION: Command to install the latest version of Transformers directly from the GitHub repository source code.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/installation.md#2025-04-22_snippet_9

LANGUAGE: bash
CODE:
```
pip install git+https://github.com/huggingface/transformers
```

----------------------------------------

TITLE: Tokenizing Text Data for BERT Fine-tuning
DESCRIPTION: Initializes a BERT tokenizer and creates a function to tokenize text with padding and truncation. The function is then applied to the entire dataset using the map method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/training.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")


def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)


tokenized_datasets = dataset.map(tokenize_function, batched=True)
```

----------------------------------------

TITLE: Saving Tokenizer and Model to Local Directory
DESCRIPTION: This Python code saves a pre-trained tokenizer and model to the specified local directory. This will allow for offline loading and usage of the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/installation.md#_snippet_19

LANGUAGE: python
CODE:
```
>>> tokenizer.save_pretrained("./your/path/bigscience_t0")
>>> model.save_pretrained("./your/path/bigscience_t0")
```

----------------------------------------

TITLE: Listing Required Dependencies for Hugging Face Transformers
DESCRIPTION: This snippet enumerates the necessary Python packages and their minimum version requirements for the Transformers project. It includes libraries for acceleration, sequence evaluation, dataset handling, PyTorch, and evaluation metrics.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/token-classification/requirements.txt#2025-04-22_snippet_0

LANGUAGE: Plain Text
CODE:
```
accelerate >= 0.12.0
seqeval
datasets >= 1.8.0
torch >= 1.3
evaluate
```

----------------------------------------

TITLE: Configuring Training Arguments for LayoutLMv2 Fine-tuning in Python
DESCRIPTION: This snippet sets up the training arguments for fine-tuning a LayoutLMv2 model on a document QA task. It specifies parameters such as batch size, number of epochs, learning rate, and push to hub settings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/document_question_answering.md#2025-04-22_snippet_21

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments

# REPLACE THIS WITH YOUR REPO ID
repo_id = "MariaK/layoutlmv2-base-uncased_finetuned_docvqa"

training_args = TrainingArguments(
    output_dir=repo_id,
    per_device_train_batch_size=4,
    num_train_epochs=20,
    save_steps=200,
    logging_steps=50,
    eval_strategy="steps",
    learning_rate=5e-5,
    save_total_limit=2,
    remove_unused_columns=False,
    push_to_hub=True,
)
```

----------------------------------------

TITLE: Generating translations with PLBart in Python
DESCRIPTION: This snippet shows how to use PLBart to generate translations of Python code into English. It includes initializing the tokenizer and model with the 'uclanlp/plbart-python-en_XX' pre-trained model, preparing input data, and generating translations. Key parameters include setting the 'decoder_start_token_id' to the appropriate language ID. The result is the translated text obtained from the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/plbart.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import PLBartForConditionalGeneration, PLBartTokenizer

>>> tokenizer = PLBartTokenizer.from_pretrained("uclanlp/plbart-python-en_XX", src_lang="python", tgt_lang="en_XX")
>>> example_python_phrase = "def maximum(a,b,c):NEW_LINE_INDENTreturn max([a,b,c])"
>>> inputs = tokenizer(example_python_phrase, return_tensors="pt")
>>> model = PLBartForConditionalGeneration.from_pretrained("uclanlp/plbart-python-en_XX")
>>> translated_tokens = model.generate(**inputs, decoder_start_token_id=tokenizer.lang_code_to_id["en_XX"])
>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]
"Returns the maximum value of a b c."
```

----------------------------------------

TITLE: Chatting using Transformers Text Generation Pipeline (Python)
DESCRIPTION: Illustrates how to format input as a list of messages with roles (system, user) for chat-based text generation. It initializes the pipeline with a specific model, handles device mapping, and prints the model's final response.
SOURCE: https://github.com/huggingface/transformers/blob/main/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
import torch
from transformers import pipeline

chat = [
    {"role": "system", "content": "You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986."},
    {"role": "user", "content": "Hey, can you tell me any fun things to do in New York?"}
]

pipeline = pipeline(task="text-generation", model="meta-llama/Meta-Llama-3-8B-Instruct", torch_dtype=torch.bfloat16, device_map="auto")
response = pipeline(chat, max_new_tokens=512)
print(response[0]["generated_text"][-1]["content"])
```

----------------------------------------

TITLE: Running Inference on Multiple Inputs with Pipeline in Python
DESCRIPTION: Passes a list of audio file URLs to the pipeline to transcribe multiple audio files.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/hi/pipeline_tutorial.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
transcriber([
    "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac",
    "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac",
])
```

----------------------------------------

TITLE: Setting Up Data Collator for PyTorch
DESCRIPTION: Creates a DefaultDataCollator for batching samples in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/image_classification.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator()
```

----------------------------------------

TITLE: Registering a Callback with Trainer
DESCRIPTION: This snippet demonstrates two methods for registering a callback with the Trainer instance, either by passing the class or its instance to the add_callback method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/main_classes/callback.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
trainer = Trainer(...)
trainer.add_callback(MyCallback)
# ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œëŠ” ì½œë°± í´ëž˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì „ë‹¬í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤
trainer.add_callback(MyCallback())
```

----------------------------------------

TITLE: Launching Training with Accelerate
DESCRIPTION: Launches a training script using the `accelerate launch` command. It specifies the script to run and its associated parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/run_scripts.md#2025-04-22_snippet_11

LANGUAGE: bash
CODE:
```
```bash
accelerate launch run_summarization_no_trainer.py \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir ~/tmp/tst-summarization \
```
```

----------------------------------------

TITLE: Loading Pretrained Configuration
DESCRIPTION: Example of loading a pretrained DistilBERT configuration with custom parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/create_a_model.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
my_config = DistilBertConfig.from_pretrained("distilbert/distilbert-base-uncased", activation="relu", attention_dropout=0.4)
```

----------------------------------------

TITLE: Applying Different Quantization Configs Per Module
DESCRIPTION: This snippet demonstrates using `AOPerModuleConfig` to apply different quantization configurations to different types of layers (linear and embedding) in an OPT-125m model. It uses `Int8DynamicActivationInt4WeightConfig` as the default for linear layers and `IntxWeightOnlyConfig` for embedding layers, while skipping another embedding layer. The model is loaded with `include_embedding=True` to allow embedding quantization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/torchao.md#_snippet_9

LANGUAGE: Python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig

model_id = "facebook/opt-125m"

from torchao.quantization import Int4WeightOnlyConfig, AOPerModuleConfig, Int8DynamicActivationInt4WeightConfig, IntxWeightOnlyConfig, PerAxis, MappingType

weight_dtype = torch.int8
granularity = PerAxis(0)
mapping_type = MappingType.ASYMMETRIC
embedding_config = IntxWeightOnlyConfig(
    weight_dtype=weight_dtype,
    granularity=granularity,
    mapping_type=mapping_type,
)
linear_config = Int8DynamicActivationInt4WeightConfig(group_size=128)
quant_config = AOPerModuleConfig({"_default": linear_config, "model.decoder.embed_tokens": embedding_config, "model.decoder.embed_positions": None})
# set `include_embedding` to True in order to include embedding in quantization
# when `include_embedding` is True, we'll remove input embedding from `modules_not_to_convert` as well
quantization_config = TorchAoConfig(quant_type=quant_config, include_embedding=True)
quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map="cpu", torch_dtype=torch.bfloat16, quantization_config=quantization_config)
print("quantized model:", quantized_model)
# make sure embedding is quantized
print("embed_tokens weight:", quantized_model.model.decoder.embed_tokens.weight)
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Manual Testing
prompt = "Hey, are you conscious? Can you talk to me?"
inputs = tokenizer(prompt, return_tensors="pt").to("cpu")
generated_ids = quantized_model.generate(**inputs, max_new_tokens=128, cache_implementation="static")
output_text = tokenizer.batch_decode(
    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
```

----------------------------------------

TITLE: Creating Preprocessing Function for Text Tokenization
DESCRIPTION: Defines a function to tokenize the text data with truncation to fit the model's maximum sequence length, which will be applied to the entire dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> def preprocess_function(examples):
...     return tokenizer(examples["text"], truncation=True)
```

----------------------------------------

TITLE: Preparing a Dataset for ASR
DESCRIPTION: This code defines a function `prepare_dataset` that processes the audio and text data for ASR. It takes an example from the dataset, extracts the audio array and text, and uses the processor to generate `input_values` (audio features) and `labels` (text tokens). The function also handles audio resampling.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_30

LANGUAGE: Python
CODE:
```
>>> def prepare_dataset(example):
...     audio = example["audio"]

...     example.update(processor(audio=audio["array"], text=example["text"], sampling_rate=16000))

...     return example
```

----------------------------------------

TITLE: Comparing Chat Templates With and Without Generation Prompt
DESCRIPTION: Example demonstrating the difference between applying a chat template with and without the generation prompt. The generation prompt adds tokens to indicate where the model should start generating the assistant's response.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/chat_templating.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
messages = [
    {"role": "user", "content": "Hi there!"},
    {"role": "assistant", "content": "Nice to meet you!"},
    {"role": "user", "content": "Can I ask a question?"}
]

tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
"""<|im_start|>user
Hi there!<|im_end|>
<|im_start|>assistant
Nice to meet you!<|im_end|>
<|im_start|>user
Can I ask a question?<|im_end|>
"""

tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
"""<|im_start|>user
Hi there!<|im_end|>
<|im_start|>assistant
Nice to meet you!<|im_end|>
<|im_start|>user
Can I ask a question?<|im_end|>
<|im_start|>assistant
"""
```

----------------------------------------

TITLE: Automatic Speech Recognition with Whisper Model in Python
DESCRIPTION: Code example showing how to transcribe speech to text using the Whisper model through the transformers pipeline API.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/task_summary.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition", model="openai/whisper-small")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

----------------------------------------

TITLE: Configuring DeepSpeed with Accelerate Plugin
DESCRIPTION: YAML configuration for DeepSpeed using the Accelerate plugin with inline settings for gradient accumulation, clipping, offloading, and ZeRO stage.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/trainer.md#2025-04-22_snippet_11

LANGUAGE: yaml
CODE:
```
compute_environment: LOCAL_MACHINE                                                                                             
deepspeed_config:                                                                                                              
  gradient_accumulation_steps: 1
  gradient_clipping: 0.7
  offload_optimizer_device: cpu
  offload_param_device: cpu
  zero3_init_flag: true
  zero_stage: 2
distributed_type: DEEPSPEED
downcast_bf16: 'no'
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 4
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

----------------------------------------

TITLE: Loading a model in 8-bit precision with bitsandbytes
DESCRIPTION: This code configures and loads a pre-trained model in 8-bit precision using the `BitsAndBytesConfig` and `AutoModelForCausalLM`. It sets `load_in_8bit=True` within the quantization configuration and uses the `device_map="auto"` to automatically distribute the model across available GPUs with Accelerate.  It then generates text from a prompt using the loaded model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_one.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM

quantization_config = BitsAndBytesConfig(load_in_8bit=True)
tokenizer = AutoTokenizer("meta-llama/Llama-3.1-8B")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.1-8B", device_map="auto", quantization_config=quantization_config)

prompt = "Hello, my llama is cute"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
generated_ids = model.generate(**inputs)
outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
```

----------------------------------------

TITLE: Loading Pre-trained DistilBert Model in PyTorch
DESCRIPTION: Shows how to load a pre-trained DistilBert model with its weights and configuration in PyTorch for better performance without training from scratch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/create_a_model.md#2025-04-23_snippet_6

LANGUAGE: python
CODE:
```
>>> model = DistilBertModel.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Custom Model and Tokenizer Pipeline Setup
DESCRIPTION: Demonstrates how to create a pipeline with custom model and tokenizer for text generation using DistilGPT2.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/pipeline_tutorial.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("distilbert/distilgpt2")
model = AutoModelForCausalLM.from_pretrained("distilbert/distilgpt2")

from transformers import pipeline

generator = pipeline(task="text-generation", model=model, tokenizer=tokenizer)
```

----------------------------------------

TITLE: Preparing tf.data.Dataset for training
DESCRIPTION: Prepares a TensorFlow Dataset using the `prepare_tf_dataset` method of the model. This method automatically infers which columns to use as model inputs and pads the batches during loading.  The tokenizer is passed to ensure correct padding during batch creation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/training.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
tf_dataset = model.prepare_tf_dataset(dataset, batch_size=16, shuffle=True, tokenizer=tokenizer)
```

----------------------------------------

TITLE: Loading BillSum Dataset for Summarization Tasks
DESCRIPTION: Code to load the California bills subset of the BillSum dataset using the Hugging Face datasets library. This dataset contains legislative bills and their summaries for training summarization models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/summarization.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> billsum = load_dataset("billsum", split="ca_test")
```

----------------------------------------

TITLE: Loading the WNUT 17 Dataset
DESCRIPTION: Loads the WNUT 17 dataset from the Hugging Face Datasets library, which contains data for training models to detect new entities in emerging discussions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> wnut = load_dataset("wnut_17")
```

----------------------------------------

TITLE: Loading GPT-2 Model for Perplexity Calculation in Python
DESCRIPTION: Initializes a GPT-2 large model and tokenizer from the Transformers library to calculate perplexity scores.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/perplexity.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import GPT2LMHeadModel, GPT2TokenizerFast

device = "cuda"
model_id = "openai-community/gpt2-large"
model = GPT2LMHeadModel.from_pretrained(model_id).to(device)
tokenizer = GPT2TokenizerFast.from_pretrained(model_id)
```

----------------------------------------

TITLE: Saving Hugging Face Model to Local Directory
DESCRIPTION: Saves the current state of the Hugging Face model instance (including its configuration and learned weights) to a specified local folder. This creates the necessary files (`pytorch_model.bin`, `config.json`) to load the model later using `from_pretrained`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/add_new_model.md#_snippet_3

LANGUAGE: python
CODE:
```
model.save_pretrained("/path/to/converted/checkpoint/folder")
```

----------------------------------------

TITLE: Image-to-Text Generation Using Transformers Pipeline
DESCRIPTION: This code utilizes the Transformers pipeline to perform image-to-text generation. It initializes a pipeline for the "image-text-to-text" task with a specified model. The pipeline simplifies the process of preprocessing inputs and generating text from images.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_text_to_text.md#_snippet_6

LANGUAGE: python
CODE:
```
from transformers import pipeline
pipe = pipeline("image-text-to-text", model="llava-hf/llava-interleave-qwen-0.5b-hf")
```

----------------------------------------

TITLE: Installing Required Libraries for Hugging Face Transformers
DESCRIPTION: This snippet provides the required command to install necessary libraries for running Hugging Face models, such as transformers, datasets, and evaluate. These libraries are crucial dependencies for fine-tuning language models and handling datasets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers datasets evaluate
```

----------------------------------------

TITLE: Using Object Detection Pipeline in Transformers
DESCRIPTION: This snippet demonstrates how to use the 'pipeline' API for object detection. It initializes an object detector and processes an image, returning a list of detected objects with their confidence scores and bounding box coordinates.
SOURCE: https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> object_detector = pipeline('object-detection')
>>> object_detector(image)
[{'score': 0.9982201457023621,
  'label': 'remote',
  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}},
 {'score': 0.9960021376609802,
  'label': 'remote',
  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}},
 {'score': 0.9954745173454285,
  'label': 'couch',
  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}},
 {'score': 0.9988006353378296,
  'label': 'cat',
  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}},
 {'score': 0.9986783862113953,
  'label': 'cat',
  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]
```

----------------------------------------

TITLE: Using Pipeline for Object Detection in Python
DESCRIPTION: Shows how to perform object detection on an image using the pipeline API. The code downloads an image, processes it through an object detection pipeline, and returns detected objects with their positions and confidence scores.
SOURCE: https://github.com/huggingface/transformers/blob/main/i18n/README_es.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> import requests
>>> from PIL import Image
>>> from transformers import pipeline

# Download an image with cute cats
>>> url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png"
>>> image_data = requests.get(url, stream=True).raw
>>> image = Image.open(image_data)

# Allocate a pipeline for object detection
>>> object_detector = pipeline('object_detection')
>>> object_detector(image)
[{'score': 0.9982201457023621,
  'label': 'remote',
  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}},
 {'score': 0.9960021376609802,
  'label': 'remote',
  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}},
 {'score': 0.9954745173454285,
  'label': 'couch',
  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}},
 {'score': 0.9988006353378296,
  'label': 'cat',
  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}},
 {'score': 0.9986783862113953,
  'label': 'cat',
  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]
```

----------------------------------------

TITLE: Initializing Image-to-Image Pipeline in Python
DESCRIPTION: This snippet initializes the image-to-image pipeline using the Swin2SR model. It automatically detects the appropriate device (e.g., CPU, CUDA) for processing images.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_to_image.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import pipeline
import torch
from accelerate.test_utils.testing import get_backend
# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)
device, _, _ = get_backend()
pipe = pipeline(task="image-to-image", model="caidas/swin2SR-lightweight-x2-64", device=device)
```

----------------------------------------

TITLE: Padding Direction Demonstration - PyTorch
DESCRIPTION: This code snippet demonstrates the impact of padding direction on LLM output. It initializes a tokenizer with right-padding and then left-padding, showing how left-padding leads to the expected output when using `model.generate()`. It uses `AutoTokenizer` and assumes a pre-trained model is available.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/llm_tutorial.md#_snippet_8

LANGUAGE: Python
CODE:
```
>>> # The tokenizer initialized above has right-padding active by default: the 1st sequence,
>>> # which is shorter, has padding on the right side. Generation fails to capture the logic.
>>> model_inputs = tokenizer(
...     ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
... ).to("cuda")
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'1, 2, 33333333333'

>>> # With left-padding, it works as expected!
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", padding_side="left")
>>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
>>> model_inputs = tokenizer(
...     ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
... ).to("cuda")
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'1, 2, 3, 4, 5, 6,'
```

----------------------------------------

TITLE: Adding a New LoRA Adapter to a Model
DESCRIPTION: Code to add a new LoRA adapter to a pre-trained model by configuring the target modules and initializing the adapter with a custom name, allowing for multiple adapters in one model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/peft.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import PeftConfig

model_id = "facebook/opt-350m"
model = AutoModelForCausalLM.from_pretrained(model_id)

lora_config = LoraConfig(
    target_modules=["q_proj", "k_proj"],
    init_lora_weights=False
)

model.add_adapter(lora_config, adapter_name="adapter_1")
```

----------------------------------------

TITLE: Loading Sharded Checkpoints Directly Using load_sharded_checkpoint
DESCRIPTION: Demonstrates how to manually load a sharded checkpoint into an existing model instance using the load_sharded_checkpoint utility function from transformers.modeling_utils.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/big_models.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers.modeling_utils import load_sharded_checkpoint

>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir, max_shard_size="200MB")
...     load_sharded_checkpoint(model, tmp_dir)
```

----------------------------------------

TITLE: Configuring and Launching Training with Accelerate
DESCRIPTION: Steps to configure and launch training using the Accelerate library. Includes commands for setting up the configuration, testing the setup, and launching the training script.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/README.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
pip install git+https://github.com/huggingface/accelerate

acccelerate config

acccelerate test

acccelerate launch path_to_script.py --args_to_script
```

----------------------------------------

TITLE: Installing Transformers with Pip
DESCRIPTION: Shows how to install the Transformers library using pip. The installation should be done in a Python virtual environment after installing one of the supported backends (Flax, PyTorch, or TensorFlow).
SOURCE: https://github.com/huggingface/transformers/blob/main/i18n/README_es.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
pip install transformers
```

----------------------------------------

TITLE: Preparing TensorFlow Dataset
DESCRIPTION: This code prepares the TensorFlow dataset using the `prepare_tf_dataset` method. This method collates and batches the dataset, handling the conversion to TensorFlow tensors and shuffling.  The `batch_size`, `shuffle`, and `tokenizer` parameters are used to configure the dataset appropriately.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
tf_dataset = model.prepare_tf_dataset(
    dataset["train"], batch_size=16, shuffle=True, tokenizer=tokenizer
)
```

----------------------------------------

TITLE: Initializing DBRX Model with Standard Attention
DESCRIPTION: Demonstrates how to load the DBRX model using the Hugging Face Transformers library, with standard attention implementation for text generation
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/dbrx.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import DbrxForCausalLM, AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained("databricks/dbrx-instruct", token="YOUR_HF_TOKEN")
model = DbrxForCausalLM.from_pretrained(
    "databricks/dbrx-instruct",
    device_map="auto",
    torch_dtype=torch.bfloat16,
    token="YOUR_HF_TOKEN",
    )

input_text = "What does it take to build a great LLM?"
messages = [{"role": "user", "content": input_text}]
input_ids = tokenizer.apply_chat_template(messages, return_dict=True, tokenize=True, add_generation_prompt=True, return_tensors="pt").to("cuda")

outputs = model.generate(**input_ids, max_new_tokens=200)
print(tokenizer.decode(outputs[0]))
```

----------------------------------------

TITLE: Launching Distributed Training with Accelerate
DESCRIPTION: Command to start distributed training using Accelerate from a script.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/accelerate.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
accelerate launch train.py
```

----------------------------------------

TITLE: Initializing Accelerator Object for Distributed Training
DESCRIPTION: Creates an Accelerator object that automatically detects the distributed setup and initializes all necessary components for training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/accelerate.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from accelerate import Accelerator

>>> accelerator = Accelerator()
```

----------------------------------------

TITLE: Loading Pre-trained Processor with AutoProcessor in Python
DESCRIPTION: This code snippet demonstrates how to load a pre-trained processor using `AutoProcessor.from_pretrained` from the `transformers` library. It loads the processor associated with the 'microsoft/layoutlmv2-base-uncased' checkpoint, which combines a feature extractor and a tokenizer for multimodal tasks like document layout analysis.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/autoclass_tutorial.md#_snippet_3

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")
```

----------------------------------------

TITLE: Step-by-step Speech Translation with Speech2Text2 and Wav2Vec2
DESCRIPTION: This code demonstrates how to perform speech translation by loading a pre-trained Speech2Text2 model with Wav2Vec2, processing audio input, and generating translations. It uses the SpeechEncoderDecoderModel architecture to combine the encoder and decoder components.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/speech_to_text_2.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import Speech2Text2Processor, SpeechEncoderDecoderModel
from datasets import load_dataset
import soundfile as sf

model = SpeechEncoderDecoderModel.from_pretrained("facebook/s2t-wav2vec2-large-en-de")
processor = Speech2Text2Processor.from_pretrained("facebook/s2t-wav2vec2-large-en-de")


def map_to_array(batch):
    speech, _ = sf.read(batch["file"])
    batch["speech"] = speech
    return batch


ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
ds = ds.map(map_to_array)

inputs = processor(ds["speech"][0], sampling_rate=16_000, return_tensors="pt")
generated_ids = model.generate(inputs=inputs["input_values"], attention_mask=inputs["attention_mask"])

transcription = processor.batch_decode(generated_ids)
```

----------------------------------------

TITLE: Accessing Specific Elements of Model Outputs
DESCRIPTION: This snippet shows how to access specific elements from a model output object when treating it as a tuple. The example demonstrates slicing the outputs object to retrieve specific attributes like loss and logits.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/output.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
outputs[:2]
```

----------------------------------------

TITLE: Loading and Preparing Food-101 Dataset
DESCRIPTION: Load a subset of the Food-101 dataset and split it into training and test sets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_classification.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> food = load_dataset("food101", split="train[:5000]")
>>> food = food.train_test_split(test_size=0.2)
```

----------------------------------------

TITLE: Loading DistilBERT Tokenizer for Question Answering
DESCRIPTION: Initializes the DistilBERT tokenizer for processing question and context fields in the dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/question_answering.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Tokenization with DistilBERT
DESCRIPTION: Initializes and applies the DistilBERT tokenizer to process input tokens with word-split handling.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/tasks/token_classification.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
tokenized_input = tokenizer(example["tokens"], is_split_into_words=True)
tokens = tokenizer.convert_ids_to_tokens(tokenized_input["input_ids"])
```

----------------------------------------

TITLE: Defining Hyperparameter Space for Optuna in Python
DESCRIPTION: This function defines the search space for hyperparameters using Optuna, specifying the learning rate and batch size. It offers suggestions for float and categorical parameters and is called to inform the hyperparameter_search method how to explore the defined space.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/hpo_train.md#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
def optuna_hp_space(trial):
    return {
        "learning_rate": trial.suggest_float("learning_rate", 1e-6, 1e-4, log=True),
        "per_device_train_batch_size": trial.suggest_categorical("per_device_train_batch_size", [16, 32, 64, 128]),
    }
```

----------------------------------------

TITLE: Generating Text with AutoModel in Python
DESCRIPTION: Illustrates text generation using the AutoModelForCausalLM and AutoTokenizer classes for more granular control. It loads the model and tokenizer, encodes the input text, moves it to the GPU, generates output tokens, and decodes the result. Note: The original snippet contains a typo ('tokenzier' instead of 'tokenizer') and a syntax error in the `input_ids` line.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gpt2.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2", torch_dtype=torch.float16, device_map="auto", attn_implementation="sdpa")
tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2")

input_ids = tokenzier("Hello, I'm a language model". return_tensors="pt").to("cuda")

output = model.generate(**input_ids, cache_implementation="static")
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Preparing TensorFlow Dataset for Fine-tuning
DESCRIPTION: This code snippet prepares a TensorFlow dataset using `model.prepare_tf_dataset`. It utilizes the provided tokenizer to pad batches during loading. The shuffle argument ensures the data is shuffled. Tokenizer is passed to `prepare_tf_dataset` to enable correct batch padding. The `dataset["train"]` dataset is passed as an argument. It's recommended approach for most cases.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_14

LANGUAGE: Python
CODE:
```
tf_dataset = model.prepare_tf_dataset(dataset["train"], batch_size=16, shuffle=True, tokenizer=tokenizer)
```

----------------------------------------

TITLE: Fine-tuning BERT on SQuAD with Flax
DESCRIPTION: Command to fine-tune a BERT base model on the SQuAD question answering dataset using the Flax framework. It configures model parameters, training settings, and evaluation options with TensorBoard logging support.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/flax/question-answering/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python run_qa.py \
  --model_name_or_path google-bert/bert-base-uncased \
  --dataset_name squad \
  --do_train   \
  --do_eval   \
  --max_seq_length 384 \
  --doc_stride 128 \
  --learning_rate 3e-5 \
  --num_train_epochs 2 \
  --per_device_train_batch_size 12 \
  --output_dir ./bert-qa-squad \
  --eval_steps 1000 \
  --push_to_hub
```

----------------------------------------

TITLE: Running Question Answering with DistilBERT on SQuAD Dataset in Bash
DESCRIPTION: This command demonstrates how to use the run_qa.py script to train and evaluate a DistilBERT model on the SQuAD dataset for question answering tasks. It specifies the model, output directory, dataset name, and indicates that both training and evaluation should be performed.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/tensorflow/question-answering/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python run_qa.py \
--model_name_or_path distilbert/distilbert-base-cased \
--output_dir output \
--dataset_name squad \
--do_train \
--do_eval
```

----------------------------------------

TITLE: Installing Required Libraries for Transformers Text Classification
DESCRIPTION: Installs the necessary Python packages for working with Hugging Face transformers, including the core transformers library, datasets for data handling, evaluate for metrics, and accelerate for performance optimization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers datasets evaluate accelerate
```

----------------------------------------

TITLE: Quantizing a Causal Language Model with QuantoConfig
DESCRIPTION: This snippet demonstrates how to quantize a causal language model using QuantoConfig with `int8` weights. It loads a pre-trained model using `AutoModelForCausalLM` and applies the specified quantization configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/quanto.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
"from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig\n\nquant_config = QuantoConfig(weights=\"int8\")\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-3.1-8B\", \n    torch_dtype=\"auto\", \n    device_map=\"auto\", \n    quantization_config=quant_config\n)"
```

----------------------------------------

TITLE: Loading Pre-trained Models with TensorFlow in Transformers
DESCRIPTION: This code demonstrates how to load and use a pre-trained BERT model with TensorFlow backend. Similar to the PyTorch version, it imports the necessary classes, initializes the tokenizer and model, and processes a simple text input.
SOURCE: https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, TFAutoModel

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> model = TFAutoModel.from_pretrained("google-bert/bert-base-uncased")

>>> inputs = tokenizer("Hello world!", return_tensors="tf")
>>> outputs = model(**inputs)
```

----------------------------------------

TITLE: Load Model and Tokenizer for Troubleshooting
DESCRIPTION: This code loads the Mistral-7B-v0.1 model and its corresponding tokenizer, setting the padding token to the end-of-sequence token.  It's used as a setup for troubleshooting common generation problems, like overly short or repetitive outputs. The model is loaded with 4-bit quantization onto the GPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/llm_tutorial.md#_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
>>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
>>> model = AutoModelForCausalLM.from_pretrained(
...     "mistralai/Mistral-7B-v0.1", device_map="auto", load_in_4bit=True
... )
```

----------------------------------------

TITLE: Authenticating with Hugging Face Hub
DESCRIPTION: Code snippet to login to the Hugging Face Hub, allowing users to upload and share models. This enables model sharing with the community.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/semantic_segmentation.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Setting up Trainer with Hub Pushing Capability
DESCRIPTION: Demonstrates how to initialize a Trainer with the training arguments that include Hub pushing configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/model_sharing.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=small_train_dataset,
...     eval_dataset=small_eval_dataset,
...     compute_metrics=compute_metrics,
... )
```

----------------------------------------

TITLE: Initializing BertTokenizer in Python
DESCRIPTION: Tokenizer for BERT models. Handles tokenization, special token management, and vocabulary operations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/bert.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
[[autodoc]] BertTokenizer
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - save_vocabulary
```

----------------------------------------

TITLE: Examining Attention Masks for Padded Sequences
DESCRIPTION: This code shows the attention masks generated by the tokenizer, where 1 indicates tokens to attend to and 0 indicates padded tokens to ignore during model processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/glossary.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> padded_sequences["attention_mask"]
[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]
```

----------------------------------------

TITLE: Extracting Features with AutoModel - Transformers, Python
DESCRIPTION: This snippet demonstrates the use of AutoModel from the Transformers library to load a vision transformer model without the task-specific head for feature extraction. The pre-trained model is specified and moved to the appropriate device for optimized processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_feature_extraction.md#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
from transformers import AutoImageProcessor, AutoModel

processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
model = AutoModel.from_pretrained("google/vit-base-patch16-224").to(DEVICE)
```

----------------------------------------

TITLE: Loading Fast Image Processor with AutoImageProcessor - Python
DESCRIPTION: This snippet illustrates loading a fast variant of the image processor. The 'use_fast=True' parameter loads a faster version of the 'google/vit-base-patch16-224' processor if available, which can accelerate processing by utilizing optimized libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/image_processors.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
from transformers import AutoImageProcessor

image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224", use_fast=True)
```

----------------------------------------

TITLE: DINOv2 Image Classification with AutoModel
DESCRIPTION: Shows how to use AutoImageProcessor and AutoModelForImageClassification to classify images using a pre-trained DINOv2 model with custom configuration
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/dinov2.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import requests
from transformers import AutoImageProcessor, AutoModelForImageClassification
from PIL import Image

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

processor = AutoImageProcessor.from_pretrained("facebook/dinov2-small-imagenet1k-1-layer")
model = AutoModelForImageClassification.from_pretrained(
    "facebook/dinov2-small-imagenet1k-1-layer", 
    torch_dtype=torch.float16, 
    device_map="auto", 
    attn_implementation="sdpa"
)

inputs = processor(images=image, return_tensors="pt")
logits = model(**inputs).logits
predicted_class_idx = logits.argmax(-1).item()
print("Predicted class:", model.config.id2label[predicted_class_idx])
```

----------------------------------------

TITLE: Saving a Custom Decoding Strategy
DESCRIPTION: This snippet shows how to save a custom generation configuration with a fine-tuned model.  First, a `GenerationConfig` object is created and its parameters are set.  Then, `save_pretrained` is used to save the configuration to the model's repository, which can then be shared on the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/generation_strategies.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM, GenerationConfig

>>> model = AutoModelForCausalLM.from_pretrained("my_account/my_model")  # doctest: +SKIP
>>> generation_config = GenerationConfig(
...     max_new_tokens=50, do_sample=True, top_k=50, eos_token_id=model.config.eos_token_id
... )
>>> generation_config.save_pretrained("my_account/my_model", push_to_hub=True)  # doctest: +SKIP
```

----------------------------------------

TITLE: Running Zero-shot Inference with BLIP-2
DESCRIPTION: Demonstrates how to run zero-shot visual question answering with BLIP-2 by preprocessing the input, generating tokens with the model, and decoding the result.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_question_answering.md#2025-04-22_snippet_24

LANGUAGE: python
CODE:
```
>>> inputs = processor(image, text=prompt, return_tensors="pt").to(device, torch.float16)

>>> generated_ids = model.generate(**inputs, max_new_tokens=10)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
>>> print(generated_text)
"He is looking at the crowd"
```

----------------------------------------

TITLE: Segmentation Inference with Pipeline in PyTorch
DESCRIPTION: This code snippet shows how to use the trained model for image segmentation by instantiating a pipeline in PyTorch, passing the loaded image to get predictions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/sequence_classification.md#2025-04-22_snippet_25

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> segmenter = pipeline("image-segmentation", model="my_awesome_seg_model")
>>> segmenter(image)
```

----------------------------------------

TITLE: Sharing the Fine-tuned Translation Model
DESCRIPTION: This code pushes the fine-tuned translation model to the Hugging Face Hub, making it available for others to use.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/translation.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> trainer.push_to_hub()
```

----------------------------------------

TITLE: Encoding and Decoding Audio with EnCodec using Transformers
DESCRIPTION: This example demonstrates how to encode and decode audio using the EnCodec model with the `transformers` library. It loads a dataset, initializes the model and processor, encodes the audio, and then decodes it back. It also shows how to achieve the same result using a forward pass.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/encodec.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset, Audio
>>> from transformers import EncodecModel, AutoProcessor
>>> librispeech_dummy = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")

>>> model = EncodecModel.from_pretrained("facebook/encodec_24khz")
>>> processor = AutoProcessor.from_pretrained("facebook/encodec_24khz")
>>> librispeech_dummy = librispeech_dummy.cast_column("audio", Audio(sampling_rate=processor.sampling_rate))
>>> audio_sample = librispeech_dummy[-1]["audio"]["array"]
>>> inputs = processor(raw_audio=audio_sample, sampling_rate=processor.sampling_rate, return_tensors="pt")

>>> encoder_outputs = model.encode(inputs["input_values"], inputs["padding_mask"])
>>> audio_values = model.decode(encoder_outputs.audio_codes, encoder_outputs.audio_scales, inputs["padding_mask"])[0]
>>> # or the equivalent with a forward pass
>>> audio_values = model(inputs["input_values"], inputs["padding_mask"]).audio_values
```

----------------------------------------

TITLE: Generating Translation with NLLB-MoE - PyTorch
DESCRIPTION: This snippet demonstrates how to translate an English text into French using the NLLB-MoE model. The process includes setting the forced beginning-of-sentence token for the target language using its BCP-47 code and generating the translated output. Dependencies include 'transformers' library with 'AutoModelForSeq2SeqLM' and 'AutoTokenizer'. Inputs are tokenized text and language specifications; the output is the translated text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/nllb-moe.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-moe-54b\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-moe-54b\")\n\narticle = \"Previously, Ring's CEO, Jamie Siminoff, remarked the company started when his doorbell wasn't audible from his shop in his garage.\"\ninputs = tokenizer(article, return_tensors=\"pt\")\n\ntranslated_tokens = model.generate(\n    **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"fra_Latn\"], max_length=50\n)\nprint(tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0])
```

----------------------------------------

TITLE: Querying the Web Server (Bash)
DESCRIPTION: This command sends a POST request to the web server running on localhost:8000.  The request includes the string "test [MASK]" as the data payload. The server will process the input using the Transformers pipeline and return a JSON response containing the prediction results.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/pipeline_webserver.md#_snippet_2

LANGUAGE: bash
CODE:
```
curl -X POST -d "test [MASK]" http://localhost:8000/
#[{"score":0.7742936015129089,"token":1012,"token_str":".","sequence":"test."},...]
```

----------------------------------------

TITLE: Setting Up Text Generation Pipeline with Octocoder Model
DESCRIPTION: This snippet loads the Octocoder model and tokenizer, then creates a text generation pipeline. The model is loaded with bfloat16 precision and automatic device mapping.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/llm_tutorial_optimization.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch

model = AutoModelForCausalLM.from_pretrained("bigcode/octocoder", torch_dtype=torch.bfloat16, device_map="auto", pad_token_id=0)
tokenizer = AutoTokenizer.from_pretrained("bigcode/octocoder")

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
```

----------------------------------------

TITLE: Initializing Llama3 Pipeline in Python
DESCRIPTION: Basic example of creating a text generation pipeline using the Llama3 8B model with bfloat16 precision and automatic device mapping.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llama3.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import transformers
import torch

model_id = "meta-llama/Meta-Llama-3-8B"

pipeline = transformers.pipeline("text-generation", model=model_id, model_kwargs={"torch_dtype": torch.bfloat16}, device_map="auto")
pipeline("Hey how are you doing today?")
```

----------------------------------------

TITLE: Flash-Attention 2 for LLaVa-Next Speedup
DESCRIPTION: This snippet shows how to enable Flash-Attention 2 for faster generation with the LLaVa-Next model.  It sets the `use_flash_attention_2` argument to `True` when loading the model using `from_pretrained`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llava_next.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers import AutoModelForImageTextToText

model = AutoModelForImageTextToText.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    use_flash_attention_2=True
).to(0)
```

----------------------------------------

TITLE: Loading BERT Model to GPU
DESCRIPTION: Loads a pre-trained BERT model (`google-bert/bert-large-uncased`) directly onto the GPU to measure the memory footprint of the model weights. The `AutoModelForSequenceClassification` class from the `transformers` library is used for loading.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/model_memory_anatomy.md#_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-large-uncased").to("cuda")
>>> print_gpu_utilization()
GPU memory occupied: 2631 MB.
```

----------------------------------------

TITLE: Running GLUE Tasks with Hugging Face Transformers
DESCRIPTION: Example command for running the GLUE benchmark script for natural language understanding tasks. Demonstrates how to specify the model, GLUE task, operations to perform, and custom prediction file.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/tensorflow/text-classification/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
python run_glue.py \
--model_name_or_path distilbert/distilbert-base-cased \
--task_name mnli \
--do_train \
--do_eval \
--do_predict \
--predict_file data_to_predict.json
```

----------------------------------------

TITLE: Preprocessing Function for Tokenization
DESCRIPTION: This function preprocesses the examples by joining the text in the 'answers.text' field and then tokenizing the combined text using the provided tokenizer.  It takes a dictionary of examples as input and returns a dictionary containing the tokenized input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_6

LANGUAGE: python
CODE:
```
>>> def preprocess_function(examples):
...     return tokenizer([" ".join(x) for x in examples["answers.text"]])
```

----------------------------------------

TITLE: Translating from Non-English Language Using NLLB-MoE - PyTorch
DESCRIPTION: This code snippet provides an example of translating from Romanian to German using the NLLB-MoE model. The `src_lang` parameter is set during tokenizer initialization to specify the source language's BCP-47 code. Essential dependencies are the 'transformers' library, and the input is a Romanian text string. The output is the German translation of the given text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/nllb-moe.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-moe-54b\", src_lang=\"ron_Latn\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-moe-54b\")\n\narticle = \"Åžeful ONU spune cÄƒ nu existÄƒ o soluÅ£ie militarÄƒ Ã®n Siria\"\ninputs = tokenizer(article, return_tensors=\"pt\")\n\ntranslated_tokens = model.generate(\n    **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"deu_Latn\"], max_length=30\n)\nprint(tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0])
```

----------------------------------------

TITLE: Inference Function with AutoModel and Processor - Python
DESCRIPTION: This code defines a function for performing inference using an image processor and AutoModel. The function processes the input image and returns the pooled output, enabling feature extraction without task-specific heads.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_feature_extraction.md#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
def infer(image):
  inputs = processor(image, return_tensors="pt").to(DEVICE)
  outputs = model(**inputs)
  return outputs.pooler_output
```

----------------------------------------

TITLE: Create Tokenize Dataset Function (TensorFlow)
DESCRIPTION: This snippet defines a function `tokenize_dataset` that tokenizes a dataset using the loaded tokenizer in TensorFlow. This function will be mapped to the entire dataset to prepare it for training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_43

LANGUAGE: python
CODE:
```
>>> def tokenize_dataset(dataset):
...	return tokenizer(dataset["text"])  # doctest: +SKIP
```

----------------------------------------

TITLE: Using CodeGen Model for Code Generation with PyTorch
DESCRIPTION: Example showing how to load and use the CodeGen model for generating code completions using the transformers library. The example demonstrates loading a pre-trained model and tokenizer, then generating code completion for a simple function definition.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/codegen.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

checkpoint = "Salesforce/codegen-350M-mono"
model = AutoModelForCausalLM.from_pretrained(checkpoint)
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

text = "def hello_world():"

completion = model.generate(**tokenizer(text, return_tensors="pt"))

print(tokenizer.decode(completion[0]))
```

----------------------------------------

TITLE: Initializing Pegasus Summarization Model in Python
DESCRIPTION: Demonstrates how to load a pre-trained Pegasus model and tokenizer for text summarization, using the XSUM model as an example. Shows model initialization, text tokenization, and summary generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/pegasus.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import PegasusForConditionalGeneration, PegasusTokenizer
>>> import torch

>>> src_text = [
...     """ PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow."""
... ]

... model_name = "google/pegasus-xsum"
... device = "cuda" if torch.cuda.is_available() else "cpu"
... tokenizer = PegasusTokenizer.from_pretrained(model_name)
... model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)
... batch = tokenizer(src_text, truncation=True, padding="longest", return_tensors="pt").to(device)
... translated = model.generate(**batch)
... tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)
... assert (
...     tgt_text[0]
...     == "California's largest electricity provider has turned off power to hundreds of thousands of customers."
... )
```

----------------------------------------

TITLE: Auto Configuring Model Data Type - Transformers - Python
DESCRIPTION: This snippet shows how to automatically configure the data type for a pretrained model using `torch_dtype="auto"`, ensuring that weights are loaded according to their stored data type to optimize memory usage.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/models.md#2025-04-22_snippet_16

LANGUAGE: Python
CODE:
```
from transformers import AutoModelForCausalLM

gemma = AutoModelForCausalLM.from_pretrained("google/gemma-7b", torch_dtype="auto")
```

----------------------------------------

TITLE: Launching FSDP Training
DESCRIPTION: Commands to launch FSDP training using Accelerate, showing both default config and custom config approaches.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/fsdp.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
accelerate launch my-trainer-script.py
```

LANGUAGE: bash
CODE:
```
accelerate launch --fsdp="full shard" --fsdp_config="path/to/fsdp_config/ my-trainer-script.py
```

----------------------------------------

TITLE: Initializing Zero-shot Image Classification Pipeline using Transformers
DESCRIPTION: This snippet demonstrates how to initialize a zero-shot image classification pipeline by importing the necessary package and specifying a model checkpoint from the Hugging Face Hub. The pipeline is configured for the classification task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_image_classification.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> checkpoint = "openai/clip-vit-large-patch14"
>>> detector = pipeline(model=checkpoint, task="zero-shot-image-classification")
```

----------------------------------------

TITLE: Initializing Zero-Shot Object Detection Pipeline
DESCRIPTION: This code snippet initializes the zero-shot object detection pipeline using the OWL-ViT model.  It downloads the specified checkpoint from the Hugging Face Hub and configures the pipeline for zero-shot object detection task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/zero_shot_object_detection.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
">>> from transformers import pipeline\n\n>>> checkpoint = \"google/owlvit-base-patch32\"\n>>> detector = pipeline(model=checkpoint, task=\"zero-shot-object-detection\")"
```

----------------------------------------

TITLE: Querying the Web Server with cURL
DESCRIPTION: This bash command sends a POST request to the web server at `http://localhost:8000/` with the data "test [MASK]". The server processes the request using the Transformers pipeline and returns a JSON response.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pipeline_webserver.md#_snippet_2

LANGUAGE: bash
CODE:
```
curl -X POST -d "test [MASK]" http://localhost:8000/
#[{"score":0.7742936015129089,"token":1012,"token_str":".","sequence":"test."},...]
```

----------------------------------------

TITLE: Direct Push to Hub Usage
DESCRIPTION: Examples of directly pushing models and tokenizers to the Hub using the push_to_hub method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/model_sharing.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
pt_model.push_to_hub("my-awesome-model")

from transformers import AutoModel
model = AutoModel.from_pretrained("your_username/my-awesome-model")

pt_model.push_to_hub("my-awesome-org/my-awesome-model")

tokenizer.push_to_hub("my-awesome-model")

tf_model.push_to_hub("my-awesome-model")
```

----------------------------------------

TITLE: DINOv2 Model Quantization with TorchAO
DESCRIPTION: Demonstrates how to apply int4 weight quantization to a DINOv2 model using torchao to reduce memory consumption
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/dinov2.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import requests
from transformers import TorchAoConfig, AutoImageProcessor, AutoModelForImageClassification
from torchao.quantization import Int4WeightOnlyConfig
from PIL import Image

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

processor = AutoImageProcessor.from_pretrained('facebook/dinov2-giant-imagenet1k-1-layer')

quant_config = Int4WeightOnlyConfig(group_size=128)
quantization_config = TorchAoConfig(quant_type=quant_config)

model = AutoModelForImageClassification.from_pretrained(
    'facebook/dinov2-giant-imagenet1k-1-layer',
    torch_dtype=torch.bfloat16,
    device_map="auto",
    quantization_config=quantization_config
)

inputs = processor(images=image, return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits
predicted_class_idx = logits.argmax(-1).item()
print("Predicted class:", model.config.id2label[predicted_class_idx])
```

----------------------------------------

TITLE: Installing Transformers with Conda
DESCRIPTION: Command to install Transformers using conda package manager through the conda-forge channel.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/installation.md#2025-04-22_snippet_13

LANGUAGE: bash
CODE:
```
conda install conda-forge::transformers
```

----------------------------------------

TITLE: Pushing tokenizer to Hub with PushToHubMixin
DESCRIPTION: This Python code shows how to push a tokenizer to the Hugging Face Hub directly using the `push_to_hub` method from `PushToHubMixin`. It assumes the tokenizer object inherits from the mixin.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
"tokenizer.push_to_hub(\"my-awesome-model\")"
```

----------------------------------------

TITLE: Initializing Mamba Model for Text Generation
DESCRIPTION: Demonstrates loading a pre-trained Mamba model and performing text generation using Hugging Face Transformers library
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mamba.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained("state-spaces/mamba-130m-hf")
model = MambaForCausalLM.from_pretrained("state-spaces/mamba-130m-hf")
input_ids = tokenizer("Hey how are you doing?", return_tensors= "pt")["input_ids"]

out = model.generate(input_ids, max_new_tokens=10)
print(tokenizer.batch_decode(out))
```

----------------------------------------

TITLE: ASR with Automatic Speech Recognition Pipeline in Python
DESCRIPTION: Illustrates automatic speech recognition using the Whisper model from OpenAI via Hugging Face's Transformers. It processes an audio file URL and outputs the transcribed text. Requires internet access to fetch the audio file.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import pipeline

pipeline = pipeline(task="automatic-speech-recognition", model="openai/whisper-large-v3")
pipeline("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its...}
```

----------------------------------------

TITLE: Classifying Images with Transformers in Python
DESCRIPTION: This code snippet shows how to utilize the ðŸ¤— Transformers library for image classification tasks. It initializes an image classifier pipeline with a model and processes an image from a URL, outputting the classification scores and labels for the identified classes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/task_summary.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline(task="image-classification")
>>> preds = classifier(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> print(*preds, sep="\n")
{'score': 0.4335, 'label': 'lynx, catamount'}
{'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}
{'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}
{'score': 0.0239, 'label': 'Egyptian cat'}
{'score': 0.0229, 'label': 'tiger cat'}
```

----------------------------------------

TITLE: Using Pipeline with Iterator for Large Dataset Processing
DESCRIPTION: This code demonstrates how to efficiently process large datasets using an iterator approach, which prevents loading the entire dataset into memory while maximizing GPU utilization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pipeline_tutorial.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
def data():
    for i in range(1000):
        yield f"My example {i}"


pipe = pipe(model="openai-community/gpt2", device=0)
generated_characters = 0
for out in pipe(data()):
    generated_characters += len(out["generated_text"])
```

----------------------------------------

TITLE: Initializing Accelerator Object
DESCRIPTION: Creating an Accelerator instance that automatically detects distributed setup and initializes training components
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/accelerate.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from accelerate import Accelerator

accelerator = Accelerator()
```

----------------------------------------

TITLE: Processing Batches of Images with TensorFlow
DESCRIPTION: Processes batches of images by applying appropriate transformations for training and validation datasets using TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_classification.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> import numpy as np
>>> import tensorflow as tf
>>> from PIL import Image


>>> def convert_to_tf_tensor(image: Image):
...     np_image = np.array(image)
...     tf_image = tf.convert_to_tensor(np_image)
...     # `expand_dims()` is used to add a batch dimension since
...     # the TF augmentation layers operates on batched inputs.
...     return tf.expand_dims(tf_image, 0)
```

LANGUAGE: python
CODE:
```
>>> def preprocess_train(example_batch):
...     """Apply train_transforms across a batch."""
...     images = [
...         train_data_augmentation(convert_to_tf_tensor(image.convert("RGB"))) for image in example_batch["image"]
...     ]
...     example_batch["pixel_values"] = [tf.transpose(tf.squeeze(image)) for image in images]
...     return example_batch
```

LANGUAGE: python
CODE:
```
... def preprocess_val(example_batch):
...     """Apply val_transforms across a batch."""
...     images = [
...         val_data_augmentation(convert_to_tf_tensor(image.convert("RGB"))) for image in example_batch["image"]
...     ]
...     example_batch["pixel_values"] = [tf.transpose(tf.squeeze(image)) for image in images]
...     return example_batch
```

LANGUAGE: python
CODE:
```
food["train"].set_transform(preprocess_train)
food["test"].set_transform(preprocess_val)
```

----------------------------------------

TITLE: Tool Usage with Chat Templates
DESCRIPTION: Demonstrates advanced tool usage in chat templates, including function definition, tool call, and response generation
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/chat_templating.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
import datetime

def current_time():
    """Get the current local time as a string."""
    return str(datetime.now())

def multiply(a: float, b: float):
    """A function that multiplies two numbers"""
    return a * b

tools = [current_time, multiply]

model_input = tokenizer.apply_chat_template(
    messages,
    tools=tools
)
```

----------------------------------------

TITLE: Implementing Named Entity Recognition with Transformers Pipeline in Python
DESCRIPTION: This snippet demonstrates token classification using the Hugging Face pipeline for Named Entity Recognition (NER). It identifies and classifies named entities in text, such as organizations, locations, and miscellaneous entities, providing entity types, confidence scores, and position information for each detected entity.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/task_summary.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline(task="ner")
>>> preds = classifier("Hugging Face is a French company based in New York City.")
>>> preds = [
...     {
...         "entity": pred["entity"],
...         "score": round(pred["score"], 4),
...         "index": pred["index"],
...         "word": pred["word"],
...         "start": pred["start"],
...         "end": pred["end"],
...     }
...     for pred in preds
... ]
>>> print(*preds, sep="\n")
{'entity': 'I-ORG', 'score': 0.9968, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}
{'entity': 'I-ORG', 'score': 0.9293, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}
{'entity': 'I-ORG', 'score': 0.9763, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}
{'entity': 'I-MISC', 'score': 0.9983, 'index': 6, 'word': 'French', 'start': 18, 'end': 24}
{'entity': 'I-LOC', 'score': 0.999, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}
{'entity': 'I-LOC', 'score': 0.9987, 'index': 11, 'word': 'York', 'start': 46, 'end': 50}
{'entity': 'I-LOC', 'score': 0.9992, 'index': 12, 'word': 'City', 'start': 51, 'end': 55}
```

----------------------------------------

TITLE: Loading the Yelp Reviews Dataset with Hugging Face Datasets
DESCRIPTION: Demonstrates how to load the Yelp Reviews dataset using the load_dataset function from the Hugging Face datasets library and access a sample entry from the training set.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/training.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> dataset = load_dataset("yelp_review_full")
>>> dataset["train"][100]
{'label': 0,
 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\nThe cashier took my friends\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \"serving off their orders\" when they didn\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\nThe manager was rude when giving me my order. She didn\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\nI\'ve eaten at various McDonalds restaurants for over 30 years. I\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}
```

----------------------------------------

TITLE: Prepare Dataset for ASR (Python)
DESCRIPTION: This function preprocesses audio and text data for ASR. It takes a dataset example, extracts the audio array, and uses the processor to generate `input_values` from the audio and `labels` from the text. The results are then added to the example dictionary.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/preprocessing.md#_snippet_27

LANGUAGE: python
CODE:
```
>>> def prepare_dataset(example):
...     audio = example["audio"]

...     example.update(processor(audio=audio["array"], text=example["text"], sampling_rate=16000))

...     return example
```

----------------------------------------

TITLE: Loading Audio Dataset for Multiple Languages
DESCRIPTION: This snippet demonstrates how to load audio datasets for English and French languages using the Hugging Face Datasets library, preparing audio samples for processing with the MMS models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mms.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from datasets import load_dataset, Audio

# English
stream_data = load_dataset("mozilla-foundation/common_voice_13_0", "en", split="test", streaming=True)
stream_data = stream_data.cast_column("audio", Audio(sampling_rate=16000))
en_sample = next(iter(stream_data))["audio"]["array"]

# French
stream_data = load_dataset("mozilla-foundation/common_voice_13_0", "fr", split="test", streaming=True)
stream_data = stream_data.cast_column("audio", Audio(sampling_rate=16000))
fr_sample = next(iter(stream_data))['audio']['array']
```

----------------------------------------

TITLE: Loading an AQLM-Quantized Model with Transformers
DESCRIPTION: Load a pre-quantized language model using AutoModelForCausalLM with automatic dtype and device mapping
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/aqlm.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM

quantized_model = AutoModelForCausalLM.from_pretrained(
    "ISTA-DASLab/Mixtral-8x7b-AQLM-2Bit-1x16-hf",
    torch_dtype="auto", 
    device_map="auto"
)
```

----------------------------------------

TITLE: Image Normalization and Transformations with Torchvision (Python)
DESCRIPTION: This code snippet normalizes an image and applies a series of transformations including random resized cropping, color jittering, and conversion to a tensor. It uses the torchvision library to perform these image augmentations and prepares the image data for a vision model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/preprocessing.md#_snippet_19

LANGUAGE: python
CODE:
```
>>> from torchvision.transforms import Compose, Normalize, RandomResizedCrop, ColorJitter, ToTensor

>>> normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
>>> _transforms = Compose(
...     [RandomResizedCrop(feature_extractor.size), ColorJitter(brightness=0.5, hue=0.5), ToTensor(), normalize]
... )
```

----------------------------------------

TITLE: Loading IMDb Dataset for Sentiment Analysis
DESCRIPTION: Loads the IMDb movie reviews dataset from the Hugging Face datasets library, which will be used for training a sentiment analysis model to classify reviews as positive or negative.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> imdb = load_dataset("imdb")
```

----------------------------------------

TITLE: Preparing Inputs for Manual Classification
DESCRIPTION: Processes the image and candidate labels using the loaded processor.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/zero_shot_image_classification.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
>>> candidate_labels = ["tree", "car", "bike", "cat"]
>>> inputs = processor(images=image, text=candidate_labels, return_tensors="pt", padding=True)
```

----------------------------------------

TITLE: Image Decoding and Preprocessing Function
DESCRIPTION: Custom function to decode, resize, normalize, and transform image data for TensorFlow dataset processing
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
BATCH_SIZE = 8 * strategy.num_replicas_in_sync

def decode_fn(sample):
    image_data = tf.io.read_file(sample["filename"])
    image = tf.io.decode_jpeg(image_data, channels=3)
    image = tf.image.resize(image, image_size)
    array = tf.cast(image, tf.float32)
    array /= 255.0
    array = (array - image_mean) / image_std
    array = tf.transpose(array, perm=[2, 0, 1])
    return {"pixel_values": array, "labels": sample["labels"]}

tf_dataset = tf_dataset.map(decode_fn)
tf_dataset = tf_dataset.batch(BATCH_SIZE, drop_remainder=True)
```

----------------------------------------

TITLE: Distributed Training and Mixed Precision with PyTorch
DESCRIPTION: This bash script launches a distributed training job for summarization using PyTorch with mixed precision. It uses `torchrun` to manage multiple processes across GPUs, enables mixed precision with the `fp16` flag, and sets training and evaluation parameters. The `nproc_per_node` argument specifies the number of GPUs to use per node.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/run_scripts.md#_snippet_5

LANGUAGE: bash
CODE:
```
torchrun \
    --nproc_per_node 8 pytorch/summarization/run_summarization.py \
    --fp16 \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Share Model to Hugging Face Hub
DESCRIPTION: Uploads the trained model to the Hugging Face Model Hub.  It configures the model name, dataset, source prefix, and output directory. It also pushes the model to the hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/run_scripts.md#_snippet_18

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --push_to_hub \
    --push_to_hub_model_id finetuned-t5-cnn_dailymail \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Loading Pretrained Model with TFAutoModelForCausalLM (TensorFlow) - Python
DESCRIPTION: This snippet illustrates how to load a pretrained model with TensorFlow using the TFAutoModelForCausalLM class. It fetches the model from the Hugging Face Hub with appropriate TensorFlow configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/models.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
from transformers import TFAutoModelForCausalLM, TFMistralForCausalLM

# load with AutoClass or model-specific class
model = TFAutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1")
model = TFMistralForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1")
```

----------------------------------------

TITLE: Pipeline Batching Example
DESCRIPTION: Shows how to implement batching with pipelines using datasets and custom batch sizes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/pipelines.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import pipeline
from transformers.pipelines.pt_utils import KeyDataset
import datasets

dataset = datasets.load_dataset("imdb", name="plain_text", split="unsupervised")
pipe = pipeline("text-classification", device=0)
for out in pipe(KeyDataset(dataset, "text"), batch_size=8, truncation="only_first"):
    print(out)
```

----------------------------------------

TITLE: Point Prompting with SAM Python
DESCRIPTION: Performs point prompting using the SAM model. It defines an input point, processes the image and point with the processor, runs inference with the model, and post-processes the output masks to match the original image size.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/mask_generation.md#_snippet_6

LANGUAGE: python
CODE:
```
input_points = [[[2592, 1728]]] # ë²Œì˜ í¬ì¸íŠ¸ ìœ„ì¹˜

inputs = processor(image, input_points=input_points, return_tensors="pt").to(device)
with torch.no_grad():
    outputs = model(**inputs)
masks = processor.image_processor.post_process_masks(outputs.pred_masks.cpu(), inputs["original_sizes"].cpu(), inputs["reshaped_input_sizes"].cpu())
```

----------------------------------------

TITLE: Fine-tuning T5 for English to German Translation
DESCRIPTION: This snippet shows how to fine-tune a T5 model for English to German translation using a pre-processed dataset. It specifies the model, languages, dataset, and training parameters for high BLEU scores.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation/README.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
python examples/pytorch/translation/run_translation.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --source_lang en \
    --target_lang de \
    --source_prefix "translate English to German: " \
    --dataset_name stas/wmt14-en-de-pre-processed \
    --output_dir /tmp/tst-translation \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Convert GPTQ/AWQ Models to AutoRound Format (Python)
DESCRIPTION: This snippet shows how to convert a GPTQ/AWQ model to the AutoRound format using Hugging Face Transformers. It loads a GPTQ model, applies the AutoRoundConfig, and performs inference. This conversion allows for better compatibility and support with Intel devices. Requires the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/auto_round.md#_snippet_9

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoRoundConfig

model_name = "ybelkada/opt-125m-gptq-4bit"
quantization_config = AutoRoundConfig()
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="cpu", quantization_config=quantization_config, torch_dtype="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)
text = "There is a girl who likes adventure,"
inputs = tokenizer(text, return_tensors="pt").to(model.device)
print(tokenizer.decode(model.generate(**inputs, max_new_tokens=50, do_sample=False)[0]))
```

----------------------------------------

TITLE: Visualizing Object Detection Predictions
DESCRIPTION: This code visualizes the object detection predictions by drawing rectangles around the detected objects and adding labels with scores on the image. It iterates through the predictions, extracts the bounding box coordinates, label, and score, and uses `ImageDraw` to draw the rectangles and text on the image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/zero_shot_object_detection.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
">>> from PIL import ImageDraw

>>> draw = ImageDraw.Draw(image)

>>> for prediction in predictions:
...     box = prediction["box"]
...     label = prediction["label"]
...     score = prediction["score"]

...     xmin, ymin, xmax, ymax = box.values()
...     draw.rectangle((xmin, ymin, xmax, ymax), outline="red", width=1)
...     draw.text((xmin, ymin), f"{label}: {round(score,2)}", fill="white")

>>> image"
```

----------------------------------------

TITLE: Loading a Pre-trained Image Processor with AutoImageProcessor in Python
DESCRIPTION: This code shows how to load a pre-trained Vision Transformer (ViT) image processor that formats images correctly for input to vision models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/autoclass_tutorial.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
```

----------------------------------------

TITLE: Generating Text with Gemma using transformers CLI (Bash)
DESCRIPTION: Illustrates how to perform text generation directly from the command line using the transformers run command. It pipes input text into the command, specifying the task, model, and device. Requires the `transformers` library installed with CLI entry points.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gemma.md#_snippet_2

LANGUAGE: bash
CODE:
```
echo -e "LLMs generate text through a process known as" | transformers run --task text-generation --model google/gemma-2b --device 0
```

----------------------------------------

TITLE: Initializing GPT-NeoX-20B Model in Half Precision
DESCRIPTION: Code for loading the GPT-NeoX-20B model with half-precision (fp16) for improved memory efficiency. This is recommended due to the model being trained with fp16.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gpt_neox.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
model = GPTNeoXForCausalLM.from_pretrained("EleutherAI/gpt-neox-20b").half().cuda()
```

----------------------------------------

TITLE: Generating Text Using a Pipeline
DESCRIPTION: This snippet demonstrates how to use a pipeline for text generation with the finetuned model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_21

LANGUAGE: python
CODE:
```
from transformers import pipeline

prompt = "Somatic hypermutation allows the immune system to"
generator = pipeline("text-generation", model="username/my_awesome_eli5_clm-model")
generator(prompt)
```

----------------------------------------

TITLE: Installing Transformers Library Using uv in Bash
DESCRIPTION: This command demonstrates the use of uv to install the Transformers Python library, beneficial for users looking for a Rust-based package manager.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
uv pip install transformers
```

----------------------------------------

TITLE: Converting Tokenized Data to TensorFlow Tensors
DESCRIPTION: Shows how to convert tokenized data to TensorFlow tensors by setting the return_tensors parameter to 'tf', creating model-ready inputs for TensorFlow-based models with appropriate padding and truncation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/preprocessing.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors="tf")
>>> print(encoded_input)
{'input_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[  101,   153,  7719, 21490,  1122,  1114,  9582,  1623,   102],
       [  101,  5226,  1122,  9649,  1199,  2610,  1236,   102,     0]],
      dtype=int32)>, 
 'token_type_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 
 'attention_mask': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 0]], dtype=int32)>}
```

----------------------------------------

TITLE: Sentiment Analysis in French
DESCRIPTION: Initializes a sentiment analysis pipeline with a specific model and tokenizer, then applies it to French text. This demonstrates how to use a custom model for a specific language.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/quicktour.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
">>> classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)\n>>> classifier("Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ðŸ¤— Transformers.")\n[{'label': '5 stars', 'score': 0.7273}]"
```

----------------------------------------

TITLE: Image Classification Pipeline Implementation
DESCRIPTION: Demonstrates setting up and using an image classification pipeline with a pre-trained model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/pipeline_tutorial.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import pipeline

vision_classifier = pipeline(task="image-classification")
preds = vision_classifier(images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg")
```

----------------------------------------

TITLE: Distributed Training with torchrun
DESCRIPTION: Launches a distributed training job using `torchrun` with mixed precision training enabled. This example shows how to use `fp16` and sets the number of processes per node for multi-GPU training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/run_scripts.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
```bash
torchrun \
    --nproc_per_node 8 pytorch/summarization/run_summarization.py \
    --fp16 \
    ...
    ...
```
```

----------------------------------------

TITLE: Adding adapter for training
DESCRIPTION: Adds the configured PEFT adapter to the model for training. The `peft_config` object is used to configure the adapter.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/peft.md#_snippet_11

LANGUAGE: python
CODE:
```
model.add_adapter(peft_config)
```

----------------------------------------

TITLE: Running Inference with TensorFlow Model
DESCRIPTION: This snippet shows how to run the input tensors through the TensorFlow model to obtain logits for the segmentation task, without needing to define loss functions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_29

LANGUAGE: python
CODE:
```
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained("MariaK/scene_segmentation")
>>> logits = model(**inputs).logits
```

----------------------------------------

TITLE: Running Flax GLUE Task Fine-tuning on BERT
DESCRIPTION: Command example for fine-tuning BERT on GLUE tasks using Flax implementation. This command runs the script with the MRPC task, setting learning rate, sequence length, and other training parameters. It also demonstrates how to push the model to Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/flax/text-classification/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
export TASK_NAME=mrpc

python run_flax_glue.py \
  --model_name_or_path google-bert/bert-base-cased \
  --task_name ${TASK_NAME} \
  --max_seq_length 128 \
  --learning_rate 2e-5 \
  --num_train_epochs 3 \
  --per_device_train_batch_size 4 \
  --eval_steps 100 \
  --output_dir ./$TASK_NAME/ \
  --push_to_hub
```

----------------------------------------

TITLE: Logging into Hugging Face Hub
DESCRIPTION: This Python snippet allows users to log into their Hugging Face account so they can upload and share their models. It requires the huggingface_hub library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/question_answering.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Logging into Hugging Face Hub for Model Sharing
DESCRIPTION: Authenticates with the Hugging Face Hub to enable uploading and sharing of trained models, using notebook_login from huggingface_hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/visual_question_answering.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Pipeline with Dataset Integration
DESCRIPTION: Demonstrates how to use pipeline with HuggingFace datasets and KeyDataset for efficient processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/pipelines.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import datasets
from transformers import pipeline
from transformers.pipelines.pt_utils import KeyDataset
from tqdm.auto import tqdm

pipe = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h", device=0)
dataset = datasets.load_dataset("superb", name="asr", split="test")

for out in tqdm(pipe(KeyDataset(dataset, "file"))):
    print(out)
```

----------------------------------------

TITLE: Loading a Specific Model Version in Python
DESCRIPTION: Demonstrates how to load a specific version of a model using the revision parameter, which can be a tag name, branch name, or commit hash.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_sharing.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
model = AutoModel.from_pretrained(
    "julien-c/EsperBERTo-small", revision="4c77982"  # tag name, or branch name, or commit hash
)
```

----------------------------------------

TITLE: Prepare TensorFlow Dataset
DESCRIPTION: Converts the dataset to a `tf.data.Dataset` format using the `prepare_tf_dataset` method of the `TFAutoModelForMaskedLM` model. This configures the data for efficient training in TensorFlow. Requires a `lm_dataset` and `data_collator` to be defined.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_19

LANGUAGE: python
CODE:
```
>>> tf_train_set = model.prepare_tf_dataset(
...	lm_dataset["train"],
...	shuffle=True,
...	batch_size=16,
...	collate_fn=data_collator,
... )

>>> tf_test_set = model.prepare_tf_dataset(
...	lm_dataset["test"],
...	shuffle=False,
...	batch_size=16,
...	collate_fn=data_collator,
... )
```

----------------------------------------

TITLE: Object Detection Pipeline
DESCRIPTION: Shows how to detect and locate objects in images using bounding boxes with the Hugging Face pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/task_summary.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> detector = pipeline(task="object-detection")
>>> preds = detector(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"], "box": pred["box"]} for pred in preds]
```

----------------------------------------

TITLE: Audio Classification Example
DESCRIPTION: Example of classifying emotions in an audio file
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/pipeline_tutorial.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
audio_classifier("jfk_moon_speech.wav")
```

----------------------------------------

TITLE: Load DistilRoBERTa Model for Masked Language Modeling (PyTorch)
DESCRIPTION: Loads the pre-trained DistilRoBERTa model for masked language modeling using `AutoModelForMaskedLM` from the `transformers` library. This initializes the model for fine-tuning on a masked language modeling task. Requires the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_12

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForMaskedLM

>>> model = AutoModelForMaskedLM.from_pretrained("distilbert/distilroberta-base")
```

----------------------------------------

TITLE: Loading Model for Semantic Segmentation with PyTorch
DESCRIPTION: This snippet demonstrates how to load a pre-trained SegFormer model for semantic segmentation using PyTorch. It requires the Hugging Face Transformers library dependency, particularly `AutoModelForSemanticSegmentation`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/sequence_classification.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer

>>> model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)
```

----------------------------------------

TITLE: Load Image Dataset (Python)
DESCRIPTION: Loads the food101 image dataset using the ðŸ¤— Datasets library, limiting the dataset to the first 100 training examples for faster processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/preprocessing.md#_snippet_16

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> dataset = load_dataset("food101", split="train[:100]")
```

----------------------------------------

TITLE: Defining Evaluation Metrics for Model Training
DESCRIPTION: Creates a compute_metrics function that evaluates the model's performance during training. This function calculates the accuracy metric based on model predictions and actual labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/knowledge_distillation_for_image_classification.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
import evaluate
import numpy as np

accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    acc = accuracy.compute(references=labels, predictions=np.argmax(predictions, axis=1))
    return {"accuracy": acc["accuracy"]}
```

----------------------------------------

TITLE: Running Summarization Script in PyTorch
DESCRIPTION: Executes the `run_summarization.py` script from the PyTorch examples directory to fine-tune a T5-small model on the CNN/DailyMail dataset.  It includes parameters for dataset configuration, training/evaluation, output directory, batch size, and pushing the model to the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/run_scripts.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
```bash
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    # remove the `max_train_samples`, `max_eval_samples` and `max_predict_samples` if everything works
    --max_train_samples 50 \
    --max_eval_samples 50 \
    --max_predict_samples 50 \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --push_to_hub \
    --push_to_hub_model_id finetuned-t5-cnn_dailymail \
    # remove if using `output_dir previous_output_dir`
    # --overwrite_output_dir \
    --output_dir previous_output_dir \
    # --resume_from_checkpoint path_to_specific_checkpoint \
    --predict_with_generate \
```
```

----------------------------------------

TITLE: Flattening a Nested Dataset Structure
DESCRIPTION: This snippet uses flatten to transform nested dataset fields into separate columns, simplifying access to text fields for preprocessing and tokenization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
eli5 = eli5.flatten()
eli5["train"][0]
```

----------------------------------------

TITLE: Processing Model Output for Audio Classification in PyTorch
DESCRIPTION: Demonstrates how to convert model logits to predicted class labels by finding the class with the highest probability and mapping it to a human-readable label using the model's configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/audio_classification.md#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
>>> import torch

>>> predicted_class_ids = torch.argmax(logits).item()
>>> predicted_label = model.config.id2label[predicted_class_ids]
>>> predicted_label
'cash_deposit'
```

----------------------------------------

TITLE: Pushing a Model to the Hub Directly
DESCRIPTION: Demonstrates how to push a model directly to the Hugging Face Hub using the push_to_hub method on the model object.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/model_sharing.md#2025-04-23_snippet_13

LANGUAGE: python
CODE:
```
>>> pt_model.push_to_hub("my-awesome-model")
```

----------------------------------------

TITLE: Preprocessing Function for Tokenization
DESCRIPTION: Defines a preprocessing function using a tokenizer to handle nested lists of text. The function concatenates text fields and applies tokenization to the resulting strings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
def preprocess_function(examples):
    return tokenizer([" ".join(x) for x in examples["answers.text"]])
```

----------------------------------------

TITLE: Converting Models Between PyTorch and TensorFlow
DESCRIPTION: Demonstrates how to convert a saved model from PyTorch to TensorFlow and vice versa using the from_pt and from_tf parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/quicktour.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)

>>> from transformers import TFAutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)
```

----------------------------------------

TITLE: Configuring Data Preloading in TrainingArguments
DESCRIPTION: This snippet shows how to enable data preloading by configuring `dataloader_pin_memory` and `dataloader_num_workers` in `TrainingArguments`. Setting `dataloader_pin_memory` to `True` allocates pinned memory on the CPU, and increasing `dataloader_num_workers` increases the number of CPU threads to preload data faster.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments

args = TrainingArguments(
    per_device_train_batch_size=4,
    gradient_accumulation_steps=16,
    gradient_checkpointing=True,
    bf16=True,
    optim="adamw_bnb_8bit",
    dataloader_pin_memory=True,
    dataloader_num_workers=4,
)
```

----------------------------------------

TITLE: Loading a Model in Half Precision with Transformers
DESCRIPTION: Demonstrates loading a large language model (Mistral-7B) in bfloat16 precision to reduce memory usage. This approach requires 13.74GB of memory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/llm_optims.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-v0.1", torch_dtype=torch.bfloat16, device_map="auto",
)
```

----------------------------------------

TITLE: Creating DataCollator for TensorFlow Implementation
DESCRIPTION: Initializes a DataCollatorForSeq2Seq object for TensorFlow, which efficiently handles padding by dynamically padding sentences to the longest length in each batch and returns tensors in TensorFlow format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors="tf")
```

----------------------------------------

TITLE: Sharing Model to Hugging Face Hub (PyTorch)
DESCRIPTION: Code to push the trained masked language model to the Hugging Face Model Hub, making it available for others to use.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
trainer.push_to_hub()
```

----------------------------------------

TITLE: Basic Text OCR with GOT-OCR2 in PyTorch
DESCRIPTION: This snippet demonstrates how to perform basic OCR on an image to extract plain text using the GOT-OCR2 model with the AutoProcessor and AutoModelForImageTextToText classes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/got_ocr2.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor, AutoModelForImageTextToText
>>> import torch

>>> device = "cuda" if torch.cuda.is_available() else "cpu"
>>> model = AutoModelForImageTextToText.from_pretrained("stepfun-ai/GOT-OCR-2.0-hf", device_map=device)
>>> processor = AutoProcessor.from_pretrained("stepfun-ai/GOT-OCR-2.0-hf", use_fast=True)

>>> image = "https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/image_ocr.jpg"
>>> inputs = processor(image, return_tensors="pt", device=device).to(device)

>>> generate_ids = model.generate(
...     **inputs,
...     do_sample=False,
...     tokenizer=processor.tokenizer,
...     stop_strings="<|im_end|>",
...     max_new_tokens=4096,
... )

>>> processor.decode(generate_ids[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
"R&D QUALITY IMPROVEMENT\nSUGGESTION/SOLUTION FORM\nName/Phone Ext. : (...)"
```

----------------------------------------

TITLE: Image Processor Configuration
DESCRIPTION: Initialize the ViT image processor for handling image transformations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_classification.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoImageProcessor

>>> checkpoint = "google/vit-base-patch16-224-in21k"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Preparing Training Components with Accelerate
DESCRIPTION: This snippet demonstrates how to prepare training objects (dataloaders, model, optimizer) for distributed training using Accelerate's prepare method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/accelerate.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)
```

----------------------------------------

TITLE: Custom Device Mapping for Model Loading
DESCRIPTION: Demonstrates how to create a custom device mapping to control which parts of a model are loaded onto specific devices when using Accelerate for large model loading.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/model.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
device_map = {"shared": 0, "encoder": 0, "decoder": 1, "lm_head": 1}
```

----------------------------------------

TITLE: Pipeline Batch Processing Configuration
DESCRIPTION: Shows how to configure batch processing in pipelines for handling multiple inputs efficiently.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/pipeline_tutorial.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
transcriber = pipeline(model="openai/whisper-large-v2", device=0, batch_size=2)
audio_filenames = [f"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/{i}.flac" for i in range(1, 5)]
texts = transcriber(audio_filenames)
```

----------------------------------------

TITLE: Training with Default Settings and Measuring Memory Usage
DESCRIPTION: Initializes and runs the Hugging Face Trainer with a batch size of 4 to demonstrate memory usage during standard training, without any optimization techniques.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_memory_anatomy.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> from transformers import TrainingArguments, Trainer, logging

>>> logging.set_verbosity_error()


>>> training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)
>>> trainer = Trainer(model=model, args=training_args, train_dataset=ds)
>>> result = trainer.train()
>>> print_summary(result)
```

----------------------------------------

TITLE: Logging into Hugging Face Hub
DESCRIPTION: Demonstrates how to authenticate with the Hugging Face Hub using the notebook_login function, which allows users to upload and share models with the community.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Running Question Answering Inference with JIT Mode on CPU
DESCRIPTION: This snippet demonstrates how to run inference for a question answering task using JIT mode on CPU. It uses the BERT model and the SQuAD dataset, with JIT mode enabled for evaluation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/perf_infer_cpu.md#2025-04-23_snippet_0

LANGUAGE: bash
CODE:
```
python run_qa.py \
--model_name_or_path csarron/bert-base-uncased-squad-v1 \
--dataset_name squad \
--do_eval \
--max_seq_length 384 \
--doc_stride 128 \
--output_dir /tmp/ \
--no_cuda \
--jit_mode_eval
```

----------------------------------------

TITLE: Loading Fine-tuned Model for Image Classification - Python
DESCRIPTION: This code snippet shows how to load a fine-tuned version of the DiT model specifically for image classification tasks, utilizing the AutoModelForImageClassification class.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/dit.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForImageClassification

model = AutoModelForImageClassification.from_pretrained("microsoft/dit-base-finetuned-rvlcdip")
```

----------------------------------------

TITLE: Convert TensorFlow to PyTorch - Python
DESCRIPTION: This snippet demonstrates how to convert a TensorFlow checkpoint to a PyTorch model using the `from_pretrained` method with `from_tf=True`. The converted model is then saved.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/model_sharing.md#_snippet_3

LANGUAGE: python
CODE:
```
>>> pt_model = DistilBertForSequenceClassification.from_pretrained("path/to/awesome-name-you-picked", from_tf=True)
>>> pt_model.save_pretrained("path/to/awesome-name-you-picked")
```

----------------------------------------

TITLE: Loading Mixed-Int8 Models with BitsAndBytesConfig
DESCRIPTION: Code to load a causal language model in 8-bit precision using BitsAndBytesConfig. This enables memory-efficient loading of large models by utilizing Int8 quantization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/perf_infer_gpu_one.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

model_name = "bigscience/bloom-2b5"
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```

----------------------------------------

TITLE: Applying a Chat Template with Llama 2 Model in Python
DESCRIPTION: Shows how to use a chat template with the Llama 2 model, demonstrating how more complex templates add control tokens to distinguish between different roles in a conversation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/chat_templating.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>> from transformers import AutoTokenizer
>> tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")

>> chat = [
...   {"role": "user", "content": "Hello, how are you?"},
...   {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
...   {"role": "user", "content": "I'd like to show off how chat templating works!"},
... ]

>> tokenizer.use_default_system_prompt = False
>> tokenizer.apply_chat_template(chat, tokenize=False)
"<s>[INST] Hello, how are you? [/INST] I'm doing great. How can I help you today? </s><s>[INST] I'd like to show off how chat templating works! [/INST]"
```

----------------------------------------

TITLE: Generating Text with the Octocoder Model
DESCRIPTION: This code generates text using the Octocoder model pipeline. It provides a prompt asking for a Python function to convert bytes to gigabytes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/llm_tutorial_optimization.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
prompt = "Question: Please write a function in Python that transforms bytes to Giga bytes.\n\nAnswer:"

result = pipe(prompt, max_new_tokens=60)[0]["generated_text"][len(prompt):]
result
```

----------------------------------------

TITLE: Generating Masks Using Pipeline Python
DESCRIPTION: Generates masks for the loaded image using the `mask-generation` pipeline. The `points_per_batch` parameter controls the batch size for parallel inference of points, and `pred_iou_thresh` sets the IoU confidence threshold for the returned masks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/mask_generation.md#_snippet_3

LANGUAGE: python
CODE:
```
masks = mask_generator(image, points_per_batch=128, pred_iou_thresh=0.88)
```

----------------------------------------

TITLE: Initializing Wav2Vec2 Model for Speech Recognition
DESCRIPTION: Load a pre-trained Wav2Vec2 model with custom CTC loss reduction and padding configuration for fine-tuning
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/asr.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCTC, TrainingArguments, Trainer

model = AutoModelForCTC.from_pretrained(
    "facebook/wav2vec2-base",
    ctc_loss_reduction="mean",
    pad_token_id=processor.tokenizer.pad_token_id,
)
```

----------------------------------------

TITLE: Installing Transformers with PyTorch
DESCRIPTION: This command installs ðŸ¤— Transformers along with PyTorch as a dependency. It simplifies the installation process when using PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/installation.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
"pip install transformers[torch]"
```

----------------------------------------

TITLE: Generate Pixel Values from Transforms (Python)
DESCRIPTION: This snippet defines a function `transforms` that takes a dictionary of examples, converts the image to RGB format, applies a series of transformations to the image, and assigns the transformed pixel values to the 'pixel_values' key within each example.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/preprocessing.md#_snippet_20

LANGUAGE: python
CODE:
```
>>> def transforms(examples):
...     examples["pixel_values"] = [_transforms(image.convert("RGB")) for image in examples["image"]]
...     return examples
```

----------------------------------------

TITLE: Perform Zero-Shot Image Classification with SigLIP2 NaFlex using AutoModel (Python)
DESCRIPTION: This example illustrates zero-shot image classification using the SigLIP2 NaFlex model with `AutoModel` and `AutoProcessor`. Unlike the FixRes variant, it uses the `max_num_patches` parameter in the processor to control image resolution adaptively while maintaining aspect ratio. The process involves loading the model, processor, image, and text, then running inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/siglip2.md#_snippet_2

LANGUAGE: Python
CODE:
```
import torch
import requests
from PIL import Image
from transformers import AutoProcessor, AutoModel

model = AutoModel.from_pretrained("google/siglip2-base-patch16-naflex", torch_dtype=torch.float16, device_map="auto", attn_implementation="sdpa")
processor = AutoProcessor.from_pretrained("google/siglip2-base-patch16-naflex")

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
image = Image.open(requests.get(url, stream=True).raw)
candidate_labels = ["a Pallas cat", "a lion", "a Siberian tiger"]
texts = [f'This is a photo of {label}.' for label in candidate_labels]

# default value for `max_num_patches` is 256, but you can increase resulted image resolution providing higher values e.g. `max_num_patches=512`
inputs = processor(text=texts, images=image, padding="max_length", max_num_patches=256, return_tensors="pt").to("cuda")

with torch.no_grad():
    outputs = model(**inputs)

logits_per_image = outputs.logits_per_image
probs = torch.sigmoid(logits_per_image)
print(f"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'")
```

----------------------------------------

TITLE: Initializing Language Model and Tokenizer
DESCRIPTION: This code initializes the OctoCoder model and its corresponding tokenizer, setting up a pipeline for text generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
model = AutoModelForCausalLM.from_pretrained("bigcode/octocoder", torch_dtype=torch.bfloat16, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained("bigcode/octocoder")

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
```

----------------------------------------

TITLE: Exporting a Locally Saved PyTorch Model to ONNX
DESCRIPTION: Command to export a locally saved PyTorch model checkpoint to ONNX format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/serialization.md#2025-04-22_snippet_7

LANGUAGE: bash
CODE:
```
python -m transformers.onnx --model=local-pt-checkpoint onnx/
```

----------------------------------------

TITLE: Performing Inference with the Image Feature Extraction Pipeline - Python
DESCRIPTION: This code demonstrates how to perform inference using the initialized image-feature-extraction pipeline by passing an array of images to it. The resulting output contains pooled embeddings for further processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_feature_extraction.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
outputs = pipe([image_real, image_gen])
```

----------------------------------------

TITLE: Installing Accelerate Package
DESCRIPTION: Command to install the Accelerate library which is required for FSDP implementation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/fsdp.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install accelerate
```

----------------------------------------

TITLE: Installing Transformers with TensorFlow
DESCRIPTION: Command to install Transformers with TensorFlow CPU support
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/installation.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
pip install transformers[tf-cpu]
```

----------------------------------------

TITLE: Loading a Dataset for TensorFlow Training with Keras
DESCRIPTION: Demonstrates loading the CoLA dataset from the GLUE benchmark for a binary text classification task when using TensorFlow with Keras.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/training.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
from datasets import load_dataset

dataset = load_dataset("glue", "cola")
dataset = dataset["train"]  # ä»Šã®ã¨ã“ã‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åˆ†å‰²ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™
```

----------------------------------------

TITLE: Model Training Setup and Execution
DESCRIPTION: Configuration and execution of model training using Trainer class
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/visual_question_answering.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import TrainingArguments, Trainer

>>> repo_id = "MariaK/vilt_finetuned_200"

>>> training_args = TrainingArguments(
...     output_dir=repo_id,
...     per_device_train_batch_size=4,
...     num_train_epochs=20,
...     save_steps=200,
...     logging_steps=50,
...     learning_rate=5e-5,
...     save_total_limit=2,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     data_collator=data_collator,
...     train_dataset=processed_dataset,
...     processing_class=processor,
... )

>>> trainer.train()
>>> trainer.push_to_hub()
```

----------------------------------------

TITLE: Loading Bark Model in Half-Precision
DESCRIPTION: Demonstrates how to load the Bark model in half-precision (float16) to reduce memory usage and improve inference speed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bark.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import BarkModel
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"
model = BarkModel.from_pretrained("suno/bark-small", torch_dtype=torch.float16).to(device)
```

----------------------------------------

TITLE: Visualizing Detection Results
DESCRIPTION: Draws bounding boxes and labels on the input image to visualize the detection results.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/object_detection.md#2025-04-22_snippet_22

LANGUAGE: python
CODE:
```
draw = ImageDraw.Draw(image)

for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
    box = [round(i, 2) for i in box.tolist()]
    x, y, x2, y2 = tuple(box)
    draw.rectangle((x, y, x2, y2), outline="red", width=1)
    draw.text((x, y), model.config.id2label[label.item()], fill="white")

image
```

----------------------------------------

TITLE: Initialize BERT Tokenizer with Pre-trained Model Python
DESCRIPTION: This snippet initializes a BertTokenizer from the transformers library using a pre-trained model. The BertTokenizer is used to convert text into numerical representations that can be fed into a BERT model. It depends on the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/glossary.md#_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-cased")

>>> sequence = "A Titan RTX has 24GB of VRAM"
```

----------------------------------------

TITLE: Pushing Custom Model to Hugging Face Hub in Python
DESCRIPTION: Shows how to upload the custom ResNet model to the Hugging Face Hub using the push_to_hub method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/custom_models.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
resnet50d.push_to_hub("custom-resnet50d")
```

----------------------------------------

TITLE: Batched Multi-Image Input and Quantization with Mistral3
DESCRIPTION: This code snippet showcases batched text-image inputs with different numbers of images for each text using Mistral3. It also demonstrates how to load the model in 4-bit quantization using `BitsAndBytesConfig`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mistral3.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig
>>> import torch

>>> torch_device = "cuda"
>>> model_checkpoint = "mistralai/Mistral-Small-3.1-24B-Instruct-2503"
>>> processor = AutoProcessor.from_pretrained(model_checkpoint)
>>> quantization_config = BitsAndBytesConfig(load_in_4bit=True)
>>> model = AutoModelForImageTextToText.from_pretrained(
...     model_checkpoint, quantization_config=quantization_config
... )

>>> messages = [
... Â  Â  [
... Â  Â  Â  Â  {
... Â  Â  Â  Â  Â  Â  "role": "user",
... Â  Â  Â  Â  Â  Â  "content": [
... Â  Â  Â  Â  Â  Â  Â  Â  {"type": "image", "url": "https://llava-vl.github.io/static/images/view.jpg"},
... Â  Â  Â  Â  Â  Â  Â  Â  {"type": "text", "text": "Write a haiku for this image"},
... Â  Â  Â  Â  Â  Â  ],
... Â  Â  Â  Â  },
... Â  Â  ],
... Â  Â  [
... Â  Â  Â  Â  {
... Â  Â  Â  Â  Â  Â  "role": "user",
... Â  Â  Â  Â  Â  Â  "content": [
... Â  Â  Â  Â  Â  Â  Â  Â  {"type": "image", "url": "https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg"},
... Â  Â  Â  Â  Â  Â  Â  Â  {"type": "image", "url": "https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg"},
... Â  Â  Â  Â  Â  Â  Â  Â  {"type": "text", "text": "These images depict two different landmarks. Can you identify them?"},
... Â  Â  Â  Â  Â  Â  ],
... Â  Â  Â  Â  },
... Â  Â  ],
>>> ]

>>> inputs = processor.apply_chat_template(messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors="pt").to(model.device, dtype=torch.bfloat16)

>>> output = model.generate(**inputs, max_new_tokens=25)

>>> decoded_outputs = processor.batch_decode(output, skip_special_tokens=True)
>>> decoded_outputs
["Write a haiku for this imageSure, here is a haiku inspired by the image:\n\nCalm lake's wooden path\nSilent forest stands guard\n", "These images depict two different landmarks. Can you identify them? Certainly! The images depict two iconic landmarks:\n\n1. The first image shows the Statue of Liberty in New York City."]
```

----------------------------------------

TITLE: Pushing Model to Organization Repository in Python
DESCRIPTION: Illustrates how to share a model to an organization's repository on the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/model_sharing.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> pt_model.push_to_hub("my-awesome-model", organization="my-awesome-org")
```

----------------------------------------

TITLE: Padding Tokenized Inputs
DESCRIPTION: Shows how to use padding to ensure all tokenized sequences in a batch have the same length.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/preprocessing.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True)
print(encoded_input)
```

----------------------------------------

TITLE: Setting Up Evaluation Metrics
DESCRIPTION: Configures evaluation metrics using the Hugging Face evaluate library to track model performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/training.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
import numpy as np
import evaluate

metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

----------------------------------------

TITLE: Defining Metric Computation Function for Model Evaluation in Python
DESCRIPTION: This code defines a function to compute accuracy metrics for model evaluation during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/training.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
import numpy as np
from datasets import load_metric

metric = load_metric("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

----------------------------------------

TITLE: Tokenizing and Truncating to Max Length
DESCRIPTION: This snippet demonstrates how to truncate a batch of sentences to the model's maximum input length when `max_length` is not specified or to a specific length when `max_length` is provided. The `truncation=True` argument enables truncation. `truncation=STRATEGY` can be used to specify truncation strategy for sequence pairs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pad_truncation.md#_snippet_4

LANGUAGE: Python
CODE:
```
tokenizer(batch_sentences, truncation=True)
```

LANGUAGE: Python
CODE:
```
tokenizer(batch_sentences, truncation=STRATEGY)
```

----------------------------------------

TITLE: Loading MInDS-14 Dataset
DESCRIPTION: Loading and splitting the MInDS-14 dataset for audio classification training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/audio_classification.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from datasets import load_dataset, Audio

minds = load_dataset("PolyAI/minds14", name="en-US", split="train")
minds = minds.train_test_split(test_size=0.2)
```

----------------------------------------

TITLE: Initializing Seq2SeqTrainer for T5 Model in PyTorch
DESCRIPTION: Creates a Seq2SeqTrainer instance with the model, training arguments, datasets, and other necessary components for training a T5 model in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/translation.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_books["train"],
    eval_dataset=tokenized_books["test"],
    processing_class=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()
```

----------------------------------------

TITLE: Handling Special Tokens in Tokenization in Python
DESCRIPTION: This snippet demonstrates how special tokens are added during tokenization and how they appear in the decoded output. It compares direct tokenization with manual token-to-id conversion.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/fast_tokenizers.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
model_inputs = tokenizer("We are very happy to show you the ðŸ¤— Transformers library.")
[2, 1734, 708, 1508, 4915, 577, 1500, 692, 573, 156808, 128149, 9581]
tokenizer.convert_tokens_to_ids(tokens)
[1734, 708, 1508, 4915, 577, 1500, 692, 573, 156808, 128149, 9581]
```

LANGUAGE: python
CODE:
```
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```

----------------------------------------

TITLE: Creating Trainer Instance for Hyperparameter Search in Transformers
DESCRIPTION: Example of creating a Trainer instance configured for hyperparameter search, specifying model initialization, training arguments, datasets, and evaluation metrics.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/hpo_train.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
trainer = Trainer(
    model=None,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
    processing_class=tokenizer,
    model_init=model_init,
    data_collator=data_collator,
)
```

----------------------------------------

TITLE: Loading a Pretrained Image Processor with AutoImageProcessor in Python
DESCRIPTION: This code shows how to load a pretrained image processor for visual tasks using the AutoImageProcessor class.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/autoclass_tutorial.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
```

----------------------------------------

TITLE: Loading Custom Model and Tokenizer
DESCRIPTION: Example of loading a specific model and tokenizer for text generation using AutoModelForCausalLM
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/pipeline_tutorial.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("distilbert/distilgpt2")
model = AutoModelForCausalLM.from_pretrained("distilbert/distilgpt2")
```

----------------------------------------

TITLE: Creating a Text Preprocessing Function for Tokenization
DESCRIPTION: Definition of a function that tokenizes text examples and applies truncation to ensure the input length does not exceed the model's maximum allowed sequence length.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/sequence_classification.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> def preprocess_function(examples):
...     return tokenizer(examples["text"], truncation=True)
```

----------------------------------------

TITLE: Loading Pre-trained Image Processor with AutoImageProcessor in Python
DESCRIPTION: This snippet shows how to load an image processor for computer vision tasks using the AutoImageProcessor class. This processor converts images into the correct input format for models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/autoclass_tutorial.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
```

----------------------------------------

TITLE: Creating a Dataset Preparation Function for ASR in Python
DESCRIPTION: Defines a function to process audio data into input_values and tokenize text into labels for ASR model input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/preprocessing.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
def prepare_dataset(example):
    audio = example["audio"]

    example.update(processor(audio=audio["array"], text=example["text"], sampling_rate=16000))

    return example
```

----------------------------------------

TITLE: Configuring Device Parameter in Pipeline
DESCRIPTION: This example demonstrates how to specify a GPU device for pipeline execution, which is essential for optimizing inference performance with large models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pipeline_tutorial.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
generator(model="openai/whisper-large", device=0)
```

----------------------------------------

TITLE: Activating a Virtual Environment (Linux/macOS)
DESCRIPTION: This command activates the virtual environment created in the `.env` directory on Linux and macOS systems.  Activating the environment makes its packages available.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/installation.md#_snippet_1

LANGUAGE: bash
CODE:
```
source .env/bin/activate
```

----------------------------------------

TITLE: Processing Multimodal Inputs with PaliGemma
DESCRIPTION: This snippet demonstrates how to use `AutoProcessor` to load a processor for the PaliGemma model, combining text and image inputs. It initializes the processor, loads an image from a URL, and preprocesses the text and image into the expected model input format using the processor.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/processors.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoProcessor, PaliGemmaForConditionalGeneration
from PIL import Image
import requests

processor = AutoProcessor.from_pretrained("google/paligemma-3b-pt-224")

prompt = "answer en Where is the cat standing?"
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=prompt, images=image, return_tensors="pt")
inputs
```

----------------------------------------

TITLE: Loading a Pretrained Model with TensorFlow
DESCRIPTION: Example illustrating the usage of the `transformers` library with the TensorFlow backend to load a GPT-2 model variant. This snippet shows model and tokenizer preparation for text generation with efficient device utilization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
from transformers import TFAutoModelForCausalLM, AutoTokenizer

model = TFAutoModelForCausalLM.from_pretrained("openai-community/gpt2-xl")
tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2-xl")
```

LANGUAGE: Python
CODE:
```
model_inputs = tokenizer(["The secret to baking a good cake is "], return_tensors="tf")
```

LANGUAGE: Python
CODE:
```
generated_ids = model.generate(**model_inputs, max_length=30)
tokenizer.batch_decode(generated_ids)[0]
'The secret to baking a good cake is \xa0to use the right ingredients. \xa0The secret to baking a good cake is to use the right'
```

----------------------------------------

TITLE: Loading GPT-J Model in Float16 on CUDA
DESCRIPTION: This snippet demonstrates how to load the GPT-J model in half-precision using the Transformers library, which helps reduce RAM usage on compatible CUDA devices. It requires the 'transformers' and 'torch' libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gptj.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import GPTJForCausalLM
>>> import torch

>>> device = "cuda"
>>> model = GPTJForCausalLM.from_pretrained(
...     "EleutherAI/gpt-j-6B",
...     revision="float16",
...     torch_dtype=torch.float16,
... ).to(device)
```

----------------------------------------

TITLE: Creating a Generation Loop with DynamicCache in Transformers (Python)
DESCRIPTION: This snippet demonstrates how to implement a generation loop using the DynamicCache class. It initializes the model and tokenizer, sets up the input data, and iteratively generates new tokens while managing the cache and attention mask correctly.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/cache_explanation.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache

model_id = "meta-llama/Llama-2-7b-chat-hf"
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map="cuda:0")
tokenizer = AutoTokenizer.from_pretrained(model_id)

past_key_values = DynamicCache()
messages = [{"role": "user", "content": "Hello, what's your name."}]
inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt", return_dict=True).to("cuda:0")

generated_ids = inputs.input_ids
cache_position = torch.arange(inputs.input_ids.shape[1], dtype=torch.int64, device="cuda:0")
max_new_tokens = 10

for _ in range(max_new_tokens):
    outputs = model(**inputs, cache_position=cache_position, past_key_values=past_key_values, use_cache=True)
    # Greedily sample one next token
    next_token_ids = outputs.logits[:, -1:].argmax(-1)
    generated_ids = torch.cat([generated_ids, next_token_ids], dim=-1)
    # Prepare inputs for the next generation step by leaving unprocessed tokens, in our case we have only one new token
    # and expanding attn mask for the new token, as explained above
    attention_mask = inputs["attention_mask"]
    attention_mask = torch.cat([attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1)
    inputs = {"input_ids": next_token_ids, "attention_mask": attention_mask}
    cache_position = cache_position[-1:] + 1 # add one more position for the next token

print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])
"[INST] Hello, what's your name. [/INST]  Hello! My name is LLaMA,"
```

----------------------------------------

TITLE: Creating a Zero-shot Image Classification Pipeline with Transformers
DESCRIPTION: Code to instantiate a zero-shot image classification pipeline using a pre-trained CLIP model from Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/zero_shot_image_classification.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> checkpoint = "openai/clip-vit-large-patch14"
>>> detector = pipeline(model=checkpoint, task="zero-shot-image-classification")
```

----------------------------------------

TITLE: Complete Training Script Modifications
DESCRIPTION: Complete diff showing all required changes to enable distributed training with Accelerate
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/hi/accelerate.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

----------------------------------------

TITLE: Create Instance of Image Classification Model
DESCRIPTION: This snippet shows how to create an instance of the `ResnetModelForImageClassification` using a previously created `resnet50d_config` object. This initializes the model with the specified configuration parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/custom_models.md#_snippet_5

LANGUAGE: python
CODE:
```
resnet50d = ResnetModelForImageClassification(resnet50d_config)
```

----------------------------------------

TITLE: Pushing Model to Hub (PyTorch)
DESCRIPTION: This code pushes the trained model and tokenizer to the Hugging Face Model Hub. This allows others to easily access and use the trained model. The `trainer.push_to_hub()` method handles the process of uploading the model and tokenizer to the Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
trainer.push_to_hub()
```

----------------------------------------

TITLE: Inferring with the Model in Python
DESCRIPTION: This code snippet executes the model's inference by passing the preprocessed pixel values, ensuring no gradients are computed to save memory and processing time.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_to_image.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
import torch

with torch.no_grad():
  outputs = model(pixel_values)
```

----------------------------------------

TITLE: Using SDPA with Flash Attention
DESCRIPTION: Shows how to use Scaled Dot Product Attention with Flash Attention kernels for optimized inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_infer_gpu_one.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import torch
from torch.nn.attention import SDPBackend, sdpa_kernel
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", torch_dtype=torch.float16).to("cuda")
model.to_bettertransformer()

input_text = "Hello my dog is cute and"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

with sdpa_kernel(SDPBackend.FLASH_ATTENTION):
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Serving a Transformer Model with vLLM
DESCRIPTION: This shell command uses vLLM to serve a specified Transformer model, specifying the task and implementation to be 'transformers'. The '--trust-remote-code' parameter allows loading models from remote, with vLLM installed as a prerequisite. The command is useful for testing and deploying models via CLI with native or fallback support to Transformers.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/serving.md#2025-04-22_snippet_1

LANGUAGE: shell
CODE:
```
vllm serve Qwen/Qwen2.5-1.5B-Instruct \
    --task generate \
    --model-impl transformers \
```

LANGUAGE: shell
CODE:
```
vllm serve Qwen/Qwen2.5-1.5B-Instruct \
    --task generate \
    --model-impl transformers \
    --trust-remote-code \
```

----------------------------------------

TITLE: Run Summarization with Custom Dataset
DESCRIPTION: This code snippet demonstrates how to run the summarization script with a custom dataset in CSV or JSON Line format. It specifies the paths to the training and validation files, as well as the column names for the input text and summary. The `train_file`, `validation_file`, `text_column`, and `summary_column` arguments are used to define the custom dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/run_scripts_fr.md#_snippet_12

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --train_file path_to_csv_or_jsonlines_file \
    --validation_file path_to_csv_or_jsonlines_file \
    --text_column text_column_name \
    --summary_column summary_column_name \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --overwrite_output_dir \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate
```

----------------------------------------

TITLE: Specifying a Multilingual Model for Sentiment Analysis in Python
DESCRIPTION: This code shows how to specify a multilingual BERT model that can be used for sentiment analysis in French text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/quicktour.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
```

----------------------------------------

TITLE: Loading Pretrained DistilBERT Model in TensorFlow
DESCRIPTION: This code demonstrates how to load a pretrained DistilBERT model in TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/create_a_model.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
tf_model = TFDistilBertModel.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Using CLIPProcessor and CLIPModel for Image-Text Similarity
DESCRIPTION: This snippet demonstrates how to use CLIPProcessor and CLIPModel to obtain image-text similarity scores for a given image and text pairs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/clip.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from PIL import Image
import requests

from transformers import CLIPProcessor, CLIPModel

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True)

outputs = model(**inputs)
logits_per_image = outputs.logits_per_image  # ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ì„± ì ìˆ˜
probs = logits_per_image.softmax(dim=1)  # í™•ë¥ ì„ ë ˆì´ë¸”ë§ í•˜ê¸°ìœ„í•´ì„œ ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì·¨í•©ë‹ˆë‹¤.
```

----------------------------------------

TITLE: Loading and Processing Audio Dataset
DESCRIPTION: Example showing how to load and process an audio dataset for speech recognition
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/quicktour.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))
>>> result = speech_recognizer(dataset[:4]["audio"])
>>> print([d["text"] for d in result])
```

----------------------------------------

TITLE: Manual Translation Inference with T5 Model in TensorFlow
DESCRIPTION: Demonstrates manual translation inference using the trained T5 model in TensorFlow, covering tokenization, generation, and decoding steps.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/translation.md#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("username/my_awesome_opus_books_model")
inputs = tokenizer(text, return_tensors="tf").input_ids

model = TFAutoModelForSeq2SeqLM.from_pretrained("username/my_awesome_opus_books_model")
outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)

tokenizer.decode(outputs[0], skip_special_tokens=True)
```

----------------------------------------

TITLE: Initializing SAM Model and Processor Python
DESCRIPTION: Initializes the SAM model and processor from the `facebook/sam-vit-base` checkpoint. The model is moved to the available device (CUDA if available, otherwise CPU).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/mask_generation.md#_snippet_5

LANGUAGE: python
CODE:
```
from transformers import SamModel, SamProcessor
import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = SamModel.from_pretrained("facebook/sam-vit-base").to(device)
processor = SamProcessor.from_pretrained("facebook/sam-vit-base")
```

----------------------------------------

TITLE: Loading a Pre-trained Feature Extractor
DESCRIPTION: This code shows how to load a pre-trained feature extractor using `AutoFeatureExtractor.from_pretrained`. Feature extractors are used to process audio signals into a format suitable for audio models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/autoclass_tutorial.md#_snippet_5

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained(
...     "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
... )
```

----------------------------------------

TITLE: Preprocessing Audio Data with AutoFeatureExtractor
DESCRIPTION: This snippet demonstrates how to use `AutoFeatureExtractor` to preprocess audio data for a specific model. It loads a feature extractor from a pretrained model, processes an audio sample from a dataset, and prints the processed sample.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/feature_extractors.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
```py
from transformers import AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
processed_sample = feature_extractor(dataset[0]["audio"]["array"], sampling_rate=16000)
processed_sample
{'input_values': [array([ 9.4472744e-05,  3.0777880e-03, -2.8888427e-03, ...,\n       -2.8888427e-03,  9.4472744e-05,  9.4472744e-05], dtype=float32)]}
```
```

----------------------------------------

TITLE: Logging into Hugging Face Hub in Python
DESCRIPTION: Logs into the Hugging Face Hub to enable model uploading and sharing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/summarization.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Loading Models with Specific Data Types in PyTorch
DESCRIPTION: Shows how to load a model with a specific data type (e.g. float16) or automatically determine the optimal data type to minimize memory usage when loading pre-trained models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/model.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
model = T5ForConditionalGeneration.from_pretrained("t5", torch_dtype=torch.float16)
```

LANGUAGE: python
CODE:
```
model = T5ForConditionalGeneration.from_pretrained("t5", torch_dtype="auto")
```

LANGUAGE: python
CODE:
```
config = T5Config.from_pretrained("t5")
model = AutoModel.from_config(config)
```

----------------------------------------

TITLE: Resume Training from Latest Checkpoint
DESCRIPTION: Resumes training from the latest checkpoint saved in the specified output directory. This allows for continuing training from where it was interrupted.  Note that `overwrite_output_dir` should be removed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/run_scripts.md#_snippet_15

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --output_dir previous_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Translating Text with AutoModel
DESCRIPTION: Demonstrates translation using the AutoModel class with bfloat16 precision and SDPA attention implementation. Shows direct model and tokenizer usage for more control over the translation process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mbart.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

article_en = "UN Chief Says There Is No Military Solution in Syria"

model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-50-many-to-many-mmt", torch_dtype=torch.bfloat16, attn_implementation="sdpa", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")

tokenizer.src_lang = "en_XX"
encoded_hi = tokenizer(article_en, return_tensors="pt").to("cuda")
generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.lang_code_to_id["fr_XX"], cache_implementation="static")
print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))
```

----------------------------------------

TITLE: TensorFlow Manual Inference with Masked Language Model
DESCRIPTION: Performs manual inference with a fine-tuned masked language model in TensorFlow by tokenizing text and identifying the mask token position.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/masked_language_modeling.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_eli5_mlm_model")
inputs = tokenizer(text, return_tensors="tf")
mask_token_index = tf.where(inputs["input_ids"] == tokenizer.mask_token_id)[0, 1]
```

----------------------------------------

TITLE: DP vs DDP ë²¤ì¹˜ë§ˆí¬ ë¹„êµ (Bash)
DESCRIPTION: DataParallel(DP)ê³¼ DistributedDataParallel(DDP)ì˜ ì„±ëŠ¥ì„ NVLink ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë¹„êµí•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ ì½”ë“œ. ëª…ë ¹ì–´ëŠ” GPT-2 ëª¨ë¸ì„ ìœ„í‚¤í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ í›ˆë ¨í•˜ë©° ì‹¤í–‰ ì‹œê°„ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/perf_train_gpu_many.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
# DP
rm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 \
python examples/pytorch/language-modeling/run_clm.py \
--model_name_or_path openai-community/gpt2 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 \
--do_train --output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{'train_runtime': 110.5948, 'train_samples_per_second': 1.808, 'epoch': 0.69}

# DDP w/ NVlink
rm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 \
torchrun --nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py \
--model_name_or_path openai-community/gpt2 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 \
--do_train --output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{'train_runtime': 101.9003, 'train_samples_per_second': 1.963, 'epoch': 0.69}

# DDP w/o NVlink
rm -r /tmp/test-clm; NCCL_P2P_DISABLE=1 CUDA_VISIBLE_DEVICES=0,1 \
torchrun --nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py \
--model_name_or_path openai-community/gpt2 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 \
--do_train --output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{'train_runtime': 131.4367, 'train_samples_per_second': 1.522, 'epoch': 0.69}
```

----------------------------------------

TITLE: Preparing Image Input and Predicting Depth
DESCRIPTION: This snippet details the steps to prepare the input image for the model, including resizing and normalization, followed by obtaining depth predictions from the model's outputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/monocular_depth_estimation.md#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
>>> pixel_values = image_processor(image, return_tensors="pt").pixel_values.to(device)
>>> import torch
>>> with torch.no_grad():
...     outputs = model(pixel_values)
```

----------------------------------------

TITLE: Creando un pipeline de reconocimiento de voz en espaÃ±ol con CUDA
DESCRIPTION: Configura un pipeline para reconocimiento automÃ¡tico de voz en espaÃ±ol usando un modelo wav2vec2 y aceleraciÃ³n GPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/quicktour.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

reconocedor_de_voz = pipeline(
    "automatic-speech-recognition", model="jonatasgrosman/wav2vec2-large-xlsr-53-spanish", device=0
)
```

----------------------------------------

TITLE: Automatic Speech Recognition with Whisper
DESCRIPTION: Shows how to transcribe speech to text using OpenAI's Whisper model through the Hugging Face pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/task_summary.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition", model="openai/whisper-small")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
```

----------------------------------------

TITLE: Manual Speech Recognition Inference with PyTorch
DESCRIPTION: Manually process audio, run model inference, and decode predicted tokens to generate transcription
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/asr.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
from transformers import AutoProcessor, AutoModelForCTC
import torch

processor = AutoProcessor.from_pretrained("stevhliu/my_awesome_asr_mind_model")
inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")

model = AutoModelForCTC.from_pretrained("stevhliu/my_awesome_asr_mind_model")
with torch.no_grad():
    logits = model(**inputs).logits

predicted_ids = torch.argmax(logits, dim=-1)
transcription = processor.batch_decode(predicted_ids)
```

----------------------------------------

TITLE: Applying a Chat Template with BlenderBot Model in Python
DESCRIPTION: Demonstrates how to use a chat template with the BlenderBot model to format a conversation for input to the model. The example shows how the template converts a structured chat into a single string.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/chat_templating.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer
>>> tokenizer = AutoTokenizer.from_pretrained("facebook/blenderbot-400M-distill")

>>> chat = [
...   {"role": "user", "content": "Hello, how are you?"},
...   {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
...   {"role": "user", "content": "I'd like to show off how chat templating works!"},
... ]

>>> tokenizer.apply_chat_template(chat, tokenize=False)
" Hello, how are you?  I'm doing great. How can I help you today?   I'd like to show off how chat templating works!</s>"
```

----------------------------------------

TITLE: Preparing Text for Inference
DESCRIPTION: Sets up a sample text for sentiment analysis inference to demonstrate how to use the fine-tuned model to make predictions on new text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_23

LANGUAGE: python
CODE:
```
>>> text = "This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three."
```

----------------------------------------

TITLE: Loading 4-bit Quantized Model
DESCRIPTION: Shows how to load a model in 4-bit precision for maximum memory savings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/quantization.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "bigscience/bloom-1b7"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", load_in_4bit=True)
```

----------------------------------------

TITLE: Fine-tuning BERT for Multiple Choice with TensorFlow
DESCRIPTION: Loads the BERT model, compiles it with the optimizer, and starts the fine-tuning process using TensorFlow and Keras API.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/multiple_choice.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> from transformers import TFAutoModelForMultipleChoice

>>> model = TFAutoModelForMultipleChoice.from_pretrained("google-bert/bert-base-uncased")

>>> model.compile(optimizer=optimizer)

>>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=2)
```

----------------------------------------

TITLE: Loading and Using FLAN-UL2 Model in Python
DESCRIPTION: This code snippet demonstrates how to load the FLAN-UL2 model and tokenizer, and use them to generate text based on a given prompt. It showcases loading the model in 8-bit precision and using device mapping for efficient memory usage.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/flan-ul2.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-ul2", load_in_8bit=True, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained("google/flan-ul2")

inputs = tokenizer("A step by step recipe to make bolognese pasta:", return_tensors="pt")
outputs = model.generate(**inputs)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
```

----------------------------------------

TITLE: PyTorch Inference Implementation
DESCRIPTION: Performs inference using a fine-tuned multiple choice model with PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/multiple_choice.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForMultipleChoice

tokenizer = AutoTokenizer.from_pretrained("username/my_awesome_swag_model")
inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors="pt", padding=True)
labels = torch.tensor(0).unsqueeze(0)

model = AutoModelForMultipleChoice.from_pretrained("username/my_awesome_swag_model")
outputs = model(**{k: v.unsqueeze(0) for k, v in inputs.items()}, labels=labels)
logits = outputs.logits
predicted_class = logits.argmax().item()
```

----------------------------------------

TITLE: Loading BERT Model with SDPA in Python
DESCRIPTION: This snippet demonstrates how to load a BERT model using the from_pretrained method with Scaled Dot Product Attention (SDPA) implementation and half-precision floating-point format for optimal performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/bert.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-uncased", torch_dtype=torch.float16, attn_implementation="sdpa")
...
```

----------------------------------------

TITLE: Loading a Quantized Model
DESCRIPTION: Loading a previously quantized model from the Hugging Face Hub, utilizing device_map for efficient memory usage.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/gptq.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="auto")
```

----------------------------------------

TITLE: Hyperparameter Search with Ray Tune in Python
DESCRIPTION: This snippet runs the hyperparameter_search method using Ray Tune as the backend, utilizing previously defined parameters for direction and number of trials. It optimizes the specified hyperparameters through trial runs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/hpo_train.md#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
best_trials = trainer.hyperparameter_search( 
    direction=["minimize", "maximize"],
    backend="ray",
    hp_space=ray_hp_space,
    n_trials=20,
    compute_objective=compute_objective,
)
```

----------------------------------------

TITLE: Loading Word Error Rate Metric
DESCRIPTION: This snippet loads the 'wer' evaluation metric from the Evaluate library, which is commonly used for assessing the performance of ASR models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/asr.md#2025-04-22_snippet_12

LANGUAGE: Python
CODE:
```
>>> import evaluate

>>> wer = evaluate.load("wer")
```

----------------------------------------

TITLE: Loading SacreBLEU Metric for Translation Evaluation
DESCRIPTION: Imports the SacreBLEU metric from the Evaluate library, which is commonly used to assess the quality of machine translations by comparing them with reference translations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/translation.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> import evaluate

>>> metric = evaluate.load("sacrebleu")
```

----------------------------------------

TITLE: Configuring ZeRO-3 with Auto Settings in JSON
DESCRIPTION: A complete ZeRO-3 configuration file with auto settings for DeepSpeed optimization. This configuration automatically detects and sets optimal values for many parameters based on your training environment.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/deepspeed.md#2025-04-22_snippet_22

LANGUAGE: json
CODE:
```
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": "auto",
            "betas": "auto",
            "eps": "auto",
            "weight_decay": "auto"
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },

    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    },

    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 2000,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false
}
```

----------------------------------------

TITLE: Running Summarization Script with Custom Dataset
DESCRIPTION: This command demonstrates how to run a summarization script with a custom dataset, specifying the paths to training and validation files, and the column names for input text and summaries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/run_scripts.md#2025-04-22_snippet_10

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --train_file path_to_csv_or_jsonlines_file \
    --validation_file path_to_csv_or_jsonlines_file \
    --text_column text_column_name \
    --summary_column summary_column_name \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --overwrite_output_dir \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate
```

----------------------------------------

TITLE: Grouping Tokenized Texts into Fixed-Length Blocks
DESCRIPTION: Creates a function to group tokenized texts into blocks of a fixed size (128 tokens), which helps manage sequence length for the model and ensures efficient training by avoiding sequences that are too long or short.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/language_modeling.md#2025-04-23_snippet_8

LANGUAGE: python
CODE:
```
>>> block_size = 128

>>> def group_texts(examples):
...     # Ø±Ø¨Ø· Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ.
...     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
...     total_length = len(concatenated_examples[list(examples.keys())[0]])
...     # Ù†ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¨Ø§Ù‚ÙŠ Ø§Ù„ØµØºÙŠØ±ØŒ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø­Ø´Ùˆ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ¯Ø¹Ù…Ù‡ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¥Ø³Ù‚Ø§Ø·ØŒ ÙŠÙ…ÙƒÙ†Ùƒ
...     # ØªØ®ØµÙŠØµ Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡ Ø­Ø³Ø¨ Ø§Ø­ØªÙŠØ§Ø¬Ø§ØªÙƒ.
...     if total_length >= block_size:
...         total_length = (total_length // block_size) * block_size
...     # Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ø¨Ø­Ø¬Ù… block_size.
...     result = {
...         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
...         for k, t in concatenated_examples.items()
...     }
...     result["labels"] = result["input_ids"].copy()
...     return result
```

----------------------------------------

TITLE: Loading and Splitting MInDS-14 Dataset
DESCRIPTION: Code to load the MInDS-14 dataset and split it into training and test sets
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/asr.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset, Audio

>>> minds = load_dataset("PolyAI/minds14", name="en-US", split="train[:100]")
>>> minds = minds.train_test_split(test_size=0.2)
```

----------------------------------------

TITLE: SuperGlue Image Matching with Transformers
DESCRIPTION: This code snippet demonstrates how to use the SuperGlue model from the `transformers` library to perform image matching. It loads two images, preprocesses them using `AutoImageProcessor`, and performs inference using `AutoModel`. The output contains keypoints and matching scores between the two images.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/superglue.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import AutoImageProcessor, AutoModel
import torch
from PIL import Image
import requests

url_image1 = "https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg"
image1 = Image.open(requests.get(url_image1, stream=True).raw)
url_image2 = "https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg"
image_2 = Image.open(requests.get(url_image2, stream=True).raw)

images = [image1, image2]

processor = AutoImageProcessor.from_pretrained("magic-leap-community/superglue_outdoor")
model = AutoModel.from_pretrained("magic-leap-community/superglue_outdoor")

inputs = processor(images, return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs)
```

----------------------------------------

TITLE: Loading a Tokenizer Object in Python
DESCRIPTION: This snippet demonstrates how to load a previously saved tokenizer object to be used in the Transformers library, demonstrating the reusability of trained tokenizers.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/fast_tokenizers.md#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
from transformers import PreTrainedTokenizerFast

fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)
```

LANGUAGE: Python
CODE:
```
fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file="tokenizer.json")
```

----------------------------------------

TITLE: Contrastive Search Decoding
DESCRIPTION: This snippet demonstrates contrastive search decoding, which aims to generate coherent and non-repetitive text. It involves the `penalty_alpha` and `top_k` parameters.  `penalty_alpha` controls the penalty for repeating tokens, and `top_k` restricts the search space to the top-k most likely tokens. This snippet uses the "openai-community/gpt2-large" model as an example.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/generation_strategies.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, AutoModelForCausalLM

>>> checkpoint = "openai-community/gpt2-large"
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)

>>> prompt = "Hugging Face Company is"
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> outputs = model.generate(**inputs, penalty_alpha=0.6, top_k=4, max_new_tokens=100)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Hugging Face Company is a family owned and operated business. We pride ourselves on being the best\nin the business and our customer service is second to none.\n\nIf you have any questions about our\nproducts or services, feel free to contact us at any time. We look forward to hearing from you!']
```

----------------------------------------

TITLE: Using Text Generation Pipeline for Chat in Python
DESCRIPTION: Example of using the text generation pipeline for conversational AI with the Zephyr model. The pipeline handles the application of chat templates automatically, making it easier to use chat models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/chat_templating.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import pipeline

pipe = pipeline("conversational", "HuggingFaceH4/zephyr-7b-beta")
messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a pirate",
    },
    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
]
print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])  # Print the assistant's response
```

----------------------------------------

TITLE: Installing Required Libraries for OWL-ViT
DESCRIPTION: Command to install the Transformers library which contains the implementation of OWL-ViT for zero-shot object detection.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/zero_shot_object_detection.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install -q transformers
```

----------------------------------------

TITLE: Enabling SDPA in Transformers with AutoModelForSeq2SeqLM
DESCRIPTION: This code snippet demonstrates how to load a pre-trained model using `AutoModelForSeq2SeqLM` from the Transformers library and explicitly enable Scaled Dot Product Attention (SDPA) via the `attn_implementation` parameter. It also loads the model in half-precision (`torch.float16`) for performance optimization. The `from_pretrained` method downloads and caches the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/nllb.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSeq2SeqLM
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/nllb-200-distilled-600M", torch_dtype=torch.float16, attn_implementation="sdpa")
...
```

----------------------------------------

TITLE: Training Configuration Setup
DESCRIPTION: Configuration of training arguments including learning rate, batch size, and evaluation settings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/image_captioning.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
training_args = TrainingArguments(
    output_dir=f"{model_name}-pokemon",
    learning_rate=5e-5,
    num_train_epochs=50,
    fp16=True,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=2,
    save_total_limit=3,
    eval_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=50,
    logging_steps=50,
    remove_unused_columns=False,
    push_to_hub=True,
    label_names=["labels"],
    load_best_model_at_end=True,
)
```

----------------------------------------

TITLE: Model Training Configuration
DESCRIPTION: Setting up training arguments and initializing the Trainer for model fine-tuning.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/audio_classification.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
training_args = TrainingArguments(
    output_dir="my_awesome_mind_model",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=3e-5,
    per_device_train_batch_size=32,
    gradient_accumulation_steps=4,
    per_device_eval_batch_size=32,
    num_train_epochs=10,
    warmup_ratio=0.1,
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_minds["train"],
    eval_dataset=encoded_minds["test"],
    processing_class=feature_extractor,
    compute_metrics=compute_metrics,
)
```

----------------------------------------

TITLE: Vision Pipeline Classification
DESCRIPTION: Example showing how to use pipeline for image classification tasks
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/pipeline_tutorial.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import pipeline

vision_classifier = pipeline(model="google/vit-base-patch16-224")
preds = vision_classifier(
    images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
)
```

----------------------------------------

TITLE: Resuming Training from Checkpoints
DESCRIPTION: Examples showing how to resume training from the latest checkpoint or a specific checkpoint directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/trainer.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
# resume from latest checkpoint
trainer.train(resume_from_checkpoint=True)

# resume from specific checkpoint saved in output directory
trainer.train(resume_from_checkpoint="your-model/checkpoint-1000")
```

----------------------------------------

TITLE: Loading Custom Model from Hub
DESCRIPTION: Loads a custom model from Hub with remote code execution enabled
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/custom_models.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
from transformers import AutoModelForImageClassification

model = AutoModelForImageClassification.from_pretrained("sgugger/custom-resnet50d", trust_remote_code=True)
```

----------------------------------------

TITLE: Generating Text with Gemma using DynamicCache (PyTorch)
DESCRIPTION: Illustrates how to perform text generation using transformers with the DynamicCache object to manage past key-value states. This is the default caching behavior for many autoregressive models. It loads the model and tokenizer and passes an initialized DynamicCache to the `generate` method. Requires the `transformers` and `torch` libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gemma.md#_snippet_5

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache

tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b")
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2b",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    attn_implementation="sdpa"
)
input_text = "LLMs generate text through a process known as"
input_ids = tokenizer(input_text, return_tensors="pt").to("cuda")
past_key_values = DynamicCache()
outputs = model.generate(**input_ids, max_new_tokens=50, past_key_values=past_key_values)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Specifying a Built-in Optimizer
DESCRIPTION: This snippet illustrates how to select a built-in optimizer for training using the `optim` argument in `TrainingArguments`. It allows choosing from a predefined list of optimizers.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/trainer.md#_snippet_13

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments
training_args = TrainingArguments(..., optim="adamw_torch")
```

----------------------------------------

TITLE: Loading SUPERB Audio Dataset for Keyword Spotting
DESCRIPTION: Loads the keyword spotting task from the SUPERB benchmark using the Hugging Face Datasets library. This provides audio data to experiment with feature extraction.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/preprocessing.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("superb", "ks")
```

----------------------------------------

TITLE: Loading a Dataset using Hugging Face Datasets
DESCRIPTION: This snippet demonstrates how to load the food101 dataset using the Hugging Face Datasets library. It uses the `load_dataset` function to load a small sample of the training split.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_19

LANGUAGE: Python
CODE:
```
>>> from datasets import load_dataset

>>> dataset = load_dataset("food101", split="train[:100]")
```

----------------------------------------

TITLE: Converting TensorFlow Model to PyTorch in Python
DESCRIPTION: Shows how to convert a TensorFlow model checkpoint to a PyTorch checkpoint using the from_tf parameter.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/model_sharing.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> pt_model = DistilBertForSequenceClassification.from_pretrained("path/to/awesome-name-you-picked", from_tf=True)
>>> pt_model.save_pretrained("path/to/awesome-name-you-picked")
```

----------------------------------------

TITLE: Using Pipeline for Text Summarization
DESCRIPTION: Shows how to use the Hugging Face pipeline API for quick summarization inference with a finetuned model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_24

LANGUAGE: python
CODE:
```
from transformers import pipeline

summarizer = pipeline("summarization", model="username/my_awesome_billsum_model")
summarizer(text)
```

----------------------------------------

TITLE: Setting Batch Size for Pipeline in Python
DESCRIPTION: Demonstrates how to set a batch size for pipeline inference to potentially improve performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/pipeline_tutorial.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
transcriber = pipeline(model="openai/whisper-large-v2", device=0, batch_size=2)
audio_filenames = [f"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/{i}.flac" for i in range(1, 5)]
texts = transcriber(audio_filenames)
```

----------------------------------------

TITLE: Perform Masked Language Modeling with Pipeline Python
DESCRIPTION: This snippet demonstrates how to use the high-level Hugging Face `pipeline` API for masked language modeling with the ModernBERT model. It initializes a 'fill-mask' pipeline, specifying the model, data type, and device, and then processes an input sentence containing a `[MASK]` token.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/modernbert.md#_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipeline = pipeline(
    task="fill-mask",
    model="answerdotai/ModernBERT-base",
    torch_dtype=torch.float16,
    device=0
)
pipeline("Plants create [MASK] through a process known as photosynthesis.")
```

----------------------------------------

TITLE: Loading Pre-trained Model for Token Classification with TFAutoModelForTokenClassification in Python
DESCRIPTION: This example demonstrates how to load a TensorFlow pre-trained model for token classification using TFAutoModelForTokenClassification, allowing seamless reuse of the same checkpoint for different tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/autoclass_tutorial.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers import TFAutoModelForTokenClassification

>>> model = TFAutoModelForTokenClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Loading Pre-trained TF Model for Token Classification with AutoModel in Python
DESCRIPTION: This code snippet demonstrates how to load a pre-trained TensorFlow model for token classification using `TFAutoModelForTokenClassification.from_pretrained` from the `transformers` library. It utilizes the same 'distilbert/distilbert-base-uncased' checkpoint but loads it for a different task, token classification, within a TensorFlow environment.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/autoclass_tutorial.md#_snippet_7

LANGUAGE: Python
CODE:
```
>>> from transformers import TFAutoModelForTokenClassification

>>> model = TFAutoModelForTokenClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Loading DistilRoBERTa with AutoModelForMaskedLM (PyTorch)
DESCRIPTION: Code to initialize a DistilRoBERTa model for masked language modeling tasks using the AutoModelForMaskedLM class in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
from transformers import AutoModelForMaskedLM

model = AutoModelForMaskedLM.from_pretrained("distilbert/distilroberta-base")
```

----------------------------------------

TITLE: Masked Language Model Inference with Pipeline (PyTorch/TensorFlow)
DESCRIPTION: Demonstrates using a pipeline for masked language model inference, filling masked tokens in a given text
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/masked_language_modeling.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> mask_filler = pipeline("fill-mask", "username/my_awesome_eli5_mlm_model")
>>> mask_filler(text, top_k=3)
```

----------------------------------------

TITLE: Loading DistilBERT model for sequence classification (TensorFlow)
DESCRIPTION: This snippet demonstrates how to load a pre-trained DistilBERT model for sequence classification using `TFAutoModelForSequenceClassification`. It also sets the number of labels and label mappings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
>>> from transformers import TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained(
...     "distilbert/distilbert-base-uncased", num_labels=2, id2label=id2label, label2id=label2id
... )
```

----------------------------------------

TITLE: Creating a Data Collator with Dynamic Padding for PyTorch
DESCRIPTION: PyTorch implementation of a data collator that handles dynamic padding, which pads sequences to the maximum length in each batch rather than the maximum possible length in the dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/sequence_classification.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorWithPadding

>>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

----------------------------------------

TITLE: Basic Trainer Configuration
DESCRIPTION: Example of configuring training arguments for the Trainer class with common parameters like learning rate, batch size, and training epochs
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/trainer.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="your-model",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=True,
)
```

----------------------------------------

TITLE: Loading and Using Exported ONNX Model
DESCRIPTION: Python code demonstrating how to load and use an exported ONNX model using ONNX Runtime
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/serialization.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import ORTModelForQuestionAnswering

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> model = ORTModelForQuestionAnswering.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> inputs = tokenizer("What am I using?", "Using DistilBERT with ONNX Runtime!", return_tensors="pt")
>>> outputs = model(**inputs)
```

----------------------------------------

TITLE: Loading a Pushed Model from the Hub
DESCRIPTION: Shows how to load a model that has been pushed to the Hugging Face Hub using the from_pretrained function.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_sharing.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
from transformers import AutoModel

model = AutoModel.from_pretrained("your_username/my-awesome-model")
```

----------------------------------------

TITLE: Saving a Transformers model without sharding in Python
DESCRIPTION: Shows how to save a Transformers model to a directory without sharding the checkpoint, resulting in a config file and single weights file.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/big_models.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> import os
>>> import tempfile

>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir)
...     print(sorted(os.listdir(tmp_dir)))
['config.json', 'pytorch_model.bin']
```

----------------------------------------

TITLE: Creating Web Demos with Gradio and Transformers Pipeline
DESCRIPTION: This code shows how to create interactive web demos for machine learning models using Gradio and the transformers pipeline API. It creates a simple image classification demo with a drag-and-drop interface.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/pipeline_tutorial.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers import pipeline
import gradio as gr

pipe = pipeline("image-classification", model="google/vit-base-patch16-224")

gr.Interface.from_pipeline(pipe).launch()
```

----------------------------------------

TITLE: Implementing Backward Pass with Accelerate
DESCRIPTION: Training loop example showing how to replace the standard loss.backward() call with accelerator.backward() method to enable distributed training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/accelerate.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> for epoch in range(num_epochs):
...     for batch in train_dataloader:
...         outputs = model(**batch)
...         loss = outputs.loss
...         accelerator.backward(loss)

...         optimizer.step()
...         lr_scheduler.step()
...         optimizer.zero_grad()
...         progress_bar.update(1)
```

----------------------------------------

TITLE: Hyperparameter Search with Optuna in Python
DESCRIPTION: This snippet calls the hyperparameter_search method on the Trainer instance, using the previously defined search space and settings for direction and number of trials. It runs a search using Optuna to optimize the specified parameters and store the best trials.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/hpo_train.md#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
best_trials = trainer.hyperparameter_search(
    direction=["minimize", "maximize"],
    backend="optuna",
    hp_space=optuna_hp_space,
    n_trials=20,
    compute_objective=compute_objective,
)
```

----------------------------------------

TITLE: Loading the SQuAD Dataset
DESCRIPTION: This Python code snippet loads a smaller subset of the SQuAD dataset for initial experimentation. It uses the load_dataset function from the datasets library to load the training split.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/question_answering.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> squad = load_dataset("squad", split="train[:5000]")
```

----------------------------------------

TITLE: Single Image Inference with LlavaNext in Python
DESCRIPTION: This code demonstrates how to perform inference using the LlavaNext model in a PyTorch environment, utilizing half-precision computation for efficiency. It guides on loading a model and processor from pre-trained checkpoints, setting up hardware for computation, and preparing an image and text prompt for inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llava_next.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration
import torch
from PIL import Image
import requests

processor = LlavaNextProcessor.from_pretrained("llava-hf/llava-v1.6-mistral-7b-hf")

model = LlavaNextForConditionalGeneration.from_pretrained("llava-hf/llava-v1.6-mistral-7b-hf", torch_dtype=torch.float16, low_cpu_mem_usage=True)
model.to("cuda:0")

# prepare image and text prompt, using the appropriate prompt template
url = "https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true"
image = Image.open(requests.get(url, stream=True).raw)

conversation = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": "What is shown in this image?"},
        ],
    },
]
prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)
inputs = processor(image, prompt, return_tensors="pt").to("cuda:0")

```

----------------------------------------

TITLE: Creating a Data Collator for Token Classification
DESCRIPTION: Initializes a DataCollatorForTokenClassification that handles padding inputs to a consistent length within each batch. Two versions are provided for PyTorch and TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/token_classification.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorForTokenClassification

>>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```

----------------------------------------

TITLE: Inspecting Dataset Example in Python
DESCRIPTION: This code snippet showcases fetching the first entry of the dataset to inspect its features such as question, question_type, question_id, image_id, answer_type, and label. It helps in understanding the structure and content of the dataset entries to prepare it for further processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_question_answering.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
dataset[0]
```

----------------------------------------

TITLE: Hugging Face CLI Login
DESCRIPTION: Command to login to Hugging Face using the CLI tool, which saves access token to the cache folder.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_sharing.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
huggingface-cli login
```

----------------------------------------

TITLE: Using Chat Templates for Training
DESCRIPTION: Shows how to apply chat templates as a preprocessing step for training datasets, with recommendations for handling generation prompts
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/chat_templating.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer
from datasets import Dataset

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-beta")

chat1 = [
    {"role": "user", "content": "Which is bigger, the moon or the sun?"},
    {"role": "assistant", "content": "The sun."}
]
chat2 = [
    {"role": "user", "content": "Which is bigger, a virus or a bacterium?"},
    {"role": "assistant", "content": "A bacterium."}
]

dataset = Dataset.from_dict({"chat": [chat1, chat2]})
dataset = dataset.map(lambda x: {"formatted_chat": tokenizer.apply_chat_template(x["chat"], tokenize=False, add_generation_prompt=False)})
print(dataset['formatted_chat'][0])
```

----------------------------------------

TITLE: Programmatic ONNX Export with Optimum
DESCRIPTION: Python code showing how to programmatically export a model to ONNX using Optimum
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/serialization.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from optimum.onnxruntime import ORTModelForSequenceClassification
>>> from transformers import AutoTokenizer

>>> model_checkpoint = "distilbert_base_uncased_squad"
>>> save_directory = "onnx/"

>>> # Load a model from transformers and export it to ONNX
>>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)
>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

>>> # Save the onnx model and tokenizer
>>> ort_model.save_pretrained(save_directory)
>>> tokenizer.save_pretrained(save_directory)
```

----------------------------------------

TITLE: Defining Model Initialization Function for Trainer in Python
DESCRIPTION: Example of defining a model initialization function to be passed to the Trainer for hyperparameter search.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/hpo_train.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
def model_init(trial):
    return AutoModelForSequenceClassification.from_pretrained(
        model_args.model_name_or_path,
        from_tf=bool(".ckpt" in model_args.model_name_or_path),
        config=config,
        cache_dir=model_args.cache_dir,
        revision=model_args.model_revision,
        token=True if model_args.use_auth_token else None,
    )
```

----------------------------------------

TITLE: Converting NER Tags to Labels
DESCRIPTION: Converts NER tag indices to their respective label names, using the dataset's label list for reference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/token_classification.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> label_list = wnut["train"].features[f"ner_tags"].feature.names
>>> label_list
[
    "O",
    "B-corporation",
    "I-corporation",
    "B-creative-work",
    "I-creative-work",
    "B-group",
    "I-group",
    "B-location",
    "I-location",
    "B-person",
    "I-person",
    "B-product",
    "I-product",
]
```

----------------------------------------

TITLE: Loading a Processor from a Pretrained Model
DESCRIPTION: This snippet shows how to load a processor using `AutoProcessor.from_pretrained`. It loads the processor associated with the 'facebook/wav2vec2-base-960h' pretrained model, which handles both audio and text processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_29

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")
```

----------------------------------------

TITLE: Distributed Training with Mixed Precision
DESCRIPTION: Example command showing how to enable distributed training and mixed precision using torchrun with 8 GPUs. Includes fp16 flag for mixed precision training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/run_scripts.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
torchrun \
    --nproc_per_node 8 pytorch/summarization/run_summarization.py \
    --fp16 \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Loading Food-101 Dataset with Python
DESCRIPTION: Loads a subset of the Food-101 dataset for image classification tasks using the Hugging Face Datasets library. The dataset is split into training and testing sets for experimentation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_classification.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> food = load_dataset("food101", split="train[:5000]")
```

LANGUAGE: python
CODE:
```
>>> food = food.train_test_split(test_size=0.2)
```

----------------------------------------

TITLE: Loading Pre-trained Feature Extractor with AutoFeatureExtractor in Python
DESCRIPTION: This example illustrates how to load a feature extractor for audio tasks using the AutoFeatureExtractor class. This extractor prepares audio signals in the right format for processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/autoclass_tutorial.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained(
...     "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
... )
```

----------------------------------------

TITLE: Splitting Dataset into Train and Test Sets
DESCRIPTION: Splits the Pokemon dataset into training and testing subsets with 90% for training and 10% for testing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_captioning.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
ds = ds["train"].train_test_split(test_size=0.1)
train_ds = ds["train"]
test_ds = ds["test"]
```

----------------------------------------

TITLE: Using ONNX Runtime for Question Answering
DESCRIPTION: This Python code snippet demonstrates how to load and use an ONNX model exported using Optimum with ONNX Runtime.  It loads the tokenizer and model, prepares input, and performs question answering using the ONNX model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/serialization.md#_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import ORTModelForQuestionAnswering

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> model = ORTModelForQuestionAnswering.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> inputs = tokenizer("What am I using?", "Using DistilBERT with ONNX Runtime!", return_tensors="pt")
>>> outputs = model(**inputs)
```

----------------------------------------

TITLE: Segmenting Images with Transformers in Python
DESCRIPTION: This code snippet showcases how to perform image segmentation using the ðŸ¤— Transformers library. It initializes an image segmenter pipeline and processes an image from a URL, returning the segmentation results with scores and labels for each detected class at a pixel level.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/task_summary.md#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
>>> from transformers import pipeline

>>> segmenter = pipeline(task="image-segmentation")
>>> preds = segmenter(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> print(*preds, sep="\n")
{'score': 0.9879, 'label': 'LABEL_184'}
{'score': 0.9973, 'label': 'snow'}
{'score': 0.9972, 'label': 'cat'}
```

----------------------------------------

TITLE: Aplicando pipeline de reconhecimento de fala em amostras de Ã¡udio em Python
DESCRIPTION: Demonstra como aplicar o pipeline de reconhecimento de fala em um conjunto de amostras de Ã¡udio.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/quicktour.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> result = speech_recognizer(dataset[:4]["audio"])
>>> print([d["text"] for d in result])
['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', "FONDERING HOW I'D SET UP A JOIN TO HET WITH MY WIFE AND WHERE THE AP MIGHT BE", "I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AND I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS", 'HOW DO I TURN A JOIN A COUNT']
```

----------------------------------------

TITLE: Using Transformers Pipeline for Audio Classification Inference
DESCRIPTION: Shows how to use the Transformers pipeline for audio classification, which simplifies the inference process by handling preprocessing and model loading automatically.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/audio_classification.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline("audio-classification", model="stevhliu/my_awesome_minds_model")
>>> classifier(audio_file)
[
    {'score': 0.09766869246959686, 'label': 'cash_deposit'},
    {'score': 0.07998877018690109, 'label': 'app_error'},
    {'score': 0.0781070664525032, 'label': 'joint_account'},
    {'score': 0.07667109370231628, 'label': 'pay_bill'},
    {'score': 0.0755252093076706, 'label': 'balance'}
]
```

----------------------------------------

TITLE: Setting Up Model for Sequence Classification
DESCRIPTION: Initializes a pre-trained BERT model for sequence classification with the appropriate number of labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/training.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=5)
```

----------------------------------------

TITLE: Compiling a Quantized Model with torch.compile
DESCRIPTION: This snippet shows how to wrap a quantized model with `torch.compile` to potentially improve inference speed. It initializes the model with a QuantoConfig for int8 quantization and then compiles it using `torch.compile`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/quanto.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
"import torch\nfrom transformers import AutoModelForSpeechSeq2Seq, QuantoConfig\n\nquant_config = QuantoConfig(weights=\"int8\")\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n  \"openai/whisper-large-v2\",\n  torch_dtype=\"auto\",\n  device_map=\"auto\",\n  quantization_config=quant_config\n)\n\nmodel = torch.compile(model)"
```

----------------------------------------

TITLE: DeepSpeed ZeRO Stage 2 configuration
DESCRIPTION: JSON configuration for DeepSpeed ZeRO Stage 2 with CPU offload and other optimizations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/deepspeed.md#2025-04-22_snippet_10

LANGUAGE: json
CODE:
```
{
  "zero_optimization": {
     "stage": 2,
     "offload_optimizer": {
         "device": "cpu",
         "pin_memory": true
     },
     "allgather_partitions": true,
     "allgather_bucket_size": 2e8,
     "reduce_scatter": true,
     "reduce_bucket_size": 2e8,
     "overlap_comm": true,
     "contiguous_gradients": true
  }
}
```

----------------------------------------

TITLE: Custom Data Collator for CTC Loss in ASR
DESCRIPTION: Python class implementing a custom data collator for CTC (Connectionist Temporal Classification) loss, used in ASR tasks. This collator handles padding of input values and labels differently, and masks padded label tokens for proper loss calculation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/asr.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> import torch

>>> from dataclasses import dataclass, field
>>> from typing import Any, Dict, List, Optional, Union


>>> @dataclass
... class DataCollatorCTCWithPadding:
...     processor: AutoProcessor
...     padding: Union[bool, str] = "longest"

...     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
...         # particiona las entradas y las etiquetas ya que tienen que tener longitudes distintas y
...         # requieren mÃ©todos de padding diferentes
...         input_features = [{"input_values": feature["input_values"][0]} for feature in features]
...         label_features = [{"input_ids": feature["labels"]} for feature in features]

...         batch = self.processor.pad(input_features, padding=self.padding, return_tensors="pt")

...         labels_batch = self.processor.pad(labels=label_features, padding=self.padding, return_tensors="pt")

...         # remplaza el padding con -100 para ignorar la pÃ©rdida de forma correcta
...         labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

...         batch["labels"] = labels

...         return batch

>>> data_collator = DataCollatorCTCWithPadding(processor=processor, padding="longest")
```

----------------------------------------

TITLE: Loading PVTv2 Model for Image Classification in PyTorch
DESCRIPTION: This snippet demonstrates how to load a pretrained PVTv2 model for image classification using the Hugging Face Transformers library in Python. It uses the AutoModelForImageClassification to load the model and AutoImageProcessor for preprocessing the image. The snippet requires the 'requests', 'torch', and 'PIL' packages.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/pvt_v2.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
import requests
import torch

from transformers import AutoModelForImageClassification, AutoImageProcessor
from PIL import Image

model = AutoModelForImageClassification.from_pretrained("OpenGVLab/pvt_v2_b0")
image_processor = AutoImageProcessor.from_pretrained("OpenGVLab/pvt_v2_b0")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
processed = image_processor(image)
outputs = model(torch.tensor(processed["pixel_values"]))
```

----------------------------------------

TITLE: Implementing ResNet Image Classification Model
DESCRIPTION: Creates a custom ResNet model class for image classification that includes loss calculation when labels are provided. Compatible with the Transformers Trainer class.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/custom_models.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch

class ResnetModelForImageClassification(PreTrainedModel):
    config_class = ResnetConfig

    def __init__(self, config):
        super().__init__(config)
        block_layer = BLOCK_MAPPING[config.block_type]
        self.model = ResNet(
            block_layer,
            config.layers,
            num_classes=config.num_classes,
            in_chans=config.input_channels,
            cardinality=config.cardinality,
            base_width=config.base_width,
            stem_width=config.stem_width,
            stem_type=config.stem_type,
            avg_down=config.avg_down,
        )

    def forward(self, tensor, labels=None):
        logits = self.model(tensor)
        if labels is not None:
            loss = torch.nn.functional.cross_entropy(logits, labels)
            return {"loss": loss, "logits": logits}
        return {"logits": logits}
```

----------------------------------------

TITLE: Tokenizer Initialization and Data Preprocessing
DESCRIPTION: Initialize DistilRoBERTa tokenizer and preprocess the dataset by flattening nested structures
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/masked_language_modeling.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilroberta-base")
>>> eli5 = eli5.flatten()
```

----------------------------------------

TITLE: Initializing DETR with Pre-trained Weights
DESCRIPTION: Initialize a DETR model with pre-trained weights for object detection using the from_pretrained method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/detr.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import DetrForObjectDetection

model = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50")
```

----------------------------------------

TITLE: Processing Images with Image Processor - Python
DESCRIPTION: This code snippet shows how to convert an image from a URL into pixel values using a pretrained image processor and return the data as PyTorch tensors. It uses the image from the specified URL, processes it, and converts it to RGB format before passing it to the processor.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/image_processors.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
from PIL import Image
import requests

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/image_processor_example.png"
image = Image.open(requests.get(url, stream=True).raw).convert("RGB")
inputs = image_processor(image, return_tensors="pt")
```

----------------------------------------

TITLE: Tokenizing and Truncating to Specific Length
DESCRIPTION: This snippet demonstrates truncating a batch of sentences to a specific length, indicated by the `max_length` argument. `truncation=True` enables the truncation feature.  `truncation=STRATEGY` can be used to specify truncation strategy for sequence pairs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pad_truncation.md#_snippet_7

LANGUAGE: Python
CODE:
```
tokenizer(batch_sentences, truncation=True, max_length=42)
```

LANGUAGE: Python
CODE:
```
tokenizer(batch_sentences, truncation=STRATEGY, max_length=42)
```

----------------------------------------

TITLE: Loading Image Processor for TensorFlow Inference
DESCRIPTION: This snippet demonstrates how to create an image processor for preparing images before feeding them into a TensorFlow model. This processor returns TensorFlow tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_28

LANGUAGE: python
CODE:
```
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("MariaK/scene_segmentation")
>>> inputs = image_processor(image, return_tensors="tf")
```

----------------------------------------

TITLE: Applying Audio Preprocessing to Batch
DESCRIPTION: Applies the preprocessing function to the first five examples in the dataset to create a batch of processed inputs ready for the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/preprocessing.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
>>> processed_dataset = preprocess_function(dataset["train"][:5])
```

----------------------------------------

TITLE: Logging into Hugging Face Hub with notebook_login
DESCRIPTION: This code snippet uses the `notebook_login` function from the `huggingface_hub` library to log into your Hugging Face account. This is required to upload your fine-tuned model to the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/document_question_answering.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Pushing a Quantized Model to the Hub
DESCRIPTION: This code snippet pushes a quantized model and tokenizer to the Hugging Face Hub. It requires that the model and tokenizer have already been quantized and that you are authenticated with the Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/gptq.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
"quantized_model.push_to_hub(\"opt-125m-gptq\")\ntokenizer.push_to_hub(\"opt-125m-gptq\")"
```

----------------------------------------

TITLE: Loading Converted Fast Tokenizer
DESCRIPTION: Demonstrates how to load the converted tokenizer using PreTrainedTokenizerFast from a local directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/tiktoken.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
tokenizer = PreTrainedTokenizerFast.from_pretrained("config/save/dir")
```

----------------------------------------

TITLE: Logging into Hugging Face using Python
DESCRIPTION: This snippet demonstrates how to log into your Hugging Face account to upload and share models. It utilizes the 'notebook_login' function from the 'huggingface_hub' library, which prompts the user to enter their token for authentication.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Initializing the Knowledge Distillation Trainer
DESCRIPTION: Sets up the custom ImageDistilTrainer with the teacher and student models, training arguments, datasets, and hyperparameters for knowledge distillation like temperature and lambda parameter.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/knowledge_distillation_for_image_classification.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from transformers import DefaultDataCollator

data_collator = DefaultDataCollator()
trainer = ImageDistilTrainer(
    student_model=student_model,
    teacher_model=teacher_model,
    training_args=training_args,
    train_dataset=processed_datasets["train"],
    eval_dataset=processed_datasets["validation"],
    data_collator=data_collator,
    tokenizer=teacher_processor,
    compute_metrics=compute_metrics,
    temperature=5,
    lambda_param=0.5
)
```

----------------------------------------

TITLE: Initializing Image Feature Extraction Pipeline with Transformers
DESCRIPTION: Code to set up the image feature extraction pipeline using a Vision Transformer (ViT) model. The pipeline is configured to run on GPU if available and pool the output embeddings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_feature_extraction.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
pipe = pipeline(task="image-feature-extraction", model_name="google/vit-base-patch16-384", device=DEVICE, pool=True)
```

----------------------------------------

TITLE: Installing Transformers and Datasets
DESCRIPTION: This command installs the latest versions of the Transformers library and the Datasets library using pip. These libraries are essential for working with pre-trained models and datasets from Hugging Face.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
"!pip install --U transformers datasets"
```

----------------------------------------

TITLE: Importing Libraries and Setting Up Model - PyTorch - Python
DESCRIPTION: This snippet demonstrates how to import necessary libraries, set a random seed, and initialize the PhiMoE model along with its tokenizer. Dependencies include 'torch' and 'transformers'. The model is set to run on a CUDA device with automatic datatype handling.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/phimoe.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline 

torch.random.manual_seed(0) 

model = AutoModelForCausalLM.from_pretrained( 
    "microsoft/Phi-3.5-MoE-instruct",  
    device_map="cuda",  
    torch_dtype="auto",  
    trust_remote_code=True,  
) 

tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3.5-MoE-instruct") 

messages = [ 
    {"role": "system", "content": "You are a helpful AI assistant."}, 
    {"role": "user", "content": "Can you provide ways to eat combinations of bananas and dragonfruits?"}, 
    {"role": "assistant", "content": "Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey."}, 
    {"role": "user", "content": "What about solving an 2x + 3 = 7 equation?"}, 
] 

pipe = pipeline( 
    "text-generation", 
    model=model, 
    tokenizer=tokenizer, 
) 

generation_args = { 
    "max_new_tokens": 500, 
    "return_full_text": False, 
    "temperature": 0.0, 
    "do_sample": False, 
} 

output = pipe(messages, **generation_args) 
print(output[0]['generated_text'])
```

----------------------------------------

TITLE: TensorFlow Training Setup
DESCRIPTION: Configures and executes model training using TensorFlow and Keras APIs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/tasks/sequence_classification.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> tf_train_set = tokenized_imdb["train"].to_tf_dataset(
...     columns=["attention_mask", "input_ids", "label"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> from transformers import create_optimizer
>>> import tensorflow as tf

>>> batch_size = 16
>>> num_epochs = 5
>>> batches_per_epoch = len(tokenized_imdb["train"]) // batch_size
>>> total_train_steps = int(batches_per_epoch * num_epochs)
>>> optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)

>>> from transformers import TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased", num_labels=2)

>>> model.compile(optimizer=optimizer)

>>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3)
```

----------------------------------------

TITLE: Defining Hyperparameters and Optimizer with TensorFlow
DESCRIPTION: This code outlines the steps to define training hyperparameters and create an optimizer in TensorFlow for fine-tuning a model using the Hugging Face Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/sequence_classification.md#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
>>> from transformers import create_optimizer

>>> batch_size = 2
>>> num_epochs = 50
>>> num_train_steps = len(train_ds) * num_epochs
>>> learning_rate = 6e-5
>>> weight_decay_rate = 0.01

>>> optimizer, lr_schedule = create_optimizer(
...     init_lr=learning_rate,
...     num_train_steps=num_train_steps,
...     weight_decay_rate=weight_decay_rate,
...     num_warmup_steps=0,
... )
```

----------------------------------------

TITLE: Running summarization script with TensorFlow and T5
DESCRIPTION: This snippet runs the `run_summarization.py` script from the TensorFlow examples directory. It fine-tunes the `google-t5/t5-small` model on the `cnn_dailymail` dataset for summarization using TensorFlow. It specifies the model name, dataset, output directory, batch sizes, and number of training epochs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/run_scripts_fr.md#_snippet_4

LANGUAGE: bash
CODE:
```
python examples/tensorflow/summarization/run_summarization.py  \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval
```

----------------------------------------

TITLE: Processing Dataset into Language Modeling Format
DESCRIPTION: Applies the group_texts function to the tokenized dataset to create properly formatted inputs for causal language modeling, with each example being a fixed-length block of tokens.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/language_modeling.md#2025-04-23_snippet_9

LANGUAGE: python
CODE:
```
>>> lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)
```

----------------------------------------

TITLE: Evaluation Metric Setup
DESCRIPTION: Setting up SacreBLEU evaluation metric and compute_metrics function for model evaluation
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/translation.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> import evaluate
>>> import numpy as np

>>> metric = evaluate.load("sacrebleu")

>>> def postprocess_text(preds, labels):
...     preds = [pred.strip() for pred in preds]
...     labels = [[label.strip()] for label in labels]
...     return preds, labels

>>> def compute_metrics(eval_preds):
...     preds, labels = eval_preds
...     if isinstance(preds, tuple):
...         preds = preds[0]
...     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

...     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
...     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

...     decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

...     result = metric.compute(predictions=decoded_preds, references=decoded_labels)
...     result = {"bleu": result["score"]}

...     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
...     result["gen_len"] = np.mean(prediction_lens)
...     result = {k: round(v, 4) for k, v in result.items()}
...     return result
```

----------------------------------------

TITLE: Logging into Hugging Face Hub in Python
DESCRIPTION: This code allows you to log into your Hugging Face account from a Jupyter notebook, enabling you to upload and share your model with the community. A token input is required for the login process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Loading Food-101 Dataset in Python
DESCRIPTION: Loads a subset of the Food-101 dataset using the Hugging Face datasets library and splits it into train and test sets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/image_classification.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> food = load_dataset("food101", split="train[:5000]")
>>> food = food.train_test_split(test_size=0.2)
```

----------------------------------------

TITLE: Streaming Data with Batch Inference in Python
DESCRIPTION: Illustrates streaming data processing using the Bart-large model for summarization, with inputs coming from a dataset. Uses datasets library for streaming, showcasing how to handle large test datasets segment by segment.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from transformers import pipeline
from transformers.pipelines.pt_utils import KeyDataset
import datasets

ds = datasets.load_dataset("cnn_dailymail", name="3.0.0", split="test", streaming=True)
sliding_window_pipeline = pipeline(task="summarization", model="facebook/bart-large-cnn", device="cuda", batch_size=8)

for output in sliding_window_pipeline(KeyDataset(ds, "article")):
    print(output)
```

----------------------------------------

TITLE: Single Image Inference with Chameleon
DESCRIPTION: Demonstrates how to load the Chameleon model and processor, prepare an image and text prompt, and perform inference in half-precision (torch.bfloat16) using a single image. The code uses the `ChameleonProcessor` to preprocess the image and text, and the `ChameleonForConditionalGeneration` model to generate the output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/chameleon.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import ChameleonProcessor, ChameleonForConditionalGeneration
import torch
from PIL import Image
import requests

processor = ChameleonProcessor.from_pretrained("facebook/chameleon-7b")
model = ChameleonForConditionalGeneration.from_pretrained("facebook/chameleon-7b", torch_dtype=torch.bfloat16, device_map="cuda")

# prepare image and text prompt
url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)
prompt = "What do you see in this image?<image>"

inputs = processor(images=image, text=prompt, return_tensors="pt").to(model.device, dtype=torch.bfloat16)

# autoregressively complete prompt
output = model.generate(**inputs, max_new_tokens=50)
print(processor.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Text Streaming Generation Example
DESCRIPTION: Shows how to use the TextStreamer class for streaming generation output token by token.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/generation_strategies.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

tok = AutoTokenizer.from_pretrained("openai-community/gpt2")
model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")
inputs = tok(["An increasing sequence: one,"], return_tensors="pt")
streamer = TextStreamer(tok)

_ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)
```

----------------------------------------

TITLE: Loading Pre-trained Model with AutoModel - Python
DESCRIPTION: This code snippet demonstrates how to load the pre-trained DiT model using the AutoModel API. It allows users to access the model for masked image modeling without the language modeling head.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/dit.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoModel

model = AutoModel.from_pretrained("microsoft/dit-base")
```

----------------------------------------

TITLE: Generating Translated Text with SeamlessM4T-v2
DESCRIPTION: This snippet shows how to generate translated French text from both audio and text inputs using the SeamlessM4T-v2 model. It demonstrates using the 'generate_speech=False' parameter to output text instead of speech.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/seamless_m4t_v2.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
# from audio
output_tokens = model.generate(**audio_inputs, tgt_lang="fra", generate_speech=False)
translated_text_from_audio = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)

# from text
output_tokens = model.generate(**text_inputs, tgt_lang="fra", generate_speech=False)
translated_text_from_text = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)
```

----------------------------------------

TITLE: Generating Translated Speech with SeamlessM4T-v2
DESCRIPTION: This code snippet demonstrates how to generate translated Russian speech from both English text and Arabic audio inputs. It uses the same model interface for both modalities, showing the model's seamless handling of different input types.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/seamless_m4t_v2.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
audio_array_from_text = model.generate(**text_inputs, tgt_lang="rus")[0].cpu().numpy().squeeze()
audio_array_from_audio = model.generate(**audio_inputs, tgt_lang="rus")[0].cpu().numpy().squeeze()
```

----------------------------------------

TITLE: Applying Preprocessing to the Dataset
DESCRIPTION: This code applies the preprocessing function to the entire dataset using batched processing for efficiency.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/translation.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> tokenized_books = books.map(preprocess_function, batched=True)
```

----------------------------------------

TITLE: Training Object Detection Model with HuggingFace Trainer
DESCRIPTION: Example command for fine-tuning a DETR model on the CPPE-5 dataset using the HuggingFace Trainer API. Includes configurations for training parameters, evaluation settings, and model pushing to HuggingFace Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/object-detection/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python run_object_detection.py \
    --model_name_or_path facebook/detr-resnet-50 \
    --dataset_name cppe-5 \
    --do_train true \
    --do_eval true \
    --output_dir detr-finetuned-cppe-5-10k-steps \
    --num_train_epochs 100 \
    --image_square_size 600 \
    --fp16 true \
    --learning_rate 5e-5 \
    --weight_decay 1e-4 \
    --dataloader_num_workers 4 \
    --dataloader_prefetch_factor 2 \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 1 \
    --remove_unused_columns false \
    --eval_do_concat_batches false \
    --ignore_mismatched_sizes true \
    --metric_for_best_model eval_map \
    --greater_is_better true \
    --load_best_model_at_end true \
    --logging_strategy epoch \
    --eval_strategy epoch \
    --save_strategy epoch \
    --save_total_limit 2 \
    --push_to_hub true \
    --push_to_hub_model_id detr-finetuned-cppe-5-10k-steps \
    --hub_strategy end \
    --seed 1337
```

----------------------------------------

TITLE: Tokenizing Input with TensorFlow
DESCRIPTION: Tokenizes the prompt and candidate answer pairs using a pre-trained tokenizer. It returns TensorFlow tensors with padding. 
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/multiple_choice.md#_snippet_26

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_swag_model")
>>> inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors="tf", padding=True)
```

----------------------------------------

TITLE: Processing the Dataset with Aligned Labels
DESCRIPTION: Applies the tokenization and label alignment function to the entire dataset using the map function with batched processing for efficiency.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/token_classification.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)
```

----------------------------------------

TITLE: Push Model to Organization Repo - Python
DESCRIPTION: This snippet demonstrates pushing a model to an organization's repository on the Hugging Face Hub by specifying the `repo_id` in the format `organization_name/model_name` when calling `push_to_hub`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/model_sharing.md#_snippet_13

LANGUAGE: python
CODE:
```
>>> pt_model.push_to_hub("my-awesome-org/my-awesome-model")
```

----------------------------------------

TITLE: Creating Pipeline with Specific Model in Python
DESCRIPTION: Creates a speech recognition pipeline using the Whisper Large v2 model from OpenAI.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/hi/pipeline_tutorial.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
transcriber = pipeline(model="openai/whisper-large-v2")
```

----------------------------------------

TITLE: Implementing Masked Language Modeling for Text Completion
DESCRIPTION: This code demonstrates how to use the fill-mask pipeline to predict a masked token in a sentence. It takes a sentence with a <mask> token and returns the most likely word to fill that position based on the context.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/task_summary.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> text = "Hugging Face is a community-based open-source <mask> for machine learning."
>>> fill_mask = pipeline(task="fill-mask")
>>> preds = fill_mask(text, top_k=1)
>>> preds = [
...     {
...         "score": round(pred["score"], 4),
...         "token": pred["token"],
...         "token_str": pred["token_str"],
...         "sequence": pred["sequence"],
...     }
...     for pred in preds
... ]
>>> preds
[{'score': 0.2236,
  'token': 1761,
  'token_str': ' platform',
  'sequence': 'Hugging Face is a community-based open-source platform for machine learning.'}]
```

----------------------------------------

TITLE: Configuring Gradient Accumulation with TrainingArguments
DESCRIPTION: Example showing how to enable gradient accumulation by setting the gradient_accumulation_steps parameter in TrainingArguments to achieve a larger effective batch size.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_train_gpu_one.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
training_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, **default_args)
```

----------------------------------------

TITLE: Casting Audio Sampling Rate
DESCRIPTION: This snippet shows how to cast the audio column's sampling rate to a specific value using `dataset.cast_column`. It ensures that the audio data's sampling rate matches the pretrained model's required sampling rate.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/processors.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
```

----------------------------------------

TITLE: Loading ONNX Model with Optimum for Question Answering
DESCRIPTION: Demonstrates loading an ONNX Runtime optimized model for question answering using Optimum and creating a pipeline
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_cpu.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, pipeline
from optimum.onnxruntime import ORTModelForQuestionAnswering

onnx_qa = pipeline("question-answering", model="optimum/roberta-base-squad2", tokenizer="deepset/roberta-base-squad2")

question = "What's my name?"
context = "My name is Philipp and I live in Nuremberg."
pred = onnx_qa(question, context)
```

----------------------------------------

TITLE: Switching Between Multiple Adapters in a Model
DESCRIPTION: Code showing how to switch between different adapters in a model using the set_adapter method, which allows generating outputs with different fine-tuned behaviors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/peft.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
# use adapter_1
model.set_adapter("adapter_1")
output = model.generate(**inputs)
print(tokenizer.decode(output_disabled[0], skip_special_tokens=True))

# use adapter_2
model.set_adapter("adapter_2")
output_enabled = model.generate(**inputs)
print(tokenizer.decode(output_enabled[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Loading an ONNX model with Optimum for sequence classification
DESCRIPTION: This code loads a pre-trained ONNX model for sequence classification using `ORTModelForSequenceClassification` from the Optimum library.  It specifies the "CUDAExecutionProvider" as the provider, indicating that the model should use CUDA for execution on NVIDIA GPUs. The `export=True` argument, which is commented out, can be used to convert a standard Transformers model to ONNX format if a `model.onnx` file is not available.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_one.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from optimum.onnxruntime import ORTModelForSequenceClassification

ort_model = ORTModelForSequenceClassification.from_pretrained(
  "distilbert/distilbert-base-uncased-finetuned-sst-2-english",
  #export=True,
  provider="CUDAExecutionProvider",
)
```

----------------------------------------

TITLE: Audio Classification with Hugging Face Pipeline
DESCRIPTION: Demonstrates how to use the Hugging Face pipeline for audio classification to detect emotions in audio samples using the HUBERT model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/task_summary.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline(task="audio-classification", model="superb/hubert-base-superb-er")
>>> preds = classifier("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
```

----------------------------------------

TITLE: Quantizing Chameleon Model with Bitsandbytes
DESCRIPTION: This snippet shows how to load the Chameleon model with 4-bit quantization using bitsandbytes, which significantly reduces memory requirements while maintaining performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/chameleon.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import ChameleonForConditionalGeneration, BitsAndBytesConfig

# Specify model quantization method
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

model = ChameleonForConditionalGeneration.from_pretrained("facebook/chameleon-7b", quantization_config=quantization_config, device_map="cuda")
```

----------------------------------------

TITLE: Installing Required Libraries for Translation Tasks
DESCRIPTION: This command installs the necessary Python packages for fine-tuning translation models, including transformers, datasets, evaluate, and sacrebleu.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/translation.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers datasets evaluate sacrebleu
```

----------------------------------------

TITLE: Saving and Loading Quantized Models with Transformers in Python
DESCRIPTION: This code shows how to use save_pretrained and from_pretrained methods from PreTrainedModel to save and load a quantized model. It specifies the path where the quantized model will be stored and retrieved using device_map set to auto.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/fbgemm_fp8.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
quant_path = "/path/to/save/quantized/model"
model.save_pretrained(quant_path)
model = AutoModelForCausalLM.from_pretrained(quant_path, device_map="auto")
```

----------------------------------------

TITLE: Initializing DBRX Model with Flash Attention
DESCRIPTION: Shows how to load the DBRX model using flash-attention implementation for faster text generation with improved performance
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/dbrx.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import DbrxForCausalLM, AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained("databricks/dbrx-instruct", token="YOUR_HF_TOKEN")
model = DbrxForCausalLM.from_pretrained(
    "databricks/dbrx-instruct",
    device_map="auto",
    torch_dtype=torch.bfloat16,
    token="YOUR_HF_TOKEN",
    attn_implementation="flash_attention_2",
    )

input_text = "What does it take to build a great LLM?"
messages = [{"role": "user", "content": input_text}]
input_ids = tokenizer.apply_chat_template(messages, return_dict=True, tokenize=True, add_generation_prompt=True, return_tensors="pt").to("cuda")

outputs = model.generate(**input_ids, max_new_tokens=200)
print(tokenizer.decode(outputs[0]))
```

----------------------------------------

TITLE: Creating AdamW Optimizer for PyTorch Model
DESCRIPTION: This code creates an AdamW optimizer for the PyTorch model. It uses the model's parameters and sets the learning rate to 5e-5. AdamW is a common optimizer used for fine-tuning Transformers models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_21

LANGUAGE: Python
CODE:
```
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

----------------------------------------

TITLE: Saving and Loading Final Checkpoint
DESCRIPTION: Code for explicitly saving the final model checkpoint and loading FP32 weights
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/deepspeed.md#2025-04-22_snippet_34

LANGUAGE: python
CODE:
```
from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint

checkpoint_dir = os.path.join(trainer.args.output_dir, "checkpoint-final")
trainer.deepspeed.save_checkpoint(checkpoint_dir)
fp32_model = load_state_dict_from_zero_checkpoint(trainer.model, checkpoint_dir)
```

----------------------------------------

TITLE: Using BARTpho with TensorFlow
DESCRIPTION: Example demonstrating how to use BARTpho model with TensorFlow for Vietnamese text processing. Shows model loading and basic inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bartpho.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
from transformers import TFAutoModel

bartpho = TFAutoModel.from_pretrained("vinai/bartpho-syllable")
input_ids = tokenizer(line, return_tensors="tf")
features = bartpho(**input_ids)
```

----------------------------------------

TITLE: Metrics Computation Setup
DESCRIPTION: Defines a function to compute accuracy metrics during model evaluation using the Evaluate library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/training.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import numpy as np
import evaluate

metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

----------------------------------------

TITLE: Converting Tiktoken Tokenizer to Fast Tokenizer
DESCRIPTION: This snippet shows how to convert a tiktoken tokenizer to the `tokenizer.json` format for use with `PreTrainedTokenizerFast`. It uses `tiktoken.get_encoding` to load the encoding and `convert_tiktoken_to_fast` to convert it to the desired format. The resulting `tokenizer.json` file is saved in the specified directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tiktoken.md#_snippet_1

LANGUAGE: Python
CODE:
```
from transformers.integrations.tiktoken import convert_tiktoken_to_fast
from tiktoken import get_encoding

# ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ ØªØ±Ù…ÙŠØ²Ùƒ Ø§Ù„Ù…Ø®ØµØµ Ø£Ùˆ Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ø°ÙŠ ØªÙˆÙØ±Ù‡ OpenAI
encoding = get_encoding("gpt2")
convert_tiktoken_to_fast(encoding, "config/save/dir")
```

----------------------------------------

TITLE: Training Arguments Configuration
DESCRIPTION: Sets up training arguments including output directory, evaluation strategy, and Hub upload settings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/training.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="yelp_review_classifier",
    eval_strategy="epoch",
    push_to_hub=True,
)
```

----------------------------------------

TITLE: Initializing Image-to-Image Pipeline with Swin2SR Model
DESCRIPTION: Sets up an image-to-image pipeline using a pre-trained Swin2SR lightweight model for super-resolution tasks, with automatic device selection (CUDA or CPU).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_to_image.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import pipeline

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
pipe = pipeline(task="image-to-image", model="caidas/swin2SR-lightweight-x2-64", device=device)
```

----------------------------------------

TITLE: Loading M2M100 Model and Tokenizer for Multilingual Translation in Python
DESCRIPTION: This code shows how to load the M2M100 model and tokenizer for multilingual translation, specifically for translating from Chinese to English using the 'facebook/m2m100_418M' checkpoint.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/multilingual.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
chinese_text = "ä¸è¦æ’æ‰‹å·«å¸«çš„äº‹å‹™, å› ç‚ºä»–å€‘æ˜¯å¾®å¦™çš„, å¾ˆå¿«å°±æœƒç™¼æ€’."

tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M", src_lang="zh")
model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")
```

----------------------------------------

TITLE: Applying Preprocessing to SQuAD Dataset
DESCRIPTION: Applies the preprocessing function to the entire SQuAD dataset using batch processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/question_answering.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad["train"].column_names)
```

----------------------------------------

TITLE: Speech Translation Using the Automatic Speech Recognition Pipeline
DESCRIPTION: This code shows how to use the transformers pipeline API to simplify speech translation. The pipeline abstracts away the model initialization and preprocessing steps, allowing for translation with minimal code.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/speech_to_text_2.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from datasets import load_dataset
from transformers import pipeline

librispeech_en = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
asr = pipeline(
    "automatic-speech-recognition",
    model="facebook/s2t-wav2vec2-large-en-de",
    feature_extractor="facebook/s2t-wav2vec2-large-en-de",
)

translation_de = asr(librispeech_en[0]["file"])
```

----------------------------------------

TITLE: Basic Bark Usage Example
DESCRIPTION: Demonstrates basic usage of Bark for text-to-speech generation with voice presets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bark.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoProcessor, BarkModel

processor = AutoProcessor.from_pretrained("suno/bark")
model = BarkModel.from_pretrained("suno/bark")

voice_preset = "v2/en_speaker_6"

inputs = processor("Hello, my dog is cute", voice_preset=voice_preset)

audio_array = model.generate(**inputs)
audio_array = audio_array.cpu().numpy().squeeze()
```

----------------------------------------

TITLE: Question Answering Pipeline using Hugging Face Transformers
DESCRIPTION: This code snippet shows how to use the question answering pipeline from Hugging Face Transformers. It extracts an answer from a given context based on a specific question.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/task_summary.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from transformers import pipeline

question_answerer = pipeline(task="question-answering")
preds = question_answerer(
    question="What is the name of the repository?",
    context="The name of the repository is huggingface/transformers",
)
print(
    f"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}"
)
```

----------------------------------------

TITLE: Accessing Dataset Items with KeyDataset in Transformers Pipeline
DESCRIPTION: This code demonstrates how to use KeyDataset to efficiently extract specific elements from a dataset and process them through a pipeline. It loads a speech recognition model and applies it to audio samples from the LibriSpeech dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/pipeline_tutorial.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers.pipelines.pt_utils import KeyDataset
from datasets import load_dataset

pipe = pipeline(model="hf-internal-testing/tiny-random-wav2vec2", device=0)
dataset = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation[:10]")

for out in pipe(KeyDataset(dataset, "audio")):
    print(out)
```

----------------------------------------

TITLE: Generating Speech with Pipeline in Transformers (Python)
DESCRIPTION: This code snippet demonstrates how to use the `pipeline` function from the Hugging Face Transformers library to convert text to speech using the VITS model. It sets a seed for reproducibility, runs the pipeline, and saves the output audio to a WAV file. The code requires the `torch`, `transformers`, and `scipy` libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/vits.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline, set_seed
from scipy.io.wavfile import write

set_seed(555)

pipe = pipeline(
    task="text-to-speech",
    model="facebook/mms-tts-eng",
    torch_dtype=torch.float16,
    device=0
)

speech = pipe("Hello, my dog is cute")

# Extract audio data and sampling rate
audio_data = speech["audio"]
sampling_rate = speech["sampling_rate"]

# Save as WAV file
write("hello.wav", sampling_rate, audio_data.squeeze())
```

----------------------------------------

TITLE: Customizing Trainer with Weighted Loss Function
DESCRIPTION: Example of customizing the Trainer class by subclassing and overriding the compute_loss method to implement a weighted cross-entropy loss for a classification task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/trainer.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from torch import nn
from transformers import Trainer

class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.pop("labels")
        # forward pass
        outputs = model(**inputs)
        logits = outputs.get("logits")
        # compute custom loss for 3 labels with different weights
        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0], device=model.device))
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss
```

----------------------------------------

TITLE: ONNX Model Export Utility
DESCRIPTION: Notebook showing how to export models to ONNX format for efficient inference workloads
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/notebooks.md#2025-04-22_snippet_2

LANGUAGE: Jupyter Notebook
CODE:
```
onnx-export.ipynb
```

----------------------------------------

TITLE: Pushing a Tokenizer to the Hub
DESCRIPTION: Shows how to push a tokenizer to an existing model repository on the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/model_sharing.md#2025-04-23_snippet_16

LANGUAGE: python
CODE:
```
>>> tokenizer.push_to_hub("my-awesome-model")
```

----------------------------------------

TITLE: Measuring Peak GPU Memory Usage
DESCRIPTION: This code snippet measures and prints the peak GPU memory usage after running the model without Flash Attention.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
bytes_to_giga_bytes(torch.cuda.max_memory_allocated())
```

----------------------------------------

TITLE: Loading Evaluation Metrics
DESCRIPTION: Loads the seqeval evaluation framework from the Hugging Face Evaluate library, which provides metrics specifically designed for sequence labeling tasks like NER.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> import evaluate

>>> seqeval = evaluate.load("seqeval")
```

----------------------------------------

TITLE: Tokenizer Initialization and Text Preprocessing
DESCRIPTION: Initializes the DistilBERT tokenizer and creates a preprocessing function to tokenize text with truncation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/tasks/sequence_classification.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")

>>> def preprocess_function(examples):
...     return tokenizer(examples["text"], truncation=True)
```

----------------------------------------

TITLE: Translation from English to French using NLLB Model
DESCRIPTION: Example of translating text from English to French using the NLLB model with specific language codes
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/nllb.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/nllb-200-distilled-600M")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("facebook/nllb-200-distilled-600M")

>>> article = "UN Chief says there is no military solution in Syria"
>>> inputs = tokenizer(article, return_tensors="pt")

>>> translated_tokens = model.generate(
...     **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids("fra_Latn"), max_length=30
... )
>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]
Le chef de l'ONU dit qu'il n'y a pas de solution militaire en Syrie
```

----------------------------------------

TITLE: Pipeline with Device Configuration
DESCRIPTION: Example showing how to configure device placement and batch processing for pipelines
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/pipeline_tutorial.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
generator = pipeline(model="openai/whisper-large", device=0, batch_size=2)
audio_filenames = [f"audio_{i}.flac" for i in range(10)]
texts = generator(audio_filenames)
```

----------------------------------------

TITLE: Initializing AutoProcessor for Audio-Text Processing in Python
DESCRIPTION: Loads an AutoProcessor that combines a feature extractor and tokenizer for processing both audio and text data.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/preprocessing.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")
```

----------------------------------------

TITLE: Generating Speech with FastSpeech2Conformer and HifiGan Separately
DESCRIPTION: Python code to generate speech using FastSpeech2Conformer model and HifiGan vocoder as separate components.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/fastspeech2_conformer.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import FastSpeech2ConformerTokenizer, FastSpeech2ConformerModel, FastSpeech2ConformerHifiGan
import soundfile as sf

tokenizer = FastSpeech2ConformerTokenizer.from_pretrained("espnet/fastspeech2_conformer")
inputs = tokenizer("Hello, my dog is cute.", return_tensors="pt")
input_ids = inputs["input_ids"]

model = FastSpeech2ConformerModel.from_pretrained("espnet/fastspeech2_conformer")
output_dict = model(input_ids, return_dict=True)
spectrogram = output_dict["spectrogram"]

hifigan = FastSpeech2ConformerHifiGan.from_pretrained("espnet/fastspeech2_conformer_hifigan")
waveform = hifigan(spectrogram)

sf.write("speech.wav", waveform.squeeze().detach().numpy(), samplerate=22050)
```

----------------------------------------

TITLE: Applying Semantic Segmentation Pipeline in Python
DESCRIPTION: This code snippet initializes a semantic segmentation pipeline with a specified pretrained model and runs inference on the previously loaded image. It outputs the segmentation results, which include class labels and segmentation masks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
semantic_segmentation = pipeline("image-segmentation", "nvidia/segformer-b1-finetuned-cityscapes-1024-1024")
results = semantic_segmentation(image)
results
```

----------------------------------------

TITLE: Training the model (TensorFlow)
DESCRIPTION: This snippet demonstrates how to train the model using the `fit` method. It takes the training and validation datasets, number of epochs, and callbacks as input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_23

LANGUAGE: python
CODE:
```
>>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)
```

----------------------------------------

TITLE: Loading MInDS-14 Dataset Subset
DESCRIPTION: Loads a subset of the MInDS-14 dataset using the Hugging Face datasets library, specifically the English (US) version with 100 samples.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/asr.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset, Audio

>>> minds = load_dataset("PolyAI/minds14", name="en-US", split="train[:100]")
```

----------------------------------------

TITLE: Loading a Sharded Transformers Model
DESCRIPTION: This code shows how to load a model that was previously saved with the sharding mechanism, which is done transparently using the from_pretrained method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/big_models.md#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir, max_shard_size="200MB")
...     new_model = AutoModel.from_pretrained(tmp_dir)
```

----------------------------------------

TITLE: Getting a Learning Rate Scheduler in PyTorch
DESCRIPTION: This snippet shows how to import and use the get_scheduler function to obtain a learning rate scheduler for PyTorch models. It allows selection from various predefined scheduler types.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/optimizer_schedules.md#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
from transformers import get_scheduler
```

----------------------------------------

TITLE: Installing bitsandbytes and accelerate
DESCRIPTION: This command installs the bitsandbytes library for quantization and the accelerate library for distributed training, which are necessary for enabling 8-bit or 4-bit quantization when loading models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_one.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install bitsandbytes accelerate
```

----------------------------------------

TITLE: Audio Preprocessing Setup
DESCRIPTION: Setting up the feature extractor and preprocessing function for audio data preparation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/audio_classification.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
>>> minds = minds.cast_column("audio", Audio(sampling_rate=16_000))

>>> def preprocess_function(examples):
...     audio_arrays = [x["array"] for x in examples["audio"]]
...     inputs = feature_extractor(
...         audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True
...     )
...     return inputs
```

----------------------------------------

TITLE: Removing Unnecessary Columns
DESCRIPTION: Removes specified columns from the dataset using the `remove_columns` method. This helps focus on the relevant 'audio' and 'intent_class' columns for the audio classification task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/audio_classification.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
">>> minds = minds.remove_columns(["path", "transcription", "english_transcription", "lang_id"])"
```

----------------------------------------

TITLE: Loading a Specific Model Version in Python
DESCRIPTION: Demonstrates how to load a specific version of a model using the revision parameter, which can be a tag name, branch name, or commit hash.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/model_sharing.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> model = AutoModel.from_pretrained(
...     "julien-c/EsperBERTo-small", revision="4c77982"  # tag name, or branch name, or commit hash
... )
```

----------------------------------------

TITLE: Initializing Fast Image Processor in Python
DESCRIPTION: Demonstrates how to instantiate a fast image processor using the AutoImageProcessor class with the 'use_fast' argument set to True.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/image_processor.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoImageProcessor

processor = AutoImageProcessor.from_pretrained("facebook/detr-resnet-50", use_fast=True)
```

----------------------------------------

TITLE: Instantiate DETR with Pre-trained Weights (Python)
DESCRIPTION: This code snippet demonstrates how to instantiate the DETR model with pre-trained weights for the entire model. It imports the DetrForObjectDetection class from the transformers library and uses the from_pretrained method to load the pre-trained weights from a specified model identifier ('facebook/detr-resnet-50').
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/detr.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import DetrForObjectDetection

>>> model = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50")
```

----------------------------------------

TITLE: Training GPT-2 with Causal Language Modeling on WikiText-2
DESCRIPTION: Fine-tunes GPT-2 model on WikiText-2 dataset using causal language modeling loss with the Trainer API. Includes training and evaluation configuration with specific batch sizes and output directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python run_clm.py \
    --model_name_or_path openai-community/gpt2 \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-clm
```

----------------------------------------

TITLE: Installing Flash Attention 2
DESCRIPTION: Command to install Flash Attention 2 for additional performance optimization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bark.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
pip install -U flash-attn --no-build-isolation
```

----------------------------------------

TITLE: Running Model Without Flash Attention
DESCRIPTION: This snippet runs the model without Flash Attention, measuring the inference time and peak GPU memory usage.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
import time

start_time = time.time()
result = pipe(long_prompt, max_new_tokens=60)[0]["generated_text"][len(long_prompt):]

print(f"Generated in {time.time() - start_time} seconds.")
result
```

----------------------------------------

TITLE: Creating a Collate Function for DETR Batch Processing in Python
DESCRIPTION: This function creates a custom collate_fn to batch images together for DETR model training. It stacks pixel values, collects labels, and optionally includes pixel masks for padding information.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/object_detection.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
import torch

def collate_fn(batch):
    data = {}
    data["pixel_values"] = torch.stack([x["pixel_values"] for x in batch])
    data["labels"] = [x["labels"] for x in batch]
    if "pixel_mask" in batch[0]:
        data["pixel_mask"] = torch.stack([x["pixel_mask"] for x in batch])
    return data
```

----------------------------------------

TITLE: Implementing Multinomial Sampling with Speculative Decoding
DESCRIPTION: Shows how to implement speculative decoding with multinomial sampling, including temperature control for randomness adjustment.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/generation_strategies.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM-1.7B")
model = AutoModelForCausalLM.from_pretrained("HuggingFaceTB/SmolLM-1.7B")
assistant_model = AutoModelForCausalLM.from_pretrained("HuggingFaceTB/SmolLM-135M")
inputs = tokenizer("Hugging Face is an open-source company", return_tensors="pt")

outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.5)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

----------------------------------------

TITLE: Converting a TensorFlow Model Checkpoint to PyTorch
DESCRIPTION: Code snippet showing how to convert a TensorFlow model checkpoint to PyTorch format by using the from_tf parameter when loading the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/model_sharing.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
>>> pt_model = DistilBertForSequenceClassification.from_pretrained("path/to/awesome-name-you-picked", from_tf=True)
>>> pt_model.save_pretrained("path/to/awesome-name-you-picked")
```

----------------------------------------

TITLE: Running Contrastive Learning with BridgeTower in Python
DESCRIPTION: This code snippet demonstrates how to utilize the BridgeTowerProcessor and BridgeTowerForContrastiveLearning classes to perform contrastive learning with an image and text inputs. It initializes the processor and model, prepares the data, and performs a forward pass to get the scores for each text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bridgetower.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
>>> from transformers import BridgeTowerProcessor, BridgeTowerForContrastiveLearning
>>> import requests
>>> from PIL import Image

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> texts = ["An image of two cats chilling on a couch", "A football player scoring a goal"]

>>> processor = BridgeTowerProcessor.from_pretrained("BridgeTower/bridgetower-large-itm-mlm-itc")
>>> model = BridgeTowerForContrastiveLearning.from_pretrained("BridgeTower/bridgetower-large-itm-mlm-itc")

>>> # forward pass
>>> scores = dict()
>>> for text in texts:
...     # prepare inputs
...     encoding = processor(image, text, return_tensors="pt")
...     outputs = model(**encoding)
...     scores[text] = outputs
```

----------------------------------------

TITLE: Setting Up Training Arguments in Python
DESCRIPTION: Define the TrainingArguments for controlling various parameters during model fine-tuning like batch size, learning rate, and device configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_captioning.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments, Trainer

model_name = checkpoint.split("/")[1]

training_args = TrainingArguments(
    output_dir=f"{model_name}-pokemon",
    learning_rate=5e-5,
    num_train_epochs=50,
    fp16=True,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=2,
    save_total_limit=3,
    eval_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=50,
    logging_steps=50,
    remove_unused_columns=False,
    push_to_hub=True,
    label_names=["labels"],
    load_best_model_at_end=True,
)
```

----------------------------------------

TITLE: Performing Zero-Shot Object Detection with Pipeline
DESCRIPTION: This code snippet uses the initialized pipeline to perform zero-shot object detection on the loaded image. It passes the image and a list of candidate labels to the pipeline, which returns a list of predictions with scores, labels, and bounding box coordinates.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/zero_shot_object_detection.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
">>> predictions = detector(
...     image,
...     candidate_labels=["human face", "rocket", "nasa badge", "star-spangled banner"],
... )
>>> predictions"
```

----------------------------------------

TITLE: Loading and Inference with EncoderDecoderModel (Python)
DESCRIPTION: This code snippet shows how to load a fine-tuned EncoderDecoderModel and perform inference to generate text. It initializes a tokenizer and the model from a pre-trained checkpoint, and then generates a summary from a given article using the `generate` method. It outputs the generated text after decoding.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/encoder-decoder.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, EncoderDecoderModel

>>> # ë¯¸ì„¸ ì¡°ì •ëœ seq2seq ëª¨ë¸ê³¼ ëŒ€ì‘í•˜ëŠ” í† í¬ë‚˜ì´ì € ê°€ì ¸ì˜¤ê¸°
>>> model = EncoderDecoderModel.from_pretrained("patrickvonplaten/bert2bert_cnn_daily_mail")
>>> tokenizer = AutoTokenizer.from_pretrained("patrickvonplaten/bert2bert_cnn_daily_mail")

>>> # let's perform inference on a long piece of text
>>> ARTICLE_TO_SUMMARIZE = (
...     "PG&E stated it scheduled the blackouts in response to forecasts for high winds "
...     "amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were "
...     "scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow."
... )
>>> input_ids = tokenizer(ARTICLE_TO_SUMMARIZE, return_tensors="pt").input_ids

>>> # ìžê¸°íšŒê·€ì ìœ¼ë¡œ ìš”ì•½ ìƒì„± (ê¸°ë³¸ì ìœ¼ë¡œ ê·¸ë¦¬ë”” ë””ì½”ë”© ì‚¬ìš©)
>>> generated_ids = model.generate(input_ids)
>>> generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
>>> print(generated_text)

```

----------------------------------------

TITLE: Applying Preprocessing to the Dataset
DESCRIPTION: Uses the map function to apply the preprocessing function to the entire dataset efficiently by processing multiple examples in batches.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/translation.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> tokenized_books = books.map(preprocess_function, batched=True)
```

----------------------------------------

TITLE: Loading Wav2Vec2 Model for CTC Training in Python
DESCRIPTION: This snippet demonstrates how to load a Wav2Vec2 model for Connectionist Temporal Classification (CTC) training using AutoModelForCTC. It sets the CTC loss reduction to mean and specifies the pad token ID.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/asr.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCTC, TrainingArguments, Trainer

>>> model = AutoModelForCTC.from_pretrained(
...     "facebook/wav2vec2-base",
...     ctc_loss_reduction="mean",
...     pad_token_id=processor.tokenizer.pad_token_id,
... )
```

----------------------------------------

TITLE: Run Gemma 2 Text Generation via CLI (transformers CLI)
DESCRIPTION: Provides a command-line example for performing text generation with a Gemma 2 model using the `transformers run` utility. It demonstrates piping input text to the command and specifying the task, model, and device via arguments.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gemma2.md#_snippet_2

LANGUAGE: bash
CODE:
```
echo -e "Explain quantum computing simply." | transformers run --task text-generation --model google/gemma-2-2b --device 0
```

----------------------------------------

TITLE: Quantizing Model Using FbgemmFp8Config and Transformers in Python
DESCRIPTION: The snippet demonstrates how to instantiate a FbgemmFp8Config and use it with AutoModelForCausalLM's from_pretrained method to quantize a model to 8-bit precision using FBGEMM. The process involves setting the torch_dtype and device_map to auto and applying the quantization configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/fbgemm_fp8.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import FbgemmFp8Config, AutoModelForCausalLM

quantization_config = FbgemmFp8Config()
quantized_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Meta-Llama-3-8B",
    torch_dtype="auto",
    device_map="auto",
    quantization_config=quantization_config
)
```

----------------------------------------

TITLE: Configuring Batch Size for Pipeline Processing
DESCRIPTION: This example shows how to set batch size for processing multiple inputs, which can improve efficiency when working with many files by optimizing GPU utilization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pipeline_tutorial.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
generator(model="openai/whisper-large", device=0, batch_size=2)
audio_filenames = [f"audio_{i}.flac" for i in range(10)]
texts = generator(audio_filenames)
```

----------------------------------------

TITLE: Enabling FP16 Inference in Transformers Pipeline
DESCRIPTION: Demonstrates how to enable FP16 inference for faster GPU processing with lower memory consumption by specifying torch_dtype parameter
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/pipelines.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
pipeline(model="xxxx", torch_dtype=torch.float16)
```

----------------------------------------

TITLE: Applying Preprocessing Function
DESCRIPTION: This snippet demonstrates how to apply the `prepare_dataset` function to a dataset element. It preprocesses the first element of the dataset and returns the preprocessed inputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/processors.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
prepare_dataset(dataset[0])
```

----------------------------------------

TITLE: Loading Tokenizer from JSON File
DESCRIPTION: This snippet demonstrates how to load a tokenizer from a JSON file into a `PreTrainedTokenizerFast` object from the `transformers` library. This allows you to load a previously saved tokenizer and use it with Transformers models. The input is the path to the JSON file, and the output is a `fast_tokenizer` object.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/fast_tokenizers.md#_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import PreTrainedTokenizerFast

>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file="tokenizer.json")
```

----------------------------------------

TITLE: Running Transformers Script with Hub Push Arguments
DESCRIPTION: This snippet shows the command structure for running a Transformers script with arguments to push the model to Hugging Face Hub, including the model ID specification.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/image-pretraining/README.md#2025-04-22_snippet_8

LANGUAGE: bash
CODE:
```
python run_xxx.py \
    --push_to_hub \
    --push_to_hub_model_id <name-of-your-model> \
    ...
```

----------------------------------------

TITLE: Preparing TensorFlow Datasets for Training
DESCRIPTION: This snippet demonstrates how to convert datasets to the tf.data.Dataset format using the prepare_tf_dataset method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
tf_train_set = model.prepare_tf_dataset(
    lm_dataset["train"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_test_set = model.prepare_tf_dataset(
    lm_dataset["test"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)
```

----------------------------------------

TITLE: Transform Images and Annotations in Python
DESCRIPTION: Defines a function that combines image augmentation and annotation formatting to process a batch of samples. This function applies transformations to images and their bounding boxes using the Albumentations library and then reformats the annotations. The function uses the `image_processor` to prepare the images and annotations for the DETR model, returning pixel values, pixel masks, and labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/object_detection.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> # transforming a batch
>>> def transform_aug_ann(examples):
...     image_ids = examples["image_id"]
...     images, bboxes, area, categories = [], [], [], []
...     for image, objects in zip(examples["image"], examples["objects"]):
...         image = np.array(image.convert("RGB"))[:, :, ::-1]
...         out = transform(image=image, bboxes=objects["bbox"], category=objects["category"])

...         area.append(objects["area"])
...         images.append(out["image"])
...         bboxes.append(out["bboxes"])
...         categories.append(out["category"])

...     targets = [
...         {"image_id": id_, "annotations": formatted_anns(id_, cat_, ar_, box_)}
...         for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)
...     ]

...     return image_processor(images=images, annotations=targets, return_tensors="pt")
```

----------------------------------------

TITLE: Running translation example with DeepSpeed on all GPUs
DESCRIPTION: Complete example of running a translation task using T5 model with DeepSpeed configuration for all available GPUs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/deepspeed.md#2025-04-22_snippet_8

LANGUAGE: bash
CODE:
```
deepspeed examples/pytorch/translation/run_translation.py \
--deepspeed tests/deepspeed/ds_config_zero3.json \
--model_name_or_path google-t5/t5-small --per_device_train_batch_size 1 \
--output_dir output_dir --overwrite_output_dir --fp16 \
--do_train --max_train_samples 500 --num_train_epochs 1 \
--dataset_name wmt16 --dataset_config "ro-en" \
--source_lang en --target_lang ro
```

----------------------------------------

TITLE: Generate Text with Mistral using Transformers CLI (Shell)
DESCRIPTION: This snippet shows how to use the Hugging Face `transformers chat` command from the command line to interact with the Mistral model. It pipes an initial prompt into the command, specifying the model, data type, device, and attention implementation. This provides a quick way to test the model without writing a Python script.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mistral.md#_snippet_2

LANGUAGE: shell
CODE:
```
echo -e "My favorite condiment is" | transformers chat mistralai/Mistral-7B-v0.3 --torch_dtype auto --device 0 --attn_implementation flash_attention_2
```

----------------------------------------

TITLE: Creating a VisionTextDualEncoderModel with Pre-trained Models
DESCRIPTION: Python code to initialize a VisionTextDualEncoderModel using pre-trained vision and text encoder models. This combines CLIP's vision model with RoBERTa as the text encoder and creates a processor for handling both image and text inputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/contrastive-image-text/README.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import (
    VisionTextDualEncoderModel,
    VisionTextDualEncoderProcessor,
    AutoTokenizer,
    AutoImageProcessor
)

model = VisionTextDualEncoderModel.from_vision_text_pretrained(
    "openai/clip-vit-base-patch32", "FacebookAI/roberta-base"
)

tokenizer = AutoTokenizer.from_pretrained("FacebookAI/roberta-base")
image_processor = AutoImageProcessor.from_pretrained("openai/clip-vit-base-patch32")
processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)

# save the model and processor
model.save_pretrained("clip-roberta")
processor.save_pretrained("clip-roberta")
```

----------------------------------------

TITLE: Loading and Computing Accuracy Metric with Python
DESCRIPTION: Loads the accuracy metric for model evaluation using the Hugging Face Evaluate library and computes it using predicted and reference labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_classification.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> import evaluate

>>> accuracy = evaluate.load("accuracy")
```

LANGUAGE: python
CODE:
```
>>> import numpy as np


>>> def compute_metrics(eval_pred):
...     predictions, labels = eval_pred
...     predictions = np.argmax(predictions, axis=1)
...     return accuracy.compute(predictions=predictions, references=labels)
```

----------------------------------------

TITLE: Loading AutoModelForSequenceClassification in PyTorch
DESCRIPTION: Shows how to load a pre-trained model for sequence classification using AutoModelForSequenceClassification in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/quicktour.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
```

----------------------------------------

TITLE: Applying Group Texts Function over Dataset
DESCRIPTION: Processes the entire dataset by applying the group_texts function to reorganize text sequences into manageable chunks. This setup is essential for efficient model training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)
```

----------------------------------------

TITLE: Text Preprocessing Function
DESCRIPTION: Creates a preprocessing function to join text lists and truncate sequences to match model's maximum input length.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/language_modeling.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
def preprocess_function(examples):
    return tokenizer([" ".join(x) for x in examples["answers.text"]], truncation=True)
```

----------------------------------------

TITLE: Transcribing Speech Using Transformers Pipeline in Python
DESCRIPTION: This snippet demonstrates how to use the transformers pipeline for automatic speech recognition with Whisper. Required dependencies include the Hugging Face Transformers library and a PyTorch installation. The example showcases loading a pretrained Whisper model to transcribe audio from a given URL link.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/whisper.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipeline = pipeline(
    task="automatic-speech-recognition",
    model="openai/whisper-large-v3-turbo",
    torch_dtype=torch.float16,
    device=0
)
pipeline("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
```

----------------------------------------

TITLE: Custom Loss Function with CustomTrainer
DESCRIPTION: This code shows how to customize the loss computation by subclassing the `Trainer` and overriding the `compute_loss` method. This allows for implementing custom loss functions, in this case, using a weighted cross-entropy loss. It requires importing `nn` from `torch`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/trainer.md#_snippet_4

LANGUAGE: python
CODE:
```
from torch import nn
from transformers import Trainer

class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        # forward pass
        outputs = model(**inputs)
        logits = outputs.get("logits")
        # compute custom loss for 3 labels with different weights
        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0], device=model.device))
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss
```

----------------------------------------

TITLE: Speech Recognition Pipeline Setup
DESCRIPTION: Example of setting up an automatic speech recognition pipeline using the wav2vec2 model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/quicktour.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> import torch
>>> from transformers import pipeline

>>> speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")
```

----------------------------------------

TITLE: Pushing Additional Files to Model Repository
DESCRIPTION: Shows how to push additional files, such as a tokenizer or TensorFlow version of a PyTorch model, to the model repository on the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_sharing.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
tokenizer.push_to_hub("my-awesome-model")
tf_model.push_to_hub("my-awesome-model")
```

----------------------------------------

TITLE: Beam Search with Transformers
DESCRIPTION: This snippet demonstrates how to use beam search decoding strategy with the Transformers library. It initializes a tokenizer and causal language model, encodes input text, and generates text using the `model.generate` function with the `num_beams` parameter set to 2.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/generation_strategies.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
inputs = tokenizer("Hugging Face is an open-source company", return_tensors="pt").to("cuda")

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", torch_dtype=torch.float16).to("cuda")
# explicitly set to 100 because Llama2 generation length is 4096
outputs = model.generate(**inputs, max_new_tokens=50, num_beams=2)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
"['Hugging Face is an open-source company that develops and maintains the Hugging Face platform, which is a collection of tools and libraries for building and deploying natural language processing (NLP) models. Hugging Face was founded in 2018 by Thomas Wolf']"
```

----------------------------------------

TITLE: Loading a Fast DistilBertTokenizer
DESCRIPTION: This code loads a DistilBertTokenizerFast (fast tokenizer) from a pre-trained model.  It uses `DistilBertTokenizerFast.from_pretrained` and requires a model identifier. The `DistilBertTokenizerFast` class is required.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/create_a_model.md#_snippet_17

LANGUAGE: python
CODE:
```
>>> from transformers import DistilBertTokenizerFast

>>> fast_tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Loading Model with Revision - Python
DESCRIPTION: This snippet demonstrates how to load a specific version of a model from the Hugging Face Model Hub using the `revision` parameter. It shows loading a model with a specific commit hash.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/model_sharing.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> model = AutoModel.from_pretrained(
...     "julien-c/EsperBERTo-small", revision="4c77982"  # Ø§Ø³Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø©ØŒ Ø£Ùˆ Ø§Ø³Ù… Ø§Ù„ÙØ±Ø¹ØŒ Ø£Ùˆ ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ø§Ù„ØªØ²Ø§Ù…
... )
```

----------------------------------------

TITLE: Tokenize and Pad a Batch (TensorFlow)
DESCRIPTION: Tokenizes a batch of text strings, applies padding and truncation to ensure uniform length, and returns TensorFlow tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_17

LANGUAGE: Python
CODE:
```
>>> tf_batch = tokenizer(
...     ["We are very happy to show you the ðŸ¤— Transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="tf",
... )
```

----------------------------------------

TITLE: Using Accelerate for Automatic Device Mapping
DESCRIPTION: This code shows how to use the 'device_map' parameter with Accelerate library to automatically distribute model weights across available hardware for large models that don't fit on a single GPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pipeline_tutorial.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
#!pip install accelerate
generator(model="openai/whisper-large", device_map="auto")
```

----------------------------------------

TITLE: Extracting Features Using a Backbone
DESCRIPTION: This snippet demonstrates how to load and preprocess an image and then pass it through the backbone to extract feature maps. It utilizes `AutoImageProcessor` for preprocessing and `AutoBackbone` to load the backbone. The `out_indices` parameter is used to specify which layer's feature map to extract.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/backbones.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers import AutoImageProcessor, AutoBackbone
import torch
from PIL import Image
import requests

model = AutoBackbone.from_pretrained("microsoft/swin-tiny-patch4-window7-224", out_indices=(1,))
processor = AutoImageProcessor.from_pretrained("microsoft/swin-tiny-patch4-window7-224")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(image, return_tensors="pt")
outputs = model(**inputs)
```

----------------------------------------

TITLE: Initializing EncoderDecoderModel from Configurations (Python)
DESCRIPTION: This code shows how to initialize an EncoderDecoderModel randomly using configurations for both the encoder and the decoder. It uses BertConfig for both encoder and decoder and creates an EncoderDecoderModel instance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/encoder-decoder.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel

>>> config_encoder = BertConfig()
>>> config_decoder = BertConfig()

>>> config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)
>>> model = EncoderDecoderModel(config=config)
```

----------------------------------------

TITLE: Logging in to Hugging Face
DESCRIPTION: This command logs in to the Hugging Face platform, which is required before sharing models to the Model Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/run_scripts.md#2025-04-22_snippet_15

LANGUAGE: bash
CODE:
```
huggingface-cli login
```

----------------------------------------

TITLE: Processing Image Inputs in Multimodal Models
DESCRIPTION: This snippet demonstrates how to configure an input with both image and text elements for multimodal models using the Transformers library. It illustrates using the AutoProcessor and LlavaOnevisionForConditionalGeneration to prepare a message for processing by outlining required input formats.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating_multimodal.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration

model = LlavaOnevisionForConditionalGeneration.from_pretrained("llava-hf/llava-onevision-qwen2-0.5b-ov-hf")
processor = AutoProcessor.from_pretrained("llava-hf/llava-onevision-qwen2-0.5b-ov-hf")

messages = [
    {
      "role": "system",
      "content": [{"type": "text", "text": "You are a friendly chatbot who always responds in the style of a pirate"}],
    },
    {
      "role": "user",
      "content": [
            {"type": "image", "url": "http://images.cocodataset.org/val2017/000000039769.jpg"},
            {"type": "text", "text": "What are these?"},
        ],
    },
]
```

----------------------------------------

TITLE: Generating Text with GPT-NeoX-Japanese in Python
DESCRIPTION: This snippet demonstrates how to use the GPTNeoXJapaneseForCausalLM and GPTNeoXJapaneseTokenizer classes to generate text with the GPT-NeoX-Japanese model. It loads the pretrained model and tokenizer, processes a prompt, and generates text using specified parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gpt_neox_japanese.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import GPTNeoXJapaneseForCausalLM, GPTNeoXJapaneseTokenizer

>>> model = GPTNeoXJapaneseForCausalLM.from_pretrained("abeja/gpt-neox-japanese-2.7b")
>>> tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained("abeja/gpt-neox-japanese-2.7b")

>>> prompt = "äººã¨AIãŒå”èª¿ã™ã‚‹ãŸã‚ã«ã¯ã€"

>>> input_ids = tokenizer(prompt, return_tensors="pt").input_ids

>>> gen_tokens = model.generate(
...     input_ids,
...     do_sample=True,
...     temperature=0.9,
...     max_length=100,
... )
>>> gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]

>>> print(gen_text)
äººã¨AIãŒå”èª¿ã™ã‚‹ãŸã‚ã«ã¯ã€AIã¨äººãŒå…±å­˜ã—ã€AIã‚’æ­£ã—ãç†è§£ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
```

----------------------------------------

TITLE: Enabling Gradient Checkpointing
DESCRIPTION: Configuration code to enable gradient checkpointing in the Trainer, which helps reduce memory usage by selectively storing activations at the cost of some computation overhead.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_train_gpu_one.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
training_args = TrainingArguments(
    per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, **default_args
)
```

----------------------------------------

TITLE: Downloading and Saving Transformers Model for Offline Use
DESCRIPTION: Python code demonstrating how to download a model and tokenizer, save them locally, and then load them for offline use.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/installation.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

>>> tokenizer = AutoTokenizer.from_pretrained("bigscience/T0_3B")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0_3B")

>>> tokenizer.save_pretrained("./your/path/bigscience_t0")
>>> model.save_pretrained("./your/path/bigscience_t0")

>>> tokenizer = AutoTokenizer.from_pretrained("./your/path/bigscience_t0")
>>> model = AutoModel.from_pretrained("./your/path/bigscience_t0")
```

----------------------------------------

TITLE: Loading BERT Model and Measuring Weight Memory
DESCRIPTION: Loads a BERT large model to GPU and measures how much memory is occupied by the model weights alone, showing the baseline memory requirement before training begins.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_memory_anatomy.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification


>>> model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-large-uncased").to("cuda")
>>> print_gpu_utilization()
GPU memory occupied: 2631 MB.
```

----------------------------------------

TITLE: Loading Converted Llama3 Model and Tokenizer
DESCRIPTION: Example of loading a converted Llama3 model and tokenizer using the Hugging Face Transformers library's Auto classes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llama3.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("/output/path")
model = AutoModelForCausalLM.from_pretrained("/output/path")
```

----------------------------------------

TITLE: Configuring Training Arguments with Liger Kernel
DESCRIPTION: Example of setting up TrainingArguments with Liger Kernel enabled, including common training parameters like learning rate, batch sizes, and training epochs
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/trainer.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="your-model",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=True,
    use_liger_kernel=True
)
```

----------------------------------------

TITLE: Starting Uvicorn Server for Transformers Inference in Bash
DESCRIPTION: This bash command starts the Uvicorn server to run the Starlette application for Transformers inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/pipeline_webserver.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
uvicorn server:app
```

----------------------------------------

TITLE: Applying Chat Template without Special Tokens
DESCRIPTION: Demonstrates how to apply a chat template without adding special tokens, which is important to avoid duplication and maintain model performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
apply_chat_template(messages, tokenize=False, add_special_tokens=False)
```

----------------------------------------

TITLE: Running Causal Language Model Training with Custom Dataset
DESCRIPTION: Command for training a causal language model with a custom training dataset file. This approach allows for domain-specific fine-tuning of the DistilGPT2 model on custom text data.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/tensorflow/language-modeling/README.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
python run_clm.py \
--model_name_or_path distilbert/distilgpt2 \
--output_dir output \
--train_file train_file_path
```

----------------------------------------

TITLE: Setting Up Optimizer and Training Parameters for TensorFlow
DESCRIPTION: Configures the optimizer, learning rate schedule, and training hyperparameters for fine-tuning a model with TensorFlow. This setup calculates the total number of training steps based on dataset size and batch size.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
>>> from transformers import create_optimizer
>>> import tensorflow as tf

>>> batch_size = 16
>>> num_epochs = 5
>>> batches_per_epoch = len(tokenized_imdb["train"]) // batch_size
>>> total_train_steps = int(batches_per_epoch * num_epochs)
>>> optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)
```

----------------------------------------

TITLE: Demonstrating ValueError with AutoModelForQuestionAnswering and GPT2
DESCRIPTION: This snippet illustrates a `ValueError` that occurs when trying to load a GPT2 model for question answering using `AutoModelForQuestionAnswering`.  GPT2 is not designed for question answering so a mapping cannot be found.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/troubleshooting.md#_snippet_9

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoProcessor, AutoModelForQuestionAnswering

>>> processor = AutoProcessor.from_pretrained("openai-community/gpt2-medium")
>>> model = AutoModelForQuestionAnswering.from_pretrained("openai-community/gpt2-medium")
ValueError: Unrecognized configuration class <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'> for this kind of AutoModel: AutoModelForQuestionAnswering.
Model type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig, BloomConfig, ...
```

----------------------------------------

TITLE: Simple Inference with TorchScript Traced Model
DESCRIPTION: This code demonstrates how to perform inference with a traced model by calling the model directly with input tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/torchscript.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
traced_model(tokens_tensor, segments_tensors)
```

----------------------------------------

TITLE: BERT Token Type IDs Retrieval
DESCRIPTION: This snippet retrieves the `token_type_ids` from the encoded dictionary returned by the tokenizer. The `token_type_ids` indicate the sequence to which each token belongs (0 for the first sequence, 1 for the second). This is used by models like BERT to differentiate between different input sequences.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/glossary.md#_snippet_15

LANGUAGE: python
CODE:
```
>>> encoded_dict["token_type_ids"]
[0ØŒ 0ØŒ 0ØŒ 0ØŒ 0ØŒ 0ØŒ 0ØŒ 0ØŒ 0ØŒ 0ØŒ 1ØŒ 1ØŒ 1ØŒ 1ØŒ 1ØŒ 1ØŒ 1ØŒ 1ØŒ 1]
```

----------------------------------------

TITLE: Using ExLlama-v2 Kernels with AWQ Quantized Models
DESCRIPTION: Python code demonstrating how to use ExLlama-v2 kernels with AWQ quantized models for faster prefill and decoding. The example shows both inference and text generation with the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/awq.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig

quantization_config = AwqConfig(version="exllama")

model = AutoModelForCausalLM.from_pretrained(
    "TheBloke/Mistral-7B-Instruct-v0.1-AWQ",
    quantization_config=quantization_config,
    device_map="auto",
)

input_ids = torch.randint(0, 100, (1, 128), dtype=torch.long, device="cuda")
output = model(input_ids)
print(output.logits)

tokenizer = AutoTokenizer.from_pretrained("TheBloke/Mistral-7B-Instruct-v0.1-AWQ")
input_ids = tokenizer.encode("How to make a cake", return_tensors="pt").to(model.device)
output = model.generate(input_ids, do_sample=True, max_length=50, pad_token_id=50256)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Using AutoBackbone for Feature Map Extraction in Python
DESCRIPTION: This code demonstrates how to use a pre-trained Swin Transformer backbone to extract feature maps from specific stages of the model, which can be useful for downstream vision tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/autoclass_tutorial.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import AutoImageProcessor, AutoBackbone
>>> import torch
>>> from PIL import Image
>>> import requests
>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> processor = AutoImageProcessor.from_pretrained("microsoft/swin-tiny-patch4-window7-224")
>>> model = AutoBackbone.from_pretrained("microsoft/swin-tiny-patch4-window7-224", out_indices=(1,))

>>> inputs = processor(image, return_tensors="pt")
>>> outputs = model(**inputs)
>>> feature_maps = outputs.feature_maps
```

LANGUAGE: python
CODE:
```
>>> list(feature_maps[0].shape)
[1, 96, 56, 56]
```

----------------------------------------

TITLE: Loading Food-101 Dataset Subset
DESCRIPTION: Loads a small subset of the Food-101 dataset using the Datasets library and splits it into train and test sets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/image_classification.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> food = load_dataset("food101", split="train[:5000]")
>>> food = food.train_test_split(test_size=0.2)
```

----------------------------------------

TITLE: Perform Inference with Masked Language Model (PyTorch)
DESCRIPTION: Passes the tokenized input through the fine-tuned masked language model and retrieves the logits for the masked token.  Requires the `transformers` library and a trained model and tokenized inputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_26

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForMaskedLM

>>> model = AutoModelForMaskedLM.from_pretrained("stevhliu/my_awesome_eli5_mlm_model")
>>> logits = model(**inputs).logits
>>> mask_token_logits = logits[0, mask_token_index, :]
```

----------------------------------------

TITLE: Generating Text with Custom Model Pipeline in Python
DESCRIPTION: Shows how to use the custom model pipeline to generate text from an input prompt.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/pipeline_tutorial.md#2025-04-23_snippet_6

LANGUAGE: python
CODE:
```
>>> generator(
...     "Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone"
... )  # doctest: +SKIP
[{'generated_text': 'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, Seven for the Dragon-lords (for them to rule in a world ruled by their rulers, and all who live within the realm'}]
```

----------------------------------------

TITLE: Initializing ViT Hybrid Model with SDPA in PyTorch
DESCRIPTION: This snippet demonstrates how to initialize a ViT Hybrid model for image classification using Scaled Dot Product Attention (SDPA) in PyTorch. It loads a pre-trained model with half-precision floating-point format for improved performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/vit_hybrid.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import ViTHybridForImageClassification
model = ViTHybridForImageClassification.from_pretrained("google/vit-hybrid-base-bit-384", attn_implementation="sdpa", torch_dtype=torch.float16)
...
```

----------------------------------------

TITLE: Loading GPT-2 Model and Tokenizer in Python
DESCRIPTION: This snippet demonstrates how to load a pre-trained GPT-2 model and its associated tokenizer using Hugging Face Transformers. It specifies the device (CUDA) and the model ID.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/perplexity.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import GPT2LMHeadModel, GPT2TokenizerFast

device = "cuda"
model_id = "openai-community/gpt2-large"
model = GPT2LMHeadModel.from_pretrained(model_id).to(device)
tokenizer = GPT2TokenizerFast.from_pretrained(model_id)
```

----------------------------------------

TITLE: Perform Inference with Masked Language Model (TensorFlow)
DESCRIPTION: Passes the tokenized input through the fine-tuned masked language model and retrieves the logits for the masked token in TensorFlow. Requires the `transformers` library and a trained model and tokenized inputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_29

LANGUAGE: python
CODE:
```
>>> from transformers import TFAutoModelForMaskedLM

>>> model = TFAutoModelForMaskedLM.from_pretrained("stevhliu/my_awesome_eli5_mlm_model")
>>> logits = model(**inputs).logits
>>> mask_token_logits = logits[0, mask_token_index, :]
```

----------------------------------------

TITLE: PyTorch Model Inference
DESCRIPTION: Loads a fine-tuned multiple-choice model and performs inference. The tokenized inputs and labels are passed to the model, and the logits representing the predicted probabilities for each candidate are returned.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/multiple_choice.md#_snippet_24

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoModelForMultipleChoice

>>> model = AutoModelForMultipleChoice.from_pretrained("my_awesome_swag_model")
>>> outputs = model(**{k: v.unsqueeze(0) for k, v in inputs.items()}, labels=labels)
>>> logits = outputs.logits
```

----------------------------------------

TITLE: Applying Transforms to Datasets
DESCRIPTION: Python code for setting the transform functions to both training and test datasets using the set_transform method from the datasets library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```

----------------------------------------

TITLE: Training Segmentation Models with Trainer in PyTorch
DESCRIPTION: Load a segmentation model using AutoModelForSemanticSegmentation and define training arguments using TrainingArguments, including hyperparameters like learning rate and batch sizes. Use Trainer to initiate and run the training cycle, define datasets and compute metrics.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md#2025-04-22_snippet_18

LANGUAGE: py
CODE:
```
>>> from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer

>>> model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)

```

LANGUAGE: py
CODE:
```
>>> training_args = TrainingArguments(
...     output_dir="segformer-b0-scene-parse-150",
...     learning_rate=6e-5,
...     num_train_epochs=50,
...     per_device_train_batch_size=2,
...     per_device_eval_batch_size=2,
...     save_total_limit=3,
...     eval_strategy="steps",
...     save_strategy="steps",
...     save_steps=20,
...     eval_steps=20,
...     logging_steps=1,
...     eval_accumulation_steps=5,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=train_ds,
...     eval_dataset=test_ds,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()

```

LANGUAGE: py
CODE:
```
>>> trainer.push_to_hub()

```

----------------------------------------

TITLE: Translating Text from English to Italian in Python
DESCRIPTION: This snippet illustrates how to prompt a model to translate a sentence from English to Italian using an effective text generation pipeline adjusted for translation tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/prompting.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> torch.manual_seed(2) # doctest: +IGNORE_RESULT
>>> prompt = """Translate the English text to Italian.\
... Text: Sometimes, I've believed as many as six impossible things before breakfast.\
... Translation:\
... """

>>> sequences = pipe(
...     prompt,
...     max_new_tokens=20,
...     do_sample=True,
...     top_k=10,
...     return_full_text = False,
... )

>>> for seq in sequences:
...     print(f"{seq['generated_text']}")
A volte, ho creduto a sei impossibili cose prima di colazione.
```

----------------------------------------

TITLE: Saving and Loading TensorFlow Model Using save_pretrained/from_pretrained
DESCRIPTION: This example shows how to save a TensorFlow model using `TFPretrainedModel.save_pretrained` and load it using `TFPreTrainedModel.from_pretrained`. This approach ensures compatibility with Hugging Face Transformers and avoids potential issues related to saving and loading entire TensorFlow models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/troubleshooting.md#_snippet_1

LANGUAGE: Python
CODE:
```
>>> from transformers import TFPreTrainedModel

>>> model.save_pretrained("path_to/model")
>>> model = TFPreTrainedModel.from_pretrained("path_to/model")
```

----------------------------------------

TITLE: Running Object Detection Inference
DESCRIPTION: Performs object detection on the input image and prints detected objects with their confidence scores and locations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/object_detection.md#2025-04-22_snippet_21

LANGUAGE: python
CODE:
```
with torch.no_grad():
    inputs = image_processor(images=[image], return_tensors="pt")
    outputs = model(**inputs.to(device))
    target_sizes = torch.tensor([[image.size[1], image.size[0]]])
    results = image_processor.post_process_object_detection(outputs, threshold=0.3, target_sizes=target_sizes)[0]

for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
    box = [round(i, 2) for i in box.tolist()]
    print(
        f"Detected {model.config.id2label[label.item()]} with confidence "
        f"{round(score.item(), 3)} at location {box}"
    )
```

----------------------------------------

TITLE: Batch Tokenization with Padding in TensorFlow
DESCRIPTION: Demonstrates batch tokenization with padding and truncation for TensorFlow models. The tokenizer processes a list of sentences and returns tensors suitable for TensorFlow models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/quicktour.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> tf_batch = tokenizer(
...     ["We are very happy to show you the ðŸ¤— Transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="tf",
... )
```

----------------------------------------

TITLE: Implementing Basic Pipeline Structure in Python
DESCRIPTION: Base structure showing the required methods for creating a custom pipeline class inheriting from the Pipeline base class.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/add_new_pipeline.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import Pipeline

class MyPipeline(Pipeline):
    def _sanitize_parameters(self, **kwargs):

    def preprocess(self, inputs, args=2):

    def _forward(self, model_inputs):

    def postprocess(self, model_outputs):
```

----------------------------------------

TITLE: Batch Processing for Text Generation
DESCRIPTION: Demonstrates how to process multiple input sequences in a batch for efficient text generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/llm_tutorial.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
model_inputs = tokenizer(
    ["A list of colors: red, blue", "Portugal is"], return_tensors="pt", padding=True
).to("cuda")
generated_ids = model.generate(**model_inputs)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
```

----------------------------------------

TITLE: Performing Text Classification Using a Prompt in Python
DESCRIPTION: This snippet classifies a given text (movie review) as positive, negative, or neutral by providing a clear prompt to the model using the pipeline set for text generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/prompting.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> torch.manual_seed(0) # doctest: +IGNORE_RESULT
>>> prompt = """Classify the text into neutral, negative or positive. \
... Text: This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\
... Sentiment:\
... """

>>> sequences = pipe(
...     prompt,
...     max_new_tokens=10,
... )

>>> for seq in sequences:
...     print(f"Result: {seq['generated_text']}")
Result: Classify the text into neutral, negative or positive. \
Text: This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.
Sentiment:
Positive
```

----------------------------------------

TITLE: Loading T5 Model for TensorFlow Sequence-to-Sequence Translation
DESCRIPTION: This code loads a pre-trained T5 model using the TensorFlow-specific AutoModelForSeq2SeqLM class for translation tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/translation.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
>>> from transformers import TFAutoModelForSeq2SeqLM

>>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Quantizing a Transformers Model with EETQ
DESCRIPTION: Python code showing how to load and quantize a pre-trained model using EetqConfig. This applies int8 quantization when loading the model with AutoModelForCausalLM.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/eetq.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, EetqConfig
path = "/path/to/model".
quantization_config = EetqConfig("int8")
model = AutoModelForCausalLM.from_pretrained(path, device_map="auto", quantization_config=quantization_config)
```

----------------------------------------

TITLE: Quantizing Model via Pipeline with BitsAndBytesConfig
DESCRIPTION: This code snippet shows how to quantize a pre-trained language model using the `pipeline` API from the `transformers` library. It uses the `BitsAndBytesConfig` to load the model in a quantized format (8-bit in this case).  The `device_map` argument specifies where the model should be loaded (auto selects the best available device).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/conversations.md#_snippet_5

LANGUAGE: python
CODE:
```
from transformers import pipeline, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_8bit=True) # ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ØªØ¬Ø±Ø¨Ø© load_in_4bit
pipe = pipeline("text-generation", "meta-llama/Meta-Llama-3-8B-Instruct", device_map="auto", model_kwargs={"quantization_config": quantization_config})
```

----------------------------------------

TITLE: Configuring AdamW Optimizer with Auto Parameters
DESCRIPTION: A JSON configuration snippet for DeepSpeed's AdamW optimizer with automatic parameter detection. The 'auto' values will be replaced by the corresponding values from Trainer command line arguments.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/deepspeed.md#2025-04-22_snippet_24

LANGUAGE: json
CODE:
```
{
   "optimizer": {
       "type": "AdamW",
       "params": {
         "lr": "auto",
         "betas": "auto",
         "eps": "auto",
         "weight_decay": "auto"
       }
   }
}
```

----------------------------------------

TITLE: Configuring WarmupLR Scheduler with Auto Parameters
DESCRIPTION: A JSON configuration snippet for DeepSpeed's WarmupLR scheduler with automatic parameter detection. The parameters will be populated based on Trainer command line arguments.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/deepspeed.md#2025-04-22_snippet_28

LANGUAGE: json
CODE:
```
{
   "scheduler": {
         "type": "WarmupLR",
         "params": {
             "warmup_min_lr": "auto",
             "warmup_max_lr": "auto",
             "warmup_num_steps": "auto"
         }
     }
}
```

----------------------------------------

TITLE: Pushing Custom Model to Hugging Face Hub in Python
DESCRIPTION: Shows how to create a custom ResNet model, load pre-trained weights, and push it to the Hugging Face Hub for sharing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/custom_models.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
resnet50d_config = ResnetConfig(block_type="bottleneck", stem_width=32, stem_type="deep", avg_down=True)
resnet50d = ResnetModelForImageClassification(resnet50d_config)

pretrained_model = timm.create_model("resnet50d", pretrained=True)
resnet50d.model.load_state_dict(pretrained_model.state_dict())

resnet50d.push_to_hub("custom-resnet50d")
```

----------------------------------------

TITLE: Define ResNet Model for Image Classification
DESCRIPTION: This snippet defines a custom ResNet model class `ResnetModelForImageClassification` for image classification. It inherits from `PreTrainedModel` and uses a similar initialization as `ResnetModel`. The forward method performs the forward pass through the `timm` ResNet model and calculates the cross-entropy loss if labels are provided. The output is a dictionary containing the logits and the loss (if labels are provided).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/custom_models.md#_snippet_4

LANGUAGE: python
CODE:
```
import torch


class ResnetModelForImageClassification(PreTrainedModel):
    config_class = ResnetConfig

    def __init__(self, config):
        super().__init__(config)
        block_layer = BLOCK_MAPPING[config.block_type]
        self.model = ResNet(
            block_layer,
            config.layers,
            num_classes=config.num_classes,
            in_chans=config.input_channels,
            cardinality=config.cardinality,
            base_width=config.base_width,
            stem_width=config.stem_width,
            stem_type=config.stem_type,
            avg_down=config.avg_down,
        )

    def forward(self, tensor, labels=None):
        logits = self.model(tensor)
        if labels is not None:
            loss = torch.nn.cross_entropy(logits, labels)
            return {"loss": loss, "logits": logits}
        return {"logits": logits}
```

----------------------------------------

TITLE: Compiling and Fitting Model with tf.data.Dataset
DESCRIPTION: This code snippet compiles and fits the TensorFlow model using a `tf.data.Dataset`. The Adam optimizer is used with a learning rate of 3e-5. No loss function is provided, relying on the model to automatically select one. The model is then trained using the prepared TensorFlow dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_15

LANGUAGE: Python
CODE:
```
model.compile(optimizer=Adam(3e-5))  # No loss argument!

model.fit(tf_dataset)
```

----------------------------------------

TITLE: Data Preprocessing and Audio Processing
DESCRIPTION: Functions for preprocessing audio data and preparing it for the ASR model
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/asr.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> def uppercase(example):
...     return {"transcription": example["transcription"].upper()}

>>> def prepare_dataset(batch):
...     audio = batch["audio"]
...     batch = processor(audio["array"], sampling_rate=audio["sampling_rate"], text=batch["transcription"])
...     batch["input_length"] = len(batch["input_values"][0])
...     return batch
```

----------------------------------------

TITLE: Configuring Training Arguments for GIT Model Fine-tuning
DESCRIPTION: Sets up training arguments for fine-tuning the GIT model, including learning rate, batch size, and evaluation strategy.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/image_captioning.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments, Trainer

model_name = checkpoint.split("/")[1]

training_args = TrainingArguments(
    output_dir=f"{model_name}-pokemon",
    learning_rate=5e-5,
    num_train_epochs=50,
    fp16=True,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=2,
    save_total_limit=3,
    eval_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=50,
    logging_steps=50,
    remove_unused_columns=False,
    push_to_hub=True,
    label_names=["labels"],
    load_best_model_at_end=True,
)
```

----------------------------------------

TITLE: Implementing NEFTune in Transformers Trainer
DESCRIPTION: Python code example showing how to enable NEFTune (Noisy Embeddings for Fine-Tuning) in the Transformers Trainer by setting the neftune_noise_alpha parameter in TrainingArguments.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/trainer.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
from transformers import Trainer, TrainingArguments

args = TrainingArguments(..., neftune_noise_alpha=0.1)
trainer = Trainer(..., args=args)

...

trainer.train()
```

----------------------------------------

TITLE: Performing Depth Estimation Manually with Transformers
DESCRIPTION: This code segment illustrates the manual process of loading a depth estimation model and image processor, preparing an image, and obtaining depth predictions directly from the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/monocular_depth_estimation.md#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoImageProcessor, AutoModelForDepthEstimation
>>> checkpoint = "Intel/zoedepth-nyu-kitti"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)
>>> model = AutoModelForDepthEstimation.from_pretrained(checkpoint).to(device)
```

----------------------------------------

TITLE: Inference with Trained Object Detection Model
DESCRIPTION: Python code demonstrating how to load a trained object detection model and perform inference on new images. Includes post-processing of model outputs and visualization of detection results.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/object-detection/README.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import requests
import torch

from PIL import Image
from transformers import AutoImageProcessor, AutoModelForObjectDetection

# Name of repo on the hub or path to a local folder
model_name = "qubvel-hf/detr-resnet-50-finetuned-10k-cppe5"

image_processor = AutoImageProcessor.from_pretrained(model_name)
model = AutoModelForObjectDetection.from_pretrained(model_name)

# Load image for inference
url = "https://images.pexels.com/photos/8413299/pexels-photo-8413299.jpeg?auto=compress&cs=tinysrgb&w=630&h=375&dpr=2"
image = Image.open(requests.get(url, stream=True).raw)

# Prepare image for the model
inputs = image_processor(images=image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

# Post process model predictions 
# this include conversion to Pascal VOC format and filtering non confident boxes
width, height = image.size
target_sizes = torch.tensor([height, width]).unsqueeze(0)  # add batch dim
results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]

for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
    box = [round(i, 2) for i in box.tolist()]
    print(
        f"Detected {model.config.id2label[label.item()]} with confidence "
        f"{round(score.item(), 3)} at location {box}"
    )
```

LANGUAGE: python
CODE:
```
from PIL import ImageDraw
draw = ImageDraw.Draw(image)

for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
    box = [round(i, 2) for i in box.tolist()]
    x, y, x2, y2 = tuple(box)
    draw.rectangle((x, y, x2, y2), outline="red", width=1)
    draw.text((x, y), model.config.id2label[label.item()], fill="white")

image
```

----------------------------------------

TITLE: Creating Audio Classification Pipeline for Emotion Recognition in Python
DESCRIPTION: Shows how to create an audio classification pipeline using a pre-trained model for speech emotion recognition.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/pipeline_tutorial.md#2025-04-23_snippet_8

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> audio_classifier = pipeline(
...     task="audio-classification", model="ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
... )
```

----------------------------------------

TITLE: Default Training Arguments
DESCRIPTION: Defines a dictionary containing the default training arguments for the Trainer. These arguments specify the output directory, evaluation strategy, number of training epochs, logging level, and reporting destinations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/model_memory_anatomy.md#_snippet_7

LANGUAGE: python
CODE:
```
default_args = {
    "output_dir": "tmp"ØŒ
    "eval_strategy": "steps"ØŒ
    "num_train_epochs": 1ØŒ
    "log_level": "error"ØŒ
    "report_to": "none"ØŒ
}
```

----------------------------------------

TITLE: Custom Generation Configuration
DESCRIPTION: Creates and saves custom generation configuration settings for different tasks
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig

tokenizer = AutoTokenizer.from_pretrained("google-t5/t5-small")
model = AutoModelForSeq2SeqLM.from_pretrained("google-t5/t5-small")

translation_generation_config = GenerationConfig(
    num_beams=4,
    early_stopping=True,
    decoder_start_token_id=0,
    eos_token_id=model.config.eos_token_id,
    pad_token=model.config.pad_token_id,
)

translation_generation_config.save_pretrained("/tmp", config_file_name="translation_generation_config.json", push_to_hub=True)

generation_config = GenerationConfig.from_pretrained("/tmp", config_file_name="translation_generation_config.json")
inputs = tokenizer("translate English to French: Configuration files are easy to use!", return_tensors="pt")
outputs = model.generate(**inputs, generation_config=generation_config)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
```

----------------------------------------

TITLE: Removing and Renaming Columns in Hugging Face Dataset
DESCRIPTION: This code snippet demonstrates how to remove a column ('text') and rename a column ('label' to 'labels') in a Hugging Face Dataset object. This is necessary to prepare the dataset for training in PyTorch. It modifies the tokenized dataset for appropriate input to the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_16

LANGUAGE: Python
CODE:
```
tokenized_datasets = tokenized_datasets.remove_columns(["text"])

tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
```

----------------------------------------

TITLE: Loading SQuAD Dataset Sample
DESCRIPTION: Loads a subset of the Stanford Question Answering Dataset (SQuAD) using the Datasets library. Only the first 5000 examples from the training split are loaded to allow for faster experimentation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/question_answering.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> squad = load_dataset("squad", split="train[:5000]")
```

----------------------------------------

TITLE: Default Data Collator Initialization in Python
DESCRIPTION: This section retrieves the default data collator implementation from the data_collator module, providing a standard method to batch input data for training or evaluation. It requires the Hugging Face Transformers library context.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/data_collator.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
[[autodoc]] data.data_collator.default_data_collator
```

----------------------------------------

TITLE: Loading BERT model to GPU
DESCRIPTION: Loads the `google-bert/bert-large-uncased` model directly onto the GPU using `AutoModelForSequenceClassification` from the `transformers` library. This allows verification of the memory occupied by the model's weights.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/model_memory_anatomy.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification


>>> model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-large-uncased").to("cuda")
>>> print_gpu_utilization()
GPU memory occupied: 2631 MB.
```

----------------------------------------

TITLE: Creating Data Collator for Language Modeling (TensorFlow)
DESCRIPTION: Code to create a data collator that dynamically pads sequences in each batch for efficient training in TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/language_modeling.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorForLanguageModeling

>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors="tf")
```

----------------------------------------

TITLE: Translation from Romanian to German using NLLB Model
DESCRIPTION: Example of translating text from Romanian to German using the NLLB model with source and target language codes
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/nllb.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained(
...     "facebook/nllb-200-distilled-600M", token=True, src_lang="ron_Latn"
... )
>>> model = AutoModelForSeq2SeqLM.from_pretrained("facebook/nllb-200-distilled-600M", token=True)

>>> article = "Åžeful ONU spune cÄƒ nu existÄƒ o soluÅ£ie militarÄƒ Ã®n Siria"
>>> inputs = tokenizer(article, return_tensors="pt")

>>> translated_tokens = model.generate(
...     **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids("deu_Latn"), max_length=30
... )
>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]
UN-Chef sagt, es gibt keine militÃ¤rische LÃ¶sung in Syrien
```

----------------------------------------

TITLE: Using Multilingual MarianMT Model in Python
DESCRIPTION: Demonstrates how to use a multilingual MarianMT model to translate text to multiple target languages by specifying language codes in the source text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/marian.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import MarianMTModel, MarianTokenizer

src_text = [
    ">>fra<< this is a sentence in english that we want to translate to french",
    ">>por<< This should go to portuguese",
    ">>esp<< And this to Spanish",
]

model_name = "Helsinki-NLP/opus-mt-en-roa"
tokenizer = MarianTokenizer.from_pretrained(model_name)
print(tokenizer.supported_language_codes)

model = MarianMTModel.from_pretrained(model_name)
translated = model.generate(**tokenizer(src_text, return_tensors="pt", padding=True))
[tokenizer.decode(t, skip_special_tokens=True) for t in translated]
```

----------------------------------------

TITLE: Few-shot Image Description with IDEFICS
DESCRIPTION: Demonstrates few-shot prompting to generate image descriptions with additional facts. Uses example of Eiffel Tower to teach the model the desired response format for describing the Statue of Liberty.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> prompt = ["User:",
...            "https://images.unsplash.com/photo-1543349689-9a4d426bee8e?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3501&q=80",
...            "Describe this image.\nAssistant: An image of the Eiffel Tower at night. Fun fact: the Eiffel Tower is the same height as an 81-storey building.\n",
...            "User:",
...            "https://images.unsplash.com/photo-1524099163253-32b7f0256868?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3387&q=80",
...            "Describe this image.\nAssistant:"]

>>> inputs = processor(prompt, return_tensors="pt").to("cuda")
>>> bad_words_ids = processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, max_new_tokens=30, bad_words_ids=bad_words_ids)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> print(generated_text[0])
```

----------------------------------------

TITLE: Applying Dataset Transformations
DESCRIPTION: This code applies the transform functions to the datasets on-the-fly, which saves disk space and processing time compared to preprocessing the entire dataset at once.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/sequence_classification.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```

----------------------------------------

TITLE: Applying Preprocessing
DESCRIPTION: Applies the earlier defined preprocessing function to the entire dataset with batching for efficiency using Datasets' map function.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/token_classification.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)
```

----------------------------------------

TITLE: Performing Translation with MBart Model in Python
DESCRIPTION: This snippet shows how to perform translation using the MBart model. It includes tokenizing the input text and generating the translation with the specified target language.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/multilingual.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
encoded_en = tokenizer(en_text, return_tensors="pt")
generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id("en_XX"))
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
```

----------------------------------------

TITLE: Evaluate Model Perplexity (PyTorch)
DESCRIPTION: Evaluates the fine-tuned model by calculating perplexity using the `evaluate()` method of the `Trainer`. It imports the `math` module to calculate the exponential of the evaluation loss. Requires a trained `trainer` object.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_15

LANGUAGE: python
CODE:
```
>>> import math

>>> eval_results = trainer.evaluate()
>>> print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```

----------------------------------------

TITLE: Custom Device Mapping
DESCRIPTION: This example shows how to define a custom device mapping for a model. It provides a dictionary that maps each layer or group of layers to a specific device. This allows for fine-grained control over device placement.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/model.md#_snippet_3

LANGUAGE: python
CODE:
```
device_map = {"shared": 0, "encoder": 0, "decoder": 1, "lm_head": 1}
```

----------------------------------------

TITLE: Enabling Flash Attention with BetterTransformer in PyTorch
DESCRIPTION: This code snippet shows how to enable Flash Attention when using BetterTransformer by utilizing the `torch.nn.kernel.sdpa_kernel` context manager. It loads a pretrained model, converts it to BetterTransformer, and then performs inference with Flash Attention enabled within the context manager. Requires PyTorch 2.0 or later.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_infer_gpu_many.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from torch.nn.attention import SDPBackend, sdpa_kernel
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m").to("cuda")
# convert the model to BetterTransformer
model.to_bettertransformer()

input_text = "Hello my dog is cute and"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

with sdpa_kernel(SDPBackend.FLASH_ATTENTION):
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Preprocessing and Inference for Semantic Segmentation with PyTorch
DESCRIPTION: Loads an image processor, preprocesses an image, and performs semantic segmentation using a PyTorch model. The code includes loading the processor, running inference, and upsampling the logits to match the original image size.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md#2025-04-22_snippet_21

LANGUAGE: python
CODE:
```
device, _, _ = get_backend()
encoding = image_processor(image, return_tensors="pt")
pixel_values = encoding.pixel_values.to(device)
```

LANGUAGE: python
CODE:
```
outputs = model(pixel_values=pixel_values)
logits = outputs.logits.cpu()
```

LANGUAGE: python
CODE:
```
upsampled_logits = nn.functional.interpolate(
    logits,
    size=image.size[::-1],
    mode="bilinear",
    align_corners=False,
)

pred_seg = upsampled_logits.argmax(dim=1)[0]
```

----------------------------------------

TITLE: Integrating HIGGS Quantization with torch.compile in Python
DESCRIPTION: This snippet integrates HIGGS quantization with the torch.compile feature for improved inference speed. The model is initially configured for quantization and then compiled with torch.compile. This method is compatible with pytorch and requires the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/higgs.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, HiggsConfig

model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2-9b-it",
    quantization_config=HiggsConfig(bits=4),
    device_map="auto",
)

model = torch.compile(model)
```

----------------------------------------

TITLE: Running Hyperparameter Search with Transformers Trainer API
DESCRIPTION: Example of executing a hyperparameter search using the Transformers Trainer API, specifying optimization direction, backend, search space, number of trials, and custom objective function.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/hpo_train.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
best_trial = trainer.hyperparameter_search(
    direction="maximize",
    backend="optuna",
    hp_space=optuna_hp_space,
    n_trials=20,
    compute_objective=compute_objective,
)
```

----------------------------------------

TITLE: Resume Training from Checkpoint (resume_from_checkpoint)
DESCRIPTION: This code snippet demonstrates how to resume training from a specific checkpoint folder. The `resume_from_checkpoint path_to_specific_checkpoint` argument is used to specify the path to the checkpoint folder.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/run_scripts_fr.md#_snippet_16

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --resume_from_checkpoint path_to_specific_checkpoint \
    --predict_with_generate
```

----------------------------------------

TITLE: Examining Dataset Structure and Labels
DESCRIPTION: Demonstrates how to inspect a dataset example and convert numeric NER tags to their corresponding label names to understand the entity types in the dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/token_classification.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> wnut["train"][0]
{'id': '0',
 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0],
 'tokens': ['@paulwalk', 'It', "'s", 'the', 'view', 'from', 'where', 'I', "'m", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.']}

```

LANGUAGE: python
CODE:
```
>>> label_list = wnut["train"].features[f"ner_tags"].feature.names
>>> label_list
[
    "O",
    "B-corporation",
    "I-corporation",
    "B-creative-work",
    "I-creative-work",
    "B-group",
    "I-group",
    "B-location",
    "I-location",
    "B-person",
    "I-person",
    "B-product",
    "I-product",
]
```

----------------------------------------

TITLE: Fine-tuning the ViLT Model for VQA
DESCRIPTION: Initiates the training process to fine-tune the ViLT model on the VQA dataset, adapting its pre-trained parameters to the specific task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/visual_question_answering.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
>>> trainer.train()
```

----------------------------------------

TITLE: Encoding the Full Dataset with the Processing Function
DESCRIPTION: Applies the encoding function to process the entire dataset, converting both training and testing splits. This transforms the raw dataset into a format suitable for training a question answering model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/document_question_answering.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
>>> encoded_train_dataset = dataset_with_ocr["train"].map(
...     encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr["train"].column_names
... )
>>> encoded_test_dataset = dataset_with_ocr["test"].map(
...     encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr["test"].column_names
... )
```

----------------------------------------

TITLE: Loading IDEFICS Model in Python
DESCRIPTION: The snippet shows how to load the IDEFICS model with 9 billion parameters and set up a processor using Transformers library. It involves initializing both the processor and model, which enable transformation and management of image and text input data.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md#2025-04-22_snippet_1

LANGUAGE: py
CODE:
```
>>> checkpoint = "HuggingFaceM4/idefics-9b"

>>> import torch

>>> from transformers import IdeficsForVisionText2Text, AutoProcessor

>>> processor = AutoProcessor.from_pretrained(checkpoint)

>>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map="auto")
```

----------------------------------------

TITLE: Loading Pre-trained VideoMAE Model for Fine-tuning
DESCRIPTION: Initializes a VideoMAE model and image processor from a pre-trained checkpoint for fine-tuning on video classification. Sets up label mappings and handles potential weight mismatches.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/video_classification.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification

>>> model_ckpt = "MCG-NJU/videomae-base"
>>> image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)
>>> model = VideoMAEForVideoClassification.from_pretrained(
...     model_ckpt,
...     label2id=label2id,
...     id2label=id2label,
...     ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint
... )
```

----------------------------------------

TITLE: Implementing ROUGE Metric Computation Function
DESCRIPTION: Defines a compute_metrics function that processes model predictions and labels, decodes them using the tokenizer, and calculates ROUGE scores. The function also reports the average length of predictions as additional information.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)

    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]
    result["gen_len"] = np.mean(prediction_lens)

    return {k: round(v, 4) for k, v in result.items()}
```

----------------------------------------

TITLE: Auto-regressive Text Generation without Key-Value Cache in PyTorch
DESCRIPTION: This code snippet demonstrates auto-regressive text generation without using the key-value cache. It iteratively feeds the generated token back into the model to produce the next token, increasing the input sequence length with each step. The process is repeated for a fixed number of iterations and relies on the Hugging Face Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_26

LANGUAGE: python
CODE:
```
input_ids = tokenizer(prompt, return_tensors="pt")["input_ids"].to("cuda")

for _ in range(5):
  next_logits = model(input_ids)["logits"][:, -1:]
  next_token_id = torch.argmax(next_logits,dim=-1)

  input_ids = torch.cat([input_ids, next_token_id], dim=-1)
  print("shape of input_ids", input_ids.shape)

generated_text = tokenizer.batch_decode(input_ids[:, -5:])
generated_text
```

----------------------------------------

TITLE: Saving a Quantized Model Locally
DESCRIPTION: This code saves a quantized model and tokenizer locally. If the model was quantized with `device_map`, it's moved to the CPU before saving. It relies on the `transformers` library and a quantized model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/gptq.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
"quantized_model.save_pretrained(\"opt-125m-gptq\")\ntokenizer.save_pretrained(\"opt-125m-gptq\")\n\n# if quantized with device_map set\nquantized_model.to(\"cpu\")\nquantized_model.save_pretrained(\"opt-125m-gptq\")"
```

----------------------------------------

TITLE: Setting Up Training Arguments for Hugging Face Trainer
DESCRIPTION: Creates a TrainingArguments object to specify hyperparameters and training configuration for the fine-tuning process, including where to save model checkpoints.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/training.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments

training_args = TrainingArguments(output_dir="test_trainer")
```

----------------------------------------

TITLE: Running BART Summarization Training
DESCRIPTION: Command to train a BART-based summarization model on the CNN/DailyMail dataset. Configures model parameters, dataset settings, output directory, batch sizes, and training epochs with evaluation.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/tensorflow/summarization/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python run_summarization.py  \
--model_name_or_path facebook/bart-base \
--dataset_name cnn_dailymail \
--dataset_config "3.0.0" \
--output_dir /tmp/tst-summarization  \
--per_device_train_batch_size 8 \
--per_device_eval_batch_size 16 \
--num_train_epochs 3 \
--do_train \
--do_eval
```

----------------------------------------

TITLE: Implementing Custom Loss Trainer in Python
DESCRIPTION: Example showing how to customize the Trainer class to use weighted loss for handling imbalanced training sets by overriding the compute_loss method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/trainer.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from torch import nn
from transformers import Trainer

class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        # forward pass
        outputs = model(**inputs)
        logits = outputs.get("logits")
        # compute custom loss (suppose one has 3 labels with different weights)
        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0], device=model.device))
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss
```

----------------------------------------

TITLE: Launching Inference with Torchrun (Bash)
DESCRIPTION: This Bash command launches a PyTorch inference script using torchrun with specified processes per node. It is designed to facilitate running the inference on multiple GPUs. It requires PyTorch to be installed and properly configured for distributed training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_multi.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
torchrun --nproc-per-node 4 demo.py
```

----------------------------------------

TITLE: Defining System Prompt for LLM Coding Assistant
DESCRIPTION: This code snippet defines a system prompt that sets up the context for an AI coding assistant. It includes instructions on the assistant's behavior and information about the Starcoder model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
system_prompt = """Below are a series of dialogues between various people and an AI technical assistant.
The assistant tries to be helpful, polite, honest, sophisticated, emotionally aware, and humble but knowledgeable.
The assistant is happy to help with code questions and will do their best to understand exactly what is needed.
It also tries to avoid giving false or misleading information, and it caveats when it isn't entirely sure about the right answer.
That said, the assistant is practical really does its best, and doesn't let caution get too much in the way of being useful.

The Starcoder models are a series of 15.5B parameter models trained on 80+ programming languages from The Stack (v1.2) (excluding opt-out requests).
The model uses Multi Query Attention, was trained using the Fill-in-the-Middle objective, and with 8,192 tokens context window for a trillion tokens of heavily deduplicated data.

-----

Question: Write a function that takes two lists and returns a list that has alternating elements from each input list.

Answer: Sure. Here is a function that does that.

def alternating(list1, list2):
   results = []
   for i in range(len(list1)):
       results.append(list1[i])
       results.append(list2[i])
   return results

Question: Can you write some test cases for this function?

Answer: Sure, here are some tests.

assert alternating([10, 20, 30], [1, 2, 3]) == [10, 1, 20, 2, 30, 3]
assert alternating([True, False], [4, 5]) == [True, 4, False, 5]
assert alternating([], []) == []

Question: Modify the function so that it returns all input elements when the lists have uneven length. The elements from the longer list should be at the end.

Answer: Here is the modified function.

def alternating(list1, list2):
   results = []
   for i in range(min(len(list1), len(list2))):
       results.append(list1[i])
       results.append(list2[i])
   if len(list1) > len(list2):
       results.extend(list1[i+1:])
   else:
       results.extend(list2[i+1:])
   return results

-----
"""
```

----------------------------------------

TITLE: Exporting a Pre-trained Model to ONNX Format
DESCRIPTION: Command to export a pre-trained DistilBERT model from Hugging Face Hub to ONNX format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/serialization.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
python -m transformers.onnx --model=distilbert/distilbert-base-uncased onnx/
```

----------------------------------------

TITLE: Extracting Car Mask from Semantic Segmentation Results in Python
DESCRIPTION: This snippet extracts the segmentation mask for the 'car' class from the results of the semantic segmentation. It illustrates how to access the mask data corresponding to a specific label.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
results[-1]["mask"]
```

----------------------------------------

TITLE: Installing Accelerate Library
DESCRIPTION: This code snippet demonstrates how to install the Accelerate library, which is required for distributed PyTorch training within the Transformers library. It also shows how to upgrade the Accelerate library if it is already installed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/trainer.md#_snippet_0

LANGUAGE: bash
CODE:
```
pip install accelerate

# upgrade
pip install accelerate --upgrade
```

----------------------------------------

TITLE: Batch Processing for Multiple Images
DESCRIPTION: This snippet demonstrates how to perform batch processing by passing multiple images and associated text queries to the model for simultaneous object detection.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> images = [image, im]
>>> text_queries = [
...     ["human face", "rocket", "nasa badge", "star-spangled banner"],
...     ["hat", "book", "sunglasses", "camera"],
... ]
>>> inputs = processor(text=text_queries, images=images, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)
...     target_sizes = [x.size[::-1] for x in images]
...     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)

>>> image_idx = 1
>>> draw = ImageDraw.Draw(images[image_idx])

>>> scores = results[image_idx]["scores"].tolist()
>>> labels = results[image_idx]["labels"].tolist()
>>> boxes = results[image_idx]["boxes"].tolist()

>>> for box, score, label in zip(boxes, scores, labels):
...     xmin, ymin, xmax, ymax = box
...     draw.rectangle((xmin, ymin, xmax, ymax), outline="red", width=1)
...     draw.text((xmin, ymin), f"{text_queries[image_idx][label]}: {round(score,2)}", fill="white")

>>> images[image_idx]
```

----------------------------------------

TITLE: Initializing Data Collator for Question Answering
DESCRIPTION: Creates a DefaultDataCollator instance to handle batching of preprocessed examples.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/question_answering.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator()
```

LANGUAGE: python
CODE:
```
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors="tf")
```

----------------------------------------

TITLE: Implementing Custom Knowledge Distillation Trainer for Computer Vision
DESCRIPTION: Creates a custom trainer that extends the Hugging Face Trainer class to implement knowledge distillation. The trainer computes KL divergence loss between teacher and student model outputs with temperature scaling and combines it with the standard classification loss.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/knowledge_distillation_for_image_classification.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments, Trainer
import torch
import torch.nn as nn
import torch.nn.functional as F


class ImageDistilTrainer(Trainer):
    def __init__(self, teacher_model=None, student_model=None, temperature=None, lambda_param=None,  *args, **kwargs):
        super().__init__(model=student_model, *args, **kwargs)
        self.teacher = teacher_model
        self.student = student_model
        self.loss_function = nn.KLDivLoss(reduction="batchmean")
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.teacher.to(device)
        self.teacher.eval()
        self.temperature = temperature
        self.lambda_param = lambda_param

    def compute_loss(self, student, inputs, return_outputs=False):
        student_output = self.student(**inputs)

        with torch.no_grad():
          teacher_output = self.teacher(**inputs)

        #  êµì‚¬ì™€ í•™ìƒì˜ ì†Œí”„íŠ¸ íƒ€ê²Ÿ(soft targets) ê³„ì‚°

        soft_teacher = F.softmax(teacher_output.logits / self.temperature, dim=-1)
        soft_student = F.log_softmax(student_output.logits / self.temperature, dim=-1)

        # ì†ì‹¤(loss) ê³„ì‚°
        distillation_loss = self.loss_function(soft_student, soft_teacher) * (self.temperature ** 2)

        # ì‹¤ì œ ë ˆì´ë¸” ì†ì‹¤ ê³„ì‚°
        student_target_loss = student_output.loss

        # ìµœì¢… ì†ì‹¤ ê³„ì‚°
        loss = (1. - self.lambda_param) * student_target_loss + self.lambda_param * distillation_loss
        return (loss, student_output) if return_outputs else loss
```

----------------------------------------

TITLE: Saving Full State Dict in Python
DESCRIPTION: This code snippet saves the full state dictionary of a model after FSDP training is complete. It checks whether FSDP is enabled before setting and saving the state dict. This requires the 'accelerate' library and is relevant when the training uses FSDP.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/fsdp.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
if trainer.is_fsdp_enabled:
  trainer.accelerator.state.fsdp_plugin.set_state_dict_type("FULL_STATE_DICT")

trainer.save_model(script_args.output_dir)
```

----------------------------------------

TITLE: Enabling Scaled Dot Product Attention (SDPA) in AutoModelForCausalLM
DESCRIPTION: This example demonstrates how to explicitly enable the Scaled Dot Product Attention (SDPA) mechanism by setting `attn_implementation="sdpa"` when loading a pre-trained model using `AutoModelForCausalLM.from_pretrained`. SDPA offers a more efficient and optimized attention mechanism.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.1-8B", device_map="auto", attn_implementation="sdpa")
```

----------------------------------------

TITLE: Configuring Training Arguments for Wav2Vec2 Model
DESCRIPTION: Define comprehensive training hyperparameters for model fine-tuning, including batch sizes, learning rate, and evaluation strategies
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/asr.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
training_args = TrainingArguments(
    output_dir="my_awesome_asr_mind_model",
    per_device_train_batch_size=8,
    gradient_accumulation_steps=2,
    learning_rate=1e-5,
    warmup_steps=500,
    max_steps=2000,
    gradient_checkpointing=True,
    fp16=True,
    group_by_length=True,
    eval_strategy="steps",
    per_device_eval_batch_size=8,
    save_steps=1000,
    eval_steps=1000,
    logging_steps=25,
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    push_to_hub=True,
)
```

----------------------------------------

TITLE: Converting PyTorch Model to TensorFlow
DESCRIPTION: Demonstrates how to convert a PyTorch checkpoint to a TensorFlow model using the from_pt parameter.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_sharing.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
tf_model = TFDistilBertForSequenceClassification.from_pretrained("path/to/awesome-name-you-picked", from_pt=True)
tf_model.save_pretrained("path/to/awesome-name-you-picked")
```

----------------------------------------

TITLE: Training the CLIP-like Model with Command Line Arguments
DESCRIPTION: Bash command to run the training script for the vision-text dual encoder model using the COCO dataset. This includes configuration for batch sizes, learning rate, and output options for fine-tuning the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/contrastive-image-text/README.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
python examples/pytorch/contrastive-image-text/run_clip.py \
    --output_dir ./clip-roberta-finetuned \
    --model_name_or_path ./clip-roberta \
    --data_dir $PWD/data \
    --dataset_name ydshieh/coco_dataset_script \
    --dataset_config_name=2017 \
    --image_column image_path \
    --caption_column caption \
    --remove_unused_columns=False \
    --do_train  --do_eval \
    --per_device_train_batch_size="64" \
    --per_device_eval_batch_size="64" \
    --learning_rate="5e-5" --warmup_steps="0" --weight_decay 0.1 \
    --overwrite_output_dir \
    --push_to_hub
```

----------------------------------------

TITLE: Creating Wav2Vec2Processor in Python
DESCRIPTION: This code snippet demonstrates the creation of a Wav2Vec2Processor, which combines a feature extractor and a tokenizer. This processor is used for handling both audio and text inputs for tasks like automatic speech recognition (ASR).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/create_a_model.md#_snippet_30

LANGUAGE: python
CODE:
```
>>> from transformers import Wav2Vec2Processor

>>> processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)
```

----------------------------------------

TITLE: Compile TensorFlow Model
DESCRIPTION: Compiles the TensorFlow model with the specified optimizer, preparing it for training. This configures the training process within TensorFlow. Requires `tensorflow` and a defined `optimizer` object.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_20

LANGUAGE: python
CODE:
```
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)
```

----------------------------------------

TITLE: Data Preprocessing Function
DESCRIPTION: Function to preprocess the SWAG dataset by tokenizing and formatting the input text for model training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/multiple_choice.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
ending_names = ["ending0", "ending1", "ending2", "ending3"]

def preprocess_function(examples):
    first_sentences = [[context] * 4 for context in examples["sent1"]]
    question_headers = examples["sent2"]
    second_sentences = [
        [f"{header} {examples[end][i]}" for end in ending_names] for i, header in enumerate(question_headers)
    ]

    first_sentences = sum(first_sentences, [])
    second_sentences = sum(second_sentences, [])

    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)
    return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}
```

----------------------------------------

TITLE: Creating Dataset Subset for Training
DESCRIPTION: Creates smaller training and evaluation datasets for faster experimentation and testing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/training.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
```

----------------------------------------

TITLE: Streaming Text Generation with TextStreamer
DESCRIPTION: This snippet illustrates how to use the `TextStreamer` class to stream generated text to the console.  A `TextStreamer` object is created using a tokenizer, and then passed to the `generate` method via the `streamer` argument.  The generated text will be printed to standard output as it is produced, while the `generate` method will still return the full output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/generation_strategies.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

>>> tok = AutoTokenizer.from_pretrained("openai-community/gpt2")
>>> model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")
>>> inputs = tok(["An increasing sequence: one,"], return_tensors="pt")
>>> streamer = TextStreamer(tok)

>>> # ìŠ¤íŠ¸ë¦¬ë¨¸ëŠ” í‰ì†Œì™€ ê°™ì€ ì¶œë ¥ê°’ì„ ë°˜í™˜í•  ë¿ë§Œ ì•„ë‹ˆë¼ ìƒì„±ëœ í…ìŠ¤íŠ¸ë„ í‘œì¤€ ì¶œë ¥(stdout)ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.
>>> _ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)
An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,
```

----------------------------------------

TITLE: Using VisionTextDualEncoderProcessor in Python
DESCRIPTION: The VisionTextDualEncoderProcessor class combines image and text processors for preparing inputs to the dual encoder model. It handles both image and text preprocessing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/vision-text-dual-encoder.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
class VisionTextDualEncoderProcessor(ProcessorMixin):
    attributes = ["image_processor", "tokenizer"]
    image_processor_class = "AutoImageProcessor"
    tokenizer_class = "AutoTokenizer"

    def __init__(self, image_processor=None, tokenizer=None, **kwargs):
        super().__init__(**kwargs)
        self.image_processor = image_processor
        self.tokenizer = tokenizer

    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):
        return super().from_pretrained(pretrained_model_name_or_path, **kwargs)

    def save_pretrained(self, save_directory, **kwargs):
        return super().save_pretrained(save_directory, **kwargs)
```

----------------------------------------

TITLE: Using IDEFICS Instruct for Conversational Scenarios in Python
DESCRIPTION: This code snippet illustrates how to use the fine-tuned instructed versions of IDEFICS for conversational use cases. It demonstrates model and processor initialization, input preparation, and generation of responses in a conversational context.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> import torch
>>> from transformers import IdeficsForVisionText2Text, AutoProcessor
>>> from accelerate.test_utils.testing import get_backend

>>> device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)
>>> checkpoint = "HuggingFaceM4/idefics-9b-instruct"
>>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)
>>> processor = AutoProcessor.from_pretrained(checkpoint)

>>> prompts = [
...     [
...         "User: What is in this image?",
...         "https://upload.wikimedia.org/wikipedia/commons/8/86/Id%C3%A9fix.JPG",
...         "<end_of_utterance>",

...         "\nAssistant: This picture depicts Idefix, the dog of Obelix in Asterix and Obelix. Idefix is running on the ground.<end_of_utterance>",

...         "\nUser:",
...         "https://static.wikia.nocookie.net/asterix/images/2/25/R22b.gif/revision/latest?cb=20110815073052",
...         "And who is that?<end_of_utterance>",

...         "\nAssistant:",
...     ],
... ]

>>> # --batched mode
>>> inputs = processor(prompts, add_end_of_utterance_token=False, return_tensors="pt").to(device)
>>> # --single sample mode
>>> # inputs = processor(prompts[0], return_tensors="pt").to(device)

>>> # Generation args
>>> exit_condition = processor.tokenizer("<end_of_utterance>", add_special_tokens=False).input_ids
>>> bad_words_ids = processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, eos_token_id=exit_condition, bad_words_ids=bad_words_ids, max_length=100)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> for i, t in enumerate(generated_text):
...     print(f"{i}:\n{t}\n")
```

----------------------------------------

TITLE: Evaluating PyTorch Model Performance with Metrics
DESCRIPTION: Evaluates the fine-tuned model's performance on the evaluation dataset, calculating accuracy metrics batch by batch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/training.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
metric = load_metric("accuracy")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

----------------------------------------

TITLE: Fine-tuning T5-small for Summarization (TensorFlow)
DESCRIPTION: This Python command runs the summarization example script using TensorFlow. It fine-tunes the `google-t5/t5-small` model on the `cnn_dailymail` dataset, specifying training and evaluation parameters. The `source_prefix` is a crucial parameter for T5, informing it that the task is summarization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/run_scripts.md#_snippet_4

LANGUAGE: bash
CODE:
```
python examples/tensorflow/summarization/run_summarization.py  \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval
```

----------------------------------------

TITLE: Processing Image with Mask and Point Prompt (Python)
DESCRIPTION: Shows how to use the SamHQProcessor to handle an input image along with an existing segmentation map and a 2D point prompt, then runs inference with the model to get updated masks and scores. Requires `torch`, `PIL`, `requests`, and `transformers` libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/sam_hq.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from PIL import Image
import requests
from transformers import SamHQModel, SamHQProcessor

device = "cuda" if torch.cuda.is_available() else "cpu"
model = SamHQModel.from_pretrained("sushmanth/sam_hq_vit_b").to(device)
processor = SamHQProcessor.from_pretrained("sushmanth/sam_hq_vit_b")

img_url = "https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png"
raw_image = Image.open(requests.get(img_url, stream=True).raw).convert("RGB")
mask_url = "https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png"
segmentation_map = Image.open(requests.get(mask_url, stream=True).raw).convert("1")
input_points = [[[450, 600]]]  # 2D location of a window in the image

inputs = processor(raw_image, input_points=input_points, segmentation_maps=segmentation_map, return_tensors="pt").to(device)
with torch.no_grad():
    outputs = model(**inputs)

masks = processor.image_processor.post_process_masks(
    outputs.pred_masks.cpu(), inputs["original_sizes"].cpu(), inputs["reshaped_input_sizes"].cpu()
)
scores = outputs.iou_scores
```

----------------------------------------

TITLE: Loading Pretrained TensorFlow Models for Different Tasks with TFAutoModelFor Classes in Python
DESCRIPTION: These snippets demonstrate how to load pretrained TensorFlow models for sequence classification and token classification tasks using TFAutoModelFor classes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/autoclass_tutorial.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")

>>> from transformers import TFAutoModelForTokenClassification

>>> model = TFAutoModelForTokenClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Exporting Model to TorchScript
DESCRIPTION: Shows how to trace and save a model to TorchScript format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/torchscript.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])
torch.jit.save(traced_model, "traced_bert.pt")
```

----------------------------------------

TITLE: Creating Data Collator for Padding
DESCRIPTION: Initializes `DataCollatorForTokenClassification` for dynamic padding of input data during batch collation, supporting both PyTorch and TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/token_classification.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorForTokenClassification

>>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorForTokenClassification

>>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors="tf")
```

----------------------------------------

TITLE: Training RoBERTa with Masked Language Modeling
DESCRIPTION: Fine-tunes RoBERTa on WikiText-2 using masked language modeling with dynamic masking. Configures batch sizes and training parameters for bidirectional model training.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/README.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
python run_mlm.py \
    --model_name_or_path FacebookAI/roberta-base \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-mlm
```

----------------------------------------

TITLE: Examining Dataset Structure
DESCRIPTION: Displays a sample from the training dataset to understand its structure, which contains question IDs, titles, texts, categories, and nested answer fields that need to be processed for language modeling.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/language_modeling.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
>>> eli5["train"][0]
```

----------------------------------------

TITLE: Examining a SQuAD Dataset Example
DESCRIPTION: Displays a sample from the training dataset to understand its structure, showing the question, context passage, answer text, and answer position within the context.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/question_answering.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> squad["train"][0]
{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']},
 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',
 'id': '5733be284776f41900661182',
 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',
 'title': 'University_of_Notre_Dame'
}
```

----------------------------------------

TITLE: Enabling torch.empty_cache_steps in TrainingArguments
DESCRIPTION: This code snippet demonstrates how to enable `torch.cuda.empty_cache` after a certain number of training steps using `torch_empty_cache_steps` in `TrainingArguments`. This helps avoid out-of-memory errors at the cost of slightly slower training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments

args = TrainingArguments(
    per_device_train_batch_size=4,
    gradient_accumulation_steps=16,
    gradient_checkpointing=True,
    bf16=True,
    optim="adamw_bnb_8bit",
    dataloader_pin_memory=True,
    dataloader_num_workers=4,
    torch_empty_cache_steps=4,
)
```

----------------------------------------

TITLE: Performing Document Question Answering with LayoutLM
DESCRIPTION: This snippet demonstrates document question answering using a multimodal approach. It loads an image of a document, asks a specific question about the content, and uses the LayoutLM model to extract the answer from the visual document.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/task_summary.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline
>>> from PIL import Image
>>> import requests

>>> url = "https://huggingface.co/datasets/hf-internal-testing/example-documents/resolve/main/jpeg_images/2.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> doc_question_answerer = pipeline("document-question-answering", model="magorshunov/layoutlm-invoices")
>>> preds = doc_question_answerer(
...     question="What is the total amount?",
...     image=image,
... )
>>> preds
[{'score': 0.8531, 'answer': '17,000', 'start': 4, 'end': 4}]
```

----------------------------------------

TITLE: Quantization with Bitsandbytes for Chameleon
DESCRIPTION: Demonstrates how to quantize the Chameleon model using Bitsandbytes to reduce memory requirements while maintaining performance. This example shows how to configure Bitsandbytes for 4-bit quantization with specific data types and then load the model with the specified quantization configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/chameleon.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import ChameleonForConditionalGeneration, BitsAndBytesConfig

# specify how to quantize the model
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

model = ChameleonForConditionalGeneration.from_pretrained("facebook/chameleon-7b", quantization_config=quantization_config, device_map="cuda")
```

----------------------------------------

TITLE: Text Translation with Falcon-7b-instruct in Python
DESCRIPTION: This code shows how to use the Falcon-7b-instruct model for text translation from English to Italian with sampling parameters to make the output more flexible.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/prompting.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> torch.manual_seed(2) # doctest: +IGNORE_RESULT
>>> prompt = """Translate the English text to Italian.
... Text: Sometimes, I've believed as many as six impossible things before breakfast.
... Translation:
... """

>>> sequences = pipe(
...     prompt,
...     max_new_tokens=20,
...     do_sample=True,
...     top_k=10,
...     return_full_text = False,
... )

>>> for seq in sequences:
...     print(f"{seq['generated_text']}")
A volte, ho creduto a sei impossibili cose prima di colazione.
```

----------------------------------------

TITLE: Manual Inference Process
DESCRIPTION: Demonstrates manual inference process including model loading, text processing, and audio generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/text-to-speech.md#2025-04-22_snippet_30

LANGUAGE: python
CODE:
```
model = SpeechT5ForTextToSpeech.from_pretrained("YOUR_ACCOUNT/speecht5_finetuned_voxpopuli_nl")
example = dataset["test"][304]
speaker_embeddings = torch.tensor(example["speaker_embeddings"]).unsqueeze(0)
text = "hallo allemaal, ik praat nederlands. groetjes aan iedereen!"
inputs = processor(text=text, return_tensors="pt")
spectrogram = model.generate_speech(inputs["input_ids"], speaker_embeddings)

with torch.no_grad():
    speech = vocoder(spectrogram)

from IPython.display import Audio
Audio(speech.numpy(), rate=16000)
```

----------------------------------------

TITLE: Preparing Training Objects with Accelerator
DESCRIPTION: Example of using the prepare method to set up training objects for distributed processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/accelerate.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)
```

----------------------------------------

TITLE: Fine-tuning SegFormer for Semantic Segmentation using Trainer API in Bash
DESCRIPTION: This bash script demonstrates how to fine-tune a SegFormer model on the sidewalk-semantic dataset using the Transformers Trainer API. It includes various training parameters and options for logging and pushing to the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/semantic-segmentation/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
python run_semantic_segmentation.py \
    --model_name_or_path nvidia/mit-b0 \
    --dataset_name segments/sidewalk-semantic \
    --output_dir ./segformer_outputs/ \
    --remove_unused_columns False \
    --do_train \
    --do_eval \
    --push_to_hub \
    --push_to_hub_model_id segformer-finetuned-sidewalk-10k-steps \
    --max_steps 10000 \
    --learning_rate 0.00006 \
    --lr_scheduler_type polynomial \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --logging_strategy steps \
    --logging_steps 100 \
    --eval_strategy epoch \
    --save_strategy epoch \
    --seed 1337
```

----------------------------------------

TITLE: Using Exported ONNX Model with ORTModelForQuestionAnswering
DESCRIPTION: Python code demonstrating how to load and use an exported ONNX model with ONNX Runtime
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/serialization.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import ORTModelForQuestionAnswering

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> model = ORTModelForQuestionAnswering.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> inputs = tokenizer("What am I using?", "Using DistilBERT with ONNX Runtime!", return_tensors="pt")
>>> outputs = model(**inputs)
```

----------------------------------------

TITLE: Exporting a Local Model to ONNX via CLI
DESCRIPTION: This command exports a locally stored ðŸ¤— Transformers model to ONNX format using optimum-cli.  The `--model` argument specifies the local path to the model, and the `--task` argument specifies the task associated with the model.  The result is saved to the specified output directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/serialization.md#_snippet_3

LANGUAGE: bash
CODE:
```
optimum-cli export onnx --model local_path --task question-answering distilbert_base_uncased_squad_onnx/
```

----------------------------------------

TITLE: Running Depth Estimation Inference Using Pipeline
DESCRIPTION: Processes an image through the depth estimation pipeline to generate depth predictions and visualization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/monocular_depth_estimation.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> predictions = depth_estimator(image)
```

----------------------------------------

TITLE: Applying Preprocessing to MInDS-14 Dataset
DESCRIPTION: Applies the preprocessing function to the entire MInDS-14 dataset using parallel processing and removes unnecessary columns.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/asr.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> encoded_minds = minds.map(prepare_dataset, remove_columns=minds.column_names["train"], num_proc=4)
```

----------------------------------------

TITLE: Setting Up Optimizer for TensorFlow Training
DESCRIPTION: Code to configure an AdamWeightDecay optimizer for training a masked language model in TensorFlow, with specified learning rate and weight decay parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
from transformers import create_optimizer, AdamWeightDecay

optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

----------------------------------------

TITLE: Setting up Static Cache for Text Generation - Python
DESCRIPTION: This snippet initializes a static cache before generating text with the model. It defines parameters like maximum batch size and sequence length to optimize performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_optims.md#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
past_key_values = StaticCache(
    config=model.config, max_batch_size=2, max_cache_len=4096, device=torch_device, dtype=model.dtype
)
cache_position = torch.arange(seq_length, device=torch_device)
generated_ids = torch.zeros(
    batch_size, seq_length + NUM_TOKENS_TO_GENERATE + 1, dtype=torch.int, device=torch_device
)
generated_ids[:, cache_position] = inputs["input_ids"].to(torch_device).to(torch.int)
```

----------------------------------------

TITLE: Quantizing with AutoRoundBest recipe (Python)
DESCRIPTION: Demonstrates using the `AutoRoundBest` recipe for quantization, which prioritizes accuracy but is slower.  It sets additional parameters like `nsamples` and `iters` for more fine-grained control of the quantization process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/auto_round.md#_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer
from auto_round import AutoRound

model_name = "facebook/opt-125m"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)
bits, group_size, sym = 4, 128, True
autoround = AutoRound(
    model,
    tokenizer,
    bits=bits,
    group_size=group_size,
    sym=sym,
    nsamples=512,
    iters=1000,
    low_gpu_mem_usage=True
)

output_dir = "./tmp_autoround"
autoround.quantize_and_save(output_dir, format='auto_round') 
```

----------------------------------------

TITLE: Initializing MLukeTokenizer in Python
DESCRIPTION: This code snippet shows how to initialize the MLukeTokenizer which is specific to the mLUKE model. The tokenizer must be loaded with the correct pre-trained model weights to function properly.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mluke.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import MLukeTokenizer

tokenizer = MLukeTokenizer.from_pretrained("studio-ousia/mluke-base")
```

----------------------------------------

TITLE: Arabic to English Translation with mBART-50
DESCRIPTION: Demonstrates Arabic to English translation using mBART-50 with proper language code handling and forced BOS token generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mbart.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-50-many-to-many-mmt", torch_dtype=torch.bfloat16, attn_implementation="sdpa", device_map="auto")
tokenizer = MBartTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")

article_ar = "Ø§Ù„Ø£Ù…ÙŠÙ† Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø© ÙŠÙ‚ÙˆÙ„ Ø¥Ù†Ù‡ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø­Ù„ Ø¹Ø³ÙƒØ±ÙŠ ÙÙŠ Ø³ÙˆØ±ÙŠØ§."
tokenizer.src_lang = "ar_AR"

encoded_ar = tokenizer(article_ar, return_tensors="pt")
generated_tokens = model.generate(**encoded_ar, forced_bos_token_id=tokenizer.lang_code_to_id["en_XX"])
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
```

----------------------------------------

TITLE: Loading the Tokenizer for DistilGPT2
DESCRIPTION: This Python code snippet loads the DistilGPT2 tokenizer from the Transformers library, which will be used for processing text data during the training phase.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilgpt2")
```

----------------------------------------

TITLE: Creating Label Mappings for Text Classification in Python
DESCRIPTION: Sets up the mappings between numeric class IDs and their string labels for a binary sentiment classification task. This mapping will be used to configure the model and interpret its outputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> id2label = {0: "NEGATIVE", 1: "POSITIVE"}
>>> label2id = {"NEGATIVE": 0, "POSITIVE": 1}
```

----------------------------------------

TITLE: Sharing the Fine-tuned Translation Model with the Community
DESCRIPTION: Uploads the trained model to the Hugging Face Hub to make it accessible to the community for future use in translation tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/translation.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> trainer.push_to_hub()
```

----------------------------------------

TITLE: Using Donut for Document Parsing in Python
DESCRIPTION: This snippet shows how to use the Donut model for document parsing tasks. It involves setting up the environment, loading necessary resources, and executing model inference to extract structured data from document images.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/donut.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> import re
>>> from transformers import DonutProcessor, VisionEncoderDecoderModel
>>> from datasets import load_dataset
>>> import torch

>>> processor = DonutProcessor.from_pretrained("naver-clova-ix/donut-base-finetuned-cord-v2")
>>> model = VisionEncoderDecoderModel.from_pretrained("naver-clova-ix/donut-base-finetuned-cord-v2")

>>> device = "cuda" if torch.cuda.is_available() else "cpu"
>>> model.to(device)  # doctest: +IGNORE_RESULT

>>> # load document image
>>> dataset = load_dataset("hf-internal-testing/example-documents", split="test")
>>> image = dataset[2]["image"]

>>> # prepare decoder inputs
>>> task_prompt = "<s_cord-v2>"
>>> decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors="pt").input_ids

>>> pixel_values = processor(image, return_tensors="pt").pixel_values

>>> outputs = model.generate(
...     pixel_values.to(device),
...     decoder_input_ids=decoder_input_ids.to(device),
...     max_length=model.decoder.config.max_position_embeddings,
...     pad_token_id=processor.tokenizer.pad_token_id,
...     eos_token_id=processor.tokenizer.eos_token_id,
...     use_cache=True,
...     bad_words_ids=[[processor.tokenizer.unk_token_id]],
...     return_dict_in_generate=True,
... )

>>> sequence = processor.batch_decode(outputs.sequences)[0]
>>> sequence = sequence.replace(processor.tokenizer.eos_token, "").replace(processor.tokenizer.pad_token, "")
>>> sequence = re.sub(r"<.*?>", "", sequence, count=1).strip()  # remove first task start token
>>> print(processor.token2json(sequence))
{'menu': {'nm': 'CINNAMON SUGAR', 'unitprice': '17,000', 'cnt': '1 x', 'price': '17,000'}, 'sub_total': {'subtotal_price': '17,000'}, 'total': 
{'total_price': '17,000', 'cashprice': '20,000', 'changeprice': '3,000'}}
```

----------------------------------------

TITLE: Converting Datasets to TensorFlow Format
DESCRIPTION: Prepares TensorFlow datasets from the tokenized data using the prepare_tf_dataset method. This configures proper batching, shuffling, and data collation for training and evaluation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
tf_train_set = model.prepare_tf_dataset(
    tokenized_billsum["train"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_test_set = model.prepare_tf_dataset(
    tokenized_billsum["test"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)
```

----------------------------------------

TITLE: Extracting Audio Features
DESCRIPTION: Extracts features from the audio data using the loaded feature extractor. It passes the audio array and sampling rate to the feature extractor, resulting in the 'input_values' which will be used by the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_14

LANGUAGE: python
CODE:
```
>>> audio_input = [dataset[0]["audio"]["array"]]
>>> feature_extractor(audio_input, sampling_rate=16000)
{'input_values': [array([ 3.8106556e-04,  2.7506407e-03,  2.8015103e-03, ...,
        5.6335266e-04,  4.6588284e-06, -1.7142107e-04], dtype=float32)]}
```

----------------------------------------

TITLE: Preprocessing Function for Tokenization
DESCRIPTION: This function takes a batch of examples and joins the answers' text into a single string before tokenizing it using the previously loaded DistilGPT2 tokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> def preprocess_function(examples):
...     return tokenizer([" ".join(x) for x in examples["answers.text"]])
```

----------------------------------------

TITLE: Resampling Audio Data
DESCRIPTION: Resamples the audio data to a specified sampling rate using the `cast_column` method from Hugging Face Datasets. In this example, the audio is resampled to 16kHz to match the requirements of the Wav2Vec2 model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_11

LANGUAGE: python
CODE:
```
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16_000))
```

----------------------------------------

TITLE: Tokenizing and Identifying Mask Token Position (PyTorch)
DESCRIPTION: Code to tokenize the input text and identify the position of the mask token for manual prediction in PyTorch, returning the inputs as PyTorch tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_24

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("username/my_awesome_eli5_mlm_model")
inputs = tokenizer(text, return_tensors="pt")
mask_token_index = torch.where(inputs["input_ids"] == tokenizer.mask_token_id)[1]
```

----------------------------------------

TITLE: Preprocessing dataset for Wav2Vec2
DESCRIPTION: Create a preprocessing function to prepare the dataset for training, including audio processing and tokenization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/tasks/asr.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
def prepare_dataset(batch):
    audio = batch["audio"]
    batch = processor(audio["array"], sampling_rate=audio["sampling_rate"], text=batch["transcription"])
    batch["input_length"] = len(batch["input_values"][0])
    return batch

encoded_minds = minds.map(prepare_dataset, remove_columns=minds.column_names["train"], num_proc=4)
```

----------------------------------------

TITLE: Compiling and Training Question Answering Model in TensorFlow
DESCRIPTION: This snippet demonstrates how to compile the model, set up callbacks for pushing to the Hub, and train the question answering model using TensorFlow and Keras.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/question_answering.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
import tensorflow as tf
from transformers.keras_callbacks import PushToHubCallback

model.compile(optimizer=optimizer)

callback = PushToHubCallback(
    output_dir="my_awesome_qa_model",
    tokenizer=tokenizer,
)

model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=[callback])
```

----------------------------------------

TITLE: Loading Pre-trained Model for Sequence Classification with AutoModelForSequenceClassification in Python
DESCRIPTION: This snippet shows how to load a pre-trained model for sequence classification tasks using the AutoModelForSequenceClassification class from the Transformers library. This allows the use of a specific model architecture for classification.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/autoclass_tutorial.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Loading SceneParse150 Dataset Subset
DESCRIPTION: Loads a small subset of the SceneParse150 dataset using the Hugging Face Datasets library for experimentation before training on the full dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/semantic_segmentation.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> ds = load_dataset("scene_parse_150", split="train[:50]")
```

----------------------------------------

TITLE: Login to Hugging Face
DESCRIPTION: Logs into the Hugging Face Hub using the `huggingface-cli` tool. This is a prerequisite for pushing the trained model to the Model Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/run_scripts.md#_snippet_17

LANGUAGE: bash
CODE:
```
huggingface-cli login
```

----------------------------------------

TITLE: Loading Tokenizer Object into Transformers
DESCRIPTION: Demonstrates how to load a tokenizer object directly into the Transformers library using PreTrainedTokenizerFast class. This allows using the tokenizer with all shared methods from ðŸ¤— Transformers.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/fast_tokenizers.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import PreTrainedTokenizerFast

>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)
```

----------------------------------------

TITLE: Configuring SLURM Script for DeepSpeed Multi-Node Training
DESCRIPTION: SLURM script template for launching distributed training with DeepSpeed across multiple nodes. This script sets up the necessary environment variables and uses torch.distributed.run to coordinate the training process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/deepspeed.md#2025-04-22_snippet_15

LANGUAGE: bash
CODE:
```
#SBATCH --job-name=test-nodes        # name
#SBATCH --nodes=2                    # nodes
#SBATCH --ntasks-per-node=1          # crucial - only 1 task per dist per node!
#SBATCH --cpus-per-task=10           # number of cores per tasks
#SBATCH --gres=gpu:8                 # number of gpus
#SBATCH --time 20:00:00              # maximum execution time (HH:MM:SS)
#SBATCH --output=%x-%j.out           # output file name

export GPUS_PER_NODE=8
export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
export MASTER_PORT=9901

srun --jobid $SLURM_JOBID bash -c 'python -m torch.distributed.run \
 --nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES --node_rank $SLURM_PROCID \
 --master_addr $MASTER_ADDR --master_port $MASTER_PORT \
your_program.py <normal cl args> --deepspeed ds_config.json'
```

----------------------------------------

TITLE: Initializing Hubert Model with Flash Attention 2
DESCRIPTION: Python code to initialize the Hubert model using the Flash Attention 2 implementation. It loads a pre-trained model, sets the data type to float16, and specifies the attention implementation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/hubert.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import HubertModel
>>> import torch

>>> model = HubertModel.from_pretrained("facebook/hubert-large-ls960-ft", torch_dtype=torch.float16, attn_implementation="flash_attention_2").to("cuda")
...
```

----------------------------------------

TITLE: Uploading Model to Hub
DESCRIPTION: Pushes the custom model to Hugging Face Hub
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/custom_models.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
resnet50d.push_to_hub("custom-resnet50d")
```

----------------------------------------

TITLE: Loading AutoFeatureExtractor
DESCRIPTION: This code snippet demonstrates how to load a feature extractor using `AutoFeatureExtractor.from_pretrained`. This approach automatically selects the correct feature extractor based on the specified pretrained model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/feature_extractors.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
```py
from transformers import AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained("openai/whisper-tiny")
```
```

----------------------------------------

TITLE: Saving a BERT Model to TorchScript in Python
DESCRIPTION: This snippet demonstrates how to export a BERT model to TorchScript by initializing the model with the torchscript flag and saving it to disk. It includes tokenization, creating dummy inputs, and tracing the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/torchscript.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
from transformers import BertModel, BertTokenizer, BertConfig
import torch

enc = BertTokenizer.from_pretrained("bert-base-uncased")

# Tokenizing input text
text = "[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]"
tokenized_text = enc.tokenize(text)

# Masking one of the input tokens
masked_index = 8
tokenized_text[masked_index] = "[MASK]"
indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

# Creating a dummy input
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

# Initializing the model with the torchscript flag
# Flag set to True even though it is not necessary as this model does not have an LM Head.
config = BertConfig(
    vocab_size_or_config_json_file=32000,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072,
    torchscript=True,
)

# Instantiating the model
model = BertModel(config)

# The model needs to be in evaluation mode
model.eval()

# If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag
model = BertModel.from_pretrained("bert-base-uncased", torchscript=True)

# Creating the trace
traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])
torch.jit.save(traced_model, "traced_bert.pt")
```

----------------------------------------

TITLE: Creating Task-Specific Model Heads
DESCRIPTION: Examples of creating DistilBERT models with different task-specific heads for sequence classification and question answering.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/create_a_model.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import DistilBertForSequenceClassification

>>> model = DistilBertForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

LANGUAGE: python
CODE:
```
>>> from transformers import DistilBertForQuestionAnswering

>>> model = DistilBertForQuestionAnswering.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Generating Text with GPT-NeoX-Japanese in Python
DESCRIPTION: This code snippet demonstrates how to generate text using the GPT-NeoX-Japanese model and tokenizer from the transformers library. It loads the pretrained model and tokenizer, defines a prompt, and uses the `generate` method to produce text. The generated tokens are then decoded into a human-readable string.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/gpt_neox_japanese.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> from transformers import GPTNeoXJapaneseForCausalLM, GPTNeoXJapaneseTokenizer

>>> model = GPTNeoXJapaneseForCausalLM.from_pretrained("abeja/gpt-neox-japanese-2.7b")
>>> tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained("abeja/gpt-neox-japanese-2.7b")

>>> prompt = "äººã¨AIãŒå”èª¿ã™ã‚‹ãŸã‚ã«ã¯ã€"

>>> input_ids = tokenizer(prompt, return_tensors="pt").input_ids

>>> gen_tokens = model.generate(
...	input_ids,
...	do_sample=True,
...	temperature=0.9,
...	max_length=100,
... )
>>> gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]

>>> print(gen_text)
```

----------------------------------------

TITLE: Using Aya Vision with Transformers Pipeline
DESCRIPTION: Demonstrates how to use the Transformers pipeline for simplified inference with Aya Vision, processing an image and text input in Turkish.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/aya_vision.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import pipeline

pipe = pipeline(model="CohereForAI/aya-vision-8b", task="image-text-to-text", device_map="auto")

messages = [
    {"role": "user",
     "content": [
       {"type": "image", "url": "https://media.istockphoto.com/id/458012057/photo/istanbul-turkey.jpg?s=612x612&w=0&k=20&c=qogAOVvkpfUyqLUMr_XJQyq-HkACXyYUSZbKhBlPrxo="},
        {"type": "text", "text": "Bu resimde hangi anÄ±t gÃ¶sterilmektedir?"},
    ]},
]
outputs = pipe(text=messages, max_new_tokens=300, return_full_text=False)

print(outputs)
```

----------------------------------------

TITLE: Training Example with Torchrun
DESCRIPTION: The snippet is an example command line for running a training session using Torch and Transformers with multiple GPUs. It is broken down with backslashes to be easily copied and pasted.
SOURCE: https://github.com/huggingface/transformers/blob/main/ISSUES.md#2025-04-22_snippet_6

LANGUAGE: Bash
CODE:
```
cd examples/seq2seq
torchrun --nproc_per_node=2 ./finetune_trainer.py \
--model_name_or_path sshleifer/distill-mbart-en-ro-12-4 --data_dir wmt_en_ro \
--output_dir output_dir --overwrite_output_dir \
--do_train --n_train 500 --num_train_epochs 1 \
--per_device_train_batch_size 1  --freeze_embeds \
--src_lang en_XX --tgt_lang ro_RO --task translation \
--fp16
```

----------------------------------------

TITLE: Creating and Training a BPE Tokenizer in Python
DESCRIPTION: This snippet demonstrates how to create a basic BPE tokenizer using the ðŸ¤— Tokenizers library. It sets up the tokenizer with special tokens, a pre-tokenizer, and trains it on a set of files.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/fast_tokenizers.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from tokenizers import Tokenizer
>>> from tokenizers.models import BPE
>>> from tokenizers.trainers import BpeTrainer
>>> from tokenizers.pre_tokenizers import Whitespace

>>> tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
>>> trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

>>> tokenizer.pre_tokenizer = Whitespace()
>>> files = [...]
>>> tokenizer.train(files, trainer)
```

----------------------------------------

TITLE: Preprocessing Function for Summarization Tasks
DESCRIPTION: Creates a preprocessing function that adds a 'summarize:' prefix to inputs, tokenizes the text and summary, and properly formats them for the T5 model. The function handles truncation and prepares the labels for model training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
prefix = "summarize: "


def preprocess_function(examples):
    inputs = [prefix + doc for doc in examples["text"]]
    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)

    labels = tokenizer(text_target=examples["summary"], max_length=128, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```

----------------------------------------

TITLE: Image Segmentation with Transformers Pipeline
DESCRIPTION: This example demonstrates image segmentation using the Transformers pipeline. It segments an image and returns the predicted labels with their scores.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/task_summary.md#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
>>> from transformers import pipeline

>>> segmenter = pipeline(task="image-segmentation")
>>> preds = segmenter(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> print(*preds, sep="\n")
{'score': 0.9879, 'label': 'LABEL_184'}
{'score': 0.9973, 'label': 'snow'}
{'score': 0.9972, 'label': 'cat'}
```

----------------------------------------

TITLE: Pushing Model to Hugging Face Hub
DESCRIPTION: This snippet shows the additional arguments needed to push your fine-tuned model to the Hugging Face Hub when running the training script.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/audio-classification/README.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
python run_audio_classification.py \
    --push_to_hub \
    --hub_model_id <username/model_id> \
    ...
```

----------------------------------------

TITLE: Saving and Exporting a PyTorch Model Locally
DESCRIPTION: Python code demonstrating how to load a PyTorch model from Hugging Face Hub, save it locally, and then export it to ONNX format using the transformers.onnx package.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/serialization.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> # Load tokenizer and PyTorch weights form the Hub
>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
>>> pt_model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
>>> # Save to disk
>>> tokenizer.save_pretrained("local-pt-checkpoint")
>>> pt_model.save_pretrained("local-pt-checkpoint")
```

----------------------------------------

TITLE: Installing FlashAttention for NVIDIA GPUs
DESCRIPTION: Install FlashAttention package for NVIDIA GPU acceleration
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_one.md#2025-04-22_snippet_12

LANGUAGE: bash
CODE:
```
pip install flash-attn --no-build-isolation
```

----------------------------------------

TITLE: Applying Chat Template with BlenderBot Model - Python
DESCRIPTION: This snippet demonstrates how to integrate the chat template functionality using the BlenderBot model's tokenizer. It constructs a chat input of messages and applies the chat template to generate a single string output suitable for processing with the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/chat_templating.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer
>>> tokenizer = AutoTokenizer.from_pretrained("facebook/blenderbot-400M-distill")
>>> chat = [
...    {"role": "user", "content": "Hello, how are you?"},
...    {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
...    {"role": "user", "content": "I'd like to show off how chat templating works!"},
... ]
>>> tokenizer.apply_chat_template(chat, tokenize=False)
" Hello, how are you?  I'm doing great. How can I help you today?   I'd like to show off how chat templating works!</s>"
```

----------------------------------------

TITLE: Compiling TensorFlow Model for Training
DESCRIPTION: Compiles the TensorFlow model with the configured optimizer. No loss function is specified because Transformers models come with default task-relevant loss functions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
import tensorflow as tf

model.compile(optimizer=optimizer)  # No loss argument!
```

----------------------------------------

TITLE: Performing Inference and Post-processing
DESCRIPTION: This code performs inference with the OWL-ViT model and post-processes the output to obtain the bounding boxes, scores, and labels in the original image coordinates. It disables gradient calculation, passes the inputs to the model, and uses the processor's `post_process_object_detection` method to adjust the bounding boxes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/zero_shot_object_detection.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
">>> import torch

>>> with torch.no_grad():
...     outputs = model(**inputs)
...     target_sizes = torch.tensor([im.size[::-1]])
...     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)[0]

>>> draw = ImageDraw.Draw(im)

>>> scores = results["scores"].tolist()
>>> labels = results["labels"].tolist()
>>> boxes = results["boxes"].tolist()

>>> for box, score, label in zip(boxes, scores, labels):
...     xmin, ymin, xmax, ymax = box
...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)
...     draw.text((xmin, ymin), f"{text_queries[label]}: {round(score,2)}", fill=\"white\")

>>> im"
```

----------------------------------------

TITLE: Applying Preprocessing Function to Dataset
DESCRIPTION: This snippet applies the `preprocess_function` to the first five examples in the dataset. This demonstrates how to prepare the audio data for model training or inference, ensuring that all audio sequences have the same length through padding or truncation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/preprocessing.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> processed_dataset = preprocess_function(dataset[:5])
```

----------------------------------------

TITLE: Saving and Loading TensorFlow Model
DESCRIPTION: This snippet shows how to save a TensorFlow model using `TFPretrainedModel.save_pretrained` and then reload it using `TFPreTrainedModel.from_pretrained`. This is another recommended method for avoiding issues with saving and loading TensorFlow models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/troubleshooting.md#_snippet_1

LANGUAGE: Python
CODE:
```
>>> from transformers import TFPreTrainedModel

>>> model.save_pretrained("path_to/model")
>>> model = TFPreTrainedModel.from_pretrained("path_to/model")
```

----------------------------------------

TITLE: Running the Starlette App with Uvicorn
DESCRIPTION: This bash command starts the Starlette application using Uvicorn. It specifies the `server.py` file and the `app` object as the entry point.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_webserver.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
"uvicorn server:app"
```

----------------------------------------

TITLE: Applying Preprocessing to the Dataset
DESCRIPTION: Maps the preprocessing function over the entire dataset using Datasets' map function with batched processing. The function removes unnecessary columns and keeps only the processed features.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_question_answering.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> processed_dataset = flat_dataset.map(preprocess_data, batched=True, remove_columns=['question','question_type',  'question_id', 'image_id', 'answer_type', 'label.ids', 'label.weights'])
>>> processed_dataset
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'pixel_values', 'pixel_mask', 'labels'],
    num_rows: 200
})
```

----------------------------------------

TITLE: Compiling and Fitting TensorFlow Model in Python
DESCRIPTION: This snippet shows how to compile a TensorFlow model with an optimizer and fit it using the previously created tf.data.Dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/training.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
model.compile(optimizer=Adam(3e-5))  # æå¤±å¼•æ•°ã¯ä¸è¦ã§ã™ï¼

model.fit(tf_dataset)
```

----------------------------------------

TITLE: Fine-tuning T5-small for Summarization (PyTorch)
DESCRIPTION: This Python command runs the summarization example script using PyTorch. It fine-tunes the `google-t5/t5-small` model on the `cnn_dailymail` dataset for summarization, specifying training and evaluation batch sizes, output directory, and other relevant parameters. The `source_prefix` is crucial for T5 models to indicate the task is summarization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/run_scripts.md#_snippet_3

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Loading a TorchScript BertModel
DESCRIPTION: This code snippet shows how to load a previously saved TorchScript BertModel from disk using `torch.jit.load`. The model is then set to evaluation mode, and the dummy input is passed through the loaded model to get the output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/torchscript.md#_snippet_1

LANGUAGE: python
CODE:
```
loaded_model = torch.jit.load("traced_bert.pt")
loaded_model.eval()

all_encoder_layers, pooled_output = loaded_model(*dummy_input)
```

----------------------------------------

TITLE: Run Summarization with Limited Samples
DESCRIPTION: Runs the summarization script with a limited number of training, evaluation, and prediction samples.  This is useful for testing the script on a smaller subset of the data before processing the entire dataset. It utilizes the arguments `--max_train_samples`, `--max_eval_samples`, and `--max_predict_samples`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/run_scripts.md#_snippet_13

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --max_train_samples 50 \
    --max_eval_samples 50 \
    --max_predict_samples 50 \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Custom Knowledge Distillation Trainer for Image Classification
DESCRIPTION: Implements a custom trainer that performs knowledge distillation by computing loss between teacher and student models using KL Divergence
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/knowledge_distillation_for_image_classification.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
class ImageDistilTrainer(Trainer):
    def __init__(self, teacher_model=None, student_model=None, temperature=None, lambda_param=None,  *args, **kwargs):
        super().__init__(model=student_model, *args, **kwargs)
        self.teacher = teacher_model
        self.student = student_model
        self.loss_function = nn.KLDivLoss(reduction="batchmean")
        device, _, _ = get_backend()
        self.teacher.to(device)
        self.teacher.eval()
        self.temperature = temperature
        self.lambda_param = lambda_param

    def compute_loss(self, student, inputs, return_outputs=False):
        student_output = self.student(**inputs)

        with torch.no_grad():
          teacher_output = self.teacher(**inputs)

        soft_teacher = F.softmax(teacher_output.logits / self.temperature, dim=-1)
        soft_student = F.log_softmax(student_output.logits / self.temperature, dim=-1)

        distillation_loss = self.loss_function(soft_student, soft_teacher) * (self.temperature ** 2)
        student_target_loss = student_output.loss

        loss = (1. - self.lambda_param) * student_target_loss + self.lambda_param * distillation_loss
        return (loss, student_output) if return_outputs else loss
```

----------------------------------------

TITLE: Using Idefics2 Processor and Model for Inference in Python
DESCRIPTION: This Python code snippet demonstrates setting up and using the Idefics2 model for generating text based on a sequence of images processed with the Idefics2 processor. Prerequisites include the Hugging Face transformers library and a CUDA-capable device. The code leverages Python's requests library and PyTorch for model inference, showcasing input preparation, inference, and decoding steps.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/idefics2.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import requests
from PIL import Image
from transformers import Idefics2Processor, Idefics2ForConditionalGeneration
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

url_1 = "http://images.cocodataset.org/val2017/000000039769.jpg"
url_2 = "http://images.cocodataset.org/val2017/000000219578.jpg"

image_1 = Image.open(requests.get(url_1, stream=True).raw)
image_2 = Image.open(requests.get(url_2, stream=True).raw)
images = [image_1, image_2]

messages = [{
    "role": "user",
    "content": [
        {"type": "text", "text": "Whatâ€™s the difference between these two images?"},
        {"type": "image"},
        {"type": "image"},
    ],
}]

processor = Idefics2Processor.from_pretrained("HuggingFaceM4/idefics2-8b")
model = Idefics2ForConditionalGeneration.from_pretrained("HuggingFaceM4/idefics2-8b")
model.to(device)

# at inference time, one needs to pass `add_generation_prompt=True` in order to make sure the model completes the prompt
text = processor.apply_chat_template(messages, add_generation_prompt=True)
print(text)
# 'User: Whatâ€™s the difference between these two images?<image><image><end_of_utterance>\nAssistant:'

inputs = processor(images=images, text=text, return_tensors="pt").to(device)

generated_text = model.generate(**inputs, max_new_tokens=500)
generated_text = processor.batch_decode(generated_text, skip_special_tokens=True)[0]
print("Generated text:", generated_text)
```

----------------------------------------

TITLE: Manual Inference with Token Classification Model in TensorFlow
DESCRIPTION: Demonstrates how to manually perform inference with a trained token classification model using TensorFlow in the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/token_classification.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, TFAutoModelForTokenClassification
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> inputs = tokenizer(text, return_tensors="tf")

>>> model = TFAutoModelForTokenClassification.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> logits = model(**inputs).logits

>>> predicted_token_class_ids = tf.math.argmax(logits, axis=-1)
>>> predicted_token_class = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]
```

----------------------------------------

TITLE: Evaluation Metrics Implementation
DESCRIPTION: Setup of Word Error Rate (WER) evaluation metric for ASR model performance tracking
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/asr.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> import evaluate
>>> import numpy as np

>>> wer = evaluate.load("wer")

>>> def compute_metrics(pred):
...     pred_logits = pred.predictions
...     pred_ids = np.argmax(pred_logits, axis=-1)
...     pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id
...     pred_str = processor.batch_decode(pred_ids)
...     label_str = processor.batch_decode(pred.label_ids, group_tokens=False)
...     wer = wer.compute(predictions=pred_str, references=label_str)
...     return {"wer": wer}
```

----------------------------------------

TITLE: Loading Trained Model and Performing Inference in Python
DESCRIPTION: This script demonstrates how to load a trained Mask2Former model from the Hugging Face Hub and perform inference on a new image. It processes the image, runs it through the model, and retrieves the segmentation masks and their associated information.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/instance-segmentation/README.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import torch
import requests
import matplotlib.pyplot as plt

from PIL import Image
from transformers import Mask2FormerForUniversalSegmentation, Mask2FormerImageProcessor

# Load image
image = Image.open(requests.get("http://farm4.staticflickr.com/3017/3071497290_31f0393363_z.jpg", stream=True).raw)

# Load model and image processor
device = "cuda"
checkpoint = "qubvel-hf/finetune-instance-segmentation-ade20k-mini-mask2former"

model = Mask2FormerForUniversalSegmentation.from_pretrained(checkpoint, device_map=device)
image_processor = Mask2FormerImageProcessor.from_pretrained(checkpoint)

# Run inference on image
inputs = image_processor(images=[image], return_tensors="pt").to(device)
with torch.no_grad():
    outputs = model(**inputs)

# Post-process outputs
outputs = image_processor.post_process_instance_segmentation(outputs, target_sizes=[(image.height, image.width)])

print("Mask shape: ", outputs[0]["segmentation"].shape)
print("Mask values: ", outputs[0]["segmentation"].unique())
for segment in outputs[0]["segments_info"]:
    print("Segment: ", segment)
```

----------------------------------------

TITLE: Enabling Flash-Attention 2 for Chameleon Model
DESCRIPTION: This example demonstrates how to load the Chameleon model with Flash-Attention 2 enabled to speed up generation. It requires the flash-attn package to be installed separately.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/chameleon.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import ChameleonForConditionalGeneration

model_id = "facebook/chameleon-7b"
model = ChameleonForConditionalGeneration.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    attn_implementation="flash_attention_2"
).to(0)
```

----------------------------------------

TITLE: Configuring DDP Multi-Node Multi-GPU with Accelerate
DESCRIPTION: YAML configuration for Distributed Data Parallel (DDP) setup with multiple nodes and GPUs, specifying machine rank, IP addresses, ports, and mixed precision settings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/trainer.md#2025-04-22_snippet_8

LANGUAGE: yaml
CODE:
```
compute_environment: LOCAL_MACHINE                                                                                             
distributed_type: MULTI_GPU                                                                                                    
downcast_bf16: 'no'
gpu_ids: all
machine_rank: 0 #change rank as per the node
main_process_ip: 192.168.20.1
main_process_port: 9898
main_training_function: main
mixed_precision: fp16
num_machines: 2
num_processes: 8
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

----------------------------------------

TITLE: Summarization with MVP
DESCRIPTION: This snippet demonstrates how to use the MVP model and its summarization-specific variant for text summarization. It loads the tokenizer and model, then generates a summary for a given input text using both the base MVP model and the MVP model with summarization prompts. The decoded output shows the summarization result from each model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mvp.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import MvpTokenizer, MvpForConditionalGeneration

>>> tokenizer = MvpTokenizer.from_pretrained("RUCAIBox/mvp")
>>> model = MvpForConditionalGeneration.from_pretrained("RUCAIBox/mvp")
>>> model_with_prompt = MvpForConditionalGeneration.from_pretrained("RUCAIBox/mvp-summarization")

>>> inputs = tokenizer(
...     "Summarize: You may want to stick it to your boss and leave your job, but don't do it if these are your reasons.",
...     return_tensors="pt",
... )
>>> generated_ids = model.generate(**inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
["Why You Shouldn't Quit Your Job"]

>>> generated_ids = model_with_prompt.generate(**inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
["Don't do it if these are your reasons"]
```

----------------------------------------

TITLE: Load Audio Feature Extractor (Python)
DESCRIPTION: Loads a pre-trained feature extractor for audio data using `AutoFeatureExtractor.from_pretrained`. This prepares the data for use with a Wav2Vec2 model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/preprocessing.md#_snippet_12

LANGUAGE: python
CODE:
```
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
```

----------------------------------------

TITLE: Applying Preprocessing to the Dataset
DESCRIPTION: Processes the entire dataset by applying the preprocessing function using the map method, removing unnecessary columns and creating model-ready features.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/visual_question_answering.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> processed_dataset = flat_dataset.map(preprocess_data, batched=True, remove_columns=['question','question_type',  'question_id', 'image_id', 'answer_type', 'label.ids', 'label.weights'])
>>> processed_dataset
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'pixel_values', 'pixel_mask', 'labels'],
    num_rows: 200
})
```

----------------------------------------

TITLE: Running Masked Language Model Training with Transformers
DESCRIPTION: Command for training or fine-tuning a masked language model using the run_mlm.py script with a pre-trained DistilBERT model on the WikiText dataset. This approach is generally faster to train and performs well for downstream tasks with task-specific output heads.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/tensorflow/language-modeling/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python run_mlm.py \
--model_name_or_path distilbert/distilbert-base-cased \
--output_dir output \
--dataset_name wikitext \
--dataset_config_name wikitext-103-raw-v1
```

----------------------------------------

TITLE: Initializing AutoImageProcessor for DETR Image Processing in Python
DESCRIPTION: This snippet initializes an AutoImageProcessor for preprocessing images to match DETR model requirements. It sets up resizing and padding parameters based on a maximum image size.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/object_detection.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers import AutoImageProcessor

MAX_SIZE = IMAGE_SIZE

image_processor = AutoImageProcessor.from_pretrained(
    MODEL_NAME,
    do_resize=True,
    size={"max_height": MAX_SIZE, "max_width": MAX_SIZE},
    do_pad=True,
    pad_size={"height": MAX_SIZE, "width": MAX_SIZE},
)
```

----------------------------------------

TITLE: Loading Mean IoU Evaluation Metric
DESCRIPTION: Loads the mean Intersection over Union (IoU) metric from the Evaluate library to assess segmentation performance during model training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/semantic_segmentation.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> import evaluate

>>> metric = evaluate.load("mean_iou")
```

----------------------------------------

TITLE: Loading a Pre-trained Feature Extractor using AutoFeatureExtractor Python
DESCRIPTION: This code snippet demonstrates loading a pre-trained feature extractor for audio tasks using `AutoFeatureExtractor.from_pretrained`. The feature extractor is initialized with the checkpoint "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition".
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/autoclass_tutorial.md#_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained(
...     "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
... )
```

----------------------------------------

TITLE: Preprocessing Dataset for Image Captioning in Python
DESCRIPTION: Load the AutoProcessor from transformers to preprocess images and tokenize captions, preparing them for model training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_captioning.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoProcessor

checkpoint = "microsoft/git-base"
processor = AutoProcessor.from_pretrained(checkpoint)
```

LANGUAGE: python
CODE:
```
def transforms(example_batch):
    images = [x for x in example_batch["image"]]
    captions = [x for x in example_batch["text"]]
    inputs = processor(images=images, text=captions, padding="max_length")
    inputs.update({"labels": inputs["input_ids"]})
    return inputs


train_ds.set_transform(transforms)
test_ds.set_transform(transforms)
```

----------------------------------------

TITLE: Resume Training from Last Checkpoint
DESCRIPTION: Resumes training from the last checkpoint in the specified output directory.  It specifies the model, dataset, source prefix, and output directory. It also removes the overwrite_output_dir argument.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/run_scripts.md#_snippet_15

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --output_dir previous_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Logging into Hugging Face Hub in Python Notebook
DESCRIPTION: Demonstrates how to log into the Hugging Face Hub from a Python notebook environment.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/custom_models.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
from huggingface_hub import notebook_login

notebook_login()
```

----------------------------------------

TITLE: Formatting Prompts with LlavaNextProcessor in Python
DESCRIPTION: This Python snippet demonstrates how to use the `LlavaNextProcessor` to format chat inputs for the LLaVa-NeXT model. The processor is initialized using a pre-trained checkpoint, and a conversation history is formatted into a text prompt compatible with the model. This setup is crucial for properly aligning inputs for inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llava_next.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import LlavaNextProcessor

processor = LlavaNextProcessor.from_pretrained("llava-hf/llava-v1.6-mistral-7b-hf")

conversation = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": "Whatâ€™s shown in this image?"},
        ],
    },
    {
        "role": "assistant",
        "content": [{"type": "text", "text": "This image shows a red stop sign."},]
    },
    {

        "role": "user",
        "content": [
            {"type": "text", "text": "Describe the image in more details."},
        ],
    },
]

text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)

# Note that the template simply formats your prompt, you still have to tokenize it and obtain pixel values for your images
print(text_prompt)
>>> "[INST] <image>\nWhat's shown in this image? [/INST] This image shows a red stop sign. [INST] Describe the image in more details. [/INST]"
```

----------------------------------------

TITLE: Launching Distributed Training with Accelerate
DESCRIPTION: This snippet shows how to launch distributed training for a translation model using the Accelerate library. It configures the environment and starts the training process, supporting various hardware setups including multi-GPU and TPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation/README.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
accelerate launch run_translation_no_trainer.py \
    --model_name_or_path Helsinki-NLP/opus-mt-en-ro \
    --source_lang en \
    --target_lang ro \
    --dataset_name wmt16 \
    --dataset_config_name ro-en \
    --output_dir ~/tmp/tst-translation
```

----------------------------------------

TITLE: Loading a Quark Model in Transformers - Python
DESCRIPTION: This Python snippet demonstrates how to load a Quark model into the Transformers library, transfer it to a CUDA device, and prepare it for inference, including tokenizing input text and generating outputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/quark.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "EmbeddedLLM/Llama-3.1-8B-Instruct-w_fp8_per_channel_sym"
model = AutoModelForCausalLM.from_pretrained(model_id)
model = model.to("cuda")

print(model.model.layers[0].self_attn.q_proj)
# QParamsLinear(
#   (weight_quantizer): ScaledRealQuantizer()
#   (input_quantizer): ScaledRealQuantizer()
#   (output_quantizer): ScaledRealQuantizer()
# )

tokenizer = AutoTokenizer.from_pretrained(model_id)
inp = tokenizer("Where is a good place to cycle around Tokyo?", return_tensors="pt")
inp = inp.to("cuda")

res = model.generate(**inp, min_new_tokens=50, max_new_tokens=100)

print(tokenizer.batch_decode(res)[0])
# <|begin_of_text|>Where is a good place to cycle around Tokyo? There are several places in Tokyo that are suitable for cycling, depending on your skill level and interests. Here are a few suggestions:
# 1. Yoyogi Park: This park is a popular spot for cycling and has a wide, flat path that's perfect for beginners. You can also visit the Meiji Shrine, a famous Shinto shrine located in the park.
# 2. Imperial Palace East Garden: This beautiful garden has a large, flat path that's perfect for cycling. You can also visit the
```

----------------------------------------

TITLE: PaliGemma Inference with AutoProcessor and PaliGemmaForConditionalGeneration in Python
DESCRIPTION: This snippet demonstrates how to perform inference with the PaliGemma model using `AutoProcessor` for input preparation and `PaliGemmaForConditionalGeneration` for generating output. It loads the model and processor, prepares the input with an image and a prompt, and then generates and decodes the output. It requires the `transformers`, `requests`, and `PIL` libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/paligemma.md#_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoProcessor, PaliGemmaForConditionalGeneration

model_id = "google/paligemma-3b-mix-224"
model = PaliGemmaForConditionalGeneration.from_pretrained(model_id)
processor = AutoProcessor.from_pretrained(model_id)

prompt = "What is on the flower?"
image_file = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg?download=true"
raw_image = Image.open(requests.get(image_file, stream=True).raw)
inputs = processor(raw_image, prompt, return_tensors="pt")
output = model.generate(**inputs, max_new_tokens=20)

print(processor.decode(output[0], skip_special_tokens=True)[len(prompt):])
```

----------------------------------------

TITLE: Custom Pair Classification Pipeline Implementation
DESCRIPTION: Complete implementation of a custom pipeline for sentence pair classification.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/add_new_pipeline.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
import numpy as np
from transformers import Pipeline

def softmax(outputs):
    maxes = np.max(outputs, axis=-1, keepdims=True)
    shifted_exp = np.exp(outputs - maxes)
    return shifted_exp / shifted_exp.sum(axis=-1, keepdims=True)

class PairClassificationPipeline(Pipeline):
    def _sanitize_parameters(self, **kwargs):
        preprocess_kwargs = {}
        if "second_text" in kwargs:
            preprocess_kwargs["second_text"] = kwargs["second_text"]
        return preprocess_kwargs, {}, {}

    def preprocess(self, text, second_text=None):
        return self.tokenizer(text, text_pair=second_text, return_tensors=self.framework)

    def _forward(self, model_inputs):
        return self.model(**model_inputs)

    def postprocess(self, model_outputs):
        logits = model_outputs.logits[0].numpy()
        probabilities = softmax(logits)

        best_class = np.argmax(probabilities)
        label = self.model.config.id2label[best_class]
        score = probabilities[best_class].item()
        logits = logits.tolist()
        return {"label": label, "score": score, "logits": logits}
```

----------------------------------------

TITLE: Resume Training from Specific Checkpoint
DESCRIPTION: This command resumes training from a specific checkpoint folder, specified by the `resume_from_checkpoint` argument. It allows restarting training from a particular point in the training process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/run_scripts.md#_snippet_16

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --resume_from_checkpoint path_to_specific_checkpoint \
    --predict_with_generate
```

----------------------------------------

TITLE: Initializing ViT Image Processor in Python
DESCRIPTION: This snippet demonstrates how to initialize a default ViTImageProcessor for image classification using the ViT model. It imports the ViTImageProcessor class from the transformers library and creates an instance of it. The output shows the default parameters of the image processor.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/create_a_model.md#_snippet_18

LANGUAGE: python
CODE:
```
>>> from transformers import ViTImageProcessor

>>> vit_extractor = ViTImageProcessor()
>>> print(vit_extractor)
ViTImageProcessor {
  "do_normalize": true,
  "do_resize": true,
  "image_processor_type": "ViTImageProcessor",
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "resample": 2,
  "size": 224
}
```

----------------------------------------

TITLE: Speculative Sampling Decoding - Python
DESCRIPTION: This snippet extends speculative decoding with sampling techniques by adding sampling parameters to the generate function, facilitating creative model output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_optims.md#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.7)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
```

----------------------------------------

TITLE: Generating Text with Pipeline in Python
DESCRIPTION: Demonstrates sending input text to the pipeline for text generation, showing basic usage with a single input example.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/pipeline_tutorial.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
>>> generator(
...     "Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone"
... )  # doctest: +SKIP
[{'generated_text': 'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, Seven for the Iron-priests at the door to the east, and thirteen for the Lord Kings at the end of the mountain'}]
```

----------------------------------------

TITLE: Loading a Pre-trained DistilBert Slow Tokenizer in Python
DESCRIPTION: Shows how to load the standard (slow) tokenizer that matches a pre-trained DistilBert model to ensure compatible vocabulary and tokenization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/create_a_model.md#2025-04-23_snippet_16

LANGUAGE: python
CODE:
```
>>> from transformers import DistilBertTokenizer

>>> slow_tokenizer = DistilBertTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Depth Estimation with ZoeDepth Classes
DESCRIPTION: This code snippet illustrates how to perform depth estimation using the individual classes `AutoImageProcessor` and `ZoeDepthForDepthEstimation` in Transformers. It initializes an image processor and a ZoeDepth model, preprocesses an image, performs inference, and then post-processes the output to obtain the predicted depth map.  The final depth map is converted to a PIL Image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/zoedepth.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoImageProcessor, ZoeDepthForDepthEstimation
>>> import torch
>>> import numpy as np
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("Intel/zoedepth-nyu-kitti")
>>> model = ZoeDepthForDepthEstimation.from_pretrained("Intel/zoedepth-nyu-kitti")

>>> # prepare image for the model
>>> inputs = image_processor(images=image, return_tensors="pt")

>>> with torch.no_grad():   
...     outputs = model(inputs)

>>> # interpolate to original size and visualize the prediction
>>> ## ZoeDepth dynamically pads the input image. Thus we pass the original image size as argument
>>> ## to `post_process_depth_estimation` to remove the padding and resize to original dimensions.
>>> post_processed_output = image_processor.post_process_depth_estimation(
...     outputs,
...     source_sizes=[(image.height, image.width)],
... )

>>> predicted_depth = post_processed_output[0]["predicted_depth"]
>>> depth = (predicted_depth - predicted_depth.min()) / (predicted_depth.max() - predicted_depth.min())
>>> depth = depth.detach().cpu().numpy() * 255
>>> depth = Image.fromarray(depth.astype("uint8"))
```

----------------------------------------

TITLE: Authenticating with Hugging Face Hub
DESCRIPTION: Login to Hugging Face account to enable model sharing and upload capabilities using notebook_login function
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/translation.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from huggingface_hub import notebook_login\n\nnotebook_login()
```

----------------------------------------

TITLE: Logging into Hugging Face Hub
DESCRIPTION: Logs into the Hugging Face Hub using the `notebook_login` function. This is required to upload and share the fine-tuned model with the community.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/audio_classification.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
">>> from huggingface_hub import notebook_login\n\n>>> notebook_login()"
```

----------------------------------------

TITLE: Pipeline with Multiple Inputs
DESCRIPTION: Demonstrates processing multiple text inputs using a pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/pipelines.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
pipe = pipeline("text-classification")
pipe(["This restaurant is awesome", "This restaurant is awful"])
```

----------------------------------------

TITLE: Training with Accelerate Library
DESCRIPTION: Commands to install, configure and run training using the Accelerate library, which provides unified training across different compute platforms.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/run_scripts.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
pip install git+https://github.com/huggingface/accelerate
accelerate config
accelerate test
accelerate launch run_summarization_no_trainer.py \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir ~/tmp/tst-summarization
```

----------------------------------------

TITLE: Mapping Preprocessing Function Over SQuAD Dataset
DESCRIPTION: This code applies the preprocessing function to the entire SQuAD dataset. It processes the dataset in batches for efficiency and removes unnecessary columns to streamline the dataset for training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/question_answering.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad["train"].column_names)
```

----------------------------------------

TITLE: Loading Audio Dataset
DESCRIPTION: This code loads the 'PolyAI/minds14' dataset, specifically the 'en-US' configuration and the 'train' split, using the `load_dataset` function from the `datasets` library. It prepares the dataset for audio processing tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/preprocessing.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")
```

----------------------------------------

TITLE: Saving and Reloading Quantized Model in Python
DESCRIPTION: This Python snippet illustrates how to save a quantized model to a specified path and how to reload the saved model using the Transformers library. This is essential for model persistence and deployment. Required dependencies include Transformers and a compatible version of PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/finegrained_fp8.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
quant_path = "/path/to/save/quantized/model"
model.save_pretrained(quant_path)
model = AutoModelForCausalLM.from_pretrained(quant_path, device_map="auto")
```

----------------------------------------

TITLE: Fast Image Processor on GPU with DetrImageProcessorFast - Python
DESCRIPTION: This snippet uses 'DetrImageProcessorFast' for loading a fast image processor, demonstrating processing on a GPU by specifying the device as 'cuda'. It highlights the deployment of fast processing for image data on accelerated hardware.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/image_processors.md#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
from torchvision.io import read_image
from transformers import DetrImageProcessorFast

images = read_image("image.jpg")
processor = DetrImageProcessorFast.from_pretrained("facebook/detr-resnet-50")
images_processed = processor(images, return_tensors="pt", device="cuda")
```

----------------------------------------

TITLE: Loading Model and Processor with Auto Classes
DESCRIPTION: This snippet shows an example of attempting to load a GPT2 model for question answering using `AutoModelForQuestionAnswering`. This results in a ValueError because the GPT2 model is not designed or configured for this specific task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/troubleshooting.md#_snippet_9

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoProcessor, AutoModelForQuestionAnswering

>>> processor = AutoProcessor.from_pretrained("openai-community/gpt2-medium")
>>> model = AutoModelForQuestionAnswering.from_pretrained("openai-community/gpt2-medium")
```

----------------------------------------

TITLE: Model Inference Example
DESCRIPTION: Demonstration of using the trained model for generating captions on new images.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/image_captioning.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
device = "cuda" if torch.cuda.is_available() else "cpu"

inputs = processor(images=image, return_tensors="pt").to(device)
pixel_values = inputs.pixel_values

generated_ids = model.generate(pixel_values=pixel_values, max_length=50)
generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(generated_caption)
```

----------------------------------------

TITLE: XLA Acceleration for Text Generation
DESCRIPTION: This snippet shows how to enable XLA for text generation in a model by wrapping the generate function in tf.function. This method combines the power of XLA with the Hugging Face Transformers library for efficient text generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tf_xla.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForCausalLM
# Will error if the minimal version of Transformers is not installed.
from transformers.utils import check_min_version

check_min_version("4.21.0")

tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2", padding_side="left", pad_token="</s>")
model = TFAutoModelForCausalLM.from_pretrained("openai-community/gpt2")
input_string = ["TensorFlow is"]

xla_generate = tf.function(model.generate, jit_compile=True)

tokenized_input = tokenizer(input_string, return_tensors="tf")
generated_tokens = xla_generate(**tokenized_input, num_beams=2)

decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)
print(f"Generated -- {decoded_text}")
```

----------------------------------------

TITLE: Using LayoutLMv2 Pipeline for Document QA Inference in Python
DESCRIPTION: This code demonstrates how to use a fine-tuned LayoutLMv2 model in a pipeline for document question answering. It takes an image and a question as input and returns the predicted answer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/document_question_answering.md#2025-04-22_snippet_24

LANGUAGE: python
CODE:
```
from transformers import pipeline

qa_pipeline = pipeline("document-question-answering", model="MariaK/layoutlmv2-base-uncased_finetuned_docvqa")
qa_pipeline(image, question)
```

----------------------------------------

TITLE: Customized Chat Template with Assistant Role Markers
DESCRIPTION: An example of customizing a chat template by adding special tokens around assistant messages. This demonstrates how to modify existing templates for different formatting requirements.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/chat_templating.md#2025-04-22_snippet_6

LANGUAGE: jinja
CODE:
```
{% for message in messages %}
    {% if message['role'] == 'user' %}
        {{ bos_token + '[INST] ' + message['content'].strip() + ' [/INST]' }}
    {% elif message['role'] == 'system' %}
        {{ '<<SYS>>\n' + message['content'].strip() + '\n<</SYS>>\n\n' }}
    {% elif message['role'] == 'assistant' %}
        {{ '[ASST] '  + message['content'] + ' [/ASST]' + eos_token }}
    {% endif %}
{% endfor %}
```

----------------------------------------

TITLE: Setting Up Optimizer for TensorFlow Training
DESCRIPTION: This snippet shows how to set up an AdamWeightDecay optimizer for training a model in TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
from transformers import create_optimizer, AdamWeightDecay

optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

----------------------------------------

TITLE: Tokenizing Text with XLNet Tokenizer in Python
DESCRIPTION: Shows how to use an XLNet tokenizer to process text. This example illustrates how a different tokenizer might segment the same text differently, with XLNet tokenizer using SentencePiece-based tokenization as indicated by the 'â–' prefix.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tokenizer_summary.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import XLNetTokenizer

>>> tokenizer = XLNetTokenizer.from_pretrained("xlnet/xlnet-base-cased")
>>> tokenizer.tokenize("Don't you love ðŸ¤— Transformers? We sure do.")
["â–Don", "'", "t", "â–you", "â–love", "â–", "ðŸ¤—", "â–", "Transform", "ers", "?", "â–We", "â–sure", "â–do", "."]
```

----------------------------------------

TITLE: Loading LayoutLMv2 Model for Document QA in Python
DESCRIPTION: This code loads a pre-trained LayoutLMv2 model for document question answering using the AutoModelForDocumentQuestionAnswering class from the transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/document_question_answering.md#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
from transformers import AutoModelForDocumentQuestionAnswering

model = AutoModelForDocumentQuestionAnswering.from_pretrained(model_checkpoint)
```

----------------------------------------

TITLE: Loading PEFT Adapter Model
DESCRIPTION: Example of loading a PEFT adapter model using AutoModelForCausalLM class
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/peft.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id)
```

----------------------------------------

TITLE: Initializing and Using StarCoder2 Model for Text Generation in Python
DESCRIPTION: This snippet demonstrates how to initialize the StarCoder2 model and tokenizer, and use them for text generation. It loads a pre-trained 7B parameter model, tokenizes a prompt, and generates a continuation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/starcoder2.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("bigcode/starcoder2-7b", device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("bigcode/starcoder2-7b")

>>> prompt = "def print_hello_world():"

>>> model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")

>>> generated_ids = model.generate(**model_inputs, max_new_tokens=10, do_sample=False)
>>> tokenizer.batch_decode(generated_ids)[0]
'def print_hello_world():\n    print("Hello World!")\n\ndef print'
```

----------------------------------------

TITLE: Applying Data Augmentation for SegFormer Training in TensorFlow
DESCRIPTION: This code applies color augmentation to images using TensorFlow's image processing functions. It defines separate transform functions for training and validation datasets, which are then applied to the datasets using set_transform.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
>>> import tensorflow as tf


>>> def aug_transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.image.random_brightness(image, 0.25)
...     image = tf.image.random_contrast(image, 0.5, 2.0)
...     image = tf.image.random_saturation(image, 0.75, 1.25)
...     image = tf.image.random_hue(image, 0.1)
...     image = tf.transpose(image, (2, 0, 1))
...     return image


>>> def transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.transpose(image, (2, 0, 1))
...     return image

>>> def train_transforms(example_batch):
...     images = [aug_transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs


>>> def val_transforms(example_batch):
...     images = [transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs

>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```

----------------------------------------

TITLE: Creating Label Mappings for Token Classification in Python
DESCRIPTION: Sets up dictionaries for mapping between label IDs and text labels for a token classification task. This is a prerequisite for training the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/token_classification.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> id2label = {
...     0: "O",
...     1: "B-corporation",
...     2: "I-corporation",
...     3: "B-creative-work",
...     4: "I-creative-work",
...     5: "B-group",
...     6: "I-group",
...     7: "B-location",
...     8: "I-location",
...     9: "B-person",
...     10: "I-person",
...     11: "B-product",
...     12: "I-product",
... }
>>> label2id = {
...     "O": 0,
...     "B-corporation": 1,
...     "I-corporation": 2,
...     "B-creative-work": 3,
...     "I-creative-work": 4,
...     "B-group": 5,
...     "I-group": 6,
...     "B-location": 7,
...     "I-location": 8,
...     "B-person": 9,
...     "I-person": 10,
...     "B-product": 11,
...     "I-product": 12,
... }
```

----------------------------------------

TITLE: Hugging Face CLI Login
DESCRIPTION: Command to login to Hugging Face using the CLI tool to store access token.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/model_sharing.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
huggingface-cli login
```

----------------------------------------

TITLE: Using CLIP for Image-Text Similarity with PyTorch
DESCRIPTION: This snippet demonstrates how to use the CLIPProcessor and CLIPModel to compute similarity scores between an image and text descriptions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/clip.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from PIL import Image
import requests

from transformers import CLIPProcessor, CLIPModel

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True)

outputs = model(**inputs)
logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities
```

----------------------------------------

TITLE: Loading Pretrained Model with FlaxAutoModelForCausalLM (Flax) - Python
DESCRIPTION: In this snippet, a pretrained model is loaded using the Flax framework through the FlaxAutoModelForCausalLM class. It is tailored for users working with Flax for model integration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/models.md#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
from transformers import FlaxAutoModelForCausalLM, FlaxMistralForCausalLM

# load with AutoClass or model-specific class
model = FlaxAutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1")
model = FlaxMistralForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1")
```

----------------------------------------

TITLE: Saving and Loading TensorFlow Model Weights with Transformers
DESCRIPTION: This snippet demonstrates how to save TensorFlow model weights as an h5 file and reload the model using `TFPreTrainedModel.from_pretrained` to avoid potential issues with saving the entire model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/troubleshooting.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import TFPreTrainedModel
>>> from tensorflow import keras

>>> model.save_weights("some_folder/tf_model.h5")
>>> model = TFPreTrainedModel.from_pretrained("some_folder")
```

----------------------------------------

TITLE: Resampling Audio Dataset
DESCRIPTION: This code demonstrates how to resample the audio data to a target sampling rate of 16kHz. The `cast_column` method from the `datasets` library is used to change the sampling rate of the 'audio' column, ensuring compatibility with the Wav2Vec2 model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/preprocessing.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16_000))
```

----------------------------------------

TITLE: Generating Translations Using M2M100 - PyTorch
DESCRIPTION: This snippet shows how to generate translations from the M2M100 model, specifically translating between Hindi to French and Chinese to English. It sets the source language, encodes the input text, and utilizes the generate method with a forced beginning of the sequence token for output consistency. It uses the Hugging Face Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/m2m_100.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

>>> hi_text = "à¤œà¥€à¤µà¤¨ à¤à¤• à¤šà¥‰à¤•à¤²à¥‡à¤Ÿ à¤¬à¥‰à¤•à¥à¤¸ à¤•à¥€ à¤¤à¤°à¤¹ à¤¹à¥ˆà¥¤"
>>> chinese_text = "ç”Ÿæ´»å°±åƒä¸€ç›’å·§å…‹åŠ›ã€‚"

>>> model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")
>>> tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M")

>>> # translate Hindi to French
>>> tokenizer.src_lang = "hi"
>>> encoded_hi = tokenizer(hi_text, return_tensors="pt")
>>> generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id("fr"))
>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
"La vie est comme une boÃ®te de chocolat."

>>> # translate Chinese to English
>>> tokenizer.src_lang = "zh"
>>> encoded_zh = tokenizer(chinese_text, return_tensors="pt")
>>> generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id("en"))
>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
"Life is like a box of chocolate."
```

----------------------------------------

TITLE: Preprocessing Images for OWL-ViT Image-Guided Detection
DESCRIPTION: Code showing how to preprocess a target image and query image using the OWL-ViT processor for image-guided object detection.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> inputs = processor(images=image_target, query_images=query_image, return_tensors="pt")
```

----------------------------------------

TITLE: Comparing Deprecated vs. Recommended TorchAoConfig Initialization
DESCRIPTION: This snippet contrasts the old, deprecated method of initializing `TorchAoConfig` using a string identifier with the new, recommended approach. The new method involves creating a specific configuration object (like `Int4WeightOnlyConfig`) first, offering improved type safety and access to more features.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/torchao.md#_snippet_16

LANGUAGE: python
CODE:
```
# Old way (deprecated)
quantization_config = TorchAoConfig("int4_weight_only", group_size=128)

# New way (recommended)
from torchao.quantization import Int4WeightOnlyConfig
quant_config = Int4WeightOnlyConfig(group_size=128)
quantization_config = TorchAoConfig(quant_type=quant_config)
```

----------------------------------------

TITLE: Removing Unnecessary Columns from a Dataset
DESCRIPTION: This snippet removes unnecessary columns (file, id, normalized_text) from the LJ Speech dataset, focusing on the `audio` and `text` columns, which are the primary inputs for ASR tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_27

LANGUAGE: Python
CODE:
```
>>> lj_speech = lj_speech.map(remove_columns=["file", "id", "normalized_text"])
```

----------------------------------------

TITLE: Training Token Classification Model with TensorFlow in Transformers
DESCRIPTION: Shows how to set up training parameters, load a pre-trained model, and train a token classification model using TensorFlow and Keras in the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/token_classification.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
>>> from transformers import create_optimizer, TFAutoModelForTokenClassification
>>> import tensorflow as tf

>>> batch_size = 16
>>> num_train_epochs = 3
>>> num_train_steps = (len(tokenized_wnut["train"]) // batch_size) * num_train_epochs
>>> optimizer, lr_schedule = create_optimizer(
...     init_lr=2e-5,
...     num_train_steps=num_train_steps,
...     weight_decay_rate=0.01,
...     num_warmup_steps=0,
... )

>>> model = TFAutoModelForTokenClassification.from_pretrained(
...     "distilbert/distilbert-base-uncased", num_labels=13, id2label=id2label, label2id=label2id
... )

>>> tf_train_set = model.prepare_tf_dataset(
...     tokenized_wnut["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_validation_set = model.prepare_tf_dataset(
...     tokenized_wnut["validation"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> model.compile(optimizer=optimizer)

>>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback

>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="my_awesome_wnut_model",
...     tokenizer=tokenizer,
... )

>>> callbacks = [metric_callback, push_to_hub_callback]

>>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)
```

----------------------------------------

TITLE: Resume Training from Latest Checkpoint
DESCRIPTION: This command resumes training from the latest checkpoint saved in the `output_dir`. The `overwrite_output_dir` argument should be removed when resuming from a checkpoint to avoid overwriting previous training progress.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/run_scripts.md#_snippet_15

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --output_dir previous_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Pushing Model to an Organization
DESCRIPTION: Demonstrates how to push a model to an organization's account on the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_sharing.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
pt_model.push_to_hub("my-awesome-org/my-awesome-model")
```

----------------------------------------

TITLE: Share Model to Hugging Face Hub
DESCRIPTION: Uploads the fine-tuned model to the Hugging Face Model Hub.  The `push_to_hub` argument enables pushing the model to the hub. The repository name is determined by your username and the `output_dir`.  `push_to_hub_model_id` allows specifying a custom repository name.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/run_scripts.md#_snippet_18

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --push_to_hub \
    --push_to_hub_model_id finetuned-t5-cnn_dailymail \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Configuring Logging Across Modules
DESCRIPTION: Code snippet showing how to set up consistent logging configuration across multiple modules including the main code, datasets, and transformers libraries based on the training arguments.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/trainer.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
logger = logging.getLogger(__name__)

logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    handlers=[logging.StreamHandler(sys.stdout)],
)

log_level = training_args.get_process_log_level()
logger.setLevel(log_level)
datasets.utils.logging.set_verbosity(log_level)
transformers.utils.logging.set_verbosity(log_level)

trainer = Trainer(...)
```

----------------------------------------

TITLE: Running an ONNX Model with ONNX Runtime
DESCRIPTION: Python code demonstrating how to load and run an exported ONNX model using ONNX Runtime. The example uses a DistilBERT model and shows how to tokenize inputs and perform inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/serialization.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer
>>> from onnxruntime import InferenceSession

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
>>> session = InferenceSession("onnx/model.onnx")
>>> # ONNX Runtime expects NumPy arrays as input
>>> inputs = tokenizer("Using DistilBERT with ONNX Runtime!", return_tensors="np")
>>> outputs = session.run(output_names=["last_hidden_state"], input_feed=dict(inputs))
```

----------------------------------------

TITLE: Adding Multiple LoRA Adapters to a Model
DESCRIPTION: Code demonstrating how to add a second LoRA adapter to a model that already has one, maintaining the same configuration but with a different adapter name.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/peft.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
# attach new adapter with same config
model.add_adapter(lora_config, adapter_name="adapter_2")
```

----------------------------------------

TITLE: PyTorch Manual Summarization - Generation
DESCRIPTION: Shows how to generate a summary using PyTorch model's generate method with specific parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_26

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained("username/my_awesome_billsum_model")
outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)
```

----------------------------------------

TITLE: Authenticating with Hugging Face Hub
DESCRIPTION: Logs in to Hugging Face Hub to enable model uploading and sharing with the community during the fine-tuning process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/video_classification.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Logging into Hugging Face Hub
DESCRIPTION: This Python snippet logs into the Hugging Face Hub using a notebook login function, allowing the user to upload and share models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/multiple_choice.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Enabling FlashAttention2 with AWQ Models
DESCRIPTION: Load an AWQ-quantized model with FlashAttention2 for accelerated inference
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/awq.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
  "TheBloke/zephyr-7B-alpha-AWQ",
  attn_implementation="flash_attention_2",
  device_map="cuda:0"
)
```

----------------------------------------

TITLE: Performing Audio Emotion Classification with Pipeline in Python
DESCRIPTION: Demonstrates using the audio classification pipeline to identify emotions in an audio file, and formatting the predictions for display.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/pipeline_tutorial.md#2025-04-23_snippet_9

LANGUAGE: python
CODE:
```
>>> preds = audio_classifier(audio_file)
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.1315, 'label': 'calm'}, {'score': 0.1307, 'label': 'neutral'}, {'score': 0.1274, 'label': 'sad'}, {'score': 0.1261, 'label': 'fearful'}, {'score': 0.1242, 'label': 'happy'}]
```

----------------------------------------

TITLE: Setting Up Data Collator for Sequence-to-Sequence Tasks
DESCRIPTION: Creates a data collator that dynamically pads sequences to the maximum length in a batch, which is more efficient than padding all sequences to the same length.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/translation.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
```

----------------------------------------

TITLE: Defining label mappings
DESCRIPTION: This snippet shows how to create mappings between numerical IDs and string labels for the classification task. These mappings are used to connect the model's output to human-readable labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> id2label = {0: "NEGATIVE", 1: "POSITIVE"}
>>> label2id = {"NEGATIVE": 0, "POSITIVE": 1}
```

----------------------------------------

TITLE: Quantized Text Generation with BitsAndBytes in Python
DESCRIPTION: Provides an example of loading and using a quantized GPT-2 model with the bitsandbytes library to reduce memory usage. It configures 4-bit quantization, loads the model and tokenizer, processes an input prompt, and generates text with a maximum length.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gpt2.md#_snippet_3

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype="float16",
    bnb_4bit_use_double_quant=True
)

model = AutoModelForCausalLM.from_pretrained(
    "openai-community/gpt2-xl",
    quantization_config=quantization_config,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2-xl")
inputs = tokenizer("Once upon a time, there was a magical forest", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Using an ONNX Model with ONNX Runtime for Question Answering
DESCRIPTION: Python code demonstrating how to load an exported ONNX model with ONNX Runtime for question answering tasks using the Optimum library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/serialization.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import ORTModelForQuestionAnswering

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> model = ORTModelForQuestionAnswering.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> inputs = tokenizer("What am I using?", "Using DistilBERT with ONNX Runtime!", return_tensors="pt")
>>> outputs = model(**inputs)
```

----------------------------------------

TITLE: Creating Web Demo from Pipeline with Gradio in Python
DESCRIPTION: This snippet demonstrates how to create a web demo for an image classification pipeline using Gradio, enabling easy creation of interactive machine learning applications.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/pipeline_tutorial.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
from transformers import pipeline
import gradio as gr

pipe = pipeline("image-classification", model="google/vit-base-patch16-224")

gr.Interface.from_pipeline(pipe).launch()
```

----------------------------------------

TITLE: Loading the WNUT 17 Dataset
DESCRIPTION: Imports and loads the WNUT 17 dataset from the Hugging Face Datasets library, which contains data for Named Entity Recognition tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/token_classification.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> wnut = load_dataset("wnut_17")
```

----------------------------------------

TITLE: Loading Beans Dataset for Image Classification
DESCRIPTION: Loads the beans dataset using Hugging Face datasets library for knowledge distillation experiment
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/knowledge_distillation_for_image_classification.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from datasets import load_dataset

dataset = load_dataset("beans")
```

----------------------------------------

TITLE: Loading SacreBLEU Evaluation Metric
DESCRIPTION: Load the SacreBLEU metric from Hugging Face Evaluate library to compute translation quality during model training
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/translation.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
import evaluate\n\nmetric = evaluate.load("sacrebleu")
```

----------------------------------------

TITLE: Auto-regressive Text Generation with Key-Value Cache in PyTorch
DESCRIPTION: This code snippet demonstrates auto-regressive text generation using the key-value cache. It passes the `past_key_values` and `use_cache=True` to the model in each forward pass. This mechanism allows the model to reuse previously computed key-value pairs, significantly improving the computational efficiency and reducing memory usage.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_27

LANGUAGE: python
CODE:
```
past_key_values = None # past_key_values is the key-value cache
generated_tokens = []
next_token_id = tokenizer(prompt, return_tensors="pt")["input_ids"].to("cuda")

for _ in range(5):
  next_logits, past_key_values = model(next_token_id, past_key_values=past_key_values, use_cache=True).to_tuple()
  next_logits = next_logits[:, -1:]
  next_token_id = torch.argmax(next_logits, dim=-1)

  print("shape of input_ids", next_token_id.shape)
  print("length of key-value cache", len(past_key_values[0][0]))  # past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]
  generated_tokens.append(next_token_id.item())

generated_text = tokenizer.batch_decode(generated_tokens)
generated_text
```

----------------------------------------

TITLE: Sample FSDP Configuration File
DESCRIPTION: Example YAML configuration demonstrating various FSDP training settings and distributed computing parameters
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/fsdp.md#2025-04-22_snippet_3

LANGUAGE: yaml
CODE:
```
compute_environment: LOCAL_MACHINE
distributed_type: FSDP
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_sharding_strategy: 1
  fsdp_offload_params: true
  fsdp_transformer_layer_cls_to_wrap: BertLayer
```

----------------------------------------

TITLE: Setting Up Metric Callback for TensorFlow Training
DESCRIPTION: Creates a KerasMetricCallback that will calculate ROUGE metrics during training evaluation. This callback uses the previously defined compute_metrics function on the evaluation dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
from transformers.keras_callbacks import KerasMetricCallback

metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_test_set)
```

----------------------------------------

TITLE: Load LJ Speech Dataset (Python)
DESCRIPTION: This code snippet loads the LJ Speech dataset using the `load_dataset` function from the ðŸ¤— Datasets library. It specifies the 'train' split of the dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/preprocessing.md#_snippet_23

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> lj_speech = load_dataset("lj_speech", split="train")
```

----------------------------------------

TITLE: Training Summarization Model with Accelerate Library
DESCRIPTION: This command demonstrates how to train a summarization model using the Accelerate library instead of the Trainer API. Accelerate provides more flexibility for customizing the training loop while maintaining support for distributed training and mixed precision.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/README.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
python run_summarization_no_trainer.py \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir ~/tmp/tst-summarization
```

----------------------------------------

TITLE: Register Model with Auto Classes
DESCRIPTION: This snippet demonstrates how to register the custom `ResnetConfig` and model classes (`ResnetModel`, `ResnetModelForImageClassification`) with the Transformers auto classes (`AutoConfig`, `AutoModel`, `AutoModelForImageClassification`). This allows you to use the auto classes to load and use your custom models seamlessly. The `model_type` in `ResnetConfig` should match the string used to register the configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/custom_models.md#_snippet_7

LANGUAGE: python
CODE:
```
from transformers import AutoConfig, AutoModel, AutoModelForImageClassification

AutoConfig.register("resnet", ResnetConfig)
AutoModel.register(ResnetConfig, ResnetModel)
AutoModelForImageClassification.register(ResnetConfig, ResnetModelForImageClassification)
```

----------------------------------------

TITLE: Multi Image Inference with Chameleon
DESCRIPTION: Shows how to perform inference with multiple images as input to the Chameleon model. The images can belong to the same prompt or different prompts (batched inference). The example showcases feeding images in the order they have to be used in the text prompt.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/chameleon.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import ChameleonProcessor, ChameleonForConditionalGeneration
import torch
from PIL import Image
import requests

processor = ChameleonProcessor.from_pretrained("facebook/chameleon-7b")

model = ChameleonForConditionalGeneration.from_pretrained("facebook/chameleon-7b", torch_dtype=torch.bfloat16, device_map="cuda")

# Get three different images
url = "https://www.ilankelman.org/stopsigns/australia.jpg"
image_stop = Image.open(requests.get(url, stream=True).raw)

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image_cats = Image.open(requests.get(url, stream=True).raw)

url = "https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg"
image_snowman = Image.open(requests.get(url, stream=True).raw)

# Prepare a batched prompt, where the first one is a multi-image prompt and the second is not
prompts = [
    "What do these images have in common?<image><image>",
    "<image>What is shown in this image?"
]

# We can simply feed images in the order they have to be used in the text prompt
# Each "<image>" token uses one image leaving the next for the subsequent "<image>" tokens
inputs = processor(images=[image_stop, image_cats, image_snowman], text=prompts, padding=True, return_tensors="pt").to(device="cuda", dtype=torch.bfloat16)

# Generate
generate_ids = model.generate(**inputs, max_new_tokens=50)
processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
```

----------------------------------------

TITLE: Multi-Image Inference with Chameleon Model
DESCRIPTION: This example shows how to perform inference with multiple images using the Chameleon model, including preparing batched prompts and handling multiple images in a single inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/chameleon.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import ChameleonProcessor, ChameleonForConditionalGeneration
import torch
from PIL import Image
import requests

processor = ChameleonProcessor.from_pretrained("facebook/chameleon-7b")

model = ChameleonForConditionalGeneration.from_pretrained("facebook/chameleon-7b", torch_dtype=torch.bfloat16, device_map="cuda")

# Get three different images
url = "https://www.ilankelman.org/stopsigns/australia.jpg"
image_stop = Image.open(requests.get(url, stream=True).raw)

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image_cats = Image.open(requests.get(url, stream=True).raw)

url = "https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg"
image_snowman = Image.open(requests.get(url, stream=True).raw)

# Prepare batched prompts: first is a multi-image prompt, second is a single-image prompt
prompts = [
    "What do these images have in common?<image><image>",
    "<image>What is depicted in this image?"
]

# We can input the images in the order they should be used in the text prompts
# Each "<image>" token will use one image, and the next "<image>" token will use the next image
inputs = processor(images=[image_stop, image_cats, image_snowman], text=prompts, padding=True, return_tensors="pt").to(device="cuda", dtype=torch.bfloat16)

# Generate
generate_ids = model.generate(**inputs, max_new_tokens=50)
processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
```

----------------------------------------

TITLE: Switching to CPU Execution for Debugging CUDA Errors in Python
DESCRIPTION: This code sets an environment variable to disable CUDA devices, forcing the code to run on CPU. This can help in getting more descriptive error messages for CUDA-related issues.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/troubleshooting.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> import os

>>> os.environ["CUDA_VISIBLE_DEVICES"] = ""
```

----------------------------------------

TITLE: Defining a Custom Pair Classification Pipeline
DESCRIPTION: This code defines a custom pipeline for pair classification tasks. It includes a softmax function for post-processing the model outputs, as well as implementations for the `_sanitize_parameters`, `preprocess`, `_forward`, and `postprocess` methods. The pipeline is designed to handle sequence classification tasks for pairs of sentences, and it can work with both PyTorch and TensorFlow models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/add_new_pipeline.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import numpy as np

from transformers import Pipeline


def softmax(outputs):
    maxes = np.max(outputs, axis=-1, keepdims=True)
    shifted_exp = np.exp(outputs - maxes)
    return shifted_exp / shifted_exp.sum(axis=-1, keepdims=True)


class PairClassificationPipeline(Pipeline):
    def _sanitize_parameters(self, **kwargs):
        preprocess_kwargs = {}
        if "second_text" in kwargs:
            preprocess_kwargs["second_text"] = kwargs["second_text"]
        return preprocess_kwargs, {}, {}

    def preprocess(self, text, second_text=None):
        return self.tokenizer(text, text_pair=second_text, return_tensors=self.framework)

    def _forward(self, model_inputs):
        return self.model(**model_inputs)

    def postprocess(self, model_outputs):
        logits = model_outputs.logits[0].numpy()
        probabilities = softmax(logits)

        best_class = np.argmax(probabilities)
        label = self.model.config.id2label[best_class]
        score = probabilities[best_class].item()
        logits = logits.tolist()
        return {"label": label, "score": score, "logits": logits}
```

----------------------------------------

TITLE: Saving and Exporting a TensorFlow Model Locally
DESCRIPTION: Python code demonstrating how to load a TensorFlow model from Hugging Face Hub, save it locally, and then export it to ONNX format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/serialization.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

>>> # Load tokenizer and TensorFlow weights from the Hub
>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
>>> # Save to disk
>>> tokenizer.save_pretrained("local-tf-checkpoint")
>>> tf_model.save_pretrained("local-tf-checkpoint")
```

----------------------------------------

TITLE: Loading Model with Flash-Attention 2 in Python
DESCRIPTION: This snippet demonstrates loading a model using Flash-Attention 2 in Python, requiring 'transformers' library and a compatible GPU. The model is loaded with 'torch.float16' datatype and a specific attention implementation setting.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/video_llava.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import VideoLlavaForConditionalGeneration

model = VideoLlavaForConditionalGeneration.from_pretrained(
    "LanguageBind/Video-LLaVA-7B-hf", 
    torch_dtype=torch.float16, 
    attn_implementation="flash_attention_2",
).to(0)
```

----------------------------------------

TITLE: Logging into Hugging Face Account in Python
DESCRIPTION: Use the huggingface_hub library for authenticating your Hugging Face account to enable model sharing and uploading during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_captioning.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from huggingface_hub import notebook_login

notebook_login()
```

----------------------------------------

TITLE: Visual Question Answering Pipeline
DESCRIPTION: Demonstrates multi-modal pipeline usage for visual question answering tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/pipeline_tutorial.md#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
from transformers import pipeline

vqa = pipeline(model="impira/layoutlm-document-qa")
output = vqa(
    image="https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png",
    question="What is the invoice number?",
)
```

----------------------------------------

TITLE: Compiling and Fitting a Model with Adam Optimizer in TensorFlow
DESCRIPTION: This code snippet demonstrates how to compile a TensorFlow model with the Adam optimizer and then fit the model to the provided data. The learning rate is set to 3e-5. No explicit loss function is passed, allowing Hugging Face models to automatically choose a suitable one. The data is assumed to be pre-tokenized and labeled.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_12

LANGUAGE: Python
CODE:
```
model.compile(optimizer=Adam(3e-5)) # Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø©!

model.fit(tokenized_data, labels)
```

----------------------------------------

TITLE: Converting Tokens to Input IDs with BERT Tokenizer
DESCRIPTION: This snippet shows how to convert tokenized text to numerical input IDs that the model can process, using the tokenizer's direct conversion functionality.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/glossary.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> inputs = tokenizer(sequence)
>>> encoded_sequence = inputs["input_ids"]
>>> print(encoded_sequence)
[101, 138, 18696, 155, 1942, 3190, 1144, 1572, 13745, 1104, 159, 9664, 2107, 102]
```

----------------------------------------

TITLE: FSDP Configuration for Accelerate
DESCRIPTION: YAML configuration for Fully Sharded Data Parallel training setup with Accelerate, specifying wrapping policies, prefetching, and sharding strategies for efficient model parallelism.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/trainer.md#2025-04-22_snippet_11

LANGUAGE: yaml
CODE:
```
compute_environment: LOCAL_MACHINE
distributed_type: FSDP
downcast_bf16: 'no'
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch_policy: BACKWARD_PRE
  fsdp_forward_prefetch: true
  fsdp_offload_params: false
  fsdp_sharding_strategy: 1
  fsdp_state_dict_type: FULL_STATE_DICT
  fsdp_sync_module_states: true
  fsdp_transformer_layer_cls_to_wrap: BertLayer
  fsdp_use_orig_params: true
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 2
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

----------------------------------------

TITLE: Fine-tuning on Custom Dataset (Bash)
DESCRIPTION: Command to fine-tune a BERT model on custom training and validation files for token classification using the Trainer API.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/token-classification/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
python run_ner.py \
  --model_name_or_path google-bert/bert-base-uncased \
  --train_file path_to_train_file \
  --validation_file path_to_validation_file \
  --output_dir /tmp/test-ner \
  --do_train \
  --do_eval
```

----------------------------------------

TITLE: Exporting Local Model to ONNX for Specific Task
DESCRIPTION: Example command to export a local model to ONNX format for a specific task using Optimum CLI. The --task argument specifies the task the model is intended for, adding task-specific structure to the export.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/serialization.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
optimum-cli export onnx --model local_path --task question-answering distilbert_base_uncased_squad_onnx/
```

----------------------------------------

TITLE: Using a Traced Model for Inference in Python
DESCRIPTION: A simple example showing how to use a traced TorchScript model for inference by calling it directly with input tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/torchscript.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
traced_model(tokens_tensor, segments_tensors)
```

----------------------------------------

TITLE: Configuring GPTQ for Model Quantization
DESCRIPTION: Setting up the GPTQConfig with the desired quantization parameters, including bit width and calibration dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/gptq.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig

model_id = "facebook/opt-125m"
tokenizer = AutoTokenizer.from_pretrained(model_id)
gptq_config = GPTQConfig(bits=4, dataset="c4", tokenizer=tokenizer)
```

----------------------------------------

TITLE: Preparing PyTorch Objects with Accelerator
DESCRIPTION: Preparing PyTorch training components (model, optimizer, scheduler, dataloaders) for distributed training using Accelerator.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)
```

----------------------------------------

TITLE: Speeding-Up Inference with Flash Attention in Python
DESCRIPTION: Illustrates how to leverage Flash Attention 2 for faster model inference. Requires installing the flash-attn Python package and must be run on hardware compatible with Flash-Attention 2, using models loaded in torch.float16 or torch.bfloat16.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llava_next_video.md#2025-04-22_snippet_4

LANGUAGE: Bash
CODE:
```
pip install -U flash-attn --no-build-isolation
```

LANGUAGE: Python
CODE:
```
from transformers import LlavaNextVideoForConditionalGeneration

model = LlavaNextVideoForConditionalGeneration.from_pretrained(
    "llava-hf/LLaVA-NeXT-Video-7B-hf", 
    torch_dtype=torch.float16, 
    attn_implementation="flash_attention_2",
).to(0)
```

----------------------------------------

TITLE: Authenticating with Hugging Face Hub
DESCRIPTION: Python code for logging into the Hugging Face Hub to enable uploading and sharing models with the community.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Loading a Pretrained Feature Extractor with AutoFeatureExtractor in Python
DESCRIPTION: This snippet illustrates how to load a pretrained feature extractor for audio tasks using the AutoFeatureExtractor class.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/autoclass_tutorial.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained(
...     "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
... )
```

----------------------------------------

TITLE: Installing ðŸ¤— Accelerate via pip
DESCRIPTION: Command to install the ðŸ¤— Accelerate library using pip package manager.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/accelerate.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install accelerate
```

----------------------------------------

TITLE: Loading Image Dataset
DESCRIPTION: This code loads the 'food101' dataset, specifically a small subset of the training split (the first 100 examples), using the `load_dataset` function. This sets up the data for image processing and model training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/preprocessing.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> dataset = load_dataset("food101", split="train[:100]")
```

----------------------------------------

TITLE: Performing Inference with the Image-to-Image Pipeline in Python
DESCRIPTION: This code snippet demonstrates how to perform inference with the initialized pipeline to upscale the provided image, yielding an output of larger dimensions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_to_image.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
upscaled = pipe(image)
print(upscaled.size)
```

----------------------------------------

TITLE: Dataset Loading and Processing
DESCRIPTION: Loading DocVQA dataset and performing initial data processing and filtering.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/document_question_answering.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> dataset = load_dataset("nielsr/docvqa_1200_examples")
>>> dataset
```

----------------------------------------

TITLE: Loading Audio Classification Model
DESCRIPTION: Loads a pre-trained Wav2Vec2 model for audio classification using `AutoModelForAudioClassification`. It configures the model with the correct number of labels and mappings between label names and IDs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/audio_classification.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
">>> from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n\n>>> num_labels = len(id2label)
>>> model = AutoModelForAudioClassification.from_pretrained(
...     "facebook/wav2vec2-base", num_labels=num_labels, label2id=label2id, id2label=id2label
... )"
```

----------------------------------------

TITLE: Transcribing Long Audio with Chunking in Python
DESCRIPTION: Creates a pipeline that can handle long audio files by processing in 30 second chunks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/hi/pipeline_tutorial.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
transcriber = pipeline(model="openai/whisper-large-v2", chunk_length_s=30, return_timestamps=True)
transcriber("https://huggingface.co/datasets/sanchit-gandhi/librispeech_long/resolve/main/audio.wav")
```

----------------------------------------

TITLE: Creating Optimizer (TensorFlow)
DESCRIPTION: This creates an optimizer and learning rate schedule for fine-tuning in TensorFlow, defining the batch size, number of training epochs, and total training steps.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/multiple_choice.md#_snippet_14

LANGUAGE: python
CODE:
```
>>> from transformers import create_optimizer

>>> batch_size = 16
>>> num_train_epochs = 2
>>> total_train_steps = (len(tokenized_swag["train"]) // batch_size) * num_train_epochs
>>> optimizer, schedule = create_optimizer(init_lr=5e-5, num_warmup_steps=0, num_train_steps=total_train_steps)
```

----------------------------------------

TITLE: Installing bitsandbytes for 4-bit Quantization
DESCRIPTION: Commands to install the required libraries for 4-bit quantization, including a newer version of bitsandbytes (>=0.39.0) and the latest transformers and accelerate libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/bitsandbytes.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
pip install bitsandbytes>=0.39.0
pip install --upgrade accelerate transformers
```

----------------------------------------

TITLE: Loading and Using HerBERT with Specific Classes in Python
DESCRIPTION: This code snippet demonstrates how to load and use the HerBERT model and tokenizer using specific classes from the transformers library. It encodes a Polish sentence using the tokenizer, passes it to the model, and retrieves the output. This approach offers explicit control over the model and tokenizer selection.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/herbert.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
>>> from transformers import HerbertTokenizer, RobertaModel

>>> tokenizer = HerbertTokenizer.from_pretrained("allegro/herbert-klej-cased-tokenizer-v1")
>>> model = RobertaModel.from_pretrained("allegro/herbert-klej-cased-v1")

>>> encoded_input = tokenizer.encode("Kto ma lepszÄ… sztukÄ™, ma lepszy rzÄ…d â€“ to jasne.", return_tensors="pt")
>>> outputs = model(encoded_input)
```

----------------------------------------

TITLE: Creating Preprocessing Function for Tokenization
DESCRIPTION: Defines a function to preprocess the text data by joining multiple answer strings and tokenizing the result, preparing the data for input to the language model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/language_modeling.md#2025-04-23_snippet_6

LANGUAGE: python
CODE:
```
>>> def preprocess_function(examples):
...     return tokenizer([" ".join(x) for x in examples["answers.text"]])
```

----------------------------------------

TITLE: Preparing Inputs for Zero-shot Classification
DESCRIPTION: This snippet prepares the inputs for the model, including image processing and text tokenization, combined using the processor. This ensures the data is in the right format for the model inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_image_classification.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> candidate_labels = ["tree", "car", "bike", "cat"]
# follows the pipeline prompt template to get same results
>>> candidate_labels = [f'This is a photo of {label}.' for label in candidate_labels]
>>> inputs = processor(images=image, text=candidate_labels, return_tensors="pt", padding=True)
```

----------------------------------------

TITLE: Tokenization Function - Python
DESCRIPTION: Defines a function to tokenize the input text field from the examples. It also implements truncation for sequences that exceed the model's input limits.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True)
```

----------------------------------------

TITLE: Preprocessing Function for SQuAD Dataset
DESCRIPTION: Defines a function to preprocess the SQuAD dataset, including tokenization, handling long contexts, and mapping answer positions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/question_answering.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> def preprocess_function(examples):
...     questions = [q.strip() for q in examples["question"]]
...     inputs = tokenizer(
...         questions,
...         examples["context"],
...         max_length=384,
...         truncation="only_second",
...         return_offsets_mapping=True,
...         padding="max_length",
...     )

...     offset_mapping = inputs.pop("offset_mapping")
...     answers = examples["answers"]
...     start_positions = []
...     end_positions = []

...     for i, offset in enumerate(offset_mapping):
...         answer = answers[i]
...         start_char = answer["answer_start"][0]
...         end_char = answer["answer_start"][0] + len(answer["text"][0])
...         sequence_ids = inputs.sequence_ids(i)

...         # Encuentra el inicio y el fin del contexto
...         idx = 0
...         while sequence_ids[idx] != 1:
...             idx += 1
...         context_start = idx
...         while sequence_ids[idx] == 1:
...             idx += 1
...         context_end = idx - 1

...         # Si la respuesta entera no estÃ¡ dentro del contexto, etiquÃ©tala como (0, 0)
...         if offset[context_start][0] > end_char or offset[context_end][1] < start_char:
...             start_positions.append(0)
...             end_positions.append(0)
...         else:
...             # De lo contrario, esta es la posiciÃ³n de los tokens de inicio y fin
...             idx = context_start
...             while idx <= context_end and offset[idx][0] <= start_char:
...                 idx += 1
...             start_positions.append(idx - 1)

...             idx = context_end
...             while idx >= context_start and offset[idx][1] >= end_char:
...                 idx -= 1
...             end_positions.append(idx + 1)

...     inputs["start_positions"] = start_positions
...     inputs["end_positions"] = end_positions
...     return inputs
```

----------------------------------------

TITLE: Running Distributed Training in Notebook Environment
DESCRIPTION: Code for launching distributed training in a notebook environment (like Colaboratory) using the notebook_launcher function.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/accelerate.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> from accelerate import notebook_launcher

>>> notebook_launcher(training_function)
```

----------------------------------------

TITLE: Launching Training in Notebook
DESCRIPTION: Code to launch distributed training in a Jupyter notebook environment using notebook_launcher
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/accelerate.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

----------------------------------------

TITLE: Truncating Long Sequences to Model Maximum Length
DESCRIPTION: Shows how to truncate sequences that exceed the model's maximum accepted length by setting the truncation parameter to True, ensuring inputs are compatible with model constraints.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/preprocessing.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)
>>> print(encoded_input)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0], 
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], 
               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]], 
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}
```

----------------------------------------

TITLE: Configuring DeepSpeed ZeRO-2 with Auto Settings (JSON)
DESCRIPTION: This JSON configuration file demonstrates a DeepSpeed ZeRO-2 setup with automatic settings for fp16, optimizer, scheduler, gradient accumulation, and more.  It provides a starting point for automatic configuration and optimization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/deepspeed.md#2025-04-22_snippet_28

LANGUAGE: json
CODE:
```
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": "auto",
            "betas": "auto",
            "eps": "auto",
            "weight_decay": "auto"
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },

    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "allgather_partitions": true,
        "allgather_bucket_size": 2e8,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2e8,
        "contiguous_gradients": true
    },

    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 2000,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false
}
```

----------------------------------------

TITLE: Loading Checkpoint in Python
DESCRIPTION: This Python code snippet demonstrates how to load a saved state from a directory containing checkpoints using the 'accelerator.load_state' method. The input is the path to the directory, and it depends on the 'accelerate' library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/fsdp.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
accelerator.load_state("directory/containing/checkpoints")
```

----------------------------------------

TITLE: Implementing Audio Classification with Hugging Face Pipeline in Python
DESCRIPTION: This snippet demonstrates how to use the Hugging Face pipeline for audio classification tasks. It loads a pre-trained HuBERT model to classify emotional tones in an audio file, returning predictions with confidence scores for different emotional states (happy, sad, neutral, angry).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/task_summary.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline(task="audio-classification", model="superb/hubert-base-superb-er")
>>> preds = classifier("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.4532, 'label': 'hap'},
 {'score': 0.3622, 'label': 'sad'},
 {'score': 0.0943, 'label': 'neu'},
 {'score': 0.0903, 'label': 'ang'}]
```

----------------------------------------

TITLE: Image-Guided Text Generation with IDEFICS in Python
DESCRIPTION: This snippet demonstrates how to generate text based on an image using the IDEFICS model. It takes an image URL and an instruction as input. The model then generates a story related to the image. Parameters such as `num_beams` and `max_new_tokens` control the text generation process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/idefics.md#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
>>> prompt = ["Instruction: Use the image to write a story. \n",
...     "https://images.unsplash.com/photo-1517086822157-2b0358e7684a?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=2203&q=80",
...     "Story: \n"]

>>> inputs = processor(prompt, return_tensors="pt").to("cuda")
>>> bad_words_ids = processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, num_beams=2, max_new_tokens=200, bad_words_ids=bad_words_ids)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> print(generated_text[0]) 
Instruction: Use the image to write a story. 
 Story: 
Once upon a time, there was a little girl who lived in a house with a red door.  She loved her red door.  It was the prettiest door in the whole world.

One day, the little girl was playing in her yard when she noticed a man standing on her doorstep.  He was wearing a long black coat and a top hat.

The little girl ran inside and told her mother about the man.

Her mother said, â€œDonâ€™t worry, honey.  Heâ€™s just a friendly ghost.â€

The little girl wasnâ€™t sure if she believed her mother, but she went outside anyway.

When she got to the door, the man was gone.

The next day, the little girl was playing in her yard again when she noticed the man standing on her doorstep.

He was wearing a long black coat and a top hat.

The little girl ran
```

----------------------------------------

TITLE: Preprocessing Audio Data with Sampling Rate
DESCRIPTION: This snippet shows how to preprocess audio data using a feature extractor and specifying the sampling rate. It processes the raw audio signal (`array`) with the feature extractor, setting the `sampling_rate` parameter to 16000.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/feature_extractors.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
```py
processed_dataset = feature_extractor(dataset[0]["audio"]["array"], sampling_rate=16000)
processed_dataset
{'input_values': [array([ 9.4472744e-05,  3.0777880e-03, -2.8888427e-03, ...,\n       -2.8888427e-03,  9.4472744e-05,  9.4472744e-05], dtype=float32)]}
```
```

----------------------------------------

TITLE: Loading a BitNet Quantized Model in Python with Transformers
DESCRIPTION: This code snippet demonstrates how to load a BitNet quantized model using the Hugging Face Transformers library. It uses the `AutoModelForCausalLM` class to initialize the model from a specified path.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/bitnet.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM
path = "/path/to/model"
model = AutoModelForCausalLM.from_pretrained(path, device_map="auto")
```

----------------------------------------

TITLE: Multi-GPU Training Benchmark with DP and DDP
DESCRIPTION: Benchmark code comparing performance between DataParallel (DP) and DistributedDataParallel (DDP) modes, with and without NVLink, using GPT-2 model on WikiText dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_train_gpu_many.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
# DP
rm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 \
python examples/pytorch/language-modeling/run_clm.py \
--model_name_or_path openai-community/gpt2 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 \
--do_train --output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

# DDP w/ NVlink
rm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 \
torchrun --nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py \
--model_name_or_path openai-community/gpt2 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 \
--do_train --output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

# DDP w/o NVlink
rm -r /tmp/test-clm; NCCL_P2P_DISABLE=1 CUDA_VISIBLE_DEVICES=0,1 \
torchrun --nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py \
--model_name_or_path openai-community/gpt2 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 \
--do_train --output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200
```

----------------------------------------

TITLE: Running DeepSpeed T0 Script
DESCRIPTION: Command line examples for running the T0 script with DeepSpeed on 1 or 2 GPUs using either deepspeed CLI or torch distributed launch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/deepspeed.md#2025-04-22_snippet_44

LANGUAGE: bash
CODE:
```
$ deepspeed --num_gpus 2 t0.py
rank0:
   in=Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy
  out=Positive
rank1:
   in=Is this review positive or negative? Review: this is the worst restaurant ever
  out=negative
```

----------------------------------------

TITLE: Loading a Model with GPTQConfig and Memory Constraints
DESCRIPTION: This code snippet demonstrates how to load a pre-trained causal language model with GPTQ quantization, while also specifying memory constraints for each device (GPU and CPU). This is useful when dealing with large datasets or limited hardware resources. Requires `transformers` and a properly configured `gptq_config` object.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/gptq.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
"quantized_model = AutoModelForCausalLM.from_pretrained(\n    \"facebook/opt-125m\",\n    device_map=\"auto\",\n    max_memory={0: \"30GiB\", 1: \"46GiB\", \"cpu\": \"30GiB\"},\n    quantization_config=gptq_config\n)"
```

----------------------------------------

TITLE: Loading Quantized IDEFICS Model in Python
DESCRIPTION: The snippet describes loading a quantized version of the IDEFICS model which helps in reducing memory usage by loading the model in 4-bit precision. It involves configuring the `BitsAndBytesConfig` during model loading.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md#2025-04-22_snippet_2

LANGUAGE: py
CODE:
```
>>> import torch
>>> from transformers import IdeficsForVisionText2Text, AutoProcessor, BitsAndBytesConfig

>>> quantization_config = BitsAndBytesConfig(
...     load_in_4bit=True,
...     bnb_4bit_compute_dtype=torch.float16,
... )

>>> processor = AutoProcessor.from_pretrained(checkpoint)

>>> model = IdeficsForVisionText2Text.from_pretrained(
...     checkpoint,
...     quantization_config=quantization_config,
...     device_map="auto"
... )
```

----------------------------------------

TITLE: Preprocessing Inputs for Video-Text-to-Text Model
DESCRIPTION: This code prepares the input for the video-text-to-text model. It constructs a prompt that includes image tokens representing the video frames and a user question, then preprocesses the prompt and images using the LlavaProcessor to create input tensors ready for the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/video_text_to_text.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
"user_prompt = \"Are these two cats in these two videos doing the same thing?\" 
toks = \"<image>\" * 12
prompt = \"<|im_start|>user\"+ toks + f\"\n{user_prompt}<|im_end|><|im_start|>assistant\" 
inputs = processor(text=prompt, images=videos, return_tensors=\"pt\").to(model.device, model.dtype)"
```

----------------------------------------

TITLE: Creating Training Subset for Fine-tuning
DESCRIPTION: Creates smaller training and evaluation datasets to reduce fine-tuning time
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/training.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
```

----------------------------------------

TITLE: Preparing Image and Question for Zero-shot VQA
DESCRIPTION: Shows how to prepare the image and format the question for BLIP-2's zero-shot visual question answering capabilities.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_question_answering.md#2025-04-22_snippet_22

LANGUAGE: python
CODE:
```
>>> example = dataset[0]
>>> image = Image.open(example['image_id'])
>>> question = example['question']
```

----------------------------------------

TITLE: Creating Evaluation Metrics Function for TensorFlow
DESCRIPTION: TensorFlow code for defining a function to compute evaluation metrics using the Mean IoU metric, with detailed per-category accuracy and IoU reporting.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
>>> def compute_metrics(eval_pred):
...     logits, labels = eval_pred
...     logits = tf.transpose(logits, perm=[0, 2, 3, 1])
...     logits_resized = tf.image.resize(
...         logits,
...         size=tf.shape(labels)[1:],
...         method="bilinear",
...     )

...     pred_labels = tf.argmax(logits_resized, axis=-1)
...     metrics = metric.compute(
...         predictions=pred_labels,
...         references=labels,
...         num_labels=num_labels,
...         ignore_index=-1,
...         reduce_labels=image_processor.do_reduce_labels,
...     )

...     per_category_accuracy = metrics.pop("per_category_accuracy").tolist()
...     per_category_iou = metrics.pop("per_category_iou").tolist()

...     metrics.update({f"accuracy_{id2label[i]}": v for i, v in enumerate(per_category_accuracy)})
...     metrics.update({f"iou_{id2label[i]}": v for i, v in enumerate(per_category_iou)})
...     return {"val_" + k: v for k, v in metrics.items()}
```

----------------------------------------

TITLE: Using Transformers for Region-Specific OCR in Python
DESCRIPTION: This Python snippet demonstrates how to use the GOT-OCR v2 model from the 'transformers' library to perform Optical Character Recognition (OCR) on a specified region of an image. The example shows initialization, processing of an image with specified bounding box coordinates or colors, and decoding the generated text. The code assumes access to a CUDA-compatible device if available, and proper installation of the 'transformers' and 'torch' libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/got_ocr2.md#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoProcessor, AutoModelForImageTextToText
>>> import torch

>>> device = "cuda" if torch.cuda.is_available() else "cpu"
>>> model = AutoModelForImageTextToText.from_pretrained("stepfun-ai/GOT-OCR-2.0-hf", device_map=device)
>>> processor = AutoProcessor.from_pretrained("stepfun-ai/GOT-OCR-2.0-hf", use_fast=True)

>>> image = "https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/multi_box.png"
>>> inputs = processor(image, return_tensors="pt", color="green", device=device).to(device) # or box=[x1, y1, x2, y2] for coordinates (image pixels)

>>> generate_ids = model.generate(
...     **inputs,
...     do_sample=False,
...     tokenizer=processor.tokenizer,
...     stop_strings="<|im_end|>",
...     max_new_tokens=4096,
... )

>>> processor.decode(generate_ids[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
"You should keep in mind what features from the module should be used, especially \nwhen youâ€™re planning to sell a template."

```

----------------------------------------

TITLE: Loading and Filtering Dataset using Datasets Library
DESCRIPTION: This snippet loads the UFO sightings dataset using the Datasets library, specifically filtering out examples where the 'specific_detail_query' column is null. It focuses on preparing the dataset for indexing by ensuring only relevant data is included.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_document_retrieval.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from datasets import load_dataset

dataset = load_dataset("davanstrien/ufo-ColPali")
dataset = dataset["train"]
dataset = dataset.filter(lambda example: example["specific_detail_query"] is not None)
dataset
```

----------------------------------------

TITLE: Loading the IMDb Dataset for Sentiment Analysis
DESCRIPTION: Code to load the IMDb movie reviews dataset using the Hugging Face datasets library, which contains labeled positive and negative reviews for sentiment analysis training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/sequence_classification.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> imdb = load_dataset("imdb")
```

----------------------------------------

TITLE: Loading a TorchScript BERT Model in Python
DESCRIPTION: This code example shows how to load a previously saved TorchScript BERT model from disk and use it with dummy input for inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/torchscript.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
loaded_model = torch.jit.load("traced_bert.pt")
loaded_model.eval()

all_encoder_layers, pooled_output = loaded_model(*dummy_input)
```

----------------------------------------

TITLE: Training a Masked Language Model with TensorFlow
DESCRIPTION: Trains the model using Keras' fit method, specifying training and validation datasets, number of epochs, and the PushToHub callback.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/masked_language_modeling.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])
```

----------------------------------------

TITLE: Loading and Running GPTBigCode with Flash Attention 2
DESCRIPTION: Python code snippet demonstrating how to load the GPTBigCode model with Flash Attention 2 and generate text. It includes importing necessary modules, loading the model and tokenizer, and generating text from a given prompt.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gpt_bigcode.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
device = "cuda" # the device to load the model onto

model = AutoModelForCausalLM.from_pretrained("bigcode/gpt_bigcode-santacoder", torch_dtype=torch.float16, attn_implementation="flash_attention_2")
tokenizer = AutoTokenizer.from_pretrained("bigcode/gpt_bigcode-santacoder")

prompt = "def hello_world():"

model_inputs = tokenizer([prompt], return_tensors="pt").to(device)
model.to(device)

generated_ids = model.generate(**model_inputs, max_new_tokens=30, do_sample=False)
tokenizer.batch_decode(generated_ids)[0]
```

----------------------------------------

TITLE: Using StableLM with Flash Attention 2 for Optimized Inference
DESCRIPTION: This snippet demonstrates how to load and run the StableLM model with Flash Attention 2 optimization. It loads the model in bfloat16 precision and explicitly configures it to use flash_attention_2 implementation for attention calculations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/stablelm.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> import torch
>>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed
>>> device = "cuda" # the device to load the model onto

>>> set_seed(0)

>>> tokenizer = AutoTokenizer.from_pretrained("stabilityai/stablelm-3b-4e1t")
>>> model = AutoModelForCausalLM.from_pretrained("stabilityai/stablelm-3b-4e1t", torch_dtype=torch.bfloat16, attn_implementation="flash_attention_2")  # doctest: +SKIP
>>> model.to(device)  # doctest: +SKIP

>>> model_inputs = tokenizer("The weather is always wonderful in", return_tensors="pt").to(model.device)

>>> generated_ids = model.generate(**model_inputs, max_length=32, do_sample=True)  # doctest: +SKIP
>>> responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)  # doctest: +SKIP
>>> responses  # doctest: +SKIP
['The weather is always wonderful in Costa Rica, which makes it a prime destination for retirees. That\'s where the Pensionado program comes in, offering']
```

----------------------------------------

TITLE: Multi-Node Configuration with Local Storage
DESCRIPTION: Configure DeepSpeed to use node-local storage for checkpointing when shared storage is not available
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/deepspeed.md#2025-04-22_snippet_26

LANGUAGE: yaml
CODE:
```
{
  "checkpoint": {
    "use_node_local_storage": true
  }
}
```

----------------------------------------

TITLE: Fine-tuning Additional MMS CTC Adapter for Swedish Speech Recognition
DESCRIPTION: Script for fine-tuning a second adapter (for Swedish) in the same MMS model. This approach reuses the output directory and extends the existing tokenizer vocabulary rather than overwriting it.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-recognition/README.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
python run_speech_recognition_ctc.py \
	--dataset_name="common_voice" \
	--model_name_or_path="facebook/mms-1b-all" \
	--dataset_config_name="sw" \
	--output_dir="./wav2vec2-common_voice-tr-mms-demo" \
	--tokenizer_name_or_path="./wav2vec2-common_voice-tr-mms-demo" \
	--num_train_epochs="4" \
	--per_device_train_batch_size="32" \
	--learning_rate="1e-3" \
	--warmup_steps="100" \
	--eval_strategy="steps" \
	--text_column_name="sentence" \
	--length_column_name="input_length" \
	--save_steps="200" \
	--eval_steps="100" \
	--save_total_limit="3" \
  --target_language="swe" \
	--gradient_checkpointing \
	--chars_to_ignore , ? . ! - \; \: \" " % ' " ï¿½ \
	--fp16 \
	--group_by_length \
	--do_train --do_eval \
  --push_to_hub
```

----------------------------------------

TITLE: Configuring Gradient Accumulation in DeepSpeed
DESCRIPTION: Configuration for gradient accumulation in DeepSpeed, which accumulates gradients over several mini-batches before updating parameters. Setting "auto" allows Trainer to use the value from gradient_accumulation_steps argument.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/deepspeed.md#2025-04-22_snippet_14

LANGUAGE: yaml
CODE:
```
{
    "gradient_accumulation_steps": "auto"
}
```

----------------------------------------

TITLE: Using Pipeline API for Sentiment Analysis in Transformers
DESCRIPTION: A code snippet demonstrating how to use the pipeline API from the Hugging Face Transformers library to perform sentiment analysis on text.
SOURCE: https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline
```

----------------------------------------

TITLE: Loading and Processing Image Datasets with HuggingFace
DESCRIPTION: Python code demonstrating different ways to load image datasets using HuggingFace's ImageFolder functionality, including loading from local folders, compressed files, and remote sources.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/image-classification/README.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from datasets import load_dataset

# example 1: local folder
dataset = load_dataset("imagefolder", data_dir="path_to_your_folder")

# example 2: local files (supported formats are tar, gzip, zip, xz, rar, zstd)
dataset = load_dataset("imagefolder", data_files="path_to_zip_file")

# example 3: remote files (supported formats are tar, gzip, zip, xz, rar, zstd)
dataset = load_dataset("imagefolder", data_files="https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip")

# example 4: providing several splits
dataset = load_dataset("imagefolder", data_files={"train": ["path/to/file1", "path/to/file2"], "test": ["path/to/file3", "path/to/file4"]})
```

----------------------------------------

TITLE: Splitting Dataset into Train and Test Sets
DESCRIPTION: Code to split the BillSum dataset into training and testing subsets using a 80/20 split ratio. This creates separate datasets for model training and evaluation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/summarization.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> billsum = billsum.train_test_split(test_size=0.2)
```

----------------------------------------

TITLE: Saving TensorFlow Model Weights with h5 Extension in Transformers
DESCRIPTION: This snippet demonstrates how to save TensorFlow model weights using the `h5` file extension and reload the model using `TFPreTrainedModel.from_pretrained`. This avoids potential issues with the standard TensorFlow `model.save` method when used with Transformers.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/troubleshooting.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> from transformers import TFPreTrainedModel

>>> model.save_weights("some_folder/tf_model.h5")
>>> model = TFPreTrainedModel.from_pretrained("some_folder")
```

----------------------------------------

TITLE: Loading Dataset and Feature Extractor
DESCRIPTION: This snippet illustrates how to load a dataset and a feature extractor using `load_dataset` from the `datasets` library and `AutoFeatureExtractor.from_pretrained` from the `transformers` library. It prepares the environment for audio data preprocessing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/feature_extractors.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
```py
from datasets import load_dataset, Audio
from transformers import AutoFeatureExtractor

dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")
feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
```
```

----------------------------------------

TITLE: Compiling the model (TensorFlow)
DESCRIPTION: This snippet demonstrates how to compile the model for training using the `compile` method. It specifies the optimizer, but does not require a loss function as the model has a task-relevant loss function by default.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)  # No loss argument!
```

----------------------------------------

TITLE: Benchmarking Transformers Model Generation Performance (Quantized vs. BF16)
DESCRIPTION: This snippet provides a utility function `benchmark_fn` using `torch._inductor.utils.do_bench_using_profiling` and then applies it to measure the text generation speed of a previously defined `quantized_model` and a newly loaded `bf16_model`. It requires `MAX_NEW_TOKENS` and `input_ids` variables to be defined in the scope.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/torchao.md#_snippet_17

LANGUAGE: python
CODE:
```
from torch._inductor.utils import do_bench_using_profiling
from typing import Callable

def benchmark_fn(func: Callable, *args, **kwargs) -> float:
    """Thin wrapper around do_bench_using_profiling"""
    no_args = lambda: func(*args, **kwargs)
    time = do_bench_using_profiling(no_args)
    return time * 1e3

MAX_NEW_TOKENS = 1000
print("int4wo-128 model:", benchmark_fn(quantized_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation="static"))

bf16_model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", torch_dtype=torch.bfloat16)
output = bf16_model.generate(**input_ids, max_new_tokens=10, cache_implementation="static") # auto-compile
print("bf16 model:", benchmark_fn(bf16_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation="static"))
```

----------------------------------------

TITLE: Customizing DistilBERT Configuration in Python
DESCRIPTION: This code demonstrates how to create a custom configuration for DistilBERT by modifying specific attributes like activation function and attention dropout rate.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/create_a_model.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
my_config = DistilBertConfig(activation="relu", attention_dropout=0.4)
print(my_config)
```

----------------------------------------

TITLE: Output Length Control Example - Python
DESCRIPTION: Demonstration of controlling output length using max_new_tokens parameter.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/llm_tutorial.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
model_inputs = tokenizer(["A sequence of numbers: 1, 2"], return_tensors="pt").to("cuda")

# By default, the output will contain up to 20 tokens
generated_ids = model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

# Setting `max_new_tokens` allows you to control the maximum length
generated_ids = model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=50)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

----------------------------------------

TITLE: Preprocessing Function for Wav2Vec2 Input
DESCRIPTION: Defines a function to preprocess the dataset by extracting input values from audio files and tokenizing transcriptions using the Wav2Vec2 processor.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/asr.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> def prepare_dataset(batch):
...     audio = batch["audio"]
...     batch = processor(audio["array"], sampling_rate=audio["sampling_rate"], text=batch["transcription"])
...     batch["input_length"] = len(batch["input_values"][0])
...     return batch
```

----------------------------------------

TITLE: Preparing TensorFlow Datasets for Question Answering
DESCRIPTION: This code prepares the training and validation datasets in TensorFlow format, including shuffling and batching for the question answering model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/question_answering.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
tf_train_set = model.prepare_tf_dataset(
    tokenized_squad["train"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_set = model.prepare_tf_dataset(
    tokenized_squad["test"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)
```

----------------------------------------

TITLE: PyTorch Manual Summarization - Tokenization
DESCRIPTION: Demonstrates how to tokenize input text using PyTorch for manual summarization inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_25

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("username/my_awesome_billsum_model")
inputs = tokenizer(text, return_tensors="pt").input_ids
```

----------------------------------------

TITLE: Data Preprocessing Setup
DESCRIPTION: Creating label mappings and preprocessing function for audio data preparation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/audio_classification.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
def preprocess_function(examples):
    audio_arrays = [x["array"] for x in examples["audio"]]
    inputs = feature_extractor(
        audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True
    )
    return inputs
```

----------------------------------------

TITLE: Loading Pretrained PyTorch Model with Custom Configuration
DESCRIPTION: Shows how to load a pretrained DistilBertModel but override its default configuration with a custom one, allowing for selective customization of model attributes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/create_a_model.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> model = DistilBertModel.from_pretrained("distilbert/distilbert-base-uncased", config=my_config)
```

----------------------------------------

TITLE: Multi-Node Deployment with Torchrun
DESCRIPTION: Launch a distributed training job across two nodes with eight GPUs each using torchrun
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/deepspeed.md#2025-04-22_snippet_27

LANGUAGE: bash
CODE:
```
torchrun --nproc_per_node=8 --nnode=2 --node_rank=0 --master_addr=hostname1 \
--master_port=9901 your_program.py <normal cl args> --deepspeed ds_config.json
```

----------------------------------------

TITLE: Loading XLM Model and Tokenizer for Causal Language Modeling (Python)
DESCRIPTION: This snippet demonstrates how to load an XLM tokenizer and model for causal language modeling in English and French. It shows how to access the language IDs from the tokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/multilingual.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import XLMTokenizer, XLMWithLMHeadModel

tokenizer = XLMTokenizer.from_pretrained("FacebookAI/xlm-clm-enfr-1024")
model = XLMWithLMHeadModel.from_pretrained("FacebookAI/xlm-clm-enfr-1024")

print(tokenizer.lang2id)
```

----------------------------------------

TITLE: Performing Inference with XLM Model
DESCRIPTION: This snippet shows how to perform inference with an XLM model using prepared input IDs and language embeddings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/multilingual.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
outputs = model(input_ids, langs=langs)
```

----------------------------------------

TITLE: Creating Smaller Dataset Subset
DESCRIPTION: This snippet creates smaller subsets of the training and evaluation datasets for faster fine-tuning.  The datasets are shuffled with a seed for reproducibility and then selects the first 1000 samples.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_18

LANGUAGE: Python
CODE:
```
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
```

----------------------------------------

TITLE: Installing Required Libraries in Bash
DESCRIPTION: The code snippet provides the bash command needed to install essential Python libraries required to run IDEFICS examples. It includes `bitsandbytes`, `sentencepiece`, `accelerate`, and `transformers`, which are necessary to implement the tasks described using the IDEFICS model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install -q bitsandbytes sentencepiece accelerate transformers
```

----------------------------------------

TITLE: Load Image Feature Extractor (Python)
DESCRIPTION: Loads a pre-trained feature extractor for image data using `AutoFeatureExtractor.from_pretrained`. This prepares the image data for use with a vision model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/preprocessing.md#_snippet_18

LANGUAGE: python
CODE:
```
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("google/vit-base-patch16-224")
```

----------------------------------------

TITLE: Installing Required Libraries
DESCRIPTION: Commands to install the necessary Python packages for multiple choice model training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/multiple_choice.md#2025-04-23_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers datasets evaluate
```

----------------------------------------

TITLE: DeepSpeed with Accelerate Plugin Configuration
DESCRIPTION: YAML configuration for DeepSpeed with direct parameter settings through Accelerate's plugin system, including gradient accumulation, clipping, and CPU offloading settings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/trainer.md#2025-04-22_snippet_13

LANGUAGE: yaml
CODE:
```
compute_environment: LOCAL_MACHINE
deepspeed_config:
  gradient_accumulation_steps: 1
  gradient_clipping: 0.7
  offload_optimizer_device: cpu
  offload_param_device: cpu
  zero3_init_flag: true
  zero_stage: 2
distributed_type: DEEPSPEED
downcast_bf16: 'no'
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 4
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false

```

----------------------------------------

TITLE: Processing Image for Manual Depth Estimation
DESCRIPTION: Shows how to prepare an image for depth estimation using the image processor, including necessary transformations like resizing and normalization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/monocular_depth_estimation.md#2025-04-23_snippet_6

LANGUAGE: python
CODE:
```
>>> pixel_values = image_processor(image, return_tensors="pt").pixel_values
```

----------------------------------------

TITLE: Testing Transformers Inference Server with cURL in Bash
DESCRIPTION: This bash command uses cURL to send a POST request to the local Transformers inference server for testing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/pipeline_webserver.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
curl -X POST -d "test [MASK]" http://localhost:8000/
```

----------------------------------------

TITLE: Pushing TensorFlow model to Hub with PushToHubCallback
DESCRIPTION: This Python code shows how to use `PushToHubCallback` to automatically push a TensorFlow model to the Hugging Face Hub during training.  It initializes the callback and adds it to the `callbacks` argument of the `model.fit` method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
"from transformers import PushToHubCallback\n\npush_to_hub_callback = PushToHubCallback(\n    output_dir=\"./your_model_save_path\", tokenizer=tokenizer, hub_model_id=\"your-username/my-awesome-model\"\n)\nmodel.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3, callbacks=push_to_hub_callback)"
```

----------------------------------------

TITLE: Loading a Model with GGUF Format using Transformers in Python
DESCRIPTION: This Python snippet demonstrates loading a model using the GGUF file format with the Transformers library. It involves importing the AutoTokenizer and AutoModelForCausalLM classes. There is a requirement to specify the 'gguf_file', and the code allows the choice of different PyTorch data types. Prerequisites include installing the 'gguf' package and having access to the specified model ID and GGUF filename. Expected output is a tokenizer and model object ready for inference or further training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/gguf.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
# pip install gguf
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF"
filename = "tinyllama-1.1b-chat-v1.0.Q6_K.gguf"

torch_dtype = torch.float32 # could be torch.float16 or torch.bfloat16 too
tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)
model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename, torch_dtype=torch_dtype)
```

----------------------------------------

TITLE: Fine-tuning the Image Captioning Model
DESCRIPTION: Starts the training process to fine-tune the GIT model on the Pokemon dataset for image captioning.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_captioning.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
trainer.train()
```

----------------------------------------

TITLE: Generating Text with TensorFlow Model
DESCRIPTION: This snippet shows how to generate text using the finetuned model in TensorFlow, including loading the model and decoding the output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_25

LANGUAGE: python
CODE:
```
from transformers import TFAutoModelForCausalLM

model = TFAutoModelForCausalLM.from_pretrained("username/my_awesome_eli5_clm-model")
outputs = model.generate(input_ids=inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)

tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

----------------------------------------

TITLE: Using Mistral with Flash Attention 2 for Improved Performance
DESCRIPTION: Demonstrates how to load and use the Mistral model with Flash Attention 2 for faster inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/mistral.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> import torch
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1", torch_dtype=torch.float16, attn_implementation="flash_attention_2", device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")

>>> prompt = "My favourite condiment is"

>>> model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
>>> model.to(device)

>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"My favourite condiment is to (...)"
```

----------------------------------------

TITLE: PyTorch Summarization Training Script
DESCRIPTION: Example command for running a summarization training script using PyTorch with T5-small model on CNN/DailyMail dataset. Includes configuration for training and evaluation parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/run_scripts.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Pushing Chat Template to Hub with Transformers
DESCRIPTION: This Python snippet shows how to set the `chat_template` attribute on a tokenizer and push it to the Hugging Face Hub.  It allows for persistance and sharing of the customized template.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating_writing.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
tokenizer.chat_template = template
tokenizer.push_to_hub("model_name")
```

----------------------------------------

TITLE: Applying Chat Template Using Transformers in Python
DESCRIPTION: This snippet demonstrates how to use the `AutoProcessor` from the Hugging Face Transformers library to apply a chat template to a conversation structured as a list of dictionaries. The template formats the prompt for processing with specific roles and content. The user must still tokenize the formatted text and handle image pixel values independently.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/vipllava.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("llava-hf/vip-llava-7b-hf")

conversation = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": "Whatâ€™s shown in this image?"},
        ],
    },
    {
        "role": "assistant",
        "content": [{"type": "text", "text": "This image shows a red stop sign."},]
    },
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "Describe the image in more details."},
        ],
    },
]

text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)

# Note that the template simply formats your prompt, you still have to tokenize it and obtain pixel values for your images
print(text_prompt)
>>> "###Human: <image>\nWhatâ€™s shown in this image?###Assistant: This image shows a red stop sign.###Human: Describe the image in more details.###Assistant:"

```

----------------------------------------

TITLE: Using Flash Attention with SigLIP for Memory Efficiency
DESCRIPTION: Shows how to enable Flash Attention 2 with SigLIP for more memory-efficient attention computation. This requires installing the flash-attn package and specifying the attention implementation when loading the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/siglip.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
# pip install -U flash-attn --no-build-isolation

from transformers import SiglipModel

model = SiglipModel.from_pretrained(
    "google/siglip-so400m-patch14-384",
    attn_implementation="flash_attention_2",
    torch_dtype=torch.float16,
    device_map=device,
)
```

----------------------------------------

TITLE: Manual ASR Inference with Fine-tuned Model in Python
DESCRIPTION: This set of snippets shows how to manually perform ASR inference using a fine-tuned model. It includes loading the processor and model, processing the audio input, running inference, and decoding the output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/asr.md#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("stevhliu/my_awesome_asr_mind_model")
>>> inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")

>>> from transformers import AutoModelForCTC

>>> model = AutoModelForCTC.from_pretrained("stevhliu/my_awesome_asr_mind_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> import torch

>>> predicted_ids = torch.argmax(logits, dim=-1)
>>> transcription = processor.batch_decode(predicted_ids)
>>> transcription
['I WOUL LIKE O SET UP JOINT ACOUNT WTH Y PARTNER']
```

----------------------------------------

TITLE: Creating and Training a Tokenizer with ðŸ¤— Tokenizers
DESCRIPTION: This snippet creates a tokenizer using the BPE algorithm from the `tokenizers` library. It initializes the tokenizer, sets up a trainer with special tokens, uses whitespace for pre-tokenization, and trains the tokenizer on a given set of files. The `files` variable is a placeholder and would need to be replaced with the actual list of training files.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/fast_tokenizers.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> from tokenizers import Tokenizer
>>> from tokenizers.models import BPE
>>> from tokenizers.trainers import BpeTrainer
>>> from tokenizers.pre_tokenizers import Whitespace

>>> tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
>>> trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

>>> tokenizer.pre_tokenizer = Whitespace()
>>> files = [...]
>>> tokenizer.train(files, trainer)
```

----------------------------------------

TITLE: Using Batch Size for Pipeline
DESCRIPTION: This snippet demonstrates how to enable batching for the pipeline to improve performance. It sets the 'batch_size' parameter to 2, which means that the pipeline will process the input in batches of 2. This is particularly useful when processing multiple inputs, as it can reduce the overhead associated with processing each input individually.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/tutoriel_pipeline.md#_snippet_6

LANGUAGE: Python
CODE:
```
transcriber = pipeline(model="openai/whisper-large-v2", device=0, batch_size=2)
audio_filenames = [f"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/{i}.flac" for i in range(1, 5)]
texts = transcriber(audio_filenames)
```

----------------------------------------

TITLE: Pushing Dataset to HuggingFace Hub
DESCRIPTION: Python code showing how to upload a dataset to the HuggingFace Hub, including options for private repositories.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/image-classification/README.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
# assuming you have ran the huggingface-cli login command in a terminal
dataset.push_to_hub("name_of_your_dataset")

# if you want to push to a private repo, simply pass private=True:
dataset.push_to_hub("name_of_your_dataset", private=True)
```

----------------------------------------

TITLE: Flattening Nested Dataset Structure
DESCRIPTION: Flattens the nested structure of the dataset to make the answer texts accessible as a separate column, which simplifies the tokenization process for the language modeling task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/language_modeling.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
>>> eli5 = eli5.flatten()
>>> eli5["train"][0]
```

----------------------------------------

TITLE: Running Code on CPU by Setting CUDA_VISIBLE_DEVICES Environment Variable
DESCRIPTION: This code snippet shows how to force the execution of the script on the CPU by setting the `CUDA_VISIBLE_DEVICES` environment variable to an empty string. This helps in getting more specific error messages when encountering CUDA errors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/troubleshooting.md#_snippet_3

LANGUAGE: Python
CODE:
```
>>> import os

>>> os.environ["CUDA_VISIBLE_DEVICES"] = ""
```

----------------------------------------

TITLE: Implementing Early Stopping Callback in Transformers Trainer
DESCRIPTION: A custom callback implementation that stops training after a specified number of steps. This callback doesn't change the training loop but inspects its state and signals when training should stop.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/trainer.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import TrainerCallback

class EarlyStoppingCallback(TrainerCallback):
    def __init__(self, num_steps=10):
        self.num_steps = num_steps

    def on_step_end(self, args, state, control, **kwargs):
        if state.global_step >= self.num_steps:
            return {"should_training_stop": True}
        else:
            return {}

```

----------------------------------------

TITLE: Decoding Tokens Using Transformer Model - Python
DESCRIPTION: This snippet defines a function to decode tokens generated by a transformer model, capturing logits and determining the next token based on the current token and cache position.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_optims.md#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
def decode_one_tokens(model, cur_token, input_pos, cache_position, past_key_values):
    logits = model(
        cur_token,
        position_ids=input_pos,
        cache_position=cache_position,
        past_key_values=past_key_values,
        return_dict=False,
        use_cache=True
    )[0]
    new_token = torch.argmax(logits[:, -1], dim=-1)[:, None]
    return new_token
```

----------------------------------------

TITLE: Loading Vision Feature Extractor
DESCRIPTION: Instantiates a feature extractor for the Vision Transformer (ViT) model to preprocess image data. The feature extractor handles normalization and resizing of images.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/preprocessing.md#2025-04-22_snippet_22

LANGUAGE: python
CODE:
```
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("google/vit-base-patch16-224")
```

----------------------------------------

TITLE: Performing Object Detection with RT-DETR in Python
DESCRIPTION: This code snippet demonstrates how to use the RT-DETR model for object detection on an image. It includes loading the model and image processor, processing the input image, running inference, and post-processing the results to obtain bounding boxes, scores, and labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/rt_detr.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
import requests

from PIL import Image
from transformers import RTDetrForObjectDetection, RTDetrImageProcessor

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

image_processor = RTDetrImageProcessor.from_pretrained("PekingU/rtdetr_r50vd")
model = RTDetrForObjectDetection.from_pretrained("PekingU/rtdetr_r50vd")

inputs = image_processor(images=image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

results = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.3)

for result in results:
    for score, label_id, box in zip(result["scores"], result["labels"], result["boxes"]):
        score, label = score.item(), label_id.item()
        box = [round(i, 2) for i in box.tolist()]
        print(f"{model.config.id2label[label]}: {score:.2f} {box}")
```

----------------------------------------

TITLE: Initializing Tokenizer for Text Processing
DESCRIPTION: Loads a pre-trained DistilBERT tokenizer to process the input tokens for the token classification task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/token_classification.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Processing Dataset to Replace String Answers with Ids in Python
DESCRIPTION: This snippet defines a function replace_ids that maps string labels to their corresponding ids based on earlier mappings. It adjusts the dataset structure to be more convenient for preprocessing by applying the function and flattening the structure via map.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_question_answering.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
def replace_ids(inputs):
  inputs["label"]["ids"] = [label2id[x] for x in inputs["label"]["ids"]]
  return inputs

dataset = dataset.map(replace_ids)
flat_dataset = dataset.flatten()
flat_dataset.features
```

----------------------------------------

TITLE: Loading and Using a TorchScript Traced Model in Python
DESCRIPTION: This snippet shows how to load a previously saved TorchScript model and use it for inference with the same dummy inputs used during tracing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/torchscript.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
loaded_model = torch.jit.load("traced_bert.pt")
loaded_model.eval()

all_encoder_layers, pooled_output = loaded_model(*dummy_input)
```

----------------------------------------

TITLE: Creating TensorFlow Dataset from Hugging Face Dataset in Python
DESCRIPTION: This snippet demonstrates how to create a tf.data.Dataset from a Hugging Face dataset using the prepare_tf_dataset method. It configures batching, shuffling, and tokenization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/training.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
tf_dataset = model.prepare_tf_dataset(dataset["train"], batch_size=16, shuffle=True, tokenizer=tokenizer)
```

----------------------------------------

TITLE: Exporting a BERT Model to TorchScript
DESCRIPTION: This snippet demonstrates how to initialize a BERT model with the torchscript flag, prepare dummy inputs, trace the model, and save it to disk for later use. It includes tokenization, creating dummy inputs with proper dimensions, and the actual export process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/torchscript.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import BertModel, BertTokenizer, BertConfig
import torch

enc = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")

# Tokenizing input text
text = "[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]"
tokenized_text = enc.tokenize(text)

# Masking one of the input tokens
masked_index = 8
tokenized_text[masked_index] = "[MASK]"
indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

# Creating a dummy input
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

# Initializing the model with the torchscript flag
# Flag set to True even though it is not necessary as this model does not have an LM Head.
config = BertConfig(
    vocab_size_or_config_json_file=32000,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072,
    torchscript=True,
)

# Instantiating the model
model = BertModel(config)

# The model needs to be in evaluation mode
model.eval()

# If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag
model = BertModel.from_pretrained("google-bert/bert-base-uncased", torchscript=True)

# Creating the trace
traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])
torch.jit.save(traced_model, "traced_bert.pt")
```

----------------------------------------

TITLE: Tokenizing with Left Padding in Python
DESCRIPTION: Demonstrates setting up left padding for sequence tokenization, customizing tokenization using `padding_side='left'`. Utilizes a specified pretrained model and tokenizer. Facilitates effective handling of different input lengths in text generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial.md#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", padding_side="left")
tokenizer.pad_token = tokenizer.eos_token
model_inputs = tokenizer(
    ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
).to("cuda")
generated_ids = model.generate(**model_inputs)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'1, 2, 3, 4, 5, 6,'
```

----------------------------------------

TITLE: Generating Text with DBRX and SDPA in Python
DESCRIPTION: This code snippet demonstrates how to use the DBRX model with PyTorch's Scaled Dot-Product Attention (SDPA) for faster text generation. It sets the `attn_implementation` parameter to `sdpa` when loading the model. It requires the `transformers` and `torch` libraries. Replace `YOUR_HF_TOKEN` with your actual Hugging Face token.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/dbrx.md#_snippet_2

LANGUAGE: python
CODE:
```
from transformers import DbrxForCausalLM, AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained("databricks/dbrx-instruct", token="YOUR_HF_TOKEN")
model = DbrxForCausalLM.from_pretrained(
    "databricks/dbrx-instruct",
    device_map="auto",
    torch_dtype=torch.bfloat16,
    token="YOUR_HF_TOKEN",
    attn_implementation="sdpa",
    )

input_text = "What does it take to build a great LLM?"
messages = [{"role": "user", "content": input_text}]
input_ids = tokenizer.apply_chat_template(messages, return_dict=True, tokenize=True, add_generation_prompt=True, return_tensors="pt").to("cuda")

outputs = model.generate(**input_ids, max_new_tokens=200)
print(tokenizer.decode(outputs[0]))
```

----------------------------------------

TITLE: Applying Grouping Function to Entire Dataset
DESCRIPTION: This code applies the grouping function defined earlier to the tokenized dataset, ensuring that the text examples are appropriately processed into blocks for model training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)
```

----------------------------------------

TITLE: Text Summarization with Falcon-7b-instruct in Python
DESCRIPTION: This code demonstrates how to use the Falcon-7b-instruct model for text summarization by creating a prompt that asks the model to summarize a paragraph about permaculture.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/prompting.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> torch.manual_seed(3) # doctest: +IGNORE_RESULT
>>> prompt = """Permaculture is a design process mimicking the diversity, functionality and resilience of natural ecosystems. The principles and practices are drawn from traditional ecological knowledge of indigenous cultures combined with modern scientific understanding and technological innovations. Permaculture design provides a framework helping individuals and communities develop innovative, creative and effective strategies for meeting basic needs while preparing for and mitigating the projected impacts of climate change.
... Write a summary of the above text.
... Summary:
... """

>>> sequences = pipe(
...     prompt,
...     max_new_tokens=30,
...     do_sample=True,
...     top_k=10,
...     return_full_text = False,
... )

>>> for seq in sequences:
...     print(f"{seq['generated_text']}")
Permaculture is an ecological design mimicking natural ecosystems to meet basic needs and prepare for climate change. It is based on traditional knowledge and scientific understanding.
```

----------------------------------------

TITLE: Dataset Preprocessing with Data Collator
DESCRIPTION: Applies preprocessing to the entire dataset and sets up data collation for dynamic padding of batches.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/tasks/sequence_classification.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
tokenized_imdb = imdb.map(preprocess_function, batched=True)

>>> from transformers import DataCollatorWithPadding

>>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

----------------------------------------

TITLE: Configuring 8-bit Quantization with Module Skipping in Python
DESCRIPTION: This snippet shows how to skip quantization for specific modules using BitsAndBytesConfig. It's useful when quantizing certain modules can cause instability in the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-23_snippet_13

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

quantization_config = BitsAndBytesConfig(
    llm_int8_skip_modules=["lm_head"],
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype="auto",
    device_map="auto",
    quantization_config=quantization_config,
)
```

----------------------------------------

TITLE: Adding New PEFT Adapter to Existing Model
DESCRIPTION: Demonstration of adding a new PEFT adapter to an existing model with an adapter already attached.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/peft.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import PeftConfig

model_id = "facebook/opt-350m"
model = AutoModelForCausalLM.from_pretrained(model_id)

lora_config = LoraConfig(
    target_modules=["q_proj", "k_proj"],
    init_lora_weights=False
)

model.add_adapter(lora_config, adapter_name="adapter_1")

# attach new adapter with same config
model.add_adapter(lora_config, adapter_name="adapter_2")
```

----------------------------------------

TITLE: Single Media Inference with Qwen2-VL
DESCRIPTION: Example showing how to load and use Qwen2-VL model for processing single images and videos. Demonstrates model initialization, input processing, and generation of responses.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/qwen2_vl.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor

# ì‚¬ìš© ê°€ëŠ¥í•œ ìž¥ì¹˜ì—ì„œ ëª¨ë¸ì„ ë°˜ ì •ë°€ë„(half-precision)ë¡œ ë¡œë“œ
model = Qwen2VLForConditionalGeneration.from_pretrained("Qwen/Qwen2-VL-7B-Instruct", device_map="auto")
processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-7B-Instruct")


conversation = [
    {
        "role":"user",
        "content":[
            {
                "type":"image",
                "url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"
            },
            {
                "type":"text",
                "text":"Describe this image."
            }
        ]
    }
]

inputs = processor.apply_chat_template(
    conversation,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt"
).to(model.device)

# ì¶”ë¡ : ì•„ì›ƒí’‹ ìƒì„±
output_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]
output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
print(output_text)
```

----------------------------------------

TITLE: Loading Pretrained DistilBERT Model with Custom Configuration in PyTorch
DESCRIPTION: This code demonstrates how to load a pretrained DistilBERT model while overriding its configuration with a custom one in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/create_a_model.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
model = DistilBertModel.from_pretrained("distilbert/distilbert-base-uncased", config=my_config)
```

----------------------------------------

TITLE: Training TensorFlow Vision EncoderDecoderModel on Image-Text Pairs - Python
DESCRIPTION: This snippet outlines the process of fine-tuning the VisionEncoderDecoderModel on (image, text) pairs, demonstrating how to load a dataset and compute the loss during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/vision-encoder-decoder.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import ViTImageProcessor, BertTokenizer, VisionEncoderDecoderModel
>>> from datasets import load_dataset

>>> image_processor = ViTImageProcessor.from_pretrained("google/vit-base-patch16-224-in21k")
>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(
...     "google/vit-base-patch16-224-in21k", "google-bert/bert-base-uncased"
... )

>>> model.config.decoder_start_token_id = tokenizer.cls_token_id
>>> model.config.pad_token_id = tokenizer.pad_token_id

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]
>>> pixel_values = image_processor(image, return_tensors="pt").pixel_values

>>> labels = tokenizer(
...     "an image of two cats chilling on a couch",
...     return_tensors="pt",
... ).input_ids

>>> # the forward function automatically creates the correct decoder_input_ids
>>> loss = model(pixel_values=pixel_values, labels=labels).loss
```

----------------------------------------

TITLE: Examining Dataset Example
DESCRIPTION: This code displays the first element from the training split of the SWAG dataset. It allows you to inspect the structure and content of the data, including the context sentences (`sent1`, `sent2`), possible endings (`ending0`, `ending1`, `ending2`, `ending3`), and the correct label (`label`).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/multiple_choice.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
">>> swag["train"][0]\n{'ending0': 'passes by walking down the street playing their instruments.',
 'ending1': 'has heard approaching them.',
 'ending2': "arrives and they're outside dancing and asleep.",
 'ending3': 'turns the lead singer watches the performance.',
 'fold-ind': '3416',
 'gold-source': 'gold',
 'label': 0,
 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',
 'sent2': 'A drum line',
 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',
 'video-id': 'anetv_jkn6uvmqwh4'}"
```

----------------------------------------

TITLE: Initializing and Using BARTpho Model with PyTorch
DESCRIPTION: Demonstrates how to load and use the BARTpho model and tokenizer with PyTorch for processing Vietnamese text. Shows basic model initialization, tokenization, and feature extraction.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/bartpho.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModel, AutoTokenizer

bartpho = AutoModel.from_pretrained("vinai/bartpho-syllable")

tokenizer = AutoTokenizer.from_pretrained("vinai/bartpho-syllable")

line = "ChÃºng tÃ´i lÃ  nhá»¯ng nghiÃªn cá»©u viÃªn."

input_ids = tokenizer(line, return_tensors="pt")

with torch.no_grad():
    features = bartpho(**input_ids)  # Models outputs are now tuples
```

----------------------------------------

TITLE: Loading and Using a Pipeline from the Hub
DESCRIPTION: This snippet shows how to load a custom pipeline from the Hugging Face Hub using the `pipeline` function. The `trust_remote_code=True` option is required to allow the pipeline to execute custom code.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/add_new_pipeline.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from transformers import pipeline

classifier = pipeline(model="{your_username}/test-dynamic-pipeline", trust_remote_code=True)
```

----------------------------------------

TITLE: Loading SacreBLEU Evaluation Metric
DESCRIPTION: This code loads the SacreBLEU metric from the Evaluate library to measure translation quality during model training and evaluation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/translation.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> import evaluate

>>> metric = evaluate.load("sacrebleu")
```

----------------------------------------

TITLE: Loading Sharded Checkpoints Directly into a Model
DESCRIPTION: This code demonstrates how to use the load_sharded_checkpoint utility to load a sharded checkpoint directly into a model instance without using from_pretrained.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/big_models.md#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
>>> from transformers.modeling_utils import load_sharded_checkpoint

>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir, max_shard_size="200MB")
...     load_sharded_checkpoint(model, tmp_dir)
```

----------------------------------------

TITLE: Single Input Inference with LLaVa
DESCRIPTION: This snippet demonstrates how to perform single input inference with the LLaVa model.  It loads a pre-trained LLaVa model and processor, prepares a conversation with an image and a text prompt, applies the chat template to format the input, and generates a response. The `device_map="auto"` argument ensures that the model is loaded on the available hardware.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llava.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration

# Load the model in half-precision
model = LlavaForConditionalGeneration.from_pretrained("llava-hf/llava-1.5-7b-hf", torch_dtype=torch.float16, device_map="auto")
processor = AutoProcessor.from_pretrained("llava-hf/llava-1.5-7b-hf")

conversation = [
    {
        "role": "user",
        "content": [
            {"type": "image", "url": "https://www.ilankelman.org/stopsigns/australia.jpg"},
            {"type": "text", "text": "What is shown in this image?"},
        ],
    },
]

inputs = processor.apply_chat_template(
    conversation,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt"
).to(model.device, torch.float16)

# Generate
generate_ids = model.generate(**inputs, max_new_tokens=30)
processor.batch_decode(generate_ids, skip_special_tokens=True)
```

----------------------------------------

TITLE: Performing Inference with a Fine-tuned SpeechEncoderDecoderModel in Python
DESCRIPTION: This code demonstrates how to load a fine-tuned SpeechEncoderDecoderModel for speech translation (English to German), process audio input, and generate translated text. It uses the generate method with greedy decoding to create the translation output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/speech-encoder-decoder.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import Wav2Vec2Processor, SpeechEncoderDecoderModel
>>> from datasets import load_dataset
>>> import torch

>>> # load a fine-tuned speech translation model and corresponding processor
>>> model = SpeechEncoderDecoderModel.from_pretrained("facebook/wav2vec2-xls-r-300m-en-to-15")
>>> processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-xls-r-300m-en-to-15")

>>> # let's perform inference on a piece of English speech (which we'll translate to German)
>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> input_values = processor(ds[0]["audio"]["array"], return_tensors="pt").input_values

>>> # autoregressively generate transcription (uses greedy decoding by default)
>>> generated_ids = model.generate(input_values)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
>>> print(generated_text)
Mr. Quilter ist der Apostel der Mittelschicht und wir freuen uns, sein Evangelium willkommen heiÃŸen zu kÃ¶nnen.
```

----------------------------------------

TITLE: Extract Audio Features (Python)
DESCRIPTION: Extracts features from the audio input using the loaded feature extractor. Includes the `sampling_rate` parameter for proper handling of potential silence errors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/preprocessing.md#_snippet_13

LANGUAGE: python
CODE:
```
>>> audio_input = [dataset[0]["audio"]["array"]]
>>> feature_extractor(audio_input, sampling_rate=16000)
{'input_values': [array([ 3.8106556e-04,  2.7506407e-03,  2.8015103e-03, ...,
        5.6335266e-04,  4.6588284e-06, -1.7142107e-04], dtype=float32)]}
```

----------------------------------------

TITLE: Loading a Multilingual Sentiment Analysis Model with TensorFlow
DESCRIPTION: Code to load a pre-trained multilingual BERT model and its tokenizer for sentiment analysis using TensorFlow AutoClasses.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/quicktour.md#2025-04-22_snippet_11

LANGUAGE: Python
CODE:
```
>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"

>>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

----------------------------------------

TITLE: Reversing GPU Order via CUDA_VISIBLE_DEVICES
DESCRIPTION: This snippet describes how to reverse the order of GPU selection using the CUDA_VISIBLE_DEVICES variable, allowing users to prioritize different GPUs for their training tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/gpu_selection.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
CUDA_VISIBLE_DEVICES=2,0 torchrun trainer-program.py ...
```

----------------------------------------

TITLE: Viewing an Example from the Dataset
DESCRIPTION: This code snippet retrieves the first entry from the training subset of the ELI5 dataset. It allows a user to inspect the structure and contents of the loaded data, particularly focusing on the nested 'answers' field.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/language_modeling.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> eli5["train"][0]
{'answers': {'a_id': ['c3d1aib', 'c3d4lya'],
  'score': [6, 3],
  'text': ["The velocity needed to remain in orbit is equal to the square root of Newton's constant times the mass of earth divided by the distance from the center of the earth. I don't know the altitude of that specific mission, but they're usually around 300 km. That means he's going 7-8 km/s.\n\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.",
   "Hope you don't mind me asking another question, but why aren't there any stars visible in this photo?"]},
 'answers_urls': {'url': []},
 'document': '',
 'q_id': 'nyxfp',
 'selftext': '_URL_0_\n\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?',
 'selftext_urls': {'url': ['http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg']},
 'subreddit': 'askscience',
 'title': 'Few questions about this space walk photograph.',
 'title_urls': {'url': []}}
```

----------------------------------------

TITLE: Downloading and Processing Label Mappings for SegFormer in Python
DESCRIPTION: This snippet downloads label mappings from Hugging Face Hub and creates dictionaries for id-to-label and label-to-id mappings. It's essential for setting up the SegFormer model for semantic segmentation tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> import json
>>> from pathlib import Path
>>> from huggingface_hub import hf_hub_download

>>> repo_id = "huggingface/label-files"
>>> filename = "ade20k-id2label.json"
>>> id2label = json.loads(Path(hf_hub_download(repo_id, filename, repo_type="dataset")).read_text())
>>> id2label = {int(k): v for k, v in id2label.items()}
>>> label2id = {v: k for k, v in id2label.items()}
>>> num_labels = len(id2label)
```

----------------------------------------

TITLE: Using Autoquant for Automatic Layer Quantization on GPU
DESCRIPTION: This snippet demonstrates using the `autoquant` API for automatic quantization selection on GPU. It creates a `TorchAoConfig` with `"autoquant"`, loads the model, performs a generation step with static compilation, and explicitly calls `finalize_autoquant` to complete the process and log input shapes. Note that `autoquant` is currently GPU-only.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/torchao.md#_snippet_10

LANGUAGE: Python
CODE:
```
import torch
from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer

quantization_config = TorchAoConfig("autoquant", min_sqnr=None)
quantized_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B-Instruct",
    torch_dtype="auto",
    device_map="auto",
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-8B-Instruct")
input_text = "What are we having for dinner?"
input_ids = tokenizer(input_text, return_tensors="pt").to("cuda")

# auto-compile the quantized model with `cache_implementation="static"` to get speed up
output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation="static")
# explicitly call `finalize_autoquant` (may be refactored and removed in the future)
quantized_model.finalize_autoquant()
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Mixed Media Processing with Video-LLaVA
DESCRIPTION: Example demonstrating how to process both images and videos together using Video-LLaVA model. Shows integration of image and video inputs in a single conversation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/video_llava.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from PIL import Image
import requests

# Generate from image and video mixed inputs
# Load and image and write a new prompt
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
prompt = "USER: <image>\nHow many cats are there in the image? ASSISTANT: There are two cats. USER: <video>\nWhy is this video funny? ASSISTANT:"

inputs = processor(text=prompt, images=image, videos=clip, padding=True, return_tensors="pt")

# Generate
generate_ids = model.generate(**inputs, max_length=50)
processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
```

----------------------------------------

TITLE: Loading Pretrained DistilBERT Configuration with Modifications in Python
DESCRIPTION: This snippet shows how to load a pretrained DistilBERT configuration and modify specific attributes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/create_a_model.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
my_config = DistilBertConfig.from_pretrained("distilbert/distilbert-base-uncased", activation="relu", attention_dropout=0.4)
```

----------------------------------------

TITLE: Data Collator for Language Modeling (TensorFlow)
DESCRIPTION: This code creates a `DataCollatorForLanguageModeling` object for TensorFlow. It initializes the data collator with the tokenizer, a masking probability of 0.15, and specifies that it should return TensorFlow tensors. This collator dynamically pads and masks the input sequences for masked language modeling during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_11

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorForLanguageModeling

>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors="tf")
```

----------------------------------------

TITLE: Generating Text with GPT-2 Model in Python
DESCRIPTION: This snippet demonstrates how to use a pre-trained GPT-2 model to generate text. It initializes the tokenizer and model, prepares input, and generates output with scores.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/internal/generation_utils.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained("openai-community/gpt2")
model = GPT2LMHeadModel.from_pretrained("openai-community/gpt2")

inputs = tokenizer("Hello, my dog is cute and ", return_tensors="pt")
generation_output = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)
```

----------------------------------------

TITLE: Generating Text with GPT-2 Model
DESCRIPTION: Example showing how to use GPT-2 model with tokenizer to generate text and access generation outputs including sequences and scores. Demonstrates the use of return_dict_in_generate and output_scores parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/internal/generation_utils.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained("openai-community/gpt2")
model = GPT2LMHeadModel.from_pretrained("openai-community/gpt2")

inputs = tokenizer("Hello, my dog is cute and ", return_tensors="pt")
generation_output = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)
```

----------------------------------------

TITLE: Loading Processor Components Separately
DESCRIPTION: This snippet shows how to load the tokenizer and feature extractor separately and then combine them into a single processor instance. It loads `WhisperTokenizerFast` and `WhisperFeatureExtractor` and then combines them into a `WhisperProcessor`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/processors.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import WhisperTokenizerFast, WhisperFeatureExtractor, WhisperProcessor

tokenizer = WhisperTokenizerFast.from_pretrained("openai/whisper-tiny")
feature_extractor = WhisperFeatureExtractor.from_pretrained("openai/whisper-tiny")
processor = WhisperProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)
```

----------------------------------------

TITLE: Using BERT with MeCab and WordPiece in Python
DESCRIPTION: This code snippet demonstrates how to use a BERT model trained on Japanese text with MeCab and WordPiece tokenization. It imports necessary libraries from the Transformers library and PyTorch, loads the pre-trained model and tokenizer, tokenizes a Japanese sentence, and prints the decoded input. Requires `transformers` and `torch` and the `ja` extras installed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bert-japanese.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> import torch
>>> from transformers import AutoModel, AutoTokenizer

>>> bertjapanese = AutoModel.from_pretrained("cl-tohoku/bert-base-japanese")
>>> tokenizer = AutoTokenizer.from_pretrained("cl-tohoku/bert-base-japanese")

>>> ## Input Japanese Text
>>> line = "å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€‚"

>>> inputs = tokenizer(line, return_tensors="pt")

>>> print(tokenizer.decode(inputs["input_ids"][0]))
[CLS] å¾è¼© ã¯ çŒ« ã§ ã‚ã‚‹ ã€‚ [SEP]

>>> outputs = bertjapanese(**inputs)
```

----------------------------------------

TITLE: Depth Estimation with Transformers Pipeline
DESCRIPTION: This snippet shows how to perform depth estimation using the Transformers pipeline. It estimates the depth of objects in an image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/task_summary.md#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
>>> from transformers import pipeline

>>> depth_estimator = pipeline(task="depth-estimation")
>>> preds = depth_estimator(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
```

----------------------------------------

TITLE: Pipeline with Multiple Inputs
DESCRIPTION: Example of using pipeline with a list of inputs for batch processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/pipelines.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> pipe = pipeline("text-classification")
>>> pipe(["This restaurant is awesome", "This restaurant is awful"])
[{'label': 'POSITIVE', 'score': 0.9998743534088135},
 {'label': 'NEGATIVE', 'score': 0.9996669292449951}]
```

----------------------------------------

TITLE: Logging into Hugging Face Account
DESCRIPTION: Logs into a Hugging Face account to enable model uploading and sharing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/image_classification.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Translating Text with Pipeline API
DESCRIPTION: Example of using the Pipeline API to translate text from English to French using mBART-50 model with GPU acceleration and float16 precision.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mbart.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import pipeline

pipeline = pipeline(
    task="translation",
    model="facebook/mbart-large-50-many-to-many-mmt",
    device=0,
    torch_dtype=torch.float16,
    src_lang="en_XX",
    tgt_lang="fr_XX",
)
print(pipeline("UN Chief Says There Is No Military Solution in Syria"))
```

----------------------------------------

TITLE: Creating a Default Data Collator in Python
DESCRIPTION: This snippet demonstrates how to create a DefaultDataCollator for managing batches of examples during training in PyTorch. It prepares the data without additional preprocessing like padding.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/question_answering.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator()
```

----------------------------------------

TITLE: Using PVTv2 as a Backbone for Object Detection in PyTorch
DESCRIPTION: This snippet shows how to use PVTv2 as a backbone in a more complex architecture like Deformable DETR with Hugging Face Transformers. It involves loading the model configuration and image processor before performing object detection on an input image. Dependencies include 'requests', 'torch', and 'PIL'.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/pvt_v2.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
import requests
import torch

from transformers import AutoConfig, AutoModelForObjectDetection, AutoImageProcessor
from PIL import Image

model = AutoModelForObjectDetection.from_config(
    config=AutoConfig.from_pretrained(
        "SenseTime/deformable-detr",
        backbone_config=AutoConfig.from_pretrained("OpenGVLab/pvt_v2_b5"),
        use_timm_backbone=False
    ),
)

image_processor = AutoImageProcessor.from_pretrained("SenseTime/deformable-detr")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
processed = image_processor(image)
outputs = model(torch.tensor(processed["pixel_values"]))
```

----------------------------------------

TITLE: Dynamic Batching Implementation (Pseudocode)
DESCRIPTION: This Python pseudocode demonstrates a dynamic batching mechanism for processing requests with a Transformers pipeline. It accumulates multiple requests in a queue before processing them in a single batch. The code shows how to wait for additional requests within a timeout period and then process them together to improve efficiency. It should be noted that this implementation lacks batch size limits and has a timeout reset issue.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_webserver.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
"async def server_loop(q):
    pipe = pipeline(task=\"fill-mask\", model=\"google-bert/bert-base-uncased\")
    while True:
        (string, rq) = await q.get()
        strings = []
        queues = []
        strings.append(string)
        queues.append(rq)
        while True:
            try:
                (string, rq) = await asyncio.wait_for(q.get(), timeout=1)
            except asyncio.exceptions.TimeoutError:
                break
            strings.append(string)
            queues.append(rq)
        outs = pipe(strings, batch_size=len(strings))
        for rq, out in zip(queues, outs):
            await rq.put(out)"
```

----------------------------------------

TITLE: Standard Forward Pass with TensorFlow Model
DESCRIPTION: Demonstrates how to perform a basic forward pass with random inputs on a TensorFlow model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/tf_xla.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
# Generate random inputs for the model.
batch_size = 16
input_vector_dim = 10
random_inputs = tf.random.normal((batch_size, input_vector_dim))

# Run a forward pass.
_ = model(random_inputs)
```

----------------------------------------

TITLE: Converting PyTorch Model to TensorFlow in Python
DESCRIPTION: Illustrates the process of converting a PyTorch model checkpoint to a TensorFlow checkpoint using the from_pt parameter.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/model_sharing.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> tf_model = TFDistilBertForSequenceClassification.from_pretrained("path/to/awesome-name-you-picked", from_pt=True)
>>> tf_model.save_pretrained("path/to/awesome-name-you-picked")
```

----------------------------------------

TITLE: Box Prompting with SAM Python
DESCRIPTION: Performs box prompting with the SAM model. It defines a bounding box around an object, processes the image and box, runs inference, and post-processes the output mask.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/mask_generation.md#_snippet_8

LANGUAGE: python
CODE:
```
# ë²Œ ì£¼ìœ„ì˜ ë°”ìš´ë”© ë°•ìŠ¤
box = [2350, 1600, 2850, 2100]

inputs = processor(
        image,
        input_boxes=[[[box]]],
        return_tensors="pt"
    ).to("cuda")

with torch.no_grad():
    outputs = model(**inputs)

mask = processor.image_processor.post_process_masks(
    outputs.pred_masks.cpu(),
    inputs["original_sizes"].cpu(),
    inputs["reshaped_input_sizes"].cpu()
)[0][0][0].numpy()
```

----------------------------------------

TITLE: Training the ViLT Model
DESCRIPTION: Initiates the training process for the ViLT model using the configured Trainer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_question_answering.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
>>> trainer.train()
```

----------------------------------------

TITLE: Running Inference with the Pipeline
DESCRIPTION: Shows how to use the pipeline to get answers for a given image and question, displaying the confidence score and predicted answer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_question_answering.md#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
>>> example = dataset[0]
>>> image = Image.open(example['image_id'])
>>> question = example['question']
>>> print(question)
>>> pipe(image, question, top_k=1)
"Where is he looking?"
[{'score': 0.5498199462890625, 'answer': 'down'}]
```

----------------------------------------

TITLE: Examining Dataset Sample Structure
DESCRIPTION: Displays a sample from the training dataset to understand its structure, which includes an image, an annotation (segmentation map), and a scene category.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/semantic_segmentation.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> train_ds[0]
{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x683 at 0x7F9B0C201F90>,
 'annotation': <PIL.PngImagePlugin.PngImageFile image mode=L size=512x683 at 0x7F9B0C201DD0>,
 'scene_category': 368}
```

----------------------------------------

TITLE: Loading Feature Extractor for Audio Preprocessing in Python
DESCRIPTION: This snippet loads a feature extractor from Hugging Face Transformers for preprocessing audio data. It depends on the `transformers` library and the specified model name. The output is a preprocessed input tensor ready for the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/audio_classification.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("stevhliu/my_awesome_minds_model")
>>> inputs = feature_extractor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
```

----------------------------------------

TITLE: Creating Dummy Data for Model Training in Python
DESCRIPTION: This Python code snippet creates a dummy dataset with random token IDs and binary labels, formatted as a PyTorch Dataset for use in model training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_memory_anatomy.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> import numpy as np
>>> from datasets import Dataset


>>> seq_len, dataset_size = 512, 512
>>> dummy_data = {
...     "input_ids": np.random.randint(100, 30000, (dataset_size, seq_len)),
...     "labels": np.random.randint(0, 1, (dataset_size)),
... }
>>> ds = Dataset.from_dict(dummy_data)
>>> ds.set_format("pt")
```

----------------------------------------

TITLE: Creating DistilBert Model with Custom Configuration in TensorFlow
DESCRIPTION: Demonstrates how to instantiate a DistilBert model using a custom configuration in TensorFlow, creating a model with random initialization of weights.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/create_a_model.md#2025-04-23_snippet_8

LANGUAGE: python
CODE:
```
>>> from transformers import TFDistilBertModel

>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/my_config.json")
>>> tf_model = TFDistilBertModel(my_config)
```

----------------------------------------

TITLE: Importing ResNetForImageClassification in PyTorch
DESCRIPTION: Example of importing and using the ResNetForImageClassification model in PyTorch. This snippet demonstrates how to load the model and perform forward pass for image classification.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/resnet.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import ResNetForImageClassification

model = ResNetForImageClassification.from_pretrained("microsoft/resnet-50")
outputs = model(inputs)
```

----------------------------------------

TITLE: Using Custom Callback with Trainer
DESCRIPTION: Example of how to pass a custom callback to the Trainer class. This shows integrating the EarlyStoppingCallback with the training process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/trainer.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    processing_class=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callback=[EarlyStoppingCallback()],
)
```

----------------------------------------

TITLE: Using Dedicated SeamlessM4T Text-to-Text Model
DESCRIPTION: Demonstrates how to use the dedicated Text-to-Text translation model which focuses solely on text translation, reducing unnecessary components and memory usage compared to the full model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/seamless_m4t.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import SeamlessM4TForTextToText
model = SeamlessM4TForTextToText.from_pretrained("facebook/hf-seamless-m4t-medium")
```

----------------------------------------

TITLE: Using SeamlessM4T-v2 Dedicated Text-to-Text Model
DESCRIPTION: This snippet demonstrates how to load and use the dedicated Text-to-Text translation model from the SeamlessM4T-v2 family. With the dedicated model, you don't need to specify 'generate_speech=False' as with the main model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/seamless_m4t_v2.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import SeamlessM4Tv2ForTextToText
model = SeamlessM4Tv2ForTextToText.from_pretrained("facebook/seamless-m4t-v2-large")
```

----------------------------------------

TITLE: Using BARTpho with TensorFlow
DESCRIPTION: Shows how to initialize and use BARTpho model with TensorFlow 2.0+. Demonstrates model loading and basic inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/bartpho.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import TFAutoModel

bartpho = TFAutoModel.from_pretrained("vinai/bartpho-syllable")
input_ids = tokenizer(line, return_tensors="tf")
features = bartpho(**input_ids)
```

----------------------------------------

TITLE: Generating Text Translation with SeamlessM4T
DESCRIPTION: Demonstrates how to generate translated text in French from both audio and text inputs using the SeamlessM4T model. The generate_speech parameter is set to False to output text instead of speech.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/seamless_m4t.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
# from audio
output_tokens = model.generate(**audio_inputs, tgt_lang="fra", generate_speech=False)
translated_text_from_audio = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)

# from text
output_tokens = model.generate(**text_inputs, tgt_lang="fra", generate_speech=False)
translated_text_from_text = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)
```

----------------------------------------

TITLE: Formatting Image Annotations to COCO Format for DETR in Python
DESCRIPTION: This function reformats image annotations to match the COCO format expected by DETR. It takes image ID, categories, areas, and bounding boxes as input and returns a dictionary with formatted annotations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/object_detection.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
def format_image_annotations_as_coco(image_id, categories, areas, bboxes):
    """Format one set of image annotations to the COCO format

    Args:
        image_id (str): image id. e.g. "0001"
        categories (List[int]): list of categories/class labels corresponding to provided bounding boxes
        areas (List[float]): list of corresponding areas to provided bounding boxes
        bboxes (List[Tuple[float]]): list of bounding boxes provided in COCO format
            ([center_x, center_y, width, height] in absolute coordinates)

    Returns:
        dict: {
            "image_id": image id,
            "annotations": list of formatted annotations
        }
    """
    annotations = []
    for category, area, bbox in zip(categories, areas, bboxes):
        formatted_annotation = {
            "image_id": image_id,
            "category_id": category,
            "iscrowd": 0,
            "area": area,
            "bbox": list(bbox),
        }
        annotations.append(formatted_annotation)

    return {
        "image_id": image_id,
        "annotations": annotations,
    }
```

----------------------------------------

TITLE: Distributed Training and Mixed Precision with PyTorch
DESCRIPTION: This command enables distributed training and mixed precision by adding the `fp16` flag to enable mixed precision, and sets the number of GPUs to use with the `nproc_per_node` argument. It showcases how to leverage multiple GPUs and mixed precision for faster training in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/run_scripts.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
```bash
torchrun \
    --nproc_per_node 8 pytorch/summarization/run_summarization.py \
    --fp16 \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```
```

----------------------------------------

TITLE: Enabling and Disabling PEFT Adapters
DESCRIPTION: Code snippets showing how to enable and disable PEFT adapters on a model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/peft.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import PeftConfig

model_id = "facebook/opt-350m"
adapter_model_id = "ybelkada/opt-350m-lora"
tokenizer = AutoTokenizer.from_pretrained(model_id)
text = "Hello"
inputs = tokenizer(text, return_tensors="pt")

model = AutoModelForCausalLM.from_pretrained(model_id)
peft_config = PeftConfig.from_pretrained(adapter_model_id)

# to initiate with random weights
peft_config.init_lora_weights = False

model.add_adapter(peft_config)
model.enable_adapters()
output = model.generate(**inputs)

model.disable_adapters()
output = model.generate(**inputs)
```

----------------------------------------

TITLE: Fine-tuning MMS CTC Adapter for Turkish Speech Recognition
DESCRIPTION: Command-line script for fine-tuning the adapter layers of an MMS model on Turkish Common Voice data. The script uses gradient checkpointing for memory efficiency and uploads the trained model to Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-recognition/README.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
python run_speech_recognition_ctc.py \
	--dataset_name="common_voice" \
	--model_name_or_path="facebook/mms-1b-all" \
	--dataset_config_name="tr" \
	--output_dir="./wav2vec2-common_voice-tr-mms-demo" \
	--num_train_epochs="4" \
	--per_device_train_batch_size="32" \
	--learning_rate="1e-3" \
	--warmup_steps="100" \
	--eval_strategy="steps" \
	--text_column_name="sentence" \
	--length_column_name="input_length" \
	--save_steps="200" \
	--eval_steps="100" \
	--save_total_limit="3" \
  --target_language="tur" \
	--gradient_checkpointing \
	--chars_to_ignore , ? . ! - \; \: \" " % ' " ï¿½ \
	--fp16 \
	--group_by_length \
	--do_train --do_eval \
  --push_to_hub
```

----------------------------------------

TITLE: Performing Translation with MBart Model
DESCRIPTION: This snippet shows how to use the MBart model to translate Finnish text to English, including tokenization and generation steps.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/multilingual.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
encoded_en = tokenizer(en_text, return_tensors="pt")
generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id("en_XX"))
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
```

----------------------------------------

TITLE: Creating PushToHubCallback (TensorFlow)
DESCRIPTION: This snippet shows how to create a `PushToHubCallback` to automatically push the trained model to the Hugging Face Hub. It configures the output directory and tokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_21

LANGUAGE: python
CODE:
```
>>> from transformers.keras_callbacks import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="my_awesome_model",
...     tokenizer=tokenizer,
... )
```

----------------------------------------

TITLE: Converting Numerical Tags to Entity Labels
DESCRIPTION: Translates the numerical NER tags into their corresponding label names to understand what entities each tag represents in the WNUT 17 dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> label_list = wnut["train"].features[f"ner_tags"].feature.names
>>> label_list
[
    "O",
    "B-corporation",
    "I-corporation",
    "B-creative-work",
    "I-creative-work",
    "B-group",
    "I-group",
    "B-location",
    "I-location",
    "B-person",
    "I-person",
    "B-product",
    "I-product",
]
```

----------------------------------------

TITLE: Panoptic Segmentation with Hugging Face Transformers in Python
DESCRIPTION: This snippet illustrates the use of the pipeline for panoptic segmentation. It combines the features of both semantic and instance segmentation to provide comprehensive object segmentation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
panoptic_segmentation = pipeline("image-segmentation", "facebook/mask2former-swin-large-cityscapes-panoptic")
results = panoptic_segmentation(image)
results
```

----------------------------------------

TITLE: Performing Inference with TensorFlow Model
DESCRIPTION: This code demonstrates inference using a TensorFlow model loaded with `TFAutoModelForTokenClassification`. The tokenized input is fed to the model, producing logits which are raw prediction scores.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_32

LANGUAGE: python
CODE:
```
>>> from transformers import TFAutoModelForTokenClassification

>>> model = TFAutoModelForTokenClassification.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> logits = model(**inputs).logits
```

----------------------------------------

TITLE: Loading DistilBERT for Question Answering in TensorFlow
DESCRIPTION: This snippet shows how to load a pre-trained DistilBERT model for question answering tasks using TensorFlow and the TFAutoModelForQuestionAnswering class.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/question_answering.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
from transformers import TFAutoModelForQuestionAnswering

model = TFAutoModelForQuestionAnswering.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Casting Audio Column and Resampling
DESCRIPTION: Casts the "audio" column to the Audio feature with a specified sampling rate of 16kHz. This ensures that the audio data is resampled to the correct sampling rate for the Wav2Vec2 model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/audio_classification.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
">>> minds = minds.cast_column("audio", Audio(sampling_rate=16_000))"
```

----------------------------------------

TITLE: Launching Training with Accelerate Arguments
DESCRIPTION: This bash script launches a training job using `accelerate launch` and directly passes Accelerate arguments. It configures distributed training parameters such as the number of processes, FSDP usage, mixed precision, and FSDP configuration options, alongside the training script and its arguments.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/trainer.md#2025-04-22_snippet_13

LANGUAGE: bash
CODE:
```
cd transformers

accelerate launch --num_processes=2 \
--use_fsdp \
--mixed_precision=bf16 \
--fsdp_auto_wrap_policy=TRANSFORMER_BASED_WRAP  \
--fsdp_transformer_layer_cls_to_wrap="BertLayer" \
--fsdp_sharding_strategy=1 \
--fsdp_state_dict_type=FULL_STATE_DICT \
./examples/pytorch/text-classification/run_glue.py
--model_name_or_path google-bert/bert-base-cased \
--task_name $TASK_NAME \
--do_train \
--do_eval \
--max_seq_length 128 \
--per_device_train_batch_size 16 \
--learning_rate 5e-5 \
--num_train_epochs 3 \
--output_dir /tmp/$TASK_NAME/ \
--overwrite_output_dir
```

----------------------------------------

TITLE: Loading Tiktoken Tokenizer with Transformers
DESCRIPTION: Demonstrates how to load a pre-trained model's tokenizer that uses the Tiktoken format using AutoTokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/tiktoken.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id, subfolder="original")
```

----------------------------------------

TITLE: Setting Up Data Collator with TensorFlow
DESCRIPTION: Establishes a data collator for batching examples in TensorFlow format without additional processing like padding.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_classification.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors="tf")
```

----------------------------------------

TITLE: Loading FP4 Quantized Model for Multi-GPU Inference with Memory Mapping
DESCRIPTION: Load a Transformers model in 4-bit precision for multi-GPU inference, specifying memory allocation per GPU using accelerate.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/perf_infer_gpu_one.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
max_memory_mapping = {0: "600MB", 1: "1GB"}
model_name = "bigscience/bloom-3b"
model_4bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map="auto", load_in_4bit=True, max_memory=max_memory_mapping
)
```

----------------------------------------

TITLE: Training TensorFlow Model with Callbacks
DESCRIPTION: Code to train the TensorFlow masked language model using the fit method, with the prepared datasets, specified number of epochs, and PushToHubCallback.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_21

LANGUAGE: python
CODE:
```
model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])
```

----------------------------------------

TITLE: Using BERT Model Outputs in Python
DESCRIPTION: This example demonstrates how to use a BERT model for sequence classification and access the structured outputs. It shows how to load a pre-trained model, tokenize input text, and retrieve various components from the model output object.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/output.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import BertTokenizer, BertForSequenceClassification
import torch

tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")
model = BertForSequenceClassification.from_pretrained("google-bert/bert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)
```

----------------------------------------

TITLE: Preprocessing Audio Function
DESCRIPTION: Defines a preprocessing function to extract audio arrays, perform feature extraction with padding and truncation, and return the processed inputs. The function handles variable-length audio sequences by either padding or truncating them to a specified maximum length.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_16

LANGUAGE: python
CODE:
```
>>> def preprocess_function(examples):
...     audio_arrays = [x["array"] for x in examples["audio"]]
...     inputs = feature_extractor(
...         audio_arrays,
...         sampling_rate=16000,
...         padding=True,
...         max_length=100000,
...         truncation=True,
...     )
...     return inputs
```

----------------------------------------

TITLE: Loading Wav2Vec2 Processor for Audio Preprocessing
DESCRIPTION: Python code to load a Wav2Vec2 processor from the Hugging Face model hub, which will be used to preprocess the audio data for the ASR model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/asr.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base")
```

----------------------------------------

TITLE: Freezing Embeddings and Language Adapters for Fine-tuning
DESCRIPTION: Demonstrates how to freeze embedding layers and language adapters during model fine-tuning
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/xmod.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
model.freeze_embeddings_and_language_adapters()
# Fine-tune the model ...
```

----------------------------------------

TITLE: Using Device Map for Large Models
DESCRIPTION: This code demonstrates how to use the `device_map` parameter to automatically distribute the model weights across multiple devices when the model is too large to fit on a single GPU. It requires the `accelerate` package.  The 'device_map' parameter is set to "auto", and handles the distribution.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/tutoriel_pipeline.md#_snippet_5

LANGUAGE: Python
CODE:
```
transcriber = pipeline(model="openai/whisper-large-v2", device_map="auto")
```

----------------------------------------

TITLE: Implementing a Framework-Agnostic Pair Classification Pipeline
DESCRIPTION: This is a complete implementation of a custom pipeline for pair classification that works with both PyTorch and TensorFlow models. It includes a softmax utility function and handles all pipeline stages correctly.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/add_new_pipeline.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import numpy as np

from transformers import Pipeline


def softmax(outputs):
    maxes = np.max(outputs, axis=-1, keepdims=True)
    shifted_exp = np.exp(outputs - maxes)
    return shifted_exp / shifted_exp.sum(axis=-1, keepdims=True)


class PairClassificationPipeline(Pipeline):
    def _sanitize_parameters(self, **kwargs):
        preprocess_kwargs = {}
        if "second_text" in kwargs:
            preprocess_kwargs["second_text"] = kwargs["second_text"]
        return preprocess_kwargs, {}, {}

    def preprocess(self, text, second_text=None):
        return self.tokenizer(text, text_pair=second_text, return_tensors=self.framework)

    def _forward(self, model_inputs):
        return self.model(**model_inputs)

    def postprocess(self, model_outputs):
        logits = model_outputs.logits[0].numpy()
        probabilities = softmax(logits)

        best_class = np.argmax(probabilities)
        label = self.model.config.id2label[best_class]
        score = probabilities[best_class].item()
        logits = logits.tolist()
        return {"label": label, "score": score, "logits": logits}
```

----------------------------------------

TITLE: Removing unnecessary columns from dataset
DESCRIPTION: Remove columns not needed for ASR training using the remove_columns method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/tasks/asr.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
minds = minds.remove_columns(["english_transcription", "intent_class", "lang_id"])
```

----------------------------------------

TITLE: Installing Accelerate Library
DESCRIPTION: Command to install the ðŸ¤— Accelerate library using pip package manager
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/accelerate.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install accelerate
```

----------------------------------------

TITLE: Loading MBart Model and Tokenizer for Multilingual Translation in Python
DESCRIPTION: This code demonstrates how to load the MBart model and tokenizer for multilingual translation, specifically for translating from Finnish to English using the 'facebook/mbart-large-50-many-to-many-mmt' checkpoint.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/multilingual.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
fi_text = "Ã„lÃ¤ sekaannu velhojen asioihin, sillÃ¤ ne ovat hienovaraisia ja nopeasti vihaisia."

tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt", src_lang="fi_FI")
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
```

----------------------------------------

TITLE: Lightweight Tuning with MVP
DESCRIPTION: This snippet demonstrates lightweight tuning (Prefix-tuning) for the MVP model, showcasing how to reduce the number of trainable parameters. It loads the MVP model and then sets it for lightweight tuning, comparing the number of trainable parameters before and after the tuning. It also shows the usage with task-specific prompts and the original Prefix-tuning with BART.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mvp.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import MvpForConditionalGeneration

>>> model = MvpForConditionalGeneration.from_pretrained("RUCAIBox/mvp", use_prompt=True)
>>> # the number of trainable parameters (full tuning)
>>> sum(p.numel() for p in model.parameters() if p.requires_grad)
468116832

>>> # lightweight tuning with randomly initialized prompts
>>> model.set_lightweight_tuning()
>>> # the number of trainable parameters (lightweight tuning)
>>> sum(p.numel() for p in model.parameters() if p.requires_grad)
61823328

>>> # lightweight tuning with task-specific prompts
>>> model = MvpForConditionalGeneration.from_pretrained("RUCAIBox/mtl-data-to-text")
>>> model.set_lightweight_tuning()
>>> # original lightweight Prefix-tuning
>>> model = MvpForConditionalGeneration.from_pretrained("facebook/bart-large", use_prompt=True)
>>> model.set_lightweight_tuning()
```

----------------------------------------

TITLE: Loading Checkpointed State
DESCRIPTION: Python code snippet showing how to load a sharded state dictionary using Accelerate's load_state method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/fsdp.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
# åŒ…å«æ£€æŸ¥ç‚¹çš„ç›®å½•
accelerator.load_state("ckpt")
```

----------------------------------------

TITLE: Preparing TensorFlow Datasets for Training
DESCRIPTION: Code to convert Hugging Face datasets to TensorFlow dataset format for model training and evaluation, with appropriate batch sizes and data collation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
tf_train_set = model.prepare_tf_dataset(
    lm_dataset["train"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_test_set = model.prepare_tf_dataset(
    lm_dataset["test"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)
```

----------------------------------------

TITLE: Loading Audio Classification Model for Inference in Python
DESCRIPTION: This snippet loads a pre-trained audio classification model from Hugging Face Transformers. It depends on the `transformers` library and the specified model name. The output is the model's logits, which represent the raw, unnormalized predictions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/audio_classification.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForAudioClassification

>>> model = AutoModelForAudioClassification.from_pretrained("stevhliu/my_awesome_minds_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

----------------------------------------

TITLE: Distributed training with mixed precision (PyTorch)
DESCRIPTION: This code demonstrates how to run the PyTorch summarization script with distributed training and mixed precision enabled. It uses `torchrun` to launch the script on multiple GPUs (`nproc_per_node`).  The `--fp16` argument activates mixed precision, and other arguments configure the model, dataset, and training parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/run_scripts.md#_snippet_5

LANGUAGE: bash
CODE:
```
torchrun \
    --nproc_per_node 8 pytorch/summarization/run_summarization.py \
    --fp16 \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Quantize Image-Text-to-Text Model with Quanto
DESCRIPTION: Loads and quantizes an image-text-to-text model using the QuantoConfig. It initializes the configuration for int8 quantization of weights and loads the pre-trained model with the specified configuration and device mapping.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_text_to_text.md#_snippet_13

LANGUAGE: python
CODE:
```
from transformers import AutoModelForImageTextToText, QuantoConfig

model_id = "HuggingFaceM4/idefics2-8b"
quantization_config = QuantoConfig(weights="int8")
quantized_model = AutoModelForImageTextToText.from_pretrained(
    model_id, device_map="cuda", quantization_config=quantization_config
)
```

----------------------------------------

TITLE: Checking memory footprint of a quantized model
DESCRIPTION: Simple code snippet to check the memory usage of a quantized model using the get_memory_footprint method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-23_snippet_7

LANGUAGE: python
CODE:
```
print(model.get_memory_footprint())
```

----------------------------------------

TITLE: Exporting BERT Model to TFLite Format
DESCRIPTION: Command to export a BERT model from Hugging Face Hub to TFLite format with specified sequence length.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/tflite.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
optimum-cli export tflite --model google-bert/bert-base-uncased --sequence_length 128 bert_tflite/
```

----------------------------------------

TITLE: Login to Hugging Face - Python
DESCRIPTION: This snippet allows users to log in to their Hugging Face account. Upon running the code, you will be prompted to enter your token for authentication.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from huggingface_hub import notebook_login

notebook_login()
```

----------------------------------------

TITLE: Initializing and Using Pipeline for Automatic Speech Recognition in Python
DESCRIPTION: This code demonstrates how to create a pipeline for automatic speech recognition and use it to transcribe audio from a URL. It shows both basic initialization and how to specify a particular model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pipeline_tutorial.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> generator = pipeline(task="automatic-speech-recognition")
>>> generator("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES'}
```

----------------------------------------

TITLE: Creating Tokenizers
DESCRIPTION: Examples of creating both slow and fast tokenizers for DistilBERT.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/create_a_model.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers import DistilBertTokenizer, DistilBertTokenizerFast

slow_tokenizer = DistilBertTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
fast_tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Loading Tokenizer and Model from Local Directory
DESCRIPTION: This Python code loads a pre-trained tokenizer and model from a local directory.  This enables offline use of the model after it has been downloaded and saved.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/installation.md#_snippet_20

LANGUAGE: python
CODE:
```
>>> tokenizer = AutoTokenizer.from_pretrained("./your/path/bigscience_t0")
>>> model = AutoModel.from_pretrained("./your/path/bigscience_t0")
```

----------------------------------------

TITLE: Defining Compute Metrics Function
DESCRIPTION: This code defines a `compute_metrics` function that takes the model's predictions and labels as input, calculates the accuracy, and returns the result. It uses `np.argmax` to get the predicted class from the model's output logits and `accuracy.compute` to calculate the accuracy.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/multiple_choice.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
">>> import numpy as np\n\n\n>>> def compute_metrics(eval_pred):\n...     predictions, labels = eval_pred\n...     predictions = np.argmax(predictions, axis=1)\n...     return accuracy.compute(predictions=predictions, references=labels)"
```

----------------------------------------

TITLE: Applying Preprocessing to Dataset using map()
DESCRIPTION: Uses the Datasets map() method to apply the preprocessing function to the entire dataset. The batched=True parameter speeds up processing by handling multiple elements at once.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
tokenized_billsum = billsum.map(preprocess_function, batched=True)
```

----------------------------------------

TITLE: Resume Training from Specific Checkpoint
DESCRIPTION: Resumes training from a specific checkpoint. It specifies the model, dataset, source prefix, output directory and the checkpoint path.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/run_scripts.md#_snippet_16

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --resume_from_checkpoint path_to_specific_checkpoint \
    --predict_with_generate
```

----------------------------------------

TITLE: Installing Starlette and Uvicorn
DESCRIPTION: This bash command installs the Starlette web framework and the Uvicorn ASGI server, which are essential for creating a web server to host the Transformers pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_webserver.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
"!pip install starlette uvicorn"
```

----------------------------------------

TITLE: Function to Group Texts for Model Input
DESCRIPTION: This function concatenates the tokenized text examples, splits them into smaller chunks that fit the model's input constraints, and prepares labels for training based on the input IDs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> block_size = 128


>>> def group_texts(examples):
...     # Concatenate all texts.
...     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
...     total_length = len(concatenated_examples[list(examples.keys())[0]])
...     # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can
...     # customize this part to your needs.
...     if total_length >= block_size:
...         total_length = (total_length // block_size) * block_size
...     # Split by chunks of block_size.
...     result = {
...         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
...         for k, t in concatenated_examples.items()
...     }
...     result["labels"] = result["input_ids"].copy()
...     return result
```

----------------------------------------

TITLE: Loading and Splitting SQuAD Dataset
DESCRIPTION: Loads a subset of the SQuAD dataset and splits it into training and testing sets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/question_answering.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> squad = load_dataset("squad", split="train[:5000]")
>>> squad = squad.train_test_split(test_size=0.2)
```

----------------------------------------

TITLE: Initializing Object Detection and Pose Estimation Models with ViTPose
DESCRIPTION: Demonstrates loading pre-trained models for human detection and keypoint estimation using Hugging Face Transformers library. Involves two-stage process: first detecting humans, then estimating pose keypoints.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/vitpose.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
import requests
import numpy as np

from PIL import Image

from transformers import AutoProcessor, RTDetrForObjectDetection, VitPoseForPoseEstimation

device = "cuda" if torch.cuda.is_available() else "cpu"

url = "http://images.cocodataset.org/val2017/000000000139.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# Stage 1. Detect humans on the image
person_image_processor = AutoProcessor.from_pretrained("PekingU/rtdetr_r50vd_coco_o365")
person_model = RTDetrForObjectDetection.from_pretrained("PekingU/rtdetr_r50vd_coco_o365", device_map=device)

inputs = person_image_processor(images=image, return_tensors="pt").to(device)

with torch.no_grad():
    outputs = person_model(**inputs)

results = person_image_processor.post_process_object_detection(
    outputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.3
)
result = results[0]  # take first image results

# Human label refers 0 index in COCO dataset
person_boxes = result["boxes"][result["labels"] == 0]
person_boxes = person_boxes.cpu().numpy()

# Convert boxes from VOC (x1, y1, x2, y2) to COCO (x1, y1, w, h) format
person_boxes[:, 2] = person_boxes[:, 2] - person_boxes[:, 0]
person_boxes[:, 3] = person_boxes[:, 3] - person_boxes[:, 1]

# Stage 2. Detect keypoints for each person found
image_processor = AutoProcessor.from_pretrained("usyd-community/vitpose-base-simple")
model = VitPoseForPoseEstimation.from_pretrained("usyd-community/vitpose-base-simple", device_map=device)

inputs = image_processor(image, boxes=[person_boxes], return_tensors="pt").to(device)

with torch.no_grad():
    outputs = model(**inputs)

pose_results = image_processor.post_process_pose_estimation(outputs, boxes=[person_boxes])
image_pose_result = pose_results[0]  # results for first image
```

----------------------------------------

TITLE: Defining Weights & Biases Hyperparameter Search Space in Python
DESCRIPTION: Example of defining a hyperparameter search space for Weights & Biases backend in Python, specifying method, metric, learning rate, and batch size parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/hpo_train.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
def wandb_hp_space(trial):
    return {
        "method": "random",
        "metric": {"name": "objective", "goal": "minimize"},
        "parameters": {
            "learning_rate": {"distribution": "uniform", "min": 1e-6, "max": 1e-4},
            "per_device_train_batch_size": {"values": [16, 32, 64, 128]},
        },
    }
```

----------------------------------------

TITLE: Visualizing Training Samples with Matplotlib
DESCRIPTION: Creates a function to visualize image samples from the training dataset along with their captions using matplotlib.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_captioning.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from textwrap import wrap
import matplotlib.pyplot as plt
import numpy as np


def plot_images(images, captions):
    plt.figure(figsize=(20, 20))
    for i in range(len(images)):
        ax = plt.subplot(1, len(images), i + 1)
        caption = captions[i]
        caption = "\n".join(wrap(caption, 12))
        plt.title(caption)
        plt.imshow(images[i])
        plt.axis("off")


sample_images_to_visualize = [np.array(train_ds[i]["image"]) for i in range(5)]
sample_captions = [train_ds[i]["text"] for i in range(5)]
plot_images(sample_images_to_visualize, sample_captions)
```

----------------------------------------

TITLE: Handling Multiple Images for Vision-Language Queries in Python
DESCRIPTION: This example configures the PaliGemma model to accept multiple images using the processor, supporting queries like comparing images for similarity. Dependencies include transformers, image handling, and HTTP requests. Input consists of a list of images and a prompt, resulting in a generated textual comparison.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/paligemma.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
import torch
import requests
from PIL import Image
from transformers import TorchAoConfig, AutoProcessor, PaliGemmaForConditionalGeneration

model = PaliGemmaForConditionalGeneration.from_pretrained("google/paligemma-3b-ft-nlvr2-448")
processor = AutoProcessor.from_pretrained("google/paligemma-3b-ft-nlvr2-448")

prompt = "Are these two images the same?"
cat_image = Image.open(
    requests.get("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg", stream=True).raw
)
cow_image = Image.open(
    requests.get(
        "https://media.istockphoto.com/id/1192867753/photo/cow-in-berchida-beach-siniscola.jpg?s=612x612&w=0&k=20&c=v0hjjniwsMNfJSuKWZuIn8pssmD5h5bSN1peBd1CmH4=", stream=True
    ).raw
)

inputs = processor(images=[[cat_image, cow_image]], text=prompt, return_tensors="pt")

output = model.generate(**inputs, max_new_tokens=20, cache_implementation="static")
print(processor.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Loading a Compressed Tensors Model with HF Quantizer - Python
DESCRIPTION: This code snippet demonstrates how to load a compressed-tensors model using the HFQuantizer integration and prints the model architecture.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/compressed_tensors.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

ct_model = AutoModelForCausalLM.from_pretrained("nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf")
print(ct_model)
"""
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): CompressedLinear(
            in_features=4096, out_features=4096, bias=False
            (input_observer): MovingAverageMinMaxObserver()
            (weight_observer): MovingAverageMinMaxObserver()
          )
          (k_proj): CompressedLinear(
            in_features=4096, out_features=1024, bias=False
            (input_observer): MovingAverageMinMaxObserver()
            (weight_observer): MovingAverageMinMaxObserver()
          )
          (v_proj): CompressedLinear(
            in_features=4096, out_features=1024, bias=False
            (input_observer): MovingAverageMinMaxObserver()
            (weight_observer): MovingAverageMinMaxObserver()
          )
          (o_proj): CompressedLinear(
            in_features=4096, out_features=4096, bias=False
            (input_observer): MovingAverageMinMaxObserver()
            (weight_observer): MovingAverageMinMaxObserver()
          )
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): CompressedLinear(
            in_features=4096, out_features=14336, bias=False
            (input_observer): MovingAverageMinMaxObserver()
            (weight_observer): MovingAverageMinMaxObserver()
          )
          (up_proj): CompressedLinear(
            in_features=4096, out_features=14336, bias=False
            (input_observer): MovingAverageMinMaxObserver()
            (weight_observer): MovingAverageMinMaxObserver()
          )
          (down_proj): CompressedLinear(
            in_features=14336, out_features=4096, bias=False
            (input_observer): MovingAverageMinMaxObserver()
            (weight_observer): MovingAverageMinMaxObserver()
          )
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
"""
```

----------------------------------------

TITLE: Saving a Model with Sharding
DESCRIPTION: This snippet demonstrates how to save a model into multiple sharded files using `model.save_pretrained` with the `max_shard_size` parameter. This reduces the size of individual weight files. The resulting directory contains `config.json`, multiple `pytorch_model-xxxxx-of-yyyyy.bin` files, and a `pytorch_model.bin.index.json` file.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/big_models.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir, max_shard_size="200MB")
...     print(sorted(os.listdir(tmp_dir)))
['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']
```

----------------------------------------

TITLE: Adding and Managing Multiple Adapters
DESCRIPTION: Example showing how to add multiple adapters to a model and switch between them.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/peft.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import PeftConfig

model_id = "facebook/opt-350m"
model = AutoModelForCausalLM.from_pretrained(model_id)

lora_config = LoraConfig(
    target_modules=["q_proj", "k_proj"],
    init_lora_weights=False
)

model.add_adapter(lora_config, adapter_name="adapter_1")

# attach new adapter with same config
model.add_adapter(lora_config, adapter_name="adapter_2")
```

----------------------------------------

TITLE: Removing Unnecessary Columns from MInDS-14 Dataset
DESCRIPTION: Removes columns not needed for the ASR task, keeping only the 'audio' and 'transcription' columns.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/asr.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> minds = minds.remove_columns(["english_transcription", "intent_class", "lang_id"])
```

----------------------------------------

TITLE: Preprocessing Inputs with Chat Template for VLM
DESCRIPTION: This code applies the chat template to the messages, adds a generation prompt, and then preprocesses the combined text and images using the processor. The processed inputs are converted to PyTorch tensors and moved to the CUDA device for model inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_text_to_text.md#_snippet_4

LANGUAGE: python
CODE:
```
prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
inputs = processor(text=prompt, images=[images[0], images[1]], return_tensors="pt").to(device)
```

----------------------------------------

TITLE: Setting Number of GPUs with Accelerate
DESCRIPTION: This snippet illustrates how to configure the number of GPUs to use with the Accelerate library. The --num_processes parameter allows users to specify how many GPUs will be allocated for the training process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/gpu_selection.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
accelerate launch --num_processes 2 trainer-program.py ...
```

----------------------------------------

TITLE: Hub Authentication via Notebook
DESCRIPTION: Python code to login to Hugging Face Hub in a notebook environment
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/custom_models.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
from huggingface_hub import notebook_login

notebook_login()
```

----------------------------------------

TITLE: Loading the ViLT Processor for Model Input Preparation
DESCRIPTION: Initializes the ViltProcessor that handles both text tokenization and image preprocessing, creating model-ready inputs from raw data.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/visual_question_answering.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> from transformers import ViltProcessor

>>> processor = ViltProcessor.from_pretrained(model_checkpoint)
```

----------------------------------------

TITLE: Initializing Trainer for Hyperparameter Search in Python
DESCRIPTION: This snippet creates an instance of the Trainer class, initializing it with essential parameters including the model, training arguments, datasets, and the model_init function. This setup is necessary to perform hyperparameter searches using the Trainer's hyperparameter_search method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/hpo_train.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
from transformers import Trainer

trainer = Trainer(
    model=None,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
    processing_class=tokenizer,
    model_init=model_init,
    data_collator=data_collator,
)
trainer.hyperparameter_search(...)

```

----------------------------------------

TITLE: Installing Required Libraries
DESCRIPTION: Installing the necessary Python packages: transformers, datasets, and evaluate.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/audio_classification.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers datasets evaluate
```

----------------------------------------

TITLE: Data Collation Setup
DESCRIPTION: Configures the DataCollatorForTokenClassification for batch processing with dynamic padding.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/tasks/token_classification.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```

----------------------------------------

TITLE: Running Inference with JIT Mode and IPEX on CPU
DESCRIPTION: Example command for running question answering inference using JIT mode with IPEX optimization on CPU using the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/perf_infer_cpu.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
python run_qa.py \
--model_name_or_path csarron/bert-base-uncased-squad-v1 \
--dataset_name squad \
--do_eval \
--max_seq_length 384 \
--doc_stride 128 \
--output_dir /tmp/ \
--no_cuda \
--use_ipex \
--jit_mode_eval
```

----------------------------------------

TITLE: Saving and Reloading a Quantized Model in Python
DESCRIPTION: In this Python code snippet, the quantized model is saved to a specified path using the `save_pretrained` method. The model can later be loaded again from this path, which retains the quantized state.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/eetq.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
quant_path = "/path/to/save/quantized/model"
model.save_pretrained(quant_path)
model = AutoModelForCausalLM.from_pretrained(quant_path, device_map="auto")
```

----------------------------------------

TITLE: Training ViT Model for Image Classification
DESCRIPTION: This Python command runs a script to fine-tune a ViT model on the Imagenette dataset. It specifies various training parameters such as output directory, model name, dataset paths, number of epochs, learning rate, and batch sizes.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/flax/vision/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
python run_image_classification.py \
    --output_dir ./vit-base-patch16-imagenette \
    --model_name_or_path google/vit-base-patch16-224-in21k \
    --train_dir="imagenette2/train" \
    --validation_dir="imagenette2/val" \
    --num_train_epochs 5 \
    --learning_rate 1e-3 \
    --per_device_train_batch_size 128 --per_device_eval_batch_size 128 \
    --overwrite_output_dir \
    --preprocessing_num_workers 32 \
    --push_to_hub
```

----------------------------------------

TITLE: Implementing Flash Attention 2
DESCRIPTION: Shows how to load Bark model with Flash Attention 2 optimization in half-precision.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bark.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
model = BarkModel.from_pretrained("suno/bark-small", torch_dtype=torch.float16, attn_implementation="flash_attention_2").to(device)
```

----------------------------------------

TITLE: Loading a ViLT Processor for Image and Text Preprocessing
DESCRIPTION: Initializes a ViLT processor from a pretrained checkpoint to prepare image and text data for the model. The processor handles tokenization of text and processing of images.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_question_answering.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> from transformers import ViltProcessor

>>> processor = ViltProcessor.from_pretrained(model_checkpoint)
```

----------------------------------------

TITLE: Loading Model (PyTorch)
DESCRIPTION: This snippet shows how to reload a saved PyTorch model using `AutoModelForSequenceClassification.from_pretrained`. It assumes that the model and tokenizer have been previously saved to the specified directory. The directory path is provided as a string.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_25

LANGUAGE: python
CODE:
```
>>> pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")
```

----------------------------------------

TITLE: Creating Label Mappings from Hugging Face Hub
DESCRIPTION: Python code for creating id2label and label2id dictionaries by downloading the label mapping file from Hugging Face Hub, which will be used for model configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> import json
>>> from pathlib import Path
>>> from huggingface_hub import hf_hub_download

>>> repo_id = "huggingface/label-files"
>>> filename = "ade20k-id2label.json"
>>> id2label = json.loads(Path(hf_hub_download(repo_id, filename, repo_type="dataset")).read_text())
>>> id2label = {int(k): v for k, v in id2label.items()}
>>> label2id = {v: k for k, v in id2label.items()}
>>> num_labels = len(id2label)
```

----------------------------------------

TITLE: Applying BlenderBot Chat Template in Python
DESCRIPTION: Example of using the BlenderBot tokenizer to apply a simple chat template to a conversation. The template converts a list of messages into a single string with spaces between dialogue rounds.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/chat_templating.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer
>>> tokenizer = AutoTokenizer.from_pretrained("facebook/blenderbot-400M-distill")

>>> chat = [
...    {"role": "user", "content": "Hello, how are you?"},
...    {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
...    {"role": "user", "content": "I'd like to show off how chat templating works!"},
... ]

>>> tokenizer.apply_chat_template(chat, tokenize=False)
" Hello, how are you?  I'm doing great. How can I help you today?   I'd like to show off how chat templating works!</s>"
```

----------------------------------------

TITLE: Performing Audio Classification with ðŸ¤— Transformers Pipeline in Python
DESCRIPTION: This snippet demonstrates how to use the ðŸ¤— Transformers pipeline for audio classification. It loads a pre-trained model and classifies an audio file, returning emotion labels and scores.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/task_summary.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline(task="audio-classification", model="superb/hubert-base-superb-er")
>>> preds = classifier("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.4532, 'label': 'hap'},
 {'score': 0.3622, 'label': 'sad'},
 {'score': 0.0943, 'label': 'neu'},
 {'score': 0.0903, 'label': 'ang'}]
```

----------------------------------------

TITLE: DeepSpeed Configuration for Accelerate
DESCRIPTION: YAML configuration for DeepSpeed integration with Accelerate, specifying the path to a DeepSpeed configuration file and enabling Zero-3 optimization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/trainer.md#2025-04-22_snippet_12

LANGUAGE: yaml
CODE:
```
compute_environment: LOCAL_MACHINE
deepspeed_config:
  deepspeed_config_file: /home/user/configs/ds_zero3_config.json
  zero3_init_flag: true
distributed_type: DEEPSPEED
downcast_bf16: 'no'
machine_rank: 0
main_training_function: main
num_machines: 1
num_processes: 4
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

----------------------------------------

TITLE: Evaluation Metrics Implementation
DESCRIPTION: Implements compute_metrics function to calculate accuracy during the training process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/knowledge_distillation_for_image_classification.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import evaluate
import numpy as np

accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    acc = accuracy.compute(references=labels, predictions=np.argmax(predictions, axis=1))
    return {"accuracy": acc["accuracy"]}
```

----------------------------------------

TITLE: Training with DeepSpeed and Transformers using Python
DESCRIPTION: This Python snippet uses DeepSpeed to train models with a specified configuration. It involves setting up TrainingArguments with DeepSpeed enabled and utilizing the Trainer API to manage the training process. Requires DeepSpeed and Transformers libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/deepspeed.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from transformers import AutoModel, Trainer, TrainingArguments

training_args = TrainingArguments(..., deepspeed=ds_config)
model = AutoModel.from_pretrained("google-t5/t5-small")
trainer = Trainer(model=model, args=training_args, ...)
```

----------------------------------------

TITLE: Loading and Processing an Image for Depth Estimation
DESCRIPTION: This snippet explains how to load an image from a URL using Pillow, preparing it for subsequent depth estimation processes within the pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/monocular_depth_estimation.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
>>> from PIL import Image
>>> import requests
>>> url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> image
```

----------------------------------------

TITLE: Disabling Cache in Transformer Generation
DESCRIPTION: Example showing how to disable caching during text generation with a Llama-2 model by setting use_cache=False
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/kv_cache.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf", torch_dtype=torch.float16).to("cuda:0")
inputs = tokenizer("I like rock music because", return_tensors="pt").to(model.device)

model.generate(**inputs, do_sample=False, max_new_tokens=20, use_cache=False)
```

----------------------------------------

TITLE: Using DePlot for Visual Question Answering in Python
DESCRIPTION: This code snippet demonstrates how to use the DePlot model to generate a data table from an image. It loads the pre-trained model and processor, processes an input image and text prompt, and generates a prediction.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/deplot.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoProcessor, Pix2StructForConditionalGeneration
import requests
from PIL import Image

model = Pix2StructForConditionalGeneration.from_pretrained("google/deplot")
processor = AutoProcessor.from_pretrained("google/deplot")
url = "https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, text="Generate underlying data table of the figure below:", return_tensors="pt")
predictions = model.generate(**inputs, max_new_tokens=512)
print(processor.decode(predictions[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Named Entity Recognition with Falcon-7b-instruct in Python
DESCRIPTION: This code demonstrates how to perform Named Entity Recognition using the Falcon-7b-instruct model by creating a prompt that asks the model to identify named entities in a given text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/prompting.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> torch.manual_seed(1) # doctest: +IGNORE_RESULT
>>> prompt = """Return a list of named entities in the text.
... Text: The Golden State Warriors are an American professional basketball team based in San Francisco.
... Named entities:
... """

>>> sequences = pipe(
...     prompt,
...     max_new_tokens=15,
...     return_full_text = False,    
... )

>>> for seq in sequences:
...     print(f"{seq['generated_text']}")
- Golden State Warriors
- San Francisco
```

----------------------------------------

TITLE: Applying Image Transformations with TensorFlow
DESCRIPTION: Defines data augmentation transformations for training data using Keras preprocessing layers to make the model robust against overfitting.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_classification.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from tensorflow import keras
>>> from tensorflow.keras import layers

>>> size = (image_processor.size["height"], image_processor.size["width"])

>>> train_data_augmentation = keras.Sequential(
...     [
...         layers.RandomCrop(size[0], size[1]),
...         layers.Rescaling(scale=1.0 / 127.5, offset=-1),
...         layers.RandomFlip("horizontal"),
...         layers.RandomRotation(factor=0.02),
...         layers.RandomZoom(height_factor=0.2, width_factor=0.2),
...     ],
...     name="train_data_augmentation",
... )
```

LANGUAGE: python
CODE:
```
>>> val_data_augmentation = keras.Sequential(
...     [
...         layers.CenterCrop(size[0], size[1]),
...         layers.Rescaling(scale=1.0 / 127.5, offset=-1),
...     ],
...     name="val_data_augmentation",
... )
```

----------------------------------------

TITLE: LLaVa Prompt Format (llava-1.5 models)
DESCRIPTION: This snippet demonstrates the required prompt format for llava-1.5 models. It highlights the structure needed for both single and multiple turn conversations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llava.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
"USER: <image>\n<prompt> ASSISTANT:"
```

LANGUAGE: bash
CODE:
```
"USER: <image>\n<prompt1> ASSISTANT: <answer1></s>USER: <prompt2> ASSISTANT: <answer2></s>USER: <prompt3> ASSISTANT:"
```

----------------------------------------

TITLE: Multilingual and Special Audio Generation
DESCRIPTION: Shows how to generate multilingual speech and music using Bark.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/bark.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> # Multilingual speech - simplified Chinese
>>> inputs = processor("æƒŠäººçš„ï¼æˆ‘ä¼šè¯´ä¸­æ–‡")

>>> # Multilingual speech - French - let's use a voice_preset as well
>>> inputs = processor("Incroyable! Je peux gÃ©nÃ©rer du son.", voice_preset="fr_speaker_5")

>>> # Bark can also generate music. You can help it out by adding music notes around your lyrics.
>>> inputs = processor("â™ª Hello, my dog is cute â™ª")

>>> audio_array = model.generate(**inputs)
>>> audio_array = audio_array.cpu().numpy().squeeze()
```

----------------------------------------

TITLE: Exporting TensorFlow Checkpoint to ONNX
DESCRIPTION: Command to export a TensorFlow checkpoint from Keras to ONNX format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/serialization.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
python -m transformers.onnx --model=keras-io/transformers-qa onnx/
```

----------------------------------------

TITLE: Model Inference for Segmentation in PyTorch
DESCRIPTION: This snippet describes how to run the prepared image input through the model to obtain logits and subsequently scale the logits back to the original image size for segmentation results.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_26

LANGUAGE: python
CODE:
```
>>> outputs = model(pixel_values=pixel_values)
>>> logits = outputs.logits.cpu()
```

----------------------------------------

TITLE: Splitting Dataset into Train and Test Sets
DESCRIPTION: Creates training and testing splits from the loaded SQuAD dataset using the train_test_split method, allocating 80% for training and 20% for testing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/question_answering.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> squad = squad.train_test_split(test_size=0.2)
```

----------------------------------------

TITLE: Preparing TensorFlow datasets
DESCRIPTION: This snippet shows how to prepare the training and validation datasets using `prepare_tf_dataset`. It configures the batch size, shuffling, and data collation function.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
>>> tf_train_set = model.prepare_tf_dataset(
...     tokenized_imdb["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_validation_set = model.prepare_tf_dataset(
...     tokenized_imdb["test"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

----------------------------------------

TITLE: Custom Loss Function Implementation
DESCRIPTION: Example of customizing the Trainer by subclassing to implement a weighted loss function
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/trainer.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from torch import nn
from transformers import Trainer

class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        # ìˆœë°©í–¥ ì „íŒŒ
        outputs = model(**inputs)
        logits = outputs.get("logits")
        # ì„œë¡œ ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ë¡œ 3ê°œì˜ ë ˆì´ë¸”ì— ëŒ€í•œ ì‚¬ìš©ìž ì •ì˜ ì†ì‹¤ì„ ê³„ì‚°
        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0], device=model.device))
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss
```

----------------------------------------

TITLE: Fine-tuning T5 on SQuAD2.0 using Seq2Seq Approach
DESCRIPTION: Script for fine-tuning T5 model on SQuAD2.0 using a sequence-to-sequence approach. Configured for generative question answering rather than span prediction.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/README.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
python run_seq2seq_qa.py \
  --model_name_or_path google-t5/t5-small \
  --dataset_name squad_v2 \
  --context_column context \
  --question_column question \
  --answer_column answers \
  --do_train \
  --do_eval \
  --per_device_train_batch_size 12 \
  --learning_rate 3e-5 \
  --num_train_epochs 2 \
  --max_seq_length 384 \
  --doc_stride 128 \
  --output_dir /tmp/debug_seq2seq_squad/
```

----------------------------------------

TITLE: Model Inference to Obtain Logits in TensorFlow
DESCRIPTION: This code shows how to call the TensorFlow model with pre-processed inputs to obtain prediction logits.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/sequence_classification.md#2025-04-22_snippet_30

LANGUAGE: python
CODE:
```
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained("MariaK/scene_segmentation")
>>> logits = model(**inputs).logits
```

----------------------------------------

TITLE: Loading and Preprocessing Audio Data for Inference with Datasets
DESCRIPTION: Demonstrates how to load an audio dataset and preprocess it with resampling to ensure the sampling rate matches the model's expected input rate of 16000Hz.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/audio_classification.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
>>> sampling_rate = dataset.features["audio"].sampling_rate
>>> audio_file = dataset[0]["audio"]["path"]
```

----------------------------------------

TITLE: Object Detection with Default Model in Python
DESCRIPTION: Example demonstrating how to detect objects in an image with bounding boxes using the transformers pipeline with its default object detection model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/task_summary.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> detector = pipeline(task="object-detection")
>>> preds = detector(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"], "box": pred["box"]} for pred in preds]
>>> preds
[{'score': 0.9865,
  'label': 'cat',
  'box': {'xmin': 178, 'ymin': 154, 'xmax': 882, 'ymax': 598}}]
```

----------------------------------------

TITLE: Examining a Sample from the IMDb Dataset
DESCRIPTION: Code to inspect a single example from the test split of the IMDb dataset, showing the structure with 'text' containing the review and 'label' indicating sentiment (0 for negative, 1 for positive).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/sequence_classification.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> imdb["test"][0]
{
    "label": 0,
    "text": "I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clichÃ©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.",
}
```

----------------------------------------

TITLE: Loading Audio Dataset for Inference in Python
DESCRIPTION: This snippet loads an audio dataset from Hugging Face Datasets, casts the audio column to the correct sampling rate, and retrieves the path to an audio file. It depends on the `datasets` library and the specified dataset name. The output is the `audio_file` path which will be used for inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/audio_classification.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
>>> sampling_rate = dataset.features["audio"].sampling_rate
>>> audio_file = dataset[0]["audio"]["path"]
```

----------------------------------------

TITLE: Pushing the Model to Hub with PyTorch
DESCRIPTION: This snippet demonstrates how to share the fine-tuned PyTorch model to the Hugging Face hub for public access after training is complete.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
>>> trainer.push_to_hub()
```

----------------------------------------

TITLE: Applying Preprocessing Function
DESCRIPTION: Applies the preprocessing function to the first few examples of the dataset. This transforms the raw audio data into a format suitable for input to the Wav2Vec2 model, including feature extraction, padding, and truncation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_17

LANGUAGE: python
CODE:
```
>>> processed_dataset = preprocess_function(dataset[:5])
```

----------------------------------------

TITLE: TensorFlow Inference Implementation
DESCRIPTION: Performs inference using a fine-tuned multiple choice model with TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/multiple_choice.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, TFAutoModelForMultipleChoice

tokenizer = AutoTokenizer.from_pretrained("username/my_awesome_swag_model")
inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors="tf", padding=True)

model = TFAutoModelForMultipleChoice.from_pretrained("username/my_awesome_swag_model")
inputs = {k: tf.expand_dims(v, 0) for k, v in inputs.items()}
outputs = model(inputs)
logits = outputs.logits
predicted_class = int(tf.math.argmax(logits, axis=-1)[0])
```

----------------------------------------

TITLE: Creating Wav2Vec2 Feature Extractor in Python
DESCRIPTION: This code snippet demonstrates the creation of a Wav2Vec2FeatureExtractor with specified padding value and normalization settings. This feature extractor is used for processing audio inputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/create_a_model.md#_snippet_28

LANGUAGE: python
CODE:
```
>>> from transformers import Wav2Vec2FeatureExtractor

>>> feature_extractor = Wav2Vec2FeatureExtractor(padding_value=1.0, do_normalize=True)
```

----------------------------------------

TITLE: Visualizing Dataset Example with Bounding Boxes
DESCRIPTION: Visualizes a dataset example by drawing bounding boxes around the detected objects. This code snippet retrieves an image and its corresponding annotations, then draws rectangles around the objects based on their bounding box coordinates and displays the image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/object_detection.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
">>> import numpy as np\n>>> import os\n>>> from PIL import Image, ImageDraw\n\n>>> image = cppe5[\"train\"][2][\"image\"]\n>>> annotations = cppe5[\"train\"][2][\"objects\"]\n>>> draw = ImageDraw.Draw(image)\n\n>>> categories = cppe5[\"train\"] .features[\"objects\"].feature[\"category\"].names\n\n>>> id2label = {index: x for index, x in enumerate(categories, start=0)}\n>>> label2id = {v: k for k, v in id2label.items()}\n\n>>> for i in range(len(annotations[\"id\"])):\n...     box = annotations[\"bbox\"][i]\n...     class_idx = annotations[\"category\"][i]\n...     x, y, w, h = tuple(box)\n...     # Check if coordinates are normalized or not\n...     if max(box) > 1.0:\n...         # Coordinates are un-normalized, no need to re-scale them\n...         x1, y1 = int(x), int(y)\n...         x2, y2 = int(x + w), int(y + h)\n...     else:\n...         # Coordinates are normalized, re-scale them\n...         x1 = int(x * width)\n...         y1 = int(y * height)\n...         x2 = int((x + w) * width)\n...         y2 = int((y + h) * height)\n...     draw.rectangle((x, y, x + w, y + h), outline=\"red\", width=1)\n...     draw.text((x, y), id2label[class_idx], fill=\"white\")\n\n>>> image"
```

----------------------------------------

TITLE: Estimating Memory Requirements for DeepSpeed ZeRO with Multiple GPUs
DESCRIPTION: This code snippet estimates memory requirements for fine-tuning a T0_3B model using DeepSpeed ZeRO with two GPUs. It shows how distributing the model across multiple GPUs reduces the per-GPU memory requirements.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/deepspeed.md#2025-04-22_snippet_39

LANGUAGE: bash
CODE:
```
$ python -c 'from transformers import AutoModel; \
from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \
model = AutoModel.from_pretrained("bigscience/T0_3B"); \
estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=2, num_nodes=1)'
[...]
Estimated memory needed for params, optim states and gradients for a:
HW: Setup with 1 node, 2 GPUs per node.
SW: Model with 2783M total params, 65M largest layer params.
  per CPU  |  per GPU |   Options
   70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1
   70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0
   62.23GB |   2.84GB | offload_param=none, offload_optimizer=cpu , zero_init=1
   62.23GB |   2.84GB | offload_param=none, offload_optimizer=cpu , zero_init=0
    0.74GB |  23.58GB | offload_param=none, offload_optimizer=none, zero_init=1
   31.11GB |  23.58GB | offload_param=none, offload_optimizer=none, zero_init=0
```

----------------------------------------

TITLE: Configuring DeepSpeed with Accelerate Plugins
DESCRIPTION: This YAML configuration file sets up DeepSpeed using Accelerate's plugin system. It configures parameters like gradient accumulation steps, gradient clipping, offloading devices, and the ZeRO stage.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/trainer.md#2025-04-22_snippet_11

LANGUAGE: yaml
CODE:
```
compute_environment: LOCAL_MACHINE                                                                                             
deepspeed_config:                                                                                                              
  gradient_accumulation_steps: 1
  gradient_clipping: 0.7
  offload_optimizer_device: cpu
  offload_param_device: cpu
  zero3_init_flag: true
  zero_stage: 2
distributed_type: DEEPSPEED
downcast_bf16: 'no'
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 4
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

----------------------------------------

TITLE: Defining Olmo2RMSNorm class in Python
DESCRIPTION: This simple snippet defines the `Olmo2RMSNorm` class, which inherits from `LlamaRMSNorm`. It showcases how inheritance allows for reuse of existing functionality without modification.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/modular_transformers.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from ..llama.modeling_llama import LlamaRMSNorm

class Olmo2RMSNorm(LlamaRMSNorm):
    pass

```

----------------------------------------

TITLE: Enabling CPU Offload for Bark
DESCRIPTION: Shows how to enable CPU offloading to reduce GPU memory usage by moving idle submodels to CPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bark.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
model.enable_cpu_offload()
```

----------------------------------------

TITLE: Loading SQuAD Dataset
DESCRIPTION: Load a subset of the SQuAD dataset and split it into train and test sets
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/question_answering.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> squad = load_dataset("squad", split="train[:5000]")
```

LANGUAGE: python
CODE:
```
>>> squad = squad.train_test_split(test_size=0.2)
```

LANGUAGE: python
CODE:
```
>>> squad["train"][0]
```

----------------------------------------

TITLE: Video Processing and Model Inference with Video-LLaVA
DESCRIPTION: Example showing how to process video input and generate responses using Video-LLaVA model in half-precision. Includes video frame extraction and model inference steps.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/video_llava.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import av
import torch
import numpy as np
from transformers import VideoLlavaForConditionalGeneration, VideoLlavaProcessor

def read_video_pyav(container, indices):
    '''
    Decode the video with PyAV decoder.
    Args:
        container (`av.container.input.InputContainer`): PyAV container.
        indices (`List[int]`): List of frame indices to decode.
    Returns:
        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).
    '''
    frames = []
    container.seek(0)
    start_index = indices[0]
    end_index = indices[-1]
    for i, frame in enumerate(container.decode(video=0)):
        if i > end_index:
            break
        if i >= start_index and i in indices:
            frames.append(frame)
    return np.stack([x.to_ndarray(format="rgb24") for x in frames])

# Load the model in half-precision
model = VideoLlavaForConditionalGeneration.from_pretrained("LanguageBind/Video-LLaVA-7B-hf", torch_dtype=torch.float16, device_map="auto")
processor = VideoLlavaProcessor.from_pretrained("LanguageBind/Video-LLaVA-7B-hf")

# Load the video as an np.arrau, sampling uniformly 8 frames
video_path = hf_hub_download(repo_id="raushan-testing-hf/videos-test", filename="sample_demo_1.mp4", repo_type="dataset")
container = av.open(video_path)
total_frames = container.streams.video[0].frames
indices = np.arange(0, total_frames, total_frames / 8).astype(int)
video = read_video_pyav(container, indices)

# For better results, we recommend to prompt the model in the following format
prompt = "USER: <video>\nWhy is this funny? ASSISTANT:"
inputs = processor(text=prompt, videos=video, return_tensors="pt")

out = model.generate(**inputs, max_new_tokens=60)
processor.batch_decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True)
```

----------------------------------------

TITLE: Loading a Sharded Model
DESCRIPTION: This snippet shows how to load a sharded model using `AutoModel.from_pretrained`.  It loads the model from a directory containing the sharded files and `index.json`. The `from_pretrained` method automatically handles loading the sharded weights.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/big_models.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir, max_shard_size="200MB")
...     new_model = AutoModel.from_pretrained(tmp_dir)
```

----------------------------------------

TITLE: Loading Model with Specific Revision
DESCRIPTION: This snippet demonstrates how to load a model from the Hugging Face Hub with a specific commit hash (`revision`) for security, ensuring that the code is not modified after the model is loaded.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/custom_models.md#_snippet_17

LANGUAGE: Python
CODE:
```
commit_hash = "ed94a7c6247d8aedce4647f00f20de6875b5b292"
model = AutoModelForImageClassification.from_pretrained(
    "sgugger/custom-resnet50d", trust_remote_code=True, revision=commit_hash
)
```

----------------------------------------

TITLE: Depth Estimation Pipeline
DESCRIPTION: Shows how to estimate depth information from a single image using the Hugging Face pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/task_summary.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> depth_estimator = pipeline(task="depth-estimation")
>>> preds = depth_estimator(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
```

----------------------------------------

TITLE: Using M2M100 for Multilingual Translation (Python)
DESCRIPTION: This example demonstrates how to use the M2M100 model for translating Chinese to English. It shows how to set the source language in the tokenizer and force the target language token during generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/multilingual.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
chinese_text = "ä¸è¦æ’æ‰‹å·«å¸«çš„äº‹å‹™, å› ç‚ºä»–å€‘æ˜¯å¾®å¦™çš„, å¾ˆå¿«å°±æœƒç™¼æ€’."

tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M", src_lang="zh")
model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")

encoded_zh = tokenizer(chinese_text, return_tensors="pt")
generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id("en"))
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
```

----------------------------------------

TITLE: GPTQ Model Quantization Setup
DESCRIPTION: Setting up GPTQ configuration and loading/quantizing a model
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/quantization.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
model_id = "facebook/opt-125m"
tokenizer = AutoTokenizer.from_pretrained(model_id)
gptq_config = GPTQConfig(bits=4, dataset = "c4", tokenizer=tokenizer)
```

----------------------------------------

TITLE: Loading Accuracy Metric for Model Evaluation
DESCRIPTION: Imports and initializes the accuracy metric from the Evaluate library, which will be used to assess the performance of the text classification model during and after training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> import evaluate

>>> accuracy = evaluate.load("accuracy")
```

----------------------------------------

TITLE: Dynamic Batching Example (Python)
DESCRIPTION: This code demonstrates a naive implementation of dynamic batching. It attempts to collect multiple requests from the queue within a short timeout before processing them as a single batch with the pipeline.  This example is designed for readability and should be carefully evaluated before use in production.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/pipeline_webserver.md#_snippet_3

LANGUAGE: python
CODE:
```
(string, rq) = await q.get()
strings = []
queues = []
while True:
    try:
        (string, rq) = await asyncio.wait_for(q.get(), timeout=0.001) # 1ms
    except asyncio.exceptions.TimeoutError:
        break
    strings.append(string)
    queues.append(rq)
strings
outs = pipe(strings, batch_size=len(strings))
for rq, out in zip(queues, outs):
    await rq.put(out)
```

----------------------------------------

TITLE: Running Falcon Text Generation via CLI (Bash)
DESCRIPTION: This bash command demonstrates how to use the `transformers chat` command-line interface to interactively generate text with the Falcon 7B Instruct model. It specifies the model ID, sets the torch dtype to auto, uses `flash_attention_2` implementation, and selects device 0. It includes a commented-out line for installing `flash-attn`. Requires `transformers` with CLI installed and optionally `flash-attn`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/falcon.md#_snippet_2

LANGUAGE: bash
CODE:
```
# pip install -U flash-attn --no-build-isolation
transformers chat tiiuae/falcon-7b-instruct --torch_dtype auto --attn_implementation flash_attention_2 --device 0
```

----------------------------------------

TITLE: Installing Flash Attention 2 (Bash)
DESCRIPTION: Provides the bash command necessary to install the Flash Attention 2 library using pip. Flash Attention 2 is a performance optimization that can accelerate model generation on compatible hardware when installed and enabled.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2_5_omni.md#_snippet_10

LANGUAGE: bash
CODE:
```
pip install -U flash-attn --no-build-isolation
```

----------------------------------------

TITLE: Loading and Processing VQA Dataset
DESCRIPTION: Loading the Graphcore/vqa dataset and processing initial examples
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/visual_question_answering.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> dataset = load_dataset("Graphcore/vqa", split="validation[:200]")
>>> dataset
>>> dataset[0]
```

----------------------------------------

TITLE: Using Dedicated SeamlessM4T Speech-to-Speech Model
DESCRIPTION: Shows how to use the dedicated Speech-to-Speech translation model instead of the full SeamlessM4T model. This reduces memory footprint while maintaining the same functionality for speech-to-speech translation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/seamless_m4t.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import SeamlessM4TForSpeechToSpeech
model = SeamlessM4TForSpeechToSpeech.from_pretrained("facebook/hf-seamless-m4t-medium")
```

----------------------------------------

TITLE: Loading and Displaying Image from URL for Classification
DESCRIPTION: This snippet shows how to load an image from a URL using the Pillow library. The image is then opened and ready for classification using the previously set up pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_image_classification.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from PIL import Image
>>> import requests

>>> url = "https://unsplash.com/photos/g8oS8-82DxI/download?ixid=MnwxMjA3fDB8MXx0b3BpY3x8SnBnNktpZGwtSGt8fHx8fDJ8fDE2NzgxMDYwODc&force=true&w=640"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image
```

----------------------------------------

TITLE: Implementing Tensor Parallelism with PyTorch Transformers
DESCRIPTION: Example code showing how to initialize and run a model with tensor parallelism across multiple GPUs. The code demonstrates setting up the distributed environment, loading a model with tensor parallelism enabled, and performing inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/perf_infer_gpu_multi.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"

# åˆå§‹åŒ–åˆ†å¸ƒå¼çŽ¯å¢ƒ
rank = int(os.environ["RANK"])
device = torch.device(f"cuda:{rank}")
torch.cuda.set_device(device)
torch.distributed.init_process_group("nccl", device_id=device)

# èŽ·å–æ”¯æŒå¼ é‡å¹¶è¡Œçš„æ¨¡åž‹
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    tp_plan="auto",
)

# å‡†å¤‡è¾“å…¥tokens
tokenizer = AutoTokenizer.from_pretrained(model_id)
prompt = "Can I help"
inputs = tokenizer(prompt, return_tensors="pt").input_ids.to(device)

# åˆ†å¸ƒå¼è¿è¡Œ
outputs = model(inputs)
```

----------------------------------------

TITLE: TensorFlow Model Setup
DESCRIPTION: Demonstrates setup for fine-tuning using TensorFlow, including model initialization and dataset preparation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/training.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers import TFAutoModelForSequenceClassification
from datasets import load_dataset
from transformers import AutoTokenizer

model = TFAutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=5)
dataset = load_dataset("yelp_review_full")
tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")

def tokenize(examples):
    return tokenizer(examples["text"])

dataset = dataset.map(tokenize)
```

----------------------------------------

TITLE: Initializing Data2VecVision with SDPA Implementation
DESCRIPTION: Demonstrates loading a Data2VecVision model with Scaled Dot Product Attention (SDPA) implementation, optimized for performance using half-precision floating point
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/data2vec.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import Data2VecVisionForImageClassification
model = Data2VecVisionForImageClassification.from_pretrained("facebook/data2vec-vision-base", attn_implementation="sdpa", torch_dtype=torch.float16)
```

----------------------------------------

TITLE: Setting Up TensorFlow Data Augmentation Functions
DESCRIPTION: This code defines two TensorFlow transform functions for image augmentation and layout conversion, preparing the data for model input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/sequence_classification.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> import tensorflow as tf


>>> def aug_transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.image.random_brightness(image, 0.25)
...     image = tf.image.random_contrast(image, 0.5, 2.0)
...     image = tf.image.random_saturation(image, 0.75, 1.25)
...     image = tf.image.random_hue(image, 0.1)
...     image = tf.transpose(image, (2, 0, 1))
...     return image


>>> def transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.transpose(image, (2, 0, 1))
...     return image
```

----------------------------------------

TITLE: Checking NVCC CUDA compiler version
DESCRIPTION: This bash command prints the version of the NVIDIA CUDA compiler (nvcc) installed on the system. This helps verify which CUDA version is detected by the build tools.
SOURCE: https://github.com/huggingface/transformers/blob/main/tests/quantization/bnb/README.md#_snippet_4

LANGUAGE: bash
CODE:
```
nvcc --version
```

----------------------------------------

TITLE: Converting a PyTorch Model Checkpoint to TensorFlow
DESCRIPTION: Code snippet demonstrating how to convert a PyTorch model checkpoint to TensorFlow format using the from_pt parameter when loading the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/model_sharing.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
>>> tf_model = TFDistilBertForSequenceClassification.from_pretrained("path/to/awesome-name-you-picked", from_pt=True)
```

----------------------------------------

TITLE: Releasing Memory Using Accelerate Library
DESCRIPTION: This snippet demonstrates how to use the 'release_memory' utility method from the Accelerate library. This method is device-agnostic and handles memory release across various hardware backends such as XPU, MLU, NPU, and MPS, making it a versatile solution for managing GPU memory in different environments.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from accelerate.utils import release_memory
```

----------------------------------------

TITLE: Loading and Preprocessing Audio Data for ASR in Python
DESCRIPTION: This code loads an audio dataset, casts the audio column to the correct sampling rate, and prepares an audio file for inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/asr.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", "en-US", split="train")
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
>>> sampling_rate = dataset.features["audio"].sampling_rate
>>> audio_file = dataset[0]["audio"]["path"]
```

----------------------------------------

TITLE: Defining a customized SamVisionAttentionSplit class in Python
DESCRIPTION: This code snippet defines a new custom attention class, SamVisionAttentionSplit, which modifies the attention mechanism by separating the q, k, and v projections. It extends the original class and includes initialization logic to handle these changes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/how_to_hack_models.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
import torch.nn as nn
from transformers.models.sam.modeling_sam import SamVisionAttention

class SamVisionAttentionSplit(SamVisionAttention, nn.Module):
    def __init__(self, config, window_size):
        super().__init__(config, window_size)
        # remove combined qkv
        del self.qkv
        # separate q, k, v projections
        self.q = nn.Linear(config.hidden_size, config.hidden_size, bias=config.qkv_bias)
        self.k = nn.Linear(config.hidden_size, config.hidden_size, bias=config.qkv_bias)
        self.v = nn.Linear(config.hidden_size, config.hidden_size, bias=config.qkv_bias)
        self._register_load_state_dict_pre_hook(self.split_q_k_v_load_hook)
```

----------------------------------------

TITLE: Using BARTpho for Masked Language Modeling
DESCRIPTION: Example demonstrating how to use BARTpho for masked language modeling tasks, including token prediction and probability calculation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/bartpho.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
from transformers import MBartForConditionalGeneration

bartpho = MBartForConditionalGeneration.from_pretrained("vinai/bartpho-syllable")
TXT = "ChÃºng tÃ´i lÃ  <mask> nghiÃªn cá»©u viÃªn."
input_ids = tokenizer([TXT], return_tensors="pt")["input_ids"]
logits = bartpho(input_ids).logits
masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()
probs = logits[0, masked_index].softmax(dim=0)
values, predictions = probs.topk(5)
print(tokenizer.decode(predictions).split())
```

----------------------------------------

TITLE: Creating Label Mappings
DESCRIPTION: Creating dictionaries to map between label IDs and names for model training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/audio_classification.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> labels = minds["train"].features["intent_class"].names
>>> label2id, id2label = dict(), dict()
>>> for i, label in enumerate(labels):
...     label2id[label] = str(i)
...     id2label[str(i)] = label
```

----------------------------------------

TITLE: Configuring FP16 Mixed Precision Training
DESCRIPTION: Example showing how to enable mixed precision training using FP16 format to improve computational efficiency.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_train_gpu_one.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
training_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)
```

----------------------------------------

TITLE: Loading Configuration from Local File
DESCRIPTION: This Python code loads a model configuration from a locally saved `config.json` file using the `AutoConfig` class from the `transformers` library. This configuration can be used to instantiate a model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/installation.md#_snippet_23

LANGUAGE: python
CODE:
```
>>> from transformers import AutoConfig

>>> config = AutoConfig.from_pretrained("./your/path/bigscience_t0/config.json")
```

----------------------------------------

TITLE: Compiling a Transformers Model with torch.compile in PyTorch
DESCRIPTION: This snippet demonstrates how to compile a Hugging Face Transformers model using `torch.compile` in PyTorch. It loads a pre-trained causal language model and compiles it for optimized inference. This is the simplest way to use `torch.compile`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_torch_compile.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("google/gemma-2b", device_map="auto")
compiled_model = torch.compile(model)
```

----------------------------------------

TITLE: Verification with nvidia-smi
DESCRIPTION: This bash code executes `nvidia-smi` to display GPU utilization and memory usage information. It is used to cross-validate the memory usage reported by the python script.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/model_memory_anatomy.md#_snippet_6

LANGUAGE: bash
CODE:
```
nvidia-smi
```

----------------------------------------

TITLE: Preprocess Function for Audio (Python)
DESCRIPTION: Defines a preprocessing function to pad or truncate audio sequences to a maximum length. This function uses the feature extractor to handle padding and truncation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/preprocessing.md#_snippet_14

LANGUAGE: python
CODE:
```
>>> def preprocess_function(examples):
...     audio_arrays = [x["array"] for x in examples["audio"]]
...     inputs = feature_extractor(
...         audio_arrays,
...         sampling_rate=16000,
...         padding=True,
...         max_length=100000,
...         truncation=True,
...     )
...     return inputs
```

----------------------------------------

TITLE: Hugging Face Hub Login with Python
DESCRIPTION: Logs into the Hugging Face account using a token to enable model uploading and sharing. Requires the huggingface_hub library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_classification.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Implementing QuantoQuantizedCache for Memory-Efficient Generation in Python
DESCRIPTION: This code shows how to use QuantoQuantizedCache for quantized caching during model generation. It sets up a tokenizer and model, then generates text using quantized cache with specific configurations for the Quanto backend.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/kv_cache.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM, QuantoQuantizedCache, QuantizedCacheConfig

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf", torch_dtype=torch.float16).to("cuda:0")
inputs = tokenizer("I like rock music because", return_tensors="pt").to(model.device)

out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation="quantized", cache_config={"nbits": 4, "axis-key": 0, "axis-value": 0, "backend": "quanto"})
print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])
```

----------------------------------------

TITLE: Testing Summarization Script with Limited Samples
DESCRIPTION: This command runs a summarization script with a limited number of samples for quick testing, using arguments to truncate the dataset size for training, evaluation, and prediction.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/run_scripts.md#2025-04-22_snippet_11

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --max_train_samples 50 \
    --max_eval_samples 50 \
    --max_predict_samples 50 \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Creating PyTorch Dataset Transform Functions
DESCRIPTION: This code defines two transform functions: one for training data with augmentation and one for validation data without augmentation. Both prepare the images and annotations for the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/sequence_classification.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> def train_transforms(example_batch):
...     images = [jitter(x) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs


>>> def val_transforms(example_batch):
...     images = [x for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

----------------------------------------

TITLE: Multi Image Inference with Janus
DESCRIPTION: This code shows how to perform inference with multiple images using the Janus model. It loads a pre-trained model and processor, defines a list of image URLs and messages, and prepares the input with `processor.apply_chat_template`, allowing the model to process multiple images in the same prompt or in batched conversations. The generated text responses are then printed to the console.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/janus.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
from PIL import Image
import requests

from transformers import JanusForConditionalGeneration, JanusProcessor

model_id = "deepseek-community/Janus-Pro-1B"

image_urls = [
    "http://images.cocodataset.org/val2017/000000039769.jpg",
    "https://www.ilankelman.org/stopsigns/australia.jpg",
    "https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg"
]

messages = [
    [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "Whatâ€™s the difference between"},
                {"type": "image", "url": image_urls[0]},
                {"type": "text", "text": " and "},
                {"type": "image", "url": image_urls[1]}
            ]
        }
    ],
    [
        {
            "role": "user",
            "content": [
                {"type": "image", "url": image_urls[2]},
                {"type": "text", "text": "What do you see in this image?"}
            ]
        }
    ]
]

# Load model and processor
processor = JanusProcessor.from_pretrained(model_id)
model = JanusForConditionalGeneration.from_pretrained(
    model_id, torch_dtype=torch.bfloat16, device_map="auto"
)

inputs = processor.apply_chat_template(
    messages,
    add_generation_prompt=True,
    generation_mode="text",
    tokenize=True,
    padding=True,
    return_dict=True,
    return_tensors="pt"
).to(model.device, dtype=torch.bfloat16)

# Generate response
output = model.generate(**inputs, max_new_tokens=40, generation_mode='text', do_sample=False)
text = processor.batch_decode(output, skip_special_tokens=True)
print(text)
```

----------------------------------------

TITLE: Using ExLlamaV2 Kernels for Faster Inference
DESCRIPTION: This code snippet configures the `GPTQConfig` to use the ExLlamaV2 kernels for faster inference with 4-bit GPTQ weights. It loads a pre-trained model from the Hub using the specified quantization configuration and `device_map`. Requires `transformers` library and a GPTQ-quantized model on the hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/gptq.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
"import torch\nfrom transformers import AutoModelForCausalLM, GPTQConfig\n\ngptq_config = GPTQConfig(bits=4, exllama_config={\"version\":2})\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"{your_username}/opt-125m-gptq\",\n    device_map=\"auto\",\n    quantization_config=gptq_config\n)"
```

----------------------------------------

TITLE: Combining Flash Attention 2 with 8-bit Quantization
DESCRIPTION: Demonstrates how to use Flash Attention 2 together with 8-bit model quantization for improved efficiency.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_infer_gpu_one.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    load_in_8bit=True,
    attn_implementation="flash_attention_2",
)
```

----------------------------------------

TITLE: Setting Data Type for Non-Linear Layers in 8-bit Quantization
DESCRIPTION: Code example showing how to specify the data type for non-linear layers like LayerNorm when loading a model in 8-bit precision.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/bitsandbytes.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_8bit=True)

model_8bit = AutoModelForCausalLM.from_pretrained(
    "facebook/opt-350m", 
    quantization_config=quantization_config, 
    torch_dtype=torch.float32
)
model_8bit.model.decoder.layers[-1].final_layer_norm.weight.dtype
```

----------------------------------------

TITLE: Converting MobileViT to TensorFlow Lite
DESCRIPTION: This snippet demonstrates how to convert a pre-trained MobileViT model checkpoint into TensorFlow Lite format, optimizing it for mobile applications. It requires the Hugging Face transformers library and TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mobilevit.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import TFMobileViTForImageClassification
import tensorflow as tf


model_ckpt = "apple/mobilevit-xx-small"
model = TFMobileViTForImageClassification.from_pretrained(model_ckpt)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS,
]
tflite_model = converter.convert()
tflite_filename = model_ckpt.split("/")[-1] + ".tflite"
with open(tflite_filename, "wb") as f:
    f.write(tflite_model)
```

----------------------------------------

TITLE: Quantizing a Transformers Model with Quanto
DESCRIPTION: Python code snippet demonstrating how to quantize a pre-trained model using Quanto configuration within the Transformers library. It shows weight quantization to int8.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/quanto.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig

model_id = "facebook/opt-125m"
tokenizer = AutoTokenizer.from_pretrained(model_id)
quantization_config = QuantoConfig(weights="int8")
quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map="cuda:0", quantization_config=quantization_config)
```

----------------------------------------

TITLE: Processing Multiple Inputs with Pipeline
DESCRIPTION: This code shows how to pass multiple inputs (audio files) to a pipeline for batch processing, allowing efficient processing of multiple files with a single pipeline call.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pipeline_tutorial.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
generator(
    [
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac",
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac",
    ]
)
```

----------------------------------------

TITLE: Compiling Image Classification Model with torch.compile()
DESCRIPTION: Demonstrates how to use torch.compile() to optimize a ViT model for image classification. The code loads a pre-trained model, applies torch.compile(), and performs inference on a sample image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/perf_torch_compile.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from PIL import Image
import requests
import numpy as np
from transformers import AutoImageProcessor, AutoModelForImageClassification

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
model = AutoModelForImageClassification.from_pretrained("google/vit-base-patch16-224").to("cuda")
model = torch.compile(model)

processed_input = processor(image, return_tensors='pt').to(device="cuda")

with torch.no_grad():
    _ = model(**processed_input)
```

----------------------------------------

TITLE: Depth Estimation Pipeline in Python using Hugging Face Transformers
DESCRIPTION: This snippet demonstrates how to use the depth estimation pipeline from Hugging Face Transformers to predict the distance of each pixel in an image from the camera. It uses a pre-trained model to estimate depths from a single image (monocular approach).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/task_summary.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import pipeline

depth_estimator = pipeline(task="depth-estimation")
preds = depth_estimator(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
)
```

----------------------------------------

TITLE: Loading SQuAD Dataset with Hugging Face Datasets
DESCRIPTION: Loads the SQuAD dataset using the Hugging Face Datasets library and displays a sample entry.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/question_answering.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> squad = load_dataset("squad")

>>> squad["train"][0]
```

----------------------------------------

TITLE: Creating a Transformation Function for Image Pixel Values in Python
DESCRIPTION: Defines a function to apply the previously created transformations to image examples, converting them to pixel values for model input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/preprocessing.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
def transforms(examples):
    examples["pixel_values"] = [_transforms(image.convert("RGB")) for image in examples["image"]]
    return examples
```

----------------------------------------

TITLE: Creating TensorFlow Data Preprocessing Functions
DESCRIPTION: Defines preprocessing functions for TensorFlow that apply image transformations and use the image processor to convert images and annotations to model inputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/semantic_segmentation.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> def train_transforms(example_batch):
...     images = [aug_transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs


>>> def val_transforms(example_batch):
...     images = [transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

----------------------------------------

TITLE: Perform Image-Guided Object Detection with OWL-ViT
DESCRIPTION: Performs image-guided object detection using the OWL-ViT model. The preprocessed inputs are passed to the `image_guided_detection` method. The target sizes are converted to a tensor for post-processing. The `post_process_image_guided_detection` method converts the model outputs into bounding box coordinates and confidence scores.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/zero_shot_object_detection.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
>>> with torch.no_grad():
...     outputs = model.image_guided_detection(**inputs)
...     target_sizes = torch.tensor([image_target.size[::-1]])
...     results = processor.post_process_image_guided_detection(outputs=outputs, target_sizes=target_sizes)[0]
```

----------------------------------------

TITLE: Trainer Initialization with Callback
DESCRIPTION: This code snippet shows how to initialize the `Trainer` with a custom callback. The callback is passed to the `callback` argument of the `Trainer`. This allows for extending the training loop with functionality such as early stopping, logging, or other monitoring actions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/trainer.md#_snippet_6

LANGUAGE: python
CODE:
```
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callback=[EarlyStoppingCallback()],
)
```

----------------------------------------

TITLE: Creating TensorFlow Dataset
DESCRIPTION: This code creates a `tf.data.Dataset` from the tokenized data and labels. It sets the batch size based on the number of TPU replicas and shuffles the dataset for better training performance. The `drop_remainder=True` argument ensures that all batches have the same size, which is required for TPU training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
"BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n\ntf_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\ntf_dataset = tf_dataset.shuffle(len(tf_dataset))\ntf_dataset = tf_dataset.batch(BATCH_SIZE, drop_remainder=True)"
```

----------------------------------------

TITLE: Using Accelerate's Memory Release Function
DESCRIPTION: This snippet demonstrates how to use the release_memory() function from the Accelerate library to free up memory used by a model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/llm_tutorial_optimization.md#2025-04-23_snippet_7

LANGUAGE: python
CODE:
```
from accelerate.utils import release_memory

release_memory(model)
```

----------------------------------------

TITLE: Loading and Examining Yelp Review Dataset in Python
DESCRIPTION: This snippet demonstrates how to load the Yelp Review dataset using the Hugging Face datasets library and examine a sample entry.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/training.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
from datasets import load_dataset

dataset = load_dataset("yelp_review_full")
dataset[100]
```

----------------------------------------

TITLE: Creating a Transformation Function for Image Preprocessing
DESCRIPTION: This function combines image data augmentation and preprocessing. It applies the defined image transformations to a batch of images, converts them to RGB, and then uses the `image_processor` to generate `pixel_values`. The `do_resize` parameter is set to `False` because resizing is handled within the `_transforms`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_22

LANGUAGE: Python
CODE:
```
>>> def transforms(examples):
...     images = [_transforms(img.convert("RGB")) for img in examples["image"]]
...     examples["pixel_values"] = image_processor(images, do_resize=False, return_tensors="pt")["pixel_values"]
...     return examples
```

----------------------------------------

TITLE: Image Classification with IDEFICS
DESCRIPTION: Demonstrates how to use IDEFICS for image classification by providing a list of categories. The model classifies an image into one of the specified categories without explicit training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> categories = ['animals','vegetables', 'city landscape', 'cars', 'office']
>>> prompt = [f"Instruction: Classify the following image into a single category from the following list: {categories}.\n",
...     "https://images.unsplash.com/photo-1471193945509-9ad0617afabf?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80",    
...     "Category: "]

>>> inputs = processor(prompt, return_tensors="pt").to("cuda")
>>> bad_words_ids = processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, max_new_tokens=6, bad_words_ids=bad_words_ids)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> print(generated_text[0])
```

----------------------------------------

TITLE: Loading and Saving FSDP Checkpoints
DESCRIPTION: Demonstrate loading checkpoints and saving model states with appropriate FSDP handling
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/fsdp.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
# Load checkpoint
accelerator.load_state("ckpt")

# Save full model state at training end
if trainer.is_fsdp_enabled:
    trainer.accelerator.state.fsdp_plugin.set_state_dict_type("FULL_STATE_DICT")
    trainer.save_model(script_args.output_dir)
```

----------------------------------------

TITLE: Using CLIP with Flash Attention 2
DESCRIPTION: This snippet shows how to load and run a CLIP model using Flash Attention 2, including setting up the device and data type.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/clip.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch
import requests
from PIL import Image

from transformers import CLIPProcessor, CLIPModel

device = "cuda"
torch_dtype = torch.float16

model = CLIPModel.from_pretrained(
    "openai/clip-vit-base-patch32",
    attn_implementation="flash_attention_2",
    device_map=device,
    torch_dtype=torch_dtype,
)
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True)
inputs.to(device)

with torch.no_grad():
    with torch.autocast(device):
        outputs = model(**inputs)

logits_per_image = outputs.logits_per_image  # ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ì„± ì ìˆ˜
probs = logits_per_image.softmax(dim=1)  # í™•ë¥ ì„ ë ˆì´ë¸”ë§ í•˜ê¸°ìœ„í•´ì„œ ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì·¨í•©ë‹ˆë‹¤.
print(probs)
```

----------------------------------------

TITLE: Configuring Image Resolution Limits with Qwen2.5-Omni Processor (Python)
DESCRIPTION: Demonstrates how to load the Qwen2.5-Omni processor while setting minimum and maximum pixel constraints using `min_pixels` and `max_pixels` arguments. This configuration helps manage the performance/resolution trade-off for image inputs. Requires the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2_5_omni.md#_snippet_4

LANGUAGE: python
CODE:
```
min_pixels = 128*28*28
max_pixels = 768*28*28
processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-Omni-7B", min_pixels=min_pixels, max_pixels=max_pixels)
```

----------------------------------------

TITLE: Authenticating with the Hugging Face Hub via CLI
DESCRIPTION: Shows how to login to the Hugging Face Hub using the command-line interface, which stores credentials in the Hugging Face cache folder.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/model_sharing.md#2025-04-23_snippet_1

LANGUAGE: bash
CODE:
```
huggingface-cli login
```

----------------------------------------

TITLE: Creating PushToHub Callback for TensorFlow Translation Model
DESCRIPTION: This code sets up a callback to automatically upload the trained model to the Hugging Face Hub once training is complete.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/translation.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
>>> from transformers.keras_callbacks import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="my_awesome_opus_books_model",
...     tokenizer=tokenizer,
... )
```

----------------------------------------

TITLE: Running IDEFICS Inference in Batch Mode with Python
DESCRIPTION: This code snippet demonstrates how to run IDEFICS inference on a batch of image-text prompts. It processes multiple images with associated text prompts, generates responses, and prints the results.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> prompts = [
...     [   "https://images.unsplash.com/photo-1543349689-9a4d426bee8e?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3501&q=80",
...         "This is an image of ",
...     ],
...     [   "https://images.unsplash.com/photo-1623944889288-cd147dbb517c?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80",
...         "This is an image of ",
...     ],
...     [   "https://images.unsplash.com/photo-1471193945509-9ad0617afabf?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80",
...         "This is an image of ",
...     ],
... ]

>>> inputs = processor(prompts, return_tensors="pt").to("cuda")
>>> bad_words_ids = processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> for i,t in enumerate(generated_text):
...     print(f"{i}:\n{t}\n") 
0:
This is an image of the Eiffel Tower in Paris, France.

1:
This is an image of a couple on a picnic blanket.

2:
This is an image of a vegetable stand.
```

----------------------------------------

TITLE: Dataset Preprocessing with Chat Templates
DESCRIPTION: Demonstrates how to use chat templates for preprocessing training datasets using the Hugging Face datasets library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/chat_templating.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer
from datasets import Dataset

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-beta")

chat1 = [
    {"role": "user", "content": "Which is bigger, the moon or the sun?"},
    {"role": "assistant", "content": "The sun."}
]
chat2 = [
    {"role": "user", "content": "Which is bigger, a virus or a bacterium?"},
    {"role": "assistant", "content": "A bacterium."}
]

dataset = Dataset.from_dict({"chat": [chat1, chat2]})
dataset = dataset.map(lambda x: {"formatted_chat": tokenizer.apply_chat_template(x["chat"], tokenize=False, add_generation_prompt=False)})
print(dataset['formatted_chat'][0])
```

----------------------------------------

TITLE: Saving Full Model State
DESCRIPTION: Code for saving the complete model state dictionary after FSDP training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/fsdp.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
if trainer.is_fsdp_enabled:
    trainer.accelerator.state.fsdp_plugin.set_state_dict_type("FULL_STATE_DICT")

trainer.save_model(script_args.output_dir)
```

----------------------------------------

TITLE: Performing Infilling with CodeLlama Python
DESCRIPTION: This snippet demonstrates how to use the `CodeLlamaTokenizer` and `LlamaForCausalLM` from the Hugging Face `transformers` library to perform code infilling using the `<FILL_ME>` token. It loads a pre-trained CodeLlama model, tokenizes a prompt containing the placeholder, generates completion tokens, decodes the generated tokens, and prints the filled result.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/code_llama.md#_snippet_5

LANGUAGE: Python
CODE:
```
from transformers import LlamaForCausalLM, CodeLlamaTokenizer

tokenizer = CodeLlamaTokenizer.from_pretrained("meta-llama/CodeLlama-7b-hf")
model = LlamaForCausalLM.from_pretrained("meta-llama/CodeLlama-7b-hf")
PROMPT = '''def remove_non_ascii(s: str) -> str:
    """ <FILL_ME>
    return result
'''
input_ids = tokenizer(PROMPT, return_tensors="pt")["input_ids"]
generated_ids = model.generate(input_ids, max_new_tokens=128)

filling = tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens = True)[0]
print(PROMPT.replace("<FILL_ME>", filling))
```

----------------------------------------

TITLE: Performing Inference with Fine-tuned Causal Language Model
DESCRIPTION: This snippet demonstrates how to use the fine-tuned causal language model for inference. It includes examples of using the model with a pipeline and directly for text generation in both PyTorch and TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/language_modeling.md#2025-04-23_snippet_9

LANGUAGE: python
CODE:
```
>>> prompt = "Somatic hypermutation allows the immune system to"

>>> from transformers import pipeline

>>> generator = pipeline("text-generation", model="my_awesome_eli5_clm-model")
>>> generator(prompt)
[{'generated_text': "Somatic hypermutation allows the immune system to be able to effectively reverse the damage caused by an infection.\n\n\nThe damage caused by an infection is caused by the immune system's ability to perform its own self-correcting tasks."}]

>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_eli5_clm-model")
>>> inputs = tokenizer(prompt, return_tensors="pt").input_ids

>>> from transformers import AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained("my_awesome_eli5_clm-model")
>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)

>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
["Somatic hypermutation allows the immune system to react to drugs with the ability to adapt to a different environmental situation. In other words, a system of 'hypermutation' can help the immune system to adapt to a different environmental situation or in some cases even a single life. In contrast, researchers at the University of Massachusetts-Boston have found that 'hypermutation' is much stronger in mice than in humans but can be found in humans, and that it's not completely unknown to the immune system. A study on how the immune system"]

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_eli5_clm-model")
>>> inputs = tokenizer(prompt, return_tensors="tf").input_ids

>>> from transformers import TFAutoModelForCausalLM

>>> model = TFAutoModelForCausalLM.from_pretrained("my_awesome_eli5_clm-model")
>>> outputs = model.generate(input_ids=inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)

>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Somatic hypermutation allows the immune system to detect the presence of other viruses as they become more prevalent. Therefore, researchers have identified a high proportion of human viruses. The proportion of virus-associated viruses in our study increases with age. Therefore, we propose a simple algorithm to detect the presence of these new viruses in our samples as a sign of improved immunity. A first study based on this algorithm, which will be published in Science on Friday, aims to show that this finding could translate into the development of a better vaccine that is more effective for']
```

----------------------------------------

TITLE: Using Mistral Instruct Model with Chat Template in Python
DESCRIPTION: Shows how to use the Mistral-7B-Instruct model with a chat template for generating responses in a conversational context.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/mistral.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2", device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")

>>> messages = [
...     {"role": "user", "content": "What is your favourite condiment?"},
...     {"role": "assistant", "content": "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!"},
...     {"role": "user", "content": "Do you have mayonnaise recipes?"}
... ]

>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")

>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"Mayonnaise can be made as follows: (...)"
```

----------------------------------------

TITLE: Enabling Sampling for Creative Text Generation
DESCRIPTION: Demonstrates how to enable sampling during text generation for more creative outputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/llm_tutorial.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers import set_seed
set_seed(42)

model_inputs = tokenizer(["I am a cat."], return_tensors="pt").to("cuda")

# LLM + greedy decoding = repetitive, boring output
generated_ids = model.generate(**model_inputs)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

# With sampling, the output becomes more creative!
generated_ids = model.generate(**model_inputs, do_sample=True)
tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

----------------------------------------

TITLE: Loading the MInDS-14 Dataset
DESCRIPTION: Loads the MInDS-14 dataset from the ðŸ¤— Datasets library using the `load_dataset` function. The dataset is loaded with the "en-US" configuration and the "train" split.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/audio_classification.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
">>> from datasets import load_dataset, Audio\n\n>>> minds = load_dataset("PolyAI/minds14", name="en-US", split="train")"
```

----------------------------------------

TITLE: Initializing SpeechEncoderDecoderModel from Pretrained Models in Python
DESCRIPTION: This snippet shows how to initialize a SpeechEncoderDecoderModel using pretrained checkpoints for both encoder and decoder components. It uses Hubert as the encoder and BERT as the decoder, demonstrating the from_encoder_decoder_pretrained method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/speech-encoder-decoder.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import SpeechEncoderDecoderModel

>>> model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(
...     "facebook/hubert-large-ll60k", "google-bert/bert-base-uncased"
... )
```

----------------------------------------

TITLE: Generating MIDI Files from Custom Audio with Pop2Piano in Python
DESCRIPTION: This Python code demonstrates how to use Pop2Piano to generate MIDI files from a custom audio file. It involves loading an audio file using librosa, processing it with a pretrained model and processor, and saving the generated MIDI output. Ensure the sampling rate is set appropriately for optimal performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/pop2piano.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
import librosa
from transformers import Pop2PianoForConditionalGeneration, Pop2PianoProcessor

audio, sr = librosa.load("<your_audio_file_here>", sr=44100)  # feel free to change the sr to a suitable value.
model = Pop2PianoForConditionalGeneration.from_pretrained("sweetcocoa/pop2piano")
processor = Pop2PianoProcessor.from_pretrained("sweetcocoa/pop2piano")

inputs = processor(audio=audio, sampling_rate=sr, return_tensors="pt")
model_output = model.generate(input_features=inputs["input_features"], composer="composer1")
tokenizer_output = processor.batch_decode(
    token_ids=model_output, feature_extractor_output=inputs
)["pretty_midi_objects"][0]
tokenizer_output.write("./Outputs/midi_output.mid")
```

----------------------------------------

TITLE: Decoding Multiple Audios with Wav2Vec2ProcessorWithLM in Python
DESCRIPTION: This code snippet demonstrates how to efficiently decode multiple batches of audio data using a user-managed multiprocessing pool with the Wav2Vec2ProcessorWithLM class. It initializes the model and feature extractor, prepares the speech data, and performs batch inference while managing the decoding process to enhance performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/wav2vec2.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> # Let's see how to use a user-managed pool for batch decoding multiple audios
>>> from multiprocessing import get_context
>>> from transformers import AutoTokenizer, AutoProcessor, AutoModelForCTC
>>> from datasets import load_dataset
>>> import datasets
>>> import torch

>>> # import model, feature extractor, tokenizer
>>> model = AutoModelForCTC.from_pretrained("patrickvonplaten/wav2vec2-base-100h-with-lm").to("cuda")
>>> processor = AutoProcessor.from_pretrained("patrickvonplaten/wav2vec2-base-100h-with-lm")

>>> # load example dataset
>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> dataset = dataset.cast_column("audio", datasets.Audio(sampling_rate=16_000))

>>> def map_to_array(batch):
...     batch["speech"] = batch["audio"]["array"]
...     return batch

>>> # prepare speech data for batch inference
>>> dataset = dataset.map(map_to_array, remove_columns=["audio"])

>>> def map_to_pred(batch, pool):
...     inputs = processor(batch["speech"], sampling_rate=16_000, padding=True, return_tensors="pt")
...     inputs = {k: v.to("cuda") for k, v in inputs.items()}

...     with torch.no_grad():
...         logits = model(**inputs).logits

...     transcription = processor.batch_decode(logits.cpu().numpy(), pool).text
...     batch["transcription"] = transcription
...     return batch

>>> # note: pool should be instantiated *after* `Wav2Vec2ProcessorWithLM`.
>>> #       otherwise, the LM won't be available to the pool's sub-processes
>>> # select number of processes and batch_size based on number of CPU cores available and on dataset size
>>> with get_context("fork").Pool(processes=2) as pool:
...     result = dataset.map(
...         map_to_pred, batched=True, batch_size=2, fn_kwargs={"pool": pool}, remove_columns=["speech"]
...     )
>>> result["transcription"][0:2]
['MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL', "NOR IS MISTER COULTER'S MANNER LESS INTERESTING THAN HIS MATTER"]
```

----------------------------------------

TITLE: Launching Training Script with Accelerate and Parameters in Bash
DESCRIPTION: This bash command shows how to launch a training script with Accelerate, specifying parameters directly in the command line. Parameters include the number of processes, FSDP usage, mixed precision, and FSDP configurations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/trainer.md#_snippet_27

LANGUAGE: bash
CODE:
```
accelerate launch --num_processes=2 \
    --use_fsdp \
    --mixed_precision=bf16 \
    --fsdp_auto_wrap_policy=TRANSFORMER_BASED_WRAP  \
    --fsdp_transformer_layer_cls_to_wrap="BertLayer" \
    --fsdp_sharding_strategy=1 \
    --fsdp_state_dict_type=FULL_STATE_DICT \
    ./examples/pytorch/text-classification/run_glue.py
    --model_name_or_path google-bert/bert-base-cased \
    --task_name $TASK_NAME \
    --do_train \
    --do_eval \
    --max_seq_length 128 \
    --per_device_train_batch_size 16 \
    --learning_rate 5e-5 \
    --num_train_epochs 3 \
    --output_dir /tmp/$TASK_NAME/ \
    --overwrite_output_dir
```

----------------------------------------

TITLE: Manual Inference with LayoutLMv2 for Document QA in Python
DESCRIPTION: This snippet shows how to manually perform inference using a fine-tuned LayoutLMv2 model for document question answering. It includes preprocessing the input, forward pass through the model, and decoding the output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/document_question_answering.md#2025-04-22_snippet_25

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoProcessor
from transformers import AutoModelForDocumentQuestionAnswering

processor = AutoProcessor.from_pretrained("MariaK/layoutlmv2-base-uncased_finetuned_docvqa")
model = AutoModelForDocumentQuestionAnswering.from_pretrained("MariaK/layoutlmv2-base-uncased_finetuned_docvqa")

with torch.no_grad():
    encoding = processor(image.convert("RGB"), question, return_tensors="pt")
    outputs = model(**encoding)
    start_logits = outputs.start_logits
    end_logits = outputs.end_logits
    predicted_start_idx = start_logits.argmax(-1).item()
    predicted_end_idx = end_logits.argmax(-1).item()

processor.tokenizer.decode(encoding.input_ids.squeeze()[predicted_start_idx : predicted_end_idx + 1])
```

----------------------------------------

TITLE: Loading Model with Custom Config (PyTorch)
DESCRIPTION: This snippet loads a DistilBertModel using a custom configuration. It first loads the configuration from a JSON file and then uses it to instantiate the `DistilBertModel`. The `DistilBertModel` class from the `transformers` library is required.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/create_a_model.md#_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import DistilBertModel

>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/config.json")
>>> model = DistilBertModel(my_config)
```

----------------------------------------

TITLE: Loading and Processing Image for Depth Estimation
DESCRIPTION: Shows how to load an image from a URL using the PIL library and requests, preparing it for depth estimation analysis.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/monocular_depth_estimation.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
>>> from PIL import Image
>>> import requests

>>> url = "https://unsplash.com/photos/HwBAsSbPBDU/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MzR8fGNhciUyMGluJTIwdGhlJTIwc3RyZWV0fGVufDB8MHx8fDE2Nzg5MDEwODg&force=true&w=640"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> image
```

----------------------------------------

TITLE: Processing Audio and Text Inputs for SeamlessM4T-v2
DESCRIPTION: This snippet shows how to load an audio sample from a dataset and process both audio and text inputs using the SeamlessM4T-v2 processor. The processor prepares the inputs in the format required by the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/seamless_m4t_v2.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
# let's load an audio sample from an Arabic speech corpus
from datasets import load_dataset
dataset = load_dataset("arabic_speech_corpus", split="test", streaming=True, trust_remote_code=True)
audio_sample = next(iter(dataset))["audio"]

# now, process it
audio_inputs = processor(audios=audio_sample["array"], return_tensors="pt")

# now, process some English text as well
text_inputs = processor(text = "Hello, my dog is cute", src_lang="eng", return_tensors="pt")
```

----------------------------------------

TITLE: Setting Up Data Collator for TensorFlow
DESCRIPTION: Creates a DefaultDataCollator for batching samples in TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/image_classification.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors="tf")
```

----------------------------------------

TITLE: Enabling Better Transformer for PyTorch Native Attention Fastpath
DESCRIPTION: Convert a Transformers model to use Better Transformer for improved inference speed using PyTorch's attention fastpath.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/perf_infer_gpu_one.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
model = model.to_bettertransformer()
```

----------------------------------------

TITLE: Pushing Model to Hugging Face Hub
DESCRIPTION: This snippet demonstrates how to push the custom ResNet model to the Hugging Face Hub using the `push_to_hub` method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/custom_models.md#_snippet_15

LANGUAGE: Python
CODE:
```
resnet50d.push_to_hub("custom-resnet50d")
```

----------------------------------------

TITLE: M2M100 Translation Setup and Inference
DESCRIPTION: Shows how to use M2M100 model for Chinese to English translation by setting source language in tokenizer and forcing target language token generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/multilingual.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
chinese_text = "ä¸è¦æ’æ‰‹å·«å¸«çš„äº‹å‹™, å› ç‚ºä»–å€‘æ˜¯å¾®å¦™çš„, å¾ˆå¿«å°±æœƒç™¼æ€’."

tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M", src_lang="zh")
model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")

encoded_zh = tokenizer(chinese_text, return_tensors="pt")
generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id("en"))
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
```

----------------------------------------

TITLE: Summarizing Text with a LLM in Python
DESCRIPTION: This snippet demonstrates the summarization of a given text using a prompt that guides the model to create a concise summary based on the provided information.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/prompting.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> torch.manual_seed(3) # doctest: +IGNORE_RESULT
>>> prompt = """Permaculture is a design process mimicking the diversity, functionality and resilience of natural ecosystems. The principles and practices are drawn from traditional ecological knowledge of indigenous cultures combined with modern scientific understanding and technological innovations. Permaculture design provides a framework helping individuals and communities develop innovative, creative and effective strategies for meeting basic needs while preparing for and mitigating the projected impacts of climate change.\
... Write a summary of the above text.\
... Summary:\
... """

>>> sequences = pipe(
...     prompt,
...     max_new_tokens=30,
...     do_sample=True,
...     top_k=10,
...     return_full_text = False,
... )

>>> for seq in sequences:
...     print(f"{seq['generated_text']}")
Permaculture is an ecological design mimicking natural ecosystems to meet basic needs and prepare for climate change. It is based on traditional knowledge and scientific understanding.
```

----------------------------------------

TITLE: Saving Configuration
DESCRIPTION: Shows how to save a custom configuration to a local directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/create_a_model.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
my_config.save_pretrained(save_directory="./your_model_save_path")
```

----------------------------------------

TITLE: Configuring TrainingArguments for Model Sharing in Python
DESCRIPTION: Shows how to set up TrainingArguments to enable model sharing to the Hugging Face Hub during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/model_sharing.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> training_args = TrainingArguments(output_dir="my-awesome-model", push_to_hub=True)
```

----------------------------------------

TITLE: Preparing for Acceleration
DESCRIPTION: This snippet prepares the training and evaluation dataloaders, the model, and the optimizer for distributed training using the `accelerator.prepare()` method. This ensures that these objects are properly configured for the distributed environment.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/accelerate.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
...     train_dataloader, eval_dataloader, model, optimizer
... )
```

----------------------------------------

TITLE: Implementing TTS Data Collator with Padding in Python
DESCRIPTION: Creates a custom data collator that handles padding of input sequences and spectrograms, masks padded portions from loss calculation, and adds speaker embeddings to the batch. This enables batched training of multi-speaker TTS models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/text-to-speech.md#2025-04-22_snippet_21

LANGUAGE: python
CODE:
```
>>> from dataclasses import dataclass
>>> from typing import Any, Dict, List, Union


>>> @dataclass
... class TTSDataCollatorWithPadding:
...     processor: Any

...     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
...         input_ids = [{"input_ids": feature["input_ids"]} for feature in features]
...         label_features = [{"input_values": feature["labels"]} for feature in features]
...         speaker_features = [feature["speaker_embeddings"] for feature in features]

...         # collate the inputs and targets into a batch
...         batch = processor.pad(input_ids=input_ids, labels=label_features, return_tensors="pt")

...         # replace padding with -100 to ignore loss correctly
...         batch["labels"] = batch["labels"].masked_fill(batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100)

...         # not used during fine-tuning
...         del batch["decoder_attention_mask"]

...         # round down target lengths to multiple of reduction factor
...         if model.config.reduction_factor > 1:
...             target_lengths = torch.tensor([len(feature["input_values"]) for feature in label_features])
...             target_lengths = target_lengths.new(
...                 [length - length % model.config.reduction_factor for length in target_lengths]
...             )
...             max_length = max(target_lengths)
...             batch["labels"] = batch["labels"][:, :max_length]

...         # also add in the speaker embeddings
...         batch["speaker_embeddings"] = torch.tensor(speaker_features)

...         return batch
```

----------------------------------------

TITLE: Pushing the Trained Model to Hub
DESCRIPTION: This pushes the trained model to the Hugging Face Hub, making it available for others to use.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/multiple_choice.md#_snippet_13

LANGUAGE: python
CODE:
```
>>> trainer.push_to_hub()
```

----------------------------------------

TITLE: Installing ðŸ¤— Accelerate Library
DESCRIPTION: This command installs the latest development version of ðŸ¤— Accelerate from GitHub, which is required to run the accelerated scripts.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/run_scripts.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
pip install git+https://github.com/huggingface/accelerate
```

----------------------------------------

TITLE: Chatting with FalconMamba using Transformers CLI (Bash)
DESCRIPTION: This snippet shows how to initiate an interactive chat session with the FalconMamba 7B instruct model directly from the command line using the `transformers chat` command. It specifies the model identifier, desired torch dtype, and the device to use for inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/falcon_mamba.md#_snippet_2

LANGUAGE: Bash
CODE:
```
transformers chat tiiuae/falcon-mamba-7b-instruct --torch_dtype auto --device 0
```

----------------------------------------

TITLE: Starting Chat via Transformers CLI (Shell)
DESCRIPTION: Shows a command-line interface command provided by the Transformers library to quickly start a chat session with a specified language model directly from the terminal.
SOURCE: https://github.com/huggingface/transformers/blob/main/README.md#_snippet_5

LANGUAGE: Shell
CODE:
```
transformers chat --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct
```

----------------------------------------

TITLE: Configuring torch.compile in TrainingArguments
DESCRIPTION: This Python snippet shows how to enable torch.compile optimization in the TrainingArguments class. It sets the compile flag to True and specifies the backend and compile mode.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/trainer.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments

training_args = TrainingArguments(
    torch.compile=True,
    torch.compile_backend="inductor",
    torch_compile_mode="default",
    ...,
)
```

----------------------------------------

TITLE: Configuring 4-bit Quantization with BFloat16 Compute in Python
DESCRIPTION: This code sets up 4-bit quantization with BFloat16 as the compute data type using BitsAndBytesConfig. It's used to speed up computation in 4-bit quantized models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-23_snippet_14

LANGUAGE: python
CODE:
```
import torch
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)
```

----------------------------------------

TITLE: Evaluating Model Performance
DESCRIPTION: Shows how to evaluate the model on a test dataset and print detailed performance metrics including mAP scores.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/object_detection.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
from pprint import pprint

metrics = trainer.evaluate(eval_dataset=cppe5["test"], metric_key_prefix="test")
pprint(metrics)
```

----------------------------------------

TITLE: Inspecting pad_token_id for a Transformers Model in Python
DESCRIPTION: This snippet demonstrates how to load a pretrained sequence classification model and access the `pad_token_id` attribute from its configuration. The value of this attribute may need manual configuration if it is `None` by default.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/troubleshooting.md#_snippet_5

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification
>>> import torch

>>> model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-uncased")
>>> model.config.pad_token_id
```

----------------------------------------

TITLE: Training GPT-2 Model with NVLink Enabled
DESCRIPTION: This snippet demonstrates how to train a GPT-2 language model using distributed data parallel (DDP) training with NVLink enabled for faster inter-GPU communication.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/perf_hardware.md#2025-04-23_snippet_1

LANGUAGE: bash
CODE:
```
rm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 torchrun \
--nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py --model_name_or_path openai-community/gpt2 \
--dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --do_train \
--output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200
```

----------------------------------------

TITLE: Loading ALBERT Model with SDPA in PyTorch
DESCRIPTION: This snippet demonstrates how to load an ALBERT model using the Transformers library, specifying the use of Scaled Dot Product Attention (SDPA) and half-precision floating point format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/albert.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import AlbertModel
model = AlbertModel.from_pretrained("albert/albert-base-v1", torch_dtype=torch.float16, attn_implementation="sdpa")
```

----------------------------------------

TITLE: Using Document Question Answering Pipeline for Multimodal Tasks in Python
DESCRIPTION: This code demonstrates document question answering, a multimodal task that extracts answers from document images. It loads an image of a document, initializes a document-question-answering pipeline with a specialized model, and queries for specific information from the document.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/task_summary.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline
>>> from PIL import Image
>>> import requests

>>> url = "https://huggingface.co/datasets/hf-internal-testing/example-documents/resolve/main/jpeg_images/2.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> doc_question_answerer = pipeline("document-question-answering", model="magorshunov/layoutlm-invoices")
>>> preds = doc_question_answerer(
...     question="What is the total amount?",
...     image=image,
... )
>>> preds
[{'score': 0.8531, 'answer': '17,000', 'start': 4, 'end': 4}]
```

----------------------------------------

TITLE: Initializing VideoMAE Model with SDPA
DESCRIPTION: Example showing how to load a pre-trained VideoMAE model with Scaled Dot Product Attention (SDPA) implementation and half-precision floating point format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/videomae.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import VideoMAEForVideoClassification
model = VideoMAEForVideoClassification.from_pretrained("MCG-NJU/videomae-base-finetuned-kinetics", attn_implementation="sdpa", torch_dtype=torch.float16)
...
```

----------------------------------------

TITLE: Creating VisionEncoderDecoderModel from Pretrained Models
DESCRIPTION: Shows how to initialize a VisionEncoderDecoderModel using pretrained Swin transformer encoder and BERT decoder checkpoints.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/vision-encoder-decoder.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import VisionEncoderDecoderModel

model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(
    "microsoft/swin-base-patch4-window7-224-in22k", "google-bert/bert-base-uncased"
)
```

----------------------------------------

TITLE: Preprocessing Function for SWAG Dataset in Python
DESCRIPTION: Defines a function to preprocess the SWAG dataset by combining sentence beginnings with possible endings and tokenizing the results.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/multiple_choice.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> ending_names = ["ending0", "ending1", "ending2", "ending3"]


>>> def preprocess_function(examples):
...     first_sentences = [[context] * 4 for context in examples["sent1"]]
...     question_headers = examples["sent2"]
...     second_sentences = [
...         [f"{header} {examples[end][i]}" for end in ending_names] for i, header in enumerate(question_headers)
...     ]

...     first_sentences = sum(first_sentences, [])
...     second_sentences = sum(second_sentences, [])

...     tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)
...     return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}
```

----------------------------------------

TITLE: Setting Compute Data Type for 4-bit Models
DESCRIPTION: Example showing how to configure the compute data type for 4-bit models to use bfloat16 instead of the default float32 for faster computation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/bitsandbytes.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
import torch
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)
```

----------------------------------------

TITLE: Initializing Large Models with ZeRO-3
DESCRIPTION: Example of initializing large models using DeepSpeed ZeRO-3 context manager
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/deepspeed.md#2025-04-22_snippet_36

LANGUAGE: python
CODE:
```
from transformers import T5ForConditionalGeneration, T5Config
import deepspeed

with deepspeed.zero.Init():
    config = T5Config.from_pretrained("google-t5/t5-small")
    model = T5ForConditionalGeneration(config)
```

----------------------------------------

TITLE: Distributed Training with Accelerate
DESCRIPTION: This bash script shows how to use the Accelerate library to run distributed training for a GLUE task without the Trainer API. It demonstrates the setup and launch of a distributed training job using Accelerate.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/README.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
export TASK_NAME=mrpc

accelerate launch run_glue_no_trainer.py \
  --model_name_or_path google-bert/bert-base-cased \
  --task_name $TASK_NAME \
  --max_length 128 \
  --per_device_train_batch_size 32 \
  --learning_rate 2e-5 \
  --num_train_epochs 3 \
  --output_dir /tmp/$TASK_NAME/
```

----------------------------------------

TITLE: Training SpeechEncoderDecoderModel
DESCRIPTION: This code demonstrates how to fine-tune a SpeechEncoderDecoderModel using the transformers library. It loads a pre-trained encoder and decoder, combines them into a Seq2Seq model, loads a dataset of (speech, text) pairs, preprocesses the data, and computes the loss.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/speech-encoder-decoder.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, AutoFeatureExtractor, SpeechEncoderDecoderModel
>>> from datasets import load_dataset

>>> encoder_id = "facebook/wav2vec2-base-960h"  # acoustic model encoder
>>> decoder_id = "google-bert/bert-base-uncased"  # text decoder

>>> feature_extractor = AutoFeatureExtractor.from_pretrained(encoder_id)
>>> tokenizer = AutoTokenizer.from_pretrained(decoder_id)
>>> # Combine pre-trained encoder and pre-trained decoder to form a Seq2Seq model
>>> model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id)

>>> model.config.decoder_start_token_id = tokenizer.cls_token_id
>>> model.config.pad_token_id = tokenizer.pad_token_id

>>> # load an audio input and pre-process (normalise mean/std to 0/1)
>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> input_values = feature_extractor(ds[0]["audio"]["array"], return_tensors="pt").input_values

>>> # load its corresponding transcription and tokenize to generate labels
>>> labels = tokenizer(ds[0]["text"], return_tensors="pt").input_ids

>>> # the forward function automatically creates the correct decoder_input_ids
>>> loss = model(input_values=input_values, labels=labels).loss
>>> loss.backward()
```

----------------------------------------

TITLE: Generating Tool Call with Chat Template
DESCRIPTION: Using apply_chat_template to generate tool calls and model inputs for tool-enabled generation
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_extras.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
inputs = tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors="pt")
inputs = {k: v for k, v in inputs.items()}
outputs = model.generate(**inputs, max_new_tokens=128)
print(tokenizer.decode(outputs[0][len(inputs["input_ids"][0]):]))
```

----------------------------------------

TITLE: Inspecting Dataset Example with Python
DESCRIPTION: Displays an example from the dataset, consisting of an image and its label. Each example is composed of a PIL image and an integer label.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_classification.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> food["train"][0]
{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at 0x7F52AFC8AC50>,
 'label': 79}
```

----------------------------------------

TITLE: Enabling TF32 Precision
DESCRIPTION: Code to enable TF32 precision mode for improved training performance on Ampere hardware, including both PyTorch backend configuration and Trainer setup.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_train_gpu_one.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import torch
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
```

LANGUAGE: python
CODE:
```
TrainingArguments(tf32=True, **default_args)
```

----------------------------------------

TITLE: Running Masked Language Model Training with Custom Dataset
DESCRIPTION: Command for training a masked language model with a custom training dataset file. This approach allows for domain-specific fine-tuning of the DistilBERT model on custom text data.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/tensorflow/language-modeling/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
python run_mlm.py \
--model_name_or_path distilbert/distilbert-base-cased \
--output_dir output \
--train_file train_file_path
```

----------------------------------------

TITLE: Setting Training Arguments for PyTorch Model
DESCRIPTION: This snippet outlines how to define training hyperparameters using the TrainingArguments class for the Trainer in PyTorch. It specifies output directory, learning rate, and other training metrics necessary for the training process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
>>> training_args = TrainingArguments(
...     output_dir="segformer-b0-scene-parse-150",
...     learning_rate=6e-5,
...     num_train_epochs=50,
...     per_device_train_batch_size=2,
...     per_device_eval_batch_size=2,
...     save_total_limit=3,
...     eval_strategy="steps",
...     save_strategy="steps",
...     save_steps=20,
...     eval_steps=20,
...     logging_steps=1,
...     eval_accumulation_steps=5,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )
```

----------------------------------------

TITLE: Loading DePlot Model and Processing Image - Python
DESCRIPTION: This snippet demonstrates how to load the DePlot model using the Hugging Face Transformers library, process an image from a URL, and generate a data table based on the visual input using a specific prompt.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/deplot.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoProcessor, Pix2StructForConditionalGeneration
import requests
from PIL import Image

model = Pix2StructForConditionalGeneration.from_pretrained("google/deplot")
processor = AutoProcessor.from_pretrained("google/deplot")
url = "https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, text="Generate underlying data table of the figure below:", return_tensors="pt")
predictions = model.generate(**inputs, max_new_tokens=512)
print(processor.decode(predictions[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Loading Custom Model and Tokenizer for Sentiment Analysis in Python (PyTorch)
DESCRIPTION: Loads a custom pretrained model and tokenizer for sentiment analysis using PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/quicktour.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

----------------------------------------

TITLE: Performing Object Detection
DESCRIPTION: This snippet uses the zero-shot detection pipeline to predict objects in the provided image. It passes an image along with candidate labels to the detector and captures the predictions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> predictions = detector(
...     image,
...     candidate_labels=["human face", "rocket", "nasa badge", "star-spangled banner"],
... )
>>> predictions
```

----------------------------------------

TITLE: Exporting BERT Model to TFLite Format
DESCRIPTION: Command to export a pre-trained BERT model from Hugging Face Hub to TFLite format. This specifies the model name, sequence length parameter, and output directory for the exported model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tflite.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
optimum-cli export tflite --model google-bert/bert-base-uncased --sequence_length 128 bert_tflite/
```

----------------------------------------

TITLE: DeepSpeed ZeRO-2 Configuration Example
DESCRIPTION: JSON configuration for DeepSpeed ZeRO stage 2 optimization with CPU offloading. This configuration enables partitioning and offloading of optimizer states while keeping model parameters in GPU memory for a balance of memory efficiency and performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/deepspeed.md#2025-04-22_snippet_20

LANGUAGE: json
CODE:
```
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": "auto",
            "betas": "auto",
            "eps": "auto",
            "weight_decay": "auto"
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },

    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "allgather_partitions": true,
        "allgather_bucket_size": 2e8,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2e8,
        "contiguous_gradients": true
    },

    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
}
```

----------------------------------------

TITLE: Exporting Model with optimum.onnxruntime Module
DESCRIPTION: Programmatically exports a Transformers model to ONNX using the optimum.onnxruntime module. The process involves instantiating an ORTModelForSequenceClassification with export enabled, followed by saving the model and tokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/serialization.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from optimum.onnxruntime import ORTModelForSequenceClassification
>>> from transformers import AutoTokenizer

>>> model_checkpoint = "distilbert/distilbert-base-uncased-distilled-squad"
>>> save_directory = "onnx/"

>>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)
>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

>>> ort_model.save_pretrained(save_directory)
>>> tokenizer.save_pretrained(save_directory)
```

----------------------------------------

TITLE: Training the Student Model with Knowledge Distillation
DESCRIPTION: Executes the training process where the student model (MobileNetV2) learns to mimic the outputs of the teacher model (ViT) while also learning from the ground truth labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/knowledge_distillation_for_image_classification.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
trainer.train()
```

----------------------------------------

TITLE: Creating Data Collator for TensorFlow
DESCRIPTION: Initializes a DefaultDataCollator for TensorFlow to handle batching of preprocessed examples during training, specifying TensorFlow tensors as the output format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/question_answering.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors="tf")
```

----------------------------------------

TITLE: Running TTS Model with VITS - Python
DESCRIPTION: This snippet demonstrates how to initialize and run a forward pass through a VITS model for text-to-speech synthesis.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mms.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
import torch
from transformers import VitsTokenizer, VitsModel, set_seed

tokenizer = VitsTokenizer.from_pretrained("facebook/mms-tts-eng")
model = VitsModel.from_pretrained("facebook/mms-tts-eng")

inputs = tokenizer(text="Hello - my dog is cute", return_tensors="pt")

set_seed(555)  # make deterministic

with torch.no_grad():
   outputs = model(**inputs)

waveform = outputs.waveform[0]
```

----------------------------------------

TITLE: Initializing ConvNextFeatureExtractor in Python
DESCRIPTION: Feature extractor class for ConvNeXT model. Prepares images for input to the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/convnext.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
[[autodoc]] ConvNextFeatureExtractor
```

----------------------------------------

TITLE: Splitting BillSum Dataset into Train/Test Sets
DESCRIPTION: Creates train and test splits from the loaded dataset using an 80/20 split ratio
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> billsum = billsum.train_test_split(test_size=0.2)
```

----------------------------------------

TITLE: Image Captioning with VisionEncoderDecoderModel
DESCRIPTION: Demonstrates how to perform image captioning using a fine-tuned VisionEncoderDecoderModel with ViT encoder and GPT2 decoder.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/vision-encoder-decoder.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import requests
from PIL import Image

from transformers import GPT2TokenizerFast, ViTImageProcessor, VisionEncoderDecoderModel

# load a fine-tuned image captioning model and corresponding tokenizer and image processor
model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
tokenizer = GPT2TokenizerFast.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
image_processor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")

# let's perform inference on an image
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
pixel_values = image_processor(image, return_tensors="pt").pixel_values

# autoregressively generate caption (uses greedy decoding by default)
generated_ids = model.generate(pixel_values)
generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(generated_text)
```

----------------------------------------

TITLE: Creating Depth Estimation Pipeline in Python
DESCRIPTION: Demonstrates how to create a depth estimation pipeline using the Transformers library. It loads a pre-trained model from the Hugging Face Hub and prepares it for inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/monocular_depth_estimation.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> checkpoint = "vinvino02/glpn-nyu"
>>> depth_estimator = pipeline("depth-estimation", model=checkpoint)
```

----------------------------------------

TITLE: Decoding Predictions in PyTorch
DESCRIPTION: This code snippet decodes the predictions from the model's `logits`. It determines the most likely class for each token and converts the predicted class IDs into textual labels using the model's `id2label` mapping.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_30

LANGUAGE: python
CODE:
```
>>> predictions = torch.argmax(logits, dim=2)
>>> predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]
>>> predicted_token_class
```

----------------------------------------

TITLE: Forward Pass in TensorFlow Model with XLA
DESCRIPTION: This snippet demonstrates how to perform a forward pass using XLA by wrapping the model in tf.function with jit_compile set to True. This approach provides excellent performance improvements by optimizing the generated graph for execution.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tf_xla.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf

model = tf.keras.Sequential(
    [tf.keras.layers.Dense(10, input_shape=(10,), activation="relu"), tf.keras.layers.Dense(5, activation="softmax")]
)
# Generate random inputs for the model.
batch_size = 16
input_vector_dim = 10
random_inputs = tf.random.normal((batch_size, input_vector_dim))

# Run a forward pass.
- _ = model(random_inputs)
+ xla_fn = tf.function(model, jit_compile=True)
+ _ = xla_fn(random_inputs)
```

----------------------------------------

TITLE: Prompted Image Captioning with IDEFICS in Python
DESCRIPTION: The snippet shows how to use IDEFICS for image captioning with an additional textual prompt. It processes both image and text prompts together, generates a coherent text description, and prints it out.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md#2025-04-22_snippet_4

LANGUAGE: py
CODE:
```
>>> prompt = [
...     "https://images.unsplash.com/photo-1543349689-9a4d426bee8e?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3501&q=80",
...     "This is an image of ",
... ]

>>> inputs = processor(prompt, return_tensors="pt").to("cuda")
>>> bad_words_ids = processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> print(generated_text[0])
This is an image of the Eiffel Tower in Paris, France.
```

----------------------------------------

TITLE: Configuring Neptune Callback for Hugging Face Trainer
DESCRIPTION: Python code for setting up Neptune logging with the Hugging Face Trainer. Demonstrates how to create a NeptuneCallback with custom configuration and integrate it with the Trainer.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/README.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers.integrations import NeptuneCallback

neptune_callback = NeptuneCallback(
    name = "DistilBERT",
    description = "DistilBERT fine-tuned on GLUE/MRPC",
    tags = ["args-callback", "fine-tune", "MRPC"],
    base_namespace="callback",
    log_checkpoints = "best",
    capture_hardware_metrics = False,
)

training_args = TrainingArguments(..., report_to=None)
trainer = Trainer(
    model,
    training_args,
    ...
    callbacks=[neptune_callback],
)
```

----------------------------------------

TITLE: Running DeepSeek-V3 with torchrun
DESCRIPTION: This code snippet provides the command to execute the DeepSeek-V3 model using `torchrun` for distributed training or inference across multiple nodes. It specifies the number of processes per node, the number of nodes, node ranking, and the rendezvous backend and endpoint.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/deepseek_v3.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
```bash
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0|1 --rdzv-id an_id --rdzv-backend c10d --rdzv-endpoint master_addr:master_port run_deepseek_r1.py
```
```

----------------------------------------

TITLE: Controlling GPU Order with CUDA_DEVICE_ORDER
DESCRIPTION: This snippet demonstrates how to use the CUDA_DEVICE_ORDER environment variable to control the ordering of GPUs based on PCIe bus IDs or compute ability. This is particularly useful for ensuring that the best GPU is utilized first in a setup with mixed generations of GPUs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/gpu_selection.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
export CUDA_DEVICE_ORDER=PCI_BUS_ID
```

LANGUAGE: bash
CODE:
```
export CUDA_DEVICE_ORDER=FASTEST_FIRST
```

----------------------------------------

TITLE: Push Model to Hub Directly - Python
DESCRIPTION: This snippet shows how to directly push a model to the Hugging Face Model Hub using the `push_to_hub` method of a model instance. It requires specifying the repository name.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/model_sharing.md#_snippet_11

LANGUAGE: python
CODE:
```
>>> pt_model.push_to_hub("my-awesome-model")
```

----------------------------------------

TITLE: Implementing Flash Attention 2 in Transformers
DESCRIPTION: Shows how to enable Flash Attention 2 when loading a model using from_pretrained. Requires torch.bfloat16 or float16 dtype and NVIDIA GPUs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_infer_gpu_one.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
)
```

----------------------------------------

TITLE: Loading Audio Dataset for Speech Recognition in Python
DESCRIPTION: Loads an audio dataset and ensures its sampling rate matches the model's requirements.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/quicktour.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")  # doctest: +IGNORE_RESULT
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))
```

----------------------------------------

TITLE: Manual Zero-shot Image Classification Initialization
DESCRIPTION: This snippet initializes the model and processor from a checkpoint on the Hugging Face Hub for manual zero-shot classification, providing flexibility in how inputs are preprocessed and fed to the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_image_classification.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor, AutoModelForZeroShotImageClassification

>>> model = AutoModelForZeroShotImageClassification.from_pretrained(checkpoint)
>>> processor = AutoProcessor.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Saving a Model with Sharded Checkpoints in Transformers
DESCRIPTION: Example showing how to save a pre-trained model with sharded checkpoints by specifying a max_shard_size parameter. This splits the model weights into multiple smaller files to reduce memory usage during loading.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/big_models.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir, max_shard_size="200MB")
...     print(sorted(os.listdir(tmp_dir)))
['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']
```

----------------------------------------

TITLE: Getting Logits from PyTorch Masked Language Model
DESCRIPTION: Loads a fine-tuned masked language model and computes logits for the masked token to determine prediction probabilities.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/masked_language_modeling.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
from transformers import AutoModelForMaskedLM

model = AutoModelForMaskedLM.from_pretrained("stevhliu/my_awesome_eli5_mlm_model")
logits = model(**inputs).logits
mask_token_logits = logits[0, mask_token_index, :]
```

----------------------------------------

TITLE: Utilizing SDPA Attention for Llama4 in Python
DESCRIPTION: This example configures the Llama4 model to utilize the 'sdpa' attention method, which is more compute-efficient compared to the default 'eager' method. The dependencies include the Transformers library and PyTorch for model setup and performance enhancement.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llama4.md#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
from transformers import Llama4ForConditionalGeneration
import torch

model = Llama4ForConditionalGeneration.from_pretrained(
    model_id,
    attn_implementation="sdpa",
    device_map="auto",
    torch_dtype=torch.bfloat16,
)
```

----------------------------------------

TITLE: Saving and Listing Sharded Checkpoints - Transformers - Python
DESCRIPTION: This code snippet demonstrates how to use the `transformers` library to save a pretrained model into sharded checkpoints and list the saved shard files. The saved checkpoint shards ensure efficient memory usage by limiting the size of each shard to 5GB, suitable for use on limited resources such as free-tier GPU instances.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/models.md#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
from transformers import AutoModel
import tempfile
import os

model = AutoModel.from_pretrained("biomistral/biomistral-7b")
with tempfile.TemporaryDirectory() as tmp_dir:
    model.save_pretrained(tmp_dir, max_shard_size="5GB")
    print(sorted(os.listdir(tmp_dir)))
```

----------------------------------------

TITLE: Loading Yelp Reviews dataset
DESCRIPTION: This snippet loads the Yelp Reviews dataset using the `datasets` library. It demonstrates how to load the dataset and access individual examples. The dataset will be used for fine-tuning a pre-trained model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> from datasets import load_dataset

>>> dataset = load_dataset("yelp_review_full")
>>> dataset["train"][100]
{'label': 0,
 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \"serving off their orders\" when they didn\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\nI\'ve eaten at various McDonalds restaurants for over 30 years. I\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}
```

----------------------------------------

TITLE: Install Accelerate Library
DESCRIPTION: This command installs the Accelerate library from GitHub, which is required for running the summarization script with distributed training capabilities. Accelerate simplifies training on various hardware setups.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/run_scripts.md#_snippet_8

LANGUAGE: bash
CODE:
```
pip install git+https://github.com/huggingface/accelerate
```

----------------------------------------

TITLE: Generating Music with Audio Conditioning
DESCRIPTION: This code generates music conditioned on a text prompt and raw audio prompt using the `MusicgenMelodyForConditionalGeneration` model. It initializes the processor and model, then generates audio based on the provided inputs. The `sample` array and `sample['sampling_rate']` come from the loaded audio dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/musicgen_melody.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor, MusicgenMelodyForConditionalGeneration

>>> processor = AutoProcessor.from_pretrained("facebook/musicgen-melody")
>>> model = MusicgenMelodyForConditionalGeneration.from_pretrained("facebook/musicgen-melody")

>>> inputs = processor(
...	audio=sample["array"],
...	sampling_rate=sample["sampling_rate"],
...	text=["80s blues track with groovy saxophone"],
...	padding=True,
...	return_tensors="pt",
...)
>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)
```

----------------------------------------

TITLE: Processing Images and Inferring Keypoints
DESCRIPTION: Prepare the input images for the SuperPoint model and perform inference to extract keypoints, scores, descriptors, and masks. This snippet demonstrates how to process multiple images simultaneously.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/keypoint_detection.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
inputs = processor(images,return_tensors="pt").to(model.device, model.dtype)
outputs = model(**inputs)
```

----------------------------------------

TITLE: Configure Accelerate
DESCRIPTION: This command initiates the Accelerate configuration process, allowing the user to set up their training environment. It creates and saves a configuration file that is used by Accelerate during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/run_scripts_fr.md#_snippet_9

LANGUAGE: bash
CODE:
```
accelerate config
```

----------------------------------------

TITLE: Installing Transformers with DeepSpeed Support
DESCRIPTION: This command installs the Transformers library with DeepSpeed support enabled. It's useful for users who need both libraries for model training with optimizations provided by DeepSpeed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/deepspeed.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
pip install transformers[deepspeed]
```

----------------------------------------

TITLE: Loading Audio Feature Extractor
DESCRIPTION: Instantiates a feature extractor for the Wav2Vec2 model to normalize and prepare audio input. The feature extractor handles normalization and padding of the audio data.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/preprocessing.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
```

----------------------------------------

TITLE: Configuring fp16 Mixed Precision Training in DeepSpeed
DESCRIPTION: Configuration for AMP-like fp16 mixed precision training, with automatic loss scaling and other precision-related parameters. These settings accelerate training by using half-precision calculations while maintaining accuracy.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/deepspeed.md#2025-04-22_snippet_17

LANGUAGE: yaml
CODE:
```
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    }
}
```

----------------------------------------

TITLE: Retrieving a Model's Chat Template in Python
DESCRIPTION: Demonstrates how to access the chat template stored in a model's tokenizer. This example uses the BlenderBot model to show the Jinja template format stored in the tokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/chat_templating.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer
>>> tokenizer = AutoTokenizer.from_pretrained("facebook/blenderbot-400M-distill")

>>> tokenizer.chat_template
"{% for message in messages %}{% if message['role'] == 'user' %}{{ ' ' }}{% endif %}{{ message['content'] }}{% if not loop.last %}{{ '  ' }}{% endif %}{% endfor %}{{ eos_token }}"
```

----------------------------------------

TITLE: Instantiating DAB-DETR with Pre-trained Weights
DESCRIPTION: This snippet shows how to instantiate a `DabDetrForObjectDetection` model with pre-trained weights. It uses the `from_pretrained` method to load the model directly from the specified pre-trained checkpoint, using the "IDEA-Research/dab-detr-resnet-50" checkpoint.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/dab-detr.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import DabDetrForObjectDetection

>>> model = DabDetrForObjectDetection.from_pretrained("IDEA-Research/dab-detr-resnet-50")
```

----------------------------------------

TITLE: Saving and Converting a Model to GGUF - Python
DESCRIPTION: This snippet highlights the process for saving a model and tokenizer from the Transformers library and then converting them back to the GGUF format using a provided conversion script. The user is expected to set the directory for saving and ensure the conversion script is executed correctly.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/gguf.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
tokenizer.save_pretrained('directory')
model.save_pretrained('directory')

!python ${path_to_llama_cpp}/convert-hf-to-gguf.py ${directory}
```

----------------------------------------

TITLE: Load SigLIP2 Model with Flash Attention 2 Implementation (Python)
DESCRIPTION: This snippet demonstrates how to load a SigLIP2 model using `SiglipModel` with the `attn_implementation` parameter set to 'flash_attention_2' for potentially improved memory efficiency and speed. It requires the `flash-attn` library to be installed. The example shows basic model loading with specific configuration options.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/siglip2.md#_snippet_4

LANGUAGE: Python
CODE:
```
# pip install -U flash-attn --no-build-isolation

from transformers import SiglipModel

model = SiglipModel.from_pretrained(
    "google/siglip2-so400m-patch14-384",
    attn_implementation="flash_attention_2",
    torch_dtype=torch.float16,
    device_map=device,
)
```

----------------------------------------

TITLE: Configuring and Launching Accelerate Training in Script Mode
DESCRIPTION: Commands for generating configuration and launching distributed training using Accelerate CLI when running as a script.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/accelerate.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
accelerate config
```

LANGUAGE: bash
CODE:
```
accelerate launch train.py
```

----------------------------------------

TITLE: Prefilling Static Cache for Text Generation in Python
DESCRIPTION: Shows how to initialize and prefill a StaticCache with a common prompt, then reuse it for generating multiple responses. Demonstrates efficient handling of repeated prefixes in text generation tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/kv_cache.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
import copy
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, StaticCache

model_id = "meta-llama/Llama-2-7b-chat-hf"
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map="cuda")
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Init StaticCache with big enough max-length (1024 tokens for the below example) 
# You can also init a DynamicCache, if that suits you better
prompt_cache = StaticCache(config=model.config, max_batch_size=1, max_cache_len=1024, device="cuda", dtype=torch.bfloat16)

INITIAL_PROMPT = "You are a helpful assistant. "
inputs_initial_prompt = tokenizer(INITIAL_PROMPT, return_tensors="pt").to("cuda")
# This is the common prompt cached, we need to run forward without grad to be able to copy
with torch.no_grad():
     prompt_cache = model(**inputs_initial_prompt, past_key_values = prompt_cache).past_key_values

prompts = ["Help me to write a blogpost about travelling.", "What is the capital of France?"]
responses = []
for prompt in prompts:
    new_inputs = tokenizer(INITIAL_PROMPT + prompt, return_tensors="pt").to("cuda")
    past_key_values = copy.deepcopy(prompt_cache)
    outputs = model.generate(**new_inputs, past_key_values=past_key_values,max_new_tokens=20) 
    response = tokenizer.batch_decode(outputs)[0]
    responses.append(response)

print(responses)
```

----------------------------------------

TITLE: Implementing Multi-modal Chat Template with Jinja for Llama 3.2 Vision Instruct
DESCRIPTION: This Jinja template demonstrates how to handle both text-only and multi-modal content in a chat format. It includes special tokens for different message parts and conditionally processes content based on its type (image or text).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating_multimodal.md#2025-04-22_snippet_6

LANGUAGE: jinja
CODE:
```
{% for message in messages %}
{% if loop.index0 == 0 %}{{ bos_token }}{% endif %}
{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n' }}
{% if message['content'] is string %}
{{ message['content'] }}
{% else %}
{% for content in message['content'] %}
{% if content['type'] == 'image' %}
{{ '<|image|>' }}
{% elif content['type'] == 'text' %}
{{ content['text'] }}
{% endif %}
{% endfor %}
{% endif %}
{{ '<|eot_id|>' }}
{% endfor %}
{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}
```

----------------------------------------

TITLE: Local TensorFlow Model Export
DESCRIPTION: Example of saving and exporting a local TensorFlow model to ONNX format
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/serialization.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

>>> # Load tokenizer and TensorFlow weights from the Hub
>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
>>> # Save to disk
>>> tokenizer.save_pretrained("local-tf-checkpoint")
>>> tf_model.save_pretrained("local-tf-checkpoint")
```

----------------------------------------

TITLE: Initializing Mask Generation Pipeline in Python
DESCRIPTION: Here, we import the pipeline functionality from the Transformers library and initialize a mask generator model with a specific checkpoint. This sets up the model for later inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/mask_generation.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> checkpoint = "facebook/sam-vit-base"
>>> mask_generator = pipeline(model=checkpoint, task="mask-generation")
```

----------------------------------------

TITLE: Registering and Using the Pair Classification Pipeline
DESCRIPTION: This code snippet demonstrates how to register the custom `PairClassificationPipeline` and then use it with a pre-trained model. It registers the pipeline with both PyTorch and TensorFlow model classes, and then shows how to load and use the pipeline to classify sentence pairs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/add_new_pipeline.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from pair_classification import PairClassificationPipeline
from transformers.pipelines import PIPELINE_REGISTRY
from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification

PIPELINE_REGISTRY.register_pipeline(
    "pair-classification",
    pipeline_class=PairClassificationPipeline,
    pt_model=AutoModelForSequenceClassification,
    tf_model=TFAutoModelForSequenceClassification,
)

from transformers import pipeline

classifier = pipeline("pair-classification", model="sgugger/finetuned-bert-mrpc")
```

----------------------------------------

TITLE: Applying Quantization for Donut's AutoModel in Python
DESCRIPTION: This snippet illustrates how to apply quantization using the `torchao` library to reduce the memory footprint of the Donut model. It demonstrates how to configure the quantization settings and utilize the model for document question answering similarly to the previous examples.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/donut.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
# pip install datasets torchao
import torch
from datasets import load_dataset
from transformers import TorchAoConfig, AutoProcessor, AutoModelForVision2Seq

quantization_config = TorchAoConfig("int4_weight_only", group_size=128)
processor = AutoProcessor.from_pretrained("naver-clova-ix/donut-base-finetuned-docvqa")
model = AutoModelForVision2Seq.from_pretrained("naver-clova-ix/donut-base-finetuned-docvqa", quantization_config=quantization_config)

dataset = load_dataset("hf-internal-testing/example-documents", split="test")
image = dataset[0]["image"]
question = "What time is the coffee break?"
task_prompt = f"<s_docvqa><s_question>{question}</s_question><s_answer>"
inputs = processor(image, task_prompt, return_tensors="pt")

outputs = model.generate(
    input_ids=inputs.input_ids,
    pixel_values=inputs.pixel_values,
    max_length=512
)
answer = processor.decode(outputs[0], skip_special_tokens=True)
print(answer)
```

----------------------------------------

TITLE: Distributed Training on Single Node with CCL Backend
DESCRIPTION: This command runs the question-answering training script (`run_qa.py`) in a distributed manner using two processes on a single node, utilizing the oneCCL backend.  It sets environment variables for thread control and specifies training parameters such as model name, dataset, batch size, and learning rate.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_cpu_many.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
"export CCL_WORKER_COUNT=1
export MASTER_ADDR=127.0.0.1
mpirun -n 2 -genv OMP_NUM_THREADS=23 \
python3 run_qa.py \
 --model_name_or_path google-bert/bert-large-uncased \
 --dataset_name squad \
 --do_train \
 --do_eval \
 --per_device_train_batch_size 12  \
 --learning_rate 3e-5  \
 --num_train_epochs 2  \
 --max_seq_length 384 \
 --doc_stride 128  \
 --output_dir /tmp/debug_squad/ \
 --no_cuda \
 --ddp_backend ccl \
 --use_ipex"
```

----------------------------------------

TITLE: Streaming Output from VLM Model
DESCRIPTION: This code demonstrates how to call the `model_inference` function and stream the values. It sets up the user prompt, chat history, maximum new tokens, and images, then iterates through the generator to print the streamed output in real-time.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_text_to_text.md#_snippet_11

LANGUAGE: python
CODE:
```
generator = model_inference(
    user_prompt="And what is in this image?",
    chat_history=messages[:2],
    max_new_tokens=100,
    images=images
)

for value in generator:
  print(value)
```

----------------------------------------

TITLE: Initializing UPerNet with ConvNeXt Backbone for Semantic Segmentation in PyTorch
DESCRIPTION: This code snippet shows how to initialize a UPerNet model using a ConvNeXt backbone instead of Swin. It configures the ConvNeXt backbone to output features from all four stages for semantic segmentation tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/upernet.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import ConvNextConfig, UperNetConfig, UperNetForSemanticSegmentation

backbone_config = ConvNextConfig(out_features=["stage1", "stage2", "stage3", "stage4"])

config = UperNetConfig(backbone_config=backbone_config)
model = UperNetForSemanticSegmentation(config)
```

----------------------------------------

TITLE: Installing Transformers with TensorFlow CPU
DESCRIPTION: This command installs ðŸ¤— Transformers along with TensorFlow CPU as a dependency. It simplifies the installation process when using TensorFlow's CPU version.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/installation.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
"pip install transformers[tf-cpu]"
```

----------------------------------------

TITLE: Preparing Training Components with Accelerate
DESCRIPTION: Code to prepare all training-related objects (dataloaders, model, optimizer) for distributed training using the Accelerator's prepare method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/accelerate.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
...     train_dataloader, eval_dataloader, model, optimizer
... )
```

----------------------------------------

TITLE: Preprocessing the Dataset with Image Processor
DESCRIPTION: Loads an image processor from a pre-trained model and applies it to preprocess all images in the dataset. The preprocessing is applied to all splits using the map function.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/knowledge_distillation_for_image_classification.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoImageProcessor
teacher_processor = AutoImageProcessor.from_pretrained("merve/beans-vit-224")

def process(examples):
    processed_inputs = teacher_processor(examples["image"])
    return processed_inputs

processed_datasets = dataset.map(process, batched=True)
```

----------------------------------------

TITLE: Loading Feature Extractor
DESCRIPTION: Loads a pre-trained feature extractor for the Wav2Vec2 model using `AutoFeatureExtractor.from_pretrained`. This feature extractor is used to normalize and pad the audio inputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_13

LANGUAGE: python
CODE:
```
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
```

----------------------------------------

TITLE: Loading Pre-trained Configuration and Modifying
DESCRIPTION: This snippet shows how to load a pre-trained DistilBert configuration from a specific model and modify its activation and attention dropout.  It uses `DistilBertConfig.from_pretrained` to load the configuration. The function requires a model identifier and the parameters to be modified.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/create_a_model.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> my_config = DistilBertConfig.from_pretrained("distilbert/distilbert-base-uncased", activation="relu", attention_dropout=0.4)
```

----------------------------------------

TITLE: Resampling Audio Data for Wav2Vec2 Compatibility
DESCRIPTION: Python code to resample the audio data in the MInDS-14 dataset from 8000Hz to 16000Hz, which is required for compatibility with the pre-trained Wav2Vec2 model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/asr.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
```

----------------------------------------

TITLE: Implementing a Custom Pair Classification Pipeline in Python
DESCRIPTION: Defines a custom PairClassificationPipeline class for sentence pair classification tasks, including preprocessing, forward pass, and postprocessing methods.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/add_new_pipeline.md#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
import numpy as np

from transformers import Pipeline


def softmax(outputs):
    maxes = np.max(outputs, axis=-1, keepdims=True)
    shifted_exp = np.exp(outputs - maxes)
    return shifted_exp / shifted_exp.sum(axis=-1, keepdims=True)


class PairClassificationPipeline(Pipeline):
    def _sanitize_parameters(self, **kwargs):
        preprocess_kwargs = {}
        if "second_text" in kwargs:
            preprocess_kwargs["second_text"] = kwargs["second_text"]
        return preprocess_kwargs, {}, {}

    def preprocess(self, text, second_text=None):
        return self.tokenizer(text, text_pair=second_text, return_tensors=self.framework)

    def _forward(self, model_inputs):
        return self.model(**model_inputs)

    def postprocess(self, model_outputs):
        logits = model_outputs.logits[0].numpy()
        probabilities = softmax(logits)

        best_class = np.argmax(probabilities)
        label = self.model.config.id2label[best_class]
        score = probabilities[best_class].item()
        logits = logits.tolist()
        return {"label": label, "score": score, "logits": logits}
```

----------------------------------------

TITLE: Loading PEFT adapter with load_adapter
DESCRIPTION: Loads a PEFT adapter by first loading a base model and then attaching the adapter using `load_adapter`. It shows an alternative method for loading PEFT adapters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/peft.md#_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "facebook/opt-350m"
peft_model_id = "ybelkada/opt-350m-lora"

model = AutoModelForCausalLM.from_pretrained(model_id)
model.load_adapter(peft_model_id)
```

----------------------------------------

TITLE: Saving and Loading Model Configurations
DESCRIPTION: Demonstrates how to save a model configuration to disk and reload it.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/create_a_model.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> my_config.save_pretrained(save_directory="./your_model_save_path")

>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/config.json")
```

----------------------------------------

TITLE: Image Classification with IDEFICS in Python
DESCRIPTION: This snippet demonstrates how to classify an image into one of several predefined categories using the IDEFICS model. It takes an image URL and a list of categories as input. The model then predicts the most likely category for the image. The `bad_words_ids` argument prevents the model from generating specified tokens.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/idefics.md#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
>>> categories = ['animals','vegetables', 'city landscape', 'cars', 'office']
>>> prompt = [f"Instruction: Classify the following image into a single category from the following list: {categories}.\n",
...     "https://images.unsplash.com/photo-1471193945509-9ad0617afabf?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80",    
...     "Category: "
... ]

>>> inputs = processor(prompt, return_tensors="pt").to("cuda")
>>> bad_words_ids = processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, max_new_tokens=6, bad_words_ids=bad_words_ids)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> print(generated_text[0])
Instruction: Classify the following image into a single category from the following list: ['animals', 'vegetables', 'city landscape', 'cars', 'office'].
Category: Vegetables
```

----------------------------------------

TITLE: Chat with Llama 2 using Transformers CLI (Bash)
DESCRIPTION: Provides a command-line example for starting a chat session with the Llama 2-Chat model using the `transformers` library's command-line interface. It specifies the model, torch data type, and attention implementation. Requires the `transformers` library installed with CLI capabilities.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llama2.md#_snippet_2

LANGUAGE: bash
CODE:
```
transformers chat meta-llama/Llama-2-7b-chat-hf --torch_dtype auto --attn_implementation flash_attention_2
```

----------------------------------------

TITLE: Generating Text with Exported Model
DESCRIPTION: This snippet illustrates how to use the exported PyTorch model with ExecuTorch to generate text based on input prompts. It includes steps for tokenizing the prompts and decoding the generated output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/executorch.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
prompts = ["Simply put, the theory of relativity states that "]
prompt_tokens = tokenizer(prompts, return_tensors="pt", padding=True).to(model.device)
prompt_token_ids = prompt_tokens["input_ids"]

generated_ids = TorchExportableModuleWithStaticCache.generate(
    exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=20,
)
generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
print(generated_text)
["Simply put, the theory of relativity states that 1) the speed of light is the"]
```

----------------------------------------

TITLE: Loading Specific Model Version using Revision Parameter
DESCRIPTION: Demonstrates how to load a specific version of a model using the revision parameter to specify a tag, branch name, or commit hash.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_sharing.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
model = AutoModel.from_pretrained(
    "julien-c/EsperBERTo-small", revision="4c77982"  # tag name, branch name, or commit hash
)
```

----------------------------------------

TITLE: PyTorch Manual Summarization - Decoding
DESCRIPTION: Demonstrates how to decode the generated token ids back into readable text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_27

LANGUAGE: python
CODE:
```
tokenizer.decode(outputs[0], skip_special_tokens=True)
```

----------------------------------------

TITLE: Installing Transformers via Conda in Bash
DESCRIPTION: This command installs the Transformers library using conda from the conda-forge channel, a cross-platform package manager for managing dependencies and environments.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md#2025-04-22_snippet_13

LANGUAGE: bash
CODE:
```
conda install conda-forge::transformers
```

----------------------------------------

TITLE: Direct StaticCache Implementation
DESCRIPTION: Shows how to manually initialize and handle StaticCache object for multi-turn generation or custom generation loops. This approach provides more control over cache management and reuse.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_optims.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM, StaticCache
import torch
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # To prevent long warnings :)

tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b")
model = AutoModelForCausalLM.from_pretrained("google/gemma-2b", torch_dtype="auto", device_map="auto")

model.forward = torch.compile(model.forward, mode="reduce-overhead", fullgraph=True)
input_text = "The theory of special relativity states "
input_ids = tokenizer(input_text, return_tensors="pt").to(model.device.type)
prompt_length = input_ids.input_ids.shape[1]
model.generation_config.max_new_tokens = 16

past_key_values = StaticCache(
    config=model.config,
    max_batch_size=1,
    # If you plan to reuse the cache, make sure the cache length is large enough for all cases
    max_cache_len=prompt_length+(model.generation_config.max_new_tokens*2),
    device=model.device,
    dtype=model.dtype
)
outputs = model.generate(**input_ids, past_key_values=past_key_values)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
```

----------------------------------------

TITLE: Exporting to ONNX Using Optimum CLI
DESCRIPTION: Exports a Transformers model to ONNX format using the Optimum CLI. The --model argument is used to specify the model architecture, and the command generates an ONNX file in the specified output directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/serialization.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
optimum-cli export onnx --model distilbert/distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/
```

----------------------------------------

TITLE: Creating DeepSpeed ZeRO-3 Configuration in Notebook
DESCRIPTION: Bash cell for creating a DeepSpeed ZeRO stage 3 configuration JSON file directly in a Jupyter notebook. This configuration enables CPU offloading for both optimizer states and parameters for memory-efficient training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/deepspeed.md#2025-04-22_snippet_19

LANGUAGE: bash
CODE:
```
cat <<'EOT' > ds_config_zero3.json
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": "auto",
            "betas": "auto",
            "eps": "auto",
            "weight_decay": "auto"
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },

    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    },

    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 2000,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false
}
EOT
```

----------------------------------------

TITLE: Installing bitsandbytes and required dependencies
DESCRIPTION: Command to install bitsandbytes along with transformers and accelerate libraries using pip.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-23_snippet_0

LANGUAGE: bash
CODE:
```
pip install --upgrade transformers accelerate bitsandbytes
```

----------------------------------------

TITLE: Accessing the Tokenizer from the Processor in Transformers
DESCRIPTION: Extracts the tokenizer component from a processor object for text preprocessing in document question answering tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/document_question_answering.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> tokenizer = processor.tokenizer
```

----------------------------------------

TITLE: Installing Optimum with TensorFlow exporters
DESCRIPTION: Command to install the necessary dependencies for exporting models to TFLite format using the Optimum library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/hi/tflite.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install optimum[exporters-tf]
```

----------------------------------------

TITLE: Creating Label Mappings for Food-101 Dataset
DESCRIPTION: Creates dictionaries to map between label names and their corresponding integer IDs for the Food-101 dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/image_classification.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> labels = food["train"].features["label"].names
>>> label2id, id2label = dict(), dict()
>>> for i, label in enumerate(labels):
...     label2id[label] = str(i)
...     id2label[str(i)] = label
```

----------------------------------------

TITLE: Setting up Model Evaluation
DESCRIPTION: Configures evaluation metrics using the Evaluate library to track model performance during training
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/training.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import numpy as np
import evaluate

metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

----------------------------------------

TITLE: Preparing Training Components with Accelerator
DESCRIPTION: Use the prepare method to set up dataloaders, model, and optimizer for distributed training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/accelerate.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)
```

----------------------------------------

TITLE: Tracing Model for AWS Neuron
DESCRIPTION: This code snippet demonstrates the crucial line change required when using the AWS Neuron SDK. Instead of using `torch.jit.trace`, you use `torch.neuron.trace` to trace and optimize the model for Inf1 instances.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/torchscript.md#_snippet_4

LANGUAGE: diff
CODE:
```
- torch.jit.trace(model, [tokens_tensor, segments_tensors])
+ torch.neuron.trace(model, [token_tensor, segments_tensors])
```

----------------------------------------

TITLE: Post-processing Detection Results
DESCRIPTION: This snippet details how to process the model outputs, transforming the predicted bounding boxes back to the original image dimensions and visualizing the results.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> import torch

>>> with torch.no_grad():
...     outputs = model(**inputs)
...     target_sizes = torch.tensor([im.size[::-1]])
...     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)[0]

>>> draw = ImageDraw.Draw(im)

>>> scores = results["scores"].tolist()
>>> labels = results["labels"].tolist()
>>> boxes = results["boxes"].tolist()

>>> for box, score, label in zip(boxes, scores, labels):
...     xmin, ymin, xmax, ymax = box
...     draw.rectangle((xmin, ymin, xmax, ymax), outline="red", width=1)
...     draw.text((xmin, ymin), f"{text_queries[label]}: {round(score,2)}", fill="white")

>>> im
```

----------------------------------------

TITLE: Visualizing Detection Results
DESCRIPTION: This snippet illustrates how to visualize the predictions made by the zero-shot object detection pipeline. It draws bounding boxes around detected objects and labels them on the image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from PIL import ImageDraw

>>> draw = ImageDraw.Draw(image)

>>> for prediction in predictions:
...     box = prediction["box"]
...     label = prediction["label"]
...     score = prediction["score"]
... 
...     xmin, ymin, xmax, ymax = box.values()
...     draw.rectangle((xmin, ymin, xmax, ymax), outline="red", width=1)
...     draw.text((xmin, ymin), f"{label}: {round(score,2)}", fill="white")

>>> image
```

----------------------------------------

TITLE: Initializing UPerNet with Swin Backbone for Semantic Segmentation in PyTorch
DESCRIPTION: This code snippet demonstrates how to initialize a UPerNet model for semantic segmentation using a Swin Transformer as the backbone. It configures the backbone to output features from all four stages.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/upernet.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import SwinConfig, UperNetConfig, UperNetForSemanticSegmentation

backbone_config = SwinConfig(out_features=["stage1", "stage2", "stage3", "stage4"])

config = UperNetConfig(backbone_config=backbone_config)
model = UperNetForSemanticSegmentation(config)
```

----------------------------------------

TITLE: Enabling Gradient Checkpointing for LED Models in PyTorch
DESCRIPTION: Code snippet showing how to enable gradient checkpointing for the LED model to handle memory issues when fine-tuning on long sequences up to 16384 tokens.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/led.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
model.gradient_checkpointing_enable()
```

----------------------------------------

TITLE: Saving and Loading Tokenizer from JSON
DESCRIPTION: Shows how to save a tokenizer to a JSON file and then load it into Transformers using PreTrainedTokenizerFast. This method allows for persistent storage and reuse of tokenizer configurations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/fast_tokenizers.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> tokenizer.save("tokenizer.json")

>>> from transformers import PreTrainedTokenizerFast

>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file="tokenizer.json")
```

----------------------------------------

TITLE: Multi-node training with torch.distributed.run
DESCRIPTION: Command to launch multi-node training using PyTorch's distributed run with DeepSpeed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/deepspeed.md#2025-04-22_snippet_12

LANGUAGE: bash
CODE:
```
python -m torch.distributed.run --nproc_per_node=8 --nnode=2 --node_rank=0 --master_addr=hostname1 \
--master_port=9901 your_program.py <normal cl args> --deepspeed ds_config.json
```

----------------------------------------

TITLE: Loading DistilRoBERTa with TFAutoModelForMaskedLM (TensorFlow)
DESCRIPTION: Code to initialize a DistilRoBERTa model for masked language modeling tasks using the TFAutoModelForMaskedLM class in TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
from transformers import TFAutoModelForMaskedLM

model = TFAutoModelForMaskedLM.from_pretrained("distilbert/distilroberta-base")
```

----------------------------------------

TITLE: Logging into Hugging Face Hub
DESCRIPTION: Authenticates the user with Hugging Face Hub to enable sharing and uploading models. This allows saving and accessing model checkpoints in the community repository.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/translation.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Loading BERT Model and Checking GPU Memory Usage in Python
DESCRIPTION: This code loads a BERT model onto the GPU and prints the GPU memory usage, demonstrating how much space the model weights occupy.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_memory_anatomy.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification


>>> model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-large-uncased").to("cuda")
>>> print_gpu_utilization()
```

----------------------------------------

TITLE: Display Image (Python)
DESCRIPTION: Accesses the first image in the dataset.  This example does not actually display the image, but it shows how to access it using dataset indexing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/preprocessing.md#_snippet_17

LANGUAGE: python
CODE:
```
>>> dataset[0]["image"]
```

----------------------------------------

TITLE: Mask Filling with BART using Python
DESCRIPTION: Example showing how to use BART model for filling masked tokens in text. The code loads a pre-trained BART model and tokenizer, processes input text with masks, and generates completed text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/bart.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import BartForConditionalGeneration, BartTokenizer

model = BartForConditionalGeneration.from_pretrained("facebook/bart-large", forced_bos_token_id=0)
tok = BartTokenizer.from_pretrained("facebook/bart-large")
example_english_phrase = "UN Chief Says There Is No <mask> in Syria"
batch = tok(example_english_phrase, return_tensors="pt")
generated_ids = model.generate(batch["input_ids"])
assert tok.batch_decode(generated_ids, skip_special_tokens=True) == [
    "UN Chief Says There Is No Plan to Stop Chemical Weapons in Syria"
]
```

----------------------------------------

TITLE: Converting PyTorch model to Flax
DESCRIPTION: This Python code converts a PyTorch pre-trained model to Flax format. It loads the PyTorch model using `FlaxDistilBertForSequenceClassification.from_pretrained` with `from_pt=True`, and then saves it as a Flax model using `save_pretrained`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
"from transformers import FlaxDistilBertForSequenceClassification\nflax_model = FlaxDistilBertForSequenceClassification.from_pretrained(\n    \"path/to/awesome-name-you-picked\", from_pt=True\n)\nflax_model.save_pretrained(\"path/to/awesome-name-you-picked\")"
```

----------------------------------------

TITLE: Applying Chat Template for Zephyr-7B using Python
DESCRIPTION: This snippet shows how to utilize the `AutoTokenizer` for the Zephyr-7B model. Similar to the previous example, it applies a chat template to format conversation inputs into a structured format, ensuring compatibility with the specific tokenizer for this model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-beta")
chat = [
  {"role": "user", "content": "Hello, how are you?"},
  {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
  {"role": "user", "content": "I'd like to show off how chat templating works!"},
]

tokenizer.apply_chat_template(chat, tokenize=False)
```

----------------------------------------

TITLE: Instantiate DETR with Random Weights (Python)
DESCRIPTION: This code demonstrates initializing DETR with randomly initialized weights for both the backbone and the Transformer. It imports DetrConfig and DetrForObjectDetection, creates a configuration with use_pretrained_backbone set to False, and then initializes the model with that config. This allows for training the entire model from scratch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/detr.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> config = DetrConfig(use_pretrained_backbone=False)
>>> model = DetrForObjectDetection(config)
```

----------------------------------------

TITLE: Run Summarization with Limited Samples
DESCRIPTION: This code snippet shows how to run the summarization script on a smaller number of dataset examples for testing purposes. It limits the number of training, evaluation, and prediction samples using the `max_train_samples`, `max_eval_samples`, and `max_predict_samples` arguments.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/run_scripts_fr.md#_snippet_13

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --max_train_samples 50 \
    --max_eval_samples 50 \
    --max_predict_samples 50 \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Initializing Image-Text Processor for GIT Model
DESCRIPTION: Loads the processor for the microsoft/git-base model which handles both image preprocessing and text tokenization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_captioning.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoProcessor

checkpoint = "microsoft/git-base"
processor = AutoProcessor.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Loading Accuracy Metric - Python
DESCRIPTION: This snippet demonstrates how to load the accuracy metric from the Evaluate library. It sets up the metric to be used later for evaluating model performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
import evaluate

accuracy = evaluate.load("accuracy")
```

----------------------------------------

TITLE: Correcting Output with Attention Mask to Ignore Padding Tokens
DESCRIPTION: This code demonstrates the correct way to handle padding tokens by providing an `attention_mask`. The `attention_mask` indicates which tokens should be attended to and which should be ignored (padding tokens).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/troubleshooting.md#_snippet_8

LANGUAGE: Python
CODE:
```
>>> attention_mask = torch.tensor([[1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0]])
>>> output = model(input_ids, attention_mask=attention_mask)
>>> print(output.logits)
tensor([[ 0.0082, -0.2307],
        [-0.1008, -0.4061]], grad_fn=<AddmmBackward0>)
```

----------------------------------------

TITLE: Using ConvNextImageProcessor in Python
DESCRIPTION: Image processor class for ConvNeXT model. Handles image preprocessing and postprocessing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/convnext.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
[[autodoc]] ConvNextImageProcessor
    - preprocess
```

----------------------------------------

TITLE: Installing Required Libraries for Zero-shot Image Classification
DESCRIPTION: This snippet installs necessary libraries, including the Hugging Face Transformers library with PyTorch and Pillow for image processing. It sets up the environment to perform zero-shot image classification.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_image_classification.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install -q "transformers[torch]" pillow
```

----------------------------------------

TITLE: Splitting the Dataset
DESCRIPTION: Splits the loaded dataset into training and testing sets using the `train_test_split` method. The `test_size` parameter is set to 0.2, indicating that 20% of the data will be used for testing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/audio_classification.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
">>> minds = minds.train_test_split(test_size=0.2)"
```

----------------------------------------

TITLE: Saving a Transformers Model with Default Configuration
DESCRIPTION: This code shows how to save a model using the save_pretrained method, displaying the standard output files (config.json and weights) created in the target directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/big_models.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
>>> import os
>>> import tempfile

>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir)
...     print(sorted(os.listdir(tmp_dir)))
['config.json', 'pytorch_model.bin']
```

----------------------------------------

TITLE: Use HybridCache for Gemma 2 Text Generation (Python)
DESCRIPTION: Illustrates the required usage of `HybridCache` for efficient caching during text generation with Gemma 2 models due to their specific sliding window attention mechanism. It shows how to instantiate and pass the cache to the model's forward method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gemma2.md#_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM, HybridCache

model = AutoModelForCausalLM.from_pretrained("google/gemma-2-2b")
tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-2b")

inputs = tokenizer(text="My name is Gemma", return_tensors="pt")
max_generated_length = inputs.input_ids.shape[1] + 10
past_key_values = HybridCache(config=model.config, max_batch_size=1,
max_cache_len=max_generated_length, device=model.device, dtype=model.dtype)
outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)
```

----------------------------------------

TITLE: Sharing Fine-Tuned Model to Hugging Face Hub in Python
DESCRIPTION: Use the push_to_hub method of the Trainer to upload the fine-tuned model to Hugging Face's Hub for community access.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_captioning.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
trainer.push_to_hub()
```

----------------------------------------

TITLE: Implementing Watermarking in Text Generation with Transformers in Python
DESCRIPTION: This snippet outlines the process of implementing watermarking in text generation using the Hugging Face Transformers library. It configures watermarking for a GPT-2 model based on the 'selfhash' algorithm, which biases the generation of specific tokens. The `WatermarkingConfig` class is used to set the bias and seeding scheme. It also demonstrates how to detect watermarked text using the `WatermarkDetector` class. This approach allows for the identification of machine-generated text without an additional classification model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/generation_features.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM, WatermarkDetector, WatermarkingConfig

model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")
tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2")
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.padding_side = "left"

inputs = tokenizer(["This is the beginning of a long story", "Alice and Bob are"], padding=True, return_tensors="pt")
input_len = inputs["input_ids"].shape[-1]

watermarking_config = WatermarkingConfig(bias=2.5, seeding_scheme="selfhash")
out = model.generate(**inputs, watermarking_config=watermarking_config, do_sample=False, max_length=20)
```

----------------------------------------

TITLE: Training Model without GPU Optimizations
DESCRIPTION: Trains a transformer model using the Trainer API from the transformers library, without any specific GPU optimization techniques, and prints a summary of the training runtime and GPU memory usage.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_memory_anatomy.md#2025-04-22_snippet_8

LANGUAGE: py
CODE:
```
>>> from transformers import TrainingArguments, Trainer, logging

>>> logging.set_verbosity_error()

>>> training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)
>>> trainer = Trainer(model=model, args=training_args, train_dataset=ds)
>>> result = trainer.train()
>>> print_summary(result)
```

Time: 57.82
Samples/second: 8.86
GPU memory occupied: 14949 MB.

```

----------------------------------------

TITLE: Installing PyTorch with pip
DESCRIPTION: This code snippet shows how to install the PyTorch framework using pip. PyTorch is a popular machine learning framework often used with Hugging Face Transformers.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_1

LANGUAGE: bash
CODE:
```
pip install torch
```

----------------------------------------

TITLE: Speculative Decoding with Transformers in Python
DESCRIPTION: This code shows how to use speculative decoding (also known as assisted decoding) for faster text generation using the Hugging Face Transformers library.  It initializes a tokenizer, a main causal language model, and a smaller assistant model.  The main model generates text with assistance from the assistant model by setting the `assistant_model` argument in the `generate` method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/generation_strategies.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> prompt = "Alice and Bob"
>>> checkpoint = "EleutherAI/pythia-1.4b-deduped"
>>> assistant_checkpoint = "EleutherAI/pythia-160m-deduped"

>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)
>>> assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)
>>> outputs = model.generate(**inputs, assistant_model=assistant_model)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a']
```

----------------------------------------

TITLE: Image Classification with Default Model in Python
DESCRIPTION: Code snippet showing how to perform image classification using the default pre-trained model in the transformers pipeline to identify a cat in an image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/task_summary.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline(task="image-classification")
>>> preds = classifier(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> print(*preds, sep="\n")
{'score': 0.4335, 'label': 'lynx, catamount'}
{'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}
{'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}
{'score': 0.0239, 'label': 'Egyptian cat'}
{'score': 0.0229, 'label': 'tiger cat'}
```

----------------------------------------

TITLE: Configuring DeepSpeed Optimizer with Auto Parameters
DESCRIPTION: Set up an AdamW optimizer with automatic parameter selection in a DeepSpeed configuration file, allowing flexible learning rate and optimization settings
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/deepspeed.md#2025-04-22_snippet_20

LANGUAGE: yaml
CODE:
```
{
   "optimizer": {
       "type": "AdamW",
       "params": {
         "lr": "auto",
         "betas": "auto",
         "eps": "auto",
         "weight_decay": "auto"
       }
   }
}
```

----------------------------------------

TITLE: Loading Model from Local Path (Offline)
DESCRIPTION: This Python code snippet demonstrates how to load a pre-trained model from a local directory when offline. The `local_files_only=True` parameter ensures that the model is loaded from the specified local path without attempting to connect to the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/installation.md#_snippet_17

LANGUAGE: python
CODE:
```
from transformers import T5Model

model = T5Model.from_pretrained("./path/to/local/directory", local_files_only=True)
```

----------------------------------------

TITLE: Using PushToHubCallback with TensorFlow
DESCRIPTION: Demonstrates how to use the PushToHubCallback in TensorFlow to automatically push a model to the Hugging Face Hub during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/model_sharing.md#2025-04-23_snippet_11

LANGUAGE: python
CODE:
```
>>> from transformers import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="./your_model_save_path", tokenizer=tokenizer, hub_model_id="your-username/my-awesome-model"
... )
```

----------------------------------------

TITLE: Loading and Inferencing Chameleon Model with Single Image
DESCRIPTION: This snippet demonstrates how to load the Chameleon model and processor, prepare an image and text prompt, and generate a response for a single image input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/chameleon.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import ChameleonProcessor, ChameleonForConditionalGeneration
import torch
from PIL import Image
import requests

processor = ChameleonProcessor.from_pretrained("facebook/chameleon-7b")
model = ChameleonForConditionalGeneration.from_pretrained("facebook/chameleon-7b", torch_dtype=torch.bfloat16, device_map="cuda")

# Prepare image and text prompt
url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)
prompt = "What do you see in this image?<image>"

inputs = processor(images=image, text=prompt, return_tensors="pt").to(model.device, dtype=torch.bfloat16)

# Complete the prompt autoregressively
output = model.generate(**inputs, max_new_tokens=50)
print(processor.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Splitting ELI5 Dataset into Train and Test sets
DESCRIPTION: This code splits the loaded ELI5 dataset into training and testing sets using the `train_test_split` method.  The `test_size` parameter is set to 0.2, indicating that 20% of the data will be used for testing, and 80% for training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_3

LANGUAGE: python
CODE:
```
>>> eli5 = eli5.train_test_split(test_size=0.2)
```

----------------------------------------

TITLE: Applying torch.compile() to Image Classification Model in Python
DESCRIPTION: This snippet demonstrates how to use torch.compile() with an AutoModelForImageClassification for improved inference speed. It includes loading the model, applying compilation, and processing an input image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_torch_compile.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from PIL import Image
import requests
import numpy as np
from transformers import AutoImageProcessor, AutoModelForImageClassification

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
model = AutoModelForImageClassification.from_pretrained("google/vit-base-patch16-224").to("cuda")
model = torch.compile(model)

processed_input = processor(image, return_tensors='pt').to(device="cuda")

with torch.no_grad():
    _ = model(**processed_input)
```

----------------------------------------

TITLE: Generating Speech with Combined FastSpeech2Conformer and HifiGan
DESCRIPTION: Python code to generate speech using a combined FastSpeech2Conformer and HifiGan model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/fastspeech2_conformer.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import FastSpeech2ConformerTokenizer, FastSpeech2ConformerWithHifiGan
import soundfile as sf

tokenizer = FastSpeech2ConformerTokenizer.from_pretrained("espnet/fastspeech2_conformer")
inputs = tokenizer("Hello, my dog is cute.", return_tensors="pt")
input_ids = inputs["input_ids"]

model = FastSpeech2ConformerWithHifiGan.from_pretrained("espnet/fastspeech2_conformer_with_hifigan")
output_dict = model(input_ids, return_dict=True)
waveform = output_dict["waveform"]

sf.write("speech.wav", waveform.squeeze().detach().numpy(), samplerate=22050)
```

----------------------------------------

TITLE: Exporting a Local Model to ONNX with Task Specification
DESCRIPTION: Command to export a locally stored model to ONNX format with a specific task designation using the Optimum CLI.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/serialization.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
optimum-cli export onnx --model local_path --task question-answering distilbert_base_uncased_squad_onnx/
```

----------------------------------------

TITLE: Generating Text with Zephyr Model Using Chat Template
DESCRIPTION: Example showing how to generate a response from the Zephyr model using an input that has been formatted with the chat template. The model will produce text in the pirate style as requested in the system message.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/chat_templating.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
outputs = model.generate(tokenized_chat, max_new_tokens=128) 
print(tokenizer.decode(outputs[0]))
```

----------------------------------------

TITLE: Creating Data Collator for Language Modeling (PyTorch)
DESCRIPTION: Code to create a data collator that dynamically pads sequences in each batch for efficient training in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/language_modeling.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorForLanguageModeling

>>> tokenizer.pad_token = tokenizer.eos_token
>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
```

----------------------------------------

TITLE: Loading ViT Image Processor
DESCRIPTION: Loads the Vision Transformer (ViT) image processor for preprocessing images.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/image_classification.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoImageProcessor

>>> checkpoint = "google/vit-base-patch16-224-in21k"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Loading XLM Model and Tokenizer for Multilingual Inference in Python
DESCRIPTION: This snippet demonstrates how to load an XLM model and tokenizer for multilingual inference, specifically using the 'FacebookAI/xlm-clm-enfr-1024' checkpoint for English-French causal language modeling.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/multilingual.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import XLMTokenizer, XLMWithLMHeadModel

tokenizer = XLMTokenizer.from_pretrained("FacebookAI/xlm-clm-enfr-1024")
model = XLMWithLMHeadModel.from_pretrained("FacebookAI/xlm-clm-enfr-1024")
```

----------------------------------------

TITLE: Using BitNet b1.58 Model for Generation (Python)
DESCRIPTION: This Python snippet demonstrates how to load the BitNet b1.58 2B 4T model and tokenizer from Hugging Face and perform basic text generation. It initializes the model in bfloat16, applies a chat template to format input messages, and generates a response. Note: Using transformers for this model does not provide the computational efficiency benefits described in the technical report, which require the dedicated C++ implementation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bitnet.md#_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "microsoft/bitnet-b1.58-2B-4T"

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16
)

# Apply the chat template
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "How are you?"},
]
chat_input = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to(model.device)

# Generate response
chat_outputs = model.generate(chat_input, max_new_tokens=50)
response = tokenizer.decode(chat_outputs[0][chat_input.shape[-1]:], skip_special_tokens=True) # Decode only the response part
print("\nAssistant Response:", response)
```

----------------------------------------

TITLE: Initializing Image-Text-to-Text Model with Transformers
DESCRIPTION: This snippet initializes an image-text-to-text model (idefics2-8b) and its associated processor from Hugging Face Transformers. It loads the model with specified data type and attention implementation, then moves it to the CUDA device if available. The processor is used for preparing the input for the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_text_to_text.md#_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoProcessor, AutoModelForImageTextToText
import torch

device = torch.device("cuda")
model = AutoModelForImageTextToText.from_pretrained(
    "HuggingFaceM4/idefics2-8b",
    torch_dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
).to(device)

processor = AutoProcessor.from_pretrained("HuggingFaceM4/idefics2-8b")
```

----------------------------------------

TITLE: Splitting the Dataset for Training and Testing
DESCRIPTION: Python code for splitting the loaded dataset into training and test sets using the train_test_split method with a 80/20 ratio.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> ds = ds.train_test_split(test_size=0.2)
>>> train_ds = ds["train"]
>>> test_ds = ds["test"]
```

----------------------------------------

TITLE: Hyperparameter Search with Weights & Biases in Python
DESCRIPTION: This final example runs the hyperparameter_search process using Weights & Biases as the backend. It specifies the search direction, hyperparameter space, and trial information to optimize the given parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/hpo_train.md#2025-04-22_snippet_10

LANGUAGE: Python
CODE:
```
best_trials = trainer.hyperparameter_search( 
    direction=["minimize", "maximize"],
    backend="wandb",
    hp_space=wandb_hp_space,
    n_trials=20,
    compute_objective=compute_objective,
)
```

----------------------------------------

TITLE: Running Summarization with Custom Dataset
DESCRIPTION: Executes the `run_summarization.py` script with a custom dataset. It includes parameters to specify training and validation files, input and target columns, and other training configurations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/run_scripts.md#2025-04-22_snippet_12

LANGUAGE: bash
CODE:
```
```bash
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --train_file path_to_csv_or_jsonlines_file \
    --validation_file path_to_csv_or_jsonlines_file \
    --text_column text_column_name \
    --summary_column summary_column_name \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --overwrite_output_dir \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate \
```
```

----------------------------------------

TITLE: Inference with Trained Masked Language Model using Transformers Pipeline
DESCRIPTION: Python code to perform fill-mask inference using a trained RoBERTa model. The code uses the Hugging Face Transformers pipeline to predict the most likely tokens to fill a masked position in a sentence.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/tensorflow/language-modeling-tpu/README.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import pipeline

model_id = "tf-tpu/roberta-base-epochs-500-no-wd"
unmasker = pipeline("fill-mask", model=model_id, framework="tf")
unmasker("Goal of my life is to [MASK].")

[{'score': 0.1003185287117958,
  'token': 52,
  'token_str': 'be',
  'sequence': 'Goal of my life is to be.'},
 {'score': 0.032648514956235886,
  'token': 5,
  'token_str': '',
  'sequence': 'Goal of my life is to .'},
 {'score': 0.02152673341333866,
  'token': 138,
  'token_str': 'work',
  'sequence': 'Goal of my life is to work.'},
 {'score': 0.019547373056411743,
  'token': 984,
  'token_str': 'act',
  'sequence': 'Goal of my life is to act.'},
 {'score': 0.01939118467271328,
  'token': 73,
  'token_str': 'have',
  'sequence': 'Goal of my life is to have.'}]
```

----------------------------------------

TITLE: Implementing Early Stopping Callback in Python
DESCRIPTION: This snippet demonstrates how to create a custom EarlyStoppingCallback class that inherits from TrainerCallback. The callback stops training after a specified number of steps. It also shows how to use the callback with the Trainer class.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/trainer.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from transformers import TrainerCallback, Trainer

class EarlyStoppingCallback(TrainerCallback):
    def __init__(self, num_steps=10):
        self.num_steps = num_steps

    def on_step_end(self, args, state, control, **kwargs):
        if state.global_step >= self.num_steps:
            return {"should_training_stop": True}
        else:
            return {}

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    processing_class=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback()],
)
```

----------------------------------------

TITLE: Fine-tuning DistilGPT2 with TensorFlow
DESCRIPTION: This snippet shows how to fine-tune DistilGPT2 using TensorFlow. It covers setting up the optimizer, loading the model, preparing datasets, compiling the model, and training with callbacks for pushing to the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/language_modeling.md#2025-04-23_snippet_8

LANGUAGE: python
CODE:
```
>>> from transformers import create_optimizer, AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)

>>> from transformers import TFAutoModelForCausalLM

>>> model = TFAutoModelForCausalLM.from_pretrained("distilbert/distilgpt2")

>>> tf_train_set = model.prepare_tf_dataset(
...     lm_dataset["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_test_set = model.prepare_tf_dataset(
...     lm_dataset["test"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)  # No loss argument!

>>> from transformers.keras_callbacks import PushToHubCallback

>>> callback = PushToHubCallback(
...     output_dir="my_awesome_eli5_clm-model",
...     tokenizer=tokenizer,
... )

>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])
```

----------------------------------------

TITLE: Using a Custom PyTorch Optimizer
DESCRIPTION: This snippet demonstrates how to use a custom PyTorch optimizer with the Transformers Trainer by passing the optimizer class and keyword arguments via the `optimizer_cls_and_kwargs` argument. This provides flexibility to configure optimizers with specific parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/trainer.md#_snippet_14

LANGUAGE: python
CODE:
```
import torch

optimizer_cls = torch.optim.AdamW
optimizer_kwargs = {
    "lr": 4e-3ØŒ
    "betas": (0.9ØŒ 0.999) ØŒ
    "weight_decay": 0.05ØŒ
}

from transformers import Trainer
trainer = Trainer(..., optimizer_cls_and_kwargs=(optimizer_cls, optimizer_kwargs))
```

----------------------------------------

TITLE: Translation with mBART Source/Target Language Configuration
DESCRIPTION: Example showing how to configure mBART for English to Romanian translation with specific language token handling and bfloat16 precision.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mbart.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-en-ro", torch_dtype=torch.bfloat16, attn_implementation="sdpa", device_map="auto")
tokenizer = MBartTokenizer.from_pretrained("facebook/mbart-large-en-ro", src_lang="en_XX")

article = "UN Chief Says There Is No Military Solution in Syria"
inputs = tokenizer(article, return_tensors="pt")

translated_tokens = model.generate(**inputs, decoder_start_token_id=tokenizer.lang_code_to_id["ro_RO"])
tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]
```

----------------------------------------

TITLE: TensorFlow Dataset Preparation
DESCRIPTION: Prepares the dataset for TensorFlow training with batching and shuffling options.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/training.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
tf_dataset = model.prepare_tf_dataset(
    dataset["train"], batch_size=16, shuffle=True, tokenizer=tokenizer
)
```

----------------------------------------

TITLE: Formatting Text Inputs for Pipeline with Chat Templates
DESCRIPTION: This snippet demonstrates formatting text inputs for the pipeline using a chat template. It structures the input as a conversation with a user role including an image and associated text. The assistant role includes generated text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_text_to_text.md#_snippet_7

LANGUAGE: python
CODE:
```
messages = [
     {
         "role": "user",
         "content": [
             {
                 "type": "image",
                 "image": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg",
             },
             {"type": "text", "text": "Describe this image."},
         ],
     },
     {
         "role": "assistant",
         "content": [
             {"type": "text", "text": "There's a pink flower"},
         ],
     },
 ]
```

----------------------------------------

TITLE: Launching multi-GPU training with PyTorch
DESCRIPTION: Command to launch multi-GPU training using PyTorch's distributed run with DeepSpeed integration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/deepspeed.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
torch.distributed.run --nproc_per_node=2 your_program.py <normal cl args> --deepspeed ds_config.json
```

----------------------------------------

TITLE: Creating a Python Virtual Environment
DESCRIPTION: This command creates a virtual environment in the project directory. It isolates project dependencies and prevents conflicts.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/installation.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
"python -m venv .env"
```

----------------------------------------

TITLE: Launching a Gradio interface with sharing enabled
DESCRIPTION: This code snippet demonstrates how to launch a Gradio interface with sharing enabled, which generates a temporary public link for others to access the app. Setting `share=True` allows users to share their machine learning application with others without hosting it on a dedicated server.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_gradio.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
gr.Interface.from_pipeline(pipeline).launch(share=True)
```

----------------------------------------

TITLE: RoBERTa Model and Masked LM Head Definition (Python)
DESCRIPTION: This code defines RobertaModel and RobertaForMaskedLM, inheriting from BertModel and BertForMaskedLM. It overrides the embeddings layer in RobertaModel with the custom RobertaEmbeddings defined previously, demonstrating how Modular Transformers allow for specific component customization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/modular_transformers.md#_snippet_1

LANGUAGE: Python
CODE:
```
# Ù†Ù…ÙˆØ°Ø¬ RoBERTa Ù…Ø·Ø§Ø¨Ù‚ Ù„Ù†Ù…ÙˆØ°Ø¬ BERTØŒ Ø¨Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø¶Ø§ÙØ§Øª.
# Ù†Ø¹ÙŠØ¯ ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø¥Ø¶Ø§ÙØ§Øª Ø£Ø¹Ù„Ø§Ù‡ØŒ Ù„Ø°Ø§ Ù‡Ù†Ø§ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø­Ø§Ø¬Ø© Ù„Ø¹Ù…Ù„ Ø¥Ø¶Ø§ÙÙŠ
class RobertaModel(BertModel):
  def __init__(self, config):
    super().__init__(config)
    self.embeddings = RobertaEmbeddings(config)

      
# Ø§Ù„Ø±Ø¤ÙˆØ³ Ø§Ù„Ø¢Ù† ØªØ­ØªØ§Ø¬ ÙÙ‚Ø· Ø¥Ù„Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¯Ø§Ø®Ù„ `RobertaModel` Ø§Ù„ØµØ­ÙŠØ­
class RobertaForMaskedLM(BertForMaskedLM):
  def __init__(self, config):
    super().__init__(config)
    self.model = RobertaModel(config)
```

----------------------------------------

TITLE: Initializing Image Transformations with torchvision in Python
DESCRIPTION: Sets up image normalization and data augmentation using torchvision's Compose, Normalize, RandomResizedCrop, ColorJitter, and ToTensor transforms.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/preprocessing.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
from torchvision.transforms import Compose, Normalize, RandomResizedCrop, ColorJitter, ToTensor

normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)
_transforms = Compose(
    [RandomResizedCrop(image_processor.size["height"]), ColorJitter(brightness=0.5, hue=0.5), ToTensor(), normalize]
)
```

----------------------------------------

TITLE: Setting Up Optimizer for TensorFlow Implementation
DESCRIPTION: Configures the AdamWeightDecay optimizer for TensorFlow training with specific learning rate and weight decay settings, which will be used to update model parameters during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
from transformers import create_optimizer, AdamWeightDecay

optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

----------------------------------------

TITLE: Logging in to Hugging Face Hub via CLI
DESCRIPTION: Command to log in to Hugging Face Hub using the command-line interface.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/custom_models.md#2025-04-22_snippet_7

LANGUAGE: bash
CODE:
```
huggingface-cli login
```

----------------------------------------

TITLE: Creating and Training a Tokenizer with ðŸ¤— Tokenizers
DESCRIPTION: This code snippet demonstrates how to create and train a tokenizer using the ðŸ¤— Tokenizers library. It initializes a BPE tokenizer, sets up a trainer with special tokens, configures a whitespace pre-tokenizer, and trains the tokenizer on a set of files. The trained tokenizer can then be used or saved for later use.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/fast_tokenizers.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> from tokenizers import Tokenizer
>>> from tokenizers.models import BPE
>>> from tokenizers.trainers import BpeTrainer
>>> from tokenizers.pre_tokenizers import Whitespace

>>> tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
>>> trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

>>> tokenizer.pre_tokenizer = Whitespace()
>>> files = [...]
>>> tokenizer.train(files, trainer)
```

----------------------------------------

TITLE: PushToHubCallback for TensorFlow - Python
DESCRIPTION: This snippet demonstrates how to use `PushToHubCallback` in TensorFlow to push the trained model to the Hugging Face Hub.  It requires specifying the output directory, tokenizer, and `hub_model_id`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/model_sharing.md#_snippet_9

LANGUAGE: python
CODE:
```
>>> from transformers import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="./your_model_save_path", tokenizer=tokenizer, hub_model_id="your-username/my-awesome-model"
... )
```

----------------------------------------

TITLE: Using HQQQuantizedCache for Memory-Efficient Generation in Python
DESCRIPTION: This snippet demonstrates how to use HQQQuantizedCache for quantized caching in model generation. It initializes a tokenizer and model, then generates text using quantized cache with specific axis configurations for the HQQ backend.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/kv_cache.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM, HQQQuantizedCache, QuantizedCacheConfig

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf", torch_dtype=torch.float16).to("cuda:0")
inputs = tokenizer("I like rock music because", return_tensors="pt").to(model.device)

out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation="quantized", cache_config={"axis-key": 1, "axis-value": 1, "backend": "hqq"})
print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])
```

----------------------------------------

TITLE: Training T5 Model with Keras fit Method in TensorFlow
DESCRIPTION: Executes the training of the T5 model using the Keras fit method, specifying datasets, epochs, and callbacks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/translation.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)
```

----------------------------------------

TITLE: Defining Data Augmentation for TensorFlow
DESCRIPTION: Creates Keras preprocessing layers for data augmentation in TensorFlow, with separate augmentations for training and validation data.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/image_classification.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> from tensorflow import keras
>>> from tensorflow.keras import layers

>>> size = (image_processor.size["height"], image_processor.size["width"])

>>> train_data_augmentation = keras.Sequential(
...     [
...         layers.RandomCrop(size[0], size[1]),
...         layers.Rescaling(scale=1.0 / 127.5, offset=-1),
...         layers.RandomFlip("horizontal"),
...         layers.RandomRotation(factor=0.02),
...         layers.RandomZoom(height_factor=0.2, width_factor=0.2),
...     ],
...     name="train_data_augmentation",
... )

>>> val_data_augmentation = keras.Sequential(
...     [
...         layers.CenterCrop(size[0], size[1]),
...         layers.Rescaling(scale=1.0 / 127.5, offset=-1),
...     ],
...     name="val_data_augmentation",
... )
```

----------------------------------------

TITLE: Configuring ZeRO-3 Optimization in YAML
DESCRIPTION: This YAML configuration sets up ZeRO stage 3, meant for both training and inference, by sharding optimizer, gradient states, and parameters. Features `offload_optimizer`, `offload_param`, `sub_group_size`, and stage 3-specific parameters to handle large models using multiple GPUs. Requires DeepSpeed library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/deepspeed.md#2025-04-22_snippet_7

LANGUAGE: yaml
CODE:
```
{
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    }
}
```

----------------------------------------

TITLE: Hugging Face CLI Login
DESCRIPTION: Logs into the Hugging Face account using the `huggingface-cli` tool. This is required before pushing a model to the Hugging Face Model Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/run_scripts.md#_snippet_17

LANGUAGE: bash
CODE:
```
huggingface-cli login
```

----------------------------------------

TITLE: Specify Inference Backend with AutoRound in Transformers (Python)
DESCRIPTION: This code snippet demonstrates how to specify a preferred inference backend (e.g., "ipex" for CPU) when using AutoRound with Hugging Face Transformers. It loads a quantized model, sets the `quantization_config` with the desired backend, and performs inference. The example requires the `transformers` library to be installed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/auto_round.md#_snippet_8

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoRoundConfig

model_name = "OPEA/Qwen2.5-1.5B-Instruct-int4-sym-inc"
quantization_config = AutoRoundConfig(backend="ipex")
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="cpu", quantization_config=quantization_config, torch_dtype="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)
text = "There is a girl who likes adventure,"
inputs = tokenizer(text, return_tensors="pt").to(model.device)
print(tokenizer.decode(model.generate(**inputs, max_new_tokens=50, do_sample=False)[0]))
```

----------------------------------------

TITLE: Using GaLore with SFTTrainer
DESCRIPTION: This snippet shows how to integrate GaLore with the SFTTrainer by specifying the `optim` argument in `TrainingArguments` and providing the target modules to adapt using `optim_target_modules`. GaLore provides memory-efficient low-rank training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/trainer.md#_snippet_16

LANGUAGE: python
CODE:
```
import torch
import datasets
import trl

from transformers import TrainingArguments, AutoConfig, AutoTokenizer, AutoModelForCausalLM

train_dataset = datasets.load_dataset('imdb'ØŒ split='train')

args = TrainingArguments(
    output_dir="./test-galore"ØŒ
    max_steps=100ØŒ
    per_device_train_batch_size=2ØŒ
    optim="galore_adamw"ØŒ
    optim_target_modules=[r".*.attn.*"ØŒ r".*.mlp.*"]
)

model_id = "google/gemma-2b"

config = AutoConfig.from_pretrained(model_id)

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_config(config).to(0)

trainer = trl.SFTTrainer(
    model=modelØŒ 
    args=argsØŒ
    train_dataset=train_datasetØŒ
    dataset_text_field='text'ØŒ
    max_seq_length=512ØŒ
)

trainer.train()
```

----------------------------------------

TITLE: Speech Recognition Pipeline
DESCRIPTION: Implementation of speech recognition using the wav2vec2 model and MInDS-14 dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quicktour.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> import torch
>>> from transformers import pipeline

>>> speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))
>>> result = speech_recognizer(dataset[:4]["audio"])
>>> print([d["text"] for d in result])
```

----------------------------------------

TITLE: Translating with T5 using Transformers CLI (Bash)
DESCRIPTION: Illustrates how to use the transformers library's command-line interface (CLI) to execute a text-to-text generation task. It pipes the input text (including the translation task prefix) into the 'transformers run' command, specifying the desired task ('text2text-generation'), the model ('google-t5/t5-base'), and the computation device.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/t5.md#_snippet_2

LANGUAGE: bash
CODE:
```
echo -e "translate English to French: The weather is nice today." | transformers run --task text2text-generation --model google-t5/t5-base --device 0
```

----------------------------------------

TITLE: Example Data Extraction from ELI5 Dataset
DESCRIPTION: This Python snippet shows how to access and display the first example from the ELI5 dataset after it has been loaded, highlighting the structure of the dataset entries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> eli5["train"][0]
{'q_id': '7h191n', 'title': 'What does the tax bill that was passed today mean? How will it affect Americans in each tax bracket?', 'selftext': '', 'category': 'Economics', 'subreddit': 'explainlikeimfive', 'answers': {'a_id': ['dqnds8l', 'dqnd1jl', 'dqng3i1', 'dqnku5x'], 'text': ["The tax bill is 500 pages long and there were a lot of changes still going on right to the end. It's not just an adjustment to the income tax brackets, it's a whole bunch of changes. As such there is no good answer to your question. The big take aways are: - Big reduction in corporate income tax rate will make large companies very happy. - Pass through rate change will make certain styles of business (law firms, hedge funds) extremely happy - Income tax changes are moderate, and are set to expire (though it's the kind of thing that might just always get re-applied without being made permanent) - People in high tax states (California, New York) lose out, and many of them will end up with their taxes raised.", 'None yet. It has to be reconciled with a vastly different house bill and then passed again.', 'Also: does this apply to 2017 taxes? Or does it start with 2018 taxes?', 'This article explains both the House and senate bills, including the proposed changes to your income taxes based on your income level. URL_0'], 'score': [21, 19, 5, 3], 'text_urls': [[], [], [], ['https://www.investopedia.com/news/trumps-tax-reform-what-can-be-done/']]}, 'title_urls': ['url'], 'selftext_urls': ['url']}
```

----------------------------------------

TITLE: Saving generated audio to a WAV file
DESCRIPTION: This code saves the generated audio to a `.wav` file using the `soundfile` library. It extracts the audio data and sampling rate from the model configuration and writes the audio to the specified file. The soundfile library needs to be installed beforehand.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/musicgen_melody.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> import soundfile as sf

>>> sampling_rate = model.config.audio_encoder.sampling_rate
>>> sf.write("musicgen_out.wav", audio_values[0].T.numpy(), sampling_rate)
```

----------------------------------------

TITLE: Loading IMDb Dataset - Python
DESCRIPTION: This code snippet loads the IMDb dataset from the datasets library and retrieves a sample entry containing a review and label. It illustrates how to access the dataset programmatically.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from datasets import load_dataset

imdb = load_dataset("imdb")
```

----------------------------------------

TITLE: Carregando e preparando dataset de Ã¡udio para reconhecimento de fala em Python
DESCRIPTION: Mostra como carregar um dataset de Ã¡udio e preparÃ¡-lo para uso com o pipeline de reconhecimento de fala.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/quicktour.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")  # doctest: +IGNORE_RESULT
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))
```

----------------------------------------

TITLE: Launching multi-GPU training with DeepSpeed
DESCRIPTION: Command to launch multi-GPU training using DeepSpeed's launcher.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/deepspeed.md#2025-04-22_snippet_7

LANGUAGE: bash
CODE:
```
deepspeed --num_gpus=2 your_program.py <normal cl args> --deepspeed ds_config.json
```

----------------------------------------

TITLE: Share Model to Hugging Face Hub
DESCRIPTION: This command runs the `run_summarization.py` script and pushes the trained model to the Hugging Face Hub. The `push_to_hub` argument enables model sharing, and `push_to_hub_model_id` specifies the repository name on the Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/run_scripts.md#_snippet_18

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --push_to_hub \
    --push_to_hub_model_id finetuned-t5-cnn_dailymail \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Multinomial Sampling with Transformers
DESCRIPTION: This snippet shows how to use multinomial sampling for text generation. It initializes a tokenizer and causal language model, encodes input text, and generates text using the `model.generate` function with `do_sample=True` and `num_beams=1`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/generation_strategies.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
inputs = tokenizer("Hugging Face is an open-source company", return_tensors="pt").to("cuda")

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", torch_dtype=torch.float16).to("cuda")
# explicitly set to 100 because Llama2 generation length is 4096
outputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, num_beams=1)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
'Hugging Face is an open-source company ðŸ¤—\nWe are open-source and believe that open-source is the best way to build technology. Our mission is to make AI accessible to everyone, and we believe that open-source is the best way to achieve that.'
```

----------------------------------------

TITLE: Audio Classification with Hubert Model in Python
DESCRIPTION: Code snippet demonstrating how to use the pipeline API for audio classification with a pre-trained Hubert model to detect emotions in speech.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/task_summary.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> classifier = pipeline(task="audio-classification", model="superb/hubert-base-superb-er")
>>> preds = classifier("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.4532, 'label': 'hap'},
 {'score': 0.3622, 'label': 'sad'},
 {'score': 0.0943, 'label': 'neu'},
 {'score': 0.0903, 'label': 'ang'}]
```

----------------------------------------

TITLE: TensorFlow Manual Summarization - Generation
DESCRIPTION: Demonstrates how to generate a summary using TensorFlow model's generate method with specific parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_29

LANGUAGE: python
CODE:
```
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained("username/my_awesome_billsum_model")
outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)
```

----------------------------------------

TITLE: Setting Dataset Format to PyTorch
DESCRIPTION: This code snippet sets the format of the Hugging Face Dataset to return PyTorch tensors instead of lists. This is required to work with PyTorch's DataLoader and training loop.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_17

LANGUAGE: Python
CODE:
```
tokenized_datasets.set_format("torch")
```

----------------------------------------

TITLE: Running Model With Flash Attention
DESCRIPTION: This code runs the model again, this time with Flash Attention enabled, to compare performance and memory usage.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_22

LANGUAGE: python
CODE:
```
start_time = time.time()
```

----------------------------------------

TITLE: Chatting with Cohere Command-R via Transformers CLI
DESCRIPTION: This bash command demonstrates how to interact with the Cohere Command-R model directly from the command line using the `transformers chat` tool. It allows quick testing of the model with options for specifying `torch_dtype` and attention implementation like FlashAttention-2. Requires the `transformers` library installed with CLI capabilities and potentially `flash-attn`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/cohere.md#_snippet_2

LANGUAGE: bash
CODE:
```
# pip install -U flash-attn --no-build-isolation
transformers chat CohereForAI/c4ai-command-r-v01 --torch_dtype auto --attn_implementation flash_attention_2
```

----------------------------------------

TITLE: Disabling GPU Usage with CUDA_VISIBLE_DEVICES
DESCRIPTION: This snippet indicates how to set the CUDA_VISIBLE_DEVICES environment variable to an empty value to create an environment that does not utilize any GPUs during the training process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/gpu_selection.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
CUDA_VISIBLE_DEVICES= python trainer-program.py ...
```

----------------------------------------

TITLE: Generating Text with Mamba2 Model in Python
DESCRIPTION: This snippet demonstrates how to load a pre-trained Mamba2 model and tokenizer, and use them to generate text based on a given input prompt.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/mamba2.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import Mamba2Config, Mamba2ForCausalLM, AutoTokenizer
import torch
model_id = 'mistralai/Mamba-Codestral-7B-v0.1'
tokenizer = AutoTokenizer.from_pretrained(model_id, revision='refs/pr/9', from_slow=True, legacy=False)
model = Mamba2ForCausalLM.from_pretrained(model_id, revision='refs/pr/9')
input_ids = tokenizer("Hey how are you doing?", return_tensors= "pt")["input_ids"]

out = model.generate(input_ids, max_new_tokens=10)
print(tokenizer.batch_decode(out))
```

----------------------------------------

TITLE: Performing Manual Zero-shot Image Classification
DESCRIPTION: Runs inference on the processed inputs and post-processes the results to obtain classification scores.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/zero_shot_image_classification.md#2025-04-23_snippet_6

LANGUAGE: python
CODE:
```
>>> import torch

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> logits = outputs.logits_per_image[0]
>>> probs = logits.softmax(dim=-1).numpy()
>>> scores = probs.tolist()

>>> result = [
...     {"score": score, "label": candidate_label}
...     for score, candidate_label in sorted(zip(probs, candidate_labels), key=lambda x: -x[0])
... ]

>>> result
```

----------------------------------------

TITLE: Loading Model-Specific Image Processor - Python
DESCRIPTION: This code snippet shows how to load a model-specific image processor for 'google/vit-base-patch16-224'. It demonstrates the straightforward process of acquiring a pre-configured processor tailored to a specific vision model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/image_processors.md#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
from transformers import ViTImageProcessor

image_processor = ViTImageProcessor.from_pretrained("google/vit-base-patch16-224")
```

----------------------------------------

TITLE: Training TensorFlow Model with PushToHubCallback in Python
DESCRIPTION: Shows how to include the PushToHubCallback in the model.fit method for automatic model sharing during TensorFlow training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/model_sharing.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3, callbacks=push_to_hub_callback)
```

----------------------------------------

TITLE: Training GPT-2 with Custom Training Loop
DESCRIPTION: Fine-tunes GPT-2 on WikiText-2 using a custom training loop without the Trainer API. Uses ðŸ¤— Accelerate library for training implementation.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
python run_clm_no_trainer.py \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \
    --model_name_or_path openai-community/gpt2 \
    --output_dir /tmp/test-clm
```

----------------------------------------

TITLE: Running Summarization Example (PyTorch)
DESCRIPTION: Command to run the PyTorch summarization example script, fine-tuning T5-small on the CNN/DailyMail dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/run_scripts.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Multi-GPU Deployment with DeepSpeed
DESCRIPTION: Deploy a translation model training script on two GPUs using the DeepSpeed launcher with a specific configuration file
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/deepspeed.md#2025-04-22_snippet_24

LANGUAGE: bash
CODE:
```
deepspeed --num_gpus=2 examples/pytorch/translation/run_translation.py \
--deepspeed tests/deepspeed/ds_config_zero3.json \
--model_name_or_path google-t5/t5-small --per_device_train_batch_size 1 \
--output_dir output_dir --overwrite_output_dir --fp16 \
--do_train --max_train_samples 500 --num_train_epochs 1 \
--dataset_name wmt16 --dataset_config "ro-en" \
--source_lang en --target_lang ro
```

----------------------------------------

TITLE: Checking GPU Device Capability for PyTorch
DESCRIPTION: Determines the CUDA compute capability of a specific GPU, indicating its architecture with respect to CUDA support for model training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/debugging.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
CUDA_VISIBLE_DEVICES=0 python -c "import torch; print(torch.cuda.get_device_capability())"
```

----------------------------------------

TITLE: Preprocessing with DistilBERT Tokenizer
DESCRIPTION: Initialize tokenizer and create preprocessing function for handling question-answer pairs with context
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/question_answering.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
```

LANGUAGE: python
CODE:
```
>>> def preprocess_function(examples):
...     questions = [q.strip() for q in examples["question"]]
...     inputs = tokenizer(
...         questions,
...         examples["context"],
...         max_length=384,
...         truncation="only_second",
...         return_offsets_mapping=True,
...         padding="max_length",
...     )

...     offset_mapping = inputs.pop("offset_mapping")
...     answers = examples["answers"]
...     start_positions = []
...     end_positions = []

...     for i, offset in enumerate(offset_mapping):
...         answer = answers[i]
...         start_char = answer["answer_start"][0]
...         end_char = answer["answer_start"][0] + len(answer["text"][0])
...         sequence_ids = inputs.sequence_ids(i)

...         # Find the start and end of the context
...         idx = 0
...         while sequence_ids[idx] != 1:
...             idx += 1
...         context_start = idx
...         while sequence_ids[idx] == 1:
...             idx += 1
...         context_end = idx - 1

...         # If the answer is not fully inside the context, label it (0, 0)
...         if offset[context_start][0] > end_char or offset[context_end][1] < start_char:
...             start_positions.append(0)
...             end_positions.append(0)
...         else:
...             # Otherwise it's the start and end token positions
...             idx = context_start
...             while idx <= context_end and offset[idx][0] <= start_char:
...                 idx += 1
...             start_positions.append(idx - 1)

...             idx = context_end
...             while idx >= context_start and offset[idx][1] >= end_char:
...                 idx -= 1
...             end_positions.append(idx + 1)

...     inputs["start_positions"] = start_positions
...     inputs["end_positions"] = end_positions
...     return inputs
```

----------------------------------------

TITLE: Model Training and Evaluation Setup
DESCRIPTION: Configures training arguments, initializes teacher and student models, and sets up evaluation metrics for knowledge distillation
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/knowledge_distillation_for_image_classification.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
training_args = TrainingArguments(
    output_dir="my-awesome-model",
    num_train_epochs=30,
    fp16=True,
    logging_dir=f"{repo_name}/logs",
    logging_strategy="epoch",
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    report_to="tensorboard",
    push_to_hub=True,
    hub_strategy="every_save",
    hub_model_id=repo_name,
)

num_labels = len(processed_datasets["train"].features["labels"].names)

teacher_model = AutoModelForImageClassification.from_pretrained(
    "merve/beans-vit-224",
    num_labels=num_labels,
    ignore_mismatched_sizes=True
)

student_config = MobileNetV2Config()
student_config.num_labels = num_labels
student_model = MobileNetV2ForImageClassification(student_config)
```

----------------------------------------

TITLE: Loading Pre-trained Model (TensorFlow)
DESCRIPTION: This code loads a pre-trained TFDistilBertModel for TensorFlow. It uses the `from_pretrained` method to load the model's weights and configuration. Requires the `TFDistilBertModel` class from the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/create_a_model.md#_snippet_9

LANGUAGE: python
CODE:
```
>>> tf_model = TFDistilBertModel.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Running Image-Guided Detection with OWL-ViT and Visualizing Results
DESCRIPTION: Code that performs image-guided detection using the OWL-ViT model, post-processes the results, and visualizes the detected objects by drawing bounding boxes on the target image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> with torch.no_grad():
...     outputs = model.image_guided_detection(**inputs)
...     target_sizes = torch.tensor([image_target.size[::-1]])
...     results = processor.post_process_image_guided_detection(outputs=outputs, target_sizes=target_sizes)[0]

>>> draw = ImageDraw.Draw(image_target)

>>> scores = results["scores"].tolist()
>>> boxes = results["boxes"].tolist()

>>> for box, score in zip(boxes, scores):
...     xmin, ymin, xmax, ymax = box
...     draw.rectangle((xmin, ymin, xmax, ymax), outline="white", width=4)

>>> image_target
```

----------------------------------------

TITLE: Setting Up TensorFlow Callbacks for Metrics and Model Sharing
DESCRIPTION: This code creates Keras callbacks to calculate evaluation metrics during training and to push the model to the Hugging Face Hub after training completes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/translation.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
>>> from transformers.keras_callbacks import KerasMetricCallback

>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)
```

----------------------------------------

TITLE: Loading and Performing Inference with Trained Semantic Segmentation Model in Python
DESCRIPTION: This Python snippet demonstrates how to load a trained semantic segmentation model and image processor, and perform inference on an image. It includes steps for processing the input, running the model, and post-processing the output.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/semantic-segmentation/README.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoImageProcessor, AutoModelForSemanticSegmentation
from PIL import Image
import requests
import torch

model_name = "name_of_repo_on_the_hub_or_path_to_local_folder"

image_processor = AutoImageProcessor.from_pretrained(model_name)
model = AutoModelForSemanticSegmentation.from_pretrained(model_name)

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# prepare image for the model
inputs = image_processor(images=image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits

# rescale logits to original image size
logits = nn.functional.interpolate(outputs.logits.detach().cpu(),
                                    size=image.size[::-1], # (height, width)
                                    mode='bilinear',
                                    align_corners=False)

predicted = logits.argmax(1)
```

----------------------------------------

TITLE: Using XLNetTokenizer with SentencePiece in Python
DESCRIPTION: Demonstrates XLNet's tokenization which uses SentencePiece, showing how it handles spaces with the special 'â–' symbol and splits rare words into subwords.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/tokenizer_summary.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import XLNetTokenizer

>>> tokenizer = XLNetTokenizer.from_pretrained("xlnet/xlnet-base-cased")
>>> tokenizer.tokenize("Don't you love ðŸ¤— Transformers? We sure do.")
["â–Don", "'", "t", "â–you", "â–love", "â–", "ðŸ¤—", "â–", "Transform", "ers", "?", "â–We", "â–sure", "â–do", "."]
```

----------------------------------------

TITLE: Installing Transformers Library via Pip
DESCRIPTION: Commands to install the Transformers library using pip, including options for specific deep learning frameworks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/installation.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
pip install transformers
```

LANGUAGE: bash
CODE:
```
pip install transformers[torch]
```

LANGUAGE: bash
CODE:
```
pip install transformers[tf-cpu]
```

LANGUAGE: bash
CODE:
```
pip install transformers[flax]
```

----------------------------------------

TITLE: Single Media Inference with SmolVLM in Python
DESCRIPTION: Illustrates how to perform inference on a single image or video using SmolVLM. It includes loading the model, processing inputs, and generating text outputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/smolvlm.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoProcessor, AutoModelForImageTextToText

processor = AutoProcessor.from_pretrained("HuggingFaceTB/SmolVLM2-256M-Video-Instruct")
model = AutoModelForImageTextToText.from_pretrained(
    "HuggingFaceTB/SmolVLM2-256M-Video-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="cuda"
)

conversation = [
    {
        "role": "user",
        "content":[
            {"type": "image", "url": "http://images.cocodataset.org/val2017/000000039769.jpg"},
            {"type": "text", "text": "Describe this image."}
        ]
    }
]

inputs = processor.apply_chat_template(
    conversation,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt",
).to(model.device, dtype=torch.bfloat16)

output_ids = model.generate(**inputs, max_new_tokens=128)
generated_texts = processor.batch_decode(output_ids, skip_special_tokens=True)
print(generated_texts)


# Video
conversation = [
    {
        "role": "user",
        "content": [
            {"type": "video", "path": "/path/to/video.mp4"},
            {"type": "text", "text": "Describe this video in detail"}
        ]
    },
]

inputs = processor.apply_chat_template(
    conversation,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt",
).to(model.device, dtype=torch.bfloat16)

generated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=100)
generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)
print(generated_texts[0])
```

----------------------------------------

TITLE: Implementing and Registering a Custom Callback with PyTorch Trainer
DESCRIPTION: Example showing how to create a custom callback class that extends TrainerCallback and register it with the Trainer. This callback prints a message at the beginning of training and demonstrates the pattern for implementing custom training behaviors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/callback.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
class MyCallback(TrainerCallback):
    "A callback that prints a message at the beginning of training"

    def on_train_begin(self, args, state, control, **kwargs):
        print("Starting training")


trainer = Trainer(
    model,
    args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    callbacks=[MyCallback],  # We can either pass the callback class this way or an instance of it (MyCallback())
)
```

----------------------------------------

TITLE: Quantizing Large Models with Memory Constraints
DESCRIPTION: Demonstrates how to quantize large models by allocating specific memory to different devices to avoid out-of-memory errors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/gptq.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", max_memory={0: "30GiB", 1: "46GiB", "cpu": "30GiB"}, quantization_config=gptq_config)
```

----------------------------------------

TITLE: Using CLVP Model for Conditional Generation in Python
DESCRIPTION: Example code demonstrating how to use CLVP processor and model for conditional text-to-speech generation. Shows loading audio data, processing text and audio inputs, and generating output using the CLVP model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/clvp.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import datasets
from transformers import ClvpProcessor, ClvpModelForConditionalGeneration

# Define the Text and Load the Audio (We are taking an audio example from HuggingFace Hub using `datasets` library).
text = "This is an example text."

ds = datasets.load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
ds = ds.cast_column("audio", datasets.Audio(sampling_rate=22050))
sample = ds[0]["audio"]

# Define processor and model.
processor = ClvpProcessor.from_pretrained("susnato/clvp_dev")
model = ClvpModelForConditionalGeneration.from_pretrained("susnato/clvp_dev")

# Generate processor output and model output.
processor_output = processor(raw_speech=sample["array"], sampling_rate=sample["sampling_rate"], text=text, return_tensors="pt")
generated_output = model.generate(**processor_output)
```

----------------------------------------

TITLE: Launching distributed training with Accelerate
DESCRIPTION: This bash command demonstrates how to launch a distributed training job using Accelerate. It sets up the environment variable for the dataset name and specifies various training parameters such as model, dataset, sequence length, batch size, learning rate, and number of epochs.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/multiple-choice/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
export DATASET_NAME=swag

accelerate launch run_swag_no_trainer.py \
  --model_name_or_path google-bert/bert-base-cased \
  --dataset_name $DATASET_NAME \
  --max_seq_length 128 \
  --per_device_train_batch_size 32 \
  --learning_rate 2e-5 \
  --num_train_epochs 3 \
  --output_dir /tmp/$DATASET_NAME/
```

----------------------------------------

TITLE: Initializing GraniteMoe Model with HuggingFace Transformers
DESCRIPTION: Demonstrates how to load the PowerMoE-3b model, tokenize input, generate text, and decode output tokens using the Hugging Face Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/granitemoe.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = "ibm/PowerMoE-3b"
tokenizer = AutoTokenizer.from_pretrained(model_path)

# drop device_map if running on CPU
model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto")
model.eval()

# change input text as desired
prompt = "Write a code to find the maximum value in a list of numbers."

# tokenize the text
input_tokens = tokenizer(prompt, return_tensors="pt")
# generate output tokens
output = model.generate(**input_tokens, max_new_tokens=100)
# decode output tokens into text
output = tokenizer.batch_decode(output)
# loop over the batch to print, in this example the batch size is 1
for i in output:
    print(i)
```

----------------------------------------

TITLE: Loading Pre-trained DistilBert Model in TensorFlow
DESCRIPTION: Shows how to load a pre-trained DistilBert model with its weights and configuration in TensorFlow for better performance without training from scratch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/create_a_model.md#2025-04-23_snippet_9

LANGUAGE: python
CODE:
```
>>> tf_model = TFDistilBertModel.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Applying Chat Template and Generating Text with Qwen2.5-Omni (Python)
DESCRIPTION: Demonstrates how to format a multimodal conversation list using the Qwen2.5-Omni processor's `apply_chat_template` and generate text output using the model's `generate` method, finally decoding the result. Requires a loaded `processor` and `model` objects.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2_5_omni.md#_snippet_3

LANGUAGE: python
CODE:
```
conversation4 = [
    {
        "role": "system",
        "content": [
            {"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."}
        ],
    },
    {
        "role": "user",
        "content": [
            {"type": "image", "path": "/path/to/image.jpg"},
            {"type": "video", "path": "/path/to/video.mp4"},
            {"type": "audio", "path": "/path/to/audio.wav"},
            {"type": "text", "text": "What are the elements can you see and hear in these medias?"},
        ],
    }
]

conversations = [conversation1, conversation2, conversation3, conversation4]

inputs = processor.apply_chat_template(
    conversations,
    load_audio_from_video=True,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt",
    video_fps=1,

    # kwargs to be passed to `Qwen2-5-OmniProcessor`
    padding=True,
    use_audio_in_video=True,
).to(model.thinker.device)

text_ids = model.generate(**inputs, use_audio_in_video=True)
text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)

print(text)
```

----------------------------------------

TITLE: Registering Custom Models for Auto Classes in Python
DESCRIPTION: Demonstrates how to register custom configuration and model classes with the Transformers Auto classes for automatic loading and usage.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/custom_models.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
ResnetConfig.register_for_auto_class()
ResnetModel.register_for_auto_class("AutoModel")
ResnetModelForImageClassification.register_for_auto_class("AutoModelForImageClassification")
```

----------------------------------------

TITLE: Draw Bounding Boxes on Target Image
DESCRIPTION: Draws the predicted bounding boxes on the target image. It iterates through the detected boxes and scores, drawing a rectangle for each detected object.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/zero_shot_object_detection.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
>>> draw = ImageDraw.Draw(image_target)

>>> scores = results["scores"].tolist()
>>> boxes = results["boxes"].tolist()

>>> for box, score, label in zip(boxes, scores, labels):
...     xmin, ymin, xmax, ymax = box
...     draw.rectangle((xmin, ymin, xmax, ymax), outline="white", width=4)

>>> image_target
```

----------------------------------------

TITLE: Configuring Push to Hub Callback for TensorFlow
DESCRIPTION: Sets up a PushToHubCallback that will automatically upload the model to Hugging Face Hub after training. This makes the trained model available for others to use.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
from transformers.keras_callbacks import PushToHubCallback

push_to_hub_callback = PushToHubCallback(
    output_dir="my_awesome_billsum_model",
    tokenizer=tokenizer,
)
```

----------------------------------------

TITLE: Loading Custom Tokenizer Vocabulary in Python
DESCRIPTION: This code snippet illustrates how to load a custom tokenizer from a specified vocabulary file using the AutoTokenizer class. It highlights the flexibility of tokenizers for handling custom vocabularies.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/fast_tokenizers.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("./model_directory/my_vocab_file.txt")
```

----------------------------------------

TITLE: Running ONNX Model with ONNX Runtime
DESCRIPTION: Python code demonstrating how to load and run an exported ONNX model using ONNX Runtime.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/serialization.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer
>>> from onnxruntime import InferenceSession

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
>>> session = InferenceSession("onnx/model.onnx")
>>> # ONNX Runtime expects NumPy arrays as input
>>> inputs = tokenizer("Using DistilBERT with ONNX Runtime!", return_tensors="np")
>>> outputs = session.run(output_names=["last_hidden_state"], input_feed=dict(inputs))
```

----------------------------------------

TITLE: Saving and Loading Tokenizer from JSON
DESCRIPTION: Shows how to save a tokenizer to a JSON file and then load it back using PreTrainedTokenizerFast, enabling persistence and reuse of tokenizer configurations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/fast_tokenizers.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> tokenizer.save("tokenizer.json")
```

LANGUAGE: python
CODE:
```
>>> from transformers import PreTrainedTokenizerFast

>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file="tokenizer.json")
```

----------------------------------------

TITLE: Setting Up Data Collator for TensorFlow Training
DESCRIPTION: Creates a DataCollatorWithPadding instance for TensorFlow that handles dynamic padding of sequences and returns TensorFlow tensors for training in the TensorFlow framework.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorWithPadding

>>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```

----------------------------------------

TITLE: Loading Model with SDPA Implementation
DESCRIPTION: Load a causal language model with scaled dot product attention implementation using Hugging Face Transformers
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_one.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.1-8B", device_map="auto", attn_implementation="sdpa")
```

----------------------------------------

TITLE: Helper Functions for GPU Memory Monitoring
DESCRIPTION: Defines two utility functions: print_gpu_utilization to report current GPU memory usage and print_summary to display training metrics including runtime, samples per second, and GPU memory consumption.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_memory_anatomy.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from pynvml import *


>>> def print_gpu_utilization():
...     nvmlInit()
...     handle = nvmlDeviceGetHandleByIndex(0)
...     info = nvmlDeviceGetMemoryInfo(handle)
...     print(f"GPU memory occupied: {info.used//1024**2} MB.")


>>> def print_summary(result):
...     print(f"Time: {result.metrics['train_runtime']:.2f}")
...     print(f"Samples/second: {result.metrics['train_samples_per_second']:.2f}")
...     print_gpu_utilization()
```

----------------------------------------

TITLE: Loading Zero-shot Image Classification Model and Processor Manually
DESCRIPTION: Code to initialize a pre-trained zero-shot image classification model and its corresponding processor directly, providing more control over the classification process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/zero_shot_image_classification.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor, AutoModelForZeroShotImageClassification

>>> model = AutoModelForZeroShotImageClassification.from_pretrained(checkpoint)
>>> processor = AutoProcessor.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Loading DistilGPT2 Tokenizer in Python
DESCRIPTION: Initializes the DistilGPT2 tokenizer to process the text data from the ELI5 dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/language_modeling.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilgpt2")
```

----------------------------------------

TITLE: Implementing a Basic Custom Pipeline Class in Python
DESCRIPTION: Basic structure of a custom pipeline class inheriting from the Pipeline base class with the four required methods: _sanitize_parameters, preprocess, _forward, and postprocess. This example shows how to handle parameters and process inputs through the pipeline stages.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/add_new_pipeline.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import Pipeline


class MyPipeline(Pipeline):
    def _sanitize_parameters(self, **kwargs):
        preprocess_kwargs = {}
        if "maybe_arg" in kwargs:
            preprocess_kwargs["maybe_arg"] = kwargs["maybe_arg"]
        return preprocess_kwargs, {}, {}

    def preprocess(self, inputs, maybe_arg=2):
        model_input = Tensor(inputs["input_ids"])
        return {"model_input": model_input}

    def _forward(self, model_inputs):
        # model_inputs == {"model_input": model_input}
        outputs = self.model(**model_inputs)
        # QuizÃ¡ {"logits": Tensor(...)}
        return outputs

    def postprocess(self, model_outputs):
        best_class = model_outputs["logits"].softmax(-1)
        return best_class
```

----------------------------------------

TITLE: Using generate() Method with Blip2ForConditionalGeneration in Python
DESCRIPTION: Example of how to use the generate() method for conditional text generation with BLIP-2 model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/blip-2.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import Blip2ForConditionalGeneration

model = Blip2ForConditionalGeneration.from_pretrained("model_name")

# Generate text based on image and optional text prompt
output = model.generate(pixel_values=processed_image, prompt="Describe this image:")
```

----------------------------------------

TITLE: Pushing Model to Hub During Training in TensorFlow
DESCRIPTION: Shows how to push a model to the Hugging Face Hub during training using the PushToHubCallback in TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_sharing.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from transformers import PushToHubCallback

push_to_hub_callback = PushToHubCallback(
    output_dir="./your_model_save_path", tokenizer=tokenizer, hub_model_id="your-username/my-awesome-model"
)

model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3, callbacks=push_to_hub_callback)
```

----------------------------------------

TITLE: Preparing Tokenized Datasets for PyTorch Training
DESCRIPTION: Processes tokenized datasets by removing unnecessary columns, renaming labels, and setting the format to PyTorch tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/training.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
tokenized_datasets = tokenized_datasets.remove_columns(["text"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
```

----------------------------------------

TITLE: Accelerate Configuration for DistributedDataParallel (DDP) in YAML
DESCRIPTION: This YAML configuration file demonstrates how to set up Accelerate for training with DistributedDataParallel (DDP) on multiple GPUs. It specifies the compute environment, distributed type, GPU IDs, and network settings for distributed training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/trainer.md#_snippet_22

LANGUAGE: yaml
CODE:
```
compute_environment: LOCAL_MACHINE                                                                                             
distributed_type: MULTI_GPU                                                                                                    
downcast_bf16: 'no'
gpu_ids: all
machine_rank: 0 #change rank as per the node
main_process_ip: 192.168.20.1
main_process_port: 9898
main_training_function: main
mixed_precision: fp16
num_machines: 2
num_processes: 8
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

----------------------------------------

TITLE: Logging into Hugging Face Account
DESCRIPTION: Use the 'notebook_login' function from the 'huggingface_hub' library to log into a Hugging Face account. This allows for easier model sharing with the community.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/token_classification.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Defining a Basic Chat Template in Python
DESCRIPTION: A Jinja2 template that formats messages with start and end tokens, and includes the role for each message.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
{%- for message in messages %}
    {{- '<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n' }}
{%- endfor %}
```

----------------------------------------

TITLE: Loading AWQ Model with CPU to GPU Migration
DESCRIPTION: Loading an AWQ model to CPU first then moving it to GPU
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/quantization.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "TheBloke/zephyr-7B-alpha-AWQ"
model = AutoModelForCausalLM.from_pretrained(model_id).to("cuda:0")
```

----------------------------------------

TITLE: Loading Pre-trained Model with Custom Config (TensorFlow)
DESCRIPTION: This snippet demonstrates loading a pre-trained TFDistilBertModel with a custom configuration, overriding the default configuration. Requires the `TFDistilBertModel` class from the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/create_a_model.md#_snippet_10

LANGUAGE: python
CODE:
```
>>> tf_model = TFDistilBertModel.from_pretrained("distilbert/distilbert-base-uncased", config=my_config)
```

----------------------------------------

TITLE: Customizing TAPAS Model Initialization in Python
DESCRIPTION: In this snippet, a customized TAPAS model initialization is demonstrated where the classification heads can be configured for specific datasets that may require both conversational questions and aggregation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/tapas.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> from transformers import TapasConfig, TapasForQuestionAnswering

>>> # you can initialize the classification heads any way you want (see docs of TapasConfig)
>>> config = TapasConfig(num_aggregation_labels=3, average_logits_per_cell=True)
>>> # initializing the pre-trained base sized model with our custom classification heads
>>> model = TapasForQuestionAnswering.from_pretrained("google/tapas-base", config=config)

```

----------------------------------------

TITLE: Visualizing Object Detection Results with PIL
DESCRIPTION: Draws bounding boxes and labels on the original image to visualize the detected objects. Each prediction includes a bounding box, label, and confidence score.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/zero_shot_object_detection.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from PIL import ImageDraw

>>> draw = ImageDraw.Draw(image)

>>> for prediction in predictions:
...     box = prediction["box"]
...     label = prediction["label"]
...     score = prediction["score"]

...     xmin, ymin, xmax, ymax = box.values()
...     draw.rectangle((xmin, ymin, xmax, ymax), outline="red", width=1)
...     draw.text((xmin, ymin), f"{label}: {round(score,2)}", fill="white")

>>> image
```

----------------------------------------

TITLE: Loading a Multimodal Dataset
DESCRIPTION: This code demonstrates how to load the LJ Speech dataset using the Hugging Face Datasets library, which is a multimodal dataset containing audio and text. The `load_dataset` function loads the training split of the LJ Speech dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_26

LANGUAGE: Python
CODE:
```
>>> from datasets import load_dataset

>>> lj_speech = load_dataset("lj_speech", split="train")
```

----------------------------------------

TITLE: Uploading Fine-tuned Model to Hugging Face Hub
DESCRIPTION: Pushes the fine-tuned image captioning model to the Hugging Face Hub to share with the community.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_captioning.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
trainer.push_to_hub()
```

----------------------------------------

TITLE: Matching Audio Sampling Rate with Model Requirements
DESCRIPTION: Code to ensure the dataset's audio sampling rate matches the sampling rate expected by the speech recognition model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/quicktour.md#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=riconoscitore_vocale.feature_extractor.sampling_rate))
```

----------------------------------------

TITLE: Data Preprocessing Function
DESCRIPTION: Function to preprocess text data by combining sentences and tokenizing for multiple choice format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/multiple_choice.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> ending_names = ["ending0", "ending1", "ending2", "ending3"]

>>> def preprocess_function(examples):
...     first_sentences = [[context] * 4 for context in examples["sent1"]]
...     question_headers = examples["sent2"]
...     second_sentences = [
...         [f"{header} {examples[end][i]}" for end in ending_names] for i, header in enumerate(question_headers)
...     ]

...     first_sentences = sum(first_sentences, [])
...     second_sentences = sum(second_sentences, [])

...     tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)
...     return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}
```

----------------------------------------

TITLE: Masked Language Modeling with BARTpho
DESCRIPTION: Example showing how to perform masked language modeling using BARTpho with the MBartForConditionalGeneration class. Demonstrates mask token prediction.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bartpho.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
from transformers import MBartForConditionalGeneration

bartpho = MBartForConditionalGeneration.from_pretrained("vinai/bartpho-syllable")
TXT = "ChÃºng tÃ´i lÃ  <mask> nghiÃªn cá»©u viÃªn."
input_ids = tokenizer([TXT], return_tensors="pt")["input_ids"]
logits = bartpho(input_ids).logits
masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()
probs = logits[0, masked_index].softmax(dim=0)
values, predictions = probs.topk(5)
print(tokenizer.decode(predictions).split())
```

----------------------------------------

TITLE: Configuring Accelerate for script-based training
DESCRIPTION: Command to create and save a configuration file for Accelerate when running training in a script.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/accelerate.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
accelerate config
```

----------------------------------------

TITLE: Resume Training from Specific Checkpoint
DESCRIPTION: Resumes training from a specific checkpoint folder. This allows for selecting a particular checkpoint to resume training from, providing more control over the training process. The argument `resume_from_checkpoint path_to_specific_checkpoint` is used.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/run_scripts.md#_snippet_16

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --resume_from_checkpoint path_to_specific_checkpoint \
    --predict_with_generate
```

----------------------------------------

TITLE: Evaluation Metric Setup
DESCRIPTION: Configure accuracy metric calculation for model evaluation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_classification.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> import evaluate
>>> import numpy as np

>>> accuracy = evaluate.load("accuracy")

>>> def compute_metrics(eval_pred):
...     predictions, labels = eval_pred
...     predictions = np.argmax(predictions, axis=1)
...     return accuracy.compute(predictions=predictions, references=labels)
```

----------------------------------------

TITLE: Evaluation Metrics Setup
DESCRIPTION: Setting up accuracy evaluation metrics for model performance assessment.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/multiple_choice.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> import evaluate

>>> accuracy = evaluate.load("accuracy")
```

LANGUAGE: python
CODE:
```
>>> import numpy as np

>>> def compute_metrics(eval_pred):
...     predictions, labels = eval_pred
...     predictions = np.argmax(predictions, axis=1)
...     return accuracy.compute(predictions=predictions, references=labels)
```

----------------------------------------

TITLE: Defining Compute Metrics Function
DESCRIPTION: Creates a function to compute accuracy metrics for model evaluation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/image_classification.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> import numpy as np


>>> def compute_metrics(eval_pred):
...     predictions, labels = eval_pred
...     predictions = np.argmax(predictions, axis=1)
...     return accuracy.compute(predictions=predictions, references=labels)
```

----------------------------------------

TITLE: Mapping Labels with Python
DESCRIPTION: Creates a mapping between label ids and label names for easy referencing when working with the dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_classification.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> labels = food["train"].features["label"].names
>>> label2id, id2label = dict(), dict()
>>> for i, label in enumerate(labels):
...     label2id[label] = str(i)
...     id2label[str(i)] = label
```

LANGUAGE: python
CODE:
```
>>> id2label[str(79)]
'prime_rib'
```

----------------------------------------

TITLE: Loading OWL-ViT Model and Processor for Manual Implementation
DESCRIPTION: Loads the OWL-ViT model and processor directly, allowing for more customization than the pipeline approach. This demonstrates how to set up the model for manual inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/zero_shot_object_detection.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection

>>> model = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint)
>>> processor = AutoProcessor.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Loading and Preprocessing MInDS-14 Dataset for ASR
DESCRIPTION: Python code to load a subset of the MInDS-14 dataset, split it into train and test sets, and remove unnecessary columns. This prepares the data for fine-tuning a speech recognition model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/asr.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset, Audio

>>> minds = load_dataset("PolyAI/minds14", name="en-US", split="train[:100]")
>>> minds = minds.train_test_split(test_size=0.2)
>>> minds = minds.remove_columns(["english_transcription", "intent_class", "lang_id"])
```

----------------------------------------

TITLE: Using CANINE Model with Tokenization for Batched Inference in Python
DESCRIPTION: This snippet illustrates how to utilize the CANINE model with the CanineTokenizer for batched inference and training. It demonstrates how to prepare text inputs using the tokenizer for padding and truncation to ensure uniform input lengths.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/canine.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import CanineTokenizer, CanineModel

>>> model = CanineModel.from_pretrained("google/canine-c")
>>> tokenizer = CanineTokenizer.from_pretrained("google/canine-c")

>>> inputs = ["Life is like a box of chocolates.", "You never know what you gonna get."]
>>> encoding = tokenizer(inputs, padding="longest", truncation=True, return_tensors="pt")

>>> outputs = model(**encoding)  # forward pass
>>> pooled_output = outputs.pooler_output
>>> sequence_output = outputs.last_hidden_state
```

----------------------------------------

TITLE: Loading a Model from Sharded Checkpoints
DESCRIPTION: Shows how to load a model from sharded checkpoints using the from_pretrained method. This process automatically handles the sharded files using the index file.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/big_models.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir, max_shard_size="200MB")
...     new_model = AutoModel.from_pretrained(tmp_dir)
```

----------------------------------------

TITLE: Accessing Model Output as Tuple in Transformers using Python
DESCRIPTION: This code snippet shows how to access the model's output as a tuple.  Only the non-None attributes are included in the tuple. In this specific instance, the example tuple contains the loss and logits from the model output of the Bert Sequence Classifier.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/output.md#_snippet_1

LANGUAGE: python
CODE:
```
outputs[:2]
```

----------------------------------------

TITLE: Loading a Specific Model Class (Llama) - Python
DESCRIPTION: This snippet loads a specific causal language model using the model class LlamaForCausalLM directly. It shows how to bypass the AutoModel feature when the exact model class is known.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/models.md#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
from transformers import LlamaModel, LlamaForCausalLM

model = LlamaForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
```

----------------------------------------

TITLE: Using GraniteMoeShared for Text Generation in Python
DESCRIPTION: This snippet demonstrates how to load and use the GraniteMoeShared model for text generation. It includes steps for loading the model and tokenizer, preparing input, generating output, and decoding the result.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/granitemoeshared.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = "ibm-research/moe-7b-1b-active-shared-experts"
tokenizer = AutoTokenizer.from_pretrained(model_path)

# drop device_map if running on CPU
model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto")
model.eval()

# change input text as desired
prompt = "Write a code to find the maximum value in a list of numbers."

# tokenize the text
input_tokens = tokenizer(prompt, return_tensors="pt")
# generate output tokens
output = model.generate(**input_tokens, max_new_tokens=100)
# decode output tokens into text
output = tokenizer.batch_decode(output)
# loop over the batch to print, in this example the batch size is 1
for i in output:
    print(i)
```

----------------------------------------

TITLE: Creating Learning Rate Scheduler
DESCRIPTION: This code creates a linear learning rate scheduler using `get_scheduler` from the Transformers library. It sets the number of training steps based on the number of epochs and the length of the training data loader. This scheduler adjusts the learning rate during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_22

LANGUAGE: Python
CODE:
```
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
)
```

----------------------------------------

TITLE: Creating Data Collator for TensorFlow Language Modeling
DESCRIPTION: Sets up a DataCollatorForLanguageModeling for TensorFlow that handles padding and batch preparation. It specifies TensorFlow tensors as the return format and configures the collator for causal language modeling rather than masked language modeling.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/language_modeling.md#2025-04-23_snippet_11

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorForLanguageModeling

>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors="tf")
```

----------------------------------------

TITLE: Setting Up Apex-like fp16 Mixed Precision in DeepSpeed
DESCRIPTION: Configuration for Apex-like fp16 mixed precision training in DeepSpeed. The Trainer automatically configures amp based on fp16_backend and fp16_opt_level values for optimized performance with maintained precision.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/deepspeed.md#2025-04-22_snippet_18

LANGUAGE: yaml
CODE:
```
{
    "amp": {
        "enabled": "auto",
        "opt_level": "auto"
    }
}
```

----------------------------------------

TITLE: Using Pre-trained EncoderDecoderModel for Sentence Fusion in Python
DESCRIPTION: This example shows how to use a pre-trained EncoderDecoderModel from the model hub for sentence fusion. It demonstrates loading the model and tokenizer, preparing input, and generating output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/bert-generation.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
# instantiate sentence fusion model
sentence_fuser = EncoderDecoderModel.from_pretrained("google/roberta2roberta_L-24_discofuse")
tokenizer = AutoTokenizer.from_pretrained("google/roberta2roberta_L-24_discofuse")

input_ids = tokenizer(
    "This is the first sentence. This is the second sentence.", add_special_tokens=False, return_tensors="pt"
).input_ids

outputs = sentence_fuser.generate(input_ids)

print(tokenizer.decode(outputs[0]))
```

----------------------------------------

TITLE: Data Preprocessing Implementation
DESCRIPTION: Functions for preprocessing images and text data using ViltProcessor
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/visual_question_answering.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> import torch

>>> def preprocess_data(examples):
...     image_paths = examples['image_id']
...     images = [Image.open(image_path) for image_path in image_paths]
...     texts = examples['question']
...     encoding = processor(images, texts, padding="max_length", truncation=True, return_tensors="pt")
...     for k, v in encoding.items():
...           encoding[k] = v.squeeze()
...     targets = []
...     for labels, scores in zip(examples['label.ids'], examples['label.weights']):
...         target = torch.zeros(len(id2label))
...         for label, score in zip(labels, scores):
...             target[label] = score
...         targets.append(target)
...     encoding["labels"] = targets
...     return encoding
```

----------------------------------------

TITLE: Splitting dataset into train and test sets
DESCRIPTION: Use the train_test_split method to create training and testing subsets from the loaded dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/tasks/asr.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
minds = minds.train_test_split(test_size=0.2)
```

----------------------------------------

TITLE: Setting Up Data Augmentation with PyTorch
DESCRIPTION: This code creates a ColorJitter transformation to randomly modify brightness, contrast, saturation, and hue of training images to improve model generalization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/sequence_classification.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from torchvision.transforms import ColorJitter

>>> jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)
```

----------------------------------------

TITLE: Setting up DeepSpeed environment in Jupyter notebook
DESCRIPTION: Python code to set up a simulated distributed environment for using DeepSpeed in a Jupyter notebook with a single GPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/deepspeed.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
import os

os.environ["MASTER_ADDR"] = "localhost"
os.environ["MASTER_PORT"] = "9994"  # modify if RuntimeError: Address already in use
os.environ["RANK"] = "0"
os.environ["LOCAL_RANK"] = "0"
os.environ["WORLD_SIZE"] = "1"

# Now proceed as normal, plus pass the deepspeed config file
training_args = TrainingArguments(..., deepspeed="ds_config_zero3.json")
trainer = Trainer(...)
trainer.train()
```

----------------------------------------

TITLE: Setting Random Seeds for Reproducible Results
DESCRIPTION: Demonstrates how to set random seeds for Python, PyTorch, NumPy, and TensorFlow to ensure reproducible test results
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/testing.md#2025-04-22_snippet_43

LANGUAGE: python
CODE:
```
seed = 42

# python RNG
import random

random.seed(seed)

# pytorch RNGs
import torch

torch.manual_seed(seed)
torch.backends.cudnn.deterministic = True
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# numpy RNG
import numpy as np

np.random.seed(seed)

# tf RNG
tf.random.set_seed(seed)
```

----------------------------------------

TITLE: Running Make Quality Command (Bash)
DESCRIPTION: This command executes the `make quality` target. It runs various static analysis and code quality checks, including `ruff` and custom scripts, to identify potential errors, inefficiencies, or deviations from best practices in your code before submitting a pull request.
SOURCE: https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#_snippet_9

LANGUAGE: bash
CODE:
```
make quality
```

----------------------------------------

TITLE: Preprocessing Images with ViT Image Processor
DESCRIPTION: Loads the ViT image processor and sets up image transformations for preprocessing the dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/image_classification.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import AutoImageProcessor
>>> from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224-in21k")
>>> normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)
>>> _transforms = Compose([RandomResizedCrop(image_processor.size["height"]), ToTensor(), normalize])

>>> def transforms(examples):
...     examples["pixel_values"] = [_transforms(img.convert("RGB")) for img in examples["image"]]
...     del examples["image"]
...     return examples

>>> food = food.with_transform(transforms)
```

----------------------------------------

TITLE: Launching Multi-GPU Script with Torchrun
DESCRIPTION: Command to launch the tensor parallelism script across multiple GPUs using torchrun. This example shows how to distribute the model across 4 GPUs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/perf_infer_gpu_multi.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
torchrun --nproc-per-node 4 demo.py
```

----------------------------------------

TITLE: Loading Depth Estimation Model and Processor
DESCRIPTION: Demonstrates how to manually load a depth estimation model and its associated image processor from the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/monocular_depth_estimation.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoImageProcessor, AutoModelForDepthEstimation

>>> checkpoint = "vinvino02/glpn-nyu"

>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)
>>> model = AutoModelForDepthEstimation.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Correct Model Output for a Single Input Sequence in Python
DESCRIPTION: This code shows the correct output when using a single input sequence without padding, highlighting the difference from the incorrect output with unmasked padding.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/troubleshooting.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> input_ids = torch.tensor([[7592]])
>>> output = model(input_ids)
>>> print(output.logits)
tensor([[-0.1008, -0.4061]], grad_fn=<AddmmBackward0>)
```

----------------------------------------

TITLE: Initializing Mask Generation Pipeline Python
DESCRIPTION: Initializes the `mask-generation` pipeline using the `facebook/sam-vit-base` checkpoint. This pipeline simplifies the process of generating masks from images using the SAM model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/mask_generation.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import pipeline

>>> checkpoint = "facebook/sam-vit-base"
>>> mask_generator = pipeline(model=checkpoint, task="mask-generation")
```

----------------------------------------

TITLE: Pushing Custom ResNet Model to Hugging Face Hub
DESCRIPTION: Pushes the custom ResNet model to the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/custom_models.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
resnet50d.push_to_hub("custom-resnet50d")
```

----------------------------------------

TITLE: Checking Default Generation Configuration
DESCRIPTION: Shows how to load a model and inspect its default generation configuration settings using AutoModelForCausalLM.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/generation_strategies.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("distilbert/distilgpt2")
model.generation_config
```

----------------------------------------

TITLE: Loading GIT Base Model for Causal Language Modeling
DESCRIPTION: Loads the microsoft/git-base model as an AutoModelForCausalLM, which will be fine-tuned for image captioning.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_captioning.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Installing Transformers via pip
DESCRIPTION: This command installs the core `transformers` library using pip, the Python package installer.  This installs the stable version of the library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/installation.md#_snippet_3

LANGUAGE: bash
CODE:
```
pip install transformers
```

----------------------------------------

TITLE: Configuring Training Arguments with Evaluation Strategy
DESCRIPTION: Shows how to configure the Trainer to evaluate the model at the end of each epoch by setting the eval_strategy parameter in the training arguments.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/training.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers import TrainingArguments, Trainer

>>> training_args = TrainingArguments(output_dir="test_trainer", eval_strategy="epoch")
```

----------------------------------------

TITLE: Setting Data Type for Non-Linear Layers in 4-bit Quantization
DESCRIPTION: Example showing how to customize the data type for non-linear layers like LayerNorm when loading a model in 4-bit precision.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/bitsandbytes.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=True)

model_4bit = AutoModelForCausalLM.from_pretrained(
    "facebook/opt-350m",
    quantization_config=quantization_config, 
    torch_dtype=torch.float32
)
model_4bit.model.decoder.layers[-1].final_layer_norm.weight.dtype
```

----------------------------------------

TITLE: Saving a Tokenizer to a JSON File
DESCRIPTION: This code snippet shows how to save a tokenizer object to a JSON file using the `save` method. This allows you to persist the trained tokenizer and load it later. The tokenizer object must first be trained before saving.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/fast_tokenizers.md#_snippet_2

LANGUAGE: Python
CODE:
```
>>> tokenizer.save("tokenizer.json")
```

----------------------------------------

TITLE: Applying Preprocessing Function to Dataset
DESCRIPTION: In this snippet, the preprocessing function is applied to the entire ELI5 dataset using the map function, optimizing processing with batched input and multiple processes while removing unnecessary columns.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> tokenized_eli5 = eli5.map(
...     preprocess_function,
...     batched=True,
...     num_proc=4,
...     remove_columns=eli5["train"].column_names,
... )
```

----------------------------------------

TITLE: Examining Dataset Structure for VQA Task
DESCRIPTION: Displays the structure of a single example from the dataset to understand the available features, including questions, image paths, and answer labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/visual_question_answering.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> dataset[0]
{'question': 'Where is he looking?',
 'question_type': 'none of the above',
 'question_id': 262148000,
 'image_id': '/root/.cache/huggingface/datasets/downloads/extracted/ca733e0e000fb2d7a09fbcc94dbfe7b5a30750681d0e965f8e0a23b1c2f98c75/val2014/COCO_val2014_000000262148.jpg',
 'answer_type': 'other',
 'label': {'ids': ['at table', 'down', 'skateboard', 'table'],
  'weights': [0.30000001192092896,
   1.0,
   0.30000001192092896,
   0.30000001192092896]}}
```

----------------------------------------

TITLE: Examining Dataset Examples
DESCRIPTION: Displays a sample from the training set to understand the dataset structure, showing the tokens and their corresponding NER tags represented as numerical values.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> wnut["train"][0]
{'id': '0',
 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0],
 'tokens': ['@paulwalk', 'It', "'s", 'the', 'view', 'from', 'where', 'I', "'m", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.']
}
```

----------------------------------------

TITLE: Installing Required Libraries for Transformers Training
DESCRIPTION: This bash command installs the necessary libraries for demonstrating model training, including transformers, datasets, accelerate, and nvidia-ml-py3 for GPU monitoring.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_memory_anatomy.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers datasets accelerate nvidia-ml-py3
```

----------------------------------------

TITLE: Computing Cosine Similarity Between Image Embeddings
DESCRIPTION: Code to calculate the similarity score between two images using cosine similarity on their embeddings, which helps determine how visually similar they are.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_feature_extraction.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from torch.nn.functional import cosine_similarity

similarity_score = cosine_similarity(torch.Tensor(outputs[0]),
                                     torch.Tensor(outputs[1]), dim=1)

print(similarity_score)

# tensor([0.6043])
```

----------------------------------------

TITLE: Loading and Using BERTweet Model and Tokenizer in Python
DESCRIPTION: This snippet demonstrates how to load the pre-trained BERTweet model and tokenizer, and use them to process a sample tweet. It covers both PyTorch and TensorFlow implementations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/bertweet.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> import torch
>>> from transformers import AutoModel, AutoTokenizer

>>> bertweet = AutoModel.from_pretrained("vinai/bertweet-base")

>>> # íŠ¸ëžœìŠ¤í¬ë¨¸ ë²„ì „ 4.x ì´ìƒ :
>>> tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base", use_fast=False)

>>> # íŠ¸ëžœìŠ¤í¬ë¨¸ ë²„ì „ 3.x ì´ìƒ:
>>> # tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base")

>>> # ìž…ë ¥ëœ íŠ¸ìœ—ì€ ì´ë¯¸ ì •ê·œí™”ë˜ì—ˆìŠµë‹ˆë‹¤!
>>> line = "SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:"

>>> input_ids = torch.tensor([tokenizer.encode(line)])

>>> with torch.no_grad():
...     features = bertweet(input_ids)  # Models outputs are now tuples

>>> # With TensorFlow 2.0+:
>>> # from transformers import TFAutoModel
>>> # bertweet = TFAutoModel.from_pretrained("vinai/bertweet-base")
```

----------------------------------------

TITLE: Loading Backbone with AutoBackbone in Transformers
DESCRIPTION: This snippet demonstrates how to load a pretrained vision model as a backbone using the `AutoBackbone` class and the `from_pretrained` method. The `out_indices` parameter specifies which layer to extract the feature map from. The example loads the Swin Transformer backbone.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/backbones.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoBackbone

model = AutoBackbone.from_pretrained("microsoft/swin-tiny-patch4-window7-224", out_indices=(1,))
```

----------------------------------------

TITLE: Creating a Data Collator with Dynamic Padding for TensorFlow
DESCRIPTION: TensorFlow implementation of a data collator that applies dynamic padding and returns tensors in the format required by TensorFlow models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/sequence_classification.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorWithPadding

>>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```

----------------------------------------

TITLE: Creating Metric Computation Function
DESCRIPTION: Defines a function to compute evaluation metrics using seqeval, which converts model predictions and labels into the format required by seqeval and returns precision, recall, F1 score, and accuracy.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
>>> import numpy as np

>>> labels = [label_list[i] for i in example[f"ner_tags"]]


>>> def compute_metrics(p):
...     predictions, labels = p
...     predictions = np.argmax(predictions, axis=2)

...     true_predictions = [
...         [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
...         for prediction, label in zip(predictions, labels)
...     ]
...     true_labels = [
...         [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
...         for prediction, label in zip(predictions, labels)
...     ]

...     results = seqeval.compute(predictions=true_predictions, references=true_labels)
...     return {
...         "precision": results["overall_precision"],
...         "recall": results["overall_recall"],
...         "f1": results["overall_f1"],
...         "accuracy": results["overall_accuracy"],
...     }
```

----------------------------------------

TITLE: Integrating Flash Attention-2 in Idefics2 Model in Python
DESCRIPTION: The Python code modification shows how to integrate Flash Attention-2 into the Idefics2 model, enhancing its performance by utilizing an efficient attention mechanism. This requires installing Flash Attention 2 and ensuring hardware compatibility. The model is loaded in half-precision to further optimize resource usage.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/idefics2.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
model = Idefics2ForConditionalGeneration.from_pretrained(
    "HuggingFaceM4/idefics2-8b",
    torch_dtype=torch.float16,    
    attn_implementation="flash_attention_2",
).to(device)
```

----------------------------------------

TITLE: Pipeline with Generator
DESCRIPTION: Shows how to use a generator function with pipeline for streaming data processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/pipelines.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import pipeline

pipe = pipeline("text-classification")

def data():
    while True:
        yield "This is a test"

for out in pipe(data()):
    print(out)
```

----------------------------------------

TITLE: Using AriaForConditionalGeneration to Process Images
DESCRIPTION: This code snippet demonstrates how to use the Aria model for conditional generation tasks based on visual input. It involves loading the model and processor, obtaining an image from a URL, formatting the input messages, and generating text output. The key parameters include max_new_tokens for limiting the response length and temperature for controlling randomness. Ensure the appropriate libraries are installed, including `torch`, `requests`, and the Hugging Face Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/aria.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
import requests
import torch
from PIL import Image

from transformers import AriaProcessor, AriaForConditionalGeneration

model_id_or_path = "rhymes-ai/Aria"

model = AriaForConditionalGeneration.from_pretrained(
    model_id_or_path, device_map="auto"
)

processor = AriaProcessor.from_pretrained(model_id_or_path)

image = Image.open(requests.get("http://images.cocodataset.org/val2017/000000039769.jpg", stream=True).raw)

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"text": "what is the image?", "type": "text"},
        ],
    }
]

text = processor.apply_chat_template(messages, add_generation_prompt=True)
inputs = processor(text=text, images=image, return_tensors="pt")
inputs.to(model.device)

output = model.generate(
    **inputs,
    max_new_tokens=15,
    stop_strings=["<|im_end|>"],
    tokenizer=processor.tokenizer,
    do_sample=True,
    temperature=0.9,
)
output_ids = output[0][inputs["input_ids"].shape[1]:]
response = processor.decode(output_ids, skip_special_tokens=True)
```

----------------------------------------

TITLE: TensorFlow-specific Data Collator
DESCRIPTION: Creates a data collator specifically for TensorFlow that will handle batching and padding of token classification data during training, with tensors returned in TensorFlow format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorForTokenClassification

>>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors="tf")
```

----------------------------------------

TITLE: Configuring DDP with Accelerate
DESCRIPTION: This YAML configuration file sets up Distributed Data Parallel (DDP) for multi-node, multi-GPU training using Accelerate. It specifies parameters such as the compute environment, distributed type, GPU IDs, machine rank, main process IP and port, mixed precision, and the number of machines and processes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/trainer.md#2025-04-22_snippet_8

LANGUAGE: yaml
CODE:
```
compute_environment: LOCAL_MACHINE                                                                                             
distributed_type: MULTI_GPU                                                                                                    
downcast_bf16: 'no'
gpu_ids: all
machine_rank: 0 #change rank as per the node
main_process_ip: 192.168.20.1
main_process_port: 9898
main_training_function: main
mixed_precision: fp16
num_machines: 2
num_processes: 8
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

----------------------------------------

TITLE: Configuring Model Data Type via AutoConfig - Transformers - Python
DESCRIPTION: This snippet demonstrates how to use `AutoConfig` to set the `torch_dtype` for a model instantiated from scratch, using the `transformers` library, which allows users to define model parameters programmatically.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/models.md#2025-04-22_snippet_17

LANGUAGE: Python
CODE:
```
import torch
from transformers import AutoConfig, AutoModel

my_config = AutoConfig.from_pretrained("google/gemma-2b", torch_dtype=torch.float16)
model = AutoModel.from_config(my_config)
```

----------------------------------------

TITLE: Combining Flash Attention 2 with 4-bit Quantization
DESCRIPTION: Shows how to enable both Flash Attention 2 and 4-bit quantization when loading a model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_infer_gpu_one.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    load_in_4bit=True,
    attn_implementation="flash_attention_2",
)
```

----------------------------------------

TITLE: Training with IPEX and BFloat16 Mixed Precision in Trainer
DESCRIPTION: This command demonstrates how to train a question-answering model using the Transformers library with IPEX and BFloat16 mixed precision. The `use_ipex`, `bf16`, and `no_cuda` flags are added to the training command to enable IPEX optimizations and BFloat16 precision on the CPU, while disabling CUDA.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/perf_train_cpu.md#_snippet_1

LANGUAGE: bash
CODE:
```
python run_qa.py \
--model_name_or_path google-bert/bert-base-uncased \
--dataset_name squad \
--do_train \
--do_eval \
--per_device_train_batch_size 12 \
--learning_rate 3e-5 \
--num_train_epochs 2 \
--max_seq_length 384 \
--doc_stride 128 \
--output_dir /tmp/debug_squad/ \
--use_ipex \
--bf16 --no_cuda
```

----------------------------------------

TITLE: Using Accelerate in Jupyter Notebooks
DESCRIPTION: Code snippet demonstrating how to use Accelerate's notebook_launcher for training in environments like Google Colaboratory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/accelerate.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

----------------------------------------

TITLE: Decoding Input IDs Back to Text with BERT Tokenizer
DESCRIPTION: This code demonstrates how to decode numerical input IDs back to human-readable text, showing how special tokens like [CLS] and [SEP] are added by the tokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/glossary.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> decoded_sequence = tokenizer.decode(encoded_sequence)
>>> print(decoded_sequence)
[CLS] A Titan RTX has 24GB of VRAM [SEP]
```

----------------------------------------

TITLE: Splitting MInDS-14 Dataset into Train and Test Sets
DESCRIPTION: Splits the loaded MInDS-14 dataset into training and testing sets using an 80-20 split ratio.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/asr.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> minds = minds.train_test_split(test_size=0.2)
```

----------------------------------------

TITLE: Loading and Using a Traced BERT Model
DESCRIPTION: This snippet shows how to load a previously saved TorchScript BERT model from disk and use it for inference with dummy inputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/torchscript.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
loaded_model = torch.jit.load("traced_bert.pt")
loaded_model.eval()

all_encoder_layers, pooled_output = loaded_model(*dummy_input)

# Using the traced model for inference
traced_model(tokens_tensor, segments_tensors)
```

----------------------------------------

TITLE: Updating Local Transformers Repository with Git in Bash
DESCRIPTION: This command pulls the latest changes from the main Transformers repository on GitHub, keeping the local repository up-to-date.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md#2025-04-22_snippet_12

LANGUAGE: bash
CODE:
```
cd ~/transformers/
git pull
```

----------------------------------------

TITLE: Running Training on TPU with TensorFlow
DESCRIPTION: Script to run training on TPU using TensorFlow TPUStrategy. Specifies TPU resource name and training parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/run_scripts.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
python run_summarization.py  \
    --tpu name_of_tpu_resource \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval
```

----------------------------------------

TITLE: Inspecting Generation Configuration
DESCRIPTION: This snippet demonstrates how to load a pre-trained causal language model and inspect its generation configuration using `model.generation_config`.  The generation configuration contains default settings used during text generation. Examining it allows users to understand the default behavior and identify which settings deviate from the defaults.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/generation_strategies.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained("distilbert/distilgpt2")
>>> model.generation_config
GenerationConfig {
    "bos_token_id": 50256,
    "eos_token_id": 50256,
}
```

----------------------------------------

TITLE: Initializing Image Processor for Preprocessing
DESCRIPTION: Load pre-trained image processor to extract normalization parameters for image preprocessing pipeline
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
from transformers import AutoImageProcessor

processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
image_size = (processor.size["height"], processor.size["width"])
image_mean = processor.image_mean
image_std = processor.image_std
```

----------------------------------------

TITLE: Creating Data Collator for Sequence-to-Sequence Tasks
DESCRIPTION: This code creates a data collator that handles dynamic padding of sequences within batches for more efficient training in both PyTorch and TensorFlow implementations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/translation.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
```

----------------------------------------

TITLE: Manual VQA Processing with ViLT
DESCRIPTION: Demonstrates manual processing steps for visual question answering using ViLT processor and model directly, without using the pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/visual_question_answering.md#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
>>> processor = ViltProcessor.from_pretrained("MariaK/vilt_finetuned_200")

>>> image = Image.open(example['image_id'])
>>> question = example['question']

>>> # prepare inputs
>>> inputs = processor(image, question, return_tensors="pt")

>>> model = ViltForQuestionAnswering.from_pretrained("MariaK/vilt_finetuned_200")

>>> # forward pass
>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> logits = outputs.logits
>>> idx = logits.argmax(-1).item()
>>> print("Predicted answer:", model.config.id2label[idx])
Predicted answer: down
```

----------------------------------------

TITLE: Custom Collate Function for Padded Images in Python
DESCRIPTION: This function is a custom collate function for batching images that may have different sizes due to scale augmentation. It uses the image processor's pad method to ensure all images in a batch have the same dimensions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/image_processors.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
def collate_fn(batch):
    pixel_values = [item["pixel_values"] for item in batch]
    encoding = image_processor.pad(pixel_values, return_tensors="pt")
    labels = [item["labels"] for item in batch]
    batch = {}
    batch["pixel_values"] = encoding["pixel_values"]
    batch["pixel_mask"] = encoding["pixel_mask"]
    batch["labels"] = labels
    return batch
```

----------------------------------------

TITLE: Converting Datasets to TensorFlow Format
DESCRIPTION: Prepares the tokenized datasets for TensorFlow training by converting them to tf.data.Dataset format. The training set is shuffled while the validation set maintains its order.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
>>> tf_train_set = model.prepare_tf_dataset(
...     tokenized_imdb["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_validation_set = model.prepare_tf_dataset(
...     tokenized_imdb["test"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

----------------------------------------

TITLE: Using XLM Model with Language Embeddings
DESCRIPTION: Example showing how to use XLM model with language embeddings for English-French translation using the FacebookAI/xlm-clm-enfr-1024 checkpoint. Demonstrates loading the model, setting language IDs, and performing inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/multilingual.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> import torch
>>> from transformers import XLMTokenizer, XLMWithLMHeadModel

>>> tokenizer = XLMTokenizer.from_pretrained("FacebookAI/xlm-clm-enfr-1024")
>>> model = XLMWithLMHeadModel.from_pretrained("FacebookAI/xlm-clm-enfr-1024")

>>> print(tokenizer.lang2id)
>>> input_ids = torch.tensor([tokenizer.encode("Wikipedia was used to")])
>>> language_id = tokenizer.lang2id["en"]
>>> langs = torch.tensor([language_id] * input_ids.shape[1])
>>> langs = langs.view(1, -1)
>>> outputs = model(input_ids, langs=langs)
```

----------------------------------------

TITLE: Training Object Detection without Trainer using Accelerate
DESCRIPTION: Command for training object detection models using the Accelerate library instead of the Trainer API. Supports distributed training environments and mixed precision training.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/object-detection/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
accelerate launch run_object_detection_no_trainer.py \
    --model_name_or_path "facebook/detr-resnet-50" \
    --dataset_name cppe-5 \
    --output_dir "detr-resnet-50-finetuned" \
    --num_train_epochs 100 \
    --image_square_size 600 \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --checkpointing_steps epoch \
    --learning_rate 5e-5 \
    --ignore_mismatched_sizes \
    --with_tracking \
    --push_to_hub
```

----------------------------------------

TITLE: Calculating GPU Memory Usage in Python
DESCRIPTION: This Python function bytes_to_giga_bytes is used to convert the maximum memory allocated on the GPU from bytes to gigabytes using PyTorch's CUDA functionality. It demonstrates memory calculation after running a model and is useful for assessing resource usage.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
bytes_to_giga_bytes(torch.cuda.max_memory_allocated())
```

----------------------------------------

TITLE: Fine-tuning XLSR-Wav2Vec2 on Common Voice with Single GPU CTC
DESCRIPTION: Command for fine-tuning XLSR-Wav2Vec2 on the Common Voice Turkish dataset using a single GPU with half-precision. This configuration runs in about 1 hour 20 minutes on a V100 GPU and yields a CTC loss of 0.39 and word error rate of 0.35.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-recognition/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python run_speech_recognition_ctc.py \
	--dataset_name="common_voice" \
	--model_name_or_path="facebook/wav2vec2-large-xlsr-53" \
	--dataset_config_name="tr" \
	--output_dir="./wav2vec2-common_voice-tr-demo" \
	--overwrite_output_dir \
	--num_train_epochs="15" \
	--per_device_train_batch_size="16" \
	--gradient_accumulation_steps="2" \
	--learning_rate="3e-4" \
	--warmup_steps="500" \
	--eval_strategy="steps" \
	--text_column_name="sentence" \
	--length_column_name="input_length" \
	--save_steps="400" \
	--eval_steps="100" \
	--layerdrop="0.0" \
	--save_total_limit="3" \
	--freeze_feature_encoder \
	--gradient_checkpointing \
	--chars_to_ignore , ? . ! - \; \: \" " % ' " ï¿½ \
	--fp16 \
	--group_by_length \
	--push_to_hub \
	--do_train --do_eval
```

----------------------------------------

TITLE: Implementing Data Preprocessing Function for VQA
DESCRIPTION: Creates a function to preprocess data by encoding images and questions with the ViLT processor, and preparing soft-encoded target labels for the multi-label classification task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/visual_question_answering.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> import torch

>>> def preprocess_data(examples):
...     image_paths = examples['image_id']
...     images = [Image.open(image_path) for image_path in image_paths]
...     texts = examples['question']

...     encoding = processor(images, texts, padding="max_length", truncation=True, return_tensors="pt")

...     for k, v in encoding.items():
...           encoding[k] = v.squeeze()

...     targets = []

...     for labels, scores in zip(examples['label.ids'], examples['label.weights']):
...         target = torch.zeros(len(id2label))

...         for label, score in zip(labels, scores):
...             target[label] = score

...         targets.append(target)

...     encoding["labels"] = targets

...     return encoding
```

----------------------------------------

TITLE: Compute Metrics Function for TensorFlow
DESCRIPTION: This function allows computing evaluation metrics for predictions using TensorFlow, resizing logits and utilizing the given metric to compute, among other things, accuracy and IoU. Dependencies include TensorFlow library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md#2025-04-22_snippet_17

LANGUAGE: py
CODE:
```
>>> def compute_metrics(eval_pred):
...     logits, labels = eval_pred
...     logits = tf.transpose(logits, perm=[0, 2, 3, 1])
...     logits_resized = tf.image.resize(
...         logits,
...         size=tf.shape(labels)[1:],
...         method="bilinear",
...     )

...     pred_labels = tf.argmax(logits_resized, axis=-1)
...     metrics = metric.compute(
...         predictions=pred_labels,
...         references=labels,
...         num_labels=num_labels,
...         ignore_index=-1,
...         reduce_labels=image_processor.do_reduce_labels,
...     )

...     per_category_accuracy = metrics.pop("per_category_accuracy").tolist()
...     per_category_iou = metrics.pop("per_category_iou").tolist()

...     metrics.update({f"accuracy_{id2label[i]}": v for i, v in enumerate(per_category_accuracy)})
...     metrics.update({f"iou_{id2label[i]}": v for i, v in enumerate(per_category_iou)})
...     return {"val_" + k: v for k, v in metrics.items()}
```

----------------------------------------

TITLE: Launching Training Script with FSDP in Bash
DESCRIPTION: This Bash command launches a training script using the 'accelerate' library with FSDP configurations specified in a configuration file. It is flexible enough to include additional FSDP arguments directly in the command line.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/fsdp.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
accelerate launch my-training-script.py
```

LANGUAGE: bash
CODE:
```
accelerate launch --fsdp="full shard" --fsdp_config="path/to/fsdp_config/" my-training-script.py
```

----------------------------------------

TITLE: Running Inference with JIT Mode on CPU
DESCRIPTION: Example command for running question answering inference using JIT mode on CPU with the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/perf_infer_cpu.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python run_qa.py \
--model_name_or_path csarron/bert-base-uncased-squad-v1 \
--dataset_name squad \
--do_eval \
--max_seq_length 384 \
--doc_stride 128 \
--output_dir /tmp/ \
--no_cuda \
--jit_mode_eval
```

----------------------------------------

TITLE: Conversational Use with IDEFICS Instruct in Python
DESCRIPTION: This snippet shows how to use a fine-tuned IDEFICS model (`HuggingFaceM4/idefics-9b-instruct`) for conversational purposes. It loads the model and processor, prepares prompts that include user utterances and corresponding image URLs, and generates responses from the model. The `eos_token_id` and `bad_words_ids` parameters are used to control the generation process, and specify when to stop generation and which tokens to avoid, respectively.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/idefics.md#2025-04-22_snippet_10

LANGUAGE: Python
CODE:
```
>>> import torch
>>> from transformers import IdeficsForVisionText2Text, AutoProcessor

>>> device = "cuda" if torch.cuda.is_available() else "cpu"

>>> checkpoint = "HuggingFaceM4/idefics-9b-instruct"
>>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)
>>> processor = AutoProcessor.from_pretrained(checkpoint)

>>> prompts = [
...     [
...         "User: What is in this image?",
...         "https://upload.wikimedia.org/wikipedia/commons/8/86/Id%C3%A9fix.JPG",
...         "<end_of_utterance>",

...         "\nAssistant: This picture depicts Idefix, the dog of Obelix in Asterix and Obelix. Idefix is running on the ground.<end_of_utterance>",

...         "\nUser:",
...         "https://static.wikia.nocookie.net/asterix/images/2/25/R22b.gif/revision/latest?cb=20110815073052",
...         "And who is that?<end_of_utterance>",

...         "\nAssistant:",
...     ],
... ]

>>> # --batched mode
>>> inputs = processor(prompts, add_end_of_utterance_token=False, return_tensors="pt").to(device)
>>> # --single sample mode
>>> # inputs = processor(prompts[0], return_tensors="pt").to(device)

>>> # Generation args
>>> exit_condition = processor.tokenizer("<end_of_utterance>", add_special_tokens=False).input_ids
>>> bad_words_ids = processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, eos_token_id=exit_condition, bad_words_ids=bad_words_ids, max_length=100)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> for i, t in enumerate(generated_text):
...     print(f"{i}:\n{t}\n")
```

----------------------------------------

TITLE: Loading and Preparing Image for Object Detection
DESCRIPTION: Loads a sample image from scikit-image and converts it to PIL format for processing with the object detection pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/zero_shot_object_detection.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> import skimage
>>> import numpy as np
>>> from PIL import Image

>>> image = skimage.data.astronaut()
>>> image = Image.fromarray(np.uint8(image)).convert("RGB")

>>> image
```

----------------------------------------

TITLE: Pushing Model to Hub with PyTorch
DESCRIPTION: After training the model, this snippet shows how to push the fine-tuned model to the Hugging Face Model Hub using the `push_to_hub` method from the Trainer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/sequence_classification.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
>>> trainer.push_to_hub()
```

----------------------------------------

TITLE: Installing ML Framework - TensorFlow
DESCRIPTION: Command to install TensorFlow as the machine learning framework.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quicktour.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
pip install tensorflow
```

----------------------------------------

TITLE: Creating a DeepSpeed Configuration File in Bash
DESCRIPTION: This snippet creates a DeepSpeed configuration file named 'ds_config_zero3.json' on the fly within a Jupyter notebook cell. The configuration includes parameters for FP16 training, optimizer settings, scheduler settings, and zero optimization configurations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/deepspeed.md#2025-04-22_snippet_31

LANGUAGE: Bash
CODE:
```
%%bash
cat <<'EOT' > ds_config_zero3.json
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": "auto",
            "betas": "auto",
            "eps": "auto",
            "weight_decay": "auto"
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },

    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    },

    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 2000,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false
}
EOT
```

----------------------------------------

TITLE: Batch Processing Multiple Images with OWL-ViT
DESCRIPTION: Demonstrates how to process multiple images and text queries in a single batch. This approach allows efficient processing of multiple detection tasks simultaneously.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/zero_shot_object_detection.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> images = [image, im]
>>> text_queries = [
...     ["human face", "rocket", "nasa badge", "star-spangled banner"],
...     ["hat", "book", "sunglasses", "camera"],
... ]
>>> inputs = processor(text=text_queries, images=images, return_tensors="pt")
```

----------------------------------------

TITLE: Computing Image and Text Features with Chinese-CLIP
DESCRIPTION: Demonstrate how to compute and normalize image and text features, and calculate image-text similarity scores using Chinese-CLIP
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/chinese_clip.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
url = "https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg"
image = Image.open(requests.get(url, stream=True).raw)
texts = ["æ°å°¼é¾Ÿ", "å¦™è›™ç§å­", "å°ç«é¾™", "çš®å¡ä¸˜"]

# compute image feature
inputs = processor(images=image, return_tensors="pt")
image_features = model.get_image_features(**inputs)
image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)  # normalize

# compute text features
inputs = processor(text=texts, padding=True, return_tensors="pt")
text_features = model.get_text_features(**inputs)
text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)  # normalize

# compute image-text similarity scores
inputs = processor(text=texts, images=image, return_tensors="pt", padding=True)
outputs = model(**inputs)
logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # probs: [[1.2686e-03, 5.4499e-02, 6.7968e-04, 9.4355e-01]]
```

----------------------------------------

TITLE: Initializing Object Detection Model
DESCRIPTION: Loads a pre-trained object detection model with custom label mappings and configuration for mismatched sizes. Uses AutoModelForObjectDetection class to initialize the model with specified label mappings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/object_detection.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
from transformers import AutoModelForObjectDetection

model = AutoModelForObjectDetection.from_pretrained(
    MODEL_NAME,
    id2label=id2label,
    label2id=label2id,
    ignore_mismatched_sizes=True,
)
```

----------------------------------------

TITLE: Loading IDEFICS Model and Processor
DESCRIPTION: Loads the IDEFICS model and processor from a specified checkpoint. It uses torch.bfloat16 for dtype and 'auto' for device mapping.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/idefics.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import IdeficsForVisionText2Text, AutoProcessor

checkpoint = "HuggingFaceM4/idefics-9b"

processor = AutoProcessor.from_pretrained(checkpoint)

model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map="auto")
```

----------------------------------------

TITLE: Apply Transformations to Dataset (Python)
DESCRIPTION: This code uses the `set_transform` method from the ðŸ¤— Datasets library to apply a transformation function to a dataset. This transformation is applied on-the-fly when accessing the image data.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/preprocessing.md#_snippet_21

LANGUAGE: python
CODE:
```
>>> dataset.set_transform(transforms)
```

----------------------------------------

TITLE: Running Summarization Script with ðŸ¤— Accelerate
DESCRIPTION: This command launches a summarization training script using ðŸ¤— Accelerate, specifying the model, dataset, and output directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/run_scripts.md#2025-04-22_snippet_9

LANGUAGE: bash
CODE:
```
accelerate launch run_summarization_no_trainer.py \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir ~/tmp/tst-summarization
```

----------------------------------------

TITLE: Loading Model with ONNX Runtime
DESCRIPTION: Illustrates loading an ONNX model and running inference using ONNX Runtime in Python. Utilizes the AutoTokenizer and ORTModelForQuestionAnswering classes to process input and generate outputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/serialization.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import ORTModelForQuestionAnswering

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> model = ORTModelForQuestionAnswering.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> inputs = tokenizer("What am I using?", "Using DistilBERT with ONNX Runtime!", return_tensors="pt")
>>> outputs = model(**inputs)
```

----------------------------------------

TITLE: Specifying Dependency Versions for Hugging Face Transformers
DESCRIPTION: This snippet defines the minimum required versions for three important packages used with Hugging Face Transformers. It ensures compatibility with datasets (version 1.4.0 or higher), TensorFlow (version 2.3.0 or higher), and the evaluate package (version 0.2.0 or higher).
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/tensorflow/translation/requirements.txt#2025-04-22_snippet_0

LANGUAGE: plaintext
CODE:
```
datasets >= 1.4.0
tensorflow >= 2.3.0
evaluate >= 0.2.0
```

----------------------------------------

TITLE: Fine-tuning with AdaLomo Optimizer in Python
DESCRIPTION: This code snippet demonstrates how to fine-tune a language model using the AdaLomo optimizer from the `lomo-optim` library. It loads the IMDB dataset, configures training arguments including the `adalomo` optimizer, initializes a `SFTTrainer`, and starts the training process. The model is google/gemma-2b.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/trainer.md#_snippet_19

LANGUAGE: python
CODE:
```
import torch
import datasets
from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM
import trl

train_dataset = datasets.load_dataset('imdb', split='train')

args = TrainingArguments(
    output_dir="./test-lomo",
    max_steps=100,
    per_device_train_batch_size=4,
    optim="adalomo",
    gradient_checkpointing=True,
    logging_strategy="steps",
    logging_steps=1,
    learning_rate=2e-6,
    save_strategy="no",
    run_name="lomo-imdb",
)

model_id = "google/gemma-2b"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True).to(0)

trainer = trl.SFTTrainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    dataset_text_field='text',
    max_seq_length=1024,
)

trainer.train()
```

----------------------------------------

TITLE: Creating Push to Hub Callback
DESCRIPTION: This creates a Keras callback to push the trained model and tokenizer to the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/multiple_choice.md#_snippet_19

LANGUAGE: python
CODE:
```
>>> from transformers.keras_callbacks import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="my_awesome_model",
...     tokenizer=tokenizer,
... )
```

----------------------------------------

TITLE: Implementing Translation Evaluation Metrics Computation
DESCRIPTION: Define functions to postprocess predictions and compute evaluation metrics like BLEU score and generation length
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/translation.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
import numpy as np\n\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n    return preds, labels\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    result = {"bleu": result["score"]}\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result["gen_len"] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result
```

----------------------------------------

TITLE: Using NF4 Data Type for 4-bit Quantization
DESCRIPTION: Code demonstrating how to use the Normal Float 4 (NF4) data type for 4-bit quantization, which is optimized for weights initialized from normal distributions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/bitsandbytes.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
from transformers import BitsAndBytesConfig

nf4_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
)

model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)
```

----------------------------------------

TITLE: Initializing ViTMSNForImageClassification with SDPA in PyTorch
DESCRIPTION: This snippet demonstrates how to initialize a ViTMSNForImageClassification model using Scaled Dot Product Attention (SDPA) in PyTorch. It loads the pre-trained 'facebook/vit-msn-base' model with half-precision floating-point format for optimal performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/vit_msn.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import ViTMSNForImageClassification
model = ViTMSNForImageClassification.from_pretrained("facebook/vit-msn-base", attn_implementation="sdpa", torch_dtype=torch.float16)
...
```

----------------------------------------

TITLE: Generating Text via Transformers CLI with Jamba (Bash)
DESCRIPTION: This snippet demonstrates performing text generation using the Hugging Face `transformers` command-line interface. It pipes an input prompt to the `transformers run` command, specifying the text generation task, the Jamba-Mini-1.6 model to use, and the target device for computation (GPU 0). The command executes the generation and outputs the result.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/jamba.md#_snippet_2

LANGUAGE: bash
CODE:
```
echo -e "Plants create energy through a process known as" | transformers run --task text-generation --model ai21labs/AI21-Jamba-Mini-1.6 --device 0
```

----------------------------------------

TITLE: Loading and Using XLM Model with Language Embeddings
DESCRIPTION: Demonstrates how to load an XLM model and tokenizer for English-French translation, create language embeddings, and perform inference using the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/multilingual.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from transformers import XLMTokenizer, XLMWithLMHeadModel

tokenizer = XLMTokenizer.from_pretrained("FacebookAI/xlm-clm-enfr-1024")
model = XLMWithLMHeadModel.from_pretrained("FacebookAI/xlm-clm-enfr-1024")

input_ids = torch.tensor([tokenizer.encode("Wikipedia was used to")])
language_id = tokenizer.lang2id["en"]
langs = torch.tensor([language_id] * input_ids.shape[1])
langs = langs.view(1, -1)
outputs = model(input_ids, langs=langs)
```

----------------------------------------

TITLE: Using Flash-Attention 2 for Chameleon
DESCRIPTION: Demonstrates how to use Flash-Attention 2 to speed up generation with the Chameleon model. This example shows how to load the model with the `attn_implementation` parameter set to `flash_attention_2`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/chameleon.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import ChameleonForConditionalGeneration

model_id = "facebook/chameleon-7b"
model = ChameleonForConditionalGeneration.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    attn_implementation="flash_attention_2"
).to(0)
```

----------------------------------------

TITLE: Installing Optimum and Exporters Module
DESCRIPTION: Installs the Optimum library along with its exporters module, which is required for converting models to ONNX format. The command can be run in a terminal supporting Python's pip package manager.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/serialization.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install optimum[exporters]
```

----------------------------------------

TITLE: Loading SpQR-Quantized Model using Transformers Python
DESCRIPTION: This snippet demonstrates how to load a SpQR-quantized model using the HuggingFace Transformers library. It imports necessary libraries, initializes a quantized model with specific parameters, and loads a tokenizer. The quantized model is specified by its unique identifier, which provides compatibility settings through parameters such as 'torch_dtype' and 'device_map'.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/spqr.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

quantized_model = AutoModelForCausalLM.from_pretrained(
    "elvircrn/Llama-2-7b-SPQR-3Bit-16x16-red_pajama-hf",
    torch_dtype=torch.half,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("elvircrn/Llama-2-7b-SPQR-3Bit-16x16-red_pajama-hf")
```

----------------------------------------

TITLE: Installing Required Dependencies
DESCRIPTION: Installation of necessary Python packages for working with datasets, transformers, and image augmentation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/object_detection.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install -q datasets transformers evaluate timm albumentations
```

----------------------------------------

TITLE: Initializing AST Model with SDPA
DESCRIPTION: Example showing how to initialize an AST model for audio classification with Scaled Dot Product Attention (SDPA). The model is loaded from a pretrained checkpoint with half-precision floating point format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/audio-spectrogram-transformer.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import ASTForAudioClassification
model = ASTForAudioClassification.from_pretrained("MIT/ast-finetuned-audioset-10-10-0.4593", attn_implementation="sdpa", torch_dtype=torch.float16)
...
```

----------------------------------------

TITLE: Downloading a File from Hub
DESCRIPTION: This Python code snippet uses the `hf_hub_download` function from the `huggingface_hub` library to download a specific file (`config.json`) from a Hugging Face Hub repository (`bigscience/T0_3B`) and saves it to a local directory (`./your/path/bigscience_t0`).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/installation.md#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
">>> from huggingface_hub import hf_hub_download\n\n>>> hf_hub_download(repo_id=\"bigscience/T0_3B\", filename=\"config.json\", cache_dir=\"./your/path/bigscience_t0\")"
```

----------------------------------------

TITLE: Saving and Loading Quantized Models with EETQ
DESCRIPTION: Python code demonstrating how to save a quantized model using save_pretrained and then load it again with from_pretrained. The quantization settings are automatically applied when loading.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/eetq.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
quant_path = "/path/to/save/quantized/model"
model.save_pretrained(quant_path)
model = AutoModelForCausalLM.from_pretrained(quant_path, device_map="auto")
```

----------------------------------------

TITLE: Initializing LayoutLMv2 Processor with OCR Enabled - Python
DESCRIPTION: This snippet initializes the LayoutLMv2Processor with the image processor and tokenizer, allowing for document image classification and token classification by applying OCR to the image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/layoutlmv2.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import LayoutLMv2ImageProcessor, LayoutLMv2TokenizerFast, LayoutLMv2Processor

image_processor = LayoutLMv2ImageProcessor()  # apply_ocr is set to True by default
tokenizer = LayoutLMv2TokenizerFast.from_pretrained("microsoft/layoutlmv2-base-uncased")
processor = LayoutLMv2Processor(image_processor, tokenizer)
```

----------------------------------------

TITLE: Loading Wav2Vec2 processor
DESCRIPTION: Initialize the Wav2Vec2 processor for handling audio signals and text processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/tasks/asr.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base")
```

----------------------------------------

TITLE: Loading a Tokenizer from a JSON File into PreTrainedTokenizerFast
DESCRIPTION: This code snippet demonstrates how to load a tokenizer from a JSON file into the PreTrainedTokenizerFast class. This allows you to load a previously saved tokenizer and use it with the ðŸ¤— Transformers library. The `tokenizer_file` parameter specifies the path to the JSON file.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/fast_tokenizers.md#_snippet_3

LANGUAGE: Python
CODE:
```
>>> from transformers import PreTrainedTokenizerFast

>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file="tokenizer.json")
```

----------------------------------------

TITLE: Creating Push to Hub Callback in TensorFlow
DESCRIPTION: This code creates a `PushToHubCallback` to push the trained model and tokenizer to the Hugging Face Hub. It specifies the output directory and tokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_23

LANGUAGE: python
CODE:
```
>>> from transformers.keras_callbacks import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="my_awesome_wnut_model",
...     tokenizer=tokenizer,
... )
```

----------------------------------------

TITLE: Loading and Measuring Memory of a Pretrained Causal Language Model - Python
DESCRIPTION: This Python code snippet loads a pretrained model using Hugging Face Transformers and prints the memory usage of its parameters in GB.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/compressed_tensors.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

ct_model = AutoModelForCausalLM.from_pretrained("nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf", device_map="auto")

# measure memory usage
mem_params = sum([param.nelement()*param.element_size() for param in ct_model.parameters()])
print(f"{mem_params/2**30:.4f} GB")
# 8.4575 GB
```

----------------------------------------

TITLE: Generating Text from Image and Text Inputs with VLM
DESCRIPTION: This code generates text from the preprocessed inputs using the image-text-to-text model.  It disables gradient calculation for efficiency, generates the output tokens, and then decodes them into text. Special tokens are skipped during decoding to obtain clean output text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_text_to_text.md#_snippet_5

LANGUAGE: python
CODE:
```
with torch.no_grad():
    generated_ids = model.generate(**inputs, max_new_tokens=500)
generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)

print(generated_texts)
```

----------------------------------------

TITLE: Loading Bark Model in Half Precision
DESCRIPTION: Demonstrates how to load the Bark model in half-precision (float16) format to reduce memory usage by 50% and improve inference speed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/bark.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import BarkModel
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"
model = BarkModel.from_pretrained("suno/bark-small", torch_dtype=torch.float16).to(device)
```

----------------------------------------

TITLE: Running Distributed Training with Mixed Precision in PyTorch
DESCRIPTION: Command to run a PyTorch-based summarization example with distributed training across 8 GPUs and mixed precision training enabled using torchrun.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/run_scripts.md#2025-04-23_snippet_5

LANGUAGE: bash
CODE:
```
torchrun \
    --nproc_per_node 8 pytorch/summarization/run_summarization.py \
    --fp16 \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Adding trainable layers
DESCRIPTION: Configures a LoRA adapter to also train the language model head (`lm_head`) in addition to the default target modules. This allows for fine-tuning specific layers beyond the LoRA adaptation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/peft.md#_snippet_14

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import LoraConfig

model_id = "facebook/opt-350m"
model = AutoModelForCausalLM.from_pretrained(model_id)

lora_config = LoraConfig(
    target_modules=["q_proj", "k_proj"],
    modules_to_save=["lm_head"]ØŒ
)

model.add_adapter(lora_config)
```

----------------------------------------

TITLE: Loading ELI5 Dataset Sample
DESCRIPTION: Load first 5000 examples from the ELI5 dataset and split into train/test sets
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/masked_language_modeling.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> eli5 = load_dataset("eli5_category", split="train[:5000]")
>>> eli5 = eli5.train_test_split(test_size=0.2)
```

----------------------------------------

TITLE: Image Preprocessing for Swin2SR Model
DESCRIPTION: Preprocesses the input image using the Swin2SR processor to convert it into pixel values suitable for the model, then moves the tensor to the appropriate device.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_to_image.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
pixel_values = processor(image, return_tensors="pt").pixel_values
print(pixel_values.shape)

pixel_values = pixel_values.to(device)
```

----------------------------------------

TITLE: Preprocessing Dataset with GIT Processor
DESCRIPTION: Loads the GIT processor and defines a transform function to preprocess images and captions for model input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/image_captioning.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoProcessor

checkpoint = "microsoft/git-base"
processor = AutoProcessor.from_pretrained(checkpoint)

def transforms(example_batch):
    images = [x for x in example_batch["image"]]
    captions = [x for x in example_batch["text"]]
    inputs = processor(images=images, text=captions, padding="max_length")
    inputs.update({"labels": inputs["input_ids"]})
    return inputs

train_ds.set_transform(transforms)
test_ds.set_transform(transforms)
```

----------------------------------------

TITLE: Pushing Trained Model to Hugging Face Hub in PyTorch
DESCRIPTION: This code demonstrates how to push the trained model to the Hugging Face Hub using the Trainer's push_to_hub method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
trainer.push_to_hub()
```

----------------------------------------

TITLE: Resampling MInDS-14 Audio to 16kHz
DESCRIPTION: Resamples the audio data in the MInDS-14 dataset from 8kHz to 16kHz to match the pre-trained Wav2Vec2 model requirements.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/asr.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
```

----------------------------------------

TITLE: Sampling Frames from Video URLs
DESCRIPTION: This code snippet downloads two videos from specified URLs, samples frames from each video using the `sample_frames` function, and concatenates the resulting frame lists into a single `videos` list.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/video_text_to_text.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
"video_1 = \"https://huggingface.co/spaces/merve/llava-interleave/resolve/main/cats_1.mp4\" 
video_2 = \"https://huggingface.co/spaces/merve/llava-interleave/resolve/main/cats_2.mp4\" 

video_1 = sample_frames(video_1, 6)
video_2 = sample_frames(video_2, 6)

videos = video_1 + video_2

videos

# [<PIL.Image.Image image mode=RGB size=1920x1080>,
# <PIL.Image.Image image mode=RGB size=1920x1080>,
# <PIL.Image.Image image mode=RGB size=1920x1080>, ...]"
```

----------------------------------------

TITLE: Defining Metrics Computation for Translation Evaluation
DESCRIPTION: This function calculates the SacreBLEU score for translation quality by processing model predictions and reference translations, handling special tokens and formatting appropriately.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/translation.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> import numpy as np


>>> def postprocess_text(preds, labels):
...     preds = [pred.strip() for pred in preds]
...     labels = [[label.strip()] for label in labels]

...     return preds, labels


>>> def compute_metrics(eval_preds):
...     preds, labels = eval_preds
...     if isinstance(preds, tuple):
...         preds = preds[0]
...     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

...     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
...     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

...     decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

...     result = metric.compute(predictions=decoded_preds, references=decoded_labels)
...     result = {"bleu": result["score"]}

...     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
...     result["gen_len"] = np.mean(prediction_lens)
...     result = {k: round(v, 4) for k, v in result.items()}
...     return result
```

----------------------------------------

TITLE: Loading SWAG Dataset in Python
DESCRIPTION: Loads the SWAG dataset using the Hugging Face Datasets library and displays a sample from the training set.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/multiple_choice.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> swag = load_dataset("swag", "regular")

>>> swag["train"][0]
```

----------------------------------------

TITLE: Removing Text Column
DESCRIPTION: Removes the 'text' column from the tokenized dataset.  The model does not accept raw text as input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/training.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
tokenized_datasets = tokenized_datasets.remove_columns(["text"])
```

----------------------------------------

TITLE: Enabling FlashAttention with SDPA Kernel
DESCRIPTION: Demonstrate how to explicitly enable FlashAttention using PyTorch's SDPA kernel context manager
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_gpu_one.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
import torch
from torch.nn.attention import SDPBackend, sdpa_kernel
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-8B")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.1-8B", device_map="auto").to("cuda")

input_text = "Hello, my llama is cute"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

with sdpa_kernel(SDPBackend.FLASH_ATTENTION):
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Object Detection with Transformers Pipeline
DESCRIPTION: This code snippet shows how to perform object detection using the Transformers pipeline. It detects objects in an image and returns their labels, scores, and bounding boxes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/task_summary.md#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
>>> from transformers import pipeline

>>> detector = pipeline(task="object-detection")
>>> preds = detector(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"], "box": pred["box"]} for pred in preds]
>>> preds
[{'score': 0.9865,
  'label': 'cat',
  'box': {'xmin': 178, 'ymin': 154, 'xmax': 882, 'ymax': 598}}]
```

----------------------------------------

TITLE: Generating Text Embeddings
DESCRIPTION: The snippet demonstrates how to generate text embeddings using the ColPali processor and model. It involves passing a text query through the processor, followed by obtaining embeddings using the model. The operation is performed on a CUDA device for acceleration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_document_retrieval.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
inputs = processor(text="a document about Mars expedition").to("cuda")
with torch.no_grad():
  text_embeds = model(**inputs, return_tensors="pt").embeddings
```

----------------------------------------

TITLE: Loading the SWAG Dataset
DESCRIPTION: This code loads the 'regular' configuration of the SWAG dataset from the Hugging Face Datasets library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/multiple_choice.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> swag = load_dataset("swag", "regular")
```

----------------------------------------

TITLE: Displaying Top Predictions for Masked Token (TensorFlow)
DESCRIPTION: Code to find the top three token predictions for the masked position and print the complete text with each prediction inserted in place of the mask token in TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_29

LANGUAGE: python
CODE:
```
top_3_tokens = tf.math.top_k(mask_token_logits, 3).indices.numpy()

for token in top_3_tokens:
    print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))
```

----------------------------------------

TITLE: Loading and Inspecting Audio Dataset
DESCRIPTION: This snippet demonstrates loading an audio dataset and inspecting its structure. It loads the `lj_speech` dataset, removes unnecessary columns, and displays the content of the `audio` and `text` fields for the first entry, showcasing the audio array and corresponding text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/processors.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from datasets import load_dataset

dataset = load_dataset("lj_speech", split="train")
dataset = dataset.map(remove_columns=["file", "id", "normalized_text"])
dataset[0]["audio"]
{'array': array([-7.3242188e-04, -7.6293945e-04, -6.4086914e-04, ...,
         7.3242188e-04,  2.1362305e-04,  6.1035156e-05], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/917ece08c95cf0c4115e45294e3cd0dee724a1165b7fc11798369308a465bd26/LJSpeech-1.1/wavs/LJ001-0001.wav',
 'sampling_rate': 22050}

dataset[0]["text"]
'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition'
```

----------------------------------------

TITLE: Saving PyTorch Model Locally for ONNX Export
DESCRIPTION: Python code to load a PyTorch model and tokenizer from the Hub and save them locally for ONNX export.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/serialization.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> # Load tokenizer and PyTorch weights form the Hub
>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
>>> pt_model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
>>> # Save to disk
>>> tokenizer.save_pretrained("local-pt-checkpoint")
>>> pt_model.save_pretrained("local-pt-checkpoint")
```

----------------------------------------

TITLE: Configuring Idefics3 Image Processor with Custom Resizing
DESCRIPTION: Demonstrates how to configure the Idefics3 image processor with custom resizing parameters, controlling the longest edge and maximum image patch size
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/idefics3.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
image_processor = Idefics3ImageProcessor(do_resize=True, size={"longest_edge": 2 * 364}, max_image_size=364)
```

----------------------------------------

TITLE: Translating Text Using MarianMTModel in Python
DESCRIPTION: This snippet demonstrates how to use the MarianMTModel and MarianTokenizer from the Transformers library to translate text from English to multiple Romance languages using a specific model. It shows how to prepare input text, load the model, and decode the output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/marian.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import MarianMTModel, MarianTokenizer

>>> src_text = [
...     ">>fr<< this is a sentence in english that we want to translate to french",
...     ">>pt<< This should go to portuguese",
...     ">>es<< And this to Spanish",
... ]

>>> model_name = "Helsinki-NLP/opus-mt-en-ROMANCE"
>>> tokenizer = MarianTokenizer.from_pretrained(model_name)

>>> model = MarianMTModel.from_pretrained(model_name)
>>> translated = model.generate(**tokenizer(src_text, return_tensors="pt", padding=True))
>>> tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]
["c'est une phrase en anglais que nous voulons traduire en franÃ§ais", 
 'Isto deve ir para o portuguÃªs.',
 'Y esto al espaÃ±ol']
```

----------------------------------------

TITLE: TAPEX Table Question Answering with Auto API in Python
DESCRIPTION: This code snippet demonstrates how to use the TAPEX model for table question answering using the Auto API from the Transformers library. It loads the pre-trained TAPEX model and tokenizer, prepares a Pandas DataFrame and a question, encodes them using the tokenizer, generates an answer using the model, and decodes the generated output. The output is the predicted answer to the question based on the table content.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/tapex.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
>>> import pandas as pd

>>> tokenizer = AutoTokenizer.from_pretrained("microsoft/tapex-large-finetuned-wtq")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("microsoft/tapex-large-finetuned-wtq")

>>> # prepare table + question
>>> data = {"Actors": ["Brad Pitt", "Leonardo Di Caprio", "George Clooney"], "Number of movies": ["87", "53", "69"]}
>>> table = pd.DataFrame.from_dict(data)
>>> question = "how many movies does Leonardo Di Caprio have?"

>>> encoding = tokenizer(table, question, return_tensors="pt")

>>> # let the model generate an answer autoregressively
>>> outputs = model.generate(**encoding)

>>> # decode back to text
>>> predicted_answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
>>> print(predicted_answer)
53
```

----------------------------------------

TITLE: Setting Up Accuracy Evaluation Metrics
DESCRIPTION: Shows how to load an accuracy metric from the Evaluate library and create a compute_metrics function to calculate model accuracy during evaluation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/training.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> import numpy as np
>>> import evaluate

>>> metric = evaluate.load("accuracy")
```

----------------------------------------

TITLE: Using DistilBert with Sequence Classification Head in TensorFlow
DESCRIPTION: Shows how to load a pre-trained DistilBert model with a sequence classification head in TensorFlow for tasks like sentiment analysis or text classification.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/create_a_model.md#2025-04-23_snippet_13

LANGUAGE: python
CODE:
```
>>> from transformers import TFDistilBertForSequenceClassification

>>> tf_model = TFDistilBertForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Distributed T0 Model Inference with DeepSpeed
DESCRIPTION: Complete script showing how to load and run inference on T0 model using DeepSpeed ZeRO-3 with CPU offloading. Handles distributed setup, model configuration, and synchronized generation across multiple GPUs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/deepspeed.md#2025-04-22_snippet_43

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM
from transformers.integrations import HfDeepSpeedConfig
import deepspeed
import os
import torch

os.environ["TOKENIZERS_PARALLELISM"] = "false"  

local_rank = int(os.getenv("LOCAL_RANK", "0"))
world_size = int(os.getenv("WORLD_SIZE", "1"))
torch.cuda.set_device(local_rank)
deepspeed.init_distributed()

model_name = "bigscience/T0_3B"

config = AutoConfig.from_pretrained(model_name)
model_hidden_size = config.d_model

train_batch_size = 1 * world_size

ds_config = {
    "fp16": {
        "enabled": False
    },
    "bf16": {
        "enabled": False
    },
    "zero_optimization": {
        "stage": 3,
        "offload_param": {
            "device": "cpu",
            "pin_memory": True
        },
        "overlap_comm": True,
        "contiguous_gradients": True,
        "reduce_bucket_size": model_hidden_size * model_hidden_size,
        "stage3_prefetch_bucket_size": 0.9 * model_hidden_size * model_hidden_size,
        "stage3_param_persistence_threshold": 10 * model_hidden_size
    },
    "steps_per_print": 2000,
    "train_batch_size": train_batch_size,
    "train_micro_batch_size_per_gpu": 1,
    "wall_clock_breakdown": False
}

dschf = HfDeepSpeedConfig(ds_config)

model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

ds_engine = deepspeed.initialize(model=model, config_params=ds_config)[0]
ds_engine.module.eval()

rank = torch.distributed.get_rank()
if rank == 0:
    text_in = "Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy"
elif rank == 1:
    text_in = "Is this review positive or negative? Review: this is the worst restaurant ever"

tokenizer = AutoTokenizer.from_pretrained(model_name)
inputs = tokenizer.encode(text_in, return_tensors="pt").to(device=local_rank)
with torch.no_grad():
    outputs = ds_engine.module.generate(inputs, synced_gpus=True)
text_out = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"rank{rank}:\n   in={text_in}\n  out={text_out}")
```

----------------------------------------

TITLE: Perform Zero-Shot Image Classification with SigLIP2 FixRes using AutoModel (Python)
DESCRIPTION: This code demonstrates manual zero-shot image classification using the SigLIP2 FixRes model with `AutoModel` and `AutoProcessor`. It involves loading the model and processor, fetching and preparing the image and text inputs, and performing the forward pass to get classification probabilities. It highlights the importance of specific padding and max_length for text processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/siglip2.md#_snippet_1

LANGUAGE: Python
CODE:
```
import torch
import requests
from PIL import Image
from transformers import AutoProcessor, AutoModel

model = AutoModel.from_pretrained("google/siglip2-base-patch16-224", torch_dtype=torch.float16, device_map="auto", attn_implementation="sdpa")
processor = AutoProcessor.from_pretrained("google/siglip2-base-patch16-224")

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
image = Image.open(requests.get(url, stream=True).raw)
candidate_labels = ["a Pallas cat", "a lion", "a Siberian tiger"]

# follows the pipeline prompt template to get same results
texts = [f'This is a photo of {label}.' for label in candidate_labels]

# IMPORTANT: we pass `padding=max_length` and `max_length=64` since the model was trained with this
inputs = processor(text=texts, images=image, padding="max_length", max_length=64, return_tensors="pt").to("cuda")

with torch.no_grad():
    outputs = model(**inputs)

logits_per_image = outputs.logits_per_image
probs = torch.sigmoid(logits_per_image)
print(f"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'")
```

----------------------------------------

TITLE: Implementing Custom ResNet Model in Python
DESCRIPTION: Defines two custom model classes for ResNet: ResnetModel for feature extraction and ResnetModelForImageClassification for image classification. Both inherit from PreTrainedModel and use the custom ResnetConfig.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/custom_models.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import PreTrainedModel
from timm.models.resnet import BasicBlock, Bottleneck, ResNet
from .configuration_resnet import ResnetConfig


BLOCK_MAPPING = {"basic": BasicBlock, "bottleneck": Bottleneck}


class ResnetModel(PreTrainedModel):
    config_class = ResnetConfig

    def __init__(self, config):
        super().__init__(config)
        block_layer = BLOCK_MAPPING[config.block_type]
        self.model = ResNet(
            block_layer,
            config.layers,
            num_classes=config.num_classes,
            in_chans=config.input_channels,
            cardinality=config.cardinality,
            base_width=config.base_width,
            stem_width=config.stem_width,
            stem_type=config.stem_type,
            avg_down=config.avg_down,
        )

    def forward(self, tensor):
        return self.model.forward_features(tensor)


import torch


class ResnetModelForImageClassification(PreTrainedModel):
    config_class = ResnetConfig

    def __init__(self, config):
        super().__init__(config)
        block_layer = BLOCK_MAPPING[config.block_type]
        self.model = ResNet(
            block_layer,
            config.layers,
            num_classes=config.num_classes,
            in_chans=config.input_channels,
            cardinality=config.cardinality,
            base_width=config.base_width,
            stem_width=config.stem_width,
            stem_type=config.stem_type,
            avg_down=config.avg_down,
        )

    def forward(self, tensor, labels=None):
        logits = self.model(tensor)
        if labels is not None:
            loss = torch.nn.functional.cross_entropy(logits, labels)
            return {"loss": loss, "logits": logits}
        return {"logits": logits}
```

----------------------------------------

TITLE: Exporting a TensorFlow Model to ONNX via CLI
DESCRIPTION: This command demonstrates how to export a TensorFlow model from the Keras organization on Hugging Face Hub to ONNX format using the optimum-cli.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/serialization.md#_snippet_5

LANGUAGE: bash
CODE:
```
optimum-cli export onnx --model keras-io/transformers-qa distilbert_base_cased_squad_onnx/
```

----------------------------------------

TITLE: Processing Multiple Images with Aya Vision
DESCRIPTION: Illustrates how to process multiple images in a single conversation, demonstrating the model's ability to handle complex multimodal inputs with landmarks from different locations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/aya_vision.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "url": "https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg",
            },
            {
                "type": "image",
                "url": "https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg",
            },
            {
                "type": "text",
                "text": "These images depict two different landmarks. Can you identify them?",
            },
        ],
    },
]
```

----------------------------------------

TITLE: Exporting a TensorFlow Model to ONNX
DESCRIPTION: Command to export a TensorFlow checkpoint from the Keras organization on Hugging Face Hub to ONNX format using the Optimum CLI.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/serialization.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
optimum-cli export onnx --model keras-io/transformers-qa distilbert_base_cased_squad_onnx/
```

----------------------------------------

TITLE: Loading PEFT adapter in 8-bit
DESCRIPTION: Loads a PEFT adapter model in 8-bit precision using the `bitsandbytes` integration. This reduces memory usage. Requires the `bitsandbytes` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/peft.md#_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```

----------------------------------------

TITLE: Creating Trainer Instance for Hyperparameter Search in Python
DESCRIPTION: Example of creating a Trainer instance with necessary arguments for hyperparameter search, including model initialization function.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/hpo_train.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
trainer = Trainer(
    model=None,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
    processing_class=tokenizer,
    model_init=model_init,
    data_collator=data_collator,
)
```

----------------------------------------

TITLE: Loading and Compiling TensorFlow Model
DESCRIPTION: This snippet shows how to load a semantic segmentation model using TFAutoModelForSemanticSegmentation and compile it with an optimizer. The need for specifying a loss function is mentioned.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained(
...     checkpoint,
...     id2label=id2label,
...     label2id=label2id,
... )
>>> model.compile(optimizer=optimizer)  # No loss argument!
```

----------------------------------------

TITLE: Loading and Processing SQuAD Data with Transformers
DESCRIPTION: This snippet demonstrates how to use SQuAD processors to load and convert examples into features for model input. It shows examples for both SQuAD v1 and v2, as well as using tensorflow_datasets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/processors.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
# Loading a V2 processor
processor = SquadV2Processor()
examples = processor.get_dev_examples(squad_v2_data_dir)

# Loading a V1 processor
processor = SquadV1Processor()
examples = processor.get_dev_examples(squad_v1_data_dir)

features = squad_convert_examples_to_features(
    examples=examples,
    tokenizer=tokenizer,
    max_seq_length=max_seq_length,
    doc_stride=args.doc_stride,
    max_query_length=max_query_length,
    is_training=not evaluate,
)

# tensorflow_datasets only handle Squad V1.
tfds_examples = tfds.load("squad")
examples = SquadV1Processor().get_examples_from_dataset(tfds_examples, evaluate=evaluate)

features = squad_convert_examples_to_features(
    examples=examples,
    tokenizer=tokenizer,
    max_seq_length=max_seq_length,
    doc_stride=args.doc_stride,
    max_query_length=max_query_length,
    is_training=not evaluate,
)
```

----------------------------------------

TITLE: Initializing TrainingArguments for GrokAdamW Optimizer in PyTorch
DESCRIPTION: This snippet initializes TrainingArguments for the GrokAdamW optimizer in PyTorch. The parameters ensure optimal training configuration for models that require advanced optimization tactics.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/optimizers.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
import torch
from transformers import TrainingArguments

args = TrainingArguments(
    output_dir="./test-grokadamw",
    max_steps=1000,
    per_device_train_batch_size=4,
+   optim="grokadamw",
    logging_strategy="steps",
    logging_steps=1,
    learning_rate=2e-5,
    save_strategy="no",
    run_name="grokadamw",
)
```

----------------------------------------

TITLE: Extracting Features from Audio Input
DESCRIPTION: This snippet demonstrates how to use the loaded feature extractor to process audio input. The audio array is passed to the feature extractor along with the sampling rate. The feature extractor returns a dictionary containing the 'input_values', which are the processed audio features.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/preprocessing.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> audio_input = [dataset[0]["audio"]["array"]]
>>> feature_extractor(audio_input, sampling_rate=16000)
{'input_values': [array([ 3.8106556e-04,  2.7506407e-03,  2.8015103e-03, ...,
        5.6335266e-04,  4.6588284e-06, -1.7142107e-04], dtype=float32)]}
```

----------------------------------------

TITLE: Generating Output from Video-Text-to-Text Model
DESCRIPTION: This code generates the model's response to the input. It calls the `generate` method of the model and then decodes the output to extract the answer, removing the prompt and the "assistant" prefix.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/video_text_to_text.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
"output = model.generate(**inputs, max_new_tokens=100, do_sample=False)
print(processor.decode(output[0][2:], skip_special_tokens=True)[len(user_prompt)+10:])

# The first cat is shown in a relaxed state, with its eyes closed and a content expression, while the second cat is shown in a more active state, with its mouth open wide, possibly in a yawn or a vocalization.

"
```

----------------------------------------

TITLE: Configuring PushToHubCallback for TensorFlow in Python
DESCRIPTION: Illustrates how to set up the PushToHubCallback for automatic model sharing when training with TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/model_sharing.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> from transformers import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="./your_model_save_path", tokenizer=tokenizer, hub_model_id="your-username/my-awesome-model"
... )
```

----------------------------------------

TITLE: Processing the Entire Dataset in Python
DESCRIPTION: Applies the processing function to the entire dataset using the map method. This converts all examples to the format required by the TTS model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/text-to-speech.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
>>> dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)
```

----------------------------------------

TITLE: Generating Response Using the Model - Python
DESCRIPTION: In this snippet, the tokenized input prepared for the Zephyr model is used to generate a text response from the model, illustrating interaction with the chat system by outputting a generated assistant message.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/chat_templating.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
outputs = model.generate(tokenized_chat, max_new_tokens=128) 
print(tokenizer.decode(outputs[0]))
```

----------------------------------------

TITLE: Creating Label Mappings for Multi-Label Classification in Python
DESCRIPTION: This code snippet uses itertools to flatten the labels and create dictionaries mapping label names to integers and vice versa. These mappings are necessary for transforming dataset labels into a suitable format for training the VQA model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_question_answering.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
import itertools

labels = [item['ids'] for item in dataset['label']]
flattened_labels = list(itertools.chain(*labels))
unique_labels = list(set(flattened_labels))

label2id = {label: idx for idx, label in enumerate(unique_labels)}
id2label = {idx: label for label, idx in label2id.items()}
```

----------------------------------------

TITLE: Upgrading Transformers Package
DESCRIPTION: This command demonstrates how to upgrade the Transformers package to the latest version using pip. This can resolve `ImportError` issues that arise due to outdated versions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/troubleshooting.md#_snippet_2

LANGUAGE: Bash
CODE:
```
pip install transformers --upgrade
```

----------------------------------------

TITLE: Loading MBart Model for Multilingual Translation
DESCRIPTION: This code loads the MBart model and tokenizer for multilingual translation, specifically for translating from Finnish to English.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/multilingual.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
fi_text = "Ã„lÃ¤ sekaannu velhojen asioihin, sillÃ¤ ne ovat hienovaraisia ja nopeasti vihaisia."

tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt", src_lang="fi_FI")
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
```

----------------------------------------

TITLE: Load Quantized Llama 2 Model with torchao (Python)
DESCRIPTION: Demonstrates how to load a Llama 2 model with quantization enabled using `torchao`. It configures `int4_weight_only` quantization and then uses the quantized model for text generation similar to the `AutoModel` example. Requires the `torchao` library installed alongside `transformers` and PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llama2.md#_snippet_3

LANGUAGE: python
CODE:
```
# pip install torchao
import torch
from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer

quantization_config = TorchAoConfig("int4_weight_only", group_size=128)
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-13b-hf",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-13b-hf")
input_ids = tokenizer("Plants create energy through a process known as", return_tensors="pt").to("cuda")

output = model.generate(**input_ids, cache_implementation="static")
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Implementing a Metrics Computation Function
DESCRIPTION: Creates a function to calculate precision, recall, F1, and accuracy metrics using the seqeval evaluation library. This handles the conversion of predictions to label names and ignores padded tokens.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/token_classification.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> import numpy as np

>>> labels = [label_list[i] for i in example[f"ner_tags"]]


>>> def compute_metrics(p):
...     predictions, labels = p
...     predictions = np.argmax(predictions, axis=2)

...     true_predictions = [
...         [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
...         for prediction, label in zip(predictions, labels)
...     ]
...     true_labels = [
...         [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
...         for prediction, label in zip(predictions, labels)
...     ]

...     results = seqeval.compute(predictions=true_predictions, references=true_labels)
...     return {
...         "precision": results["overall_precision"],
...         "recall": results["overall_recall"],
...         "f1": results["overall_f1"],
...         "accuracy": results["overall_accuracy"],
...     }
```

----------------------------------------

TITLE: Multi-node training with DeepSpeed launcher
DESCRIPTION: Command to launch multi-node training using DeepSpeed's launcher with a hostfile.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/deepspeed.md#2025-04-22_snippet_14

LANGUAGE: bash
CODE:
```
deepspeed --num_gpus 8 --num_nodes 2 --hostfile hostfile --master_addr hostname1 --master_port=9901 \
your_program.py <normal cl args> --deepspeed ds_config.json
```

----------------------------------------

TITLE: Compiling Object Detection Model with torch.compile()
DESCRIPTION: Shows how to apply torch.compile() to a DETR model for object detection. The code loads a pre-trained model, applies optimization, and performs inference on text and image inputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/perf_torch_compile.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoImageProcessor, AutoModelForObjectDetection

processor = AutoImageProcessor.from_pretrained("facebook/detr-resnet-50")
model = AutoModelForObjectDetection.from_pretrained("facebook/detr-resnet-50").to("cuda")
model = torch.compile(model)

texts = ["a photo of a cat", "a photo of a dog"]
inputs = processor(text=texts, images=image, return_tensors="pt").to("cuda")

with torch.no_grad():
    _ = model(**inputs)
```

----------------------------------------

TITLE: Using DeiTForImageClassification with SDPA - PyTorch
DESCRIPTION: This snippet demonstrates how to load the DeiTForImageClassification model with scaled dot-product attention (SDPA) for efficient inference. Ensure that PyTorch version is 2.1.1 or higher.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/deit.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import DeiTForImageClassification
model = DeiTForImageClassification.from_pretrained("facebook/deit-base-distilled-patch16-224", attn_implementation="sdpa", torch_dtype=torch.float16)
```

----------------------------------------

TITLE: Running OWL-ViT Model Inference and Post-processing Results
DESCRIPTION: Performs inference with the OWL-ViT model and post-processes the results to get the bounding boxes aligned with the original image. The results are then visualized on the image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/zero_shot_object_detection.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> import torch

>>> with torch.no_grad():
...     outputs = model(**inputs)
...     target_sizes = torch.tensor([im.size[::-1]])
...     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)[0]

>>> draw = ImageDraw.Draw(im)

>>> scores = results["scores"].tolist()
>>> labels = results["labels"].tolist()
>>> boxes = results["boxes"].tolist()

>>> for box, score, label in zip(boxes, scores, labels):
...     xmin, ymin, xmax, ymax = box
...     draw.rectangle((xmin, ymin, xmax, ymax), outline="red", width=1)
...     draw.text((xmin, ymin), f"{text_queries[label]}: {round(score,2)}", fill="white")

>>> im
```

----------------------------------------

TITLE: Loading a Previously Quantized Model from Hub
DESCRIPTION: Code showing how to load a previously quantized model from the Hugging Face Hub without specifying quantization config.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/bitsandbytes.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("{your_username}/bloom-560m-8bit", device_map="auto")
```

----------------------------------------

TITLE: Loading Model and Processor for Manual Classification
DESCRIPTION: Loads a pre-trained model and processor for manual zero-shot image classification.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/zero_shot_image_classification.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor, AutoModelForZeroShotImageClassification

>>> model = AutoModelForZeroShotImageClassification.from_pretrained(checkpoint)
>>> processor = AutoProcessor.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Extracting predicted class (TensorFlow)
DESCRIPTION: This snippet demonstrates how to extract the predicted class with the highest probability from the logits and convert it to a text label using id2label from the model's configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_31

LANGUAGE: python
CODE:
```
>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])
>>> model.config.id2label[predicted_class_id]
'POSITIVE'
```

----------------------------------------

TITLE: Depth Estimation with Pipeline API in ZoeDepth
DESCRIPTION: This snippet demonstrates how to use the pipeline API in Transformers to perform depth estimation using the ZoeDepth model. It initializes a pipeline with the "depth-estimation" task and the "Intel/zoedepth-nyu-kitti" model, loads an image from a URL, and then uses the pipeline to predict the depth map. The resulting depth map is stored in the `depth` variable.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/zoedepth.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
>>> from transformers import pipeline
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> pipe = pipeline(task="depth-estimation", model="Intel/zoedepth-nyu-kitti")
>>> result = pipe(image)
>>> depth = result["depth"]
```

----------------------------------------

TITLE: Custom Distillation Trainer Implementation
DESCRIPTION: Implements a custom trainer class that handles the knowledge distillation process using KL divergence loss between teacher and student models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/knowledge_distillation_for_image_classification.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments, Trainer
import torch
import torch.nn as nn
import torch.nn.functional as F


class ImageDistilTrainer(Trainer):
    def __init__(self, *args, teacher_model=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.teacher = teacher_model
        self.student = student_model
        self.loss_function = nn.KLDivLoss(reduction="batchmean")
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.teacher.to(device)
        self.teacher.eval()
        self.temperature = temperature
        self.lambda_param = lambda_param

    def compute_loss(self, student, inputs, return_outputs=False):
        student_output = self.student(**inputs)

        with torch.no_grad():
          teacher_output = self.teacher(**inputs)

        # Compute soft targets for teacher and student
        soft_teacher = F.softmax(teacher_output.logits / self.temperature, dim=-1)
        soft_student = F.log_softmax(student_output.logits / self.temperature, dim=-1)

        # Compute the loss
        distillation_loss = self.loss_function(soft_student, soft_teacher) * (self.temperature ** 2)

        # Compute the true label loss
        student_target_loss = student_output.loss

        # Calculate final loss
        loss = (1. - self.lambda_param) * student_target_loss + self.lambda_param * distillation_loss
        return (loss, student_output) if return_outputs else loss
```

----------------------------------------

TITLE: Directly Pushing a Model to Hub
DESCRIPTION: Shows how to use the push_to_hub method directly on a model object to upload it to the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/model_sharing.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
>>> pt_model.push_to_hub("il-mio-bellissimo-modello")
```

----------------------------------------

TITLE: Using SigLIP Model Directly for Image Classification
DESCRIPTION: Shows how to use the SigLIP model directly, performing preprocessing and postprocessing manually. It loads the model and processor, processes inputs, and computes probabilities using sigmoid activation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/siglip.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
from PIL import Image
import requests
from transformers import AutoProcessor, AutoModel
import torch

model = AutoModel.from_pretrained("google/siglip-base-patch16-224")
processor = AutoProcessor.from_pretrained("google/siglip-base-patch16-224")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

candidate_labels = ["2 cats", "2 dogs"]
texts = [f'This is a photo of {label}.' for label in candidate_labels]
inputs = processor(text=texts, images=image, padding="max_length", return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

logits_per_image = outputs.logits_per_image
probs = torch.sigmoid(logits_per_image)
print(f"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'")
```

----------------------------------------

TITLE: TensorFlow Predicted Class
DESCRIPTION: Determines the predicted class by finding the index with the highest logit value. The `argmax` function returns the index, which is then extracted and converted to a Python integer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/multiple_choice.md#_snippet_28

LANGUAGE: Python
CODE:
```
>>> predicted_class = int(tf.math.argmax(logits, axis=-1)[0])
>>> predicted_class
'0'
```

----------------------------------------

TITLE: Running DeepSpeed Inference Script with Multiple GPUs in Bash
DESCRIPTION: This bash command demonstrates how to run the DeepSpeed-enabled Python script across multiple GPUs using the deepspeed command-line tool.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/deepspeed.md#2025-04-22_snippet_32

LANGUAGE: bash
CODE:
```
$ deepspeed --num_gpus 2 t0.py
```

----------------------------------------

TITLE: Initializing TrainingArguments for AdaLomo Optimizer in PyTorch
DESCRIPTION: This snippet illustrates how to initialize TrainingArguments for the AdaLomo optimizer, designed for low-memory full-parameter fine-tuning of large language models while incorporating an adaptive learning rate.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/optimizers.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
args = TrainingArguments(
    output_dir="./test-lomo",
    max_steps=1000,
    per_device_train_batch_size=4,
+   optim="adalomo",
    gradient_checkpointing=True,
    gradient_checkpointing=True,
    logging_strategy="steps",
    logging_steps=1,
    learning_rate=2e-6,
    save_strategy="no",
    run_name="adalomo",
)
```

----------------------------------------

TITLE: Text Dataset Tokenization and Preparation
DESCRIPTION: Tokenize text dataset, add tokenizer outputs as columns, and prepare TensorFlow dataset for sequence classification
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
def tokenize_function(examples):
    return tokenizer(
        examples["sentence"], padding="max_length", truncation=True, max_length=128
    )

dataset = dataset.map(tokenize_function)

tf_dataset = model.prepare_tf_dataset(
    dataset, batch_size=BATCH_SIZE, shuffle=True, tokenizer=tokenizer
)
```

----------------------------------------

TITLE: Distributed Training with Mixed Precision
DESCRIPTION: Command to enable distributed training across multiple GPUs with mixed precision using torchrun. Includes fp16 flag and node process configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/run_scripts.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
torchrun \
    --nproc_per_node 8 pytorch/summarization/run_summarization.py \
    --fp16 \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Saving Multiple Generation Configurations
DESCRIPTION: This example demonstrates saving multiple named generation configurations for a single model. The `config_file_name` argument of `save_pretrained` is used to specify a unique filename for each configuration. These configurations can then be loaded later using `from_pretrained` with the appropriate filename. This allows for storing different decoding strategies for the same model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/generation_strategies.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig

>>> tokenizer = AutoTokenizer.from_pretrained("google-t5/t5-small")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("google-t5/t5-small")

>>> translation_generation_config = GenerationConfig(
...     num_beams=4,
...     early_stopping=True,
...     decoder_start_token_id=0,
...     eos_token_id=model.config.eos_token_id,
...     pad_token=model.config.pad_token_id,
... )

>>> # íŒ: Hubì— pushí•˜ë ¤ë©´ `push_to_hub=True`ë¥¼ ì¶”ê°€
>>> translation_generation_config.save_pretrained("/tmp", "translation_generation_config.json")

>>> # ëª…ëª…ëœ ìƒì„± ì„¤ì • íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ì„ ë§¤ê°œë³€ìˆ˜í™”í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
>>> generation_config = GenerationConfig.from_pretrained("/tmp", "translation_generation_config.json")
>>> inputs = tokenizer("translate English to French: Configuration files are easy to use!", return_tensors="pt")
>>> outputs = model.generate(**inputs, generation_config=generation_config)
>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
['Les fichiers de configuration sont faciles Ã  utiliser!']
```

----------------------------------------

TITLE: MBart Translation from Finnish to English in Python
DESCRIPTION: This snippet showcases how to use the MBart model for translation. It loads the tokenizer and model, setting the source language to Finnish ('fi_FI'). The input text is encoded, and the translation is generated by forcing the beginning-of-sequence token ID to English ('en_XX').
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/multilingual.md#_snippet_2

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

>>> en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
>>> fi_text = "Ã„lÃ¤ sekaannu velhojen asioihin, sillÃ¤ ne ovat hienovaraisia ja nopeasti vihaisia."

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt", src_lang="fi_FI")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
```

LANGUAGE: Python
CODE:
```
>>> encoded_en = tokenizer(en_text, return_tensors="pt")
```

LANGUAGE: Python
CODE:
```
>>> generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id["en_XX"])
>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
"Don't interfere with the wizard's affairs, because they are subtle, will soon get angry."
```

----------------------------------------

TITLE: Loading Best Model Checkpoint using DeepSpeed Utils
DESCRIPTION: This code snippet demonstrates how to load the best model checkpoint after training using the 'load_state_dict_from_zero_checkpoint' utility from DeepSpeed. It highlights the importance of reinitializing the DeepSpeed engine after loading the state dict.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/deepspeed.md#2025-04-22_snippet_35

LANGUAGE: Python
CODE:
```
from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint

checkpoint_dir = os.path.join(trainer.args.output_dir, "checkpoint-final")
trainer.deepspeed.save_checkpoint(checkpoint_dir)
fp32_model = load_state_dict_from_zero_checkpoint(trainer.model, checkpoint_dir)
```

----------------------------------------

TITLE: Initializing and Publishing LiLT Model with PyTorch
DESCRIPTION: Code snippet demonstrating how to load a locally customized LiLT model from files and publish it to the Hugging Face Hub. This approach is used after combining a RoBERTa checkpoint with the Layout Transformer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/lilt.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import LiltModel

model = LiltModel.from_pretrained("path_to_your_files")
model.push_to_hub("name_of_repo_on_the_hub")
```

----------------------------------------

TITLE: Contrastive Search Text Generation
DESCRIPTION: Shows how to implement contrastive search decoding for generating non-repetitive and coherent text outputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/generation_strategies.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForCausalLM

checkpoint = "openai-community/gpt2-large"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint)

prompt = "Hugging Face Company is"
inputs = tokenizer(prompt, return_tensors="pt")

outputs = model.generate(**inputs, penalty_alpha=0.6, top_k=4, max_new_tokens=100)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

----------------------------------------

TITLE: Resume Training from Checkpoint (output_dir)
DESCRIPTION: This code snippet demonstrates how to resume training from the last checkpoint stored in the `output_dir`. The `output_dir previous_output_dir` arguments are used to specify the directory containing the checkpoint. The `overwrite_output_dir` argument should be removed in this case.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/run_scripts_fr.md#_snippet_15

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --output_dir previous_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Initializing TrainingArguments for APOLLO Optimizer in PyTorch
DESCRIPTION: This snippet shows how to initialize TrainingArguments for the APOLLO optimizer in PyTorch. It includes parameters for specifying the optimizer and which target layers to train for memory-efficient learning.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/optimizers.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch
from transformers import TrainingArguments

args = TrainingArguments(
    output_dir="./test-apollo",
    max_steps=100,
    per_device_train_batch_size=2,
+   optim="apollo_adamw",
+   optim_target_modules=[r".*.attn.*", r".*.mlp.*"],
    logging_strategy="steps",
    logging_steps=1,
    learning_rate=2e-5,
    save_strategy="no",
    run_name="apollo_adamw",
)
```

----------------------------------------

TITLE: Saving and Loading TensorFlow Model Weights in Python
DESCRIPTION: This code snippet shows how to save TensorFlow model weights as an h5 file and reload them using the from_pretrained method to avoid issues with saving and loading TensorFlow models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/troubleshooting.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import TFPreTrainedModel
>>> from tensorflow import keras

>>> model.save_weights("some_folder/tf_model.h5")
>>> model = TFPreTrainedModel.from_pretrained("some_folder")
```

----------------------------------------

TITLE: Installing Example Requirements
DESCRIPTION: Installs the necessary Python packages for a specific example script, using the `requirements.txt` file in the example's directory. This ensures all dependencies are met for the script to run.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/run_scripts.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
```bash
pip install -r requirements.txt
```
```

----------------------------------------

TITLE: Saving a TensorFlow Model
DESCRIPTION: Demonstrates how to save a TensorFlow model checkpoint after conversion from PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/model_sharing.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> tf_model.save_pretrained("path/verso/il-nome-magnifico-che-hai-scelto")
```

----------------------------------------

TITLE: Defining a Custom Callback in PyTorch Trainer
DESCRIPTION: This snippet shows how to create a custom callback class by subclassing TrainerCallback and defining specific behavior for the on_train_begin event, which triggers at the start of training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/main_classes/callback.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
class MyCallback(TrainerCallback):
    "A callback that prints a message at the beginning of training"

    def on_train_begin(self, args, state, control, **kwargs):
        print("Starting training")


trainer = Trainer(
    model,
    args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    callbacks=[MyCallback],  # ìš°ë¦¬ëŠ” ì½œë°± í´ëž˜ìŠ¤ë¥¼ ì´ ë°©ì‹ìœ¼ë¡œ ì „ë‹¬í•˜ê±°ë‚˜ ê·¸ê²ƒì˜ ì¸ìŠ¤í„´ìŠ¤(MyCallback())ë¥¼ ì „ë‹¬í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤
)
```

----------------------------------------

TITLE: Tokenizing Input for Text Generation in TensorFlow
DESCRIPTION: This code demonstrates how to tokenize input text and prepare it for text generation in TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md#2025-04-22_snippet_24

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("username/my_awesome_eli5_clm-model")
inputs = tokenizer(prompt, return_tensors="tf").input_ids
```

----------------------------------------

TITLE: Saving a Quantized Model Locally (Non-Safetensors)
DESCRIPTION: This snippet shows how to save a quantized model locally using the `save_pretrained` method. It is crucial to set `safe_serialization=False` because torchao utilizes torch.Tensor subclasses which are not compatible with Safetensors serialization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/torchao.md#_snippet_11

LANGUAGE: Python
CODE:
```
# don't serialize model with Safetensors
output_dir = "llama3-8b-int4wo-128"
quantized_model.save_pretrained("llama3-8b-int4wo-128", safe_serialization=False)
```

----------------------------------------

TITLE: Multilingual Speech Translation with Speech2Text
DESCRIPTION: Example showing multilingual speech translation using Speech2Text, specifically translating English speech to French text. Uses the MUST-C multilingual model and demonstrates forced language token generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/speech_to_text.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration
from datasets import load_dataset

model = Speech2TextForConditionalGeneration.from_pretrained("facebook/s2t-medium-mustc-multilingual-st")
processor = Speech2TextProcessor.from_pretrained("facebook/s2t-medium-mustc-multilingual-st")

ds = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")

inputs = processor(ds[0]["audio"]["array"], sampling_rate=ds[0]["audio"]["sampling_rate"], return_tensors="pt")
generated_ids = model.generate(
    inputs["input_features"],
    attention_mask=inputs["attention_mask"],
    forced_bos_token_id=processor.tokenizer.lang_code_to_id["fr"],
)

translation = processor.batch_decode(generated_ids, skip_special_tokens=True)
```

----------------------------------------

TITLE: Training TensorFlow Model
DESCRIPTION: This code trains the TensorFlow model using `model.fit`. It specifies the training and validation datasets, the number of epochs, and the callbacks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_25

LANGUAGE: python
CODE:
```
>>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)
```

----------------------------------------

TITLE: Importing and Using Blip2Processor in Python
DESCRIPTION: Example of how to use Blip2Processor to prepare images for the model and decode predicted token IDs back to text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/blip-2.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import Blip2Processor

# Use Blip2Processor to prepare images for the model
processor = Blip2Processor.from_pretrained("model_name")

# Prepare image
processed_image = processor(images=image, return_tensors="pt").pixel_values

# Decode predicted token IDs back to text
generated_text = processor.decode(output_ids, skip_special_tokens=True)
```

----------------------------------------

TITLE: MBart Translation Configuration and Usage
DESCRIPTION: Demonstrates MBart model setup for Finnish to English translation, including source language configuration and forced token generation for target language.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/multilingual.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
fi_text = "Ã„lÃ¤ sekaannu velhojen asioihin, sillÃ¤ ne ovat hienovaraisia ja nopeasti vihaisia."

tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt", src_lang="fi_FI")
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")

encoded_en = tokenizer(en_text, return_tensors="pt")
generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id("en_XX"))
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
```

----------------------------------------

TITLE: Cargando un dataset de audio en espaÃ±ol
DESCRIPTION: Carga un dataset de audio en espaÃ±ol desde el hub de Hugging Face para procesamiento.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/quicktour.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from datasets import load_dataset, Audio

dataset = load_dataset("PolyAI/minds14", name="es-ES", split="train")  # doctest: +IGNORE_RESULT
```

----------------------------------------

TITLE: Processing Multiple Images with SuperPoint in Python
DESCRIPTION: This example extends the usage of SuperPoint to handle multiple images, demonstrating how to detect keypoints for each image. It processes the images, retrieves keypoints, scores, and descriptors, and then prints these results. The process relies on the usage of the mask attribute to handle dynamic numbers of keypoints. Required libraries include `transformers`, `torch`, `PIL`, and `requests`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/superpoint.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
from transformers import AutoImageProcessor, SuperPointForKeypointDetection
import torch
from PIL import Image
import requests

url_image_1 = "http://images.cocodataset.org/val2017/000000039769.jpg"
image_1 = Image.open(requests.get(url_image_1, stream=True).raw)
url_image_2 = "http://images.cocodataset.org/test-stuff2017/000000000568.jpg"
image_2 = Image.open(requests.get(url_image_2, stream=True).raw)

images = [image_1, image_2]

processor = AutoImageProcessor.from_pretrained("magic-leap-community/superpoint")
model = SuperPointForKeypointDetection.from_pretrained("magic-leap-community/superpoint")

inputs = processor(images, return_tensors="pt")
outputs = model(**inputs)
image_sizes = [(image.height, image.width) for image in images]
outputs = processor.post_process_keypoint_detection(outputs, image_sizes)

for output in outputs:
    for keypoints, scores, descriptors in zip(output["keypoints"], output["scores"], output["descriptors"]):
        print(f"Keypoints: {keypoints}")
        print(f"Scores: {scores}")
        print(f"Descriptors: {descriptors}")
```

----------------------------------------

TITLE: TAPEX Batched Table Question Answering in Python
DESCRIPTION: This code snippet demonstrates batched inference with TAPEX, where multiple questions are asked based on a single table. It prepares a Pandas DataFrame and a list of questions, encodes them using the tokenizer with padding enabled, generates answers using the model, and decodes the generated outputs.  The tokenizer handles the batching and padding automatically.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/tapex.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> # prepare table + question
>>> data = {"Actors": ["Brad Pitt", "Leonardo Di Caprio", "George Clooney"], "Number of movies": ["87", "53", "69"]}
>>> table = pd.DataFrame.from_dict(data)
>>> questions = [
...     "how many movies does Leonardo Di Caprio have?",
...     "which actor has 69 movies?",
...     "what's the first name of the actor who has 87 movies?",
... ]
>>> encoding = tokenizer(table, questions, padding=True, return_tensors="pt")

>>> # let the model generate an answer autoregressively
>>> outputs = model.generate(**encoding)

>>> # decode back to text
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
[' 53', ' george clooney', ' brad pitt']
```

----------------------------------------

TITLE: Image-Text Similarity Scoring with ALIGN Model
DESCRIPTION: Example demonstrating how to use the AlignProcessor and AlignModel to compute similarity scores between an image and candidate text labels. The code loads a pre-trained model, processes an image and text inputs, and calculates probability scores for text-image matches.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/align.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import requests
import torch
from PIL import Image
from transformers import AlignProcessor, AlignModel

processor = AlignProcessor.from_pretrained("kakaobrain/align-base")
model = AlignModel.from_pretrained("kakaobrain/align-base")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
candidate_labels = ["an image of a cat", "an image of a dog"]

inputs = processor(text=candidate_labels, images=image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

# this is the image-text similarity score
logits_per_image = outputs.logits_per_image

# we can take the softmax to get the label probabilities
probs = logits_per_image.softmax(dim=1)
print(probs)
```

----------------------------------------

TITLE: Loading Evaluation Metrics for Token Classification
DESCRIPTION: Sets up the seqeval evaluation metric from the Evaluate library to measure the model's performance on the NER task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/token_classification.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> import evaluate

>>> seqeval = evaluate.load("seqeval")
```

----------------------------------------

TITLE: Text Generation with `return_dict_in_generate` in Transformers
DESCRIPTION: This code snippet showcases how to generate text using the `generate` method in Transformers and retrieve the `past_key_values` along with the generated sequences. This is achieved by setting `return_dict_in_generate=True` and `use_cache=True`. The generated output is then decoded into human-readable text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_28

LANGUAGE: python
CODE:
```
# Generation as usual
prompt = system_prompt + "Question: Please write a function in Python that transforms bytes to Giga bytes.\n\nAnswer: Here"
model_inputs = tokenizer(prompt, return_tensors='pt')
generation_output = model.generate(**model_inputs, max_new_tokens=60, return_dict_in_generate=True)
decoded_output = tokenizer.batch_decode(generation_output.sequences)[0]
```

----------------------------------------

TITLE: Data Preprocessing Pipeline
DESCRIPTION: Setting up image and text preprocessing using the model's processor for training data preparation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/image_captioning.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
def transforms(example_batch):
    images = [x for x in example_batch["image"]]
    captions = [x for x in example_batch["text"]]
    inputs = processor(images=images, text=captions, padding="max_length")
    inputs.update({"labels": inputs["input_ids"]})
    return inputs

train_ds.set_transform(transforms)
test_ds.set_transform(transforms)
```

----------------------------------------

TITLE: Loading Quantized IDEFICS Model
DESCRIPTION: Python code to load a 4-bit quantized version of the IDEFICS model using BitsAndBytesConfig. This approach enables using the model with lower memory requirements.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/idefics.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> import torch
>>> from transformers import IdeficsForVisionText2Text, AutoProcessor, BitsAndBytesConfig

>>> quantization_config = BitsAndBytesConfig(
...     load_in_4bit=True,
...     bnb_4bit_compute_dtype=torch.float16,
... )

>>> processor = AutoProcessor.from_pretrained(checkpoint)

>>> model = IdeficsForVisionText2Text.from_pretrained(
...     checkpoint,
...     quantization_config=quantization_config,
...     device_map="auto"
... )
```

----------------------------------------

TITLE: Preprocessing Test Image for Model Input
DESCRIPTION: Preprocesses the test image using the model processor to prepare it for captioning prediction.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_captioning.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
device = "cuda" if torch.cuda.is_available() else "cpu"

inputs = processor(images=image, return_tensors="pt").to(device)
pixel_values = inputs.pixel_values
```

----------------------------------------

TITLE: Examining Dataset Example for Summarization
DESCRIPTION: Code to inspect a sample from the BillSum dataset, showing the structure with 'summary', 'text', and 'title' fields. This demonstrates the input format for training a summarization model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/summarization.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> billsum["train"][0]
```

----------------------------------------

TITLE: Loading Audio Feature Extractor
DESCRIPTION: This code loads a pre-trained feature extractor for audio data, specifically the 'facebook/wav2vec2-base' model. The `AutoFeatureExtractor.from_pretrained` method is used to load the feature extractor, which is responsible for normalizing and padding the audio input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/preprocessing.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
```

----------------------------------------

TITLE: Defining TFRagTokenForGeneration in TensorFlow
DESCRIPTION: This snippet demonstrates the TFRagTokenForGeneration model in TensorFlow, detailing the call and generate methods used for token-based generation and retrieval, crucial for efficient text generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/rag.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
## TFRagTokenForGeneration
[[autodoc]] TFRagTokenForGeneration
    - call
    - generate
```

----------------------------------------

TITLE: Enabling Flash Attention with BetterTransformer
DESCRIPTION: This snippet converts the model to BetterTransformer, enabling PyTorch's SDPA self-attention which can use Flash Attention.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_21

LANGUAGE: python
CODE:
```
model.to_bettertransformer()
```

----------------------------------------

TITLE: Checking GPU Topology with nvidia-smi
DESCRIPTION: Command to check the interconnection topology between GPUs on a system, showing how GPUs are connected to each other and the system.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_hardware.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
nvidia-smi topo -m
```

----------------------------------------

TITLE: Loading Mean IoU Metric with Evaluate in Py
DESCRIPTION: This snippet demonstrates how to load the mean Intersection over Union (IoU) metric using the Evaluate library. This is necessary for evaluating a model's performance during training. Ensure the library is installed and properly configured.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/semantic_segmentation.md#2025-04-22_snippet_15

LANGUAGE: py
CODE:
```
>>> import evaluate

>>> metric = evaluate.load("mean_iou")
```

----------------------------------------

TITLE: Template File Management
DESCRIPTION: Python code for writing and loading chat templates from external files
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating_writing.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
open("template.jinja", "w").write(tokenizer.chat_template)

tokenizer.chat_template = open("template.jinja").read()
```

----------------------------------------

TITLE: Creating TensorFlow Evaluation Metrics Computation Function
DESCRIPTION: This function computes evaluation metrics for semantic segmentation using TensorFlow, including resizing predictions and calculating detailed per-category accuracy and IoU metrics.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/sequence_classification.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
>>> def compute_metrics(eval_pred):
...     logits, labels = eval_pred
...     logits = tf.transpose(logits, perm=[0, 2, 3, 1])
...     logits_resized = tf.image.resize(
...         logits,
...         size=tf.shape(labels)[1:],
...         method="bilinear",
...     )

...     pred_labels = tf.argmax(logits_resized, axis=-1)
...     metrics = metric.compute(
...         predictions=pred_labels,
...         references=labels,
...         num_labels=num_labels,
...         ignore_index=-1,
...         reduce_labels=image_processor.do_reduce_labels,
...     )

...     per_category_accuracy = metrics.pop("per_category_accuracy").tolist()
...     per_category_iou = metrics.pop("per_category_iou").tolist()

...     metrics.update({f"accuracy_{id2label[i]}": v for i, v in enumerate(per_category_accuracy)})
...     metrics.update({f"iou_{id2label[i]}": v for i, v in enumerate(per_category_iou)})
...     return {"val_" + k: v for k, v in metrics.items()}
```

----------------------------------------

TITLE: Assigning Device Map Manually - Transformers - Python
DESCRIPTION: This code snippet illustrates manually setting a `device_map` to allocate specific model layers across different computing devices using the `transformers` library, which allows for custom memory distribution of large models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/models.md#2025-04-22_snippet_14

LANGUAGE: Python
CODE:
```
device_map = {"model.layers.1": 0, "model.layers.14": 1, "model.layers.31": "cpu", "lm_head": "disk"}
model.hf_device_map
```

----------------------------------------

TITLE: Loading an Accuracy Metric for Model Evaluation
DESCRIPTION: Code to load the accuracy metric from the Evaluate library, which will be used to measure the model's performance during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/sequence_classification.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> import evaluate

>>> accuracy = evaluate.load("accuracy")
```

----------------------------------------

TITLE: Setting Up TensorFlow Training for Segmentation Model
DESCRIPTION: This snippet describes initializing optimizer and learning rate schedule parameters necessary for training a segmentation model using TensorFlow and the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
>>> from transformers import create_optimizer

>>> batch_size = 2
>>> num_epochs = 50
>>> num_train_steps = len(train_ds) * num_epochs
>>> learning_rate = 6e-5
>>> weight_decay_rate = 0.01

>>> optimizer, lr_schedule = create_optimizer(
...     init_lr=learning_rate,
...     num_train_steps=num_train_steps,
...     weight_decay_rate=weight_decay_rate,
...     num_warmup_steps=0,
... )
```

----------------------------------------

TITLE: Applying Custom Inference Function to Images
DESCRIPTION: Code to extract embeddings from both real and generated images using the custom inference function.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_feature_extraction.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
embed_real = infer(image_real)
embed_gen = infer(image_gen)
```

----------------------------------------

TITLE: Enabling BetterTransformer on PyTorch Model
DESCRIPTION: Shows how to convert a pre-trained PyTorch model to BetterTransformer for faster inference on CPUs
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_cpu.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("bigscience/bloom")
model = model.to_bettertransformer()
```

----------------------------------------

TITLE: Classifying Image with Zero-shot Pipeline
DESCRIPTION: This snippet runs the image through the zero-shot classification pipeline. It passes the image and a list of candidate labels to the detector and retrieves the classification results.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_image_classification.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> predictions = detector(image, candidate_labels=["fox", "bear", "seagull", "owl"])
>>> predictions
[{'score': 0.9996670484542847, 'label': 'owl'},
 {'score': 0.000199399160919711, 'label': 'seagull'},
 {'score': 7.392891711788252e-05, 'label': 'fox'},
 {'score': 5.96074532950297e-05, 'label': 'bear'}]
```

----------------------------------------

TITLE: Adding a Callback to an Existing Trainer
DESCRIPTION: Example showing how to add a callback to an already instantiated Trainer using the add_callback() method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/callback.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
trainer = Trainer(...)
trainer.add_callback(MyCallback)
# Alternatively, we can pass an instance of the callback class
trainer.add_callback(MyCallback())
```

----------------------------------------

TITLE: Basic Chat Template in Jinja
DESCRIPTION: A simple Jinja template for handling chat messages with role printing, message content, and end-of-sequence tokens
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating_writing.md#2025-04-22_snippet_0

LANGUAGE: jinja
CODE:
```
{%- for message in messages %}
    {{- '<|' + message['role'] + '|>\n' }}
    {{- message['content'] + eos_token }}
{%- endfor %}
{%- if add_generation_prompt %}
    {{- '<|assistant|>\n' }}
{%- endif %}
```

----------------------------------------

TITLE: Install requirements for an example
DESCRIPTION: This snippet installs the required packages from the requirements.txt file in a specific example directory. This ensures that all dependencies for the example are met before running the script. It relies on the `pip` package manager and a correctly configured `requirements.txt` file.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/run_scripts.md#_snippet_2

LANGUAGE: bash
CODE:
```
pip install -r requirements.txt
```

----------------------------------------

TITLE: Saving TensorFlow Model Locally for ONNX Export
DESCRIPTION: Python code to load a TensorFlow model and tokenizer from the Hub and save them locally for ONNX export.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/serialization.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

>>> # Load tokenizer and TensorFlow weights from the Hub
>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
>>> # Save to disk
>>> tokenizer.save_pretrained("local-tf-checkpoint")
>>> tf_model.save_pretrained("local-tf-checkpoint")
```

----------------------------------------

TITLE: Launch Summarization Script with Accelerate
DESCRIPTION: Launches the `run_summarization_no_trainer.py` script using Accelerate.  This command executes the summarization training process with the specified model, dataset, and output directory.  It uses the google-t5/t5-small model on the cnn_dailymail dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/run_scripts.md#_snippet_11

LANGUAGE: bash
CODE:
```
accelerate launch run_summarization_no_trainer.py \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir ~/tmp/tst-summarization
```

----------------------------------------

TITLE: ONNX Configuration with Past States
DESCRIPTION: Configuration class for decoder-based models that require handling of past states during ONNX export.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/onnx.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
[[autodoc]] onnx.config.OnnxConfigWithPast
```

----------------------------------------

TITLE: Using Fast Implementation of ViTImageProcessor - Python
DESCRIPTION: This snippet demonstrates how to load and use the fast implementation class 'ViTImageProcessorFast' for the 'google/vit-base-patch16-224' model, providing faster image processing capabilities, especially beneficial when batching images.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/image_processors.md#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
from transformers import ViTImageProcessorFast

image_processor = ViTImageProcessorFast.from_pretrained("google/vit-base-patch16-224")
```

----------------------------------------

TITLE: Setting Up PyTorch T5 Model for Sequence-to-Sequence Translation
DESCRIPTION: This code loads a pre-trained T5 model for sequence-to-sequence learning that will be fine-tuned for translation tasks using PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/translation.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Loading Custom Image Datasets using ImageFolder
DESCRIPTION: Python code examples showing different ways to load image datasets using the ImageFolder feature from Hugging Face datasets, including local folders, compressed files, and remote sources.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/tensorflow/image-classification/README.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from datasets import load_dataset

# example 1: local folder
dataset = load_dataset("imagefolder", data_dir="path_to_your_folder")

# example 2: local files (supported formats are tar, gzip, zip, xz, rar, zstd)
dataset = load_dataset("imagefolder", data_files="path_to_zip_file")

# example 3: remote files (supported formats are tar, gzip, zip, xz, rar, zstd)
dataset = load_dataset("imagefolder", data_files="https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip")

# example 4: providing several splits
dataset = load_dataset("imagefolder", data_files={"train": ["path/to/file1", "path/to/file2"], "test": ["path/to/file3", "path/to/file4"]})
```

----------------------------------------

TITLE: Training with Additional Adapter in Python
DESCRIPTION: This code demonstrates adding another trainable module 'lm_head' to a model already having a LoRA adapter, then proceeding with training using the PEFT library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/peft.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM
from peft import LoraConfig

model = AutoModelForCausalLM.from_pretrained("google/gemma-2-2b")

lora_config = LoraConfig(
    target_modules=["q_proj", "k_proj"],
    modules_to_save=["lm_head"],
)

model.add_adapter(lora_config)
trainer = Trainer(model=model, ...)
trainer.train()

```

----------------------------------------

TITLE: Implementing a Custom Callback in PyTorch Trainer
DESCRIPTION: Example of how to create and register a custom callback with the PyTorch Trainer that prints a message at the beginning of training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/callback.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
class MyCallback(TrainerCallback):
    "A callback that prints a message at the beginning of training"

    def on_train_begin(self, args, state, control, **kwargs):
        print("Starting training")


trainer = Trainer(
    model,
    args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    callbacks=[MyCallback],  # We can either pass the callback class this way or an instance of it (MyCallback())
)
```

----------------------------------------

TITLE: Defining default training arguments
DESCRIPTION: Defines a dictionary `default_args` containing common training arguments such as output directory, evaluation strategy, number of training epochs, logging level, and reporting destination. These arguments are used to configure the training process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/model_memory_anatomy.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
default_args = {
    "output_dir": "tmp",
    "eval_strategy": "steps",
    "num_train_epochs": 1,
    "log_level": "error",
    "report_to": "none",
}
```

----------------------------------------

TITLE: Continuing Final Message with Chat Template
DESCRIPTION: Demonstrates how to use continue_final_message parameter to extend the last message in a conversation instead of starting a new one
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/chat_templating.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
chat = [
    {"role": "user", "content": "Can you format the answer in JSON?"},
    {"role": "assistant", "content": '{"name": "'}
]

formatted_chat = tokenizer.apply_chat_template(chat, tokenize=True, return_dict=True, continue_final_message=True)
model.generate(**formatted_chat)
```

----------------------------------------

TITLE: TensorFlow TAPAS Configuration and Training
DESCRIPTION: Shows how to configure and train a TAPAS model in TensorFlow, including model initialization, optimizer setup, and training loop implementation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/tapas.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
import tensorflow as tf
from transformers import TapasConfig, TFTapasForQuestionAnswering

# this is the default WTQ configuration
config = TapasConfig(
    num_aggregation_labels=4,
    use_answer_as_supervision=True,
    answer_loss_cutoff=0.664694,
    cell_selection_preference=0.207951,
    huber_loss_delta=0.121194,
    init_cell_selection_weights_to_zero=True,
    select_one_column=True,
    allow_empty_column_selection=False,
    temperature=0.0352513,
)
model = TFTapasForQuestionAnswering.from_pretrained("google/tapas-base", config=config)

optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)

for epoch in range(2):  # loop over the dataset multiple times
    for batch in train_dataloader:
        # get the inputs;
        input_ids = batch[0]
        attention_mask = batch[1]
        token_type_ids = batch[4]
        labels = batch[-1]
        numeric_values = batch[2]
        numeric_values_scale = batch[3]
        float_answer = batch[6]

        # forward + backward + optimize
        with tf.GradientTape() as tape:
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                token_type_ids=token_type_ids,
                labels=labels,
                numeric_values=numeric_values,
                numeric_values_scale=numeric_values_scale,
                float_answer=float_answer,
            )
        grads = tape.gradient(outputs.loss, model.trainable_weights)
        optimizer.apply_gradients(zip(grads, model.trainable_weights))
```

----------------------------------------

TITLE: Initializing Models with ZeRO-3 using Python
DESCRIPTION: This Python code snippet demonstrates how to initialize a model using the ZeRO-3 stage in DeepSpeed. The model configuration is loaded, and the model is instantiated within the `deepspeed.zero.Init` context. Requires DeepSpeed and Transformers libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/deepspeed.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from transformers import T5ForConditionalGeneration, T5Config
import deepspeed

with deepspeed.zero.Init():
    config = T5Config.from_pretrained("google-t5/t5-small")
    model = T5ForConditionalGeneration(config)
```

----------------------------------------

TITLE: Enabling CUDA Launch Blocking
DESCRIPTION: This snippet demonstrates how to enable CUDA launch blocking by setting the `CUDA_LAUNCH_BLOCKING` environment variable to "1". This allows for better tracebacks and helps pinpoint the source of CUDA errors on the GPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/troubleshooting.md#_snippet_4

LANGUAGE: Python
CODE:
```
>>> import os

>>> os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
```

----------------------------------------

TITLE: Loading Model and Processor for Manual Detection
DESCRIPTION: This snippet demonstrates how to manually load the OWL-ViT model and processor from the Hugging Face model hub, preparing them for zero-shot object detection.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection

>>> model = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint)
>>> processor = AutoProcessor.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Creating a Custom DistilBert Tokenizer from Vocabulary File in Python
DESCRIPTION: Demonstrates how to create a custom tokenizer for DistilBert using a vocabulary file and specific tokenization settings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/create_a_model.md#2025-04-23_snippet_15

LANGUAGE: python
CODE:
```
>>> from transformers import DistilBertTokenizer

>>> my_tokenizer = DistilBertTokenizer(vocab_file="my_vocab_file.txt", do_lower_case=False, padding_side="left")
```

----------------------------------------

TITLE: Initializing DataCollator for Multiple Choice in Python
DESCRIPTION: Creates a DataCollatorForMultipleChoice instance for dynamic padding and batch preparation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/multiple_choice.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorForMultipleChoice
>>> collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)
```

----------------------------------------

TITLE: Mask Filling with BART in Python
DESCRIPTION: Demonstrates how to use the BART model for filling multi-token masks in text sequences. This example loads a pre-trained BART model, tokenizes an input phrase with a mask, generates text to fill the mask, and decodes the result.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/bart.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import BartForConditionalGeneration, BartTokenizer

model = BartForConditionalGeneration.from_pretrained("facebook/bart-large", forced_bos_token_id=0)
tok = BartTokenizer.from_pretrained("facebook/bart-large")
example_english_phrase = "UN Chief Says There Is No <mask> in Syria"
batch = tok(example_english_phrase, return_tensors="pt")
generated_ids = model.generate(batch["input_ids"])
assert tok.batch_decode(generated_ids, skip_special_tokens=True) == [
    "UN Chief Says There Is No Plan to Stop Chemical Weapons in Syria"
]
```

----------------------------------------

TITLE: Classification against Custom Policies using ShieldGemma 2 in Python
DESCRIPTION: This snippet shows how to use ShieldGemma 2 for image classification with custom safety policies. It loads the model and processor, defines custom policies, processes an input image with both built-in and custom policies, and generates classification probabilities.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/shieldgemma2.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from PIL import Image
import requests
from transformers import AutoProcessor, ShieldGemma2ForImageClassification

model_id = "google/shieldgemma-2-4b-it"
model = ShieldGemma2ForImageClassification.from_pretrained(model_id, device_map="auto")
processor = AutoProcessor.from_pretrained(model_id)

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg"
image = Image.open(requests.get(url, stream=True).raw)

custom_policies = {
    "key_a": "descrition_a",
    "key_b": "descrition_b",
}

inputs = processor(
    images=[image],
    custom_policies=custom_policies,
    policies=["dangerous", "key_a", "key_b"],
    return_tensors="pt",
).to(model.device)

output = model(**inputs)
print(output.probabilities)
```

----------------------------------------

TITLE: Evaluating Image Captioning Model Using WER Metric in Python
DESCRIPTION: Implement a function to compute the Word Error Rate (WER) using the evaluate library for model performance assessment during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_captioning.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from evaluate import load
import torch

wer = load("wer")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predicted = logits.argmax(-1)
    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)
    decoded_predictions = processor.batch_decode(predicted, skip_special_tokens=True)
    wer_score = wer.compute(predictions=decoded_predictions, references=decoded_labels)
    return {"wer_score": wer_score}
```

----------------------------------------

TITLE: Using Speech Recognition Pipeline for Long Audio Files in Python
DESCRIPTION: This example shows how to use the 'chunk_length_s' parameter in the Automatic Speech Recognition pipeline to handle long audio files by processing them in chunks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/pipeline_tutorial.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> transcriber = pipeline(model="openai/whisper-large-v2", chunk_length_s=30)
>>> transcriber("https://huggingface.co/datasets/reach-vb/random-audios/resolve/main/ted_60.wav")
{'text': " So in college, I was a government major, which means I had to write a lot of papers. Now, when a normal student writes a paper, they might spread the work out a little like this. So, you know. You get started maybe a little slowly, but you get enough done in the first week that with some heavier days later on, everything gets done and things stay civil. And I would want to do that like that. That would be the plan. I would have it all ready to go, but then actually the paper would come along, and then I would kind of do this. And that would happen every single paper. But then came my 90-page senior thesis, a paper you're supposed to spend a year on. I knew for a paper like that, my normal workflow was not an option, it was way too big a project. So I planned things out and I decided I kind of had to go something like this. This is how the year would go. So I'd start off light and I'd bump it up"}
```

----------------------------------------

TITLE: Initializing XLM with Language Embeddings in Python
DESCRIPTION: This code snippet demonstrates how to initialize an XLM model that uses language embeddings for inference. It loads the tokenizer and model, sets the language ID, and passes both input IDs and language embeddings to the model for processing. The language embedding is a tensor of the same shape as the input IDs, filled with the language ID.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/multilingual.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> import torch
>>> from transformers import XLMTokenizer, XLMWithLMHeadModel

>>> tokenizer = XLMTokenizer.from_pretrained("FacebookAI/xlm-clm-enfr-1024")
>>> model = XLMWithLMHeadModel.from_pretrained("FacebookAI/xlm-clm-enfr-1024")
```

LANGUAGE: Python
CODE:
```
>>> print(tokenizer.lang2id)
{'en': 0, 'fr': 1}
```

LANGUAGE: Python
CODE:
```
>>> input_ids = torch.tensor([tokenizer.encode("Wikipedia was used to")])  # batch size of 1
```

LANGUAGE: Python
CODE:
```
>>> language_id = tokenizer.lang2id["en"]  # 0
>>> langs = torch.tensor([language_id] * input_ids.shape[1])  # torch.tensor([0, 0, 0, ..., 0])

>>> # Ù†Ù‚ÙˆÙ… Ø¨Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„Ù‡Ø§ Ù„ØªÙƒÙˆÙ† Ø¨Ø§Ù„Ø­Ø¬Ù… (batch_sizeØŒ sequence_length)
>>> langs = langs.view(1, -1)  # Ø§Ù„Ø¢Ù† Ø¨Ø§Ù„Ø­Ø¬Ù… [1ØŒ sequence_length] (Ù„Ø¯ÙŠÙ†Ø§ batch size ØªØ³Ø§ÙˆÙŠ 1)
```

LANGUAGE: Python
CODE:
```
>>> outputs = model(input_ids, langs=langs)
```

----------------------------------------

TITLE: Verifying Transformers Installation
DESCRIPTION: Python command to verify the installation of Transformers by running a sentiment analysis pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/installation.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
```

----------------------------------------

TITLE: Using Trained T5 Model for Translation Inference with Pipeline
DESCRIPTION: Demonstrates how to use the trained T5 model for translation inference using the Hugging Face pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/translation.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
from transformers import pipeline

translator = pipeline("translation_xx_to_yy", model="username/my_awesome_opus_books_model")
translator(text)
```

----------------------------------------

TITLE: Fine-tuning with Schedule Free Optimizer (SFO) in Python
DESCRIPTION: This code snippet shows how to fine-tune a model using the Schedule Free Optimizer (SFO). It loads the IMDB dataset, sets training arguments with `optim="schedule_free_adamw"`, initializes a `SFTTrainer`, and starts training. Requires the `schedulefree` package to be installed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/trainer.md#_snippet_21

LANGUAGE: python
CODE:
```
import torch
import datasets
from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM
import trl

train_dataset = datasets.load_dataset('imdb', split='train')

args = TrainingArguments(
    output_dir="./test-schedulefree",
    max_steps=1000,
    per_device_train_batch_size=4,
    optim="schedule_free_adamw",
    gradient_checkpointing=True,
    logging_strategy="steps",
    logging_steps=1,
    learning_rate=2e-6,
    save_strategy="no",
    run_name="sfo-imdb",
)

model_id = "google/gemma-2b"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True).to(0)

trainer = trl.SFTTrainer(
    model=model, 
    args=args,
    train_dataset=train_dataset,
    dataset_text_field='text',
    max_seq_length=1024,
)

trainer.train()
```

----------------------------------------

TITLE: Loading Int8 Quantized Model for Multi-GPU Inference with Memory Mapping
DESCRIPTION: Load a Transformers model using int8 mixed-precision matrix decomposition for multi-GPU inference, specifying memory allocation per GPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/perf_infer_gpu_one.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
max_memory_mapping = {0: "1GB", 1: "2GB"}
model_name = "bigscience/bloom-3b"
model_8bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map="auto", load_in_8bit=True, max_memory=max_memory_mapping
)
```

----------------------------------------

TITLE: Setting Up Trainer with DeepSpeed
DESCRIPTION: Example of configuring Trainer with DeepSpeed arguments
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/deepspeed.md#2025-04-22_snippet_37

LANGUAGE: python
CODE:
```
from transformers import AutoModel, Trainer, TrainingArguments

training_args = TrainingArguments(..., deepspeed=ds_config)
model = AutoModel.from_pretrained("google-t5/t5-small")
trainer = Trainer(model=model, args=training_args, ...)
```

----------------------------------------

TITLE: Configuring DistributedDataParallel Training with YAML
DESCRIPTION: This YAML configuration sets up a distributed training environment using DistributedDataParallel (DDP) across multiple GPUs and machines. It specifies parameters such as the number of processes, machines, and mixed precision settings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/trainer.md#2025-04-22_snippet_10

LANGUAGE: yaml
CODE:
```
compute_environment: LOCAL_MACHINE
distributed_type: MULTI_GPU
downcast_bf16: 'no'
gpu_ids: all
machine_rank: 0 #change rank as per the node
main_process_ip: 192.168.20.1
main_process_port: 9898
main_training_function: main
mixed_precision: fp16
num_machines: 2
num_processes: 8
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

----------------------------------------

TITLE: Running Multi-Objective Hyperparameter Optimization with Optuna in Python
DESCRIPTION: Example of running a multi-objective hyperparameter optimization using Optuna backend with the Trainer API, specifying direction, number of trials, and custom compute objective.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/hpo_train.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
best_trials = trainer.hyperparameter_search(
    direction=["minimize", "maximize"],
    backend="optuna",
    hp_space=optuna_hp_space,
    n_trials=20,
    compute_objective=compute_objective,
)
```

----------------------------------------

TITLE: Launch Summarization with Accelerate
DESCRIPTION: This command launches the summarization training script using Hugging Face Accelerate.  It trains the model on the specified dataset using the configured Accelerate settings. The script `run_summarization_no_trainer.py` is used, which is compatible with Accelerate.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/run_scripts_fr.md#_snippet_11

LANGUAGE: bash
CODE:
```
accelerate launch run_summarization_no_trainer.py \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir ~/tmp/tst-summarization
```

----------------------------------------

TITLE: Creating Labelled Inputs for PaliGemma Model Fine-Tuning in Python
DESCRIPTION: This snippet demonstrates how to prepare images and text with labels for PaliGemma models using a processor. It requires the PaliGemma processor and tensors library, potentially supporting multiple images as input. The inputs are images, a text prompt, and an answer which collectively generate the corresponding processed tensor inputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/paligemma.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
prompt = "What is in this image?"
answer = "a pallas cat"
inputs = processor(images=image, text=prompt, suffix=answer, return_tensors="pt")
```

----------------------------------------

TITLE: Loading Custom Model from Hugging Face Hub in Python
DESCRIPTION: Demonstrates how to load a custom model with custom code from the Hugging Face Hub using the AutoModelForImageClassification class.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/custom_models.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoModelForImageClassification

model = AutoModelForImageClassification.from_pretrained("sgugger/custom-resnet50d", trust_remote_code=True)

# Using a specific commit hash for security
commit_hash = "ed94a7c6247d8aedce4647f00f20de6875b5b292"
model = AutoModelForImageClassification.from_pretrained(
    "sgugger/custom-resnet50d", trust_remote_code=True, revision=commit_hash
)
```

----------------------------------------

TITLE: Contrastive Search with Transformers
DESCRIPTION: This snippet shows how to use the contrastive search decoding strategy. It initializes the tokenizer and model, encodes the input text, and generates text using the `model.generate` function with the `penalty_alpha` and `top_k` parameters enabled for contrastive search.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/generation_strategies.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
inputs = tokenizer("Hugging Face is an open-source company", return_tensors="pt").to("cuda")

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", torch_dtype=torch.float16).to("cuda")
# explicitly set to 100 because Llama2 generation length is 4096
outputs = model.generate(**inputs, max_new_tokens=100, penalty_alpha=0.6, top_k=4)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
'Hugging Face is an open-source company that provides a platform for building and deploying AI models.\nHugging Face is an open-source company that provides a platform for building and deploying AI models. The platform allows developers to build and deploy AI models, as well as collaborate with other developers.\nHugging Face was founded in 2019 by Thibault Wittemberg and ClÃ©ment Delangue. The company is based in Paris, France.\nHugging Face has'
```

----------------------------------------

TITLE: Performing Inference with XLM Model in Python
DESCRIPTION: This snippet demonstrates how to perform inference using an XLM model by passing the input IDs and language embeddings to the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/multilingual.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
outputs = model(input_ids, langs=langs)
```

----------------------------------------

TITLE: Loading and Displaying Image for Super-Resolution
DESCRIPTION: Loads an image from a URL using the PIL library and displays its size.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/image_to_image.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from PIL import Image
import requests

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/cat.jpg"
image = Image.open(requests.get(url, stream=True).raw)

print(image.size)
```

----------------------------------------

TITLE: Loading Image Dataset
DESCRIPTION: This code loads an image dataset (beans) using the `datasets` library and retrieves the file paths and labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
"from datasets import load_dataset\n\nimage_dataset = load_dataset(\"beans\", split=\"train\")\nfilenames = image_dataset[\"image_file_path\"]\nlabels = image_dataset[\"labels\"]"
```

----------------------------------------

TITLE: Preprocessing Text Data for Tokenization
DESCRIPTION: Function to preprocess text data by joining list elements and tokenizing them using the DistilGPT2 tokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/language_modeling.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> def preprocess_function(examples):
...     return tokenizer([" ".join(x) for x in examples["answers.text"]])
```

LANGUAGE: python
CODE:
```
>>> tokenized_eli5 = eli5.map(
...     preprocess_function,
...     batched=True,
...     num_proc=4,
...     remove_columns=eli5["train"].column_names,
... )
```

----------------------------------------

TITLE: Loading ROUGE Evaluation Metric
DESCRIPTION: Loads the ROUGE evaluation metric from the Evaluate library, which is commonly used to assess the quality of summaries by comparing them with reference summaries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
import evaluate

rouge = evaluate.load("rouge")
```

----------------------------------------

TITLE: Creating Label Mappings for Semantic Segmentation
DESCRIPTION: This code downloads label mappings from the Hugging Face Hub and creates dictionaries to convert between label IDs and class names for the ADE20K dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/sequence_classification.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> import json
>>> from pathlib import Path
>>> from huggingface_hub import hf_hub_download

>>> repo_id = "huggingface/label-files"
>>> filename = "ade20k-id2label.json"
>>> id2label = json.loads(Path(hf_hub_download(repo_id, filename, repo_type="dataset")).read_text())
>>> id2label = {int(k): v for k, v in id2label.items()}
>>> label2id = {v: k for k, v in id2label.items()}
>>> num_labels = len(id2label)
```

----------------------------------------

TITLE: Exporting BERT Model to TorchScript with Python
DESCRIPTION: This code demonstrates how to export a BERT model to TorchScript format by initializing the model with proper configuration, creating dummy inputs, and saving the traced model to disk.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/torchscript.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import BertModel, BertTokenizer, BertConfig
import torch

enc = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")

# å¯¹è¾“å…¥æ–‡æœ¬åˆ†è¯
text = "[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]"
tokenized_text = enc.tokenize(text)

# å±è”½ä¸€ä¸ªè¾“å…¥ token
masked_index = 8
tokenized_text[masked_index] = "[MASK]"
indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

# åˆ›å»ºè™šæ‹Ÿè¾“å…¥
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

# ä½¿ç”¨ torchscript å‚æ•°åˆå§‹åŒ–æ¨¡åž‹
# å³ä½¿æ­¤æ¨¡åž‹æ²¡æœ‰ LM Headï¼Œä¹Ÿå°†å‚æ•°è®¾ç½®ä¸º Trueã€‚
config = BertConfig(
    vocab_size_or_config_json_file=32000,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072,
    torchscript=True,
)

# å®žä¾‹åŒ–æ¨¡åž‹
model = BertModel(config)

# æ¨¡åž‹éœ€è¦å¤„äºŽè¯„ä¼°æ¨¡å¼
model.eval()

# å¦‚æžœæ‚¨ä½¿ç”¨ *from_pretrained* å®žä¾‹åŒ–æ¨¡åž‹ï¼Œè¿˜å¯ä»¥è½»æ¾è®¾ç½® TorchScript å‚æ•°
model = BertModel.from_pretrained("google-bert/bert-base-uncased", torchscript=True)

# åˆ›å»º trace
traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])
torch.jit.save(traced_model, "traced_bert.pt")
```

----------------------------------------

TITLE: Loading Model-Specific Feature Extractor
DESCRIPTION: This snippet shows how to load a feature extractor directly from its model-specific class, in this case, `WhisperFeatureExtractor`. This ensures that the correct feature extractor is used for a particular model architecture, such as Whisper.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/feature_extractors.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
```py
from transformers import WhisperFeatureExtractor

feature_extractor = WhisperFeatureExtractor.from_pretrained("openai/whisper-tiny")
```
```

----------------------------------------

TITLE: Importing BERTweet Model and Tokenizer - Python
DESCRIPTION: This snippet demonstrates how to import the BERTweet model and tokenizer from the Transformers library. It includes setup for using the model with either PyTorch or TensorFlow, showcasing the methods to load pretrained weights for the BERTweet model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bertweet.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> import torch
>>> from transformers import AutoModel, AutoTokenizer

>>> bertweet = AutoModel.from_pretrained("vinai/bertweet-base")

>>> # For transformers v4.x+:
>>> tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base", use_fast=False)

>>> # For transformers v3.x:
>>> # tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base")

```

----------------------------------------

TITLE: Setting Up Data Collator for LayoutLMv2 Training in Python
DESCRIPTION: This code defines a default data collator for batching examples together during the training of a LayoutLMv2 model for document question answering.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/document_question_answering.md#2025-04-22_snippet_22

LANGUAGE: python
CODE:
```
from transformers import DefaultDataCollator

data_collator = DefaultDataCollator()
```

----------------------------------------

TITLE: Disabling Advisory Warnings Using Environment Variable
DESCRIPTION: Demonstrates how to disable advisory warnings by setting an environment variable before running a Python script.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/logging.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
TRANSFORMERS_NO_ADVISORY_WARNINGS=1 ./myprogram.py
```

----------------------------------------

TITLE: Initializing Dynamic Cache for Transformer Generation
DESCRIPTION: Demonstrates how to explicitly initialize and use a DynamicCache object for text generation
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/kv_cache.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf", torch_dtype=torch.float16).to("cuda:0")
inputs = tokenizer("I like rock music because", return_tensors="pt").to(model.device)

past_key_values = DynamicCache()
out = model.generate(**inputs, do_sample=False, max_new_tokens=20, past_key_values=past_key_values)
```

----------------------------------------

TITLE: Converting a model to BetterTransformer
DESCRIPTION: This code snippet shows how to convert a model to use PyTorch's BetterTransformer integration, which leverages native attention mechanisms for improved performance. This assumes the `optimum` package is installed. The converted model can then be trained as usual.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perf_train_gpu_one.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
"model = model.to_bettertransformer()"
```

----------------------------------------

TITLE: Generating text with Llama using CLI (Bash)
DESCRIPTION: This snippet provides a command-line example for text generation using the `transformers` CLI. It pipes the input prompt string "Plants create energy through a process known as" to the `transformers run` command. The command specifies the `text-generation` task, the `huggyllama/llama-7b` model, and uses device 0.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llama.md#_snippet_2

LANGUAGE: bash
CODE:
```
echo -e "Plants create energy through a process known as" | transformers run --task text-generation --model huggyllama/llama-7b --device 0
```

----------------------------------------

TITLE: Load Images for Image-Guided Object Detection (OWL-ViT)
DESCRIPTION: Loads the target and query images from URLs using the requests library and PIL's Image module.  The target image is the image to search within, and the query image is the image of the object to find.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/zero_shot_object_detection.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image_target = Image.open(requests.get(url, stream=True).raw)

>>> query_url = "http://images.cocodataset.org/val2017/000000524280.jpg"
>>> query_image = Image.open(requests.get(query_url, stream=True).raw)
```

----------------------------------------

TITLE: Customizing ViT Image Processor in Python
DESCRIPTION: This snippet shows how to customize a ViTImageProcessor by modifying its parameters. It changes the resampling method, disables normalization, and sets custom image mean values. The output shows the updated parameters of the customized image processor.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/create_a_model.md#_snippet_19

LANGUAGE: python
CODE:
```
>>> from transformers import ViTImageProcessor

>>> my_vit_extractor = ViTImageProcessor(resample="PIL.Image.BOX", do_normalize=False, image_mean=[0.3, 0.3, 0.3])
>>> print(my_vit_extractor)
ViTImageProcessor {
  "do_normalize": false,
  "do_resize": true,
 "image_processor_type": "ViTImageProcessor",
  "image_mean": [
    0.3,
    0.3,
    0.3
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "resample": "PIL.Image.BOX",
  "size": 224
}
```

----------------------------------------

TITLE: Multilingual Speech Generation
DESCRIPTION: Examples of generating speech in multiple languages and with music notation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bark.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
inputs = processor("æƒŠäººçš„ï¼æˆ‘ä¼šè¯´ä¸­æ–‡")

inputs = processor("Incroyable! Je peux gÃ©nÃ©rer du son.", voice_preset="fr_speaker_5")

inputs = processor("â™ª Hello, my dog is cute â™ª")

audio_array = model.generate(**inputs)
audio_array = audio_array.cpu().numpy().squeeze()
```

----------------------------------------

TITLE: Custom Model Configuration
DESCRIPTION: Demonstrates how to create custom model configurations and initialize models from them
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/quicktour.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
from transformers import AutoConfig

my_config = AutoConfig.from_pretrained("distilbert/distilbert-base-uncased", n_heads=12)

# PyTorch
from transformers import AutoModel
my_model = AutoModel.from_config(my_config)

# TensorFlow
from transformers import TFAutoModel
my_model = TFAutoModel.from_config(my_config)
```

----------------------------------------

TITLE: Quantizing a model with AutoRound CLI
DESCRIPTION: Demonstrates quantizing a model using the AutoRound command-line interface. It specifies the model name, number of bits, group size, and output directory for the quantized model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/auto_round.md#_snippet_1

LANGUAGE: bash
CODE:
```
auto-round \
    --model facebook/opt-125m \
    --bits 4 \
    --group_size 128 \
    --output_dir ./tmp_autoround
```

----------------------------------------

TITLE: ONNX Model Export CLI Usage
DESCRIPTION: Example of using the transformers.onnx CLI tool to export a model
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/serialization.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
python -m transformers.onnx --model=distilbert/distilbert-base-uncased onnx/
```

----------------------------------------

TITLE: MBart Finnish to English Translation
DESCRIPTION: Shows how to use MBart model for Finnish to English translation using the facebook/mbart-large-50-many-to-many-mmt checkpoint. Includes source language setting and forced BOS token generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/multilingual.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

>>> en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
>>> fi_text = "Ã„lÃ¤ sekaannu velhojen asioihin, sillÃ¤ ne ovat hienovaraisia ja nopeasti vihaisia."

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt", src_lang="fi_FI")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")

>>> encoded_en = tokenizer(en_text, return_tensors="pt")
>>> generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id("en_XX"))
>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
```

----------------------------------------

TITLE: Using Transformers Logger in Custom Module (Python)
DESCRIPTION: Example of how to use the same logger as the library in a custom module or script, setting verbosity and using different log levels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/main_classes/logging.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
from transformers.utils import logging

logging.set_verbosity_info()
logger = logging.get_logger("transformers")
logger.info("INFO")
logger.warning("WARN")
```

----------------------------------------

TITLE: Pushing TensorFlow Model to Hub in Python
DESCRIPTION: Demonstrates how to add a TensorFlow version of a model to an existing repository on the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/model_sharing.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
>>> tf_model.push_to_hub("my-awesome-model")
```

----------------------------------------

TITLE: Token Classification for Training
DESCRIPTION: Illustrates using MarkupLMProcessor for token classification with training data, showing how node labels are handled and converted into token-level labels necessary for training models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/markuplm.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import MarkupLMProcessor

processor = MarkupLMProcessor.from_pretrained("microsoft/markuplm-base")
processor.parse_html = False

nodes = ["hello", "world", "how", "are"]
xpaths = ["/html/body/div/li[1]/div/span", "/html/body/div/li[1]/div/span", "html/body", "html/body/div"]
node_labels = [1, 2, 2, 1]
encoding = processor(nodes=nodes, xpaths=xpaths, node_labels=node_labels, return_tensors="pt")
print(encoding.keys())
```

----------------------------------------

TITLE: DINOv2 Model Tracing with torch.jit.trace
DESCRIPTION: Demonstrates how to use torch.jit.trace to optimize model inference with a potential minor precision trade-off
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/dinov2.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoImageProcessor, AutoModel
from PIL import Image
import requests

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')
model = AutoModel.from_pretrained('facebook/dinov2-base')

inputs = processor(images=image, return_tensors="pt")
outputs = model(**inputs)
last_hidden_states = outputs[0]

# We have to force return_dict=False for tracing
model.config.return_dict = False

with torch.no_grad():
    traced_model = torch.jit.trace(model, [inputs.pixel_values])
    traced_outputs = traced_model(inputs.pixel_values)

print((last_hidden_states - traced_outputs[0]).abs().max())
```

----------------------------------------

TITLE: Defining Model Initialization Function in Python
DESCRIPTION: This snippet shows how to define a model initialization function that returns a pretrained model for sequence classification based on the provided model name and arguments.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/hpo_train.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> def model_init(trial):
...     return AutoModelForSequenceClassification.from_pretrained(
...         model_args.model_name_or_path,
...         from_tf=bool(".ckpt" in model_args.model_name_or_path),
...         config=config,
...         cache_dir=model_args.cache_dir,
...         revision=model_args.model_revision,
...         token=True if model_args.use_auth_token else None,
...     )
```

----------------------------------------

TITLE: Using AltCLIP for Image-Text Similarity
DESCRIPTION: Example showing how to use AltCLIPProcessor and AltCLIPModel to compute image-text similarity scores. The code demonstrates loading the model and processor, processing an image and text inputs, and obtaining similarity probabilities.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/altclip.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from PIL import Image
import requests

from transformers import AltCLIPModel, AltCLIPProcessor

model = AltCLIPModel.from_pretrained("BAAI/AltCLIP")
processor = AltCLIPProcessor.from_pretrained("BAAI/AltCLIP")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True)

outputs = model(**inputs)
logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities
```

----------------------------------------

TITLE: Decoding Predictions in TensorFlow
DESCRIPTION: This code decodes model predictions by finding the class with the highest probability and mapping those class IDs back to label strings using the model's `id2label` mapping in TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_33

LANGUAGE: python
CODE:
```
>>> predicted_token_class_ids = tf.math.argmax(logits, axis=-1)
>>> predicted_token_class = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]
>>> predicted_token_class
```

----------------------------------------

TITLE: Previewing Tokenized Input Structure
DESCRIPTION: Demonstrates how the inputs look after tokenization, showing how the question and document are combined in the encoded format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/document_question_answering.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
>>> encoding = tokenizer(example["question"], example["words"], example["boxes"])
>>> tokenizer.decode(encoding["input_ids"])
```

----------------------------------------

TITLE: Creating and Training a Dummy Tokenizer with ðŸ¤— Tokenizers
DESCRIPTION: This snippet demonstrates how to create a basic BPE tokenizer using the ðŸ¤— Tokenizers library, including setting up the tokenizer, trainer, and pre-tokenizer. It also shows how to train the tokenizer on a set of files.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/fast_tokenizers.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
>>> from tokenizers import Tokenizer
>>> from tokenizers.models import BPE
>>> from tokenizers.trainers import BpeTrainer
>>> from tokenizers.pre_tokenizers import Whitespace

>>> tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
>>> trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

>>> tokenizer.pre_tokenizer = Whitespace()
>>> files = [...]
>>> tokenizer.train(files, trainer)
```

----------------------------------------

TITLE: Grouping Texts for Language Modeling
DESCRIPTION: Prepares text sequences for language modeling by grouping them into fixed-size blocks. Concatenates and splits sequences based on a specified block size to optimize processing without exceeding model input limitations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
block_size = 128

def group_texts(examples):
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    if total_length >= block_size:
        total_length = (total_length // block_size) * block_size
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    return result
```

----------------------------------------

TITLE: Modifying and Uploading a Chat Template to HuggingFace Hub
DESCRIPTION: Shows how to modify an existing chat template, set it for a tokenizer, and upload the updated template to the HuggingFace Hub to ensure everyone uses the correct template.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/chat_templating.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
template = tokenizer.chat_template
template = template.replace("SYS", "SYSTEM")  # Change the system token
tokenizer.chat_template = template  # Set the new template
tokenizer.push_to_hub("model_name")  # Upload your new template to the Hub!
```

----------------------------------------

TITLE: LlavaOnevision Model with Flash Attention 2
DESCRIPTION: This snippet demonstrates how to enable Flash Attention 2 for faster generation with the LlavaOnevision model. It sets `use_flash_attention_2` to True when loading the model using `from_pretrained`. It also uses `low_cpu_mem_usage=True` and moves the model to GPU 0.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llava_onevision.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
"from transformers import LlavaOnevisionForConditionalGeneration

model = LlavaOnevisionForConditionalGeneration.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    use_flash_attention_2=True
).to(0)"
```

----------------------------------------

TITLE: Testing ðŸ¤— Accelerate Configuration
DESCRIPTION: This command tests the ðŸ¤— Accelerate configuration to ensure it is set up correctly before starting the training process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/run_scripts.md#2025-04-22_snippet_8

LANGUAGE: bash
CODE:
```
accelerate test
```

----------------------------------------

TITLE: Preprocessing Dataset with Image Processor
DESCRIPTION: Applies image preprocessing using a pre-trained image processor to prepare dataset for model training
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/knowledge_distillation_for_image_classification.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoImageProcessor
teacher_processor = AutoImageProcessor.from_pretrained("merve/beans-vit-224")

def process(examples):
    processed_inputs = teacher_processor(examples["image"])
    return processed_inputs

processed_datasets = dataset.map(process, batched=True)
```

----------------------------------------

TITLE: Decode Top Predicted Tokens (PyTorch)
DESCRIPTION: Identifies the top 3 predicted tokens based on the logits and decodes them to generate complete sentences.  Requires the `transformers` library and the `mask_token_logits` obtained from the model.  Prints the sentences with the masked token replaced by the top predictions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_27

LANGUAGE: python
CODE:
```
>>> top_3_tokens = torch.topk(mask_token_logits, 3, dim=1).indices[0].tolist()

>>> for token in top_3_tokens:
...	print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))
```

----------------------------------------

TITLE: Filtering Speakers Based on Example Count in Python
DESCRIPTION: Filters the dataset to include only speakers with between 100 and 400 examples. This helps to balance the dataset and improve training efficiency.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/text-to-speech.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> def select_speaker(speaker_id):
...     return 100 <= speaker_counts[speaker_id] <= 400


>>> dataset = dataset.filter(select_speaker, input_columns=["speaker_id"])
```

----------------------------------------

TITLE: Getting Top Predictions from TensorFlow MLM Logits
DESCRIPTION: Extracts the top token predictions from the TensorFlow model's logits and formats them as complete sentences by replacing the mask token.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/masked_language_modeling.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
top_3_tokens = tf.math.top_k(mask_token_logits, 3).indices.numpy()

for token in top_3_tokens:
    print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))
```

----------------------------------------

TITLE: Generating Answers with BLIP-2
DESCRIPTION: Demonstrates the full process of generating answers for visual questions using BLIP-2, including input processing and text generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/visual_question_answering.md#2025-04-22_snippet_24

LANGUAGE: python
CODE:
```
>>> inputs = processor(image, text=prompt, return_tensors="pt").to(device, torch.float16)

>>> generated_ids = model.generate(**inputs, max_new_tokens=10)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
>>> print(generated_text)
"He is looking at the crowd"
```

----------------------------------------

TITLE: Hugging Face Login
DESCRIPTION: Authentication setup for accessing Hugging Face Hub features
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/token_classification.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Initializing EncoderDecoderModel from Pretrained Checkpoints (Python)
DESCRIPTION: This code demonstrates how to initialize an EncoderDecoderModel using pre-trained encoder and decoder checkpoints.  It loads a BERT tokenizer and then initializes the model with two BERT-based checkpoints. The model should be fine-tuned after initialization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/encoder-decoder.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import EncoderDecoderModel, BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> model = EncoderDecoderModel.from_encoder_decoder_pretrained("google-bert/bert-base-uncased", "google-bert/bert-base-uncased")
```

----------------------------------------

TITLE: Setting the Active Adapter in a Model Using Python
DESCRIPTION: This shows how to set a specific adapter as active in a model containing multiple adapters, providing a mechanism to switch between configurations as needed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/peft.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
model.set_adapter("lora_2")

```

----------------------------------------

TITLE: Loading Pre-trained Model with Custom Config
DESCRIPTION: Shows how to load a pre-trained model while overriding its configuration with custom parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/create_a_model.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
model = DistilBertModel.from_pretrained("distilbert/distilbert-base-uncased", config=my_config)
```

----------------------------------------

TITLE: Defining Metric Computation Function for Training
DESCRIPTION: Creates a function that calculates accuracy by comparing model predictions with ground truth labels. It processes the model output by taking the argmax along the prediction dimension to get the predicted class.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> import numpy as np

>>> def compute_metrics(eval_pred):
...     predictions, labels = eval_pred
...     predictions = np.argmax(predictions, axis=1)
...     return accuracy.compute(predictions=predictions, references=labels)
```

----------------------------------------

TITLE: GPU Usage Monitoring Functions
DESCRIPTION: Defines helper functions to print GPU utilization and training summary statistics. `print_gpu_utilization` uses the `pynvml` library to retrieve and print the GPU memory occupied. `print_summary` prints the training time, samples per second, and GPU memory utilization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/model_memory_anatomy.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> from pynvml import *

>>> def print_gpu_utilization():
...     nvmlInit()
...     handle = nvmlDeviceGetHandleByIndex(0)
...     info = nvmlDeviceGetMemoryInfo(handle)
...     print(f"GPU memory occupied: {info.used//1024**2} MB.")

>>> def print_summary(result):
...     print(f"Time: {result.metrics['train_runtime']:.2f}")
...     print(f"Samples/second: {result.metrics['train_samples_per_second']:.2f}")
...     print_gpu_utilization()
```

----------------------------------------

TITLE: Create Model from Custom Config (PyTorch)
DESCRIPTION: This snippet creates a PyTorch model from a custom configuration using `AutoModel.from_config`.  It assumes a `my_config` object has been previously created and modified. The model is initialized with random weights and requires training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_31

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModel

>>> my_model = AutoModel.from_config(my_config)
```

----------------------------------------

TITLE: Removing Unnecessary Columns in Python
DESCRIPTION: This snippet removes the unnecessary columns 'question_type', 'question_id', and 'answer_type' from the dataset using the remove_columns method in preparation for processing, keeping only the essential features for the VQA task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_question_answering.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
dataset = dataset.remove_columns(['question_type', 'question_id', 'answer_type'])
```

----------------------------------------

TITLE: Fine-tuning XLSR-Wav2Vec2 on Common Voice with Multi GPU CTC
DESCRIPTION: Command for fine-tuning XLSR-Wav2Vec2 on the Common Voice Turkish dataset using 8 GPUs in half-precision. This configuration runs in about 18 minutes on 8 V100 GPUs and yields a CTC loss of 0.39 and word error rate of 0.36.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-recognition/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
torchrun \
	--nproc_per_node 8 run_speech_recognition_ctc.py \
	--dataset_name="common_voice" \
	--model_name_or_path="facebook/wav2vec2-large-xlsr-53" \
	--dataset_config_name="tr" \
	--output_dir="./wav2vec2-common_voice-tr-demo-dist" \
	--overwrite_output_dir \
	--num_train_epochs="15" \
	--per_device_train_batch_size="4" \
	--learning_rate="3e-4" \
	--warmup_steps="500" \
	--eval_strategy="steps" \
	--text_column_name="sentence" \
	--length_column_name="input_length" \
	--save_steps="400" \
	--eval_steps="100" \
	--logging_steps="1" \
	--layerdrop="0.0" \
	--save_total_limit="3" \
	--freeze_feature_encoder \
	--gradient_checkpointing \
	--chars_to_ignore , ? . ! - \; \: \" " % ' " ï¿½ \
	--fp16 \
	--group_by_length \
	--push_to_hub \
	--do_train --do_eval
```

----------------------------------------

TITLE: Creating a Learning Rate Scheduler
DESCRIPTION: Creates a learning rate scheduler to adjust the learning rate during training, using a linear schedule from the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/training.md#2025-04-22_snippet_21

LANGUAGE: python
CODE:
```
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
)
```

----------------------------------------

TITLE: Preprocessing ELI5 Dataset in Python
DESCRIPTION: Defines and applies preprocessing functions to flatten the dataset structure, tokenize the text, and group texts into chunks of specified block size.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/language_modeling.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
>>> def preprocess_function(examples):
...     return tokenizer([" ".join(x) for x in examples["answers.text"]])

>>> tokenized_eli5 = eli5.map(
...     preprocess_function,
...     batched=True,
...     num_proc=4,
...     remove_columns=eli5["train"].column_names,
... )

>>> block_size = 128

>>> def group_texts(examples):
...     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
...     total_length = len(concatenated_examples[list(examples.keys())[0]])
...     if total_length >= block_size:
...         total_length = (total_length // block_size) * block_size
...     result = {
...         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
...         for k, t in concatenated_examples.items()
...     }
...     result["labels"] = result["input_ids"].copy()
...     return result

>>> lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)
```

----------------------------------------

TITLE: Feature Extraction for Audio Classification with PyTorch
DESCRIPTION: Demonstrates how to manually extract features from audio data using a feature extractor that prepares the input for model processing with PyTorch tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/audio_classification.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("stevhliu/my_awesome_minds_model")
>>> inputs = feature_extractor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
```

----------------------------------------

TITLE: Using Flash Attention with Transformers in Python
DESCRIPTION: This snippet demonstrates loading the Mixtral model with Flash Attention 2 for optimized performance, utilizing half-precision and specific attention implementations to speed up inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mixtral.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> import torch
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> model = AutoModelForCausalLM.from_pretrained("mistralai/Mixtral-8x7B-v0.1", torch_dtype=torch.float16, attn_implementation="flash_attention_2", device_map="auto")
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mixtral-8x7B-v0.1")

>>> prompt = "My favourite condiment is"

>>> model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
>>> model.to(device)

>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)
>>> tokenizer.batch_decode(generated_ids)[0]
"The expected output"
```

----------------------------------------

TITLE: Adding IPEX parameters to TrainingArguments
DESCRIPTION: Code diff showing how to enable IPEX and bf16 mixed precision training by modifying TrainingArguments in Python script.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/perf_train_cpu.md#2025-04-22_snippet_1

LANGUAGE: diff
CODE:
```
training_args = TrainingArguments(
    output_dir=args.output_path,
+   bf16=True,
+   use_ipex=True,
+   use_cpu=True,
    **kwargs
)
```

----------------------------------------

TITLE: Generating Speech with AutoModel in Transformers (Python)
DESCRIPTION: This code snippet illustrates how to use the `AutoModel` class from the Hugging Face Transformers library for a more manual approach to text-to-speech conversion using the VITS model. It demonstrates tokenization of input text, model inference, and saving the output waveform to a WAV file. The code requires `torch`, `scipy`, and `transformers` libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/vits.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
import scipy
from IPython.display import Audio
from transformers import AutoTokenizer, VitsModel, set_seed

tokenizer = AutoTokenizer.from_pretrained("facebook/mms-tts-eng")
model = VitsModel.from_pretrained("facebook/mms-tts-eng", torch_dtype=torch.float16).to("cuda")
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt").to("cuda")

set_seed(555)

with torch.no_grad():
    outputs = model(**inputs)

waveform = outputs.waveform[0]
scipy.io.wavfile.write("hello.wav", rate=model.config.sampling_rate, data=waveform)

# display in Colab notebook
Audio(waveform, rate=model.config.sampling_rate)
```

----------------------------------------

TITLE: Initializing TAPAS Model with WikiSQL Configuration in TensorFlow
DESCRIPTION: This snippet illustrates how to set up the TAPAS model with WikiSQL configuration using TensorFlow, allowing for strong supervision of aggregation tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/tapas.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> # or, the base sized model with WikiSQL configuration
>>> config = TapasConfig("google-base-finetuned-wikisql-supervised")
>>> model = TFTapasForQuestionAnswering.from_pretrained("google/tapas-base", config=config)

```

----------------------------------------

TITLE: Preparing Inputs for Model Inference in PyTorch
DESCRIPTION: This snippet illustrates how to process the image input before passing it to the model for predictions, ensuring the input is moved to the correct device (GPU/CPU).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/sequence_classification.md#2025-04-22_snippet_26

LANGUAGE: python
CODE:
```
>>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # use GPU if available, otherwise use a CPU
>>> encoding = image_processor(image, return_tensors="pt")
>>> pixel_values = encoding.pixel_values.to(device)
```

----------------------------------------

TITLE: Python Equivalent of a Jinja Chat Template
DESCRIPTION: A Python code snippet showing the equivalent logic of the BlenderBot chat template. This demonstrates what the Jinja template does in more familiar Python syntax.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/chat_templating.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
for idx, message in enumerate(messages):
    if message['role'] == 'user':
        print(' ')
    print(message['content'])
    if not idx == len(messages) - 1:  # Check for the last message in the conversation
        print('  ')
print(eos_token)
```

----------------------------------------

TITLE: Pushing a Quantized Model to Hugging Face Hub (Non-Safetensors)
DESCRIPTION: This snippet demonstrates pushing a quantized model and its tokenizer to the Hugging Face Hub. Similar to local saving, `safe_serialization=False` must be used when pushing the model to avoid issues with torchao's tensor subclasses. The tokenizer can be pushed normally.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/torchao.md#_snippet_12

LANGUAGE: Python
CODE:
```
# don't serialize model with Safetensors
USER_ID = "your_huggingface_user_id"
REPO_ID = "llama3-8b-int4wo-128"
quantized_model.push_to_hub(f"{USER_ID}/llama3-8b-int4wo-128", safe_serialization=False)
tokenizer.push_to_hub(f"{USER_ID}/llama3-8b-int4wo-128")
```

----------------------------------------

TITLE: Getting Timestamps with Speech Recognition Pipeline in Python
DESCRIPTION: Runs speech recognition inference with word-level timestamps returned.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/hi/pipeline_tutorial.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
transcriber = pipeline(model="openai/whisper-large-v2", return_timestamps=True)
transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
```

----------------------------------------

TITLE: AltCLIP Image-Text Similarity Calculation in Python
DESCRIPTION: This code snippet demonstrates how to use the AltCLIP model and processor to calculate image-text similarity scores. It loads a pre-trained AltCLIP model and processor, fetches an image from a URL, and encodes the image and text prompts using the processor.  The model then computes the similarity scores between the image and the text prompts, and applies softmax to convert the logits into probabilities.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/altclip.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> from PIL import Image
>>> import requests

>>> from transformers import AltCLIPModel, AltCLIPProcessor

>>> model = AltCLIPModel.from_pretrained("BAAI/AltCLIP")
>>> processor = AltCLIPProcessor.from_pretrained("BAAI/AltCLIP")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True)

>>> outputs = model(**inputs)
>>> logits_per_image = outputs.logits_per_image  # ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì ìˆ˜
>>> probs = logits_per_image.softmax(dim=1)  # ë¼ë²¨ ë§ˆë‹¤ í™•ë¥ ì„ ì–»ê¸° ìœ„í•´ softmax ì ìš©
```

----------------------------------------

TITLE: Preparing Batch Mixed Media Input Qwen2.5-Omni Python
DESCRIPTION: This snippet demonstrates how to initialize the `Qwen2_5OmniForConditionalGeneration` model and `Qwen2_5OmniProcessor` and prepare multiple conversations (`conversation1`, `conversation2`, `conversation3`) containing different media types (video, audio, pure text) for potential batch processing. The code shows the structure required for diverse inputs before they would be processed together using the `processor.apply_chat_template` method for batch inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2_5_omni.md#_snippet_2

LANGUAGE: python
CODE:
```
import soundfile as sf
from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor

model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-Omni-7B",
    torch_dtype="auto",
    device_map="auto"
)
processor = Qwen2_5OmniProcessor.from_pretrained("Qwen/Qwen2.5-Omni-7B")

# Conversation with video only
conversation1 = [
    {
        "role": "system",
        "content": [
            {"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."}
        ],
    },
    {
        "role": "user",
        "content": [
            {"type": "video", "path": "/path/to/video.mp4"},
        ]
    }
]

# Conversation with audio only
conversation2 = [
    {
        "role": "system",
        "content": [
            {"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."}
        ],
    },
    {
        "role": "user",
        "content": [
            {"type": "audio", "path": "/path/to/audio.wav"},
        ]
    }
]

# Conversation with pure text
conversation3 = [
    {
        "role": "system",
        "content": [
            {"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."}
        ],
    },
    {
        "role": "user",
        "content": [{"type": "text", "text": "who are you?"}],
    }
]
```

----------------------------------------

TITLE: Logging into Hugging Face Hub
DESCRIPTION: This code snippet logs the user into the Hugging Face Hub using their notebook login. This is necessary to share the fine-tuned model with the community.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/object_detection.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
">>> from huggingface_hub import notebook_login\n\n>>> notebook_login()"
```

----------------------------------------

TITLE: Loading Backbone and Neck Directly into Model Configuration
DESCRIPTION: This snippet shows how to load a backbone and neck directly into a model's configuration. It sets the `backbone` parameter to a pretrained model and `use_pretrained_backbone=True` to use pretrained weights. This is then used to initialize a `MaskFormerForInstanceSegmentation` model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/backbones.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation

config = MaskFormerConfig(backbone="microsoft/resnet-50", use_pretrained_backbone=True)
model = MaskFormerForInstanceSegmentation(config)
```

----------------------------------------

TITLE: Casting Audio Column to Correct Sampling Rate in Hugging Face Datasets in Python
DESCRIPTION: This code ensures that the audio dataset's sampling rate matches the pre-trained model's sampling rate.  It casts the audio column to the correct sampling rate using the `cast_column` function.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_8

LANGUAGE: python
CODE:
```
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))
```

----------------------------------------

TITLE: Post-Processing Depth Estimation Results
DESCRIPTION: This code demonstrates how to post-process the depth estimation outputs to remove padding and resize the depth map, including visualizing the results.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/monocular_depth_estimation.md#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
>>> post_processed_output = image_processor.post_process_depth_estimation(
...     outputs,
...     source_sizes=[(image.height, image.width)],
... )
>>> predicted_depth = post_processed_output[0]["predicted_depth"]
>>> depth = (predicted_depth - predicted_depth.min()) / (predicted_depth.max() - predicted_depth.min())
>>> depth = depth.detach().cpu().numpy() * 255
>>> depth = Image.fromarray(depth.astype("uint8"))
```

----------------------------------------

TITLE: Loading Model with Custom Code
DESCRIPTION: This snippet demonstrates how to load a model from the Hugging Face Hub that includes custom code, using the `AutoModelForImageClassification` class and setting `trust_remote_code=True`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/custom_models.md#_snippet_16

LANGUAGE: Python
CODE:
```
from transformers import AutoModelForImageClassification

model = AutoModelForImageClassification.from_pretrained("sgugger/custom-resnet50d", trust_remote_code=True)
```

----------------------------------------

TITLE: Installing ONNX Export Dependencies for Transformers
DESCRIPTION: Command to install the necessary dependencies for exporting Transformers models to ONNX format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/serialization.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers[onnx]
```

----------------------------------------

TITLE: Loading Audio Data for Language Identification - Python
DESCRIPTION: This snippet demonstrates loading audio data from datasets for language identification tasks, ensuring the samples are properly formatted.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mms.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from datasets import load_dataset, Audio

# English
stream_data = load_dataset("mozilla-foundation/common_voice_13_0", "en", split="test", streaming=True)
stream_data = stream_data.cast_column("audio", Audio(sampling_rate=16000))
en_sample = next(iter(stream_data))["audio"]["array"]

# Arabic
stream_data = load_dataset("mozilla-foundation/common_voice_13_0", "ar", split="test", streaming=True)
stream_data = stream_data.cast_column("audio", Audio(sampling_rate=16000))
ar_sample = next(iter(stream_data))["audio"]["array"]
```

----------------------------------------

TITLE: Tokenizing data and converting to NumPy arrays
DESCRIPTION: Loads a pre-trained tokenizer and tokenizes the input text data into NumPy arrays. The labels are also converted into a NumPy array. The tokenized data is converted to a dictionary for use with Keras.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/training.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")
tokenized_data = tokenizer(dataset["text"], return_tensors="np", padding=True)
# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras
tokenized_data = dict(tokenized_data)

labels = np.array(dataset["label"])  # Label is already an array of 0 and 1
```

----------------------------------------

TITLE: Loading a Pretrained BERT Model and Accessing pad_token_id in Python
DESCRIPTION: This code loads a pretrained BERT model for sequence classification and demonstrates how to access the pad_token_id from the model's configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/troubleshooting.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSequenceClassification
>>> import torch

>>> model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-uncased")
>>> model.config.pad_token_id
0
```

----------------------------------------

TITLE: Loading the OPUS Books Dataset for English-French Translation
DESCRIPTION: Loads the English-French subset of the OPUS Books dataset and splits it into training and testing sets with an 80/20 ratio for model training and evaluation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/translation.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> books = load_dataset("opus_books", "en-fr")
>>> books = books["train"].train_test_split(test_size=0.2)
>>> books["train"][0]
```

----------------------------------------

TITLE: Loading SeamlessM4T-v2 Model and Processor
DESCRIPTION: This code snippet demonstrates how to load the SeamlessM4T-v2 processor and model from the Hugging Face model hub. The processor handles tokenization of inputs, while the model is used for the actual translation tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/seamless_m4t_v2.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoProcessor, SeamlessM4Tv2Model

processor = AutoProcessor.from_pretrained("facebook/seamless-m4t-v2-large")
model = SeamlessM4Tv2Model.from_pretrained("facebook/seamless-m4t-v2-large")
```

----------------------------------------

TITLE: Installing Intel Extension for PyTorch (IPEX) via pip
DESCRIPTION: This command installs a specific version of the Intel Extension for PyTorch (IPEX) using pip, specifying the version name and the Intel developer's wheel repository. It allows users to leverage CPU optimizations for PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/perf_train_cpu.md#_snippet_0

LANGUAGE: bash
CODE:
```
pip install intel_extension_for_pytorch==<version_name> -f https://developer.intel.com/ipex-whl-stable-cpu
```

----------------------------------------

TITLE: Initializing Model and Processor Without Pipeline in Python
DESCRIPTION: This snippet shows the initialization of the model and the corresponding processor for image super resolution using the Swin2SR architecture, allowing for custom preprocessing and inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_to_image.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import Swin2SRForImageSuperResolution, Swin2SRImageProcessor

model = Swin2SRForImageSuperResolution.from_pretrained("caidas/swin2SR-lightweight-x2-64").to(device)
processor = Swin2SRImageProcessor("caidas/swin2SR-lightweight-x2-64")
```

----------------------------------------

TITLE: BERT Encoded Input Decoding
DESCRIPTION: This code snippet shows the decoded output after encoding two sequences with a BertTokenizer. It demonstrates how the tokenizer combines two sequences with the [CLS] and [SEP] special tokens, providing a single input sequence suitable for BERT models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/glossary.md#_snippet_14

LANGUAGE: python
CODE:
```
>>> print(decoded)
[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace basedØŸ [SEP]
```

----------------------------------------

TITLE: Compiling TensorFlow Model for MLM Training
DESCRIPTION: Compiles the TensorFlow model with the optimizer, leveraging the default task-related loss function built into the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/masked_language_modeling.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
import tensorflow as tf

model.compile(optimizer=optimizer)  # No loss argument!
```

----------------------------------------

TITLE: Batch Processing with FeatureExtractor and Tokenizer in Pop2Piano for Python
DESCRIPTION: This code example uses the Pop2PianoFeatureExtractor and Pop2PianoTokenizer for batch processing multiple audio files. Audio files are loaded via librosa and processed into tensors for the model to generate MIDI outputs. Feature extraction and tokenizing ensure that audio inputs are adequately prepared for the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/pop2piano.md#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
import librosa
from transformers import Pop2PianoForConditionalGeneration, Pop2PianoFeatureExtractor, Pop2PianoTokenizer

# feel free to change the sr to a suitable value.
audio1, sr1 = librosa.load("<your_first_audio_file_here>", sr=44100)  
audio2, sr2 = librosa.load("<your_second_audio_file_here>", sr=44100)
model = Pop2PianoForConditionalGeneration.from_pretrained("sweetcocoa/pop2piano")
feature_extractor = Pop2PianoFeatureExtractor.from_pretrained("sweetcocoa/pop2piano")
tokenizer = Pop2PianoTokenizer.from_pretrained("sweetcocoa/pop2piano")

inputs = feature_extractor(
    audio=[audio1, audio2], 
    sampling_rate=[sr1, sr2], 
    return_attention_mask=True, 
    return_tensors="pt",
)
# Since we now generating in batch(2 audios) we must pass the attention_mask
model_output = model.generate(
    input_features=inputs["input_features"],
    attention_mask=inputs["attention_mask"],
    composer="composer1",
)
tokenizer_output = tokenizer.batch_decode(
    token_ids=model_output, feature_extractor_output=inputs
)["pretty_midi_objects"]

# Since we now have 2 generated MIDI files
tokenizer_output[0].write("./Outputs/midi_output1.mid")
tokenizer_output[1].write("./Outputs/midi_output2.mid")
```

----------------------------------------

TITLE: Flushing GPU Memory for Accurate Measurement
DESCRIPTION: This code defines a 'flush' function that collects garbage, empties the CUDA cache, and resets peak memory statistics to ensure that subsequent memory measurements are accurate. This is particularly useful when performing multiple experiments with GPU memory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
del pipe
del model

import gc
import torch

def flush():
  gc.collect()
  torch.cuda.empty_cache()
  torch.cuda.reset_peak_memory_stats()
```

----------------------------------------

TITLE: Extracting Dates with Few-Shot Prompting (Python)
DESCRIPTION: Initializes a text-generation pipeline. It constructs a prompt containing one example input/output pair (date extraction) followed by a new input, guiding the model to extract the date in the desired format. Runs the pipeline and prints the output. Requires `transformers` and `torch`. Takes text containing a date within the prompt as input and outputs the original prompt plus the extracted date in MM/DD/YYYY format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/prompting.md#_snippet_1

LANGUAGE: python
CODE:
```
from transformers import pipeline
import torch

pipeline = pipeline(model="mistralai/Mistral-7B-Instruct-v0.1", torch_dtype=torch.bfloat16, device_map="auto")
prompt = """Text: The first human went into space and orbited the Earth on April 12, 1961.
Date: 04/12/1961
Text: The first-ever televised presidential debate in the United States took place on September 28, 1960, between presidential candidates John F. Kennedy and Richard Nixon.
Date:"""

outputs = pipeline(prompt, max_new_tokens=12, do_sample=True, top_k=10)
for output in outputs:
    print(f"Result: {output['generated_text']}")
```

----------------------------------------

TITLE: Image Retrieval with ColPali in Python
DESCRIPTION: This code snippet demonstrates how to use ColPali for image retrieval. It loads a pre-trained ColPali model and processor, fetches images from URLs, and formulates text queries. It then processes the images and queries, performs a forward pass through the model to generate embeddings, and calculates retrieval scores to determine the similarity between the queries and images.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/colpali.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import requests
import torch
from PIL import Image
from transformers import ColPaliForRetrieval, ColPaliProcessor

# Load model (bfloat16 support is limited; fallback to float32 if needed)
model = ColPaliForRetrieval.from_pretrained(
    "vidore/colpali-v1.2-hf",
    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",  # "cpu", "cuda", or "mps" for Apple Silicon
).eval()

processor = ColPaliProcessor.from_pretrained(model_name)

url1 = "https://upload.wikimedia.org/wikipedia/commons/8/89/US-original-Declaration-1776.jpg"
url2 = "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Romeoandjuliet1597.jpg/500px-Romeoandjuliet1597.jpg"

images = [
    Image.open(requests.get(url1, stream=True).raw),
    Image.open(requests.get(url2, stream=True).raw),
]

queries = [
    "Who printed the edition of Romeo and Juliet?",
    "When was the United States Declaration of Independence proclaimed?",
]

# Process the inputs
inputs_images = processor(images=images, return_tensors="pt").to(model.device)
inputs_text = processor(text=queries, return_tensors="pt").to(model.device)

# Forward pass
with torch.no_grad():
    image_embeddings = model(**inputs_images).embeddings
    query_embeddings = model(**inputs_text).embeddings

scores = processor.score_retrieval(query_embeddings, image_embeddings)

print("Retrieval scores (query x image):")
print(scores)
```

----------------------------------------

TITLE: Performing Zero-Shot Object Detection with Custom Labels
DESCRIPTION: Uses the detector pipeline to identify objects in the image based on specified text labels. The model returns bounding boxes and confidence scores for each detected object.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/zero_shot_object_detection.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> predictions = detector(
...     image,
...     candidate_labels=["human face", "rocket", "nasa badge", "star-spangled banner"],
... )
>>> predictions
```

----------------------------------------

TITLE: Obtaining Better Tracebacks from GPU by Setting CUDA_LAUNCH_BLOCKING
DESCRIPTION: This code demonstrates setting the `CUDA_LAUNCH_BLOCKING` environment variable to "1". This can provide a more detailed traceback when encountering CUDA errors, pointing to the source of the error.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/troubleshooting.md#_snippet_4

LANGUAGE: Python
CODE:
```
>>> import os

>>> os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
```

----------------------------------------

TITLE: Using ConvNextForImageClassification in PyTorch
DESCRIPTION: ConvNeXT model for image classification tasks in PyTorch. Extends base model with classification head.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/convnext.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
[[autodoc]] ConvNextForImageClassification
    - forward
```

----------------------------------------

TITLE: Loading Image Processor for SegFormer
DESCRIPTION: This code loads the SegFormer image processor with label reduction to handle background class conversion, preparing images for the model input format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/sequence_classification.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> from transformers import AutoImageProcessor

>>> checkpoint = "nvidia/mit-b0"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint, do_reduce_labels=True)
```

----------------------------------------

TITLE: Training Idefics2 with Specific Token Handling in Python
DESCRIPTION: This code snippet shows how to train the Idefics2 model by specifying token handling for image and padding tokens, which the model should not learn. It uses PyTorch with Hugging Face's transformers library, highlighting the creation of input labels and the backward pass for computing loss. Prerequisites include availability of CUDA and the necessary pre-trained model files.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/idefics2.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import requests
from PIL import Image
from transformers import Idefics2Processor, Idefics2ForConditionalGeneration
import torch

url_1 = "http://images.cocodataset.org/val2017/000000039769.jpg"
url_2 = "http://images.cocodataset.org/val2017/000000219578.jpg"

image_1 = Image.open(requests.get(url_1, stream=True).raw)
image_2 = Image.open(requests.get(url_2, stream=True).raw)
images = [image_1, image_2]

messages = [{
    "role": "user",
    "content": [
        {"type": "text", "text": "Whatâ€™s the difference between these two images?"},
        {"type": "image"},
        {"type": "image"},
    ],
},
{
    "role": "assistant",
    "content": [
        {"type": "text", "text": "The difference is that one image is about dogs and the other one about cats."},
    ],
}]

device = "cuda" if torch.cuda.is_available() else "cpu"

processor = Idefics2Processor.from_pretrained("HuggingFaceM4/idefics2-8b")
model = Idefics2ForConditionalGeneration.from_pretrained("HuggingFaceM4/idefics2-8b")
model.to(device)

text = processor.apply_chat_template(messages, add_generation_prompt=False)
inputs = processor(images=images, text=text, return_tensors="pt").to(device)

labels = inputs.input_ids.clone()
labels[labels == processor.tokenizer.pad_token_id] = -100
labels[labels == model.config.image_token_id] = -100

inputs["labels"] = labels

outputs = model(**inputs)
loss = outputs.loss
loss.backward()
```

----------------------------------------

TITLE: Resizing Logits for TensorFlow Segmentation Output
DESCRIPTION: In this snippet, logits are resized to match the original image's dimensions, and argmax is used to retrieve segmentation predictions for each pixel, applying it to the logits output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_30

LANGUAGE: python
CODE:
```
>>> logits = tf.transpose(logits, [0, 2, 3, 1])

>>> upsampled_logits = tf.image.resize(
...     logits,
...     # We reverse the shape of `image` because `image.size` returns width and height.
...     image.size[::-1],
... )

>>> pred_seg = tf.math.argmax(upsampled_logits, axis=-1)[0]
```

----------------------------------------

TITLE: Configuring DeepSpeed Scheduler with Auto Parameters
DESCRIPTION: Set up a WarmupDecayLR scheduler with automatic parameter selection, enabling flexible learning rate scheduling during training
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/deepspeed.md#2025-04-22_snippet_22

LANGUAGE: yaml
CODE:
```
{
   "scheduler": {
         "type": "WarmupDecayLR",
         "params": {
             "total_num_steps": "auto",
             "warmup_min_lr": "auto",
             "warmup_max_lr": "auto",
             "warmup_num_steps": "auto"
         }
     }
}
```

----------------------------------------

TITLE: Setting Up TensorFlow Callbacks for Metrics and Model Sharing
DESCRIPTION: Creates Keras callbacks to calculate SacreBLEU metrics during training and to upload the trained model to the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/translation.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
>>> from transformers.keras_callbacks import KerasMetricCallback

>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)
>>> from transformers.keras_callbacks import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="my_awesome_opus_books_model",
...     tokenizer=tokenizer,
... )

>>> callbacks = [metric_callback, push_to_hub_callback]
```

----------------------------------------

TITLE: Loading and Encoding WikiText-2 Dataset in Python
DESCRIPTION: This code loads the WikiText-2 dataset and encodes it using the previously loaded tokenizer. It prepares the data for perplexity calculation by joining the text and creating tensor encodings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/perplexity.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from datasets import load_dataset

test = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")
encodings = tokenizer("\n\n".join(test["text"]), return_tensors="pt")
```

----------------------------------------

TITLE: Loading AWQ Quantized Model to GPU
DESCRIPTION: Loading a pre-quantized AWQ model directly to GPU using AutoModelForCausalLM
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/quantization.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "TheBloke/zephyr-7B-alpha-AWQ"
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="cuda:0")
```

----------------------------------------

TITLE: Defining Helper Functions for GPU Utilization Monitoring in Python
DESCRIPTION: These Python functions use the pynvml library to print GPU memory utilization and summarize training results, including runtime and samples per second.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_memory_anatomy.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from pynvml import *


>>> def print_gpu_utilization():
...     nvmlInit()
...     handle = nvmlDeviceGetHandleByIndex(0)
...     info = nvmlDeviceGetMemoryInfo(handle)
...     print(f"GPU memory occupied: {info.used//1024**2} MB.")


>>> def print_summary(result):
...     print(f"Time: {result.metrics['train_runtime']:.2f}")
...     print(f"Samples/second: {result.metrics['train_samples_per_second']:.2f}")
...     print_gpu_utilization()
```

----------------------------------------

TITLE: Exporting Local PyTorch Checkpoint to ONNX
DESCRIPTION: Command to export a locally saved PyTorch checkpoint to ONNX format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/serialization.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
python -m transformers.onnx --model=local-pt-checkpoint onnx/
```

----------------------------------------

TITLE: Configuring FSDP with Accelerate
DESCRIPTION: YAML configuration for Fully Sharded Data Parallel (FSDP) setup, specifying wrapping policies, prefetch strategies, sharding strategy, and mixed precision settings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/trainer.md#2025-04-22_snippet_9

LANGUAGE: yaml
CODE:
```
compute_environment: LOCAL_MACHINE
distributed_type: FSDP
downcast_bf16: 'no'
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch_policy: BACKWARD_PRE
  fsdp_forward_prefetch: true
  fsdp_offload_params: false
  fsdp_sharding_strategy: 1
  fsdp_state_dict_type: FULL_STATE_DICT
  fsdp_sync_module_states: true
  fsdp_transformer_layer_cls_to_wrap: BertLayer
  fsdp_use_orig_params: true
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 2
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

----------------------------------------

TITLE: Configuring 8-bit Quantization with Custom Threshold in Python
DESCRIPTION: This code demonstrates how to set a custom threshold for 8-bit quantization using BitsAndBytesConfig. It's useful for balancing inference speed and accuracy in quantized models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-23_snippet_12

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

quantization_config = BitsAndBytesConfig(
    llm_int8_threshold=0.0,
    llm_int8_enable_fp32_cpu_offload=True
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype="auto",
    device_map=device_map,
    quantization_config=quantization_config,
)
```

----------------------------------------

TITLE: Calculating Similarity Score with Cosine Similarity - Torch, Python
DESCRIPTION: This snippet computes the cosine similarity score between the outputs of two different images using the torch.nn.functional cosine_similarity method. This score indicates how similar the images are based on their embeddings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_feature_extraction.md#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
from torch.nn.functional import cosine_similarity

similarity_score = cosine_similarity(torch.Tensor(outputs[0]),
                                     torch.Tensor(outputs[1]), dim=1)

print(similarity_score)

# tensor([0.6043])
```

----------------------------------------

TITLE: Performing Translation with M2M100 Model in Python
DESCRIPTION: This snippet demonstrates how to perform translation using the M2M100 model. It includes tokenizing the input text and generating the translation with the specified target language.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/multilingual.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
encoded_zh = tokenizer(chinese_text, return_tensors="pt")
generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id("en"))
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
```

----------------------------------------

TITLE: Loading BLIP-2 Model for Zero-shot VQA
DESCRIPTION: Shows how to load and prepare the BLIP-2 model and processor for zero-shot visual question answering.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/visual_question_answering.md#2025-04-22_snippet_21

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor, Blip2ForConditionalGeneration
>>> import torch

>>> processor = AutoProcessor.from_pretrained("Salesforce/blip2-opt-2.7b")
>>> model = Blip2ForConditionalGeneration.from_pretrained("Salesforce/blip2-opt-2.7b", torch_dtype=torch.float16)
>>> device = "cuda" if torch.cuda.is_available() else "cpu"
>>> model.to(device)
```

----------------------------------------

TITLE: Converting TensorFlow Model to PyTorch
DESCRIPTION: Shows how to convert a TensorFlow checkpoint to a PyTorch model using the from_tf parameter.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_sharing.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
pt_model = DistilBertForSequenceClassification.from_pretrained("path/to/awesome-name-you-picked", from_tf=True)
pt_model.save_pretrained("path/to/awesome-name-you-picked")
```

----------------------------------------

TITLE: Initializing Semantic Segmentation Model with PyTorch
DESCRIPTION: This snippet demonstrates how to load a pre-trained SegFormer model for semantic segmentation using the Hugging Face Transformers library in PyTorch. It loads the model with a specified checkpoint and label mappings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer

>>> model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)
```

----------------------------------------

TITLE: Initializing VisionEncoderDecoderModel from Configurations
DESCRIPTION: Demonstrates how to initialize a VisionEncoderDecoderModel from scratch using ViT encoder and BERT decoder configurations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/vision-encoder-decoder.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel

config_encoder = ViTConfig()
config_decoder = BertConfig()

config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)
model = VisionEncoderDecoderModel(config=config)
```

----------------------------------------

TITLE: Inspecting Device Mapping
DESCRIPTION: This code shows how to inspect the device mapping of a model after loading it with `device_map="auto"`.  The `hf_device_map` attribute provides a dictionary that maps each layer of the model to its assigned device.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/model.md#_snippet_2

LANGUAGE: python
CODE:
```
t0pp.hf_device_map
{'shared': 0,
 'decoder.embed_tokens': 0,
 'encoder': 0,
 'decoder.block.0': 0,
 'decoder.block.1': 1,
 'decoder.block.2': 1,
 'decoder.block.3': 1,
 'decoder.block.4': 1,
 'decoder.block.5': 1,
 'decoder.block.6': 1,
 'decoder.block.7': 1,
 'decoder.block.8': 1,
 'decoder.block.9': 1,
 'decoder.block.10': 1,
 'decoder.block.11': 1,
 'decoder.block.12': 1,
 'decoder.block.13': 1,
 'decoder.block.14': 1,
 'decoder.block.15': 1,
 'decoder.block.16': 1,
 'decoder.block.17': 1,
 'decoder.block.18': 1,
 'decoder.block.19': 1,
 'decoder.block.20': 1,
 'decoder.block.21': 1,
 'decoder.block.22': 'cpu',
 'decoder.block.23': 'cpu',
 'decoder.final_layer_norm': 'cpu',
 'decoder.dropout': 'cpu',
 'lm_head': 'cpu'}
```

----------------------------------------

TITLE: Exporting a Locally Saved TensorFlow Model to ONNX
DESCRIPTION: Command to export a locally saved TensorFlow model checkpoint to ONNX format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/serialization.md#2025-04-22_snippet_9

LANGUAGE: bash
CODE:
```
python -m transformers.onnx --model=local-tf-checkpoint onnx/
```

----------------------------------------

TITLE: Image Captioning with IDEFICS in Python
DESCRIPTION: This snippet demonstrates how to use the IDEFICS model to caption an image by inputting it through a processor without requiring a text prompt. It illustrates setting up processor inputs, generating model outputs, and printing the resultant caption.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md#2025-04-22_snippet_3

LANGUAGE: py
CODE:
```
>>> prompt = [
...     "https://images.unsplash.com/photo-1583160247711-2191776b4b91?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3542&q=80",
... ]

>>> inputs = processor(prompt, return_tensors="pt").to("cuda")
>>> bad_words_ids = processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> print(generated_text[0])
A puppy in a flower bed
```

----------------------------------------

TITLE: Initializing DepthPro Model with Configuration Overrides
DESCRIPTION: This snippet demonstrates how to initialize the DepthPro model where the FOV encoder can be enabled or disabled through a configuration object. It allows the instantiation of the model while customizing initialization options. Required dependencies include 'transformers'.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/depth_pro.md#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
>>> from transformers import DepthProConfig, DepthProForDepthEstimation
>>> config = DepthProConfig()
>>> model = DepthProForDepthEstimation(config, use_fov_model=True)
```

----------------------------------------

TITLE: Installing Transformers from Source
DESCRIPTION: Commands to clone and install Transformers library from source code, which is required for running the latest version of example scripts.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/run_scripts.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
git clone https://github.com/huggingface/transformers
cd transformers
pip install .
```

----------------------------------------

TITLE: Configuring DeepSpeed with Config File
DESCRIPTION: YAML configuration for DeepSpeed using a reference to an external JSON config file, enabling ZeRO-3 initialization and specifying distribution settings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/trainer.md#2025-04-22_snippet_10

LANGUAGE: yaml
CODE:
```
compute_environment: LOCAL_MACHINE
deepspeed_config:
  deepspeed_config_file: /home/user/configs/ds_zero3_config.json
  zero3_init_flag: true
distributed_type: DEEPSPEED
downcast_bf16: 'no'
machine_rank: 0
main_training_function: main
num_machines: 1
num_processes: 4
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

----------------------------------------

TITLE: Preparing TensorFlow Datasets for Training and Validation
DESCRIPTION: This code prepares the training and validation datasets in `tf.data.Dataset` format using `prepare_tf_dataset`. It configures shuffling, batch size, and the data collator for the datasets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
>>> tf_train_set = model.prepare_tf_dataset(
...     tokenized_wnut["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_validation_set = model.prepare_tf_dataset(
...     tokenized_wnut["validation"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

----------------------------------------

TITLE: Using transformers CLI for Fill-Mask with Longformer (Bash)
DESCRIPTION: Illustrates how to perform the "fill-mask" task using the `transformers run` command-line tool. It pipes the masked text into the command, specifies the Longformer model and device, and executes the inference task directly from the terminal.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/longformer.md#_snippet_2

LANGUAGE: bash
CODE:
```
echo -e "San Francisco 49ers cornerback Shawntae Spencer will miss the rest of the <mask> with a torn ligament in his left knee." | transformers run --task fill-mask --model allenai/longformer-base-4096 --device 0
```

----------------------------------------

TITLE: Resampling Audio to Match Model Requirements
DESCRIPTION: Uses the cast_column method to resample the audio to 16kHz, which is the sampling rate expected by the Wav2Vec2 model. This is necessary to match the model's pretraining conditions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/preprocessing.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> lj_speech = lj_speech.cast_column("audio", Audio(sampling_rate=16_000))
```

----------------------------------------

TITLE: Loading Wav2Vec2 Processor
DESCRIPTION: This snippet loads the Wav2Vec2 processor which is necessary for processing audio input in a format suitable for the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/asr.md#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base")
```

----------------------------------------

TITLE: Exporting a Hub Model to ONNX using Optimum CLI
DESCRIPTION: Command to export a pre-trained DistilBERT question-answering model from the Hugging Face Hub to ONNX format using the Optimum CLI.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/serialization.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
optimum-cli export onnx --model distilbert/distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/
```

----------------------------------------

TITLE: Generating Image Caption with Fine-tuned Model
DESCRIPTION: Uses the fine-tuned model to generate a caption for the test Pokemon image and decodes the output tokens.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_captioning.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
generated_ids = model.generate(pixel_values=pixel_values, max_length=50)
generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(generated_caption)
```

----------------------------------------

TITLE: Compiling TensorFlow Model with XLA
DESCRIPTION: This snippet shows how to enable XLA compilation in a TensorFlow Keras model by setting the jit_compile parameter to True during compilation. This optimization can significantly improve model runtime performance without modifying the model structure.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tf_xla.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
model.compile(jit_compile=True)
```

----------------------------------------

TITLE: Logging into Hugging Face Hub via CLI in Bash
DESCRIPTION: Shows the command to log into the Hugging Face Hub using the command-line interface.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/custom_models.md#2025-04-22_snippet_11

LANGUAGE: bash
CODE:
```
huggingface-cli login
```

----------------------------------------

TITLE: Creating DistilBERT Model with Custom Configuration in TensorFlow
DESCRIPTION: This snippet shows how to create a DistilBERT model using a custom configuration in TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/create_a_model.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from transformers import TFDistilBertModel

my_config = DistilBertConfig.from_pretrained("./your_model_save_path/my_config.json")
tf_model = TFDistilBertModel(my_config)
```

----------------------------------------

TITLE: Preparing Inputs for Image and Text Queries
DESCRIPTION: This snippet prepares inputs for the OWL-ViT model by combining an image and the corresponding text queries. It demonstrates how to utilize the processor to format the inputs correctly.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> text_queries = ["hat", "book", "sunglasses", "camera"]
>>> inputs = processor(text=text_queries, images=im, return_tensors="pt")
```

----------------------------------------

TITLE: Setting Number of GPUs with torchrun
DESCRIPTION: This snippet demonstrates how to select the number of GPUs to use with the torchrun command. The parameter --nproc_per_node specifies the number of processes (GPUs) to utilize during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/gpu_selection.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
torchrun --nproc_per_node=2  trainer-program.py ...
```

----------------------------------------

TITLE: Configuring Optimizer for TensorFlow Training
DESCRIPTION: Sets up the AdamWeightDecay optimizer with appropriate learning rate and weight decay for fine-tuning a T5 translation model in TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/translation.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> from transformers import AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

----------------------------------------

TITLE: Extracting Unpooled Hidden States from Images
DESCRIPTION: Code to initialize a pipeline without pooling to get the last hidden state, which can be useful for training new classifiers based on the model's features.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_feature_extraction.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
pipe = pipeline(task="image-feature-extraction", model_name="google/vit-base-patch16-224", device=DEVICE)
output = pipe(image_real)
```

----------------------------------------

TITLE: Estimating Memory Requirements for DeepSpeed ZeRO with a Single GPU
DESCRIPTION: This code snippet demonstrates how to estimate memory requirements for fine-tuning a T0_3B model using DeepSpeed ZeRO with a single GPU. It shows various offloading options and their corresponding memory needs for both CPU and GPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/deepspeed.md#2025-04-22_snippet_38

LANGUAGE: bash
CODE:
```
$ python -c 'from transformers import AutoModel; \
from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \
model = AutoModel.from_pretrained("bigscience/T0_3B"); \
estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=1, num_nodes=1)'
[...]
Estimated memory needed for params, optim states and gradients for a:
HW: Setup with 1 node, 1 GPU per node.
SW: Model with 2783M total params, 65M largest layer params.
  per CPU  |  per GPU |   Options
   70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1
   70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0
   62.23GB |   5.43GB | offload_param=none, offload_optimizer=cpu , zero_init=1
   62.23GB |   5.43GB | offload_param=none, offload_optimizer=cpu , zero_init=0
    0.37GB |  46.91GB | offload_param=none, offload_optimizer=none, zero_init=1
   15.56GB |  46.91GB | offload_param=none, offload_optimizer=none, zero_init=0
```

----------------------------------------

TITLE: Installing TensorFlow CPU-only Version of Transformers in Bash
DESCRIPTION: These commands install the CPU-only version of the Transformers library adapted for TensorFlow using pip and uv, useful for non-GPU setups.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md#2025-04-22_snippet_7

LANGUAGE: bash
CODE:
```
pip install 'transformers[tf-cpu]'
uv pip install 'transformers[tf-cpu]'
```

----------------------------------------

TITLE: Running Code on CPU
DESCRIPTION: This snippet shows how to force code execution on the CPU by setting the `CUDA_VISIBLE_DEVICES` environment variable to an empty string. This is useful for debugging CUDA errors and obtaining more detailed error messages.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/troubleshooting.md#_snippet_3

LANGUAGE: Python
CODE:
```
>>> import os

>>> os.environ["CUDA_VISIBLE_DEVICES"] = ""
```

----------------------------------------

TITLE: Setting Up NLP Environment for Hugging Face Transformers in Python
DESCRIPTION: This snippet shows the command to install the required packages to use Hugging Face Transformers and Accelerate libraries for NLP tasks. Ensuring the proper environment setup is crucial before running your models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/prompting.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
pip install -q transformers accelerate
```

----------------------------------------

TITLE: Creating DatasetDict for Custom Semantic Segmentation Data in Python
DESCRIPTION: This snippet demonstrates how to create a DatasetDict object with 'image' and 'label' columns for custom semantic segmentation data, and push it to the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/semantic-segmentation/README.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from datasets import Dataset, DatasetDict, Image

# your images can of course have a different extension
# semantic segmentation maps are typically stored in the png format
image_paths_train = ["path/to/image_1.jpg/jpg", "path/to/image_2.jpg/jpg", ..., "path/to/image_n.jpg/jpg"]
label_paths_train = ["path/to/annotation_1.png", "path/to/annotation_2.png", ..., "path/to/annotation_n.png"]

# same for validation
# image_paths_validation = [...]
# label_paths_validation = [...]

def create_dataset(image_paths, label_paths):
    dataset = Dataset.from_dict({"image": sorted(image_paths),
                                "label": sorted(label_paths)})
    dataset = dataset.cast_column("image", Image())
    dataset = dataset.cast_column("label", Image())

    return dataset

# step 1: create Dataset objects
train_dataset = create_dataset(image_paths_train, label_paths_train)
validation_dataset = create_dataset(image_paths_validation, label_paths_validation)

# step 2: create DatasetDict
dataset = DatasetDict({
    "train": train_dataset,
    "validation": validation_dataset,
  }
)

# step 3: push to hub (assumes you have ran the huggingface-cli login command in a terminal/notebook)
dataset.push_to_hub("name of repo on the hub")

# optionally, you can push to a private repo on the hub
# dataset.push_to_hub("name of repo on the hub", private=True)
```

----------------------------------------

TITLE: Loading an Audio Dataset for Speech Recognition
DESCRIPTION: Code to load the MInDS-14 Italian dataset using the Datasets library for speech recognition tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/quicktour.md#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="it-IT", split="train")  # doctest: +IGNORE_RESULT
```

----------------------------------------

TITLE: Loading timm Backbone Config Separately in Python
DESCRIPTION: This snippet demonstrates how to load a timm backbone configuration separately, create a TimmBackbone instance and then create MaskFormer model with it. It initializes the TimmBackboneConfig setting use_pretrained_backbone=False to initialize with random weights. 
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/create_a_model.md#_snippet_25

LANGUAGE: python
CODE:
```
from transformers import TimmBackboneConfig, TimmBackbone

backbone_config = TimmBackboneConfig("resnet50", use_pretrained_backbone=False)

# Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ù…Ù† Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„ÙÙ‚Ø±ÙŠ
backbone = TimmBackbone(config=backbone_config)

# Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ù…ÙˆØ¯ ÙÙ‚Ø±ÙŠ timm
from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation

config = MaskFormerConfig(backbone_config=backbone_config)
model = MaskFormerForInstanceSegmentation(config)
```

----------------------------------------

TITLE: Defining Evaluation Metrics Using Word Error Rate
DESCRIPTION: Creates a function to compute Word Error Rate (WER) between predicted captions and ground truth captions for model evaluation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_captioning.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from evaluate import load
import torch

wer = load("wer")


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predicted = logits.argmax(-1)
    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)
    decoded_predictions = processor.batch_decode(predicted, skip_special_tokens=True)
    wer_score = wer.compute(predictions=decoded_predictions, references=decoded_labels)
    return {"wer_score": wer_score}
```

----------------------------------------

TITLE: Loading Image Processor for TensorFlow Inference
DESCRIPTION: This snippet demonstrates how to load an image processor in TensorFlow, which pre-processes the input image into a suitable format for the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/sequence_classification.md#2025-04-22_snippet_29

LANGUAGE: python
CODE:
```
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("MariaK/scene_segmentation")
>>> inputs = image_processor(image, return_tensors="tf")
```

----------------------------------------

TITLE: Loading a sharded checkpoint directly into a model in Python
DESCRIPTION: Shows how to load a sharded checkpoint directly into a model instance using the load_sharded_checkpoint function from Transformers.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/big_models.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers.modeling_utils import load_sharded_checkpoint

>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir, max_shard_size="200MB")
...     load_sharded_checkpoint(model, tmp_dir)
```

----------------------------------------

TITLE: Flushing GPU Memory Before Next Experiment
DESCRIPTION: This Python snippet calls the previously defined 'flush' function to free memory before starting the next memory measurement experiment. This ensures that the GPU memory statistics are reset and accurate for the current test.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
flush()
```

----------------------------------------

TITLE: Visualizing Gemma Attention Mask (Python)
DESCRIPTION: Shows how to instantiate and use the AttentionMaskVisualizer utility from transformers.utils to visualize the attention mask for a specific model like Gemma. It takes a model identifier and an input text to display how the model attends to tokens. Requires the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gemma.md#_snippet_4

LANGUAGE: python
CODE:
```
from transformers.utils.attention_visualizer import AttentionMaskVisualizer

visualizer = AttentionMaskVisualizer("google/gemma-2b")
visualizer("LLMs generate text through a process known as")
```

----------------------------------------

TITLE: Launch Summarization Script with Accelerate
DESCRIPTION: Launches the `run_summarization_no_trainer.py` script using Hugging Face Accelerate for distributed training.  It specifies the model name, dataset, source prefix, and output directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/run_scripts.md#_snippet_11

LANGUAGE: bash
CODE:
```
accelerate launch run_summarization_no_trainer.py \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir ~/tmp/tst-summarization
```

----------------------------------------

TITLE: Fine-tuning BERT on CoNLL-2003 Dataset (Bash)
DESCRIPTION: Command to fine-tune a BERT model on the CoNLL-2003 dataset for Named Entity Recognition using the Trainer API.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/token-classification/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
python run_ner.py \
  --model_name_or_path google-bert/bert-base-uncased \
  --dataset_name conll2003 \
  --output_dir /tmp/test-ner \
  --do_train \
  --do_eval
```

----------------------------------------

TITLE: Save Converted Model
DESCRIPTION: Code to save the converted model with its configuration to a local directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
model.save_pretrained("/path/to/converted/checkpoint/folder")
```

----------------------------------------

TITLE: Measuring Peak GPU Memory Allocation
DESCRIPTION: This code measures the peak GPU memory allocation using PyTorch's CUDA utilities and converts it to gigabytes using the previously defined function.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/llm_tutorial_optimization.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
bytes_to_giga_bytes(torch.cuda.max_memory_allocated())
```

----------------------------------------

TITLE: Instantiate DETR with Random Transformer Weights (Python)
DESCRIPTION: This snippet shows how to instantiate DETR with randomly initialized weights for the Transformer component while retaining pre-trained weights for the backbone. It imports DetrConfig and DetrForObjectDetection, creates a default configuration, and initializes the model using that config. This approach allows for fine-tuning the Transformer part while leveraging the backbone's pre-trained features.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/detr.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import DetrConfig, DetrForObjectDetection

>>> config = DetrConfig()
>>> model = DetrForObjectDetection(config)
```

----------------------------------------

TITLE: Post-processing Super-Resolution Output
DESCRIPTION: Post-processes the model output to convert it back into a viewable image format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/image_to_image.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
import numpy as np

# squeeze, take to CPU and clip the values
output = outputs.reconstruction.data.squeeze().cpu().clamp_(0, 1).numpy()
# rearrange the axes
output = np.moveaxis(output, source=0, destination=-1)
# bring values back to pixel values range
output = (output * 255.0).round().astype(np.uint8)
Image.fromarray(output)
```

----------------------------------------

TITLE: Installing Git LFS and Configuring Git
DESCRIPTION: This snippet shows how to install Git Large File Storage (LFS) and set up basic Git configuration for user email and name.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/image-pretraining/README.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
$ apt install git-lfs
$ git config --global user.email "you@example.com"
$ git config --global user.name "Your Name"
```

----------------------------------------

TITLE: Creating Python Virtual Environment for Transformers
DESCRIPTION: Commands to create a Python virtual environment, activate it, and install the development dependencies for the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/templates/adding_a_new_model/open_model_proposals/ADD_BIG_BIRD.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
python -m venv .env
source .env/bin/activate
pip install -e ".[dev]"
```

----------------------------------------

TITLE: Loading DocVQA Dataset from Hugging Face Hub
DESCRIPTION: This code loads the "nielsr/docvqa_1200_examples" dataset from the Hugging Face Hub using the `load_dataset` function from the `datasets` library. The loaded dataset is a `DatasetDict` containing training and testing splits.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/document_question_answering.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> dataset = load_dataset("nielsr/docvqa_1200_examples")
>>> dataset
DatasetDict({
    train: Dataset({
        features: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],
        num_rows: 1000
    })
    test: Dataset({
        features: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],
        num_rows: 200
    })
})
```

----------------------------------------

TITLE: Compiling TensorFlow Model for Training
DESCRIPTION: This code compiles the TensorFlow model for training using `model.compile`. It specifies the optimizer and notes that a loss function is not needed because Transformers models have default task-related loss functions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_21

LANGUAGE: python
CODE:
```
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)  # No loss argument!
```

----------------------------------------

TITLE: PyTorch TAPAS Inference Example
DESCRIPTION: Demonstrates how to perform inference with a trained TAPAS model in PyTorch, including tokenization and converting predictions to readable answers.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/tapas.md#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
from transformers import TapasTokenizer, TapasForQuestionAnswering
import pandas as pd

model_name = "google/tapas-base-finetuned-wtq"
model = TapasForQuestionAnswering.from_pretrained(model_name)
tokenizer = TapasTokenizer.from_pretrained(model_name)

data = {"Actors": ["Brad Pitt", "Leonardo Di Caprio", "George Clooney"], "Number of movies": ["87", "53", "69"]}
queries = [
    "What is the name of the first actor?",
    "How many movies has George Clooney played in?",
    "What is the total number of movies?",
]
table = pd.DataFrame.from_dict(data)
inputs = tokenizer(table=table, queries=queries, padding="max_length", return_tensors="pt")
outputs = model(**inputs)
predicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(
    inputs, outputs.logits.detach(), outputs.logits_aggregation.detach()
)
```

----------------------------------------

TITLE: XLNet Tokenizer Implementation Example
DESCRIPTION: Example showing how to use XLNetTokenizer from the transformers library to tokenize text using SentencePiece tokenization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tokenizer_summary.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import XLNetTokenizer

>>> tokenizer = XLNetTokenizer.from_pretrained("xlnet/xlnet-base-cased")
>>> tokenizer.tokenize("Don't you love ðŸ¤— Transformers? We sure do.")
["â–Don", "'", "t", "â–you", "â–love", "â–", "ðŸ¤—", "â–", "Transform", "ers", "?", "â–We", "â–sure", "â–do", "."]
```

----------------------------------------

TITLE: Zero-shot Image Classification with AutoModel in PyTorch
DESCRIPTION: This code snippet demonstrates how to perform zero-shot image classification using the `AutoModel` class in Transformers. It loads a CLIP model and processor, downloads an image from a URL, defines candidate labels, preprocesses the inputs, and then uses the model to classify the image. The code then calculates the probabilities and determines the most likely label, printing both the label and its probability, while using `attn_implementation="sdpa"` for potentially faster and more memory efficient attention.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/clip.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import requests
import torch
from PIL import Image
from transformers import AutoProcessor, AutoModel

model = AutoModel.from_pretrained("openai/clip-vit-base-patch32", torch_dtype=torch.bfloat16, attn_implementation="sdpa")
processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
labels = ["a photo of a cat", "a photo of a dog", "a photo of a car"]

inputs = processor(text=labels, images=image, return_tensors="pt", padding=True)

outputs = model(**inputs)
logits_per_image = outputs.logits_per_image
probs = logits_per_image.softmax(dim=1)
most_likely_idx = probs.argmax(dim=1).item()
most_likely_label = labels[most_likely_idx]
print(f"Most likely label: {most_likely_label} with probability: {probs[0][most_likely_idx].item():.3f}")
```

----------------------------------------

TITLE: Creating Preprocessing Function for QA Data
DESCRIPTION: Defines a function to preprocess examples for question answering by tokenizing questions and contexts, handling truncation for long contexts, and mapping answer positions to token indices. The function returns inputs ready for model training with start and end positions of answers.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/question_answering.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> def preprocess_function(examples):
...     questions = [q.strip() for q in examples["question"]]
...     inputs = tokenizer(
...         questions,
...         examples["context"],
...         max_length=384,
...         truncation="only_second",
...         return_offsets_mapping=True,
...         padding="max_length",
...     )

...     offset_mapping = inputs.pop("offset_mapping")
...     answers = examples["answers"]
...     start_positions = []
...     end_positions = []

...     for i, offset in enumerate(offset_mapping):
...         answer = answers[i]
...         start_char = answer["answer_start"][0]
...         end_char = answer["answer_start"][0] + len(answer["text"][0])
...         sequence_ids = inputs.sequence_ids(i)

...         # Find the start and end of the context
...         idx = 0
...         while sequence_ids[idx] != 1:
...             idx += 1
...         context_start = idx
...         while sequence_ids[idx] == 1:
...             idx += 1
...         context_end = idx - 1

...         # If the answer is not fully inside the context, label it (0, 0)
...         if offset[context_start][0] > end_char or offset[context_end][1] < start_char:
...             start_positions.append(0)
...             end_positions.append(0)
...         else:
...             # Otherwise it's the start and end token positions
...             idx = context_start
...             while idx <= context_end and offset[idx][0] <= start_char:
...                 idx += 1
...             start_positions.append(idx - 1)

...             idx = context_end
...             while idx >= context_start and offset[idx][1] >= end_char:
...                 idx -= 1
...             end_positions.append(idx + 1)

...     inputs["start_positions"] = start_positions
...     inputs["end_positions"] = end_positions
...     return inputs
```

----------------------------------------

TITLE: Post-processing Batch Object Detection
DESCRIPTION: This code shows how to post-process the results of batch object detection. It iterates over the results for a specific image index and draws the bounding boxes and labels on the corresponding image. The target sizes are provided as a list of tuples, one for each image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/zero_shot_object_detection.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
">>> with torch.no_grad():
...     outputs = model(**inputs)
...     target_sizes = [x.size[::-1] for x in images]
...     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)

>>> image_idx = 1
>>> draw = ImageDraw.Draw(images[image_idx])

>>> scores = results[image_idx]["scores"].tolist()
>>> labels = results[image_idx]["labels"].tolist()
>>> boxes = results[image_idx]["boxes"].tolist()

>>> for box, score, label in zip(boxes, scores, labels):
...     xmin, ymin, xmax, ymax = box
...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)
...     draw.text((xmin, ymin), f"{text_queries[image_idx][label]}: {round(score,2)}", fill=\"white\")

>>> images[image_idx]"
```

----------------------------------------

TITLE: Converting Tokenized Datasets to TensorFlow Datasets
DESCRIPTION: Converts the tokenized datasets to TensorFlow datasets using the to_tf_dataset method, specifying input columns, label columns, and batch size for training and validation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/training.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
tf_train_dataset = small_train_dataset.to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = small_eval_dataset.to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

----------------------------------------

TITLE: Running the Web Server with Uvicorn
DESCRIPTION: This bash command starts the web server created in the previous Python snippet using the Uvicorn ASGI server. It specifies the `server.py` file and the `app` object as the entry point.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/pipeline_webserver.md#_snippet_1

LANGUAGE: bash
CODE:
```
uvicorn server:app
```

----------------------------------------

TITLE: Saving a Trained Tokenizer in Python
DESCRIPTION: This code snippet illustrates how to save the configuration and vocabulary of a trained tokenizer to a JSON file, which can later be loaded for reuse.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/fast_tokenizers.md#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
tokenizer.save("tokenizer.json")
```

----------------------------------------

TITLE: Pushing an 8-bit quantized model to the Hugging Face Hub
DESCRIPTION: Code to quantize a model to 8-bit and push it to the Hugging Face Hub, which requires recent versions of Transformers and bitsandbytes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/bitsandbytes.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_8bit=True)

model = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-560m", 
    device_map="auto",
    quantization_config=quantization_config
)

model.push_to_hub("bloom-560m-8bit")
```

----------------------------------------

TITLE: Loading a Tokenizer Object into PreTrainedTokenizerFast in Python
DESCRIPTION: This code shows how to load a tokenizer object from ðŸ¤— Tokenizers into ðŸ¤— Transformers using the PreTrainedTokenizerFast class. This allows the tokenizer to be used with all shared methods in ðŸ¤— Transformers.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/fast_tokenizers.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import PreTrainedTokenizerFast

>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)
```

----------------------------------------

TITLE: Configuring Accelerate
DESCRIPTION: This command creates and saves a configuration file that specifies settings for distributed training. This can be used to specify parameters such as the number of GPUs, the backend to use, and other training-related options.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/accelerate.md#_snippet_5

LANGUAGE: bash
CODE:
```
accelerate config
```

----------------------------------------

TITLE: Preparing TensorFlow Datasets for BERT Fine-tuning
DESCRIPTION: Converts the preprocessed datasets to TensorFlow format for use with Keras API.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/multiple_choice.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)
>>> tf_train_set = model.prepare_tf_dataset(
...     tokenized_swag["train"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )

>>> tf_validation_set = model.prepare_tf_dataset(
...     tokenized_swag["validation"],
...     shuffle=False,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )
```

----------------------------------------

TITLE: Loading and Processing SQuAD Data Example
DESCRIPTION: Example showing how to use SQuAD processors to load development examples and convert them to features using both V1 and V2 processors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/processors.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
# Loading a V2 processor
processor = SquadV2Processor()
examples = processor.get_dev_examples(squad_v2_data_dir)

# Loading a V1 processor
processor = SquadV1Processor()
examples = processor.get_dev_examples(squad_v1_data_dir)

features = squad_convert_examples_to_features(
    examples=examples,
    tokenizer=tokenizer,
    max_seq_length=max_seq_length,
    doc_stride=args.doc_stride,
    max_query_length=max_query_length,
    is_training=not evaluate,
)
```

----------------------------------------

TITLE: Loading LayoutLMv2Processor
DESCRIPTION: This code loads the LayoutLMv2Processor from the specified `model_checkpoint`. The processor combines an image processor and a tokenizer, handling both image and text data preprocessing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/document_question_answering.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained(model_checkpoint)
```

----------------------------------------

TITLE: Non-Trainer DeepSpeed Integration with Models from Configuration
DESCRIPTION: This code shows how to integrate DeepSpeed with Transformers models created from configuration (not pre-trained) without using the Trainer class. This approach is needed for ZeRO-3 parameter gathering.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/deepspeed.md#2025-04-22_snippet_42

LANGUAGE: python
CODE:
```
from transformers.integrations import HfDeepSpeedConfig
from transformers import AutoModel, AutoConfig
import deepspeed

ds_config = {...}  # deepspeed config object or path to the file
# must run before instantiating the model to detect zero 3
dschf = HfDeepSpeedConfig(ds_config)  # keep this object alive
config = AutoConfig.from_pretrained("openai-community/gpt2")
model = AutoModel.from_config(config)
engine = deepspeed.initialize(model=model, config_params=ds_config, ...)
```

----------------------------------------

TITLE: Performing OCR with TrOCR in Python
DESCRIPTION: This snippet demonstrates how to use the TrOCR model to perform optical character recognition on an image. It shows loading the model and processor, preprocessing the image, generating text, and decoding the output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/trocr.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
import requests
from PIL import Image

processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten")
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten")

# load image from the IAM dataset
url = "https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg"
image = Image.open(requests.get(url, stream=True).raw).convert("RGB")

pixel_values = processor(image, return_tensors="pt").pixel_values
generated_ids = model.generate(pixel_values)

generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

----------------------------------------

TITLE: Initializing GPTQConfig with a Dataset
DESCRIPTION: This code snippet demonstrates how to initialize a `GPTQConfig` object for quantization. It sets the number of bits to quantize to (4), specifies the 'c4' dataset for calibration, and uses a tokenizer to prepare the dataset. It depends on the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/gptq.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
"from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\ngptq_config = GPTQConfig(bits=4, dataset=\"c4\", tokenizer=tokenizer)"
```

----------------------------------------

TITLE: Pipeline with Generator Input
DESCRIPTION: Demonstrates using a generator function as input source for pipeline processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/pipelines.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import pipeline

pipe = pipeline("text-classification")

def data():
    while True:
        yield "This is a test"

for out in pipe(data()):
    print(out)
```

----------------------------------------

TITLE: Creating smaller subset of the dataset
DESCRIPTION: This snippet shows how to create a smaller subset of the tokenized dataset for faster fine-tuning. It shuffles the dataset and selects the first 1000 examples for both the training and evaluation sets.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/training.md#_snippet_2

LANGUAGE: Python
CODE:
```
>>> small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
>>> small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
```

----------------------------------------

TITLE: Evaluation Metric Setup
DESCRIPTION: Setting up accuracy metric calculation for model evaluation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/audio_classification.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> import evaluate
>>> import numpy as np

>>> accuracy = evaluate.load("accuracy")

>>> def compute_metrics(eval_pred):
...     predictions = np.argmax(eval_pred.predictions, axis=1)
...     return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)
```

----------------------------------------

TITLE: Running Translation Example in Offline Mode
DESCRIPTION: This command runs the same translation example script, but in offline mode. It sets the HF_DATASETS_OFFLINE and HF_HUB_OFFLINE environment variables to 1.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/installation.md#2025-04-22_snippet_14

LANGUAGE: bash
CODE:
```
"HF_DATASETS_OFFLINE=1 HF_HUB_OFFLINE=1 \\
python examples/pytorch/translation/run_translation.py --model_name_or_path google-t5/t5-small --dataset_name wmt16 --dataset_config ro-en ..."
```

----------------------------------------

TITLE: Using Blenderbot for Conditional Generation in Python
DESCRIPTION: This snippet demonstrates how to load a pre-trained Blenderbot model and tokenizer, generate a response to a given utterance, and decode the output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/blenderbot.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration

>>> mname = "facebook/blenderbot-400M-distill"
>>> model = BlenderbotForConditionalGeneration.from_pretrained(mname)
>>> tokenizer = BlenderbotTokenizer.from_pretrained(mname)
>>> UTTERANCE = "My friends are cool but they eat too many carbs."
>>> inputs = tokenizer([UTTERANCE], return_tensors="pt")
>>> reply_ids = model.generate(**inputs)
>>> print(tokenizer.batch_decode(reply_ids))
["<s> That's unfortunate. Are they trying to lose weight or are they just trying to be healthier?</s>"]
```

----------------------------------------

TITLE: GPU Device Selection Commands
DESCRIPTION: Examples of how to select specific GPUs for training using environment variables and launch commands.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/trainer.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
python -m torch.distributed.launch --nproc_per_node=2  trainer-program.py ...
```

LANGUAGE: bash
CODE:
```
CUDA_VISIBLE_DEVICES=0,2 python -m torch.distributed.launch trainer-program.py ...
```

LANGUAGE: bash
CODE:
```
CUDA_VISIBLE_DEVICES=2,0 python trainer-program.py ...
```

----------------------------------------

TITLE: Loading the Beans Dataset for Image Classification
DESCRIPTION: Loads the 'beans' dataset from the Hugging Face datasets library which will be used for fine-tuning and evaluating the knowledge distillation process.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/knowledge_distillation_for_image_classification.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from datasets import load_dataset

dataset = load_dataset("beans")
```

----------------------------------------

TITLE: Creating a PushToHubCallback for TensorFlow
DESCRIPTION: Shows how to create a PushToHubCallback for TensorFlow models to automatically push models to Hub during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/model_sharing.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
>>> from transformers import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="./il_path_dove_salvare_il_tuo_modello",
...     tokenizer=tokenizer,
...     hub_model_id="il-tuo-username/il-mio-bellissimo-modello",
... )
```

----------------------------------------

TITLE: Setting Up Push to Hub Callback for TensorFlow
DESCRIPTION: Creates a callback to automatically push the trained model and tokenizer to the Hugging Face Hub after training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/masked_language_modeling.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir="my_awesome_eli5_mlm_model",
    tokenizer=tokenizer,
)
```

----------------------------------------

TITLE: Setting Up PushToHubCallback for TensorFlow
DESCRIPTION: Code to create a PushToHubCallback that will automatically upload the model to the Hugging Face Hub after training is complete.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir="my_awesome_eli5_mlm_model",
    tokenizer=tokenizer,
)
```

----------------------------------------

TITLE: Installing Required Libraries
DESCRIPTION: This snippet demonstrates how to install the necessary libraries, specifically the 'transformers' library, which is essential for using the OWL-ViT model for zero-shot object detection.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install -q transformers
```

----------------------------------------

TITLE: Document Question Answering with Transformers
DESCRIPTION: Demonstrates how to use the document-question-answering pipeline to extract information from document images. This example uses a LayoutLM model specifically fine-tuned for invoice processing to extract the total amount from a document image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/task_summary.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from transformers import pipeline
from PIL import Image
import requests

url = "https://huggingface.co/datasets/hf-internal-testing/example-documents/resolve/main/jpeg_images/2.jpg"
image = Image.open(requests.get(url, stream=True).raw)

doc_question_answerer = pipeline("document-question-answering", model="magorshunov/layoutlm-invoices")
preds = doc_question_answerer(
    question="What is the total amount?",
    image=image,
)
preds
```

----------------------------------------

TITLE: Performing Object Detection with OWL-ViT in Python
DESCRIPTION: This code example demonstrates how to use the OWL-ViT model and processor to perform object detection with images and text queries. It covers loading the model and the processor, preparing inputs, and processing outputs to retrieve detected objects and their coordinates.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/owlvit.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> import requests
>>> from PIL import Image
>>> import torch

>>> from transformers import OwlViTProcessor, OwlViTForObjectDetection

>>> processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
>>> model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch32")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> text_labels = [["a photo of a cat", "a photo of a dog"]]
>>> inputs = processor(text=text_labels, images=image, return_tensors="pt")
>>> outputs = model(**inputs)

>>> # Target image sizes (height, width) to rescale box predictions [batch_size, 2]
>>> target_sizes = torch.tensor([(image.height, image.width)])
>>> # Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)
>>> results = processor.post_process_grounded_object_detection(
...     outputs=outputs, target_sizes=target_sizes, threshold=0.1, text_labels=text_labels
... )
>>> # Retrieve predictions for the first image for the corresponding text queries
>>> result = results[0]
>>> boxes, scores, text_labels = result["boxes"], result["scores"], result["text_labels"]
>>> for box, score, text_label in zip(boxes, scores, text_labels):
...     box = [round(i, 2) for i in box.tolist()]
...     print(f"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}")
Detected a photo of a cat with confidence 0.707 at location [324.97, 20.44, 640.58, 373.29]
Detected a photo of a cat with confidence 0.717 at location [1.46, 55.26, 315.55, 472.17]
```

----------------------------------------

TITLE: Custom Data Collator for CTC
DESCRIPTION: This snippet defines a custom data collator that handles padding during the batching process for the ASR task, allowing for different padding methods for input values and labels.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/asr.md#2025-04-22_snippet_10

LANGUAGE: Python
CODE:
```
>>> import torch

>>> from dataclasses import dataclass, field
>>> from typing import Any, Dict, List, Optional, Union


>>> @dataclass
... class DataCollatorCTCWithPadding:
...     processor: AutoProcessor
...     padding: Union[bool, str] = "longest"

...     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
...         input_features = [{"input_values": feature["input_values"][0]} for feature in features]
...         label_features = [{"input_ids": feature["labels"]} for feature in features]

...         batch = self.processor.pad(input_features, padding=self.padding, return_tensors="pt")

...         labels_batch = self.processor.pad(labels=label_features, padding=self.padding, return_tensors="pt")

...         labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

...         batch["labels"] = labels

...         return batch
```

----------------------------------------

TITLE: Upgrading Transformers to the Latest Version via pip
DESCRIPTION: This command upgrades the installed Transformers library to the latest version using pip. This is often necessary to resolve `ImportError` issues, especially when working with newly released models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/troubleshooting.md#_snippet_2

LANGUAGE: Bash
CODE:
```
pip install transformers --upgrade
```

----------------------------------------

TITLE: Defining Data Collator for Multiple Choice
DESCRIPTION: This snippet imports the `DataCollatorForMultipleChoice` class and instantiates it with the pre-loaded tokenizer.  The data collator is responsible for dynamically padding the input sequences within each batch to the maximum length in that batch, ensuring efficient processing during training. The collator also flattens the inputs, applies padding, and unflattens the result.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/multiple_choice.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
">>> from transformers import DataCollatorForMultipleChoice\n>>> collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)"
```

----------------------------------------

TITLE: Generating Mask Token Predictions with TensorFlow Model
DESCRIPTION: Code to run inference with a TensorFlow masked language model to get logits for the masked token position, which represent prediction probabilities for each token in the vocabulary.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_28

LANGUAGE: python
CODE:
```
from transformers import TFAutoModelForMaskedLM

model = TFAutoModelForMaskedLM.from_pretrained("username/my_awesome_eli5_mlm_model")
logits = model(**inputs).logits
mask_token_logits = logits[0, mask_token_index, :]
```

----------------------------------------

TITLE: Sentiment Analysis with Falcon-7b-instruct in Python
DESCRIPTION: This code shows how to use a prompt to perform sentiment analysis (text classification) with the Falcon-7b-instruct model, classifying movie review text as positive, negative, or neutral.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/prompting.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> torch.manual_seed(0)
>>> prompt = """Classify the text into neutral, negative or positive. 
... Text: This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.
... Sentiment:
... """

>>> sequences = pipe(
...     prompt,
...     max_new_tokens=10,
... )

>>> for seq in sequences:
...     print(f"Result: {seq['generated_text']}")
Result: Classify the text into neutral, negative or positive. 
Text: This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.
Sentiment:
Positive
```

----------------------------------------

TITLE: Using GPT-Sw3 Model for Text Generation in Python
DESCRIPTION: Demonstrates how to load the GPT-Sw3 model and tokenizer from Hugging Face's model hub and generate text based on a Swedish prompt. The example shows how to instantiate the model, tokenize input text, and generate new tokens with sampling.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gpt-sw3.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, AutoModelForCausalLM

>>> tokenizer = AutoTokenizer.from_pretrained("AI-Sweden-Models/gpt-sw3-356m")
>>> model = AutoModelForCausalLM.from_pretrained("AI-Sweden-Models/gpt-sw3-356m")

>>> input_ids = tokenizer("TrÃ¤d Ã¤r fina fÃ¶r att", return_tensors="pt")["input_ids"]

>>> generated_token_ids = model.generate(inputs=input_ids, max_new_tokens=10, do_sample=True)[0]

>>> print(tokenizer.decode(generated_token_ids))
TrÃ¤d Ã¤r fina fÃ¶r att de Ã¤r fÃ¤rgstarka. Men ibland Ã¤r det fint
```

----------------------------------------

TITLE: Pipeline with Generator Input
DESCRIPTION: Demonstrates using a generator to feed data into a pipeline for continuous processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/pipelines.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import pipeline

pipe = pipeline("text-classification")

def data():
    while True:
        yield "This is a test"

for out in pipe(data()):
    print(out)
```

----------------------------------------

TITLE: Training T5 Tokenizer with SentencePieceUnigram
DESCRIPTION: Trains a SentencePieceUnigram tokenizer on Norwegian OSCAR dataset with a vocabulary size of 32,000. Includes dataset loading, batch iteration, and tokenizer training with special tokens.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/README.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
import datasets

from t5_tokenizer_model import SentencePieceUnigramTokenizer


vocab_size = 32_000
input_sentence_size = None

# Initialize a dataset
dataset = datasets.load_dataset("oscar", name="unshuffled_deduplicated_no", split="train")

tokenizer = SentencePieceUnigramTokenizer(unk_token="<unk>", eos_token="</s>", pad_token="<pad>")


# Build an iterator over this dataset
def batch_iterator(input_sentence_size=None):
    if input_sentence_size is None:
        input_sentence_size = len(dataset)
    batch_length = 100
    for i in range(0, input_sentence_size, batch_length):
        yield dataset[i: i + batch_length]["text"]


# Train tokenizer
tokenizer.train_from_iterator(
    iterator=batch_iterator(input_sentence_size=input_sentence_size),
    vocab_size=vocab_size,
    show_progress=True,
)

# Save files to disk
tokenizer.save("./norwegian-t5-base/tokenizer.json")
```

----------------------------------------

TITLE: Training Model
DESCRIPTION: This code snippet initiates the training process using the `fit` method. The `tf_dataset` is passed as input to the `fit` method for training the model on the TPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
"model.fit(tf_dataset)"
```

----------------------------------------

TITLE: Loading a Specific Model Version from Hugging Face Hub in Python
DESCRIPTION: Demonstrates how to load a specific version of a model using the revision parameter to specify a tag, branch, or commit hash.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/model_sharing.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> model = AutoModel.from_pretrained(
...     "julien-c/EsperBERTo-small", revision="4c77982"  # nome di un tag, di un branch, o commit hash
... )
```

----------------------------------------

TITLE: Displaying Tokens from BERT Tokenization
DESCRIPTION: This code shows how the BERT tokenizer splits text into tokens and handles subwords with special prefix markers like '##', illustrating WordPiece tokenization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/glossary.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> tokenized_sequence = tokenizer.tokenize(sequence)
>>> print(tokenized_sequence)
['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']
```

----------------------------------------

TITLE: Verifying Transformers Installation with Sentiment Analysis
DESCRIPTION: Python command to verify the successful installation of Transformers by running a simple sentiment analysis pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/installation.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
```

----------------------------------------

TITLE: Defining a Custom Collate Function for Padding
DESCRIPTION: This code defines a custom collate function, `collate_fn`, for padding batches of images with variable sizes, often required when fine-tuning models like DETR. The function extracts `pixel_values` and `labels` from the batch, pads the `pixel_values` using the `image_processor.pad` method, and returns a dictionary containing the padded `pixel_values`, `pixel_mask`, and `labels`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_25

LANGUAGE: Python
CODE:
```
>>> def collate_fn(batch):
...     pixel_values = [item["pixel_values"] for item in batch]
...     encoding = image_processor.pad(pixel_values, return_tensors="pt")
...     labels = [item["labels"] for item in batch]
...     batch = {}
...     batch["pixel_values"] = encoding["pixel_values"]
...     batch["pixel_mask"] = encoding["pixel_mask"]
...     batch["labels"] = labels
...     return batch
```

----------------------------------------

TITLE: Run Summarization Script with Custom Dataset
DESCRIPTION: Executes the `run_summarization.py` script using a custom dataset. It requires specifying the paths to the training and validation files, along with the column names for the text and summary.  It also sets the model, dataset, source prefix, and output directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/run_scripts.md#_snippet_12

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --train_file path_to_csv_or_jsonlines_file \
    --validation_file path_to_csv_or_jsonlines_file \
    --text_column text_column_name \
    --summary_column summary_column_name \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --overwrite_output_dir \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate
```

----------------------------------------

TITLE: Loading and Quantizing Model with GPTQ
DESCRIPTION: Demonstrates loading a model with GPTQ quantization configuration for reduced precision.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/quantization.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=gptq_config)
```

----------------------------------------

TITLE: Loading ELI5 Dataset Subset
DESCRIPTION: This code loads a subset of the ELI5 dataset's 'train_asks' split, specifically the first 5000 examples. It uses the `load_dataset` function from the `datasets` library to retrieve the data. This allows for faster experimentation before training on the full dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> eli5 = load_dataset("eli5", split="train_asks[:5000]")
```

----------------------------------------

TITLE: PyTorch Manual Inference with Masked Language Model
DESCRIPTION: Performs manual inference with a fine-tuned masked language model in PyTorch by tokenizing text, identifying the mask token position, and generating predictions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/masked_language_modeling.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_eli5_mlm_model")
inputs = tokenizer(text, return_tensors="pt")
mask_token_index = torch.where(inputs["input_ids"] == tokenizer.mask_token_id)[1]
```

----------------------------------------

TITLE: Downloading Model Files Using huggingface_hub
DESCRIPTION: Python code showing how to use the huggingface_hub library to download specific model files for offline use.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/installation.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import hf_hub_download

>>> hf_hub_download(repo_id="bigscience/T0_3B", filename="config.json", cache_dir="./your/path/bigscience_t0")
```

----------------------------------------

TITLE: Loading Sharded Checkpoint with load_sharded_checkpoint
DESCRIPTION: This snippet demonstrates how to load a sharded checkpoint directly into a model using the `load_sharded_checkpoint` function. This is an alternative to using `from_pretrained` and allows for more control over the loading process.  It requires importing the function from `transformers.modeling_utils`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/big_models.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers.modeling_utils import load_sharded_checkpoint

>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir, max_shard_size="200MB")
...     load_sharded_checkpoint(model, tmp_dir)
```

----------------------------------------

TITLE: Tokenizing Chat Contents with Apply Chat Template
DESCRIPTION: In this snippet, processing messages using ProcessorMixin is outlined to prepare chat content for generation tasks. The focus is on tokenizing inputs and generating a dictionary output suitable for further processing in models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating_multimodal.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
processed_chat = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors="pt")
print(processed_chat.keys())
```

----------------------------------------

TITLE: Instantiating Sentence Fusion Model for Sequence Generation in Python
DESCRIPTION: This code snippet demonstrates how to instantiate a pre-trained EncoderDecoderModel for sentence fusion using the roberta2roberta_L-24_discofuse model. It includes steps to load the model and tokenizer, tokenize input text, and perform text generation. The snippet exemplifies generating output from fused sentences with the transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bert-generation.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> sentence_fuser = EncoderDecoderModel.from_pretrained("google/roberta2roberta_L-24_discofuse")
>>> tokenizer = AutoTokenizer.from_pretrained("google/roberta2roberta_L-24_discofuse")

>>> input_ids = tokenizer(
...     "This is the first sentence. This is the second sentence.", add_special_tokens=False, return_tensors="pt"
... ).input_ids

>>> outputs = sentence_fuser.generate(input_ids)

>>> print(tokenizer.decode(outputs[0]))
```

----------------------------------------

TITLE: UMT5 Model Usage with Transformers
DESCRIPTION: This snippet demonstrates how to load a pre-trained UMT5 model and tokenizer from the transformers library and use it to generate text. It loads the "google/umt5-small" model and tokenizer, tokenizes an input string, and generates output using the model. The generated output is then decoded using the tokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/umt5.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

>>> model = AutoModelForSeq2SeqLM.from_pretrained("google/umt5-small")
>>> tokenizer = AutoTokenizer.from_pretrained("google/umt5-small")

>>> inputs = tokenizer(
...     "A <extra_id_0> walks into a bar and orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.",
...     return_tensors="pt",
... )
>>> outputs = model.generate(**inputs)
>>> print(tokenizer.batch_decode(outputs))
['<pad><extra_id_0>nyone who<extra_id_1> drink<extra_id_2> a<extra_id_3> alcohol<extra_id_4> A<extra_id_5> A. This<extra_id_6> I<extra_id_7><extra_id_52><extra_id_53></s>']
```

----------------------------------------

TITLE: Creating Translation Preprocessing Function
DESCRIPTION: Define a function to preprocess translation examples by adding translation task prefix, setting source and target languages, and tokenizing inputs
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/translation.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
source_lang = "en"\ntarget_lang = "fr"\nprefix = "translate English to French: "\n\ndef preprocess_function(examples):\n    inputs = [prefix + example[source_lang] for example in examples["translation"]]\n    targets = [example[target_lang] for example in examples["translation"]]\n    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n    return model_inputs
```

----------------------------------------

TITLE: Generating Segmentation Masks with SAM using 2D Point Input
DESCRIPTION: This code demonstrates how to use the SAM model to generate segmentation masks for an object in an image using a 2D point as input. It loads the pre-trained model, processes an image with a specified point, and generates a mask prediction.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/sam.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from PIL import Image
import requests
from transformers import SamModel, SamProcessor

device = "cuda" if torch.cuda.is_available() else "cpu"
model = SamModel.from_pretrained("facebook/sam-vit-huge").to(device)
processor = SamProcessor.from_pretrained("facebook/sam-vit-huge")

img_url = "https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png"
raw_image = Image.open(requests.get(img_url, stream=True).raw).convert("RGB")
input_points = [[[450, 600]]]  # 2D location of a window in the image

inputs = processor(raw_image, input_points=input_points, return_tensors="pt").to(device)
with torch.no_grad():
    outputs = model(**inputs)

masks = processor.image_processor.post_process_masks(
    outputs.pred_masks.cpu(), inputs["original_sizes"].cpu(), inputs["reshaped_input_sizes"].cpu()
)
scores = outputs.iou_scores
```

----------------------------------------

TITLE: Creating LoRA configuration in Python
DESCRIPTION: This code snippet shows how to create a LoraConfig object to define LoRA parameters such as rank, lora_alpha, and the target modules. This configuration is essential for applying LoRA to the model's specific components.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/how_to_hack_models.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=16,
    lora_alpha=32,
    # apply LoRA to q and v
    target_modules=["q", "v"],
    lora_dropout=0.1,
    task_type="mask-generation"
)
```

----------------------------------------

TITLE: Setting up ViT Model for Image Classification
DESCRIPTION: Initializes the ViT model for image classification with the appropriate number of labels and label mappings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/image_classification.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForImageClassification, TrainingArguments, Trainer

>>> model = AutoModelForImageClassification.from_pretrained(
...     "google/vit-base-patch16-224-in21k",
...     num_labels=len(labels),
...     id2label=id2label,
...     label2id=label2id,
... )
```

----------------------------------------

TITLE: Document Image Classification with OCR Enabled - Python
DESCRIPTION: This code shows how to utilize the LayoutLMv2Processor to encode a document image for classification tasks, while performing OCR on the image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/layoutlmv2.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import LayoutLMv2Processor
from PIL import Image

processor = LayoutLMv2Processor.from_pretrained("microsoft/layoutlmv2-base-uncased")

image = Image.open(
    "name_of_your_document - can be a png, jpg, etc. of your documents (PDFs must be converted to images)."
).convert("RGB")
encoding = processor(
    image, return_tensors="pt"
)  # you can also add all tokenizer parameters here such as padding, truncation
print(encoding.keys())
# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'image'])
```

----------------------------------------

TITLE: Implementing ResNet Classification Model
DESCRIPTION: Creates a custom ResNet model class for image classification that includes loss calculation when labels are provided. Extends the base ResNet model for classification tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/custom_models.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch

class ResnetModelForImageClassification(PreTrainedModel):
    config_class = ResnetConfig

    def __init__(self, config):
        super().__init__(config)
        block_layer = BLOCK_MAPPING[config.block_type]
        self.model = ResNet(
            block_layer,
            config.layers,
            num_classes=config.num_classes,
            in_chans=config.input_channels,
            cardinality=config.cardinality,
            base_width=config.base_width,
            stem_width=config.stem_width,
            stem_type=config.stem_type,
            avg_down=config.avg_down,
        )

    def forward(self, tensor, labels=None):
        logits = self.model(tensor)
        if labels is not None:
            loss = torch.nn.functional.cross_entropy(logits, labels)
            return {"loss": loss, "logits": logits}
        return {"logits": logits}
```

----------------------------------------

TITLE: Setting Verbosity to INFO - Python
DESCRIPTION: This snippet demonstrates how to set the verbosity level of the Transformers logging system to INFO. This is useful for users who want detailed information during execution without the noise of DEBUG level logs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/logging.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import transformers

transformers.logging.set_verbosity_info()
```

----------------------------------------

TITLE: Load Custom Config (Transformers)
DESCRIPTION: This snippet loads a pre-trained model configuration and modifies it, specifically changing the number of attention heads. It imports `AutoConfig`, loads a DistilBERT configuration, and sets `n_heads` to 12. This prepares the configuration for creating a model with a modified architecture.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_30

LANGUAGE: python
CODE:
```
>>> from transformers import AutoConfig

>>> my_config = AutoConfig.from_pretrained("distilbert/distilbert-base-uncased", n_heads=12)
```

----------------------------------------

TITLE: Configure Accelerate
DESCRIPTION: This command initializes the Accelerate configuration, allowing the user to specify the training environment (CPU, GPU, TPU). The configuration file is saved for later use during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/run_scripts.md#_snippet_9

LANGUAGE: bash
CODE:
```
accelerate config
```

----------------------------------------

TITLE: Importing and Using RetriBertModel in Python
DESCRIPTION: Imports the RetriBertModel class from the Transformers library and demonstrates how to use its forward method for processing input through the RetriBERT model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/retribert.md#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
[[autodoc]] RetriBertModel
    - forward
```

----------------------------------------

TITLE: Installing Required Dependencies
DESCRIPTION: Commands to install the core Transformers library and related packages needed for pipeline functionality.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quicktour.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
!pip install transformers datasets evaluate accelerate
```

----------------------------------------

TITLE: Initializing BatchEncoding in Python
DESCRIPTION: The BatchEncoding class contains the output of the tokenizer's encoding methods. It behaves like a dictionary for Python tokenizers and provides additional alignment methods for fast tokenizers to map between raw strings and token space.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/tokenizer.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
class BatchEncoding:
    # Implementation details
    pass
```

----------------------------------------

TITLE: Installing Development Dependencies (Bash)
DESCRIPTION: This command installs the Hugging Face Transformers library in editable mode (`-e`) along with all necessary development dependencies specified by the `.[dev]` extra. Installing in editable mode allows you to modify the source code and see the changes reflected immediately without reinstalling, which is crucial for development.
SOURCE: https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#_snippet_4

LANGUAGE: bash
CODE:
```
pip install -e ".[dev]"
```

----------------------------------------

TITLE: Training Reformer Model with Input Tokenization in Python
DESCRIPTION: This code snippet demonstrates how to prepare input data for training the Reformer model using PyTorch. It encodes the input sentence and computes the loss for the model output compared to the input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/reformer.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
input_ids = tokenizer.encode("This is a sentence from the training data", return_tensors="pt")
loss = model(input_ids, labels=input_ids)[0]
```

----------------------------------------

TITLE: Converting Llama2 Weights to Hugging Face Format
DESCRIPTION: Command to run the conversion script for transforming original Llama2 weights into the Hugging Face model format. Requires specifying input directory, model size, and output directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/llama2.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python src/transformers/models/llama/convert_llama_weights_to_hf.py \
    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path
```

----------------------------------------

TITLE: Improving ZoeDepth Accuracy by Averaging Flipped Image Predictions
DESCRIPTION: This snippet demonstrates how to improve the accuracy of ZoeDepth by performing inference on both the original and flipped images and averaging the results. It passes the flipped outputs to the optional `outputs_flipped` argument of the `post_process_depth_estimation` function. This is based on the original ZoeDepth implementation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/zoedepth.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
>>> with torch.no_grad():   
...     outputs = model(pixel_values)
...     outputs_flipped = model(pixel_values=torch.flip(inputs.pixel_values, dims=[3]))
>>> post_processed_output = image_processor.post_process_depth_estimation(
...     outputs,
...     source_sizes=[(image.height, image.width)],
...     outputs_flipped=outputs_flipped,
... )
```

----------------------------------------

TITLE: Installing required libraries for summarization with Transformers
DESCRIPTION: Command to install the necessary Python libraries including transformers, datasets, evaluate, and rouge_score for the summarization task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/summarization.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers datasets evaluate rouge_score
```

----------------------------------------

TITLE: Installing Transformers Library
DESCRIPTION: Command to install the Transformers library using pip.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/image_to_image.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers
```

----------------------------------------

TITLE: Training a TensorFlow Model with PushToHubCallback
DESCRIPTION: Shows how to train a TensorFlow model with the PushToHubCallback to automatically upload the model to the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/model_sharing.md#2025-04-23_snippet_12

LANGUAGE: python
CODE:
```
>>> model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3, callbacks=push_to_hub_callback)
```

----------------------------------------

TITLE: Generating Text-Only Response with Audio-Enabled Model (Python)
DESCRIPTION: Demonstrates how to load the Qwen2.5-Omni model with audio output enabled (`enable_audio_output=True`) for flexibility, but then generate only a text response using the `generate` method by explicitly setting `return_audio=False`. This allows faster text-only responses even when the model's audio components are loaded.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2_5_omni.md#_snippet_7

LANGUAGE: python
CODE:
```
model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-Omni-7B",
    torch_dtype="auto",
    device_map="auto",
    enable_audio_output=True,
)
...
text_ids = model.generate(**inputs, return_audio=False)
```

----------------------------------------

TITLE: Fast Image Processing with ConditionalDetrImageProcessorFast in Python
DESCRIPTION: Fast image processor class for Conditional DETR. Handles preprocessing and postprocessing for various tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/conditional_detr.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
[[autodoc]] ConditionalDetrImageProcessorFast
    - preprocess
    - post_process_object_detection
    - post_process_instance_segmentation
    - post_process_semantic_segmentation
    - post_process_panoptic_segmentation
```

----------------------------------------

TITLE: Examining Decoded Output from BERT Tokenizer in Python
DESCRIPTION: This code displays the decoded output from a BERT tokenizer showing how special tokens ([CLS] and [SEP]) are used to combine two sequences into a single input. This format is essential for tasks like sentence pair classification.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/glossary.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> print(decoded)
[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]
```

----------------------------------------

TITLE: Installing Required Libraries for LLM Generation
DESCRIPTION: This bash command installs the necessary libraries, including transformers and bitsandbytes, for working with LLMs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/llm_tutorial.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers bitsandbytes>=0.39.0 -q
```

----------------------------------------

TITLE: Loading T5 Model for Seq2Seq Tasks in TensorFlow
DESCRIPTION: Shows how to load a T5 model using TFAutoModelForSeq2SeqLM for sequence-to-sequence tasks in TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/translation.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Installing Flash-Attention 2 for Qwen2VL in Python
DESCRIPTION: This snippet shows how to install the latest version of Flash-Attention 2, which is used to speed up generation in the Qwen2VL model. It requires compatible hardware and the model to be loaded in float16 or bfloat16 precision.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2_vl.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
pip install -U flash-attn --no-build-isolation
```

----------------------------------------

TITLE: PyTorch TAPAS Training Loop
DESCRIPTION: Example showing how to fine-tune a TAPAS model in PyTorch including gradient calculation, backpropagation and optimization steps.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/tapas.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
numeric_values_scale = batch["numeric_values_scale"]
float_answer = batch["float_answer"]

# zero the parameter gradients
optimizer.zero_grad()

# forward + backward + optimize
outputs = model(
    input_ids=input_ids,
    attention_mask=attention_mask, 
    token_type_ids=token_type_ids,
    labels=labels,
    numeric_values=numeric_values,
    numeric_values_scale=numeric_values_scale,
    float_answer=float_answer,
)
loss = outputs.loss
loss.backward()
optimizer.step()
```

----------------------------------------

TITLE: Manual Inference with Token Classification Model in PyTorch
DESCRIPTION: Shows how to manually perform inference with a trained token classification model using PyTorch in the Transformers library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/token_classification.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer, AutoModelForTokenClassification
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> inputs = tokenizer(text, return_tensors="pt")

>>> model = AutoModelForTokenClassification.from_pretrained("stevhliu/my_awesome_wnut_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predictions = torch.argmax(logits, dim=2)
>>> predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]
```

----------------------------------------

TITLE: Loading and Splitting OPUS Books Translation Dataset
DESCRIPTION: Load the English-French subset of the OPUS Books dataset and split it into training and testing sets for machine translation model training
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/translation.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from datasets import load_dataset\n\nbooks = load_dataset("opus_books", "en-fr")\nbooks = books["train"].train_test_split(test_size=0.2)
```

----------------------------------------

TITLE: Pushing model to Hub with PushToHubMixin
DESCRIPTION: This Python code demonstrates how to push a model to the Hugging Face Hub directly using the `push_to_hub` method from `PushToHubMixin`. It assumes the model object inherits from the mixin.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
"model.push_to_hub(\"my-awesome-model\")"
```

----------------------------------------

TITLE: Exporting BERT model to TFLite format
DESCRIPTION: Command demonstrating how to export a BERT model from the Hugging Face Hub to TFLite format, specifying the sequence length and output directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/hi/tflite.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
optimum-cli export tflite --model google-bert/bert-base-uncased --sequence_length 128 bert_tflite/
```

----------------------------------------

TITLE: Visualizing Segmentation Results
DESCRIPTION: This snippet shows how to visualize the segmentation results by combining the predicted segmentation color mapping with the original image using matplotlib.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/sequence_classification.md#2025-04-22_snippet_32

LANGUAGE: python
CODE:
```
>>> import matplotlib.pyplot as plt
>>> import numpy as np

>>> color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)
>>> palette = np.array(ade_palette())
>>> for label, color in enumerate(palette):
...     color_seg[pred_seg == label, :] = color
>>> color_seg = color_seg[..., ::-1]  # convert to BGR

>>> img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map
>>> img = img.astype(np.uint8)

>>> plt.figure(figsize=(15, 10))
>>> plt.imshow(img)
>>> plt.show()
```

----------------------------------------

TITLE: Formatted Text OCR with GOT-OCR2 in PyTorch
DESCRIPTION: This snippet demonstrates how to extract formatted text like LaTeX from images using GOT-OCR2. The format parameter is set to True to tell the model to generate formatted output instead of plain text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/got_ocr2.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor, AutoModelForImageTextToText
>>> import torch

>>> device = "cuda" if torch.cuda.is_available() else "cpu"
>>> model = AutoModelForImageTextToText.from_pretrained("stepfun-ai/GOT-OCR-2.0-hf", device_map=device)
>>> processor = AutoProcessor.from_pretrained("stepfun-ai/GOT-OCR-2.0-hf", use_fast=True)

>>> image = "https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/latex.png"
>>> inputs = processor(image, return_tensors="pt", format=True, device=device).to(device)

>>> generate_ids = model.generate(
...     **inputs,
...     do_sample=False,
...     tokenizer=processor.tokenizer,
...     stop_strings="<|im_end|>",
...     max_new_tokens=4096,
... )

>>> processor.decode(generate_ids[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
"\\author{\nHanwen Jiang* \\(\\quad\\) Arjun Karpur \\({ }^{\\dagger} \\quad\\) Bingyi Cao \\({ }^{\\dagger} \\quad\\) (...)"
```

----------------------------------------

TITLE: Exporting Local TensorFlow Checkpoint to ONNX
DESCRIPTION: Command to export a locally saved TensorFlow checkpoint to ONNX format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/serialization.md#2025-04-22_snippet_8

LANGUAGE: bash
CODE:
```
python -m transformers.onnx --model=local-tf-checkpoint onnx/
```

----------------------------------------

TITLE: Quantizing, Saving, Reloading, and Inferencing Transformers Model (CPU, Int4)
DESCRIPTION: This comprehensive example demonstrates the end-to-end process of `torchao` int4 weight-only quantization on CPU using the new configuration API. It initializes quantization configurations, loads and quantizes the Llama model on the CPU, saves the quantized model, reloads it, loads the tokenizer, prepares input, and performs text generation on the CPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/torchao.md#_snippet_15

LANGUAGE: python
CODE:
```
import torch
from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
from torchao.quantization import Int4WeightOnlyConfig
from torchao.dtypes import Int4CPULayout

quant_config = Int4WeightOnlyConfig(group_size=128, layout=Int4CPULayout())
quantization_config = TorchAoConfig(quant_type=quant_config)

# Load and quantize the model
quantized_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B-Instruct",
    torch_dtype="auto",
    device_map="cpu",
    quantization_config=quantization_config
)
# save the quantized model
output_dir = "llama-3.1-8b-torchao-int4-cpu"
quantized_model.save_pretrained(output_dir, safe_serialization=False)

# reload the quantized model
reloaded_model = AutoModelForCausalLM.from_pretrained(
    output_dir, 
    device_map="cpu", 
    torch_dtype=torch.bfloat16
)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-8B-Instruct")
input_text = "What are we having for dinner?"
input_ids = tokenizer(input_text, return_tensors="pt")

output = reloaded_model.generate(**input_ids, max_new_tokens=10)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Creating and Training a BPE Tokenizer
DESCRIPTION: Creates a basic BPE tokenizer with special tokens and whitespace pre-tokenization, then trains it on provided files. The tokenizer is configured with UNK, CLS, SEP, PAD and MASK special tokens.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/fast_tokenizers.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from tokenizers import Tokenizer
>>> from tokenizers.models import BPE
>>> from tokenizers.trainers import BpeTrainer
>>> from tokenizers.pre_tokenizers import Whitespace

>>> tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
>>> trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

>>> tokenizer.pre_tokenizer = Whitespace()
>>> files = [...]
>>> tokenizer.train(files, trainer)
```

----------------------------------------

TITLE: TensorFlow TAPAS Inference Example
DESCRIPTION: Shows how to use a trained TAPAS model for inference in TensorFlow, including data preparation and prediction processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/tapas.md#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
from transformers import TapasTokenizer, TFTapasForQuestionAnswering
import pandas as pd

model_name = "google/tapas-base-finetuned-wtq"
model = TFTapasForQuestionAnswering.from_pretrained(model_name)
tokenizer = TapasTokenizer.from_pretrained(model_name)

data = {"Actors": ["Brad Pitt", "Leonardo Di Caprio", "George Clooney"], "Number of movies": ["87", "53", "69"]}
queries = [
    "What is the name of the first actor?",
    "How many movies has George Clooney played in?",
    "What is the total number of movies?",
]
table = pd.DataFrame.from_dict(data)
inputs = tokenizer(table=table, queries=queries, padding="max_length", return_tensors="tf")
outputs = model(**inputs)
predicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(
    inputs, outputs.logits, outputs.logits_aggregation
)
```

----------------------------------------

TITLE: TensorFlow Training Setup
DESCRIPTION: Initializes training parameters and optimizer for TensorFlow implementation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/multiple_choice.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
from transformers import create_optimizer

batch_size = 16
num_train_epochs = 2
total_train_steps = (len(tokenized_swag["train"]) // batch_size) * num_train_epochs
optimizer, schedule = create_optimizer(init_lr=5e-5, num_warmup_steps=0, num_train_steps=total_train_steps)
```

----------------------------------------

TITLE: Chunk Batching Pipeline Processing in Python
DESCRIPTION: Demonstrates how to handle pipeline processing with multiple forward passes, specifically for zero-shot classification and question answering tasks
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/pipelines.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
preprocessed = pipe.preprocess(inputs)
model_outputs = pipe.forward(preprocessed)
outputs = pipe.postprocess(model_outputs)
```

LANGUAGE: python
CODE:
```
all_model_outputs = []
for preprocessed in pipe.preprocess(inputs):
    model_outputs = pipe.forward(preprocessed)
    all_model_outputs.append(model_outputs)
outputs = pipe.postprocess(all_model_outputs)
```

----------------------------------------

TITLE: Pretraining BERT with Masked Language Modeling and Next Sentence Prediction
DESCRIPTION: BERT is pretrained with two objectives: masked language modeling and next-sentence prediction. In masked language modeling, random tokens are masked and predicted. For next-sentence prediction, the model predicts if sentence B follows sentence A.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks_explained.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
# Pseudo-code for BERT pretraining
def masked_language_modeling(input_ids, masked_positions):
    masked_lm_loss = model(input_ids, masked_positions)
    return masked_lm_loss

def next_sentence_prediction(sentence_a, sentence_b):
    nsp_loss = model(sentence_a, sentence_b)
    return nsp_loss

total_loss = masked_language_modeling(input_ids, masked_positions) + next_sentence_prediction(sentence_a, sentence_b)
```

----------------------------------------

TITLE: Performing Manual Super-Resolution Inference
DESCRIPTION: Runs the super-resolution model inference manually using the preprocessed image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/image_to_image.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
import torch

with torch.no_grad():
  outputs = model(pixel_values)
```

----------------------------------------

TITLE: Computing WER Metrics
DESCRIPTION: This snippet defines a function to compute the Word Error Rate (WER) from the model's predictions and labels, utilizing NumPy for processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/asr.md#2025-04-22_snippet_13

LANGUAGE: Python
CODE:
```
>>> import numpy as np

>>> def compute_metrics(pred):
...     pred_logits = pred.predictions
...     pred_ids = np.argmax(pred_logits, axis=-1)
...     pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id
...     pred_str = processor.batch_decode(pred_ids)
...     label_str = processor.batch_decode(pred.label_ids, group_tokens=False)
...     wer = wer.compute(predictions=pred_str, references=label_str)
...     return {"wer": wer}
```

----------------------------------------

TITLE: Evaluating Model Perplexity (PyTorch)
DESCRIPTION: Code to evaluate the trained masked language model and calculate its perplexity score, which is a measure of how well the model predicts the test data.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
import math

eval_results = trainer.evaluate()
print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```

----------------------------------------

TITLE: Demonstrating Token Type IDs with BERT Tokenizer in Python
DESCRIPTION: This snippet shows how to use the BERT tokenizer to generate token type IDs when processing a pair of sequences. It demonstrates how the tokenizer automatically adds special tokens and creates a binary mask to differentiate between the two sequences.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/glossary.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-cased")
>>> sequence_a = "HuggingFace is based in NYC"
>>> sequence_b = "Where is HuggingFace based?"

>>> encoded_dict = tokenizer(sequence_a, sequence_b)
>>> decoded = tokenizer.decode(encoded_dict["input_ids"])
```

----------------------------------------

TITLE: Creating Label Mappings
DESCRIPTION: Code to create label-to-id and id-to-label mappings for classification
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/visual_question_answering.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> import itertools

>>> labels = [item['ids'] for item in dataset['label']]
>>> flattened_labels = list(itertools.chain(*labels))
>>> unique_labels = list(set(flattened_labels))

>>> label2id = {label: idx for idx, label in enumerate(unique_labels)}
>>> id2label = {idx: label for label, idx in label2id.items()}
```

----------------------------------------

TITLE: Loading Word Error Rate metric
DESCRIPTION: Load the Word Error Rate (WER) metric from the Evaluate library for model evaluation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/tasks/asr.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
import evaluate

wer = evaluate.load("wer")
```

----------------------------------------

TITLE: Process a Single Dataset Example (Python)
DESCRIPTION: This code snippet applies the `prepare_dataset` function to the first example in the `lj_speech` dataset, demonstrating how to preprocess the data for ASR.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/preprocessing.md#_snippet_28

LANGUAGE: python
CODE:
```
>>> prepare_dataset(lj_speech[0])
```

----------------------------------------

TITLE: Using an ONNX Model with ONNX Runtime (Legacy)
DESCRIPTION: Python code showing how to use an ONNX model with ONNX Runtime directly, using the legacy approach with the InferenceSession.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/serialization.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer
>>> from onnxruntime import InferenceSession

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
>>> session = InferenceSession("onnx/model.onnx")
>>> # ONNX Runtime expects NumPy arrays as input
>>> inputs = tokenizer("Using DistilBERT with ONNX Runtime!", return_tensors="np")
>>> outputs = session.run(output_names=["last_hidden_state"], input_feed=dict(inputs))
```

----------------------------------------

TITLE: Configuring Accelerate for Script-based Training
DESCRIPTION: Command to create and save a configuration file for Accelerate when training from a script.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/accelerate.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
accelerate config
```

----------------------------------------

TITLE: Replacing SamVisionAttention in the model with SamVisionAttentionSplit using Python
DESCRIPTION: This snippet demonstrates how to replace the original SamVisionAttention class with the newly created SamVisionAttentionSplit class in the modeling_sam module. It includes loading a pretrained model with the updated attention configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/how_to_hack_models.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import SamModel
from transformers.models.sam import modeling_sam

# replace the attention class in the modeling_sam module
modeling_sam.SamVisionAttention = SamVisionAttentionSplit

# load the pretrained SAM model
model = SamModel.from_pretrained("facebook/sam-vit-base")
```

----------------------------------------

TITLE: Loading and Customizing DistilBERT Configuration
DESCRIPTION: Demonstrates how to load and customize DistilBERT model configuration by modifying activation function and attention dropout rate.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/create_a_model.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import DistilBertConfig

config = DistilBertConfig()
print(config)
```

----------------------------------------

TITLE: Classifying Text with ELECTRA AutoModel (PyTorch)
DESCRIPTION: This example shows how to perform text classification by explicitly loading the ELECTRA tokenizer and model for sequence classification. It tokenizes the input text, runs it through the model, and extracts the predicted class label from the output logits using PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/electra.md#_snippet_1

LANGUAGE: Python
CODE:
```
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained(
    "bhadresh-savani/electra-base-emotion",
)
model = AutoModelForSequenceClassification.from_pretrained(
    "bhadresh-savani/electra-base-emotion",
    torch_dtype=torch.float16
)
inputs = tokenizer("ELECTRA is more efficient than BERT", return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits
    predicted_class_id = logits.argmax(dim=-1).item()
    predicted_label = model.config.id2label[predicted_class_id]
print(f"Predicted label: {predicted_label}")
```

----------------------------------------

TITLE: Loading and using OPT model with Flash Attention 2 in Python
DESCRIPTION: This Python code snippet demonstrates loading the OPT model with Flash Attention 2 implementation for performing inference on a device, such as a GPU. Dependencies include PyTorch and the Transformers library, with half-precision datatype and Flash Attention 2 specified. Inputs are transformed into tensor format for generation, and outputs include decoded tokens.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/opt.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
from transformers import OPTForCausalLM, GPT2Tokenizer
device = "cuda" # the device to load the model onto

model = OPTForCausalLM.from_pretrained("facebook/opt-350m", torch_dtype=torch.float16, attn_implementation="flash_attention_2")
tokenizer = GPT2Tokenizer.from_pretrained("facebook/opt-350m")

prompt = ("A chat between a curious human and the Statue of Liberty.\n\nHuman: What is your name?\nStatue: I am the "
          "Statue of Liberty.\nHuman: Where do you live?\nStatue: New York City.\nHuman: How long have you lived "
          "there?")

model_inputs = tokenizer([prompt], return_tensors="pt").to(device)
model.to(device)

generated_ids = model.generate(**model_inputs, max_new_tokens=30, do_sample=False)
tokenizer.batch_decode(generated_ids)[0]
```

----------------------------------------

TITLE: Installing Gradio using pip
DESCRIPTION: This command installs the Gradio library using pip, which is required to create machine learning app interfaces. Gradio provides a fast and easy way to build and share machine learning apps.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_gradio.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```

!pip install gradio

```

----------------------------------------

TITLE: Accessing Raw Audio Data
DESCRIPTION: This snippet demonstrates how to access the raw audio signal (array) from a dataset. It retrieves the first example from the dataset and accesses the 'audio' column, which contains the raw audio signal.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/feature_extractors.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
```py
dataset[0]["audio"]["array"]
array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,
        0.        ,  0.        ])
```
```

----------------------------------------

TITLE: Initializing T5 Tokenizer for Translation Preprocessing
DESCRIPTION: Sets up the T5 tokenizer to process English and French text pairs, which will be used to convert raw text into token IDs that the model can understand.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/translation.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from transformers import AutoTokenizer

>>> checkpoint = "google-t5/t5-small"
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

----------------------------------------

TITLE: Preprocessing Function for Translation Data
DESCRIPTION: This function prepares data for the T5 model by adding a translation task prefix to inputs, separating source and target languages, and handling tokenization with appropriate length constraints.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/translation.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> source_lang = "en"
>>> target_lang = "fr"
>>> prefix = "translate English to French: "


>>> def preprocess_function(examples):
...     inputs = [prefix + example[source_lang] for example in examples["translation"]]
...     targets = [example[target_lang] for example in examples["translation"]]
...     model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)
...     return model_inputs
```

----------------------------------------

TITLE: Initializing BertTokenizerFast in PyTorch
DESCRIPTION: Fast tokenizer implementation for BERT models in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/bert.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
[[autodoc]] BertTokenizerFast
```

----------------------------------------

TITLE: Creating Evaluation Metrics Function for PyTorch
DESCRIPTION: PyTorch code for defining a function to compute evaluation metrics using the Mean IoU metric, including resizing predictions to match label dimensions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> import numpy as np
>>> import torch
>>> from torch import nn

>>> def compute_metrics(eval_pred):
...     with torch.no_grad():
...         logits, labels = eval_pred
...         logits_tensor = torch.from_numpy(logits)
...         logits_tensor = nn.functional.interpolate(
...             logits_tensor,
...             size=labels.shape[-2:],
...             mode="bilinear",
...             align_corners=False,
...         ).argmax(dim=1)

...         pred_labels = logits_tensor.detach().cpu().numpy()
...         metrics = metric.compute(
...             predictions=pred_labels,
...             references=labels,
...             num_labels=num_labels,
...             ignore_index=255,
...             reduce_labels=False,
...         )
...         for key, value in metrics.items():
...             if type(value) is np.ndarray:
...                 metrics[key] = value.tolist()
...         return metrics
```

----------------------------------------

TITLE: Creating Subset of Tokenized Dataset in Python
DESCRIPTION: This snippet demonstrates how to create smaller subsets of the training and evaluation datasets for quicker fine-tuning.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/training.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
```

----------------------------------------

TITLE: Processing Audio Data and Running Inference
DESCRIPTION: This snippet demonstrates the steps to process audio data using the processor, feeding it into the MMS model, and obtaining the transcribed output. It shows how to use the model output logits to decode the transcription.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mms.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
inputs = processor(en_sample, sampling_rate=16_000, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs).logits

ids = torch.argmax(outputs, dim=-1)[0]
transcription = processor.decode(ids)
```

----------------------------------------

TITLE: Installing Required Libraries for Image Captioning
DESCRIPTION: Installs the necessary Python libraries for image captioning tasks, including transformers, datasets, evaluate, and jiwer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/image_captioning.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers datasets evaluate -q
pip install jiwer -q
```

----------------------------------------

TITLE: Generating Code with CodeGen Model
DESCRIPTION: This code snippet demonstrates how to use the CodeGen model from the transformers library to generate code. It loads a pre-trained model and tokenizer, then uses them to generate code based on a given prompt.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/codegen.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> checkpoint = "Salesforce/codegen-350M-mono"
>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)

>>> text = "def hello_world():"

>>> completion = model.generate(**tokenizer(text, return_tensors="pt"))

>>> print(tokenizer.decode(completion[0]))
def hello_world():
    print("Hello World")

hello_world()
```

----------------------------------------

TITLE: Installing Transformers Library for Depth Estimation
DESCRIPTION: Installs the Transformers library using pip, which is required for performing depth estimation tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/monocular_depth_estimation.md#2025-04-23_snippet_0

LANGUAGE: bash
CODE:
```
pip install -q transformers
```

----------------------------------------

TITLE: Running Manual Depth Estimation Inference
DESCRIPTION: Performs depth estimation inference directly using the model without the pipeline wrapper.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/monocular_depth_estimation.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> import torch

>>> with torch.no_grad():
...     outputs = model(pixel_values)
...     predicted_depth = outputs.predicted_depth
```

----------------------------------------

TITLE: Loading TFDistilBertForSequenceClassification (TensorFlow)
DESCRIPTION: This code loads a pre-trained TFDistilBertForSequenceClassification model for sequence classification tasks in TensorFlow. Requires `TFDistilBertForSequenceClassification` from the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/create_a_model.md#_snippet_13

LANGUAGE: python
CODE:
```
>>> from transformers import TFDistilBertForSequenceClassification

>>> tf_model = TFDistilBertForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Loading COCO Dataset with Python
DESCRIPTION: Python code to load the COCO dataset using the Hugging Face datasets library with a custom dataset script.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/tensorflow/contrastive-image-text/README.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import os
import datasets

COCO_DIR = os.path.join(os.getcwd(), "data")
ds = datasets.load_dataset("ydshieh/coco_dataset_script", "2017", data_dir=COCO_DIR)
```

----------------------------------------

TITLE: Fine-tuning Whisper Small on Hindi Common Voice (Multi-GPU)
DESCRIPTION: Command for fine-tuning Whisper small model on Hindi Common Voice data using distributed training across 2 GPUs. This configuration reduces training time by half compared to single-GPU training.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-recognition/README.md#2025-04-22_snippet_8

LANGUAGE: bash
CODE:
```
torchrun \
 	--nproc_per_node 2 run_speech_recognition_seq2seq.py \
	--model_name_or_path="openai/whisper-small" \
	--dataset_name="mozilla-foundation/common_voice_11_0" \
	--dataset_config_name="hi" \
	--language="hindi" \
	--task="transcribe" \
	--train_split_name="train+validation" \
	--eval_split_name="test" \
	--max_steps="5000" \
	--output_dir="./whisper-small-hi" \
	--per_device_train_batch_size="16" \
	--per_device_eval_batch_size="16" \
	--logging_steps="25" \
	--learning_rate="1e-5" \
	--warmup_steps="500" \
	--eval_strategy="steps" \
	--eval_steps="1000" \
	--save_strategy="steps" \
	--save_steps="1000" \
	--generation_max_length="225" \
	--preprocessing_num_workers="16" \
	--max_duration_in_seconds="30" \
	--text_column_name="sentence" \
	--freeze_feature_encoder="False" \
	--gradient_checkpointing \
	--fp16 \
	--overwrite_output_dir \
	--do_train \
	--do_eval \
	--predict_with_generate \
	--use_auth_token
```

----------------------------------------

TITLE: Fine-tuning BERT on IMDB Dataset
DESCRIPTION: This bash script shows how to fine-tune a BERT model on the IMDB dataset using the Transformers library. It uses the 'run_glue.py' script with parameters tailored for the IMDB dataset, including training and prediction steps.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
python run_glue.py \
  --model_name_or_path google-bert/bert-base-cased \
  --dataset_name imdb  \
  --do_train \
  --do_predict \
  --max_seq_length 128 \
  --per_device_train_batch_size 32 \
  --learning_rate 2e-5 \
  --num_train_epochs 3 \
  --output_dir /tmp/imdb/
```

----------------------------------------

TITLE: Passing inputs to the model and retrieving logits (TensorFlow)
DESCRIPTION: This snippet demonstrates how to pass the tokenized inputs to the model and retrieve the logits.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/sequence_classification.md#2025-04-22_snippet_30

LANGUAGE: python
CODE:
```
>>> from transformers import TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained("stevhliu/my_awesome_model")
>>> logits = model(**inputs).logits
```

----------------------------------------

TITLE: Defining RagSequenceForGeneration in PyTorch
DESCRIPTION: This code snippet describes the RagSequenceForGeneration model in PyTorch, specifically detailing the methods for forward processing and output generation. This model enables text generation using retrieved documents.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/rag.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
## RagSequenceForGeneration
[[autodoc]] RagSequenceForGeneration
    - forward
    - generate
```

----------------------------------------

TITLE: Configuring DataParallel in PyTorch
DESCRIPTION: Basic implementation of data parallelism for distributing model training across single machine GPUs. Replicates model to multiple GPUs and synchronizes gradients.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_many.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
model = torch.nn.DataParallel(model)
```

----------------------------------------

TITLE: Loading CPPE-5 Dataset
DESCRIPTION: Loading the CPPE-5 dataset using Hugging Face datasets library and displaying its structure.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/object_detection.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> cppe5 = load_dataset("cppe-5")
>>> cppe5
```

----------------------------------------

TITLE: Installing Required Libraries for Wav2Vec2 Fine-Tuning
DESCRIPTION: Installs the necessary Python libraries for fine-tuning Wav2Vec2 on the MInDS-14 dataset, including transformers, datasets, evaluate, and jiwer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/asr.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers datasets evaluate jiwer
```

----------------------------------------

TITLE: Running Distributed Training on Multiple Nodes
DESCRIPTION: Bash command to run distributed training on two Xeon nodes with four processes total, using BF16 auto mixed precision.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/perf_train_cpu_many.md#2025-04-23_snippet_5

LANGUAGE: bash
CODE:
```
export CCL_WORKER_COUNT=1
export MASTER_ADDR=xxx.xxx.xxx.xxx #node0 ip
mpirun -f hostfile -n 4 -ppn 2 \
-genv OMP_NUM_THREADS=23 \
python3 run_qa.py \
--model_name_or_path google-bert/bert-large-uncased \
--dataset_name squad \
--do_train \
--do_eval \
--per_device_train_batch_size 12  \
--learning_rate 3e-5  \
--num_train_epochs 2  \
--max_seq_length 384 \
--doc_stride 128  \
--output_dir /tmp/debug_squad/ \
--no_cuda \
--ddp_backend ccl \
--use_ipex \
--bf16
```

----------------------------------------

TITLE: Querying the Web Server (Bash)
DESCRIPTION: This command uses `curl` to send a POST request to the running web server.  It sends the string "test [MASK]" as the payload, expecting the server to process this input using the Transformers pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/pipeline_webserver.md#_snippet_2

LANGUAGE: bash
CODE:
```
curl -X POST -d "test [MASK]" http://localhost:8000/
#[{"score":0.7742936015129089,"token":1012,"token_str":".","sequence":"test."},...]
```

----------------------------------------

TITLE: Accessing Model Configuration in PyTorch
DESCRIPTION: This snippet demonstrates how to load a pretrained model and access its configuration object. It shows the relationship between models and their configurations in the Transformers library architecture.
SOURCE: https://github.com/huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
# assuming that `brand_new_bert` belongs to the organization `brandy`
model = BrandNewBertModel.from_pretrained("brandy/brand_new_bert")
model.config  # model has access to its config
```

----------------------------------------

TITLE: PretrainedConfig Documentation
DESCRIPTION: Documentation section describing the PretrainedConfig base class functionality and its common attributes across model configurations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/configuration.md#2025-04-22_snippet_1

LANGUAGE: markdown
CODE:
```
[[autodoc]] PretrainedConfig
    - push_to_hub
    - all
```

----------------------------------------

TITLE: Masked Language Modeling with BridgeTower
DESCRIPTION: Example showing how to perform masked language modeling using BridgeTowerProcessor and BridgeTowerForMaskedLM with an image and masked text input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/bridgetower.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import BridgeTowerProcessor, BridgeTowerForMaskedLM
from PIL import Image
import requests

url = "http://images.cocodataset.org/val2017/000000360943.jpg"
image = Image.open(requests.get(url, stream=True).raw).convert("RGB")
text = "a <mask> looking out of the window"

processor = BridgeTowerProcessor.from_pretrained("BridgeTower/bridgetower-base-itm-mlm")
model = BridgeTowerForMaskedLM.from_pretrained("BridgeTower/bridgetower-base-itm-mlm")

# prepare inputs
encoding = processor(image, text, return_tensors="pt")

# forward pass
outputs = model(**encoding)

results = processor.decode(outputs.logits.argmax(dim=-1).squeeze(0).tolist())

print(results)
```

----------------------------------------

TITLE: Creating Smaller Dataset Subsets for Faster Fine-tuning
DESCRIPTION: Creates smaller subsets of the tokenized datasets for training and evaluation to reduce fine-tuning time. Uses shuffle and select methods to create random subsets of 1000 examples.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/training.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
```

----------------------------------------

TITLE: Configuring Batch Size for GPU Training - Python
DESCRIPTION: This snippet demonstrates how to configure the training and evaluation batch sizes using the `TrainingArguments` class. It helps in optimizing GPU memory usage and enhancing training speed by setting appropriate batch sizes depending on the model and GPU capabilities.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import TrainingArguments

args = TrainingArguments(
    per_device_train_batch_size=256,
    per_device_eval_batch_size=256,
)
```

----------------------------------------

TITLE: Loading and Using TorchScript Model
DESCRIPTION: Demonstrates how to load and use a traced TorchScript model for inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/torchscript.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
loaded_model = torch.jit.load("traced_bert.pt")
loaded_model.eval()

all_encoder_layers, pooled_output = loaded_model(*dummy_input)
```

----------------------------------------

TITLE: Implementing Self-Speculative Decoding
DESCRIPTION: Shows implementation of self-speculative decoding using early exiting and layer skipping for efficient text generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/generation_strategies.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

prompt = "Alice and Bob"
checkpoint = "facebook/layerskip-llama3.2-1B"

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
inputs = tokenizer(prompt, return_tensors="pt")

model = AutoModelForCausalLM.from_pretrained(checkpoint)
outputs = model.generate(**inputs, assistant_early_exit=4, do_sample=False, max_new_tokens=20)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

----------------------------------------

TITLE: Loading Model with Commit Hash
DESCRIPTION: Loads a specific version of custom model using commit hash for security
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/custom_models.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
commit_hash = "ed94a7c6247d8aedce4647f00f20de6875b5b292"
model = AutoModelForImageClassification.from_pretrained(
    "sgugger/custom-resnet50d", trust_remote_code=True, revision=commit_hash
)
```

----------------------------------------

TITLE: Training Summarization Models with Custom Data Files using Trainer API
DESCRIPTION: This script shows how to train a summarization model using custom CSV or JSONLINES files instead of built-in datasets. It specifies paths to training and validation files and configures the same training parameters as with standard datasets.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --train_file path_to_csv_or_jsonlines_file \
    --validation_file path_to_csv_or_jsonlines_file \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --overwrite_output_dir \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate
```

----------------------------------------

TITLE: Downloading Specific Model Files using huggingface_hub
DESCRIPTION: Python code showing how to use the huggingface_hub library to download specific model files for offline use.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/installation.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import hf_hub_download

>>> hf_hub_download(repo_id="bigscience/T0_3B", filename="config.json", cache_dir="./your/path/bigscience_t0")
```

----------------------------------------

TITLE: Push Tokenizer to Hub - Python
DESCRIPTION: This snippet shows how to push a tokenizer to the Hugging Face Hub using the `push_to_hub` method of a tokenizer instance. It uploads the tokenizer files to the specified repository.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/model_sharing.md#_snippet_14

LANGUAGE: python
CODE:
```
>>> tokenizer.push_to_hub("my-awesome-model")
```

----------------------------------------

TITLE: Listing Flax Dependencies with Version Requirements
DESCRIPTION: This snippet specifies the minimum version requirements for packages needed to use Flax with the Hugging Face transformers library. It includes datasets, JAX, JAXlib, Flax, and Optax with their respective minimum versions.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/flax/question-answering/requirements.txt#2025-04-22_snippet_0

LANGUAGE: plaintext
CODE:
```
datasets >= 1.8.0
jax>=0.2.17
jaxlib>=0.1.68
flax>=0.3.5
optax>=0.0.8
```

----------------------------------------

TITLE: Accessing Feature Maps from AutoBackbone
DESCRIPTION: This snippet showcases how to access the shape of the feature map obtained from the `AutoBackbone` model. It prints the shape of the first feature map in the `feature_maps` list.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/autoclass_tutorial.md#_snippet_4

LANGUAGE: Python
CODE:
```
>>> list(feature_maps[0].shape)
[1, 96, 56, 56]
```

----------------------------------------

TITLE: Initializing Aya Vision Model with Transformers
DESCRIPTION: Demonstrates how to load the Aya Vision 8B model using the Hugging Face Transformers library, setting up a model and processor for image-text generation on a CUDA device.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/aya_vision.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import AutoProcessor, AutoModelForImageTextToText
import torch

model_id = "CohereForAI/aya-vision-8b"
torch_device = "cuda:0"

processor = AutoProcessor.from_pretrained(model_id, use_fast=True)
model = AutoModelForImageTextToText.from_pretrained(
    model_id, device_map=torch_device, torch_dtype=torch.float16
)
```

----------------------------------------

TITLE: Hugging Face Hub Authentication
DESCRIPTION: Authentication setup for accessing Hugging Face Hub to share and upload models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/object_detection.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Logging into Hugging Face Account
DESCRIPTION: Logs into a Hugging Face account to enable model uploading and sharing with the community.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/asr.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Fine-tuning BERT on CoNLL-2003 for Named Entity Recognition using Flax
DESCRIPTION: This command runs the Flax NER script to fine-tune the google-bert/bert-base-cased model on the CoNLL-2003 dataset. It trains for 3 epochs with specified hyperparameters and pushes the results to the Hugging Face Hub.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/flax/token-classification/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python run_flax_ner.py \
  --model_name_or_path google-bert/bert-base-cased \
  --dataset_name conll2003 \
  --max_seq_length 128 \
  --learning_rate 2e-5 \
  --num_train_epochs 3 \
  --per_device_train_batch_size 4 \
  --output_dir ./bert-ner-conll2003 \
  --eval_steps 300 \
  --push_to_hub
```

----------------------------------------

TITLE: Configuration of Full Model Quantization in Python
DESCRIPTION: This snippet demonstrates how to configure HQQ to quantize an entire model by setting up a HqqConfig with specific bit-widths and group sizes. It is dependent on the transformers library and torch for setting data types and device maps. The example replaces all linear layers with a uniform quantization configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/hqq.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, HqqConfig

quant_config = HqqConfig(nbits=8, group_size=64)
model = transformers.AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B", 
    torch_dtype=torch.float16, 
    device_map="cuda", 
    quantization_config=quant_config
)
```

----------------------------------------

TITLE: Running Masked Language Modeling with BridgeTower in Python
DESCRIPTION: This code snippet illustrates how to use the BridgeTowerProcessor and BridgeTowerForMaskedLM classes to carry out masked language modeling with an image and a text that includes a mask placeholder. The code prepares inputs and retrieves the model's output, decoding it to get the predicted text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bridgetower.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
>>> from transformers import BridgeTowerProcessor, BridgeTowerForMaskedLM
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000360943.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw).convert("RGB")
>>> text = "a <mask> looking out of the window"

>>> processor = BridgeTowerProcessor.from_pretrained("BridgeTower/bridgetower-base-itm-mlm")
>>> model = BridgeTowerForMaskedLM.from_pretrained("BridgeTower/bridgetower-base-itm-mlm")

>>> # prepare inputs
>>> encoding = processor(image, text, return_tensors="pt")

>>> # forward pass
>>> outputs = model(**encoding)

>>> results = processor.decode(outputs.logits.argmax(dim=-1).squeeze(0).tolist())

>>> print(results)
.a cat looking out of the window.
```

----------------------------------------

TITLE: Loading Saved Configuration
DESCRIPTION: Demonstrates how to load a previously saved configuration from a local JSON file.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/create_a_model.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
my_config = DistilBertConfig.from_pretrained("./your_model_save_path/my_config.json")
```

----------------------------------------

TITLE: Initializing BioGPT with Scaled Dot-Product Attention in PyTorch
DESCRIPTION: This Python code snippet demonstrates the initialization of the BioGPT model using the scaled dot-product attention (SDPA) operator in PyTorch. It requires the 'transformers' library and the model has to be loaded with 'microsoft/biogpt', setting the 'attn_implementation' parameter to 'sdpa'.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/biogpt.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import BioGptForCausalLM
model = BioGptForCausalLM.from_pretrained("microsoft/biogpt", attn_implementation="sdpa", torch_dtype=torch.float16)
```

----------------------------------------

TITLE: Using CANINE Model Without Tokenization in Python
DESCRIPTION: This code snippet demonstrates how to use the CANINE model provided by the Transformers library directly with raw character data without the need for a tokenizer. It shows how to convert characters into their Unicode code point IDs and perform a forward pass to obtain model outputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/canine.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import CanineModel
>>> import torch

>>> model = CanineModel.from_pretrained("google/canine-c")  # model pre-trained with autoregressive character loss

>>> text = "hello world"
>>> # use Python's built-in ord() function to turn each character into its unicode code point id
>>> input_ids = torch.tensor([[ord(char) for char in text]])

>>> outputs = model(input_ids)  # forward pass
>>> pooled_output = outputs.pooler_output
>>> sequence_output = outputs.last_hidden_state
```

----------------------------------------

TITLE: Loading and Using BARTpho Model with PyTorch
DESCRIPTION: Example showing how to load the BARTpho model and tokenizer, and process Vietnamese text using PyTorch. Demonstrates basic model initialization and inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bartpho.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
import torch
from transformers import AutoModel, AutoTokenizer

bartpho = AutoModel.from_pretrained("vinai/bartpho-syllable")

tokenizer = AutoTokenizer.from_pretrained("vinai/bartpho-syllable")

line = "ChÃºng tÃ´i lÃ  nhá»¯ng nghiÃªn cá»©u viÃªn."

input_ids = tokenizer(line, return_tensors="pt")

with torch.no_grad():
    features = bartpho(**input_ids)  # Models outputs are now tuples
```

----------------------------------------

TITLE: Running Inference with a Converted Hugging Face Model
DESCRIPTION: This snippet demonstrates how to load a converted model using the Hugging Face Transformers API and perform inference with it, showing the basic forward pass pattern.
SOURCE: https://github.com/huggingface/transformers/blob/main/templates/adding_a_new_model/open_model_proposals/ADD_BIG_BIRD.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
model = BigBirdModel.from_pretrained("/path/to/converted/checkpoint/folder")
input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]
output = model(input_ids).last_hidden_states
```

----------------------------------------

TITLE: Fine-tuning MBart for English to Romanian Translation
DESCRIPTION: This snippet illustrates how to fine-tune an MBart model for English to Romanian translation. It uses the specific language code format required by MBart models and specifies the model, dataset, and training parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
python examples/pytorch/translation/run_translation.py \
    --model_name_or_path facebook/mbart-large-en-ro  \
    --do_train \
    --do_eval \
    --dataset_name wmt16 \
    --dataset_config_name ro-en \
    --source_lang en_XX \
    --target_lang ro_RO \
    --output_dir /tmp/tst-translation \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: PyTorch Simple Model Example
DESCRIPTION: Example implementation of a simple PyTorch model to demonstrate layer naming and weight initialization
SOURCE: https://github.com/huggingface/transformers/blob/main/templates/adding_a_new_model/open_model_proposals/ADD_BIG_BIRD.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from torch import nn

class SimpleModel(nn.Module):
    def __init__(self):
            super().__init__()
            self.dense = nn.Linear(10, 10)
            self.intermediate = nn.Linear(10, 10)
            self.layer_norm = nn.LayerNorm(10)
```

LANGUAGE: python
CODE:
```
model = SimpleModel()

print(model)
```

LANGUAGE: python
CODE:
```
print(model.dense.weight.data)
```

----------------------------------------

TITLE: Initializing Wav2Vec2 Feature Extractor in Python
DESCRIPTION: This snippet demonstrates how to initialize a default Wav2Vec2FeatureExtractor for audio classification using the Wav2Vec2 model. It imports the Wav2Vec2FeatureExtractor class from the transformers library and creates an instance of it. The output shows the default parameters of the feature extractor.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/create_a_model.md#_snippet_26

LANGUAGE: python
CODE:
```
>>> from transformers import Wav2Vec2FeatureExtractor

>>> w2v2_extractor = Wav2Vec2FeatureExtractor()
>>> print(w2v2_extractor)
Wav2Vec2FeatureExtractor {
  "do_normalize": true,
  "feature_extractor_type": "Wav2Vec2FeatureExtractor",
  "feature_size": 1,
  "padding_side": "right",
  "padding_value": 0.0,
  "return_attention_mask": false,
  "sampling_rate": 16000
}
```

----------------------------------------

TITLE: Using and Sharing a Custom Pipeline on Hugging Face Hub
DESCRIPTION: Code demonstrating how to use a registered custom pipeline with a pre-trained model and then share it on the Hugging Face Hub. This shows how to leverage the push_to_hub method to make the custom pipeline available to others.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/add_new_pipeline.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from transformers import pipeline

classifier = pipeline("pair-classification", model="sgugger/finetuned-bert-mrpc")

# Share on the Hub
classifier.push_to_hub("test-dynamic-pipeline")

# How others can use your shared pipeline
from transformers import pipeline

classifier = pipeline(model="{your_username}/test-dynamic-pipeline", trust_remote_code=True)
```

----------------------------------------

TITLE: Evaluating a Masked Language Model and Computing Perplexity
DESCRIPTION: Evaluates the trained model using the Trainer's evaluate method and calculates perplexity from the evaluation loss.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/masked_language_modeling.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import math

eval_results = trainer.evaluate()
print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```

----------------------------------------

TITLE: Accelerate Configuration for FSDP in YAML
DESCRIPTION: This YAML configuration file demonstrates how to set up Accelerate for training with Fully Sharded Data Parallel (FSDP). It includes settings for auto-wrapping, prefetching, offloading, and sharding strategies.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/trainer.md#_snippet_23

LANGUAGE: yaml
CODE:
```
compute_environment: LOCAL_MACHINE
distributed_type: FSDP
downcast_bf16: 'no'
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch_policy: BACKWARD_PRE
  fsdp_forward_prefetch: true
  fsdp_offload_params: false
  fsdp_sharding_strategy: 1
  fsdp_state_dict_type: FULL_STATE_DICT
  fsdp_sync_module_states: true
  fsdp_transformer_layer_cls_to_wrap: BertLayer
  fsdp_use_orig_params: true
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 2
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

----------------------------------------

TITLE: PyTorch-specific Data Collator
DESCRIPTION: Creates a data collator specifically for PyTorch that will handle batching and padding of token classification data during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorForTokenClassification

>>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```

----------------------------------------

TITLE: Installing TFLite Export Dependencies for Optimum
DESCRIPTION: Command to install the necessary dependencies for exporting models to TFLite format using Optimum. This installs the TensorFlow exporters package for Optimum.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tflite.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install optimum[exporters-tf]
```

----------------------------------------

TITLE: Registering Custom Models for Auto Classes in a Library
DESCRIPTION: Registers custom models and configurations for use with Hugging Face's auto classes when extending the library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/custom_models.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
from transformers import AutoConfig, AutoModel, AutoModelForImageClassification

AutoConfig.register("resnet", ResnetConfig)
AutoModel.register(ResnetConfig, ResnetModel)
AutoModelForImageClassification.register(ResnetConfig, ResnetModelForImageClassification)
```

----------------------------------------

TITLE: Implementing Evaluation Metrics for Translation Quality
DESCRIPTION: Defines functions to postprocess generated text and compute SacreBLEU scores for evaluating translation quality during model training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/translation.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> import numpy as np


>>> def postprocess_text(preds, labels):
...     preds = [pred.strip() for pred in preds]
...     labels = [[label.strip()] for label in labels]

...     return preds, labels


>>> def compute_metrics(eval_preds):
...     preds, labels = eval_preds
...     if isinstance(preds, tuple):
...         preds = preds[0]
...     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

...     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
...     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

...     decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

...     result = metric.compute(predictions=decoded_preds, references=decoded_labels)
...     result = {"bleu": result["score"]}

...     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
...     result["gen_len"] = np.mean(prediction_lens)
...     result = {k: round(v, 4) for k, v in result.items()}
...     return result
```

----------------------------------------

TITLE: Installing Dependencies for FastSpeech2Conformer
DESCRIPTION: Commands to install the required dependencies: Transformers library and g2p-en package.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/fastspeech2_conformer.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install --upgrade pip
pip install --upgrade transformers g2p-en
```

----------------------------------------

TITLE: Running LLM Benchmark with Custom Parameters
DESCRIPTION: Command to run the language model benchmarking script with specific parameters including batch size, sequence length, and model name. This allows for customized performance testing scenarios.
SOURCE: https://github.com/huggingface/transformers/blob/main/tests/fixtures/empty.txt#2025-04-22_snippet_8

LANGUAGE: bash
CODE:
```
PYTHONPATH=src/ TF_CPP_MIN_VLOG_LEVEL=0 TF_CPP_MIN_LOG_LEVEL=0 python examples/tensorflow/language-modeling/run_benchmarks.py --model_name_or_path=Xenova/llama2.c-stories15M --per_device_train_batch_size=1 --per_device_eval_batch_size=1 --sequence_length=8 --num_warmup_iterations=10 --num_iterations=100
```

----------------------------------------

TITLE: Dataset Preprocessing Functions
DESCRIPTION: Functions for preprocessing document images and extracting OCR information using LayoutLMv2Processor.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/document_question_answering.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained(model_checkpoint)
>>> image_processor = processor.image_processor

>>> def get_ocr_words_and_boxes(examples):
...     images = [image.convert("RGB") for image in examples["image"]]
...     encoded_inputs = image_processor(images)

...     examples["image"] = encoded_inputs.pixel_values
...     examples["words"] = encoded_inputs.words
...     examples["boxes"] = encoded_inputs.boxes

...     return examples

>>> dataset_with_ocr = updated_dataset.map(get_ocr_words_and_boxes, batched=True, batch_size=2)
```

----------------------------------------

TITLE: Setting Dataset Transform for Image Preprocessing in Python
DESCRIPTION: This line applies the combined augmentation and preprocessing function to the entire dataset on the fly using the set_transform method from the datasets library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/image_processors.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
dataset.set_transform(transforms)
```

----------------------------------------

TITLE: Loading and Displaying Image for VQA in Python
DESCRIPTION: This code snippet loads an image from the dataset's image_id path and displays it using PIL's Image.open method. This allows for the visual inspection of images associated with questions to provide better context.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_question_answering.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from PIL import Image

image = Image.open(dataset[0]['image_id'])
image
```

----------------------------------------

TITLE: Implementing Universal Assisted Decoding
DESCRIPTION: Demonstrates universal assisted decoding (UAD) implementation allowing main and assistant models to use different tokenizers.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/generation_strategies.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

prompt = "Alice and Bob"

assistant_tokenizer = AutoTokenizer.from_pretrained("double7/vicuna-68m")
tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-9b")
inputs = tokenizer(prompt, return_tensors="pt")

model = AutoModelForCausalLM.from_pretrained("google/gemma-2-9b")
assistant_model = AutoModelForCausalLM.from_pretrained("double7/vicuna-68m")
outputs = model.generate(**inputs, assistant_model=assistant_model, tokenizer=tokenizer, assistant_tokenizer=assistant_tokenizer)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

----------------------------------------

TITLE: Inspecting Sharded Index File
DESCRIPTION: This snippet demonstrates how to load and inspect the `pytorch_model.bin.index.json` file associated with a sharded model. It shows how to access the metadata and the weight map, which maps parameter names to their respective shard files. This is useful for understanding the structure of sharded checkpoints.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/big_models.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> import json

>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir, max_shard_size="200MB")
...     with open(os.path.join(tmp_dir, "pytorch_model.bin.index.json"), "r") as f:
...         index = json.load(f)

>>> print(index.keys())
dict_keys(['metadata', 'weight_map'])
```

----------------------------------------

TITLE: Directory Structure for Custom Model
DESCRIPTION: Shows the required directory structure for organizing custom model files
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/custom_models.md#2025-04-22_snippet_7

LANGUAGE: text
CODE:
```
.
â””â”€â”€ resnet_model
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ configuration_resnet.py
    â””â”€â”€ modeling_resnet.py
```

----------------------------------------

TITLE: Installing Transformers with TensorFlow CPU support
DESCRIPTION: This command installs the `transformers` library along with the dependencies required to use TensorFlow (CPU version). This will allow Transformers to work with TensorFlow models using only the CPU.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/installation.md#_snippet_5

LANGUAGE: bash
CODE:
```
pip install 'transformers[tf-cpu]'
```

----------------------------------------

TITLE: Tokenizing Input with Hugging Face Transformers
DESCRIPTION: Example showing how to tokenize text using the Hugging Face Transformers implementation of BigBird, using BertGenerationTokenizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/templates/adding_a_new_model/open_model_proposals/ADD_BIG_BIRD.md#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
from transformers import BertGenerationTokenizer
input_str = "This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words."

tokenizer = BertGenerationTokenizer.from_pretrained("/path/big/bird/folder")

input_ids = tokenizer(input_str).input_ids
```

----------------------------------------

TITLE: Enabling bf16 Mixed Precision Training in DeepSpeed
DESCRIPTION: Configuration for bfloat16 (bf16) mixed precision training which offers the same dynamic range as fp32 without requiring loss scaling. Requires DeepSpeed 0.6.0 or later and can be enabled via command line arguments.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/deepspeed.md#2025-04-22_snippet_19

LANGUAGE: yaml
CODE:
```
{
    "bf16": {
        "enabled": "auto"
    }
}
```

----------------------------------------

TITLE: Converting TensorFlow model to PyTorch
DESCRIPTION: This Python code converts a TensorFlow pre-trained model to PyTorch format. It loads the TensorFlow model using `DistilBertForSequenceClassification.from_pretrained` with `from_tf=True`, and then saves it as a PyTorch model using `save_pretrained`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
"from transformers import DistilBertForSequenceClassification\n\npt_model = DistilBertForSequenceClassification.from_pretrained(\"path/to/awesome-name-you-picked\", from_tf=True)\npt_model.save_pretrained(\"path/to/awesome-name-you-picked\")"
```

----------------------------------------

TITLE: Installing Transformers via Conda
DESCRIPTION: This command installs the `transformers` library using Conda from the `conda-forge` channel. Conda is a package, dependency and environment management system.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/installation.md#_snippet_15

LANGUAGE: bash
CODE:
```
conda install conda-forge::transformers
```

----------------------------------------

TITLE: Reloading Sharded Checkpoints - Transformers - Python
DESCRIPTION: This snippet illustrates the process of reloading a previously saved sharded model checkpoint using the `transformers` library. It highlights the ease of reloading the model from its shards into the primary model structure.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/models.md#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
with tempfile.TemporaryDirectory() as tmp_dir:
    model.save_pretrained(tmp_dir)
    new_model = AutoModel.from_pretrained(tmp_dir)
```

----------------------------------------

TITLE: Initializing SpeechEncoderDecoderModel from Model Configurations in Python
DESCRIPTION: This code snippet demonstrates how to randomly initialize a SpeechEncoderDecoderModel using Wav2Vec2Config for the encoder and BertConfig for the decoder. It creates configuration objects for each component and combines them to initialize the model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/speech-encoder-decoder.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import BertConfig, Wav2Vec2Config, SpeechEncoderDecoderConfig, SpeechEncoderDecoderModel

>>> config_encoder = Wav2Vec2Config()
>>> config_decoder = BertConfig()

>>> config = SpeechEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)
>>> model = SpeechEncoderDecoderModel(config=config)
```

----------------------------------------

TITLE: Installing Transformers with Flax
DESCRIPTION: Command to install Transformers with Flax support
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/installation.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
pip install transformers[flax]
```

----------------------------------------

TITLE: Verifying Source Installation with Sentiment Analysis
DESCRIPTION: Python command to verify the source installation of Transformers by running a sentiment analysis pipeline on a sample text.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/fr/installation.md#2025-04-22_snippet_10

LANGUAGE: bash
CODE:
```
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))"
```

----------------------------------------

TITLE: Deriving Class Labels and Creating Mappings
DESCRIPTION: This code extracts unique class labels from the video file paths and creates two dictionaries: `label2id` and `id2label`. `label2id` maps class names to integer IDs, while `id2label` maps integer IDs back to class names. These mappings are used during model initialization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
">>> class_labels = sorted({str(path).split(\"/\")[2] for path in all_video_file_paths})
>>> label2id = {label: i for i, label in enumerate(class_labels)}
>>> id2label = {i: label for label, i in label2id.items()}

>>> print(f\"Unique classes: {list(label2id.keys())}.\")

# Unique classes: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress']."
```

----------------------------------------

TITLE: Preprocessing for Non-Roman Alphabets in Text-to-Speech (Python)
DESCRIPTION: This code snippet provides a solution for preprocessing text inputs for languages with non-Roman scripts using the `uroman` package. It demonstrates how to check if the tokenizer requires Romanization, how to install `uroman`, and how to preprocess inputs before feeding them into the VITS model. It also includes a function to perform the Romanization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/vits.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
# pip install -U uroman
from transformers import VitsTokenizer

tokenizer = VitsTokenizer.from_pretrained("facebook/mms-tts-eng")
print(tokenizer.is_uroman)

# Function to Romanize non-Roman text
import torch
from transformers import VitsTokenizer, VitsModel, set_seed
import os
import subprocess

def uromanize(input_string, uroman_path):
    """Convert non-Roman strings to Roman using the `uroman` perl package."""
    script_path = os.path.join(uroman_path, "bin", "uroman.pl")

    command = ["perl", script_path]

    process = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    # Execute the perl command
    stdout, stderr = process.communicate(input=input_string.encode())

    if process.returncode != 0:
        raise ValueError(f"Error {process.returncode}: {stderr.decode()}")

    # Return the output as a string and skip the new-line character at the end
    return stdout.decode()[:-1]

text = "ì´ë´ ë¬´ìŠ¨ ì¼ì´ì•¼"
uromanized_text = uromanize(text, uroman_path=os.environ["UROMAN"])

inputs = tokenizer(text=uromanized_text, return_tensors="pt")

set_seed(555)  # make deterministic
with torch.no_grad():
   outputs = model(inputs["input_ids"])

waveform = outputs.waveform[0]
```

----------------------------------------

TITLE: Grouping and Chunking Text for Language Modeling
DESCRIPTION: Function to concatenate tokenized texts and split them into chunks of fixed size for training the language model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/language_modeling.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> block_size = 128


>>> def group_texts(examples):
...     # Concatenate all texts.
...     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
...     total_length = len(concatenated_examples[list(examples.keys())[0]])
...     # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can
...     # customize this part to your needs.
...     if total_length >= block_size:
...         total_length = (total_length // block_size) * block_size
...     # Split by chunks of block_size.
...     result = {
...         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
...         for k, t in concatenated_examples.items()
...     }
...     result["labels"] = result["input_ids"].copy()
...     return result
```

LANGUAGE: python
CODE:
```
>>> lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)
```

----------------------------------------

TITLE: Configuring AdamW Optimizer with Manual Parameters
DESCRIPTION: A JSON configuration snippet for DeepSpeed's AdamW optimizer with explicitly specified parameter values. These values override any defaults or command line arguments.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/deepspeed.md#2025-04-22_snippet_25

LANGUAGE: json
CODE:
```
{
   "optimizer": {
       "type": "AdamW",
       "params": {
         "lr": 0.001,
         "betas": [0.8, 0.999],
         "eps": 1e-8,
         "weight_decay": 3e-7
       }
   }
}
```

----------------------------------------

TITLE: Creating Custom Pipeline Subclass in Python
DESCRIPTION: Shows how to subclass an existing pipeline to customize processing, specifically for text classification tasks by overriding postprocessing method
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/pipelines.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
class MyPipeline(TextClassificationPipeline):
    def postprocess():
        # Your code goes here
        scores = scores * 100
        # And here

my_pipeline = MyPipeline(model=model, tokenizer=tokenizer, ...)
# or if you use *pipeline* function, then:
my_pipeline = pipeline(model="xxxx", pipeline_class=MyPipeline)
```

----------------------------------------

TITLE: Programmatic ONNX Export using Optimum
DESCRIPTION: Python code showing how to export a model to ONNX format using the optimum.onnxruntime module
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/serialization.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from optimum.onnxruntime import ORTModelForSequenceClassification
>>> from transformers import AutoTokenizer

>>> model_checkpoint = "distilbert_base_uncased_squad"
>>> save_directory = "onnx/"

>>> # Load model from transformers and export to ONNX
>>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)
>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

>>> # Save the onnx model and tokenizer
>>> ort_model.save_pretrained(save_directory)
>>> tokenizer.save_pretrained(save_directory)
```

----------------------------------------

TITLE: Inference with Zamba Model
DESCRIPTION: This Python snippet demonstrates how to load the Zamba model and tokenizer, generate text based on a prompt, and decode the output. It requires the `transformers` and `torch` libraries and assumes a CUDA-enabled device. It utilizes `AutoTokenizer` and `AutoModelForCausalLM` for ease of use.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/zamba.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
"from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

tokenizer = AutoTokenizer.from_pretrained("Zyphra/Zamba-7B-v1")
model = AutoModelForCausalLM.from_pretrained("Zyphra/Zamba-7B-v1", device_map="auto", torch_dtype=torch.bfloat16)

input_text = "A funny prompt would be "
input_ids = tokenizer(input_text, return_tensors="pt").to("cuda")

outputs = model.generate(**input_ids, max_new_tokens=100)
print(tokenizer.decode(outputs[0]))"
```

----------------------------------------

TITLE: Batch Music Generation using Musicgen in Python
DESCRIPTION: This snippet illustrates batch music generation in Python, using both audio and text inputs with Musicgen. It requires the 'transformers' and 'datasets' libraries. The code processes the initial segments of audio samples along with text prompts and generates audio using the pre-trained model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/musicgen_melody.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor, MusicgenMelodyForConditionalGeneration
>>> from datasets import load_dataset

>>> processor = AutoProcessor.from_pretrained("facebook/musicgen-melody")
>>> model = MusicgenMelodyForConditionalGeneration.from_pretrained("facebook/musicgen-melody")

>>> # take the first quarter of the audio sample
>>> sample_1 = sample["array"][: len(sample["array"]) // 4]

>>> # take the first half of the audio sample
>>> sample_2 = sample["array"][: len(sample["array"]) // 2]

>>> inputs = processor(
...     audio=[sample_1, sample_2],
...     sampling_rate=sample["sampling_rate"],
...     text=["80s blues track with groovy saxophone", "90s rock song with loud guitars and heavy drums"],
...     padding=True,
...     return_tensors="pt",
... )
>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)
```

----------------------------------------

TITLE: Notebook Training Launch
DESCRIPTION: Code to launch training in a notebook environment using Accelerate's notebook launcher
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/hi/accelerate.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

----------------------------------------

TITLE: Quantizing with AutoRoundLight recipe (Python)
DESCRIPTION: Illustrates using the `AutoRoundLight` recipe, which optimizes for speed at the potential expense of accuracy. It adjusts parameters like `iters` and `lr` to achieve faster quantization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/auto_round.md#_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer
from auto_round import AutoRound

model_name = "facebook/opt-125m"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)
bits, group_size, sym = 4, 128, True
autoround = AutoRound(
    model,
    tokenizer,
    bits=bits,
    group_size=group_size,
    sym=sym,
    iters=50,
    lr=5e-3,
)

output_dir = "./tmp_autoround"
autoround.quantize_and_save(output_dir, format='auto_round') 
```

----------------------------------------

TITLE: Implementing backward pass with Accelerator
DESCRIPTION: Snippet showing how to replace the standard loss.backward() with Accelerator's backward method in the training loop.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/accelerate.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
for epoch in range(num_epochs):
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

----------------------------------------

TITLE: Creating Label Mappings for VQA Classification
DESCRIPTION: Creates mappings between label names and IDs needed for the classification task, extracting unique labels from the dataset and generating bidirectional mappings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/visual_question_answering.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> import itertools

>>> labels = [item['ids'] for item in dataset['label']]
>>> flattened_labels = list(itertools.chain(*labels))
>>> unique_labels = list(set(flattened_labels))

>>> label2id = {label: idx for idx, label in enumerate(unique_labels)}
>>> id2label = {idx: label for label, idx in label2id.items()}
```

----------------------------------------

TITLE: Implementing Nested Quantization for 4-bit Models
DESCRIPTION: Example showing how to enable nested quantization for 4-bit models, which performs a secondary quantization on already quantized weights to save additional memory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/bitsandbytes.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
from transformers import BitsAndBytesConfig

double_quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
)

model_double_quant = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-13b", quantization_config=double_quant_config)
```

----------------------------------------

TITLE: Registering Custom Model with Auto Classes
DESCRIPTION: Shows how to extend Auto Classes by registering custom model and config classes to enable automatic instantiation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/auto.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)
```

----------------------------------------

TITLE: Registering and Using a Custom Pair Classification Pipeline
DESCRIPTION: Code for registering the PairClassificationPipeline with support for both PyTorch and TensorFlow models, and then using it with a pre-trained model. This demonstrates how to make the custom pipeline available through the pipeline() factory function.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/add_new_pipeline.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from pair_classification import PairClassificationPipeline
from transformers.pipelines import PIPELINE_REGISTRY
from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification

PIPELINE_REGISTRY.register_pipeline(
    "pair-classification",
    pipeline_class=PairClassificationPipeline,
    pt_model=AutoModelForSequenceClassification,
    tf_model=TFAutoModelForSequenceClassification,
)

```

----------------------------------------

TITLE: Pushing a Model to an Organization Repository
DESCRIPTION: Demonstrates how to push a model to an organization's repository on the Hugging Face Hub instead of a personal account.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/model_sharing.md#2025-04-23_snippet_15

LANGUAGE: python
CODE:
```
>>> pt_model.push_to_hub("my-awesome-org/my-awesome-model")
```

----------------------------------------

TITLE: Saving a sharded Transformers model checkpoint in Python
DESCRIPTION: Demonstrates saving a Transformers model with a sharded checkpoint by specifying a max shard size, resulting in multiple weight files and an index.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/big_models.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir, max_shard_size="200MB")
...     print(sorted(os.listdir(tmp_dir)))
['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']
```

----------------------------------------

TITLE: Multi Image Inference with LLaVA-OneVision in Python
DESCRIPTION: This example demonstrates how to perform multi-image inference with LLaVA-OneVision. It loads the model and processor, prepares a batch of two prompts, each containing one or more images and corresponding text. The model generates responses for each prompt, and the results are decoded and printed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/llava_onevision.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
```python
import requests
from PIL import Image
import torch
from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration

# Load the model in half-precision
model = LlavaOnevisionForConditionalGeneration.from_pretrained("llava-hf/llava-onevision-qwen2-7b-ov-hf", torch_dtype=torch.float16, device_map="auto")
processor = AutoProcessor.from_pretrained("llava-hf/llava-onevision-qwen2-7b-ov-hf")

# Prepare a batch of two prompts, where the first one is a multi-turn conversation and the second is not
conversation_1 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "url": "https://www.ilankelman.org/stopsigns/australia.jpg"},
            {"type": "text", "text": "What is shown in this image?"},
            ],
    },
    {
        "role": "assistant",
        "content": [
            {"type": "text", "text": "There is a red stop sign in the image."},],
    },
    {
        "role": "user",
        "content": [
            {"type": "image", "url": "http://images.cocodataset.org/val2017/000000039769.jpg"},
            {"type": "text", "text": "What about this image? How many cats do you see?"},
            ],
    },
]

conversation_2 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "url": "https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg"},
            {"type": "text", "text": "What is shown in this image?"},
            ],
    },
]

inputs = processor.apply_chat_template(
    [conversation_1, conversation_2],
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    padding=True,
    return_tensors="pt"
).to(model.device, torch.float16)

# Generate
generate_ids = model.generate(**inputs, max_new_tokens=30)
processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
['user\n\nWhat is shown in this image?\nassistant\nThere is a red stop sign in the image.\nuser\n\nWhat about this image? How many cats do you see?\nassistant\ntwo', 'user\n\nWhat is shown in this image?\nassistant\n']
```
```

----------------------------------------

TITLE: Defining Image Transformations using torchvision
DESCRIPTION: This snippet defines a series of image transformations using the `torchvision.transforms` module. It combines `RandomResizedCrop` and `ColorJitter` to perform data augmentation. The `size` variable is determined based on the presence of 'shortest_edge' in the image_processor.size, otherwise it is derived from height and width.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_21

LANGUAGE: Python
CODE:
```
>>> from torchvision.transforms import RandomResizedCrop, ColorJitter, Compose

>>> size = (
...     image_processor.size["shortest_edge"]
...     if "shortest_edge" in image_processor.size
...     else (image_processor.size["height"], image_processor.size["width"])
... )

>>> _transforms = Compose([RandomResizedCrop(size), ColorJitter(brightness=0.5, hue=0.5)])
```

----------------------------------------

TITLE: Create PushToHub Callback (TensorFlow)
DESCRIPTION: Creates a `PushToHubCallback` to automatically upload the model and tokenizer to the Hugging Face Hub after training.  Specifies the output directory and the tokenizer. Requires the `transformers` library and a defined `tokenizer` object.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/masked_language_modeling.md#_snippet_21

LANGUAGE: python
CODE:
```
>>> from transformers.keras_callbacks import PushToHubCallback

>>> callback = PushToHubCallback(
...	output_dir="my_awesome_eli5_mlm_model",
...	tokenizer=tokenizer,
... )
```

----------------------------------------

TITLE: Loading Audio Dataset for Inference
DESCRIPTION: Load and prepare an audio dataset with correct sampling rate for speech recognition inference
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/asr.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from datasets import load_dataset, Audio

dataset = load_dataset("PolyAI/minds14", "en-US", split="train")
dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
sampling_rate = dataset.features["audio"].sampling_rate
audio_file = dataset[0]["audio"]["path"]
```

----------------------------------------

TITLE: Loading a Specific Model Version Using Revision Parameter
DESCRIPTION: Demonstrates how to load a specific version of a model from the Hugging Face Hub by specifying a revision parameter, which can be a tag name, branch name, or commit hash.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/model_sharing.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
>>> model = AutoModel.from_pretrained(
...     "julien-c/EsperBERTo-small", revision="4c77982"  # tag name, or branch name, or commit hash
... )
```

----------------------------------------

TITLE: Loading M2M100 with Scaled Dot Product Attention - PyTorch
DESCRIPTION: The code snippet illustrates how to load the M2M100 model while explicitly using the Scaled Dot Product Attention (SDPA) implementation provided by PyTorch. It suggests using half-precision for optimal performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/m2m_100.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import M2M100ForConditionalGeneration
model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M", torch_dtype=torch.float16, attn_implementation="sdpa")
```

----------------------------------------

TITLE: Loading Tokenizer Object into PreTrainedTokenizerFast
DESCRIPTION: This snippet demonstrates how to load an existing tokenizer object, created with the `tokenizers` library, into a `PreTrainedTokenizerFast` object from the `transformers` library. This allows you to use the tokenizer with Transformers models. The input is a `tokenizer` object, and the output is a `fast_tokenizer` object.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/fast_tokenizers.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import PreTrainedTokenizerFast

>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)
```

----------------------------------------

TITLE: Modifying DistilBert Configuration
DESCRIPTION: This code demonstrates how to modify the default DistilBert configuration by changing the activation function and attention dropout rate.  The `DistilBertConfig` is instantiated with specific parameters. It requires the `DistilBertConfig` class from the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/create_a_model.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> my_config = DistilBertConfig(activation="relu", attention_dropout=0.4)
>>> print(my_config)
DistilBertConfig {
  "activation": "relu",
  "attention_dropout": 0.4,
 
```

----------------------------------------

TITLE: Loading PEFT Adapter with load_adapter Method
DESCRIPTION: Alternative approach to loading a PEFT adapter using the load_adapter method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/peft.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "facebook/opt-350m"
peft_model_id = "ybelkada/opt-350m-lora"

model = AutoModelForCausalLM.from_pretrained(model_id)
model.load_adapter(peft_model_id)
```

----------------------------------------

TITLE: Loading ViTMAE Model with SDPA in PyTorch
DESCRIPTION: Example of loading a ViTMAE model with Scaled Dot Product Attention (SDPA) implementation in PyTorch. This code demonstrates how to explicitly request SDPA usage and load the model in half-precision for optimal performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/vit_mae.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from transformers import ViTMAEModel
model = ViTMAEModel.from_pretrained("facebook/vit-mae-base", attn_implementation="sdpa", torch_dtype=torch.float16)
...
```

----------------------------------------

TITLE: Displaying Top Predictions for Masked Token (PyTorch)
DESCRIPTION: Code to find the top three token predictions for the masked position and print the complete text with each prediction inserted in place of the mask token in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_26

LANGUAGE: python
CODE:
```
top_3_tokens = torch.topk(mask_token_logits, 3, dim=1).indices[0].tolist()

for token in top_3_tokens:
    print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))
```

----------------------------------------

TITLE: Speculative Decoding with Sampling and Temperature
DESCRIPTION: This code extends the speculative decoding example by adding sampling and temperature control. It initializes a tokenizer, a main model, and an assistant model, sets a seed for reproducibility, and generates text using speculative decoding with sampling (`do_sample=True`) and a temperature of 0.5. Using a lower temperature can improve latency.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/generation_strategies.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed
>>> set_seed(42)  # ìž¬í˜„ì„±ì„ ìœ„í•´

>>> prompt = "Alice and Bob"
>>> checkpoint = "EleutherAI/pythia-1.4b-deduped"
>>> assistant_checkpoint = "EleutherAI/pythia-160m-deduped"

>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)
>>> assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)
>>> outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.5)
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Alice and Bob, who were both in their early twenties, were both in the process of']
```

----------------------------------------

TITLE: Extracting Unique Characters from Dataset
DESCRIPTION: Creates a function to extract all unique characters from the dataset's transcriptions for vocabulary analysis.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/text-to-speech.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
def extract_all_chars(batch):
    all_text = " ".join(batch["normalized_text"])
    vocab = list(set(all_text))
    return {"vocab": [vocab], "all_text": [all_text]}

vocabs = dataset.map(
    extract_all_chars,
    batched=True,
    batch_size=-1,
    keep_in_memory=True,
    remove_columns=dataset.column_names,
)

dataset_vocab = set(vocabs["vocab"][0])
tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}
```

----------------------------------------

TITLE: Exporting a Model to ONNX via CLI
DESCRIPTION: This command exports a ðŸ¤— Transformers model checkpoint from the ðŸ¤— Hub to the ONNX format using the optimum-cli.  It saves the resulting ONNX model to the specified directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/serialization.md#_snippet_2

LANGUAGE: bash
CODE:
```
optimum-cli export onnx --model distilbert/distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/
```

----------------------------------------

TITLE: Loading and Processing Dataset
DESCRIPTION: Loads the Beans dataset and implements preprocessing using an image processor from the teacher model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/knowledge_distillation_for_image_classification.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from datasets import load_dataset

dataset = load_dataset("beans")
```

LANGUAGE: python
CODE:
```
from transformers import AutoImageProcessor
teacher_processor = AutoImageProcessor.from_pretrained("merve/beans-vit-224")

def process(examples):
    processed_inputs = teacher_processor(examples["image"])
    return processed_inputs

processed_datasets = dataset.map(process, batched=True)
```

----------------------------------------

TITLE: Applying Image Transformations with PyTorch
DESCRIPTION: Applies transformations such as random cropping, resizing, and normalization on images to enhance model robustness using PyTorch library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_classification.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor

>>> normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)
>>> size = (
...     image_processor.size["shortest_edge"]
...     if "shortest_edge" in image_processor.size
...     else (image_processor.size["height"], image_processor.size["width"])
... )
>>> _transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])
```

LANGUAGE: python
CODE:
```
>>> def transforms(examples):
...     examples["pixel_values"] = [_transforms(img.convert("RGB")) for img in examples["image"]]
...     del examples["image"]
...     return examples
```

LANGUAGE: python
CODE:
```
>>> food = food.with_transform(transforms)
```

LANGUAGE: python
CODE:
```
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator()
```

----------------------------------------

TITLE: Implementing Text Streaming for VLM Output with Transformers
DESCRIPTION: This code implements text streaming for a better generation experience using Transformers. It defines a function `model_inference` that preprocesses user prompts and chat history, initializes `TextIteratorStreamer` for handling generation in a separate thread, and yields the generated text tokens in real-time.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_text_to_text.md#_snippet_10

LANGUAGE: python
CODE:
```
import time
from transformers import TextIteratorStreamer
from threading import Thread

def model_inference(
    user_prompt,
    chat_history,
    max_new_tokens,
    images
):
    user_prompt = {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": user_prompt},
        ]
    }
    chat_history.append(user_prompt)
    streamer = TextIteratorStreamer(
        processor.tokenizer,
        skip_prompt=True,
        timeout=5.0,
    )

    generation_args = {
        "max_new_tokens": max_new_tokens,
        "streamer": streamer,
        "do_sample": False
    }

    # add_generation_prompt=True makes model generate bot response
    prompt = processor.apply_chat_template(chat_history, add_generation_prompt=True)
    inputs = processor(
        text=prompt,
        images=images,
        return_tensors="pt",
    ).to(device)
    generation_args.update(inputs)

    thread = Thread(
        target=model.generate,
        kwargs=generation_args,
    )
    thread.start()

    acc_text = ""
    for text_token in streamer:
        time.sleep(0.04)
        acc_text += text_token
        if acc_text.endswith("<end_of_utterance>"):
            acc_text = acc_text[:-18]
        yield acc_text

    thread.join()
```

----------------------------------------

TITLE: Obtaining Similarity Score from AutoModel Outputs - Torch, Python
DESCRIPTION: This snippet computes the cosine similarity score between embeddings obtained from the AutoModel inference. The similarity score reflects the degree of similarity between the real and generated image embeddings.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_feature_extraction.md#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
embed_real = infer(image_real)
embed_gen = infer(image_gen)

similarity_score = cosine_similarity(embed_real, embed_gen, dim=1)
print(similarity_score)

# tensor([0.6061], device='cuda:0', grad_fn=<SumBackward1>)
```

----------------------------------------

TITLE: TensorFlow Summarization Training Script
DESCRIPTION: Example command for running a summarization training script using TensorFlow with T5-small model on CNN/DailyMail dataset. Includes configuration for training and evaluation parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/run_scripts.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
python examples/tensorflow/summarization/run_summarization.py  \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval
```

----------------------------------------

TITLE: Applying Int4 Weight-Only Quantization with torchao (Python)
DESCRIPTION: Provides examples for torchao's Int4 weight-only quantization via the Hugging Face Transformers library for A100 GPUs, recommending Gemlite for batch size N or tinygemm (with optional HQQ) for batch size 1. It shows how to configure `GemliteUIntXWeightOnlyConfig` or `Int4WeightOnlyConfig`, load the model, and perform text generation with compilation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/torchao.md#_snippet_5

LANGUAGE: Python
CODE:
```
import torch
from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer
from torchao.quantization import GemliteUIntXWeightOnlyConfig, Int4WeightOnlyConfig

# For batch size N, we recommend gemlite, which may require autotuning
# default is 4 bit, 8 bit is also supported by passing `bit_width=8`
quant_config = GemliteUIntXWeightOnlyConfig(group_size=128)

# For batch size 1, we also have custom tinygemm kernel that's only optimized for this
# We can set `use_hqq` to `True` for better accuracy
# quant_config = Int4WeightOnlyConfig(group_size=128, use_hqq=True)

quantization_config = TorchAoConfig(quant_type=quant_config)

# Load and quantize the model
quantized_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B-Instruct",
    torch_dtype="auto",
    device_map="auto",
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-8B-Instruct")
input_text = "What are we having for dinner?"
input_ids = tokenizer(input_text, return_tensors="pt").to("cuda")

# auto-compile the quantized model with `cache_implementation="static"` to get speed up
output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation="static")
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

----------------------------------------

TITLE: Defining RobertaConfig and RobertaModel classes in Python
DESCRIPTION: This code snippet defines the `RobertaConfig`, `RobertaEmbeddings`, and `RobertaModel` classes, demonstrating how to use inheritance to create a variant of BERT called RoBERTa while minimizing code duplication. It shows modifications specific to the embeddings and model definitions.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/modular_transformers.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from torch import nn
from ..bert.configuration_bert import BertConfig
from ..bert.modeling_bert import (
    BertModel,
    BertEmbeddings,
    BertForMaskedLM
)

# RoBERTa and BERT config is identical
class RobertaConfig(BertConfig):
    model_type = 'roberta'

# Redefine the embeddings to highlight the padding id difference, and redefine the position embeddings
class RobertaEmbeddings(BertEmbeddings):
    def __init__(self, config):
        super().__init__(config())

        self.padding_idx = config.pad_token_id
        self.position_embeddings = nn.Embedding(
            config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx
        )

# RoBERTa and BERT model is identical except for the embedding layer, which is defined above, so no need for additional changes here
class RobertaModel(BertModel):
    def __init__(self, config):
        super().__init__(config)
        self.embeddings = RobertaEmbeddings(config)

# The model heads now only need to redefine the model inside to `RobertaModel`
class RobertaForMaskedLM(BertForMaskedLM):
    def __init__(self, config):
        super().__init__(config)
        self.model = RobertaModel(config)
```

----------------------------------------

TITLE: Using MBart for Multilingual Translation (Python)
DESCRIPTION: This snippet shows how to use the MBart model for translating Finnish to English. It demonstrates setting the source language in the tokenizer and forcing the target language token during generation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/multilingual.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
fi_text = "Ã„lÃ¤ sekaannu velhojen asioihin, sillÃ¤ ne ovat hienovaraisia ja nopeasti vihaisia."

tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt", src_lang="fi_FI")
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")

encoded_en = tokenizer(en_text, return_tensors="pt")
generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id["en_XX"])
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
```

----------------------------------------

TITLE: Fetching Specific CUDA Device Properties
DESCRIPTION: Retrieves and prints detailed CUDA properties, including device name and memory, which aids in configuring the correct CUDA setup.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/debugging.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
CUDA_VISIBLE_DEVICES=0 python -c "import torch; \
print(torch.cuda.get_device_properties(torch.device('cuda')))"
```

----------------------------------------

TITLE: Creando una Instancia de ResNet y Cargando Pesos Preentrenados en Python
DESCRIPTION: Ejemplo de cÃ³mo crear una instancia del modelo ResNet personalizado y transferir pesos preentrenados desde un modelo timm equivalente.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/custom_models.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
resnet50d = ResnetModelForImageClassification(resnet50d_config)
```

----------------------------------------

TITLE: Creating Transform Functions for PyTorch
DESCRIPTION: PyTorch code defining transformation functions for training and validation datasets, applying jitter augmentation for training and basic processing for validation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> def train_transforms(example_batch):
...     images = [jitter(x) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs


>>> def val_transforms(example_batch):
...     images = [x for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

----------------------------------------

TITLE: Creating a Data Collator for Token Classification
DESCRIPTION: Sets up a DataCollatorForTokenClassification which handles dynamic padding during training, only padding sequences to the maximum length within each batch rather than the entire dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorForTokenClassification

>>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```

----------------------------------------

TITLE: Generating Code with Transformers CLI (Bash)
DESCRIPTION: Illustrates how to generate code directly from the command line using the `transformers run` utility. It pipes a code prompt into the command, specifying the text generation task, the Code Llama model, and the device.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/code_llama.md#_snippet_2

LANGUAGE: bash
CODE:
```
echo -e "# Function to calculate the factorial of a number\ndef factorial(n):" | transformers run --task text-generation --model meta-llama/CodeLlama-7b-hf --device 0
```

----------------------------------------

TITLE: ZeRO-1 Configuration in DeepSpeed
DESCRIPTION: This JSON configuration demonstrates ZeRO stage 1 optimization, which shards the optimizer state but not the gradients. This offers a middle ground between no sharding and full sharding, providing a potential speedup compared to ZeRO-0.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/deepspeed.md#2025-04-22_snippet_26

LANGUAGE: json
CODE:
```
{
    "zero_optimization": {
        "stage": 1
    }
}
```

----------------------------------------

TITLE: Installing Latest AutoAWQ with ExLlama-v2 Support
DESCRIPTION: Command to install the latest version of AutoAWQ from GitHub, which is required for ExLlama-v2 kernel support with AWQ quantized models.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/awq.md#2025-04-22_snippet_7

LANGUAGE: bash
CODE:
```
pip install git+https://github.com/casper-hansen/AutoAWQ.git
```

----------------------------------------

TITLE: Verifying Initial GPU Memory State
DESCRIPTION: Runs a predefined function to check the availability of GPU memory before loading models. Ensures that GPU memory is unoccupied and suggests stopping processes that may consume GPU resources if memory is not free.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_memory_anatomy.md#2025-04-22_snippet_3

LANGUAGE: py
CODE:
```
>>> print_gpu_utilization()
GPU memory occupied: 0 MB.

```

----------------------------------------

TITLE: Activating a Virtual Environment (Windows)
DESCRIPTION: This command activates the virtual environment created in the `.env` directory on Windows systems. Activating the environment makes its packages available.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/installation.md#_snippet_2

LANGUAGE: bash
CODE:
```
.env/Scripts/activate
```

----------------------------------------

TITLE: Fine-tuning T5-small for Summarization with TensorFlow
DESCRIPTION: This python script fine-tunes the T5-small model on the CNN/DailyMail dataset for summarization using TensorFlow/Keras. It specifies model, dataset, and training parameters including batch sizes, number of epochs, and output directory. The `source_prefix` argument is required for T5 models to indicate the summarization task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/run_scripts.md#_snippet_4

LANGUAGE: bash
CODE:
```
python examples/tensorflow/summarization/run_summarization.py  \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval
```

----------------------------------------

TITLE: Preparing TensorFlow Datasets for MLM Training
DESCRIPTION: Converts dataset objects to TensorFlow datasets compatible with the model, setting up training and test sets with the appropriate data collator.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/masked_language_modeling.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
tf_train_set = model.prepare_tf_dataset(
    lm_dataset["train"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_test_set = model.prepare_tf_dataset(
    lm_dataset["test"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)
```

----------------------------------------

TITLE: Training BART Tokenizer
DESCRIPTION: Trains a ByteLevelBPE tokenizer for BART on Norwegian OSCAR dataset with custom vocabulary size and special tokens.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/README.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
from datasets import load_dataset
from tokenizers import trainers, Tokenizer, normalizers, ByteLevelBPETokenizer

# load dataset
dataset = load_dataset("oscar", "unshuffled_deduplicated_no", split="train")

# Instantiate tokenizer
tokenizer = ByteLevelBPETokenizer()

def batch_iterator(batch_size=1000):
    for i in range(0, len(dataset), batch_size):
        yield dataset[i: i + batch_size]["text"]

# Customized training
tokenizer.train_from_iterator(batch_iterator(), vocab_size=50265, min_frequency=2, special_tokens=[
    "<s>",
    "<pad>",
    "</s>",
    "<unk>",
    "<mask>",
])

# Save files to disk
tokenizer.save("./norwegian-bart-base/tokenizer.json")
```

----------------------------------------

TITLE: Authenticating with Hugging Face Hub
DESCRIPTION: Logs in to the Hugging Face Hub to enable model uploading and sharing capabilities. This allows users to save and share their fine-tuned models with the community.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/language_modeling.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

----------------------------------------

TITLE: Registering Custom Models to Auto Classes in Python
DESCRIPTION: Shows how to register custom models and configurations to the Transformers Auto classes for use in a custom library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/custom_models.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import AutoConfig, AutoModel, AutoModelForImageClassification

AutoConfig.register("resnet", ResnetConfig)
AutoModel.register(ResnetConfig, ResnetModel)
AutoModelForImageClassification.register(ResnetConfig, ResnetModelForImageClassification)
```

----------------------------------------

TITLE: Running Causal Language Model Training with Transformers
DESCRIPTION: Command for training or fine-tuning a causal language model (like GPT) using the run_clm.py script with DistilGPT2 on the WikiText dataset. This approach produces models with stronger text generation capabilities.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/tensorflow/language-modeling/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
python run_clm.py \
--model_name_or_path distilbert/distilgpt2 \
--output_dir output \
--dataset_name wikitext \
--dataset_config_name wikitext-103-raw-v1
```

----------------------------------------

TITLE: TorchScript Model Inference
DESCRIPTION: Shows how to perform inference with a traced model using the __call__ method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/torchscript.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
traced_model(tokens_tensor, segments_tensors)
```

----------------------------------------

TITLE: Installing Transformers from Source
DESCRIPTION: This command installs ðŸ¤— Transformers from the GitHub repository's main branch. This installs the latest, potentially unstable, version of the library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/installation.md#2025-04-22_snippet_8

LANGUAGE: bash
CODE:
```
"pip install git+https://github.com/huggingface/transformers"
```

----------------------------------------

TITLE: Creating Custom DistilBERT Configuration
DESCRIPTION: Shows how to create a customized DistilBERT configuration with modified activation and dropout parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/create_a_model.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
my_config = DistilBertConfig(activation="relu", attention_dropout=0.4)
print(my_config)
```

----------------------------------------

TITLE: Custom Forward Pass with AMP Control (Python)
DESCRIPTION: Implementation of a custom forward pass with automatic mixed precision control for overflow prevention
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/debugging.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
def _forward(self, hidden_states):
    hidden_gelu = self.gelu_act(self.wi_0(hidden_states))
    hidden_linear = self.wi_1(hidden_states)
    hidden_states = hidden_gelu * hidden_linear
    hidden_states = self.dropout(hidden_states)
    hidden_states = self.wo(hidden_states)
    return hidden_states

import torch

def forward(self, hidden_states):
    if torch.is_autocast_enabled():
        with torch.cuda.amp.autocast(enabled=False):
            return self._forward(hidden_states)
    else:
        return self._forward(hidden_states)
```

----------------------------------------

TITLE: Object Detection with ConditionalDetrForObjectDetection in Python
DESCRIPTION: Conditional DETR model for object detection tasks. Extends base model for detection-specific outputs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/conditional_detr.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
[[autodoc]] ConditionalDetrForObjectDetection
    - forward
```

----------------------------------------

TITLE: Implementing Resilient Generation with Fallback Cache
DESCRIPTION: Implementation of a resilient generation function that falls back to OffloadedCache when encountering memory errors
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/kv_cache.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def resilient_generate(model, *args, **kwargs):
    oom = False
    try:
        return model.generate(*args, **kwargs)
    except torch.cuda.OutOfMemoryError as e:
        print(e)
        print("retrying with cache_implementation='offloaded'")
        oom = True
    if oom:
        torch.cuda.empty_cache()
        kwargs["cache_implementation"] = "offloaded"
        return model.generate(*args, **kwargs)

ckpt = "microsoft/Phi-3-mini-4k-instruct"
tokenizer = AutoTokenizer.from_pretrained(ckpt)
model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16).to("cuda:0")
prompt = ["okay "*1000 + "Fun fact: The most"]
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
beams = { "num_beams": 40, "num_beam_groups": 40, "num_return_sequences": 40, "diversity_penalty": 1.0, "max_new_tokens": 23, "early_stopping": True, }
out = resilient_generate(model, **inputs, **beams)
responses = tokenizer.batch_decode(out[:,-28:], skip_special_tokens=True)
```

----------------------------------------

TITLE: Listing Pre-trained MarianMT Models in Python
DESCRIPTION: Shows how to retrieve a list of all pre-trained MarianMT models available on the Hugging Face Hub using the huggingface_hub library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/marian.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from huggingface_hub import list_models

model_list = list_models()
org = "Helsinki-NLP"
model_ids = [x.id for x in model_list if x.id.startswith(org)]
suffix = [x.split("/")[1] for x in model_ids]
old_style_multi_models = [f"{org}/{s}" for s in suffix if s != s.lower()]
```

----------------------------------------

TITLE: Converting PyTorch Model to Flax in Python
DESCRIPTION: Demonstrates the conversion of a PyTorch model checkpoint to a Flax checkpoint using the from_pt parameter.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/model_sharing.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> flax_model = FlaxDistilBertForSequenceClassification.from_pretrained(
...     "path/to/awesome-name-you-picked", from_pt=True
... )
```

----------------------------------------

TITLE: Loading Tiktoken Tokenizer and Model
DESCRIPTION: This snippet demonstrates how to load a tokenizer and model from the same file using `AutoTokenizer.from_pretrained`. It assumes that the tokenizer.model file is a tiktoken file, which will be automatically loaded when using `from_pretrained`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tiktoken.md#_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import AutoTokenizer

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id, subfolder="original")
```

----------------------------------------

TITLE: Converting Dataset to TensorFlow Format
DESCRIPTION: The snippet illustrates converting a dataset into tf.data.Dataset format required for training the TensorFlow model, using the DefaultDataCollator.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors="tf")

>>> tf_train_dataset = train_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )

>>> tf_eval_dataset = test_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )
```

----------------------------------------

TITLE: Inspecting IMDb Dataset Sample - Python
DESCRIPTION: This snippet displays the content of the first test sample from the IMDb dataset, showing both the review text and its corresponding label indicating sentiment polarity.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
imdb["test"][0]
{
    "label": 0,
    "text": "I love sci-fi..."
}
```

----------------------------------------

TITLE: Creating Transform Functions for TensorFlow
DESCRIPTION: TensorFlow code defining transformation functions for training and validation datasets, applying augmentation for training and basic processing for validation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> def train_transforms(example_batch):
...     images = [aug_transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs


>>> def val_transforms(example_batch):
...     images = [transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

----------------------------------------

TITLE: Implementando el Modelo ResNet para ExtracciÃ³n de CaracterÃ­sticas en Python
DESCRIPTION: DefiniciÃ³n de una clase de modelo ResNet que hereda de PreTrainedModel para extraer caracterÃ­sticas de imÃ¡genes. Se implementa como un wrapper alrededor del modelo ResNet de la biblioteca timm.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/custom_models.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import PreTrainedModel
from timm.models.resnet import BasicBlock, Bottleneck, ResNet
from .configuration_resnet import ResnetConfig


BLOCK_MAPPING = {"basic": BasicBlock, "bottleneck": Bottleneck}


class ResnetModel(PreTrainedModel):
    config_class = ResnetConfig

    def __init__(self, config):
        super().__init__(config)
        block_layer = BLOCK_MAPPING[config.block_type]
        self.model = ResNet(
            block_layer,
            config.layers,
            num_classes=config.num_classes,
            in_chans=config.input_channels,
            cardinality=config.cardinality,
            base_width=config.base_width,
            stem_width=config.stem_width,
            stem_type=config.stem_type,
            avg_down=config.avg_down,
        )

    def forward(self, tensor):
        return self.model.forward_features(tensor)
```

----------------------------------------

TITLE: Initializing DeepSpeed for T0 Model Inference in Python
DESCRIPTION: This snippet sets up DeepSpeed configuration, loads a T0 model, and performs distributed inference across multiple GPUs. It demonstrates CPU offloading and ZeRO stage 3 optimization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/deepspeed.md#2025-04-22_snippet_31

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM
from transformers.integrations import HfDeepSpeedConfig
import deepspeed
import os
import torch

os.environ["TOKENIZERS_PARALLELISM"] = "false"  # To avoid warnings about parallelism in tokenizers

# distributed setup
local_rank = int(os.getenv("LOCAL_RANK", "0"))
world_size = int(os.getenv("WORLD_SIZE", "1"))
torch.cuda.set_device(local_rank)
deepspeed.init_distributed()

model_name = "bigscience/T0_3B"

config = AutoConfig.from_pretrained(model_name)
model_hidden_size = config.d_model

# batch size has to be divisible by world_size, but can be bigger than world_size
train_batch_size = 1 * world_size

# ds_config notes
#
# - enable bf16 if you use Ampere or higher GPU - this will run in mixed precision and will be
# faster.
#
# - for older GPUs you can enable fp16, but it'll only work for non-bf16 pretrained models - e.g.
# all official t5 models are bf16-pretrained
#
# - set offload_param.device to "none" or completely remove the `offload_param` section if you don't
# - want CPU offload
#
# - if using `offload_param` you can manually finetune stage3_param_persistence_threshold to control
# - which params should remain on gpus - the larger the value the smaller the offload size
#
# For in-depth info on Deepspeed config see
# https://huggingface.co/docs/transformers/main/main_classes/deepspeed

# keeping the same format as json for consistency, except it uses lower case for true/false
# fmt: off
ds_config = {
    "fp16": {
        "enabled": False
    },
    "bf16": {
        "enabled": False
    },
    "zero_optimization": {
        "stage": 3,
        "offload_param": {
            "device": "cpu",
            "pin_memory": True
        },
        "overlap_comm": True,
        "contiguous_gradients": True,
        "reduce_bucket_size": model_hidden_size * model_hidden_size,
        "stage3_prefetch_bucket_size": 0.9 * model_hidden_size * model_hidden_size,
        "stage3_param_persistence_threshold": 10 * model_hidden_size
    },
    "steps_per_print": 2000,
    "train_batch_size": train_batch_size,
    "train_micro_batch_size_per_gpu": 1,
    "wall_clock_breakdown": False
}
# fmt: on

# next line instructs transformers to partition the model directly over multiple gpus using
# deepspeed.zero.Init when model's `from_pretrained` method is called.
#
# **it has to be run before loading the model AutoModelForSeq2SeqLM.from_pretrained(model_name)**
#
# otherwise the model will first be loaded normally and only partitioned at forward time which is
# less efficient and when there is little CPU RAM may fail
dschf = HfDeepSpeedConfig(ds_config)  # keep this object alive

# now a model can be loaded.
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# initialise Deepspeed ZeRO and store only the engine object
ds_engine = deepspeed.initialize(model=model, config_params=ds_config)[0]
ds_engine.module.eval()  # inference

# Deepspeed ZeRO can process unrelated inputs on each GPU. So for 2 gpus you process 2 inputs at once.
# If you use more GPUs adjust for more.
# And of course if you have just one input to process you then need to pass the same string to both gpus
# If you use only one GPU, then you will have only rank 0.
rank = torch.distributed.get_rank()
if rank == 0:
    text_in = "Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy"
elif rank == 1:
    text_in = "Is this review positive or negative? Review: this is the worst restaurant ever"

tokenizer = AutoTokenizer.from_pretrained(model_name)
inputs = tokenizer.encode(text_in, return_tensors="pt").to(device=local_rank)
with torch.no_grad():
    outputs = ds_engine.module.generate(inputs, synced_gpus=True)
text_out = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"rank{rank}:\n   in={text_in}\n  out={text_out}")
```

----------------------------------------

TITLE: Loading AWQ Quantized Model with Custom Dtype
DESCRIPTION: Python code to load an AWQ quantized model with a specific torch data type instead of the default fp16. This example uses float32 precision for the weights.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/awq.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "TheBloke/zephyr-7B-alpha-AWQ"
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32)
```

----------------------------------------

TITLE: Distributed Training with Mixed Precision (PyTorch)
DESCRIPTION: This command demonstrates how to enable distributed training and mixed precision using `torchrun` for the PyTorch summarization script. The `fp16` flag enables mixed precision, and `nproc_per_node` specifies the number of GPUs to use.  The rest of the parameters are similar to the single-GPU training setup.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/run_scripts.md#_snippet_5

LANGUAGE: bash
CODE:
```
torchrun \
    --nproc_per_node 8 pytorch/summarization/run_summarization.py \
    --fp16 \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Loading Pre-trained Model with Custom Configuration in PyTorch
DESCRIPTION: Demonstrates how to load a pre-trained DistilBert model while overriding its default configuration with custom settings in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/create_a_model.md#2025-04-23_snippet_7

LANGUAGE: python
CODE:
```
>>> model = DistilBertModel.from_pretrained("distilbert/distilbert-base-uncased", config=my_config)
```

----------------------------------------

TITLE: Text Grouping Function
DESCRIPTION: Function to concatenate and chunk text sequences into blocks of specified size
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/masked_language_modeling.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> block_size = 128

>>> def group_texts(examples):
...     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
...     total_length = len(concatenated_examples[list(examples.keys())[0]])
...     if total_length >= block_size:
...         total_length = (total_length // block_size) * block_size
...     result = {
...         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
...         for k, t in concatenated_examples.items()
...     }
...     return result
```

----------------------------------------

TITLE: Visualizing Generated Masks
DESCRIPTION: This code visualizes the generated masks over the original image using matplotlib. It overlays each mask with a translucent coloring to show the segmentation results.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/mask_generation.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import matplotlib.pyplot as plt

plt.imshow(image, cmap='gray')

for i, mask in enumerate(masks["masks"]):
    plt.imshow(mask, cmap='viridis', alpha=0.1, vmin=0, vmax=1)

plt.axis('off')
plt.show()
```

----------------------------------------

TITLE: Loading TFDistilBertForQuestionAnswering (TensorFlow)
DESCRIPTION: This code loads a pre-trained TFDistilBertForQuestionAnswering model for question answering tasks in TensorFlow. Requires `TFDistilBertForQuestionAnswering` from the `transformers` library.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/create_a_model.md#_snippet_14

LANGUAGE: python
CODE:
```
>>> from transformers import TFDistilBertForQuestionAnswering

>>> tf_model = TFDistilBertForQuestionAnswering.from_pretrained("distilbert/distilbert-base-uncased")
```

----------------------------------------

TITLE: Running the Starlette App (Bash)
DESCRIPTION: This command demonstrates how to run the Starlette application using Uvicorn, a popular ASGI server. It assumes the server code is in a file named `server.py` and the Starlette app instance is named `app`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/pipeline_webserver.md#_snippet_1

LANGUAGE: bash
CODE:
```
uvicorn server:app
```

----------------------------------------

TITLE: Using ByT5 with a Tokenizer for Batch Processing in Python
DESCRIPTION: This example shows how to use ByT5 with a tokenizer for batch inference and training. It demonstrates tokenizing multiple sentences and computing the loss.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/byt5.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from transformers import T5ForConditionalGeneration, AutoTokenizer

model = T5ForConditionalGeneration.from_pretrained("google/byt5-small")
tokenizer = AutoTokenizer.from_pretrained("google/byt5-small")

model_inputs = tokenizer(
    ["Life is like a box of chocolates.", "Today is Monday."], padding="longest", return_tensors="pt"
)
labels_dict = tokenizer(
    ["La vie est comme une boÃ®te de chocolat.", "Aujourd'hui c'est lundi."], padding="longest", return_tensors="pt"
)
labels = labels_dict.input_ids

loss = model(**model_inputs, labels=labels).loss
loss.item()
```

----------------------------------------

TITLE: Initializing Model for Hyperparameter Search in Transformers
DESCRIPTION: Example of defining a model initialization function for use with hyperparameter search in the Transformers Trainer API.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/hpo_train.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
def model_init(trial):
    return AutoModelForSequenceClassification.from_pretrained(
        model_args.model_name_or_path,
        from_tf=bool(".ckpt" in model_args.model_name_or_path),
        config=config,
        cache_dir=model_args.cache_dir,
        revision=model_args.model_revision,
        use_auth_token=True if model_args.use_auth_token else None,
    )
```

----------------------------------------

TITLE: Dequantizing a bitsandbytes Model
DESCRIPTION: Complete example showing how to dequantize a model that was previously quantized using bitsandbytes, returning it to its original precision for inference.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/bitsandbytes.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer

model_id = "facebook/opt-125m"

model = AutoModelForCausalLM.from_pretrained(model_id, BitsAndBytesConfig(load_in_4bit=True))
tokenizer = AutoTokenizer.from_pretrained(model_id)

model.dequantize()

text = tokenizer("Hello my name is", return_tensors="pt").to(0)

out = model.generate(**text)
print(tokenizer.decode(out[0]))
```

----------------------------------------

TITLE: FSDP Configuration Example
DESCRIPTION: Complete YAML configuration example for FSDP showing all major settings including sharding strategy, CPU offloading, and wrapping policies.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/fsdp.md#2025-04-22_snippet_4

LANGUAGE: yaml
CODE:
```
compute_environment: LOCAL_MACHINE
debug: false
distributed_type: FSDP
downcast_bf16: "no"
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch_policy: BACKWARD_PRE
  fsdp_cpu_ram_efficient_loading: true
  fsdp_forward_prefetch: false
  fsdp_offload_params: true
  fsdp_sharding_strategy: 1
  fsdp_state_dict_type: SHARDED_STATE_DICT
  fsdp_sync_module_states: true
  fsdp_transformer_layer_cls_to_wrap: BertLayer
  fsdp_use_orig_params: true
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 2
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

----------------------------------------

TITLE: Audio Analysis Inference with Qwen2-Audio
DESCRIPTION: This snippet shows how to perform audio analysis by providing both audio and text instructions to the Qwen2-Audio model. The model analyzes and responds to the content. Dependencies include the transformers library for handling the model and librosa for managing audio data.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/qwen2_audio.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from io import BytesIO
from urllib.request import urlopen
import librosa
from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor

processor = AutoProcessor.from_pretrained("Qwen/Qwen2-Audio-7B-Instruct")
model = Qwen2AudioForConditionalGeneration.from_pretrained("Qwen/Qwen2-Audio-7B-Instruct", device_map="auto")

conversation = [
    {'role': 'system', 'content': 'You are a helpful assistant.'},
    {"role": "user", "content": [
        {"type": "audio", "audio_url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3"},
        {"type": "text", "text": "What's that sound?"},
    ]},
    {"role": "assistant", "content": "It is the sound of glass shattering."},
    {"role": "user", "content": [
        {"type": "text", "text": "What can you do when you hear that?"},
    ]},
    {"role": "assistant", "content": "Stay alert and cautious, and check if anyone is hurt or if there is any damage to property."},
    {"role": "user", "content": [
        {"type": "audio", "audio_url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac"},
        {"type": "text", "text": "What does the person say?"},
    ]},
]
text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
audios = []
for message in conversation:
    if isinstance(message["content"], list):
        for ele in message["content"]:
            if ele["type"] == "audio":
                audios.append(
                    librosa.load(
                        BytesIO(urlopen(ele['audio_url']).read()),
                        sr=processor.feature_extractor.sampling_rate)[0]
                )

inputs = processor(text=text, audios=audios, return_tensors="pt", padding=True)
inputs.input_ids = inputs.input_ids.to("cuda")

generate_ids = model.generate(**inputs, max_length=256)
generate_ids = generate_ids[:, inputs.input_ids.size(1):]

response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
```

----------------------------------------

TITLE: Creating T5 Configuration
DESCRIPTION: Creates a T5 model configuration based on google/t5-v1_1-base with custom vocabulary size.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/README.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
from transformers import T5Config

config = T5Config.from_pretrained("google/t5-v1_1-base", vocab_size=tokenizer.get_vocab_size())
config.save_pretrained("./norwegian-t5-base")
```

----------------------------------------

TITLE: Token and Label Alignment Processing
DESCRIPTION: Function to align tokenized inputs with their corresponding labels while handling special tokens and subword tokenization
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/tasks/token_classification.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> def tokenize_and_align_labels(examples):
...     tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)
...     labels = []
...     for i, label in enumerate(examples[f"ner_tags"]):
...         word_ids = tokenized_inputs.word_ids(batch_index=i)
...         previous_word_idx = None
...         label_ids = []
...         for word_idx in word_ids:
...             if word_idx is None:
...                 label_ids.append(-100)
...             elif word_idx != previous_word_idx:
...                 label_ids.append(label[word_idx])
...             else:
...                 label_ids.append(-100)
...             previous_word_idx = word_idx
...         labels.append(label_ids)
...     tokenized_inputs["labels"] = labels
...     return tokenized_inputs
```

----------------------------------------

TITLE: Initializing PyTorch Model with Custom Configuration
DESCRIPTION: Shows how to create a new DistilBertModel with a custom configuration, resulting in a model with random weights that would need training before being useful for tasks.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/create_a_model.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import DistilBertModel

>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/my_config.json")
>>> model = DistilBertModel(my_config)
```

----------------------------------------

TITLE: Loading Pre-trained Model with Custom Configuration in TensorFlow
DESCRIPTION: Demonstrates how to load a pre-trained DistilBert model while overriding its default configuration with custom settings in TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/create_a_model.md#2025-04-23_snippet_10

LANGUAGE: python
CODE:
```
>>> tf_model = TFDistilBertModel.from_pretrained("distilbert/distilbert-base-uncased", config=my_config)
```

----------------------------------------

TITLE: ZeRO-0 Configuration in DeepSpeed
DESCRIPTION: This JSON configuration demonstrates ZeRO stage 0 optimization, which essentially disables ZeRO and uses DeepSpeed as DDP. It provides a baseline configuration for comparison.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/main_classes/deepspeed.md#2025-04-22_snippet_25

LANGUAGE: json
CODE:
```
{
    "zero_optimization": {
        "stage": 0
    }
}
```

----------------------------------------

TITLE: Loading Pre-trained Configuration with Custom Parameters
DESCRIPTION: Demonstrates loading a pre-trained DistilBERT configuration while overriding specific parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/create_a_model.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
my_config = DistilBertConfig.from_pretrained("distilbert/distilbert-base-uncased", activation="relu", attention_dropout=0.4)
```

----------------------------------------

TITLE: Updating Local Transformers Installation
DESCRIPTION: Commands to update a local installation of Transformers to the latest version from the repository.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/installation.md#2025-04-22_snippet_8

LANGUAGE: bash
CODE:
```
cd ~/transformers/
```

LANGUAGE: bash
CODE:
```
git pull
```

----------------------------------------

TITLE: Setting up Optimizer and Learning Rate for TensorFlow
DESCRIPTION: Prepares the optimizer and learning rate schedule for fine-tuning with TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/multiple_choice.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers import create_optimizer

>>> batch_size = 16
>>> num_train_epochs = 2
>>> total_train_steps = (len(tokenized_swag["train"]) // batch_size) * num_train_epochs
>>> optimizer, schedule = create_optimizer(init_lr=5e-5, num_warmup_steps=0, num_train_steps=total_train_steps)
```

----------------------------------------

TITLE: Installing Intel Extension for PyTorch via pip
DESCRIPTION: This code snippet demonstrates how to install the Intel Extension for PyTorch (IPEX) using pip. Ensure to specify the correct version of PyTorch found in the previous step. This installation may be required to utilize optimizations available when training on Intel CPUs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_cpu.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install intel_extension_for_pytorch==<version_name> -f https://developer.intel.com/ipex-whl-stable-cpu
```

----------------------------------------

TITLE: Using BERT with Character Tokenization in Python
DESCRIPTION: This code snippet demonstrates how to use a BERT model trained on Japanese text with character tokenization. It imports necessary libraries from the Transformers library and PyTorch, loads the pre-trained model and tokenizer, tokenizes a Japanese sentence, and prints the decoded input.  Requires `transformers` and `torch`.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bert-japanese.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> bertjapanese = AutoModel.from_pretrained("cl-tohoku/bert-base-japanese-char")
>>> tokenizer = AutoTokenizer.from_pretrained("cl-tohoku/bert-base-japanese-char")

>>> ## Input Japanese Text
>>> line = "å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€‚"

>>> inputs = tokenizer(line, return_tensors="pt")

>>> print(tokenizer.decode(inputs["input_ids"][0]))
[CLS] å¾ è¼© ã¯ çŒ« ã§ ã‚ ã‚‹ ã€‚ [SEP]

>>> outputs = bertjapanese(**inputs)
```

----------------------------------------

TITLE: Running Image-Text Retrieval with BridgeTower in Python
DESCRIPTION: This code snippet provides an example of using the BridgeTowerProcessor and BridgeTowerForImageAndTextRetrieval classes to perform image-text retrieval. It involves processing the image and text, followed by a forward pass to evaluate the model's output logits.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/bridgetower.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
>>> from transformers import BridgeTowerProcessor, BridgeTowerForImageAndTextRetrieval
>>> import requests
>>> from PIL import Image

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> texts = ["An image of two cats chilling on a couch", "A football player scoring a goal"]

>>> processor = BridgeTowerProcessor.from_pretrained("BridgeTower/bridgetower-base-itm-mlm")
>>> model = BridgeTowerForImageAndTextRetrieval.from_pretrained("BridgeTower/bridgetower-base-itm-mlm")

>>> # forward pass
>>> scores = dict()
>>> for text in texts:
...     # prepare inputs
...     encoding = processor(image, text, return_tensors="pt")
...     outputs = model(**encoding)
...     scores[text] = outputs.logits[0, 1].item()
```

----------------------------------------

TITLE: Batched Text OCR with GOT-OCR2 in PyTorch
DESCRIPTION: This snippet shows how to process multiple images in a batch for OCR, which can be more efficient than processing images one at a time. It demonstrates loading multiple images and extracting text from all of them simultaneously.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/got_ocr2.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor, AutoModelForImageTextToText
>>> import torch

>>> device = "cuda" if torch.cuda.is_available() else "cpu"
>>> model = AutoModelForImageTextToText.from_pretrained("stepfun-ai/GOT-OCR-2.0-hf", device_map=device)
>>> processor = AutoProcessor.from_pretrained("stepfun-ai/GOT-OCR-2.0-hf", use_fast=True)

>>> image1 = "https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/multi_box.png"
>>> image2 = "https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/image_ocr.jpg"

>>> inputs = processor([image1, image2], return_tensors="pt", device=device).to(device)

>>> generate_ids = model.generate(
...     **inputs,
...     do_sample=False,
...     tokenizer=processor.tokenizer,
...     stop_strings="<|im_end|>",
...     max_new_tokens=4,
... )

>>> processor.batch_decode(generate_ids[:, inputs["input_ids"].shape[1] :], skip_special_tokens=True)
["Reducing the number", "R&D QUALITY"]
```

----------------------------------------

TITLE: Calculating Perplexity using Sliding Window Approach with GPT-2
DESCRIPTION: Implements a sliding window strategy to calculate perplexity on the WikiText-2 dataset with GPT-2. This approach allows the model to have more context for each token prediction, resulting in a more accurate perplexity score.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/perplexity.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch
from tqdm import tqdm

max_length = model.config.n_positions
stride = 512
seq_len = encodings.input_ids.size(1)

nlls = []
prev_end_loc = 0
for begin_loc in tqdm(range(0, seq_len, stride)):
    end_loc = min(begin_loc + max_length, seq_len)
    trg_len = end_loc - prev_end_loc  # ë§ˆì§€ë§‰ ë£¨í”„ì˜ ìŠ¤íŠ¸ë¼ì´ë“œ ê°’ê³¼ ë‹¤ë¥¼ ìˆ˜ ìžˆìŒ
    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)
    target_ids = input_ids.clone()
    target_ids[:, :-trg_len] = -100

    with torch.no_grad():
        outputs = model(input_ids, labels=target_ids)

        # ì†ì‹¤ì€ ëª¨ë“  ìœ íš¨í•œ ë ˆì´ë¸”ì— ëŒ€í•œ í‰ê· ê°’ì„ êµ¬í•˜ëŠ” êµì°¨ ì—”íŠ¸ë¡œí”¼(cross entropy)ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.
        # ë‚˜ì´ë¸Œ ë² ì´ì§€ì•ˆ ëª¨ë¸ì€ ë‚´ë¶€ì ìœ¼ë¡œ ë ˆì´ë¸”ì„ ì™¼ìª½ìœ¼ë¡œ 1ê°œì”© ë°€ê¸° ë•Œë¬¸ì—, (íƒ€ì¼“ - 1)ê°œ ë§Œí¼ì˜ ë ˆì´ë¸”ì— ëŒ€í•´ ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        neg_log_likelihood = outputs.loss

    nlls.append(neg_log_likelihood)

    prev_end_loc = end_loc
    if end_loc == seq_len:
        break

ppl = torch.exp(torch.stack(nlls).mean())
```

----------------------------------------

TITLE: Skipping Specific Modules During 8-bit Quantization
DESCRIPTION: Code demonstrating how to skip specific modules during 8-bit quantization, which can be useful for certain models where quantizing all modules causes instability.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/bitsandbytes.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

quantization_config = BitsAndBytesConfig(
    llm_int8_skip_modules=["lm_head"],
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    quantization_config=quantization_config,
)
```

----------------------------------------

TITLE: Loading DistilBert Configuration from File
DESCRIPTION: This snippet loads a `DistilBertConfig` from a JSON file located in a specified directory. It uses the `from_pretrained` method to load the config.  The path to the config.json file is required.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/create_a_model.md#_snippet_4

LANGUAGE: python
CODE:
```
>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/config.json")
```

----------------------------------------

TITLE: Generating Masks in Segment Everything Mode
DESCRIPTION: In this snippet, we use the initialized mask generator to produce masks from an image. The parameters `points_per_batch` and `pred_iou_thresh` control the batching of points and the IoU confidence threshold respectively.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/mask_generation.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
masks = mask_generator(image, points_per_batch=128, pred_iou_thresh=0.88)
```

----------------------------------------

TITLE: Loading Custom ResNet Configuration from Disk in Python
DESCRIPTION: Shows how to load a previously saved custom ResNet configuration using the from_pretrained method, which retrieves the configuration from a local directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/custom_models.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
resnet50d_config = ResnetConfig.from_pretrained("custom-resnet")
```

----------------------------------------

TITLE: Performing Inference with Fine-Tuned Image Captioning Model in Python
DESCRIPTION: Load a sample test image, preprocess it, and generate captions using the fine-tuned model, illustrating model performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/image_captioning.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
from PIL import Image
import requests

url = "https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/pokemon.png"
image = Image.open(requests.get(url, stream=True).raw)
image
```

LANGUAGE: python
CODE:
```
from accelerate.test_utils.testing import get_backend
# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)
device, _, _ = get_backend()
inputs = processor(images=image, return_tensors="pt").to(device)
pixel_values = inputs.pixel_values
```

LANGUAGE: python
CODE:
```
generated_ids = model.generate(pixel_values=pixel_values, max_length=50)
generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(generated_caption)
```

LANGUAGE: bash
CODE:
```
a drawing of a pink and blue pokemon
```

----------------------------------------

TITLE: Optimized XLA Text Generation with Padding
DESCRIPTION: Enhanced version of text generation that uses padding to maintain consistent input shapes for better XLA performance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/tf_xla.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2", padding_side="left", pad_token="</s>")
model = TFAutoModelForCausalLM.from_pretrained("openai-community/gpt2")
input_string = ["TensorFlow is"]

xla_generate = tf.function(model.generate, jit_compile=True)

# Here, we call the tokenizer with padding options.
tokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors="tf")

generated_tokens = xla_generate(**tokenized_input, num_beams=2)
decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)
print(f"Generated -- {decoded_text}")
```

----------------------------------------

TITLE: Combining Callbacks
DESCRIPTION: This combines the metric and push-to-hub callbacks into a single list.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/multiple_choice.md#_snippet_20

LANGUAGE: python
CODE:
```
>>> callbacks = [metric_callback, push_to_hub_callback]
```

----------------------------------------

TITLE: Creating a Custom Data Collator for Video Batches in Python
DESCRIPTION: Defines a collate_fn to batch video examples together. Handles pixel values and labels, performing necessary permutations on the video tensor.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
>>> def collate_fn(examples):
...     # permute to (num_frames, num_channels, height, width)
...     pixel_values = torch.stack(
...         [example["video"].permute(1, 0, 2, 3) for example in examples]
...     )
...     labels = torch.tensor([example["label"] for example in examples])
...     return {"pixel_values": pixel_values, "labels": labels}
```

----------------------------------------

TITLE: Padding Audio Sequences
DESCRIPTION: This code defines a preprocessing function that pads audio sequences to a uniform length. It iterates through audio examples, extracts the audio array, applies padding using the feature extractor, and returns the processed input.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/feature_extractors.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
```py
def preprocess_function(examples):
    audio_arrays = [x["array"] for x in examples["audio"]]
    inputs = feature_extractor(
        audio_arrays,
        sampling_rate=16000,
        padding=True,
    )
    return inputs

processed_dataset = preprocess_function(dataset[:5])
processed_dataset["input_values"][0].shape
(86699,)

processed_dataset["input_values"][1].shape
(86699,)
```
```

----------------------------------------

TITLE: Filtering Dataset by Speaker Example Count
DESCRIPTION: Filters the dataset to include only speakers with between 100 and 400 examples for better balance and training efficiency.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/text-to-speech.md#2025-04-23_snippet_8

LANGUAGE: python
CODE:
```
def select_speaker(speaker_id):
    return 100 <= speaker_counts[speaker_id] <= 400

dataset = dataset.filter(select_speaker, input_columns=["speaker_id"])

print(len(set(dataset["speaker_id"])))
print(len(dataset))
```

----------------------------------------

TITLE: Loading an Image Processor from a Pretrained Model
DESCRIPTION: This code snippet shows how to load an image processor using `AutoImageProcessor.from_pretrained`.  It loads the image processor associated with the 'google/vit-base-patch16-224' pretrained model, which handles image preprocessing steps such as resizing and normalization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_20

LANGUAGE: Python
CODE:
```
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
```

----------------------------------------

TITLE: Loading T5v1.1 Model in Python
DESCRIPTION: This snippet demonstrates how to load a pre-trained T5v1.1 model using the Transformers library. It shows that the weights of T5v1.1 can be directly plugged into a T5 model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/t5v1.1.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from transformers import T5ForConditionalGeneration

model = T5ForConditionalGeneration.from_pretrained("google/t5-v1_1-base")
```

----------------------------------------

TITLE: Save Custom Configuration
DESCRIPTION: This snippet demonstrates how to create an instance of the `ResnetConfig` and save it to a local directory. The `save_pretrained` method saves the configuration as a `config.json` file within the specified directory (`custom-resnet`).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/custom_models.md#_snippet_1

LANGUAGE: python
CODE:
```
resnet50d_config = ResnetConfig(block_type="bottleneck", stem_width=32, stem_type="deep", avg_down=True)
resnet50d_config.save_pretrained("custom-resnet")
```

----------------------------------------

TITLE: Accessing Index File of Sharded Checkpoints - Transformers - Python
DESCRIPTION: This snippet shows how to access and parse the index file created when saving sharded checkpoints of a model. The index file includes metadata and a weight map that associates parameters with their respective shards.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/models.md#2025-04-22_snippet_10

LANGUAGE: Python
CODE:
```
import json

with tempfile.TemporaryDirectory() as tmp_dir:
    model.save_pretrained(tmp_dir, max_shard_size="5GB")
    with open(os.path.join(tmp_dir, "model.safetensors.index.json"), "r") as f:
        index = json.load(f)

print(index.keys())
```

----------------------------------------

TITLE: Launching Training with Inline Accelerate Configuration
DESCRIPTION: Command line example showing how to launch training with Accelerate while specifying FSDP configuration parameters directly in the command line without using a separate config file.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/trainer.md#2025-04-22_snippet_15

LANGUAGE: bash
CODE:
```
accelerate launch --num_processes=2 \
    --use_fsdp \
    --mixed_precision=bf16 \
    --fsdp_auto_wrap_policy=TRANSFORMER_BASED_WRAP  \
    --fsdp_transformer_layer_cls_to_wrap="BertLayer" \
    --fsdp_sharding_strategy=1 \
    --fsdp_state_dict_type=FULL_STATE_DICT \
    ./examples/pytorch/text-classification/run_glue.py
    --model_name_or_path bert-base-cased \
    --task_name $TASK_NAME \
    --do_train \
    --do_eval \
    --max_seq_length 128 \
    --per_device_train_batch_size 16 \
    --learning_rate 5e-5 \
    --num_train_epochs 3 \
    --output_dir /tmp/$TASK_NAME/ \
    --overwrite_output_dir
```

----------------------------------------

TITLE: Setting Up DataCollator for Language Modeling in TensorFlow
DESCRIPTION: Configures the DataCollatorForLanguageModeling for TensorFlow, disabling masked language modeling and setting the return tensors to TensorFlow format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/language_modeling.md#2025-04-23_snippet_6

LANGUAGE: python
CODE:
```
>>> from transformers import DataCollatorForLanguageModeling

>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors="tf")
```

----------------------------------------

TITLE: Run summarization script on TPU (TensorFlow)
DESCRIPTION: This code shows how to run the TensorFlow summarization script on a TPU. It requires passing the name of the TPU resource to the `--tpu` argument. Other arguments specify the model, dataset, and training parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/run_scripts.md#_snippet_7

LANGUAGE: bash
CODE:
```
python run_summarization.py  \
    --tpu name_of_tpu_resource \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval
```

----------------------------------------

TITLE: Loading Tokenizer Object Directly into PreTrainedTokenizerFast
DESCRIPTION: This code snippet demonstrates how to load a tokenizer object directly into the PreTrainedTokenizerFast class from the ðŸ¤— Transformers library. This allows you to leverage the functionality of the ðŸ¤— Tokenizers library within the ðŸ¤— Transformers ecosystem.  The `tokenizer` object from the previous snippet is used here.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/fast_tokenizers.md#_snippet_1

LANGUAGE: Python
CODE:
```
>>> from transformers import PreTrainedTokenizerFast

>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)
```

----------------------------------------

TITLE: Resampling Audio Data
DESCRIPTION: This snippet shows how to resample audio data using the `datasets` library. It uses the `cast_column` function to change the sampling rate of the audio column to 16kHz.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/feature_extractors.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
```py
dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
```
```

----------------------------------------

TITLE: Installing Accelerate in Bash
DESCRIPTION: This Bash command installs the 'accelerate' library, which is required for managing distributed training setups, including FSDP. Ensure that you have Python and pip installed before running this command.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/fsdp.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install accelerate
```

----------------------------------------

TITLE: Training GPT-2 with NVLink Enabled
DESCRIPTION: Example of training the GPT-2 model on wikitext dataset using Distributed Data Parallel (DDP) with NVLink enabled, showing the default high-performance configuration with two GPUs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/perf_hardware.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
rm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 torchrun \
--nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py --model_name_or_path openai-community/gpt2 \
--dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --do_train \
--output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200
```

----------------------------------------

TITLE: Running Summarization Training Script in TensorFlow
DESCRIPTION: Command to run a TensorFlow-based summarization example script, fine-tuning T5-small on the CNN/DailyMail dataset using Keras.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/run_scripts.md#2025-04-23_snippet_4

LANGUAGE: bash
CODE:
```
python examples/tensorflow/summarization/run_summarization.py  \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --output_dir /tmp/tst-summarization  \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --num_train_epochs 3 \
    --do_train \
    --do_eval
```

----------------------------------------

TITLE: Inference on Single Image using Mistral3
DESCRIPTION: This code snippet demonstrates how to perform inference on a single image with the Mistral3 model using chat templates. It initializes the processor and model from the specified checkpoint and then processes the input image and text using the `apply_chat_template` method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/mistral3.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor, AutoModelForImageTextToText
>>> import torch

>>> torch_device = "cuda"
>>> model_checkpoint = "mistralai/Mistral-Small-3.1-24B-Instruct-2503"
>>> processor = AutoProcessor.from_pretrained(model_checkpoint)
>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)

>>> messages = [
...     {
...         "role": "user",
...         "content": [
...             {"type": "image", "url": "http://images.cocodataset.org/val2017/000000039769.jpg"},
...             {"type": "text", "text": "Describe this image"},
...         ],
...     }
... ]

>>> inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors="pt").to(model.device, dtype=torch.bfloat16)

>>> generate_ids = model.generate(**inputs, max_new_tokens=20)
>>> decoded_output = processor.decode(generate_ids[0, inputs["input_ids"].shape[1] :], skip_special_tokens=True)

>>> decoded_output
"The image depicts two cats lying on a pink blanket. The larger cat, which appears to be an"...
```

----------------------------------------

TITLE: Accessing Resampled Audio Data
DESCRIPTION: Accesses the audio data after resampling using the `cast_column` function.  It retrieves the resampled audio data (array), file path, and sampling rate.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/preprocessing.md#_snippet_12

LANGUAGE: python
CODE:
```
>>> dataset[0]["audio"]
{'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,
         3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',
 'sampling_rate': 16000}
```

----------------------------------------

TITLE: Custom Chat Template with Role-Specific Formatting
DESCRIPTION: A more complex Jinja template that adds custom tags for different message roles, including user, system, and assistant messages
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating_writing.md#2025-04-22_snippet_1

LANGUAGE: jinja
CODE:
```
{%- for message in messages %}
    {%- if message['role'] == 'user' %}
        {{- bos_token + '[INST] ' + message['content'].strip() + ' [/INST]' }}
    {%- elif message['role'] == 'system' %}
        {{- '<<SYS>>\n' + message['content'].strip() + '\n<</SYS>>\n\n' }}
    {%- elif message['role'] == 'assistant' %}
        {{- '[ASST] '  + message['content'] + ' [/ASST]' + eos_token }}
    {%- endif %}
{%- endfor %}
```

----------------------------------------

TITLE: Using Custom Dataset for GPTQ Quantization
DESCRIPTION: Demonstrates how to use a custom dataset for GPTQ quantization instead of the default C4 dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/quantization/gptq.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
dataset = ["auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm."]
gptq_config = GPTQConfig(bits=4, dataset=dataset, tokenizer=tokenizer)
```

----------------------------------------

TITLE: Implementing Preprocess Method
DESCRIPTION: Example implementation of the preprocess method that transforms inputs into model-compatible format.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/add_new_pipeline.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
def preprocess(self, inputs, maybe_arg=2):
    model_input = Tensor(inputs["input_ids"])
    return {"model_input": model_input}
```

----------------------------------------

TITLE: Running Multi-Objective Hyperparameter Optimization with Optuna in Python
DESCRIPTION: Example of running a multi-objective hyperparameter optimization using Optuna backend with the Trainer API.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/hpo_train.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
best_trials = trainer.hyperparameter_search(
    direction=["minimize", "maximize"],
    backend="optuna",
    hp_space=optuna_hp_space,
    n_trials=20,
    compute_objective=compute_objective,
)
```

----------------------------------------

TITLE: Initializing TAPAS with TensorFlow in Python
DESCRIPTION: This snippet demonstrates how to initialize a TAPAS model for question answering using TensorFlow, suitable for models that require TensorFlow backend handling.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/tapas.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers import TapasConfig, TFTapasForQuestionAnswering

>>> # for example, the base sized model with default SQA configuration
>>> model = TFTapasForQuestionAnswering.from_pretrained("google/tapas-base")

```

----------------------------------------

TITLE: Saving and Re-exporting to GGUF (Python)
DESCRIPTION: This code snippet demonstrates how to save a loaded model and tokenizer and then convert it back to the GGUF format using a conversion script from `llama.cpp`. It requires the model and tokenizer to be saved to a directory first, and then calls an external Python script to perform the GGUF conversion.  The `${path_to_llama_cpp}` and `${directory}` need to be properly defined for the command to work.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/gguf.md#_snippet_1

LANGUAGE: python
CODE:
```
tokenizer.save_pretrained('directory')
model.save_pretrained('directory')

!python ${path_to_llama_cpp}/convert-hf-to-gguf.py ${directory}
```

----------------------------------------

TITLE: Saving Tokenizer to JSON File
DESCRIPTION: This snippet shows how to save a tokenizer object created with the `tokenizers` library to a JSON file. The saved tokenizer can then be loaded later for reuse. The input is the `tokenizer` object, and the output is a `tokenizer.json` file.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/fast_tokenizers.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> tokenizer.save("tokenizer.json")
```

----------------------------------------

TITLE: Fine-tuning BERT with Custom Files for Token Classification
DESCRIPTION: This command fine-tunes a BERT base uncased model on custom training and validation files for token classification tasks, specifying input file paths and output directory.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/tensorflow/token-classification/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
python run_ner.py \
  --model_name_or_path google-bert/bert-base-uncased \
  --train_file path_to_train_file \
  --validation_file path_to_validation_file \
  --output_dir /tmp/test-ner
```

----------------------------------------

TITLE: Creating DistilBERT Model with Custom Config
DESCRIPTION: Shows how to initialize a DistilBERT model using a custom configuration.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/create_a_model.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import DistilBertModel

my_config = DistilBertConfig.from_pretrained("./your_model_save_path/my_config.json")
model = DistilBertModel(my_config)
```

----------------------------------------

TITLE: Setting Up Metrics Callback for TensorFlow Training
DESCRIPTION: Creates a Keras callback to compute and log evaluation metrics during training. The callback uses the provided compute_metrics function and validation dataset.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/sequence_classification.md#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
>>> from transformers.keras_callbacks import KerasMetricCallback

>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)
```

----------------------------------------

TITLE: Adding Callback to Existing Trainer
DESCRIPTION: Alternative method for registering a callback by using the add_callback() method on an existing Trainer instance.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/callback.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
trainer = Trainer(...)
trainer.add_callback(MyCallback)
# Alternatively, we can pass an instance of the callback class
trainer.add_callback(MyCallback())
```

----------------------------------------

TITLE: Running translation example with DeepSpeed
DESCRIPTION: Example command to run translation task using DeepSpeed with all available GPUs.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/deepspeed.md#2025-04-22_snippet_8

LANGUAGE: bash
CODE:
```
deepspeed examples/pytorch/translation/run_translation.py \
--deepspeed tests/deepspeed/ds_config_zero3.json \
--model_name_or_path google-t5/t5-small --per_device_train_batch_size 1 \
--output_dir output_dir --overwrite_output_dir --fp16 \
--do_train --max_train_samples 500 --num_train_epochs 1 \
--dataset_name wmt16 --dataset_config "ro-en" \
--source_lang en --target_lang ro
```

----------------------------------------

TITLE: Defining CamembertTokenizer
DESCRIPTION: This snippet defines the CamemBERT tokenizer, which is responsible for processing input text into a format suitable for the model, including generating special tokens and masking.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/camembert.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
[[autodoc]] CamembertTokenizer
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - save_vocabulary
```

----------------------------------------

TITLE: Evaluating Fine-tuned LongT5 on PubMed Dataset
DESCRIPTION: Example code demonstrating how to evaluate a fine-tuned LongT5 model on the PubMed scientific papers dataset using the Hugging Face transformers library. Shows loading the model, tokenizing input, generating predictions, and computing ROUGE scores.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import evaluate
from datasets import load_dataset
from transformers import AutoTokenizer, LongT5ForConditionalGeneration

dataset = load_dataset("scientific_papers", "pubmed", split="validation")
model = (
    LongT5ForConditionalGeneration.from_pretrained("Stancld/longt5-tglobal-large-16384-pubmed-3k_steps")
    .to("cuda")
    .half()
)
tokenizer = AutoTokenizer.from_pretrained("Stancld/longt5-tglobal-large-16384-pubmed-3k_steps")


def generate_answers(batch):
    inputs_dict = tokenizer(
        batch["article"], max_length=16384, padding="max_length", truncation=True, return_tensors="pt"
    )
    input_ids = inputs_dict.input_ids.to("cuda")
    attention_mask = inputs_dict.attention_mask.to("cuda")
    output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=512, num_beams=2)
    batch["predicted_abstract"] = tokenizer.batch_decode(output_ids, skip_special_tokens=True)
    return batch


result = dataset.map(generate_answer, batched=True, batch_size=2)
rouge = evaluate.load("rouge")
rouge.compute(predictions=result["predicted_abstract"], references=result["abstract"])
```

----------------------------------------

TITLE: Creating and Compiling Model in Strategy Scope
DESCRIPTION: This code snippet creates a sequence classification model inside the TPUStrategy scope. This ensures that the model layers are replicated on each TPU device. It also compiles the model using the Adam optimizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
"from transformers import TFAutoModelForSequenceClassification\n\nwith strategy.scope():\n    model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n    model.compile(optimizer=\"adam\")"
```

----------------------------------------

TITLE: Removing Images with Out-of-Bounds Bounding Boxes
DESCRIPTION: This code snippet removes images from the training set that contain bounding boxes extending beyond the image boundaries. These "runaway" bounding boxes can cause errors during training and are thus removed.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/object_detection.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
">>> remove_idx = [590, 821, 822, 875, 876, 878, 879]
>>> keep = [i for i in range(len(cppe5["train"])) if i not in remove_idx]
>>> cppe5["train"] = cppe5["train"].select(keep)"
```

----------------------------------------

TITLE: Compiling T5 Model for Training in TensorFlow
DESCRIPTION: Compiles the T5 model for training in TensorFlow, specifying the optimizer.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/translation.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
import tensorflow as tf

model.compile(optimizer=optimizer)  # No loss argument!
```

----------------------------------------

TITLE: Custom Trainer with Weighted Loss Implementation
DESCRIPTION: Example showing how to customize the Trainer class to use weighted loss for imbalanced training sets by overriding the compute_loss method.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/trainer.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from torch import nn
from transformers import Trainer


class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        # forward pass
        outputs = model(**inputs)
        logits = outputs.get("logits")
        # compute custom loss (suppose one has 3 labels with different weights)
        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0], device=model.device))
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss
```

----------------------------------------

TITLE: Loading and Generating Output with Moshi in Python
DESCRIPTION: This snippet demonstrates how to load a dataset, prepare input values, and generate text and audio outputs using the Moshi model in PyTorch. It requires PyTorch, the HuggingFace Transformers library, and a compatible GPU device. The snippet shows preparation of user and model input, execution of the generation process, and retrieval of output sequences.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/moshi.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
>>> from datasets import load_dataset, Audio
>>> import torch, math
>>> from transformers import MoshiForConditionalGeneration, AutoFeatureExtractor, AutoTokenizer


>>> librispeech_dummy = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> feature_extractor = AutoFeatureExtractor.from_pretrained("kyutai/moshiko-pytorch-bf16")
>>> tokenizer = AutoTokenizer.from_pretrained("kyutai/moshiko-pytorch-bf16")
>>> device = "cuda"
>>> dtype = torch.bfloat16

>>> # prepare user input audio 
>>> librispeech_dummy = librispeech_dummy.cast_column("audio", Audio(sampling_rate=feature_extractor.sampling_rate))
>>> audio_sample = librispeech_dummy[-1]["audio"]["array"]
>>> user_input_values = feature_extractor(raw_audio=audio_sample, sampling_rate=feature_extractor.sampling_rate, return_tensors="pt").to(device=device, dtype=dtype)

>>> # prepare moshi input values - we suppose moshi didn't say anything while the user spoke
>>> moshi_input_values = torch.zeros_like(user_input_values.input_values)

>>> # prepare moshi input ids - we suppose moshi didn't say anything while the user spoke
>>> num_tokens = math.ceil(moshi_input_values.shape[-1] * waveform_to_token_ratio)
>>> input_ids = torch.ones((1, num_tokens), device=device, dtype=torch.int64) * tokenizer.encode("<pad>")[0]

>>> # generate 25 new tokens (around 2s of audio)
>>> output = model.generate(input_ids=input_ids, user_input_values=user_input_values.input_values, moshi_input_values=moshi_input_values, max_new_tokens=25)

>>> text_tokens = output.sequences
>>> audio_waveforms = output.audio_sequences
```

----------------------------------------

TITLE: Fine-tuning BERT using Accelerate Library
DESCRIPTION: Script demonstrating how to fine-tune BERT on SQuAD using the Accelerate library for distributed training support across CPU, GPU, and TPU setups.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/README.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
accelerate launch run_qa_no_trainer.py \
  --model_name_or_path google-bert/bert-base-uncased \
  --dataset_name squad \
  --max_seq_length 384 \
  --doc_stride 128 \
  --output_dir ~/tmp/debug_squad
```

----------------------------------------

TITLE: Installing HQQ Latest Version via Pip
DESCRIPTION: This command installs the latest version of the HQQ library and builds its corresponding CUDA kernels. It is necessary for utilizing HQQ for model quantization. Ensure that CUDA is configured on your system to proceed with this installation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/hqq.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install hqq
```

----------------------------------------

TITLE: Loading ViLT Model for Question Answering
DESCRIPTION: Initializes the ViltForQuestionAnswering model with the correct number of classification labels and label mappings, preparing it for fine-tuning on VQA data.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/visual_question_answering.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
>>> from transformers import ViltForQuestionAnswering

>>> model = ViltForQuestionAnswering.from_pretrained(model_checkpoint, num_labels=len(id2label), id2label=id2label, label2id=label2id)
```

----------------------------------------

TITLE: Visual Question Answering with IDEFICS
DESCRIPTION: Shows how to perform visual question answering by providing specific instructions to the model. The example demonstrates answering questions about location and weather conditions from an image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> prompt = [
...     "Instruction: Provide an answer to the question. Use the image to answer.\n",
...     "https://images.unsplash.com/photo-1623944889288-cd147dbb517c?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80",
...     "Question: Where are these people and what's the weather like? Answer:"]

>>> inputs = processor(prompt, return_tensors="pt").to("cuda")
>>> bad_words_ids = processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, max_new_tokens=20, bad_words_ids=bad_words_ids)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> print(generated_text[0])
```

----------------------------------------

TITLE: Computing Similarity with Custom Embeddings
DESCRIPTION: Code to calculate the cosine similarity between the embeddings obtained from the AutoModel approach, demonstrating an alternative to using the pipeline.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_feature_extraction.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
from torch.nn.functional import cosine_similarity

similarity_score = cosine_similarity(embed_real, embed_gen, dim=1)
print(similarity_score)

# tensor([0.6061], device='cuda:0', grad_fn=<SumBackward1>)
```

----------------------------------------

TITLE: Loading Blenderbot Model and Tokenizer with Example Utterance
DESCRIPTION: This snippet demonstrates how to load the Blenderbot model and tokenizer and generate a reply for a given input utterance. It initializes the model using 'facebook/blenderbot-400M-distill', prepares the input data, and outputs the model's reply. Required dependencies include the 'transformers' library from Hugging Face.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/blenderbot.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration

>>> mname = "facebook/blenderbot-400M-distill"
>>> model = BlenderbotForConditionalGeneration.from_pretrained(mname)
>>> tokenizer = BlenderbotTokenizer.from_pretrained(mname)
>>> UTTERANCE = "My friends are cool but they eat too many carbs."
>>> inputs = tokenizer([UTTERANCE], return_tensors="pt")
>>> reply_ids = model.generate(**inputs)
>>> print(tokenizer.batch_decode(reply_ids))
["<s> That's unfortunate. Are they trying to lose weight or are they just trying to be healthier?</s>"]
```

----------------------------------------

TITLE: Model Fit with PushToHubCallback - Python
DESCRIPTION: This snippet shows how to integrate the `PushToHubCallback` into the `fit` method for training a TensorFlow model, which will automatically push the model to the Hub after training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/model_sharing.md#_snippet_10

LANGUAGE: python
CODE:
```
>>> model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3, callbacks=push_to_hub_callback)
```

----------------------------------------

TITLE: Fine-tuning Semantic Segmentation Model without Trainer using Accelerate in Bash
DESCRIPTION: This bash script shows how to use Accelerate to fine-tune a semantic segmentation model without using the Trainer API. It includes commands for configuration, testing, and launching the training process.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/semantic-segmentation/README.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
accelerate config
accelerate test
accelerate launch run_semantic_segmentation_no_trainer.py --output_dir segformer-finetuned-sidewalk --with_tracking --push_to_hub
```

----------------------------------------

TITLE: Post-processing the Super-Resolution Output
DESCRIPTION: Converts the model's output tensor into a displayable image by squeezing dimensions, moving axes, clipping values, and converting to the appropriate pixel value range (0-255).
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_to_image.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
import numpy as np

# í¬ê¸°ë¥¼ ì¤„ì´ê³ , CPUë¡œ ì´ë™í•˜ê³ , ê°’ì„ í´ë¦¬í•‘
output = outputs.reconstruction.data.squeeze().cpu().clamp_(0, 1).numpy()
# ì¶•ì„ ìž¬ì •ë ¬
output = np.moveaxis(output, source=0, destination=-1)
# ê°’ì„ í”½ì…€ê°’ ë²”ìœ„ë¡œ ë˜ëŒë¦¬ê¸°
output = (output * 255.0).round().astype(np.uint8)
Image.fromarray(output)
```

----------------------------------------

TITLE: Generating Music with Text Prompts using Musicgen in Python
DESCRIPTION: This Python snippet demonstrates generating music from text prompts using Musicgen's 'MusicgenMelodyForConditionalGeneration'. It requires the 'transformers' library, and uses a pre-trained model to transform text descriptions into audio sequences. The key parameters include 'do_sample' and 'guidance_scale', controlling the sampling strategy and prompt conditioning strength, respectively.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/musicgen_melody.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor, MusicgenMelodyForConditionalGeneration

>>> processor = AutoProcessor.from_pretrained("facebook/musicgen-melody")
>>> model = MusicgenMelodyForConditionalGeneration.from_pretrained("facebook/musicgen-melody")

>>> inputs = processor(
...     text=["80s pop track with bassy drums and synth", "90s rock song with loud guitars and heavy drums"],
...     padding=True,
...     return_tensors="pt",
... )
>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)
```

----------------------------------------

TITLE: Registering Custom ResNet Models for Auto Classes
DESCRIPTION: Registers the custom ResNet models for use with Hugging Face's auto classes.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/custom_models.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
ResnetConfig.register_for_auto_class()
ResnetModel.register_for_auto_class("AutoModel")
ResnetModelForImageClassification.register_for_auto_class("AutoModelForImageClassification")
```

----------------------------------------

TITLE: Accessing Model Output as a Tuple in Transformers
DESCRIPTION: This example shows how to access the first two elements of the model output as a tuple. It assumes that the model output contains at least a loss and logits attribute. Only non-None attributes are included in the tuple representation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/output.md#_snippet_1

LANGUAGE: python
CODE:
```
outputs[:2]
```

----------------------------------------

TITLE: Defining Olmo2Attention class in Python
DESCRIPTION: This snippet provides the start of an `Olmo2Attention` class definition, intended to mirror the structure of the original Olmo attention while allowing for modifications. It demonstrates how modular files streamline the inheritance and adaptation of model functionalities.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/modular_transformers.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from ..llama.modeling_llama import eager_attention_forward
from ..olmo.modeling_olmo import OlmoAttention, apply_rotary_pos_emb

# Olmo2 attention is identical to OLMo attention except:
# - Norm is applied to attention queries and keys.
```

----------------------------------------

TITLE: Running Summarization Training Script in PyTorch
DESCRIPTION: Command to run a PyTorch-based summarization example script, fine-tuning T5-small on the CNN/DailyMail dataset using the Trainer API.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/run_scripts.md#2025-04-23_snippet_3

LANGUAGE: bash
CODE:
```
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

----------------------------------------

TITLE: Loading and Preprocessing Image
DESCRIPTION: This code loads an image using `skimage`, converts it to a PIL Image object, and ensures it's in RGB format. This is necessary for compatibility with the OWL-ViT model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/zero_shot_object_detection.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
">>> import skimage
>>> import numpy as np
>>> from PIL import Image

>>> image = skimage.data.astronaut()
>>> image = Image.fromarray(np.uint8(image)).convert("RGB")

>>> image"
```

----------------------------------------

TITLE: Batch Processing Multiple Images
DESCRIPTION: This code demonstrates batch processing of multiple images with different text queries. It combines the astronaut image and the beach image, and provides a nested list of text queries for each image. The processor handles the batch processing of both images and text queries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/zero_shot_object_detection.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
">>> images = [image, im]
>>> text_queries = [
...     ["human face", "rocket", "nasa badge", "star-spangled banner"],
...     ["hat", "book", "sunglasses", "camera"],
... ]
>>> inputs = processor(text=text_queries, images=images, return_tensors=\"pt\")"
```

----------------------------------------

TITLE: Loading Pretrained DistilBERT Model with Custom Configuration in TensorFlow
DESCRIPTION: This snippet shows how to load a pretrained DistilBERT model while overriding its configuration with a custom one in TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/create_a_model.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
tf_model = TFDistilBertModel.from_pretrained("distilbert/distilbert-base-uncased", config=my_config)
```

----------------------------------------

TITLE: Installing Flash Attention 2 for GPTBigCode
DESCRIPTION: Command to install the latest version of Flash Attention 2, which includes the sliding window attention feature required for optimal performance with GPTBigCode.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/gpt_bigcode.md#2025-04-23_snippet_0

LANGUAGE: bash
CODE:
```
pip install -U flash-attn --no-build-isolation
```

----------------------------------------

TITLE: Initializing InformerConfig in Python
DESCRIPTION: Defines the configuration class for the Informer model, specifying various hyperparameters and architectural choices.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_doc/informer.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
[[autodoc]] InformerConfig
```

----------------------------------------

TITLE: Calculating Perplexity with GPT-2 and Sliding Window in Python
DESCRIPTION: This comprehensive example illustrates calculating the perplexity of a text sequence using GPT-2 and an efficient sliding-window approach. It outlines a method to compute negative log-likelihood with context overlap and adjust token targets to ignore prediction tokens during loss computation. This strategy, implemented in a loop with the Tensor library 'torch', enhances prediction accuracy.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perplexity.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
import torch
from tqdm import tqdm

max_length = model.config.n_positions
stride = 512
seq_len = encodings.input_ids.size(1)

nll_sum = 0.0
n_tokens = 0
prev_end_loc = 0
for begin_loc in tqdm(range(0, seq_len, stride)):
    end_loc = min(begin_loc + max_length, seq_len)
    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop
    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)
    target_ids = input_ids.clone()
    target_ids[:, :-trg_len] = -100

    with torch.no_grad():
        outputs = model(input_ids, labels=target_ids)

        # loss is calculated using CrossEntropyLoss which averages over valid labels
        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels
        # to the left by 1.
        neg_log_likelihood = outputs.loss

    # Accumulate the total negative log-likelihood and the total number of tokens
    num_valid_tokens = (target_ids != -100).sum().item()  # number of valid tokens in target_ids
    batch_size = target_ids.size(0)
    num_loss_tokens = num_valid_tokens - batch_size  # subtract batch_size due to internal label shift
    nll_sum += neg_log_likelihood * num_loss_tokens
    n_tokens += num_loss_tokens

    prev_end_loc = end_loc
    if end_loc == seq_len:
        break

avg_nll = nll_sum / n_tokens  # average negative log-likelihood per token
ppl = torch.exp(avg_nll)
```

----------------------------------------

TITLE: Setting Up Optimizer for TensorFlow MLM Fine-tuning
DESCRIPTION: Configures an AdamWeightDecay optimizer for fine-tuning a masked language model in TensorFlow.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/masked_language_modeling.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import create_optimizer, AdamWeightDecay

optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

----------------------------------------

TITLE: Configuring Audio Generation Parameters
DESCRIPTION: Describes inspecting and modifying the default configuration of the Musicgen model's generation parameters, including guidance scale and token length. This allows overriding default settings during the audio generation process within the 'transformers' library context.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/musicgen.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> from transformers import MusicgenForConditionalGeneration

>>> model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")

>>> # inspect the default generation config
>>> model.generation_config

>>> # increase the guidance scale to 4.0
>>> model.generation_config.guidance_scale = 4.0

>>> # decrease the max length to 256 tokens
>>> model.generation_config.max_length = 256
```

----------------------------------------

TITLE: Verifying Transformers Installation from Source
DESCRIPTION: This Python command verifies the installation of the `transformers` library from source by downloading a pre-trained model and performing sentiment analysis. It imports the `pipeline` function and uses it to analyze the text "I love you".
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/installation.md#_snippet_10

LANGUAGE: bash
CODE:
```
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))"
```

----------------------------------------

TITLE: Installing Required Libraries for Model Training Analysis
DESCRIPTION: Command to install the necessary libraries for transformer model training and GPU memory monitoring, including transformers, datasets, accelerate, and nvidia-ml-py3.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/model_memory_anatomy.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install transformers datasets accelerate nvidia-ml-py3
```

----------------------------------------

TITLE: Installing PEFT via pip
DESCRIPTION: Installs the PEFT package from Python Package Index or directly from its GitHub repository. This command is essential to access PEFT's features and methods.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/peft.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install -U peft
```

----------------------------------------

TITLE: Initializing BertConfig in Python
DESCRIPTION: Configures hyperparameters for the BERT model. BertConfig contains all the configuration options for the model architecture.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/bert.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
[[autodoc]] BertConfig
    - all
```

----------------------------------------

TITLE: Classifying Text with DistilBERT using Transformers CLI
DESCRIPTION: This snippet demonstrates how to use the command-line interface provided by the `transformers` library to perform text classification. It pipes an input string to the `transformers run` command, specifying the task (`text-classification`) and the pretrained DistilBERT model to use. This provides a quick way to perform inference without writing Python code.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/distilbert.md#_snippet_2

LANGUAGE: bash
CODE:
```
echo -e "I love using Hugging Face Transformers!" | transformers run --task text-classification --model distilbert-base-uncased-finetuned-sst-2-english
```

----------------------------------------

TITLE: M2M100 Multilingual Translation from Chinese to English
DESCRIPTION: Demonstrates loading and using the M2M100 model for translating text between Chinese and English, with source language configuration
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/multilingual.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

chinese_text = "ä¸è¦æ’æ‰‹å·«å¸«çš„äº‹å‹™, å› ç‚ºä»–å€‘æ˜¯å¾®å¦™çš„, å¾ˆå¿«å°±æœƒç™¼æ€’."

tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M", src_lang="zh")
model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")

encoded_zh = tokenizer(chinese_text, return_tensors="pt")
generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id("en"))
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
```

----------------------------------------

TITLE: Visualizing Segmentation Results
DESCRIPTION: This snippet demonstrates how to visualize the predicted segmentation results by applying a color palette to the segmentation map and overlaying it on the original image.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/semantic_segmentation.md#2025-04-22_snippet_31

LANGUAGE: python
CODE:
```
>>> import matplotlib.pyplot as plt
>>> import numpy as np

>>> color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)
>>> palette = np.array(ade_palette())
>>> for label, color in enumerate(palette):
...     color_seg[pred_seg == label, :] = color
>>> color_seg = color_seg[..., ::-1]  # convert to BGR

>>> img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map
>>> img = img.astype(np.uint8)

>>> plt.figure(figsize=(15, 10))
>>> plt.imshow(img)
>>> plt.show()
```

----------------------------------------

TITLE: Loading ResNet Backbone with Pretrained Weights in Python
DESCRIPTION: This snippet demonstrates how to load a ResNet backbone with pretrained weights for a MaskFormer model. It sets the `use_pretrained_backbone` parameter to `True` in the MaskFormerConfig. This will load the pretrained weights for the ResNet backbone.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ar/create_a_model.md#_snippet_20

LANGUAGE: python
CODE:
```
from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation

config = MaskFormerConfig(backbone="microsoft/resnet-50", use_pretrained_backbone=True) # ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ ÙˆØ§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ³ÙŠØ·
model = MaskFormerForInstanceSegmentation(config) # Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
```

----------------------------------------

TITLE: Loading and Encoding WikiText-2 Dataset in Python
DESCRIPTION: This code loads the WikiText-2 dataset and encodes it using the GPT-2 tokenizer for perplexity calculation.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/perplexity.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from datasets import load_dataset

test = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")
encodings = tokenizer("\n\n".join(test["text"]), return_tensors="pt")
```

----------------------------------------

TITLE: Creating an Optimizer with create_optimizer Function
DESCRIPTION: This snippet demonstrates how to use the create_optimizer function to create an optimizer for TensorFlow models. It likely sets up an AdamWeightDecay optimizer with specific parameters.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/optimizer_schedules.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
from transformers import create_optimizer
```

----------------------------------------

TITLE: Skipping Quantization for Specific Modules
DESCRIPTION: This example demonstrates using `AOPerModuleConfig` to apply a default quantization (Int4 weight-only) to most layers of a Llama-3.1-8B-Instruct model while explicitly skipping quantization for specified layers (`model.layers.0.self_attn.q_proj`) by setting their config to `None`. The model is loaded with the quantization config, and its structure is printed to verify skipped modules.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/torchao.md#_snippet_8

LANGUAGE: Python
CODE:
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig

model_id = "meta-llama/Llama-3.1-8B-Instruct"

from torchao.quantization import Int4WeightOnlyConfig, AOPerModuleConfig
config = Int4WeightOnlyConfig(group_size=128)

# set default to int4 (for linears), and skip quantizing `model.layers.0.self_attn.q_proj`
quant_config = AOPerModuleConfig({"_default": config, "model.layers.0.self_attn.q_proj": None})
quantization_config = TorchAoConfig(quant_type=quant_config)
quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype=torch.bfloat16, quantization_config=quantization_config)
# lm_head is not quantized and model.layers.0.self_attn.q_proj is not quantized
print("quantized model:", quantized_model)
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Manual Testing
prompt = "Hey, are you conscious? Can you talk to me?"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
generated_ids = quantized_model.generate(**inputs, max_new_tokens=128)
output_text = tokenizer.batch_decode(
    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
```

----------------------------------------

TITLE: MBart Multilingual Translation from Finnish to English
DESCRIPTION: Loading and using the MBart model for translating text between Finnish and English, with language-specific configurations
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/multilingual.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

fi_text = "Ã„lÃ¤ sekaannu velhojen asioihin, sillÃ¤ ne ovat hienovaraisia ja nopeasti vihaisia."

tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt", src_lang="fi_FI")
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")

encoded_en = tokenizer(fi_text, return_tensors="pt")
generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id("en_XX"))
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
```

----------------------------------------

TITLE: Collect Tests from a Specific File with Pytest
DESCRIPTION: Executes pytest with the --collect-only flag to list all tests in a specified test file. This aids in quickly identifying available test cases without running them. Requires pytest and the specific test file as prerequisites.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/testing.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
pytest tests/test_optimization.py --collect-only -q
```

----------------------------------------

TITLE: Loading Custom Model from Hugging Face Hub
DESCRIPTION: Loads a custom model from Hugging Face Hub using AutoModelForImageClassification, with trust_remote_code set to True.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/pt/custom_models.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
from transformers import AutoModelForImageClassification

model = AutoModelForImageClassification.from_pretrained("sgugger/custom-resnet50d", trust_remote_code=True)
```

----------------------------------------

TITLE: XLNet Tokenizer Implementation
DESCRIPTION: Example showing how to use XLNet tokenizer from the transformers library
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tokenizer_summary.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from transformers import XLNetTokenizer

tokenizer = XLNetTokenizer.from_pretrained("xlnet/xlnet-base-cased")
tokenizer.tokenize("Don't you love ðŸ¤— Transformers? We sure do.")
```

----------------------------------------

TITLE: Retrieving Top k Image Indices in Batches
DESCRIPTION: This snippet defines a function that retrieves the indices of the top k most relevant images for a given text embedding. It processes the dataset in batches, computes similarity scores, and sorts results to obtain top matches efficiently using the ColPali processor.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_document_retrieval.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
def find_top_k_indices_batched(dataset, text_embedding, processor, k=10, batch_size=4):
    scores_and_indices = []

    for start_idx in range(0, len(dataset), batch_size):

        end_idx = min(start_idx + batch_size, len(dataset))
        batch = dataset[start_idx:end_idx]        
        batch_embeddings = [torch.tensor(emb[0], dtype=torch.float32) for emb in batch["embeddings"]]
        scores = processor.score_retrieval(text_embedding.to("cpu").to(torch.float32), batch_embeddings)

        if hasattr(scores, "tolist"):
            scores = scores.tolist()[0]

        for i, score in enumerate(scores):
            scores_and_indices.append((score, start_idx + i))

    sorted_results = sorted(scores_and_indices, key=lambda x: -x[0])

    topk = sorted_results[:k]
    indices = [idx for _, idx in topk]
    scores = [score for score, _ in topk]

    return indices, scores
```

----------------------------------------

TITLE: Tokenizing and Identifying Mask Token Position (TensorFlow)
DESCRIPTION: Code to tokenize the input text and identify the position of the mask token for manual prediction in TensorFlow, returning the inputs as TensorFlow tensors.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_27

LANGUAGE: python
CODE:
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("username/my_awesome_eli5_mlm_model")
inputs = tokenizer(text, return_tensors="tf")
mask_token_index = tf.where(inputs["input_ids"] == tokenizer.mask_token_id)[0, 1]
```

----------------------------------------

TITLE: Batched Audio-Prompted Generation
DESCRIPTION: Demonstrates generating audio from multiple text prompts, using batched inputs. This snippet takes different portions of the input audio for generating batched outputs, decoding them, and removing padding. It uses the 'transformers' and 'datasets' libraries.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/musicgen.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import AutoProcessor, MusicgenForConditionalGeneration
>>> from datasets import load_dataset

>>> processor = AutoProcessor.from_pretrained("facebook/musicgen-small")
>>> model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")

>>> dataset = load_dataset("sanchit-gandhi/gtzan", split="train", streaming=True)
>>> sample = next(iter(dataset))["audio"]

>>> # take the first quarter of the audio sample
>>> sample_1 = sample["array"][: len(sample["array"]) // 4]

>>> # take the first half of the audio sample
>>> sample_2 = sample["array"][: len(sample["array"]) // 2]

>>> inputs = processor(
...     audio=[sample_1, sample_2],
...     sampling_rate=sample["sampling_rate"],
...     text=["80s blues track with groovy saxophone", "90s rock song with loud guitars and heavy drums"],
...     padding=True,
...     return_tensors="pt",
... )
>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)

>>> # post-process to remove padding from the batched audio
>>> audio_values = processor.batch_decode(audio_values, padding_mask=inputs.padding_mask)
```

----------------------------------------

TITLE: Loading VQA Dataset from Hugging Face
DESCRIPTION: Loads a small sample of the Graphcore/vqa dataset from the Hugging Face Hub for fine-tuning, containing 200 examples from the validation split.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/visual_question_answering.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset

>>> dataset = load_dataset("Graphcore/vqa", split="validation[:200]")
>>> dataset
Dataset({
    features: ['question', 'question_type', 'question_id', 'image_id', 'answer_type', 'label'],
    num_rows: 200
})
```

----------------------------------------

TITLE: Creating hostfile for DeepSpeed multi-node
DESCRIPTION: Example content of a hostfile used for DeepSpeed multi-node training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/zh/main_classes/deepspeed.md#2025-04-22_snippet_13

LANGUAGE: text
CODE:
```
hostname1 slots=8
hostname2 slots=8
```

----------------------------------------

TITLE: Loading Tensor to Trigger GPU Kernels
DESCRIPTION: Utilizes PyTorch to load a small tensor onto the GPU to trigger loading of CUDA kernels, which consumes a certain amount of GPU memory necessary for further computations.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_memory_anatomy.md#2025-04-22_snippet_4

LANGUAGE: py
CODE:
```
>>> import torch

>>> torch.ones((1, 1)).to("cuda")
>>> print_gpu_utilization()
GPU memory occupied: 1343 MB.

```

----------------------------------------

TITLE: Text Grouping Function
DESCRIPTION: Groups preprocessed text into blocks of specified size and prepares labels for language modeling.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/es/tasks/language_modeling.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
block_size = 128

def group_texts(examples):
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    total_length = (total_length // block_size) * block_size
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result
```

----------------------------------------

TITLE: Creating ResNet Model for Image Classification
DESCRIPTION: Develops a custom ResNet model for image classification with loss calculation support
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/custom_models.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import torch

class ResnetModelForImageClassification(PreTrainedModel):
    config_class = ResnetConfig

    def __init__(self, config):
        super().__init__(config)
        block_layer = BLOCK_MAPPING[config.block_type]
        self.model = ResNet(
            block_layer,
            config.layers,
            num_classes=config.num_classes,
            in_chans=config.input_channels,
            cardinality=config.cardinality,
            base_width=config.base_width,
            stem_width=config.stem_width,
            stem_type=config.stem_type,
            avg_down=config.avg_down,
        )

    def forward(self, tensor, labels=None):
        logits = self.model(tensor)
        if labels is not None:
            loss = torch.nn.functional.cross_entropy(logits, labels)
            return {"loss": loss, "logits": logits}
        return {"logits": logits}
```

----------------------------------------

TITLE: Model Utility Mixins
DESCRIPTION: Utility mixins that provide additional functionality for models across different deep learning frameworks, including common operations like resizing embeddings and pruning attention heads
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/model.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
class ModuleUtilsMixin:
    # Utility methods for PyTorch models
    def resize_token_embeddings(self):
        # Resize input token embeddings
        pass
```

LANGUAGE: Python
CODE:
```
class TFModuleUtilsMixin:
    # Utility methods for TensorFlow models
    def resize_token_embeddings(self):
        # Resize input token embeddings
        pass
```

----------------------------------------

TITLE: Point Prompting for Mask Generation
DESCRIPTION: This snippet shows how to use point prompting with the SAM model. It initializes the model and processor, processes an input point, and then generates masks from the model's output.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/mask_generation.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from transformers import SamModel, SamProcessor
import torch
from accelerate.test_utils.testing import get_backend
# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)
device, _, _ = get_backend()
model = SamModel.from_pretrained("facebook/sam-vit-base").to(device)
processor = SamProcessor.from_pretrained("facebook/sam-vit-base")
```

LANGUAGE: python
CODE:
```
input_points = [[[2592, 1728]]] # point location of the bee

inputs = processor(image, input_points=input_points, return_tensors="pt").to(device)
with torch.no_grad():
    outputs = model(**inputs)
masks = processor.image_processor.post_process_masks(outputs.pred_masks.cpu(), inputs["original_sizes"].cpu(), inputs["reshaped_input_sizes"].cpu())
```

----------------------------------------

TITLE: Defining Evaluation Metrics Computation Function
DESCRIPTION: Creates a function to compute the Word Error Rate (WER) metric for evaluating the ASR model's performance during training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/asr.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
>>> import numpy as np


>>> def compute_metrics(pred):
...     pred_logits = pred.predictions
...     pred_ids = np.argmax(pred_logits, axis=-1)

...     pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id

...     pred_str = processor.batch_decode(pred_ids)
...     label_str = processor.batch_decode(pred.label_ids, group_tokens=False)

...     wer = wer.compute(predictions=pred_str, references=label_str)

...     return {"wer": wer}
```

----------------------------------------

TITLE: Creating Custom ResNet Classification Model in Python
DESCRIPTION: Implements a custom ResNet model for image classification with loss computation, inheriting from PreTrainedModel
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/custom_models.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch

class ResnetModelForImageClassification(PreTrainedModel):
    config_class = ResnetConfig

    def __init__(self, config):
        super().__init__(config)
        block_layer = BLOCK_MAPPING[config.block_type]
        self.model = ResNet(
            block_layer,
            config.layers,
            num_classes=config.num_classes,
            in_chans=config.input_channels,
            cardinality=config.cardinality,
            base_width=config.base_width,
            stem_width=config.stem_width,
            stem_type=config.stem_type,
            avg_down=config.avg_down,
        )

    def forward(self, tensor, labels=None):
        logits = self.model(tensor)
        if labels is not None:
            loss = torch.nn.functional.cross_entropy(logits, labels)
            return {"loss": loss, "logits": logits}
        return {"logits": logits}
```

----------------------------------------

TITLE: Creating DistilBert Model with Custom Configuration in PyTorch
DESCRIPTION: Demonstrates how to instantiate a DistilBert model using a custom configuration in PyTorch, creating a model with random initialization of weights.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/create_a_model.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
>>> from transformers import DistilBertModel

>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/config.json")
>>> model = DistilBertModel(my_config)
```

----------------------------------------

TITLE: Apply Softmax to Model Output (PyTorch)
DESCRIPTION: Applies the softmax function to the model's logits to obtain probabilities for each class in PyTorch.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/te/quicktour.md#_snippet_20

LANGUAGE: Python
CODE:
```
>>> from torch import nn

>>> pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
>>> print(pt_predictions)
tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],
        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)
```

----------------------------------------

TITLE: Enabling Universal Checkpointing in DeepSpeed
DESCRIPTION: Configure DeepSpeed to use universal checkpointing for easier model training continuation and fine-tuning across different architectures and configurations
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/deepspeed.md#2025-04-22_snippet_23

LANGUAGE: yaml
CODE:
```
{
    "checkpoint": {
        "load_universal": true
    }
}
```

----------------------------------------

TITLE: Registering a Custom Model with Auto Classes in Python
DESCRIPTION: This code snippet demonstrates how to register a custom model class (`NewModel`) and its configuration (`NewModelConfig`) with the Auto Classes from the `transformers` library. This allows you to use `AutoConfig` and `AutoModel` to automatically load your custom model like any other pre-trained model.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/auto.md#_snippet_1

LANGUAGE: python
CODE:
```
from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)
```

----------------------------------------

TITLE: Preparing an Image for Detection
DESCRIPTION: This snippet imports necessary libraries and prepares an image of an astronaut for object detection. The image is converted to an RGB format suitable for processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> import skimage
>>> import numpy as np
>>> from PIL import Image

>>> image = skimage.data.astronaut()
>>> image = Image.fromarray(np.uint8(image)).convert("RGB")
>>> image
```

----------------------------------------

TITLE: Single-GPU Deployment with DeepSpeed
DESCRIPTION: Run a translation model training script on a single GPU using DeepSpeed with a ZeRO-2 configuration file
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/deepspeed.md#2025-04-22_snippet_25

LANGUAGE: bash
CODE:
```
deepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \
--deepspeed tests/deepspeed/ds_config_zero2.json \
--model_name_or_path google-t5/t5-small --per_device_train_batch_size 1 \
--output_dir output_dir --overwrite_output_dir --fp16 \
--do_train --max_train_samples 500 --num_train_epochs 1 \
--dataset_name wmt16 --dataset_config "ro-en" \
--source_lang en --target_lang ro
```

----------------------------------------

TITLE: Loading the ViLT Model for Question Answering
DESCRIPTION: Initializes a ViltForQuestionAnswering model from a pretrained checkpoint, configuring it with the appropriate number of labels and label mappings for the VQA task.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/visual_question_answering.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
>>> from transformers import ViltForQuestionAnswering

>>> model = ViltForQuestionAnswering.from_pretrained(model_checkpoint, num_labels=len(id2label), id2label=id2label, label2id=label2id)
```

----------------------------------------

TITLE: Generating Mask Token Predictions with PyTorch Model
DESCRIPTION: Code to run inference with a PyTorch masked language model to get logits for the masked token position, which represent prediction probabilities for each token in the vocabulary.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md#2025-04-22_snippet_25

LANGUAGE: python
CODE:
```
from transformers import AutoModelForMaskedLM

model = AutoModelForMaskedLM.from_pretrained("username/my_awesome_eli5_mlm_model")
logits = model(**inputs).logits
mask_token_logits = logits[0, mask_token_index, :]
```

----------------------------------------

TITLE: Aligning Tokens and Labels for NER
DESCRIPTION: Creates a function to align the tokenized inputs with their corresponding NER labels, handling special tokens and subword tokenization by mapping tokens to their original words.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/token_classification.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> def tokenize_and_align_labels(examples):
...     tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

...     labels = []
...     for i, label in enumerate(examples[f"ner_tags"]):
...         word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.
...         previous_word_idx = None
...         label_ids = []
...         for word_idx in word_ids:  # Set the special tokens to -100.
...             if word_idx is None:
...                 label_ids.append(-100)
...             elif word_idx != previous_word_idx:  # Only label the first token of a given word.
...                 label_ids.append(label[word_idx])
...             else:
...                 label_ids.append(-100)
...             previous_word_idx = word_idx
...         labels.append(label_ids)

...     tokenized_inputs["labels"] = labels
...     return tokenized_inputs
```

----------------------------------------

TITLE: Creating id2label Mapping for Semantic Segmentation Classes in Python
DESCRIPTION: This snippet shows how to create and save an id2label mapping JSON file, which is required for semantic segmentation tasks to map integer labels to class names.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/semantic-segmentation/README.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import json
# simple example
id2label = {0: 'cat', 1: 'dog'}
with open('id2label.json', 'w') as fp:
    json.dump(id2label, fp)
```

----------------------------------------

TITLE: Running Transformers question-answering with IPEX and BF16
DESCRIPTION: Example command for training a question-answering model using the Transformers library with IPEX enabled, BFloat16 auto mixed precision, and CPU execution.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/it/perf_train_cpu.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
python run_qa.py \
--model_name_or_path google-bert/bert-base-uncased \
--dataset_name squad \
--do_train \
--do_eval \
--per_device_train_batch_size 12 \
--learning_rate 3e-5 \
--num_train_epochs 2 \
--max_seq_length 384 \
--doc_stride 128 \
--output_dir /tmp/debug_squad/ \
--use_ipex \
--bf16 --no_cuda
```

----------------------------------------

TITLE: Whitespace Management in Jinja Template
DESCRIPTION: Example of proper whitespace handling in Jinja templates using the '-' modifier to trim unnecessary whitespace
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/chat_templating_writing.md#2025-04-22_snippet_3

LANGUAGE: jinja
CODE:
```
{%- for message in messages %}
    {{- message['role'] + message['content'] }}
{%- endfor %}
```

LANGUAGE: jinja
CODE:
```
{% for message in messages %}
    {{ message['role'] + message['content'] }}
{% endfor %}
```

----------------------------------------

TITLE: Passing Config File to Trainer for DeepSpeed
DESCRIPTION: In this snippet, a configuration file path is passed to the TrainingArguments for enabling DeepSpeed features. This setup is essential for utilizing DeepSpeed during model training.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/deepspeed.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
TrainingArguments(\
    deepspeed="path/to/deepspeed_config.json",\
    ...,\
)
```

----------------------------------------

TITLE: Implementing Pipeline with Top-k Processing
DESCRIPTION: Extended implementation showing how to handle additional parameters like top_k in postprocessing and parameter sanitization.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/de/add_new_pipeline.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
def postprocess(self, model_outputs, top_k=5):
    best_class = model_outputs["logits"].softmax(-1)
    # Add logic to handle top_k
    return best_class

def _sanitize_parameters(self, **kwargs):
    preprocess_kwargs = {}
    if "maybe_arg" in kwargs:
        preprocess_kwargs["maybe_arg"] = kwargs["maybe_arg"]

    postprocess_kwargs = {}
    if "top_k" in kwargs:
        postprocess_kwargs["top_k"] = kwargs["top_k"]
    return preprocess_kwargs, {}, postprocess_kwargs
```

----------------------------------------

TITLE: Getting Top Predictions from PyTorch MLM Logits
DESCRIPTION: Extracts the top token predictions from the model's logits and formats them as complete sentences by replacing the mask token.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/tasks/masked_language_modeling.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
top_3_tokens = torch.topk(mask_token_logits, 3, dim=1).indices[0].tolist()

for token in top_3_tokens:
    print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))
```

----------------------------------------

TITLE: Utilizing FlaxResNetForImageClassification in Flax
DESCRIPTION: Example of importing and using the FlaxResNetForImageClassification model in Flax (JAX). This snippet demonstrates how to load the model and perform a forward pass for image classification.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/resnet.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
from transformers import FlaxResNetForImageClassification

model = FlaxResNetForImageClassification.from_pretrained("microsoft/resnet-50")
outputs = model(inputs)
```

----------------------------------------

TITLE: Dynamic Batching Pseudo-code (Python)
DESCRIPTION: This code snippet demonstrates a pseudo-code implementation of dynamic batching within the web server.  It accumulates multiple requests from the queue within a short timeframe (1ms) and then processes them as a single batch using the Transformers pipeline. It is a high-level illustration and is not optimized for production. It accumulates string inputs from the queue, performs a batch inference, and sends back the responses to the individual requests.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/pipeline_webserver.md#_snippet_3

LANGUAGE: python
CODE:
```
(string, rq) = await q.get()
strings = []
queues = []
while True:
    try:
        (string, rq) = await asyncio.wait_for(q.get(), timeout=0.001)  # 1ms
    except asyncio.exceptions.TimeoutError:
        break
    strings.append(string)
    queues.append(rq)
strings
outs = pipe(strings, batch_size=len(strings))
for rq, out in zip(queues, outs):
    await rq.put(out)
```

----------------------------------------

TITLE: Using CANINE Model with Tokenizer in Python
DESCRIPTION: Shows how to use the CANINE model with a tokenizer for batch inference and training. This approach is recommended as it handles padding and truncation to ensure all sequences in a batch have the same length.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ja/model_doc/canine.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> from transformers import CanineTokenizer, CanineModel

>>> model = CanineModel.from_pretrained("google/canine-c")
>>> tokenizer = CanineTokenizer.from_pretrained("google/canine-c")

>>> inputs = ["Life is like a box of chocolates.", "You never know what you gonna get."]
>>> encoding = tokenizer(inputs, padding="longest", truncation=True, return_tensors="pt")

>>> outputs = model(**encoding)  # forward pass
>>> pooled_output = outputs.pooler_output
>>> sequence_output = outputs.last_hidden_state
```

----------------------------------------

TITLE: Loading an Image for Processing
DESCRIPTION: Loads an image from a URL using PIL and requests libraries, displaying its original dimensions for reference before super-resolution processing.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/ko/tasks/image_to_image.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from PIL import Image
import requests

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/cat.jpg"
image = Image.open(requests.get(url, stream=True).raw)

print(image.size)
```

----------------------------------------

TITLE: Initializing TrainingArguments for APOLLO-Mini Optimizer in PyTorch
DESCRIPTION: This snippet demonstrates the initialization of TrainingArguments for the APOLLO-Mini optimizer in PyTorch, which is a rank 1 variant of APOLLO designed for ultra-low rank efficiency. This allows for fine-tuning with low memory usage.
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/optimizers.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from transformers import TrainingArguments

args = TrainingArguments(
    output_dir="./test-apollo_mini",
    max_steps=100,
    per_device_train_batch_size=2,
    optim="apollo_adamw",
    optim_target_modules=[r".*.attn.*", r".*.mlp.*"],
    optim_args="proj=random,rank=1,scale=128.0,scale_type=tensor,update_proj_gap=200",
)
```

----------------------------------------

TITLE: Optimizing PyTorch Inference with IPEX
DESCRIPTION: Shows how to install and enable Intel Extension for PyTorch (IPEX) for graph optimization on Intel CPUs
SOURCE: https://github.com/huggingface/transformers/blob/main/docs/source/en/perf_infer_cpu.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
!pip install intel_extension_for_pytorch
```

LANGUAGE: bash
CODE:
```
python examples/pytorch/question-answering/run_qa.py \
--model_name_or_path csarron/bert-base-uncased-squad-v1 \
--dataset_name squad \
--do_eval \
--max_seq_length 384 \
--doc_stride 128 \
--output_dir /tmp/ \
--no_cuda \
--use_ipex \
--jit_mode_eval
```

----------------------------------------

TITLE: Distributed Training with Accelerate (Bash)
DESCRIPTION: Commands to set up and launch distributed training for token classification using the Accelerate library, supporting various hardware configurations.
SOURCE: https://github.com/huggingface/transformers/blob/main/examples/pytorch/token-classification/README.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
accelerate config
acccelerate test

export TASK_NAME=ner

accelerate launch run_ner_no_trainer.py \
  --model_name_or_path google-bert/bert-base-cased \
  --dataset_name conll2003 \
  --task_name $TASK_NAME \
  --max_length 128 \
  --per_device_train_batch_size 32 \
  --learning_rate 2e-5 \
  --num_train_epochs 3 \
  --output_dir /tmp/$TASK_NAME/
```