TITLE: Command Line Usage Syntax for Llama-bench
DESCRIPTION: Shows the complete command-line interface options for llama-bench, including model configuration, processing parameters, thread management, GPU settings, and output format options.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_0

LANGUAGE: plaintext
CODE:
```
usage: ./llama-bench [options]

options:
  -h, --help
  -m, --model <filename>                    (default: models/7B/ggml-model-q4_0.gguf)
  -p, --n-prompt <n>                        (default: 512)
  -n, --n-gen <n>                           (default: 128)
  -pg <pp,tg>                               (default: )
  -b, --batch-size <n>                      (default: 2048)
  -ub, --ubatch-size <n>                    (default: 512)
  -ctk, --cache-type-k <t>                  (default: f16)
  -ctv, --cache-type-v <t>                  (default: f16)
  -t, --threads <n>                         (default: 8)
  -C, --cpu-mask <hex,hex>                  (default: 0x0)
  --cpu-strict <0|1>                        (default: 0)
  --poll <0...100>                          (default: 50)
  -ngl, --n-gpu-layers <n>                  (default: 99)
  -rpc, --rpc <rpc_servers>                 (default: )
  -sm, --split-mode <none|layer|row>        (default: layer)
  -mg, --main-gpu <i>                       (default: 0)
  -nkvo, --no-kv-offload <0|1>              (default: 0)
  -fa, --flash-attn <0|1>                   (default: 0)
  -mmp, --mmap <0|1>                        (default: 1)
  --numa <distribute|isolate|numactl>       (default: disabled)
  -embd, --embeddings <0|1>                 (default: 0)
  -ts, --tensor-split <ts0/ts1/..>          (default: 0)
  -r, --repetitions <n>                     (default: 5)
  --prio <0|1|2|3>                          (default: 0)
  --delay <0...N> (seconds)                 (default: 0)
  -o, --output <csv|json|jsonl|md|sql>      (default: md)
  -oe, --output-err <csv|json|jsonl|md|sql> (default: none)
  -v, --verbose                             (default: 0)
```

----------------------------------------

TITLE: Supporting Multiple Users and Parallel Decoding in llama-server
DESCRIPTION: Shows how to configure llama-server to handle multiple concurrent requests with specific context size limitations.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
# up to 4 concurrent requests, each with 4096 max context
llama-server -m model.gguf -c 16384 -np 4
```

----------------------------------------

TITLE: Running Simple Text Completion with llama-cli
DESCRIPTION: Demonstrates basic text completion with llama-cli by explicitly disabling conversation mode and providing a prompt with a specified number of tokens to generate.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
llama-cli -m model.gguf -p "I believe the meaning of life is" -n 128 -no-cnv
```

----------------------------------------

TITLE: Constraining Output with Custom Grammar in llama-cli
DESCRIPTION: Shows how to use a custom grammar file to constrain the model's output format, which is useful for generating structured data like JSON.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
llama-cli -m model.gguf -n 256 --grammar-file grammars/json.gbnf -p 'Request: schedule a call at 8pm; Command:'
```

----------------------------------------

TITLE: Adjusting Temperature in LLaMA.cpp
DESCRIPTION: Controls the randomness of text generation with a default of 0.8. Higher values (e.g., 1.5) increase creativity, while lower values (e.g., 0.5) produce more focused, deterministic output.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_16

LANGUAGE: bash
CODE:
```
--temp N
```

----------------------------------------

TITLE: Running llama-mtmd-cli with LLaVA Model
DESCRIPTION: Command to run the llama-mtmd-cli with a LLaVA model, specifying the model file, multimodal projector, and chat template. It's recommended to use a lower temperature (e.g., 0.1) for better quality output.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_0

LANGUAGE: sh
CODE:
```
./llama-mtmd-cli -m ../llava-v1.5-7b/ggml-model-f16.gguf \
    --mmproj ../llava-v1.5-7b/mmproj-model-f16.gguf \
    --chat-template vicuna
```

----------------------------------------

TITLE: Converting and Quantizing LLaMA Models with llama.cpp
DESCRIPTION: This code demonstrates the process of converting LLaMA models to GGUF format and quantizing them. It includes steps for installing dependencies, converting models to FP16 format, and quantizing to lower precision formats like Q4_K_M.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/quantize/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
# obtain the official LLaMA model weights and place them in ./models
ls ./models
llama-2-7b tokenizer_checklist.chk tokenizer.model
# [Optional] for models using BPE tokenizers
ls ./models
<folder containing weights and tokenizer json> vocab.json
# [Optional] for PyTorch .bin models like Mistral-7B
ls ./models
<folder containing weights and tokenizer json>

# install Python dependencies
python3 -m pip install -r requirements.txt

# convert the model to ggml FP16 format
python3 convert_hf_to_gguf.py models/mymodel/

# quantize the model to 4-bits (using Q4_K_M method)
./llama-quantize ./models/mymodel/ggml-model-f16.gguf ./models/mymodel/ggml-model-Q4_K_M.gguf Q4_K_M

# update the gguf filetype to current version if older version is now unsupported
./llama-quantize ./models/mymodel/ggml-model-Q4_K_M.gguf ./models/mymodel/ggml-model-Q4_K_M-v2.gguf COPY
```

----------------------------------------

TITLE: Using OpenAI Python Library for Completions in llama.cpp
DESCRIPTION: This snippet demonstrates how to use the OpenAI Python library to interact with the llama.cpp completions API. It sets up the client with a custom base URL and sends a completion request.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
import openai

client = openai.OpenAI(
    base_url="http://localhost:8080/v1", # "http://<Your api-server IP>:port"
    api_key = "sk-no-key-required"
)

completion = client.completions.create(
  model="davinci-002",
  prompt="I believe the meaning of life is",
  max_tokens=8
)

print(completion.choices[0].text)
```

----------------------------------------

TITLE: Running llama-cli in Conversation Mode
DESCRIPTION: Demonstrates how to use llama-cli to interact with a model in conversation mode, which activates automatically with models having built-in chat templates.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
llama-cli -m model.gguf
```

----------------------------------------

TITLE: Configuring Mirostat Sampling in LLaMA.cpp
DESCRIPTION: Example command showing how to enable Mirostat 2.0 with custom learning rate and target entropy values. Mirostat actively maintains the quality of generated text within a desired range, balancing coherence and diversity.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_24

LANGUAGE: shell
CODE:
```
--mirostat 2 --mirostat-lr 0.05 --mirostat-ent 3.0
```

----------------------------------------

TITLE: Constraining Server Outputs with Grammar in llama-server
DESCRIPTION: Shows how to apply grammar constraints to all outputs from llama-server using either custom or predefined grammar files.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_9

LANGUAGE: bash
CODE:
```
# custom grammar
llama-server -m model.gguf --grammar-file grammar.gbnf

# JSON
llama-server -m model.gguf --grammar-file grammars/json.gbnf
```

----------------------------------------

TITLE: Using Custom Chat Templates with llama-cli
DESCRIPTION: Shows how to use llama-cli with custom chat templates, either by specifying a predefined template or by setting custom prefixes and prompts.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
# use the "chatml" template (use -h to see the list of supported templates)
llama-cli -m model.gguf -cnv --chat-template chatml

# use a custom template
llama-cli -m model.gguf -cnv --in-prefix 'User: ' --reverse-prompt 'User:'
```

----------------------------------------

TITLE: Configuring GGML Backend Options in CMake for llama.cpp
DESCRIPTION: This CMake configuration sets up build options for various GGML backends including hardware acceleration options (CUDA, Metal, Vulkan, OpenCL, etc.) and their configuration parameters. It enables customization of the library build based on the target platform and requirements.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_15

LANGUAGE: CMake
CODE:
```
# 3rd party libs / backends
option(GGML_ACCELERATE                      "ggml: enable Accelerate framework"               ON)
option(GGML_BLAS                            "ggml: use BLAS"                                  ${GGML_BLAS_DEFAULT})
set(GGML_BLAS_VENDOR ${GGML_BLAS_VENDOR_DEFAULT} CACHE STRING
                                            "ggml: BLAS library vendor")
option(GGML_LLAMAFILE                       "ggml: use LLAMAFILE"                             ${GGML_LLAMAFILE_DEFAULT})

option(GGML_CUDA                            "ggml: use CUDA"                                  OFF)
option(GGML_MUSA                            "ggml: use MUSA"                                  OFF)
option(GGML_CUDA_FORCE_MMQ                  "ggml: use mmq kernels instead of cuBLAS"         OFF)
option(GGML_CUDA_FORCE_CUBLAS               "ggml: always use cuBLAS instead of mmq kernels"  OFF)
option(GGML_CUDA_F16                        "ggml: use 16 bit floats for some calculations"   OFF)
set   (GGML_CUDA_PEER_MAX_BATCH_SIZE "128" CACHE STRING
                                            "ggml: max. batch size for using peer access")
option(GGML_CUDA_NO_PEER_COPY               "ggml: do not use peer to peer copies"            OFF)
option(GGML_CUDA_NO_VMM                     "ggml: do not try to use CUDA VMM"                OFF)
option(GGML_CUDA_FA                         "ggml: compile ggml FlashAttention CUDA kernels"  ON)
option(GGML_CUDA_FA_ALL_QUANTS              "ggml: compile all quants for FlashAttention"     OFF)
option(GGML_CUDA_GRAPHS                     "ggml: use CUDA graphs (llama.cpp only)"          ${GGML_CUDA_GRAPHS_DEFAULT})
set   (GGML_CUDA_COMPRESSION_MODE "size" CACHE STRING
                                            "ggml: cuda link binary compression mode; requires cuda 12.8+")
set_property(CACHE GGML_CUDA_COMPRESSION_MODE PROPERTY STRINGS "none;speed;balance;size")

option(GGML_HIP                             "ggml: use HIP"                                   OFF)
option(GGML_HIP_GRAPHS                      "ggml: use HIP graph, experimental, slow"         OFF)
option(GGML_HIP_NO_VMM                      "ggml: do not try to use HIP VMM"                 ON)
option(GGML_HIP_ROCWMMA_FATTN               "ggml: enable rocWMMA for FlashAttention"         OFF)
option(GGML_VULKAN                          "ggml: use Vulkan"                                OFF)
option(GGML_VULKAN_CHECK_RESULTS            "ggml: run Vulkan op checks"                      OFF)
option(GGML_VULKAN_DEBUG                    "ggml: enable Vulkan debug output"                OFF)
option(GGML_VULKAN_MEMORY_DEBUG             "ggml: enable Vulkan memory debug output"         OFF)
option(GGML_VULKAN_SHADER_DEBUG_INFO        "ggml: enable Vulkan shader debug info"           OFF)
option(GGML_VULKAN_PERF                     "ggml: enable Vulkan perf output"                 OFF)
option(GGML_VULKAN_VALIDATE                 "ggml: enable Vulkan validation"                  OFF)
option(GGML_VULKAN_RUN_TESTS                "ggml: run Vulkan tests"                          OFF)
option(GGML_KOMPUTE                         "ggml: use Kompute"                               OFF)
option(GGML_METAL                           "ggml: use Metal"                                 ${GGML_METAL_DEFAULT})
option(GGML_METAL_USE_BF16                  "ggml: use bfloat if available"                   OFF)
option(GGML_METAL_NDEBUG                    "ggml: disable Metal debugging"                   OFF)
option(GGML_METAL_SHADER_DEBUG              "ggml: compile Metal with -fno-fast-math"         OFF)
option(GGML_METAL_EMBED_LIBRARY             "ggml: embed Metal library"                       ${GGML_METAL})
set   (GGML_METAL_MACOSX_VERSION_MIN "" CACHE STRING
                                            "ggml: metal minimum macOS version")
set   (GGML_METAL_STD "" CACHE STRING       "ggml: metal standard version (-std flag)")
option(GGML_OPENMP                          "ggml: use OpenMP"                                ON)
option(GGML_RPC                             "ggml: use RPC"                                   OFF)
option(GGML_SYCL                            "ggml: use SYCL"                                  OFF)
option(GGML_SYCL_F16                        "ggml: use 16 bit floats for sycl calculations"   OFF)
option(GGML_SYCL_GRAPH                      "ggml: enable graphs in the SYCL backend"         ON)
set   (GGML_SYCL_TARGET "INTEL" CACHE STRING
                                            "ggml: sycl target device")
set   (GGML_SYCL_DEVICE_ARCH "" CACHE STRING
                                            "ggml: sycl device architecture")

option(GGML_OPENCL                          "ggml: use OpenCL"                                OFF)
option(GGML_OPENCL_PROFILING                "ggml: use OpenCL profiling (increases overhead)" OFF)
option(GGML_OPENCL_EMBED_KERNELS            "ggml: embed kernels"                             ON)
option(GGML_OPENCL_USE_ADRENO_KERNELS       "ggml: use optimized kernels for Adreno"          ON)
set   (GGML_OPENCL_TARGET_VERSION "300" CACHE STRING
                                            "gmml: OpenCL API version to target")

# toolchain for vulkan-shaders-gen
set   (GGML_VULKAN_SHADERS_GEN_TOOLCHAIN "" CACHE FILEPATH "ggml: toolchain file for vulkan-shaders-gen")

# extra artifacts
option(GGML_BUILD_TESTS    "ggml: build tests"    ${GGML_STANDALONE})
option(GGML_BUILD_EXAMPLES "ggml: build examples" ${GGML_STANDALONE})
```

----------------------------------------

TITLE: Using cURL for Chat Completions in llama.cpp
DESCRIPTION: This snippet demonstrates how to use cURL to send a chat completion request to the llama.cpp API. It includes headers for content type and authorization, along with a JSON payload containing the model and messages.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_19

LANGUAGE: shell
CODE:
```
curl http://localhost:8080/v1/chat/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer no-key" \
-d '{
"model": "gpt-3.5-turbo",
"messages": [
{
    "role": "system",
    "content": "You are ChatGPT, an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests."
},
{
    "role": "user",
    "content": "Write a limerick about python exceptions"
}
]
}'
```

----------------------------------------

TITLE: Using OpenAI Python Library for Chat Completions in llama.cpp
DESCRIPTION: This example shows how to use the OpenAI Python library to interact with the llama.cpp chat completions API. It sets up the client and sends a chat completion request with system and user messages.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
import openai

client = openai.OpenAI(
    base_url="http://localhost:8080/v1", # "http://<Your api-server IP>:port"
    api_key = "sk-no-key-required"
)

completion = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "system", "content": "You are ChatGPT, an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests."},
    {"role": "user", "content": "Write a limerick about python exceptions"}
  ]
)

print(completion.choices[0].message)
```

----------------------------------------

TITLE: Basic Text Completion with llama-simple
DESCRIPTION: Shows how to use the minimal llama-simple example for basic text completion, which is designed as a starting point for developers implementing their own applications.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_14

LANGUAGE: bash
CODE:
```
llama-simple -m model.gguf
```

----------------------------------------

TITLE: Starting llama-server with Default Configuration
DESCRIPTION: Demonstrates how to start the llama-server HTTP service with default settings, which provides an OpenAI API-compatible interface for LLMs.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
llama-server -m model.gguf --port 8080
```

----------------------------------------

TITLE: Multi-GPU Configuration Commands
DESCRIPTION: Commands for controlling GPU selection and tensor splitting across multiple GPUs for optimized performance.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_32

LANGUAGE: bash
CODE:
```
-mg i, --main-gpu i
-ts SPLIT, --tensor-split SPLIT
```

----------------------------------------

TITLE: Docker Compose Configuration for Llama.cpp Server
DESCRIPTION: Sample Docker compose configuration demonstrating how to set up the llama.cpp server with environment variables for model loading and server configuration.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_2

LANGUAGE: yml
CODE:
```
services:
  llamacpp-server:
    image: ghcr.io/ggml-org/llama.cpp:server
    ports:
      - 8080:8080
    volumes:
      - ./models:/models
    environment:
      # alternatively, you can use "LLAMA_ARG_MODEL_URL" to download the model
      LLAMA_ARG_MODEL: /models/my_model.gguf
      LLAMA_ARG_CTX_SIZE: 4096
      LLAMA_ARG_N_PARALLEL: 2
      LLAMA_ARG_ENDPOINT_METRICS: 1
      LLAMA_ARG_PORT: 8080
```

----------------------------------------

TITLE: Single-Turn Query with System Prompt in Unix
DESCRIPTION: Command to execute a single interaction with custom system prompt using Jinja template in Unix-based systems
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
./llama-cli -m models/gemma-1.1-7b-it.Q4_K_M.gguf --jinja --single-turn -sys "You are a helpful assistant" -p "Hello"
```

----------------------------------------

TITLE: Running Inference with MiniCPM-Llama3-V 2.5 on Linux or Mac
DESCRIPTION: Commands to run inference using the MiniCPM-Llama3-V 2.5 model in both single-turn and conversation modes. The single-turn mode requires specifying an image file and prompt, while conversation mode provides an interactive experience.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/minicpmv2.5.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
# run in single-turn mode
./build/bin/llama-mtmd-cli -m ../MiniCPM-Llama3-V-2_5/model/model-8B-F16.gguf --mmproj ../MiniCPM-Llama3-V-2_5/mmproj-model-f16.gguf -c 4096 --temp 0.7 --top-p 0.8 --top-k 100 --repeat-penalty 1.05 --image xx.jpg -p "What is in the image?"

# run in conversation mode
./build/bin/llama-mtmd-cli -m ../MiniCPM-Llama3-V-2_5/model/ggml-model-Q4_K_M.gguf --mmproj ../MiniCPM-Llama3-V-2_5/mmproj-model-f16.gguf
```

----------------------------------------

TITLE: Applying Logit Bias in LLaMA.cpp
DESCRIPTION: Command example showing how to prevent the generation of LaTeX code by setting a negative infinity bias on the backslash token. Logit bias allows manual adjustment of specific token likelihoods in the generated text.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_27

LANGUAGE: shell
CODE:
```
--logit-bias 29905-inf
```

----------------------------------------

TITLE: Implementing Node.js LLama Client
DESCRIPTION: JavaScript code that implements a client to interact with the llama.cpp server. Makes a POST request to the completion endpoint with a prompt and receives generated text.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_6

LANGUAGE: javascript
CODE:
```
const prompt = "Building a website can be done in 10 simple steps:"

async function test() {
    let response = await fetch("http://127.0.0.1:8080/completion", {
        method: "POST",
        body: JSON.stringify({
            prompt,
            n_predict: 64,
        })
    })
    console.log((await response.json()).content)
}

test()
```

----------------------------------------

TITLE: Building llama.cpp with CUDA Support
DESCRIPTION: Commands to build llama.cpp with CUDA support for NVIDIA GPU acceleration using CMake. Requires the CUDA toolkit to be installed.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_7

LANGUAGE: bash
CODE:
```
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

----------------------------------------

TITLE: Running Server with Docker in llama.cpp
DESCRIPTION: Command to run the llama.cpp server using the server Docker image. It mounts the models directory, maps port 8000, specifies the model path, binds to all interfaces, and sets the token generation limit.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
docker run -v /path/to/models:/models -p 8000:8000 ghcr.io/ggml-org/llama.cpp:server -m /models/7B/ggml-model-q4_0.gguf --port 8000 --host 0.0.0.0 -n 512
```

----------------------------------------

TITLE: Running Text Generation with llama.cpp from Command Line
DESCRIPTION: This snippet demonstrates how to run the llama-simple executable with a model file and a prompt, showing the generated text output and performance statistics.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/simple/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
./llama-simple -m ./models/llama-7b-v2/ggml-model-f16.gguf "Hello my name is"

...

main: n_len = 32, n_ctx = 2048, n_parallel = 1, n_kv_req = 32

 Hello my name is Shawn and I'm a 20 year old male from the United States. I'm a 20 year old

main: decoded 27 tokens in 2.31 s, speed: 11.68 t/s

llama_print_timings:        load time =   579.15 ms
llama_print_timings:      sample time =     0.72 ms /    28 runs   (    0.03 ms per token, 38888.89 tokens per second)
llama_print_timings: prompt eval time =   655.63 ms /    10 tokens (   65.56 ms per token,    15.25 tokens per second)
llama_print_timings:        eval time =  2180.97 ms /    27 runs   (   80.78 ms per token,    12.38 tokens per second)
llama_print_timings:       total time =  2891.13 ms
```

----------------------------------------

TITLE: Serving an Embedding Model with llama-server
DESCRIPTION: Shows how to configure llama-server to serve embeddings using the dedicated endpoint with specific pooling strategy.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_7

LANGUAGE: bash
CODE:
```
# use the /embedding endpoint
llama-server -m model.gguf --embedding --pooling cls -ub 8192
```

----------------------------------------

TITLE: CPU Architecture Detection and Flag Configuration in CMake
DESCRIPTION: CMake script that detects the system processor architecture and configures appropriate compilation flags and definitions. Handles various CPU architectures and their specific instruction sets including AVX, SSE, BMI2, PowerPC, RISC-V, and s390x. Also includes KleidiAI optimization integration for ARM processors.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: cmake
CODE:
```
if (GGML_BMI2)
    # MSVC does not define macro __BMI2__
    list(APPEND ARCH_DEFINITIONS __BMI2__ GGML_BMI2)
endif()
else ()
    if (GGML_NATIVE)
        list(APPEND ARCH_FLAGS -march=native)
    else ()
        if (GGML_SSE42)
            list(APPEND ARCH_FLAGS -msse4.2)
            list(APPEND ARCH_DEFINITIONS GGML_SSE42)
        endif()
        # ... [additional architecture checks] ...
        if (GGML_AMX_BF16)
            list(APPEND ARCH_FLAGS -mamx-bf16)
            list(APPEND ARCH_DEFINITIONS GGML_AMX_BF16)
        endif()
    endif()
endif()
```

----------------------------------------

TITLE: Simple HTML and JavaScript for llama.cpp Completion
DESCRIPTION: This snippet demonstrates a simple HTML page with embedded JavaScript that uses the llama.cpp completion API. It imports the llama function from '/completion.js' and uses it to generate and display dad jokes.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_21

LANGUAGE: html
CODE:
```
<html>
  <body>
    <pre>
      <script type="module">
        import { llama } from '/completion.js'

        const prompt = `### Instruction:
Write dad jokes, each one paragraph.
You can use html formatting if needed.

### Response:`

        for await (const chunk of llama(prompt)) {
          document.write(chunk.data.content)
        }
      </script>
    </pre>
  </body>
</html>
```

----------------------------------------

TITLE: Starting Conversation Mode in Unix
DESCRIPTION: Command to start an interactive chat session using the Gemma chat template in Unix-based systems
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
./llama-cli -m models/gemma-1.1-7b-it.Q4_K_M.gguf --chat-template gemma
```

----------------------------------------

TITLE: Using cURL for Embeddings in llama.cpp
DESCRIPTION: These examples show how to use cURL to send embedding requests to the llama.cpp API. The first example demonstrates a single input string, while the second shows an array of input strings.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_20

LANGUAGE: shell
CODE:
```
curl http://localhost:8080/v1/embeddings \
-H "Content-Type: application/json" \
-H "Authorization: Bearer no-key" \
-d '{
        "input": "hello",
        "model":"GPT-4",
        "encoding_format": "float"
}'
```

LANGUAGE: shell
CODE:
```
curl http://localhost:8080/v1/embeddings \
-H "Content-Type: application/json" \
-H "Authorization: Bearer no-key" \
-d '{
        "input": ["hello", "world"],
        "model":"GPT-4",
        "encoding_format": "float"
}'
```

----------------------------------------

TITLE: Enabling Speculative Decoding in llama-server
DESCRIPTION: Demonstrates how to use a smaller draft model to speed up generation in llama-server through speculative decoding.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
# the draft.gguf model should be a small variant of the target model.gguf
llama-server -m model.gguf -md draft.gguf
```

----------------------------------------

TITLE: Splitting LLaVA Model with llava_surgery.py
DESCRIPTION: Python command to split the LLaVA model into LLaMA and multimodal projector components using the llava_surgery.py script.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_3

LANGUAGE: sh
CODE:
```
python ./examples/llava/llava_surgery.py -m ../llava-v1.5-7b
```

----------------------------------------

TITLE: Building Llama Server with CMake
DESCRIPTION: Commands for building the llama-server using CMake, including both standard and SSL-enabled builds using OpenSSL 3.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
cmake -B build
cmake --build build --config Release -t llama-server
```

LANGUAGE: bash
CODE:
```
cmake -B build -DLLAMA_SERVER_SSL=ON
cmake --build build --config Release -t llama-server
```

----------------------------------------

TITLE: Serving a Reranking Model with llama-server
DESCRIPTION: Demonstrates how to set up llama-server to provide reranking capabilities via the dedicated endpoint.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_8

LANGUAGE: bash
CODE:
```
# use the /reranking endpoint
llama-server -m model.gguf --reranking
```

----------------------------------------

TITLE: Building and Installing llama.cpp with CMake
DESCRIPTION: Commands to clone the llama.cpp repository, build it with CMake, and install it to a local directory. This creates a relocatable CMake package that can be used by other projects.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/simple-cmake-pkg/README.md#2025-04-22_snippet_0

LANGUAGE: shell
CODE:
```
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
cmake -S . -B build
cmake --build build
cmake --install build --prefix inst
```

----------------------------------------

TITLE: Running Default Benchmark with llama-bench
DESCRIPTION: Shows how to use llama-bench to evaluate the performance of a model with default benchmark settings, reporting tokens per second.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_12

LANGUAGE: bash
CODE:
```
llama-bench -m model.gguf
```

----------------------------------------

TITLE: Quantizing and Running the Model
DESCRIPTION: Commands for quantizing the LLM and running the complete model in llama.cpp.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/granitevision.md#2025-04-22_snippet_7

LANGUAGE: bash
CODE:
```
$ ./build/bin/llama-quantize $LLM_EXPORT_PATH/granite_llm.gguf $LLM_EXPORT_PATH/granite_llm_q4_k_m.gguf Q4_K_M
$ LLM_GGUF_PATH=$LLM_EXPORT_PATH/granite_llm_q4_k_m.gguf

$ ./build/bin/llama-mtmd-cli -m $LLM_GGUF_PATH \
    --mmproj $VISUAL_GGUF_PATH \
    -c 16384 \
    --temp 0
```

----------------------------------------

TITLE: Setting DRY Sampling Parameters in LLaMA.cpp
DESCRIPTION: Configures the DRY (Don't Repeat Yourself) sampling parameters to reduce repetition across long contexts. Example shows recommended settings for multiplier, base, allowed length, penalty window, and sequence breakers.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_19

LANGUAGE: bash
CODE:
```
--dry-multiplier N
```

LANGUAGE: bash
CODE:
```
--dry-base N
```

LANGUAGE: bash
CODE:
```
--dry-allowed-length N
```

LANGUAGE: bash
CODE:
```
--dry-penalty-last-n N
```

LANGUAGE: bash
CODE:
```
--dry-sequence-breaker STRING
```

LANGUAGE: bash
CODE:
```
--dry-multiplier 0.8 --dry-base 1.75 --dry-allowed-length 2 --dry-penalty-last-n -1 --dry-sequence-breaker "â€”" --dry-sequence-breaker "##"
```

----------------------------------------

TITLE: Running Local CI with Different Hardware Acceleration Options
DESCRIPTION: Commands to execute the full CI workflow locally with options for CPU-only, CUDA, SYCL, and MUSA builds. Creates a temporary directory structure and runs the CI script with appropriate environment variables.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ci/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
mkdir tmp

# CPU-only build
bash ./ci/run.sh ./tmp/results ./tmp/mnt

# with CUDA support
GG_BUILD_CUDA=1 bash ./ci/run.sh ./tmp/results ./tmp/mnt

# with SYCL support
source /opt/intel/oneapi/setvars.sh
GG_BUILD_SYCL=1 bash ./ci/run.sh ./tmp/results ./tmp/mnt

# with MUSA support
GG_BUILD_MUSA=1 bash ./ci/run.sh ./tmp/results ./tmp/mnt
```

----------------------------------------

TITLE: Using Jinja Chat Template in Unix
DESCRIPTION: Command to start a conversation using the built-in Jinja chat template in Unix-based systems
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
./llama-cli -m models/gemma-1.1-7b-it.Q4_K_M.gguf --jinja
```

----------------------------------------

TITLE: Running a Model with llama-run
DESCRIPTION: Demonstrates how to use llama-run to execute a model, by default pulling it from the Ollama registry, which is useful for inferencing and integration with RamaLama.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_13

LANGUAGE: bash
CODE:
```
llama-run granite-code
```

----------------------------------------

TITLE: Running Text Generation with Full Docker Image in llama.cpp
DESCRIPTION: Command to run text generation using a quantized model with the full Docker image. It mounts the models directory, specifies the model path, a prompt, and sets the number of tokens to generate.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
docker run -v /path/to/models:/models ghcr.io/ggml-org/llama.cpp:full --run -m /models/7B/ggml-model-q4_0.gguf -p "Building a website can be done in 10 simple steps:" -n 512
```

----------------------------------------

TITLE: Executing Batched Benchmark Commands in llama.cpp
DESCRIPTION: Examples of running the llama-batched-bench tool with different configurations, including model types (F16, Q8_0), batch sizes, and prompts. Shows various parameter combinations for benchmarking performance.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/batched-bench/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
./llama-batched-bench -m model.gguf -c 2048 -b 2048 -ub 512 -npp 128,256,512 -ntg 128,256 -npl 1,2,4,8,16,32 [-pps]

# LLaMA 7B, F16, N_KV_MAX = 16384 (8GB), prompt not shared
./llama-batched-bench -m ./models/llama-7b/ggml-model-f16.gguf -c 16384 -b 2048 -ub 512 -ngl 99

# LLaMA 7B, Q8_0, N_KV_MAX = 16384 (8GB), prompt is shared
./llama-batched-bench -m ./models/llama-7b/ggml-model-q8_0.gguf -c 16384 -b 2048 -ub 512 -ngl 99 -pps

# custom set of batches
./llama-batched-bench -m ./models/llama-7b/ggml-model-q8_0.gguf -c 2048 -b 512 -ub 512 -ngl 999 -npp 128,256,512 -ntg 128,256 -npl 1,2,4,8,16,32
```

----------------------------------------

TITLE: Starting Conversation Mode in Windows
DESCRIPTION: Command to start an interactive chat session using the Gemma chat template in Windows
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_6

LANGUAGE: powershell
CODE:
```
./llama-cli.exe -m models\gemma-1.1-7b-it.Q4_K_M.gguf --chat-template gemma
```

----------------------------------------

TITLE: Running TTS with automatic model download in llama.cpp
DESCRIPTION: Command for running the text-to-speech feature with automatic model download when llama.cpp is built with CURL support. The output is saved as a WAV file and played with aplay.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_0

LANGUAGE: shell
CODE:
```
build/bin/llama-tts --tts-oute-default -p "Hello world" && aplay output.wav
```

----------------------------------------

TITLE: Building CUDA-enabled Docker Images for llama.cpp Locally
DESCRIPTION: Commands to build local Docker images with CUDA support for full, light, and server variants. Each command targets a specific build type from the CUDA Dockerfile.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
docker build -t local/llama.cpp:full-cuda --target full -f .devops/cuda.Dockerfile .
docker build -t local/llama.cpp:light-cuda --target light -f .devops/cuda.Dockerfile .
docker build -t local/llama.cpp:server-cuda --target server -f .devops/cuda.Dockerfile .
```

----------------------------------------

TITLE: Configuring Top-K Sampling in LLaMA.cpp
DESCRIPTION: Limits token selection to the K most probable tokens predicted by the model. Default is 40, with higher values producing more diverse output.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_20

LANGUAGE: bash
CODE:
```
--top-k N
```

LANGUAGE: bash
CODE:
```
--top-k 30
```

----------------------------------------

TITLE: Configuring LLaVA Library Build in CMake
DESCRIPTION: Sets up the LLaVA library as an object library, configures its dependencies, include directories, and creates static/shared versions. It links against ggml and llama libraries with C++17 support.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llava/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
add_library(llava OBJECT
            llava.cpp
            llava.h
            clip.cpp
            clip.h
            )

target_link_libraries(llava PRIVATE ggml llama ${CMAKE_THREAD_LIBS_INIT})

target_include_directories(llava PUBLIC .)
target_include_directories(llava PUBLIC ../..)
target_include_directories(llava PUBLIC ../../common)

target_compile_features(llava PRIVATE cxx_std_17)

add_library(llava_static STATIC $<TARGET_OBJECTS:llava>)
if (BUILD_SHARED_LIBS)
    set_target_properties(llava PROPERTIES POSITION_INDEPENDENT_CODE ON)
    target_compile_definitions(llava PRIVATE LLAMA_SHARED LLAMA_BUILD)
    add_library(llava_shared SHARED $<TARGET_OBJECTS:llava>)
    target_link_libraries(llava_shared PRIVATE ggml llama ${CMAKE_THREAD_LIBS_INIT})
    install(TARGETS llava_shared LIBRARY)
endif()
```

----------------------------------------

TITLE: Running text-to-speech with converted models
DESCRIPTION: Command to run the llama-tts tool with the converted LLM model and voice decoder model to generate speech from a text prompt. The output is saved as a WAV file.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_7

LANGUAGE: shell
CODE:
```
build/bin/llama-tts -m  ./models/outetts-0.2-0.5B-q8_0.gguf \
    -mv ./models/wavtokenizer-large-75-f16.gguf \
    -p "Hello world"
```

----------------------------------------

TITLE: Basic Text Embedding Generation - Unix
DESCRIPTION: Basic command to generate embeddings for a simple text input 'Hello World!' using mean pooling on Unix-based systems. The output is space-separated float values with logging disabled.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/embedding/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
./llama-embedding -m ./path/to/model --pooling mean --log-disable -p "Hello World!" 2>/dev/null
```

----------------------------------------

TITLE: Configuring Min-P Sampling in LLaMA.cpp
DESCRIPTION: Sets a minimum probability threshold for token selection relative to the most likely token. Default is 0.1, and lower values allow for more diverse tokens to be considered.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_22

LANGUAGE: bash
CODE:
```
--min-p N
```

LANGUAGE: bash
CODE:
```
--min-p 0.05
```

----------------------------------------

TITLE: Running llama-cli with RPC Servers
DESCRIPTION: Command to run llama-cli with multiple RPC servers for distributed inference. It specifies model parameters and RPC server addresses.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/rpc/README.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
$ bin/llama-cli -m ../models/tinyllama-1b/ggml-model-f16.gguf -p "Hello, my name is" --repeat-penalty 1.0 -n 64 --rpc 192.168.88.10:50052,192.168.88.11:50052 -ngl 99
```

----------------------------------------

TITLE: Running llama.cpp with CUDA-enabled Docker Images
DESCRIPTION: Commands to run the locally built CUDA-enabled Docker images with GPU support. These examples use the --gpus flag to access NVIDIA GPUs and set n-gpu-layers to utilize GPU acceleration.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
docker run --gpus all -v /path/to/models:/models local/llama.cpp:full-cuda --run -m /models/7B/ggml-model-q4_0.gguf -p "Building a website can be done in 10 simple steps:" -n 512 --n-gpu-layers 1
docker run --gpus all -v /path/to/models:/models local/llama.cpp:light-cuda -m /models/7B/ggml-model-q4_0.gguf -p "Building a website can be done in 10 simple steps:" -n 512 --n-gpu-layers 1
docker run --gpus all -v /path/to/models:/models local/llama.cpp:server-cuda -m /models/7B/ggml-model-q4_0.gguf --port 8000 --host 0.0.0.0 -n 512 --n-gpu-layers 1
```

----------------------------------------

TITLE: Retaining Initial Prompt Tokens in LLaMA.cpp
DESCRIPTION: Specifies how many tokens from the initial prompt to keep when the model's context window fills up. Default is 0 (no tokens kept), and -1 retains all prompt tokens.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_14

LANGUAGE: bash
CODE:
```
--keep N
```

----------------------------------------

TITLE: Using Generated Control Vector with llama-cli
DESCRIPTION: Example command demonstrating how to apply a generated control vector with llama-cli, specifying scaling factor and layer range. Control vectors work better when applied to layers higher than 10.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/cvector-generator/README.md#2025-04-22_snippet_6

LANGUAGE: sh
CODE:
```
./llama-cli -m ./llama-3.Q4_K_M.gguf -p "<|start_header_id|>system<|end_header_id|>\n\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nSing a song<|im_end|><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n" --special --control-vector-scaled ./control_vector.gguf 0.8 --control-vector-layer-range 10 31
```

----------------------------------------

TITLE: Grammar Constraint Commands
DESCRIPTION: Commands for specifying grammar constraints either inline or via file to control model output format.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_29

LANGUAGE: bash
CODE:
```
--grammar GRAMMAR
--grammar-file FILE
```

----------------------------------------

TITLE: Configuring Common Library Target for llama.cpp in CMake
DESCRIPTION: Defines the common library target with all source files. This static library contains core components like argument parsing, console utilities, chat handling, logging, and sampling functionalities for llama.cpp.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/common/CMakeLists.txt#2025-04-22_snippet_2

LANGUAGE: CMake
CODE:
```
set(TARGET common)

add_library(${TARGET} STATIC
    arg.cpp
    arg.h
    base64.hpp
    chat.cpp
    chat.h
    common.cpp
    common.h
    console.cpp
    console.h
    json-schema-to-grammar.cpp
    json.hpp
    llguidance.cpp
    log.cpp
    log.h
    minja/chat-template.hpp
    minja/minja.hpp
    ngram-cache.cpp
    ngram-cache.h
    sampling.cpp
    sampling.h
    speculative.cpp
    speculative.h
    )

if (BUILD_SHARED_LIBS)
    set_target_properties(${TARGET} PROPERTIES POSITION_INDEPENDENT_CODE ON)
endif()

set(LLAMA_COMMON_EXTRA_LIBS build_info)
```

----------------------------------------

TITLE: Running Text Generation with Light Docker Image in llama.cpp
DESCRIPTION: Command to run text generation using the light Docker image which only includes the main executable. It mounts the models directory and passes parameters for model path, prompt, and token count.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
docker run -v /path/to/models:/models ghcr.io/ggml-org/llama.cpp:light -m /models/7B/ggml-model-q4_0.gguf -p "Building a website can be done in 10 simple steps:" -n 512
```

----------------------------------------

TITLE: Advanced Text Embedding Generation - Windows
DESCRIPTION: Advanced command demonstrating multiple text inputs with custom separator, euclidean normalization, and specific output format for Windows systems. Uses GPU acceleration and disables logging.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/embedding/README.md#2025-04-22_snippet_3

LANGUAGE: powershell
CODE:
```
llama-embedding.exe -p 'Castle<#sep#>Stronghold<#sep#>Dog<#sep#>Cat' --pooling mean --embd-separator '<#sep#>' --embd-normalize 2  --embd-output-format '' -m './path/to/model.gguf' --n-gpu-layers 99 --log-disable 2>/dev/null
```

----------------------------------------

TITLE: Setting XTC Sampling Parameters in LLaMA.cpp
DESCRIPTION: Command example for configuring Exclude Top Choices (XTC) sampling with 50% token removal probability and a 0.1 threshold. XTC removes top tokens from consideration to improve variety and break repetitive patterns.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_25

LANGUAGE: shell
CODE:
```
--xtc-probability 0.5 --xtc-threshold 0.1
```

----------------------------------------

TITLE: Converting Image Encoder for MobileVLM
DESCRIPTION: Command for converting the CLIP image encoder to GGUF format with LDP projector type for the standard MobileVLM model.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_3

LANGUAGE: sh
CODE:
```
python ./examples/llava/convert_image_encoder_to_gguf.py \
    -m path/to/clip-vit-large-patch14-336 \
    --llava-projector path/to/MobileVLM-1.7B/llava.projector \
    --output-dir path/to/MobileVLM-1.7B \
    --projector-type ldp
```

----------------------------------------

TITLE: Splitting GLMV-EDGE model components
DESCRIPTION: Python command to run the glmedge-surgery.py script, which splits the GLMV-EDGE model into its LLM and multimodal projector components. This is a prerequisite for GGUF conversion.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/glmedge.md#2025-04-22_snippet_2

LANGUAGE: sh
CODE:
```
python ./examples/llava/glmedge-surgery.py -m ../model_path
```

----------------------------------------

TITLE: Converting MiniCPM-o 2.6 PyTorch Model to GGUF Format
DESCRIPTION: Commands to convert the MiniCPM-o 2.6 PyTorch model to gguf files and quantize it. This includes converting the image encoder and creating an optimized int4 version of the model.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/minicpmo2.6.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
python ./examples/llava/minicpmv-surgery.py -m ../MiniCPM-o-2_6
python ./examples/llava/minicpmv-convert-image-encoder-to-gguf.py -m ../MiniCPM-o-2_6 --minicpmv-projector ../MiniCPM-o-2_6/minicpmv.projector --output-dir ../MiniCPM-o-2_6/ --image-mean 0.5 0.5 0.5 --image-std 0.5 0.5 0.5 --minicpmv_version 4
python ./convert_hf_to_gguf.py ../MiniCPM-o-2_6/model

# quantize int4 version
./build/bin/llama-quantize ../MiniCPM-o-2_6/model/ggml-model-f16.gguf ../MiniCPM-o-2_6/model/ggml-model-Q4_K_M.gguf Q4_K_M
```

----------------------------------------

TITLE: OpenAI Models API Response Format
DESCRIPTION: JSON response structure for the OpenAI-compatible models endpoint showing model information including ID, metadata and parameters.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_16

LANGUAGE: json
CODE:
```
{
    "object": "list",
    "data": [
        {
            "id": "../models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
            "object": "model",
            "created": 1735142223,
            "owned_by": "llamacpp",
            "meta": {
                "vocab_type": 2,
                "n_vocab": 128256,
                "n_ctx_train": 131072,
                "n_embd": 4096,
                "n_params": 8030261312,
                "size": 4912898304
            }
        }
    ]
}
```

----------------------------------------

TITLE: Registering a Custom Model Class for GGUF Conversion in Python
DESCRIPTION: Python code for registering a new model class that extends the Model class with a custom architecture type. This is the first step in defining how a new model architecture should be converted to GGUF format.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/HOWTO-add-model.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
@Model.register("MyModelForCausalLM")
class MyModel(Model):
    model_arch = gguf.MODEL_ARCH.MYMODEL
```

----------------------------------------

TITLE: Reverse Prompt with Input Prefix Configuration
DESCRIPTION: Command showing how to configure reverse prompts with input prefix for interactive conversations
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_10

LANGUAGE: sh
CODE:
```
./llama-cli -r "User:" --in-prefix " "
```

----------------------------------------

TITLE: Converting GLMV-EDGE LLM to GGUF
DESCRIPTION: Python command to convert the LLM part of GLMV-EDGE to GGUF format using the convert_hf_to_gguf.py script. This final step completes the conversion process for use with llama.cpp.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/glmedge.md#2025-04-22_snippet_4

LANGUAGE: sh
CODE:
```
python convert_hf_to_gguf.py ../model_path
```

----------------------------------------

TITLE: Converting LLaVA Image Encoder to GGUF
DESCRIPTION: Python command to convert the LLaVA image encoder to GGUF format using the convert_image_encoder_to_gguf.py script.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_4

LANGUAGE: sh
CODE:
```
python ./examples/llava/convert_image_encoder_to_gguf.py -m ../clip-vit-large-patch14-336 --llava-projector ../llava-v1.5-7b/llava.projector --output-dir ../llava-v1.5-7b
```

----------------------------------------

TITLE: JSON Schema Constraint Command
DESCRIPTION: Command for applying JSON schema constraints to model output, with support for external references through a Python conversion script.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_30

LANGUAGE: bash
CODE:
```
--json-schema SCHEMA
--grammar "$( python examples/json_schema_to_grammar.py myschema.json )"
```

----------------------------------------

TITLE: Function Naming Convention Examples in C++
DESCRIPTION: Examples of proper function naming patterns following the <class>_<method> convention, where <method> follows the <action>_<noun> pattern. This demonstrates the recommended naming style for functions in the llama.cpp project.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/CONTRIBUTING.md#2025-04-22_snippet_1

LANGUAGE: cpp
CODE:
```
llama_model_init();           // class: "llama_model",         method: "init"
llama_sampler_chain_remove(); // class: "llama_sampler_chain", method: "remove"
llama_sampler_get_seed();     // class: "llama_sampler",       method: "get_seed"
llama_set_embeddings();       // class: "llama_context",       method: "set_embeddings"
llama_n_threads();            // class: "llama_context",       method: "n_threads"
llama_adapter_lora_free();    // class: "llama_adapter_lora",  method: "free"
```

----------------------------------------

TITLE: Setting Compiler Options for LLaVA and MTMD in CMake
DESCRIPTION: Configures compiler-specific options for LLaVA and MTMD libraries, disabling certain warnings for non-MSVC compilers and adding build dependencies if required.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llava/CMakeLists.txt#2025-04-22_snippet_2

LANGUAGE: CMake
CODE:
```
if (NOT MSVC)
    target_compile_options(llava PRIVATE -Wno-cast-qual) # stb_image.h
    target_compile_options(mtmd PRIVATE -Wno-cast-qual) # stb_image.h
endif()

if(TARGET BUILD_INFO)
    add_dependencies(llava BUILD_INFO)
    add_dependencies(mtmd BUILD_INFO)
endif()
```

----------------------------------------

TITLE: Running llama.cpp with SYCL using Batch Script
DESCRIPTION: A Windows batch file for executing llama2 model with SYCL backend. This script provides a simple way to run the model without manual command line parameters.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_25

LANGUAGE: batch
CODE:
```
examples\sycl\win-run-llama2.bat
```

----------------------------------------

TITLE: Running Greedy Speculative Decoding in llama.cpp
DESCRIPTION: This command demonstrates how to run the speculative decoding algorithm using llama-speculative-simple with a large model and a smaller draft model. It configures sampling parameters for greedy decoding (top-k=1, temp=0.0) and speculative settings (draft-max, draft-min, draft-p-min) to optimize the generation process.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/speculative-simple/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
./bin/llama-speculative-simple \
    -m  ../models/qwen2.5-32b-coder-instruct/ggml-model-q8_0.gguf \
    -md ../models/qwen2.5-1.5b-coder-instruct/ggml-model-q4_0.gguf \
    -f test.txt -c 0 -ngl 99 --color \
    --sampling-seq k --top-k 1 -fa --temp 0.0 \
    -ngld 99 --draft-max 16 --draft-min 5 --draft-p-min 0.9
```

----------------------------------------

TITLE: Llama2 Chat Template Example
DESCRIPTION: The official template for Llama-2-chat models, showing the specific formatting requirements including B_INST, E_INST tags and the specific system prompt format.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama.android/README.md#2025-04-22_snippet_3

LANGUAGE: plaintext
CODE:
```
{% set loop_messages = messages %}{% if add_generation_prompt %}{% set loop_messages = messages[:-1] %}{% endif %}<s>{{ bos_token }}{% for message in loop_messages %}{% if message['role'] == 'user' %}{{ B_INST }} {{ message['content'] }} {{ E_INST }}{% elif message['role'] == 'assistant' %}{{ message['content'] }}{{ bos_token }}{% elif message['role'] == 'system' %}{{ B_SYS }} {{ message['content'] }} {{ E_SYS }}{% endif %}{% endfor %}{% if add_generation_prompt %}{% if messages[-1]['role'] == 'user' %}{{ B_INST }} {{ messages[-1]['content'] }} {{ E_INST }}{% elif messages[-1]['role'] == 'assistant' %}{{ messages[-1]['content'] }}{% elif messages[-1]['role'] == 'system' %}{{ B_SYS }} {{ messages[-1]['content'] }} {{ E_SYS }}{% endif %}{% endif %}
```

----------------------------------------

TITLE: Displaying Common Parameters Table in Markdown
DESCRIPTION: A markdown table listing common command-line parameters for the llama.cpp project, including their descriptions and default values.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_0

LANGUAGE: markdown
CODE:
```
| Argument | Explanation |
| -------- | ----------- |
| `-h, --help, --usage` | print usage and exit |
| `--version` | show version and build info |
| `--verbose-prompt` | print a verbose prompt before generation (default: false) |
| `-t, --threads N` | number of threads to use during generation (default: -1)<br/>(env: LLAMA_ARG_THREADS) |
| `-tb, --threads-batch N` | number of threads to use during batch and prompt processing (default: same as --threads) |
| `-C, --cpu-mask M` | CPU affinity mask: arbitrarily long hex. Complements cpu-range (default: "") |
| `-Cr, --cpu-range lo-hi` | range of CPUs for affinity. Complements --cpu-mask |
| `--cpu-strict <0\|1>` | use strict CPU placement (default: 0)<br/> |
| `--prio N` | set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime (default: 0)<br/> |
| `--poll <0...100>` | use polling level to wait for work (0 - no polling, default: 50)<br/> |
| `-Cb, --cpu-mask-batch M` | CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch (default: same as --cpu-mask) |
| `-Crb, --cpu-range-batch lo-hi` | ranges of CPUs for affinity. Complements --cpu-mask-batch |
| `--cpu-strict-batch <0\|1>` | use strict CPU placement (default: same as --cpu-strict) |
| `--prio-batch N` | set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime (default: 0)<br/> |
| `--poll-batch <0\|1>` | use polling to wait for work (default: same as --poll) |
| `-c, --ctx-size N` | size of the prompt context (default: 4096, 0 = loaded from model)<br/>(env: LLAMA_ARG_CTX_SIZE) |
| `-n, --predict, --n-predict N` | number of tokens to predict (default: -1, -1 = infinity, -2 = until context filled)<br/>(env: LLAMA_ARG_N_PREDICT) |
| `-b, --batch-size N` | logical maximum batch size (default: 2048)<br/>(env: LLAMA_ARG_BATCH) |
| `-ub, --ubatch-size N` | physical maximum batch size (default: 512)<br/>(env: LLAMA_ARG_UBATCH) |
| `--keep N` | number of tokens to keep from the initial prompt (default: 0, -1 = all) |
| `-fa, --flash-attn` | enable Flash Attention (default: disabled)<br/>(env: LLAMA_ARG_FLASH_ATTN) |
| `--no-perf` | disable internal libllama performance timings (default: false)<br/>(env: LLAMA_ARG_NO_PERF) |
| `-e, --escape` | process escapes sequences (\n, \r, \t, \', \", \\) (default: true) |
| `--no-escape` | do not process escape sequences |
| `--rope-scaling {none,linear,yarn}` | RoPE frequency scaling method, defaults to linear unless specified by the model<br/>(env: LLAMA_ARG_ROPE_SCALING_TYPE) |
| `--rope-scale N` | RoPE context scaling factor, expands context by a factor of N<br/>(env: LLAMA_ARG_ROPE_SCALE) |
| `--rope-freq-base N` | RoPE base frequency, used by NTK-aware scaling (default: loaded from model)<br/>(env: LLAMA_ARG_ROPE_FREQ_BASE) |
| `--rope-freq-scale N` | RoPE frequency scaling factor, expands context by a factor of 1/N<br/>(env: LLAMA_ARG_ROPE_FREQ_SCALE) |
| `--yarn-orig-ctx N` | YaRN: original context size of model (default: 0 = model training context size)<br/>(env: LLAMA_ARG_YARN_ORIG_CTX) |
| `--yarn-ext-factor N` | YaRN: extrapolation mix factor (default: -1.0, 0.0 = full interpolation)<br/>(env: LLAMA_ARG_YARN_EXT_FACTOR) |
| `--yarn-attn-factor N` | YaRN: scale sqrt(t) or attention magnitude (default: 1.0)<br/>(env: LLAMA_ARG_YARN_ATTN_FACTOR) |
| `--yarn-beta-slow N` | YaRN: high correction dim or alpha (default: 1.0)<br/>(env: LLAMA_ARG_YARN_BETA_SLOW) |
| `--yarn-beta-fast N` | YaRN: low correction dim or beta (default: 32.0)<br/>(env: LLAMA_ARG_YARN_BETA_FAST) |
| `-dkvc, --dump-kv-cache` | verbose print of the KV cache |
| `-nkvo, --no-kv-offload` | disable KV offload<br/>(env: LLAMA_ARG_NO_KV_OFFLOAD) |
| `-ctk, --cache-type-k TYPE` | KV cache data type for K<br/>allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1<br/>(default: f16)<br/>(env: LLAMA_ARG_CACHE_TYPE_K) |
| `-ctv, --cache-type-v TYPE` | KV cache data type for V<br/>allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1<br/>(default: f16)<br/>(env: LLAMA_ARG_CACHE_TYPE_V) |
| `-dt, --defrag-thold N` | KV cache defragmentation threshold (default: 0.1, < 0 - disabled)<br/>(env: LLAMA_ARG_DEFRAG_THOLD) |
| `-np, --parallel N` | number of parallel sequences to decode (default: 1)<br/>(env: LLAMA_ARG_N_PARALLEL) |
| `--mlock` | force system to keep model in RAM rather than swapping or compressing<br/>(env: LLAMA_ARG_MLOCK) |
| `--no-mmap` | do not memory-map model (slower load but may reduce pageouts if not using mlock)<br/>(env: LLAMA_ARG_NO_MMAP) |
| `--numa TYPE` | attempt optimizations that help on some NUMA systems<br/>- distribute: spread execution evenly over all nodes<br/>- isolate: only spawn threads on CPUs on the node that execution started on<br/>- numactl: use the CPU map provided by numactl<br/>if run without this previously, it is recommended to drop the system page cache before using this<br/>see https://github.com/ggml-org/llama.cpp/issues/1437<br/>(env: LLAMA_ARG_NUMA) |
| `-dev, --device <dev1,dev2,..>` | comma-separated list of devices to use for offloading (none = don't offload)<br/>use --list-devices to see a list of available devices<br/>(env: LLAMA_ARG_DEVICE) |
| `--list-devices` | print list of available devices and exit |
| `-ngl, --gpu-layers, --n-gpu-layers N` | number of layers to store in VRAM<br/>(env: LLAMA_ARG_N_GPU_LAYERS) |
| `-sm, --split-mode {none,layer,row}` | how to split the model across multiple GPUs, one of:<br/>- none: use one GPU only<br/>- layer (default): split layers and KV across GPUs<br/>- row: split rows across GPUs<br/>(env: LLAMA_ARG_SPLIT_MODE) |
| `-ts, --tensor-split N0,N1,N2,...` | fraction of the model to offload to each GPU, comma-separated list of proportions, e.g. 3,1<br/>(env: LLAMA_ARG_TENSOR_SPLIT) |
| `-mg, --main-gpu INDEX` | the GPU to use for the model (with split-mode = none), or for intermediate results and KV (with split-mode = row) (default: 0)<br/>(env: LLAMA_ARG_MAIN_GPU) |
| `--check-tensors` | check model tensor data for invalid values (default: false) |
| `--override-kv KEY=TYPE:VALUE` | advanced option to override model metadata by key. may be specified multiple times.<br/>types: int, float, bool, str. example: --override-kv tokenizer.ggml.add_bos_token=bool:false |
| `--lora FNAME` | path to LoRA adapter (can be repeated to use multiple adapters) |
| `--lora-scaled FNAME SCALE` | path to LoRA adapter with user defined scaling (can be repeated to use multiple adapters) |
| `--control-vector FNAME` | add a control vector<br/>note: this argument can be repeated to add multiple control vectors |
| `--control-vector-scaled FNAME SCALE` | add a control vector with user defined scaling SCALE<br/>note: this argument can be repeated to add multiple scaled control vectors |
| `--control-vector-layer-range START END` | layer range to apply the control vector(s) to, start and end inclusive |
| `-m, --model FNAME` | model path (default: `models/$filename` with filename from `--hf-file` or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)<br/>(env: LLAMA_ARG_MODEL) |
| `-mu, --model-url MODEL_URL` | model download url (default: unused)<br/>(env: LLAMA_ARG_MODEL_URL) |
| `-hfr, --hf-repo REPO` | Hugging Face model repository (default: unused)<br/>(env: LLAMA_ARG_HF_REPO) |
| `-hff, --hf-file FILE` | Hugging Face model file (default: unused)<br/>(env: LLAMA_ARG_HF_FILE) |
| `-hft, --hf-token TOKEN` | Hugging Face access token (default: value from HF_TOKEN environment variable)<br/>(env: HF_TOKEN) |
| `--log-disable` | Log disable |
| `--log-file FNAME` | Log to file |
| `--log-colors` | Enable colored logging<br/>(env: LLAMA_LOG_COLORS) |
| `-v, --verbose, --log-verbose` | Set verbosity level to infinity (i.e. log all messages, useful for debugging) |
| `-lv, --verbosity, --log-verbosity N` | Set the verbosity threshold. Messages with a higher verbosity will be ignored.<br/>(env: LLAMA_LOG_VERBOSITY) |
| `--log-prefix` | Enable prefx in log messages<br/>(env: LLAMA_LOG_PREFIX) |
| `--log-timestamps` | Enable timestamps in log messages<br/>(env: LLAMA_LOG_TIMESTAMPS) |
```

----------------------------------------

TITLE: Measuring Perplexity Over a Text File
DESCRIPTION: Demonstrates how to use llama-perplexity to evaluate a model's perplexity score for a given text file, which is a quality metric for language models.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_10

LANGUAGE: bash
CODE:
```
llama-perplexity -m model.gguf -f file.txt
```

----------------------------------------

TITLE: Running the simple-cmake-pkg Example Application
DESCRIPTION: Command to run the built example application with a specified model file and prompt text. This demonstrates the execution of a program built against the llama.cpp library.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/simple-cmake-pkg/README.md#2025-04-22_snippet_2

LANGUAGE: shell
CODE:
```
./build/llama-simple-cmake-pkg -m ./models/llama-7b-v2/ggml-model-f16.gguf "Hello my name is"
```

----------------------------------------

TITLE: Building llama.cpp with Static Libraries
DESCRIPTION: Command to build llama.cpp with static libraries instead of shared libraries using CMake.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
cmake -B build -DBUILD_SHARED_LIBS=OFF
cmake --build build --config Release
```

----------------------------------------

TITLE: Building llama.cpp with Vulkan on Linux
DESCRIPTION: Commands to build llama.cpp with Vulkan support using CMake on Linux and testing the output binary.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_20

LANGUAGE: bash
CODE:
```
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
# Test the output binary (with "-ngl 33" to offload all layers to GPU)
./bin/llama-cli -m "PATH_TO_MODEL" -p "Hi you how are you" -n 50 -e -ngl 33 -t 4

# You should see in the output, ggml_vulkan detected your GPU. For example:
# ggml_vulkan: Using Intel(R) Graphics (ADL GT2) | uma: 1 | fp16: 1 | warp size: 32
```

----------------------------------------

TITLE: LoRA Adapter Commands
DESCRIPTION: Commands for integrating LoRA adapters with optional scaling factors to customize model behavior.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_31

LANGUAGE: bash
CODE:
```
--lora my_adapter_1.gguf --lora my_adapter_2.gguf
--lora-scaled lora_task_A.gguf 0.5 --lora-scaled lora_task_B.gguf 0.5
```

----------------------------------------

TITLE: Example Workflow for Generating and Using Importance Matrix
DESCRIPTION: A complete workflow showing how to generate an importance matrix and then use it for quantizing a model. The example demonstrates using GPU offloading for faster computation and applying the generated imatrix for Q4_K_M quantization.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/imatrix/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
# generate importance matrix (imatrix.dat)
./llama-imatrix -m ggml-model-f16.gguf -f train-data.txt -ngl 99

# use the imatrix to perform a Q4_K_M quantization
./llama-quantize --imatrix imatrix.dat ggml-model-f16.gguf ./ggml-model-q4_k_m.gguf q4_k_m
```

----------------------------------------

TITLE: Running MobileVLM Model with llama-mtmd-cli
DESCRIPTION: Command-line example for running a converted MobileVLM model using the llama-mtmd-cli tool with the deepseek chat template.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_0

LANGUAGE: sh
CODE:
```
./llama-mtmd-cli -m MobileVLM-1.7B/ggml-model-q4_k.gguf \
    --mmproj MobileVLM-1.7B/mmproj-model-f16.gguf \
    --chat-template deepseek
```

----------------------------------------

TITLE: Running Llama Infill with Code Llama Model for Code Completion
DESCRIPTION: Example command for running the llama-infill program with a CodeLlama model to complete code between a prefix and suffix. This demonstrates providing code context before and after the cursor position where completion should happen.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/infill/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
./llama-infill -t 10 -ngl 0 -m models/codellama-13b.Q5_K_S.gguf -c 4096 --temp 0.7 --repeat_penalty 1.1 -n 20 --in-prefix "def helloworld():\n    print(\"hell" --in-suffix "\n   print(\"goodbye world\")\n    "
```

----------------------------------------

TITLE: Setting Context Size in LLaMA.cpp
DESCRIPTION: Sets the size of the prompt context window for LLaMA models. The default is 4096 tokens, and setting to 0 loads the size from the model.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_12

LANGUAGE: bash
CODE:
```
-c N, --ctx-size N
```

----------------------------------------

TITLE: Converting Gemma 3 Models with MMPROJ Support
DESCRIPTION: Commands for converting Hugging Face models to GGUF format with MMPROJ support for vision capabilities.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/gemma3.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
cd gemma-3-4b-it
python ../llama.cpp/convert_hf_to_gguf.py --outfile model.gguf --outtype f16 --mmproj .
# output file: mmproj-model.gguf
```

----------------------------------------

TITLE: Building llama.cpp with OpenBLAS
DESCRIPTION: Command to build llama.cpp with OpenBLAS acceleration using CMake. This can provide performance improvements for certain operations.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

----------------------------------------

TITLE: Configuring RoPE Scaling for Extended Context in LLaMA.cpp
DESCRIPTION: Sets the linear scaling factor for RoPE (Rotary Position Embedding) when using models fine-tuned for extended context lengths.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_13

LANGUAGE: bash
CODE:
```
--rope-scale N
```

----------------------------------------

TITLE: Running Automated Benchmark with CI Python Script
DESCRIPTION: Example command for running the bench.py script which automates server startup, benchmark execution, and metrics collection. Includes various parameters for customizing the benchmark configuration.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/bench/README.md#2025-04-22_snippet_7

LANGUAGE: shell
CODE:
```
LLAMA_SERVER_BIN_PATH=../../../cmake-build-release/bin/llama-server python bench.py \
              --runner-label local \
              --name local \
              --branch `git rev-parse --abbrev-ref HEAD` \
              --commit `git rev-parse HEAD` \
              --scenario script.js \
              --duration 5m \
              --hf-repo ggml-org/models	 \
              --hf-file phi-2/ggml-model-q4_0.gguf \
              --model-path-prefix models \
              --parallel 4 \
              -ngl 33 \
              --batch-size 2048 \
              --ubatch-size	256 \
              --ctx-size 4096 \
              --n-prompts 200 \
              --max-prompt-tokens 256 \
              --max-tokens 256
```

----------------------------------------

TITLE: Token Probability Response Format in JSON
DESCRIPTION: JSON structure showing token probability information including logprob/prob values, token IDs, and byte representations. Used for both standard logprob and post-sampling probability outputs.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_9

LANGUAGE: json
CODE:
```
{
  "logprob": "float",
  "token": "<most likely token>",
  "bytes": ["int", "int", "..."],
  "top_logprobs": [
    {
      "id": "<token id>",
      "logprob": "float",
      "token": "<token text>",
      "bytes": ["int", "int", "..."]
    }
  ]
}
```

----------------------------------------

TITLE: Document Reranking Request Example
DESCRIPTION: Example curl command demonstrating how to use the reranking endpoint to rank documents based on relevance to a query.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_12

LANGUAGE: shell
CODE:
```
curl http://127.0.0.1:8012/v1/rerank \
    -H "Content-Type: application/json" \
    -d '{
        "model": "some-model",
            "query": "What is panda?",
            "top_n": 3,
            "documents": [
                "hi",
            "it is a bear",
            "The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China."
            ]
    }' | jq
```

----------------------------------------

TITLE: Building llama.cpp with SYCL for NVIDIA GPU
DESCRIPTION: CMake commands to build llama.cpp with SYCL support for NVIDIA GPUs, including options for FP32 and FP16.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_7

LANGUAGE: sh
CODE:
```
GGML_SYCL_DEVICE_ARCH=sm_80 # Example architecture

# Option 1: Use FP32 (recommended for better performance in most cases)
cmake -B build -DGGML_SYCL=ON -DGGML_SYCL_TARGET=NVIDIA -DGGML_SYCL_DEVICE_ARCH=${GGML_SYCL_DEVICE_ARCH} -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DDNNL_DIR=/path/to/oneDNN/build-nvidia/install/lib/cmake/dnnl

# Option 2: Use FP16
cmake -B build -DGGML_SYCL=ON -DGGML_SYCL_TARGET=NVIDIA -DGGML_SYCL_DEVICE_ARCH=${GGML_SYCL_DEVICE_ARCH} -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_SYCL_F16=ON -DDNNL_DIR=/path/to/oneDNN/build-nvidia/install/lib/cmake/dnnl

# build all binary
cmake --build build --config Release -j -v
```

----------------------------------------

TITLE: Enabling Link-Time Optimization
DESCRIPTION: Checks if Link-Time Optimization (LTO) is supported by the compiler and enables it when GGML_LTO is set. LTO improves performance by allowing optimizations across compilation units.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_5

LANGUAGE: CMake
CODE:
```
if (GGML_LTO)
    include(CheckIPOSupported)
    check_ipo_supported(RESULT result OUTPUT output)
    if (result)
        set(CMAKE_INTERPROCEDURAL_OPTIMIZATION TRUE)
    else()
        message(WARNING "IPO is not supported: ${output}")
    endif()
endif()
```

----------------------------------------

TITLE: Building and Running Pre-quantized Gemma 3 Models
DESCRIPTION: Instructions for building llama.cpp and running pre-quantized Gemma 3 vision models from ggml-org's Hugging Face repository.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/gemma3.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
# build
cmake -B build
cmake --build build --target llama-gemma3-cli

# alternatively, install from brew (MacOS)
brew install llama.cpp

# run it
llama-gemma3-cli -hf ggml-org/gemma-3-4b-it-GGUF
llama-gemma3-cli -hf ggml-org/gemma-3-12b-it-GGUF
llama-gemma3-cli -hf ggml-org/gemma-3-27b-it-GGUF

# note: 1B model does not support vision
```

----------------------------------------

TITLE: Setting Token Prediction Count in LLaMA.cpp
DESCRIPTION: Configures how many tokens the model should generate. Default is -1 (infinity), -2 stops when context is filled.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_15

LANGUAGE: bash
CODE:
```
-n N, --predict N
```

----------------------------------------

TITLE: Using llava_surgery_v2.py for LLaVA 1.6
DESCRIPTION: Python command to use the llava_surgery_v2.py script for processing LLaVA 1.6 models, which supports both PyTorch and safetensor formats.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_7

LANGUAGE: sh
CODE:
```
python examples/llava/llava_surgery_v2.py -C -m ../llava-v1.6-vicuna-7b/
```

----------------------------------------

TITLE: Building llama.cpp with CMake (CPU)
DESCRIPTION: Commands to build llama.cpp using CMake for a CPU-only configuration. This is the basic build process without any specific accelerations.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
cmake -B build
cmake --build build --config Release
```

----------------------------------------

TITLE: Referencing Function Calling Implementation in chat.h
DESCRIPTION: The chat.h file in the common directory adds support for OpenAI-style function calling as implemented in PR #9639. This functionality is used in llama-server with the --jinja flag and is being worked on for llama-cli in PR #11556.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/function-calling.md#2025-04-22_snippet_0

LANGUAGE: markdown
CODE:
```
[chat.h](../common/chat.h) (https://github.com/ggml-org/llama.cpp/pull/9639) adds support for [OpenAI-style function calling](https://platform.openai.com/docs/guides/function-calling) and is used in:
- `llama-server` when started w/ `--jinja` flag
- `llama-cli` (WIP: https://github.com/ggml-org/llama.cpp/pull/11556)
```

----------------------------------------

TITLE: Running MobileVLM on Android (Llama Image Example)
DESCRIPTION: Example command for running the MobileVLM model on a Snapdragon 778G device to identify animals in an image. This example demonstrates the model's performance on mid-range Android hardware.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_10

LANGUAGE: sh
CODE:
```
/data/local/tmp/llama-mtmd-cli \
    -m /data/local/tmp/ggml-model-q4_k.gguf \
    --mmproj /data/local/tmp/mmproj-model-f16.gguf \
    -t 4 \
    --image /data/local/tmp/many_llamas.jpeg \
    -p "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\nWhat's that? ASSISTANT:"
```

----------------------------------------

TITLE: Configuring Repetition Penalty in LLaMA.cpp
DESCRIPTION: Controls how strongly the model penalizes repetition of token sequences. Default is 1.0 (disabled), and higher values (e.g., 1.5) more strongly discourage repetition.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_17

LANGUAGE: bash
CODE:
```
--repeat-penalty N
```

----------------------------------------

TITLE: Querying llama.cpp Server Metrics Endpoint
DESCRIPTION: Command to retrieve server metrics from the metrics endpoint, which can be compared against k6 metrics for performance analysis.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/bench/README.md#2025-04-22_snippet_6

LANGUAGE: shell
CODE:
```
curl http://localhost:8080/metrics
```

----------------------------------------

TITLE: Converting MiniCPM-V 2.6 PyTorch Model to GGUF Format
DESCRIPTION: Series of commands to convert the MiniCPM-V 2.6 PyTorch model to GGUF format for use with llama.cpp. Includes creating the projector model, converting the image encoder, and quantizing to int4 for better performance.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/minicpmv2.6.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
python ./examples/llava/minicpmv-surgery.py -m ../MiniCPM-V-2_6
python ./examples/llava/minicpmv-convert-image-encoder-to-gguf.py -m ../MiniCPM-V-2_6 --minicpmv-projector ../MiniCPM-V-2_6/minicpmv.projector --output-dir ../MiniCPM-V-2_6/ --image-mean 0.5 0.5 0.5 --image-std 0.5 0.5 0.5 --minicpmv_version 3
python ./convert_hf_to_gguf.py ../MiniCPM-V-2_6/model

# quantize int4 version
./build/bin/llama-quantize ../MiniCPM-V-2_6/model/ggml-model-f16.gguf ../MiniCPM-V-2_6/model/ggml-model-Q4_K_M.gguf Q4_K_M
```

----------------------------------------

TITLE: Running MiniCPM-V 2.6 Inference with llama.cpp
DESCRIPTION: Commands for running inference with the converted MiniCPM-V 2.6 model on Linux or Mac. Shows how to run in both single-turn mode for individual image analysis and conversation mode for interactive use.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/minicpmv2.6.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
# run in single-turn mode
./build/bin/llama-mtmd-cli -m ../MiniCPM-V-2_6/model/ggml-model-f16.gguf --mmproj ../MiniCPM-V-2_6/mmproj-model-f16.gguf -c 4096 --temp 0.7 --top-p 0.8 --top-k 100 --repeat-penalty 1.05 --image xx.jpg -p "What is in the image?"

# run in conversation mode
./build/bin/llama-mtmd-cli -m ../MiniCPM-V-2_6/model/ggml-model-Q4_K_M.gguf --mmproj ../MiniCPM-V-2_6/mmproj-model-f16.gguf
```

----------------------------------------

TITLE: Building llama.cpp with LLGuidance Support
DESCRIPTION: Commands to build llama.cpp with LLGuidance support enabled. Requires Rust compiler and cargo tool to be installed. Includes commands for both Unix-like systems and Windows.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/llguidance.md#2025-04-22_snippet_0

LANGUAGE: sh
CODE:
```
cmake -B build -DLLAMA_LLGUIDANCE=ON
make -C build -j
```

----------------------------------------

TITLE: Building llama.cpp with CANN Support
DESCRIPTION: CMake commands to build llama.cpp with CANN backend support
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CANN.md#2025-04-22_snippet_6

LANGUAGE: shell
CODE:
```
cmake -B build -DGGML_CANN=on -DCMAKE_BUILD_TYPE=release
cmake --build build --config release
```

----------------------------------------

TITLE: Building CUDA Backend with RPC Support
DESCRIPTION: Commands to build the CUDA backend with RPC support using CMake. This enables the use of CUDA GPUs for distributed inference.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/rpc/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
mkdir build-rpc-cuda
cd build-rpc-cuda
cmake .. -DGGML_CUDA=ON -DGGML_RPC=ON
cmake --build . --config Release
```

----------------------------------------

TITLE: Server Properties Response Format
DESCRIPTION: JSON structure returned by the GET /props endpoint showing server's global properties including default generation settings, total slots, model path and build info.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_13

LANGUAGE: json
CODE:
```
{
  "default_generation_settings": {
    "id": 0,
    "id_task": -1,
    "n_ctx": 1024,
    "speculative": false,
    "is_processing": false,
    "params": {
      "n_predict": -1,
      "seed": 4294967295,
      "temperature": 0.800000011920929,
      "dynatemp_range": 0.0,
      "dynatemp_exponent": 1.0,
      "top_k": 40,
      "top_p": 0.949999988079071,
      "min_p": 0.05000000074505806,
      "xtc_probability": 0.0,
      "xtc_threshold": 0.10000000149011612,
      "typical_p": 1.0,
      "repeat_last_n": 64,
      "repeat_penalty": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "dry_multiplier": 0.0,
      "dry_base": 1.75,
      "dry_allowed_length": 2,
      "dry_penalty_last_n": -1,
      "dry_sequence_breakers": [
        "\n",
        ":",
        "\"",
        "*"
      ],
      "mirostat": 0,
      "mirostat_tau": 5.0,
      "mirostat_eta": 0.10000000149011612,
      "stop": [],
      "max_tokens": -1,
      "n_keep": 0,
      "n_discard": 0,
      "ignore_eos": false,
      "stream": true,
      "n_probs": 0,
      "min_keep": 0,
      "grammar": "",
      "samplers": [
        "dry",
        "top_k",
        "typ_p",
        "top_p",
        "min_p",
        "xtc",
        "temperature"
      ],
      "speculative.n_max": 16,
      "speculative.n_min": 5,
      "speculative.p_min": 0.8999999761581421,
      "timings_per_token": false
    },
    "prompt": "",
    "next_token": {
      "has_next_token": true,
      "has_new_line": false,
      "n_remain": -1,
      "n_decoded": 0,
      "stopping_word": ""
    }
  },
  "total_slots": 1,
  "model_path": "../models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
  "chat_template": "...",
  "build_info": "b(build number)-(build commit hash)"
}
```

----------------------------------------

TITLE: Setting Repetition Penalty Window in LLaMA.cpp
DESCRIPTION: Determines how many previous tokens to consider when applying repetition penalty. Default is 64, 0 disables the feature, and -1 sets it to the context size.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_18

LANGUAGE: bash
CODE:
```
--repeat-last-n N
```

----------------------------------------

TITLE: Building llama.cpp with SYCL for Intel GPU
DESCRIPTION: CMake commands to build llama.cpp with SYCL support for Intel GPUs, including options for FP32 and FP16.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_6

LANGUAGE: sh
CODE:
```
source /opt/intel/oneapi/setvars.sh

# Option 1: Use FP32 (recommended for better performance in most cases)
cmake -B build -DGGML_SYCL=ON -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx

# Option 2: Use FP16
cmake -B build -DGGML_SYCL=ON -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_SYCL_F16=ON

# build all binary
cmake --build build --config Release -j -v
```

----------------------------------------

TITLE: Starting llama.cpp Server for Benchmarking
DESCRIPTION: Example command to start the llama-server with recommended settings for benchmarking. Configures continuous batching, metrics collection, and other performance parameters.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/bench/README.md#2025-04-22_snippet_3

LANGUAGE: shell
CODE:
```
llama-server --host localhost --port 8080 \
  --model ggml-model-q4_0.gguf \
  --cont-batching \
  --metrics \
  --parallel 8 \
  --batch-size 512 \
  --ctx-size 4096 \
  -ngl 33
```

----------------------------------------

TITLE: Using perplexity with KL divergence calculation in llama.cpp
DESCRIPTION: Commands for using the perplexity tool to calculate KL divergence between a full precision and quantized model by first recording logits from the FP16 model and then comparing them to the quantized model.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/perplexity/README.md#2025-04-22_snippet_0

LANGUAGE: markdown
CODE:
```
--kl-divergence-base path/to/logit/binary/file.kld
```

----------------------------------------

TITLE: Configuring Top-P (Nucleus) Sampling in LLaMA.cpp
DESCRIPTION: Limits token selection to a subset with cumulative probability above threshold P. Default is 0.9, with higher values enabling more diverse generation.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_21

LANGUAGE: bash
CODE:
```
--top-p N
```

LANGUAGE: bash
CODE:
```
--top-p 0.95
```

----------------------------------------

TITLE: Cloning and Building llama.cpp with CMake
DESCRIPTION: Instructions for cloning the llama.cpp repository from GitHub and building it using CMake. This is a prerequisite step before working with the MiniCPM-V 2.6 model.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/minicpmv2.6.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
```

LANGUAGE: bash
CODE:
```
cmake -B build
cmake --build build --config Release
```

----------------------------------------

TITLE: Running llama-bench with JSON output format
DESCRIPTION: Command to run the llama-bench tool with JSON output format. This produces structured benchmark results as a JSON array, suitable for programmatic processing.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_8

LANGUAGE: sh
CODE:
```
$ ./llama-bench -o json
```

----------------------------------------

TITLE: Batch Size Performance Testing Command
DESCRIPTION: Example command for testing prompt processing performance with different batch sizes.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_2

LANGUAGE: sh
CODE:
```
./llama-bench -n 0 -p 1024 -b 128,256,512,1024
```

----------------------------------------

TITLE: HIP GPU Compilation for Linux
DESCRIPTION: CMake compilation commands for AMD GPU support using HIP on Linux, including ROCm configuration and optional rocWMMA optimization.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_13

LANGUAGE: bash
CODE:
```
HIPCXX="$(hipconfig -l)/clang" HIP_PATH="$(hipconfig -R)" \
    cmake -S . -B build -DGGML_HIP=ON -DAMDGPU_TARGETS=gfx1030 -DCMAKE_BUILD_TYPE=Release \
    && cmake --build build --config Release -- -j 16
```

----------------------------------------

TITLE: Configuring GritLM Executable Target in CMake
DESCRIPTION: Defines a CMake target named llama-gritlm that builds an executable from gritlm.cpp. The configuration specifies installation rules, links necessary libraries (common, llama, and thread libraries), and requires C++17 standard support.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gritlm/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(TARGET llama-gritlm)
add_executable(${TARGET} gritlm.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Cross-compiling llama.cpp with Android NDK
DESCRIPTION: CMake command for configuring a cross-compilation build of llama.cpp for Android using the Android NDK, with specific flags for arm64-v8a architecture and other optimizations.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/android.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
$ cmake \
  -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \
  -DANDROID_ABI=arm64-v8a \
  -DANDROID_PLATFORM=android-28 \
  -DCMAKE_C_FLAGS="-march=armv8.7a" \
  -DCMAKE_CXX_FLAGS="-march=armv8.7a" \
  -DGGML_OPENMP=OFF \
  -DGGML_LLAMAFILE=OFF \
  -B build-android
```

----------------------------------------

TITLE: Running One-Shot Prompt Generation in Unix
DESCRIPTION: Command to execute a single prompt inference using the Gemma model in Unix-based systems
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
./llama-cli -m models/gemma-1.1-7b-it.Q4_K_M.gguf -no-cnv --prompt "Once upon a time"
```

----------------------------------------

TITLE: Installing Vulkan SDK on Ubuntu
DESCRIPTION: Commands to install the Vulkan SDK on Ubuntu 22.04 (jammy) for building llama.cpp with Vulkan support.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_19

LANGUAGE: bash
CODE:
```
wget -qO - https://packages.lunarg.com/lunarg-signing-key-pub.asc | apt-key add -
wget -qO /etc/apt/sources.list.d/lunarg-vulkan-jammy.list https://packages.lunarg.com/vulkan/lunarg-vulkan-jammy.list
apt update -y
apt-get install -y vulkan-sdk
# To verify the installation, use the command below:
vulkaninfo
```

----------------------------------------

TITLE: Using Locally Typical Sampling in LLaMA.cpp
DESCRIPTION: Command line example for enabling locally typical sampling with a parameter value of 0.9. This promotes the generation of contextually coherent and diverse text by sampling tokens that are typical based on context.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_23

LANGUAGE: shell
CODE:
```
--typical 0.9
```

----------------------------------------

TITLE: Running Simple Chat Example with llama.cpp
DESCRIPTION: This command demonstrates how to run the simple chat example using llama.cpp. It specifies the model file to use and sets the context size. The ellipsis indicates that more output would follow when executing this command.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/simple-chat/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
./llama-simple-chat -m Meta-Llama-3.1-8B-Instruct.gguf -c 2048
...
```

----------------------------------------

TITLE: Running Node.js Client
DESCRIPTION: Command to execute the Node.js client script
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_7

LANGUAGE: bash
CODE:
```
node index.js
```

----------------------------------------

TITLE: Example GBNF Grammar Generated from JSON Schema
DESCRIPTION: A sample GBNF grammar produced from converting the name-age JSON schema. The grammar defines rules for generating valid JSON that conforms to the original schema constraints.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md#2025-04-22_snippet_7

LANGUAGE: plaintext
CODE:
```
char ::= [^"\\\x7F\x00-\x1F] | [\\] (["\\bfnrt] | "u" [0-9a-fA-F]{4})
item ::= "{" space item-name-kv "," space item-age-kv "}" space
item-age ::= ([0-9] | ([1-8] [0-9] | [9] [0-9]) | "1" ([0-4] [0-9] | [5] "0")) space
item-age-kv ::= "\"age\"" space ":" space item-age
item-name ::= "\"" char{1,100} "\"" space
item-name-kv ::= "\"name\"" space ":" space item-name
root ::= "[" space item ("," space item){9,99} "]" space
space ::= | " " | "\n" [ \t]{0,20}
```

----------------------------------------

TITLE: Defining Python Package Dependencies
DESCRIPTION: Specifies the required Python packages and their version constraints. Includes numpy for numerical computing, sentencepiece for tokenization, transformers for machine learning models, gguf for model format support, and protobuf for data serialization.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-convert_legacy_llama.txt#2025-04-22_snippet_0

LANGUAGE: plaintext
CODE:
```
numpy~=1.26.4
sentencepiece~=0.2.0
transformers>=4.45.1,<5.0.0
gguf>=0.1.0
protobuf>=4.21.0,<5.0.0
```

----------------------------------------

TITLE: Executing llama.cpp with SYCL on Linux (Multiple Devices)
DESCRIPTION: Shell command to run llama.cpp inference using multiple SYCL devices on Linux.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_12

LANGUAGE: sh
CODE:
```
./examples/sycl/run-llama2.sh
```

----------------------------------------

TITLE: Running Inference with MiniCPM-o 2.6 on Linux or Mac
DESCRIPTION: Commands to run inference with the MiniCPM-o 2.6 model in both single-turn and conversation modes. The single-turn mode processes a specific image with a prompt, while conversation mode enables interactive dialogue.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/minicpmo2.6.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
# run in single-turn mode
./build/bin/llama-mtmd-cli -m ../MiniCPM-o-2_6/model/ggml-model-f16.gguf --mmproj ../MiniCPM-o-2_6/mmproj-model-f16.gguf -c 4096 --temp 0.7 --top-p 0.8 --top-k 100 --repeat-penalty 1.05 --image xx.jpg -p "What is in the image?"

# run in conversation mode
./build/bin/llama-mtmd-cli -m ../MiniCPM-o-2_6/model/ggml-model-Q4_K_M.gguf --mmproj ../MiniCPM-o-2_6/mmproj-model-f16.gguf
```

----------------------------------------

TITLE: Building llama.cpp with OpenCL for Android
DESCRIPTION: Commands to clone and build llama.cpp with OpenCL support for Android, using CMake and Ninja to configure the build for ARM64 architecture.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/OPENCL.md#2025-04-22_snippet_3

LANGUAGE: sh
CODE:
```
cd ~/dev/llm

git clone https://github.com/ggml-org/llama.cpp && \
cd llama.cpp && \
mkdir build-android && cd build-android

cmake .. -G Ninja \
  -DCMAKE_TOOLCHAIN_FILE=$HOME/android-sdk/ndk/26.3.11579264/build/cmake/android.toolchain.cmake \
  -DANDROID_ABI=arm64-v8a \
  -DANDROID_PLATFORM=android-28 \
  -DBUILD_SHARED_LIBS=OFF \
  -DGGML_OPENCL=ON

ninja
```

----------------------------------------

TITLE: Configuring build targets and linking dependencies for llama-server
DESCRIPTION: Sets up the executable build target, installation rules, and links required libraries including threading support. Conditionally includes OpenSSL for SSL support and adds Windows-specific socket libraries when building on Windows platforms.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: cmake
CODE:
```
add_executable(${TARGET} ${TARGET_SRCS})
install(TARGETS ${TARGET} RUNTIME)

target_include_directories(${TARGET} PRIVATE ${CMAKE_SOURCE_DIR})
target_link_libraries(${TARGET} PRIVATE common ${CMAKE_THREAD_LIBS_INIT})

if (LLAMA_SERVER_SSL)
    find_package(OpenSSL REQUIRED)
    target_link_libraries(${TARGET} PRIVATE OpenSSL::SSL OpenSSL::Crypto)
    target_compile_definitions(${TARGET} PRIVATE CPPHTTPLIB_OPENSSL_SUPPORT)
endif()

if (WIN32)
    TARGET_LINK_LIBRARIES(${TARGET} PRIVATE ws2_32)
endif()

target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Setting Up Bash Completion for llama-cli
DESCRIPTION: Shows how to generate and enable Bash command-line completion for the llama-cli tool. Includes the command to generate completion script and how to load it in the current session or permanently in the .bashrc file.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_16

LANGUAGE: bash
CODE:
```
$ build/bin/llama-cli --completion-bash > ~/.llama-completion.bash
$ source ~/.llama-completion.bash
```

LANGUAGE: bash
CODE:
```
$ echo "source ~/.llama-completion.bash" >> ~/.bashrc
```

----------------------------------------

TITLE: Running One-Shot Prompt Generation in Windows
DESCRIPTION: Command to execute a single prompt inference using the Gemma model in Windows
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_5

LANGUAGE: powershell
CODE:
```
./llama-cli.exe -m models\gemma-1.1-7b-it.Q4_K_M.gguf -no-cnv --prompt "Once upon a time"
```

----------------------------------------

TITLE: Configuring GGML Backend Library for HIP in CMake
DESCRIPTION: Sets up the GGML backend library for HIP, including source files, compile definitions, and linking libraries. It handles different configurations based on CUDA and HIP-specific options.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-hip/CMakeLists.txt#2025-04-22_snippet_3

LANGUAGE: cmake
CODE:
```
file(GLOB   GGML_HEADERS_ROCM "../ggml-cuda/*.cuh")
list(APPEND GGML_HEADERS_ROCM "../../include/ggml-cuda.h")

file(GLOB   GGML_SOURCES_ROCM "../ggml-cuda/*.cu")
file(GLOB   SRCS "../ggml-cuda/template-instances/fattn-mma*.cu")
list(APPEND GGML_SOURCES_ROCM ${SRCS})
file(GLOB   SRCS "../ggml-cuda/template-instances/mmq*.cu")
list(APPEND GGML_SOURCES_ROCM ${SRCS})

if (GGML_CUDA_FA_ALL_QUANTS)
    file(GLOB   SRCS "../ggml-cuda/template-instances/fattn-vec*.cu")
    list(APPEND GGML_SOURCES_ROCM ${SRCS})
    add_compile_definitions(GGML_CUDA_FA_ALL_QUANTS)
else()
    file(GLOB   SRCS "../ggml-cuda/template-instances/fattn-vec*q4_0-q4_0.cu")
    list(APPEND GGML_SOURCES_ROCM ${SRCS})
    file(GLOB   SRCS "../ggml-cuda/template-instances/fattn-vec*q8_0-q8_0.cu")
    list(APPEND GGML_SOURCES_ROCM ${SRCS})
    file(GLOB   SRCS "../ggml-cuda/template-instances/fattn-vec*f16-f16.cu")
    list(APPEND GGML_SOURCES_ROCM ${SRCS})
endif()

ggml_add_backend_library(ggml-hip
                         ${GGML_HEADERS_ROCM}
                         ${GGML_SOURCES_ROCM}
                        )

# TODO: do not use CUDA definitions for HIP
if (NOT GGML_BACKEND_DL)
    target_compile_definitions(ggml PUBLIC GGML_USE_CUDA)
endif()

add_compile_definitions(GGML_USE_HIP)

if (GGML_CUDA_FORCE_MMQ)
    add_compile_definitions(GGML_CUDA_FORCE_MMQ)
endif()

if (GGML_CUDA_FORCE_CUBLAS)
    add_compile_definitions(GGML_CUDA_FORCE_CUBLAS)
endif()

if (GGML_CUDA_NO_PEER_COPY)
    add_compile_definitions(GGML_CUDA_NO_PEER_COPY)
endif()

if (GGML_HIP_GRAPHS)
    add_compile_definitions(GGML_HIP_GRAPHS)
endif()

if (GGML_HIP_NO_VMM)
    add_compile_definitions(GGML_HIP_NO_VMM)
endif()

if (GGML_HIP_ROCWMMA_FATTN)
    add_compile_definitions(GGML_HIP_ROCWMMA_FATTN)
endif()

if (NOT GGML_CUDA_FA)
    add_compile_definitions(GGML_CUDA_NO_FA)
endif()

if (CXX_IS_HIPCC)
    set_source_files_properties(${GGML_SOURCES_ROCM} PROPERTIES LANGUAGE CXX)
    target_link_libraries(ggml-hip PRIVATE hip::device)
else()
    set_source_files_properties(${GGML_SOURCES_ROCM} PROPERTIES LANGUAGE HIP)
endif()

if (GGML_STATIC)
    message(FATAL_ERROR "Static linking not supported for HIP/ROCm")
endif()

target_link_libraries(ggml-hip PRIVATE ggml-base hip::host roc::rocblas roc::hipblas)
```

----------------------------------------

TITLE: Converting LLaVA 1.6 Model to GGUF
DESCRIPTION: Python command to convert the LLaVA 1.6 model to GGUF format using the convert_legacy_llama.py script.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_10

LANGUAGE: sh
CODE:
```
python ./examples/convert_legacy_llama.py ../llava-v1.6-vicuna-7b/ --skip-unknown
```

----------------------------------------

TITLE: Running SimpleChat with Python HTTP Server
DESCRIPTION: Commands to start llama-server and then serve the SimpleChat frontend using Python's HTTP server
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/public_simplechat/readme.md#2025-04-22_snippet_1

LANGUAGE: shell
CODE:
```
./llama-server -m path/model.gguf
cd ../examples/server/public_simplechat
python3 -m http.server PORT
```

----------------------------------------

TITLE: Starting RPC Server with Local Cache
DESCRIPTION: Command to start the RPC server with local caching enabled. This can significantly speed up model loading for large models.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/rpc/README.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
$ bin/rpc-server -c
```

----------------------------------------

TITLE: Vicuna 1.1 Template Example
DESCRIPTION: A template for the Vicuna 1.1 chat format, which uses a specific structure for conversation history with user and assistant labels.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama.android/README.md#2025-04-22_snippet_2

LANGUAGE: plaintext
CODE:
```
{% if system_prompt %}
{{ system_prompt }}
{% endif %}

{% for message in messages %}
{% if message['role'] == 'user' %}
{{ message['role'].upper() }}: {{ message['content'] }}

{% elif message['role'] == 'assistant' %}
{{ message['role'].upper() }}: {{ message['content'] }}

{% endif %}
{% endfor %}

ASSISTANT:
```

----------------------------------------

TITLE: GPU Offloading Diagnostic Output
DESCRIPTION: Example diagnostic output showing successful GPU offloading with cuBLAS, indicating the number of layers moved to GPU and total VRAM usage.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/token_generation_performance_tips.md#2025-04-22_snippet_1

LANGUAGE: shell
CODE:
```
llama_model_load_internal: [cublas] offloading 60 layers to GPU
llama_model_load_internal: [cublas] offloading output layer to GPU
llama_model_load_internal: [cublas] total VRAM used: 17223 MB
... rest of inference
```

----------------------------------------

TITLE: Preparing CLIP Model Files for LLaVA 1.6
DESCRIPTION: Shell commands to prepare the CLIP model files for LLaVA 1.6, including creating directories, copying files, and downloading configuration.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_8

LANGUAGE: sh
CODE:
```
mkdir vit
cp ../llava-v1.6-vicuna-7b/llava.clip vit/pytorch_model.bin
cp ../llava-v1.6-vicuna-7b/llava.projector vit/
curl -s -q https://huggingface.co/cmp-nct/llava-1.6-gguf/raw/main/config_vit.json -o vit/config.json
```

----------------------------------------

TITLE: Basic Chat Template Example
DESCRIPTION: A simple example of a chat template for llama.cpp showing the syntax with variables like {{system}}, {{#each messages}}, etc. This demonstrates how to format conversations between users and assistants.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama.android/README.md#2025-04-22_snippet_0

LANGUAGE: plaintext
CODE:
```
{% for message in messages %}
{% if message['role'] == 'system' %}
{{ message['content'] }}
{% else %}
{{ message['role'] }}: {{ message['content'] }}
{% endif %}
{% endfor %}
{{ roles['assistant'] }}:
```

----------------------------------------

TITLE: Displaying Sampling Parameters Table in Markdown
DESCRIPTION: A markdown table listing sampling-related command-line parameters for the llama.cpp project, including their descriptions and default values.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_1

LANGUAGE: markdown
CODE:
```
| Argument | Explanation |
| -------- | ----------- |
| `--samplers SAMPLERS` | samplers that will be used for generation in the order, separated by ';'<br/>(default: dry;top_k;typ_p;top_p;min_p;xtc;temperature) |
| `-s, --seed SEED` | RNG seed (default: -1, use random seed for -1) |
| `--sampling-seq SEQUENCE` | simplified sequence for samplers that will be used (default: dkypmxt) |
| `--ignore-eos` | ignore end of stream token and continue generating (implies --logit-bias EOS-inf) |
| `--temp N` | temperature (default: 0.8) |
| `--top-k N` | top-k sampling (default: 40, 0 = disabled) |
| `--top-p N` | top-p sampling (default: 0.9, 1.0 = disabled) |
| `--min-p N` | min-p sampling (default: 0.1, 0.0 = disabled) |
| `--xtc-probability N` | xtc probability (default: 0.0, 0.0 = disabled) |
| `--xtc-threshold N` | xtc threshold (default: 0.1, 1.0 = disabled) |
| `--typical N` | locally typical sampling, parameter p (default: 1.0, 1.0 = disabled) |
| `--repeat-last-n N` | last n tokens to consider for penalize (default: 64, 0 = disabled, -1 = ctx_size) |
| `--repeat-penalty N` | penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled) |
| `--presence-penalty N` | repeat alpha presence penalty (default: 0.0, 0.0 = disabled) |
| `--frequency-penalty N` | repeat alpha frequency penalty (default: 0.0, 0.0 = disabled) |
| `--dry-multiplier N` | set DRY sampling multiplier (default: 0.0, 0.0 = disabled) |
| `--dry-base N` | set DRY sampling base value (default: 1.75) |
| `--dry-allowed-length N` | set allowed length for DRY sampling (default: 2) |
| `--dry-penalty-last-n N` | set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 = context size) |
```

----------------------------------------

TITLE: Running LLaMA Multi-Modal CLI for Image Description
DESCRIPTION: Command example for generating a natural language description of an image using llama-mtmd-cli with GPU acceleration.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_13

LANGUAGE: sh
CODE:
```
./llama-mtmd-cli \
    -m /data/local/tmp/ggml-model-q4_k.gguf \
    --mmproj /data/local/tmp/mmproj-model-f16.gguf \
    -p "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\nWhat is in the image? ASSISTANT:" \
    --n-gpu-layers 999
```

----------------------------------------

TITLE: Configuring and Building llama-infill Executable with CMake
DESCRIPTION: Sets up the llama-infill target by defining the executable, specifying installation parameters, linking required libraries, and setting the C++ standard to C++17.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/infill/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
set(TARGET llama-infill)
add_executable(${TARGET} infill.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Configuring LLM Model Parameters in Bash
DESCRIPTION: Configuration script setup for specifying model path, name, prompt prefix, and additional options for running the Jeopardy test suite.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/jeopardy/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
MODEL=(path to your model)
MODEL_NAME=(name of your model)
prefix=(basically, if you use vicuna it's Human: , if you use something else it might be User: , etc)
opts=(add -instruct here if needed for your model, or anything else you want to test out)
```

----------------------------------------

TITLE: Using Jinja Chat Template in Windows
DESCRIPTION: Command to start a conversation using the built-in Jinja chat template in Windows
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_7

LANGUAGE: powershell
CODE:
```
./llama-cli.exe -m models\gemma-1.1-7b-it.Q4_K_M.gguf --jinja
```

----------------------------------------

TITLE: Running llama.cpp on Multiple SYCL Devices
DESCRIPTION: Command for running llama.cpp inference distributed across multiple SYCL devices. This uses 'layer' split mode to automatically distribute model layers across available devices with the same backend.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_27

LANGUAGE: batch
CODE:
```
build\bin\llama-cli.exe -no-cnv -m models\llama-2-7b.Q4_0.gguf -p "Building a website can be done in 10 simple steps:\nStep 1:" -n 400 -e -ngl 33 -s 0 -sm layer
```

----------------------------------------

TITLE: LLaMA 2 vs LLaMA 3 Quantization Metrics Table
DESCRIPTION: Markdown table comparing quantization metrics between LLaMA 2 7B and LLaMA 3 8B models across different quantization methods (q2_K through q8_0), including perplexity, correlation, and probability distribution metrics.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/perplexity/README.md#2025-04-22_snippet_1

LANGUAGE: markdown
CODE:
```
| Metric          |          L2 7b q2_K |          L3 8b q2_K |        L2 7b q4_K_M |        L3 8b q4_K_M |          L2 7b q6_K |          L3 8b q6_K |          L2 7b q8_0 |          L3 8b q8_0 |
|-----------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|
| Mean PPL        | 5.794552 Â± 0.032298 | 9.751568 Â± 0.063312 | 5.877078 Â± 0.032781 | 6.407115 Â± 0.039119 | 5.808494 Â± 0.032425 | 6.253382 Â± 0.038078 | 5.798542 Â± 0.032366 | 6.234284 Â± 0.037878 |
```

----------------------------------------

TITLE: LoRA Adapters List Response Format
DESCRIPTION: JSON response structure showing the list of loaded LoRA adapters with their IDs, paths and scale values.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_15

LANGUAGE: json
CODE:
```
[
    {
        "id": 0,
        "path": "my_adapter_1.gguf",
        "scale": 0.0
    },
    {
        "id": 1,
        "path": "my_adapter_2.gguf",
        "scale": 0.0
    }
]
```

----------------------------------------

TITLE: Running llama-cli with GPU Offloading in CUDA
DESCRIPTION: Example command for running llama-cli with maximum GPU layer offloading using the -ngl flag, which attempts to move as many layers as possible to the GPU.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/token_generation_performance_tips.md#2025-04-22_snippet_0

LANGUAGE: shell
CODE:
```
./llama-cli -m "path/to/model.gguf" -ngl 200000 -p "Please sir, may I have some "
```

----------------------------------------

TITLE: Applying Multiple LORA Adapters with Custom Scaling
DESCRIPTION: Shows how to apply multiple LORA adapters to a base model using the llama-export-lora tool. It demonstrates the use of the --lora-scaled option to specify custom scaling for each adapter.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/export-lora/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
./bin/llama-export-lora \
    -m your_base_model.gguf \
    -o your_merged_model.gguf \
    --lora-scaled lora_task_A.gguf 0.5 \
    --lora-scaled lora_task_B.gguf 0.5
```

----------------------------------------

TITLE: Building and Running Tests in One Command
DESCRIPTION: A combined command that builds the server and then immediately runs the tests, which is useful for local development.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/tests/README.md#2025-04-22_snippet_7

LANGUAGE: shell
CODE:
```
cmake --build build -j --target llama-server && ./examples/server/tests/tests.sh
```

----------------------------------------

TITLE: Splitting LLaVA Model with llava_surgery.py
DESCRIPTION: Python command for splitting the LLaVA model into LLaMA and multimodal projector components using the llava_surgery.py script.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_2

LANGUAGE: sh
CODE:
```
python ./examples/llava/llava_surgery.py -m path/to/MobileVLM-1.7B
```

----------------------------------------

TITLE: Chatml Template Example
DESCRIPTION: The ChatML format template showing how to structure messages with explicit role markers. This is used by models like GPT-4 that require specific formatting for chats.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama.android/README.md#2025-04-22_snippet_1

LANGUAGE: plaintext
CODE:
```
<|im_start|>system
{{ system_prompt }}<|im_end|>
{% for message in messages %}
<|im_start|>{{ message['role'] }}
{{ message['content'] }}<|im_end|>
{% endfor %}
<|im_start|>assistant
```

----------------------------------------

TITLE: Validating LLaVA Tensors
DESCRIPTION: Python script to verify LLaVA tensor files are not empty.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/granitevision.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import os
import torch

MODEL_PATH = os.getenv("GRANITE_MODEL")
if not MODEL_PATH:
    raise ValueError("env var GRANITE_MODEL is unset!")

encoder_tensors = torch.load(os.path.join(MODEL_PATH, "llava.clip"))
projector_tensors = torch.load(os.path.join(MODEL_PATH, "llava.projector"))

assert len(encoder_tensors) > 0
assert len(projector_tensors) > 0
```

----------------------------------------

TITLE: Running All-in-One Model Setup with Docker in llama.cpp
DESCRIPTION: Command to download, convert to ggml, and optimize a 7B model using the full Docker image. It mounts a local models directory to the container and runs the all-in-one process.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
docker run -v /path/to/models:/models ghcr.io/ggml-org/llama.cpp:full --all-in-one "/models/" 7B
```

----------------------------------------

TITLE: Running llama.cpp on Single SYCL Device
DESCRIPTION: Command for running llama.cpp inference on a single SYCL device (GPU) with device ID 0. This uses 'none' split mode to ensure execution on a single specified device.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_26

LANGUAGE: batch
CODE:
```
build\bin\llama-cli.exe -no-cnv -m models\llama-2-7b.Q4_0.gguf -p "Building a website can be done in 10 simple steps:\nStep 1:" -n 400 -e -ngl 33 -s 0 -sm none -mg 0
```

----------------------------------------

TITLE: Configuring General Build Options for GGML
DESCRIPTION: Sets up general build options including static linking, native optimization, link-time optimization, and ccache support.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_7

LANGUAGE: CMake
CODE:
```
# general
option(GGML_STATIC "ggml: static link libraries"                     OFF)
option(GGML_NATIVE "ggml: optimize the build for the current system" ${GGML_NATIVE_DEFAULT})
option(GGML_LTO    "ggml: enable link time optimization"             OFF)
option(GGML_CCACHE "ggml: use ccache if available"                   ON)
```

----------------------------------------

TITLE: Cloning LLaVA and CLIP Models
DESCRIPTION: Git commands to clone the LLaVA model and the CLIP model, which are required for the LLaVA implementation.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_1

LANGUAGE: sh
CODE:
```
git clone https://huggingface.co/liuhaotian/llava-v1.5-7b

git clone https://huggingface.co/openai/clip-vit-large-patch14-336
```

----------------------------------------

TITLE: Single-Turn Query with System Prompt in Windows
DESCRIPTION: Command to execute a single interaction with custom system prompt using Jinja template in Windows
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_8

LANGUAGE: powershell
CODE:
```
./llama-cli.exe -m models\gemma-1.1-7b-it.Q4_K_M.gguf --jinja --single-turn -sys "You are a helpful assistant" -p "Hello"
```

----------------------------------------

TITLE: Running Model Inference with Callback in llama.cpp
DESCRIPTION: Command to run the llama-eval-callback utility with a PHI-2 model. It specifies the model repository, file, output model name, prompt text, random seed, and number of GPU layers to offload.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/eval-callback/README.md#2025-04-22_snippet_0

LANGUAGE: shell
CODE:
```
llama-eval-callback \
  --hf-repo ggml-org/models \
  --hf-file phi-2/ggml-model-q4_0.gguf \
  --model phi-2-q4_0.gguf \
  --prompt hello \
  --seed 42 \
  -ngl 33
```

----------------------------------------

TITLE: LLaMA 3 BF16 vs FP16 Comparison Metrics
DESCRIPTION: Markdown table showing detailed comparison metrics between BF16 and FP16 precision for LLaMA 3 8B model, including perplexity ratios, KL divergence, and probability distribution differences.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/perplexity/README.md#2025-04-22_snippet_2

LANGUAGE: markdown
CODE:
```
| Metric                         |                    Value |
|--------------------------------|---------------------------|
| Mean PPL(Q)                    |      6.227711 Â± 0.037833 |
| Mean PPL(base)                 |      6.225194 Â± 0.037771 |
```

----------------------------------------

TITLE: Tokenization Response Format
DESCRIPTION: JSON response format for the /tokenize endpoint showing both simple token ID array and detailed token information with pieces.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_10

LANGUAGE: json
CODE:
```
{
  "tokens": [123, 456, 789]
}

{
  "tokens": [
    {"id": 123, "piece": "Hello"},
    {"id": 456, "piece": " world"},
    {"id": 789, "piece": "!"}
  ]
}
```

----------------------------------------

TITLE: Running GRIT Model Example with llama-gritlm
DESCRIPTION: Command to execute the llama-gritlm example using the previously downloaded model. The output demonstrates both embedding similarity comparisons between different text pairs and a creative text generation sample.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gritlm/README.md#2025-04-22_snippet_1

LANGUAGE: console
CODE:
```
$ ./llama-gritlm -m models/gritlm-7b_q4_1.gguf

Cosine similarity between "Bitcoin: A Peer-to-Peer Electronic Cash System" and "A purely peer-to-peer version of electronic cash w" is: 0.605
Cosine similarity between "Bitcoin: A Peer-to-Peer Electronic Cash System" and "All text-based language problems can be reduced to" is: 0.103
Cosine similarity between "Generative Representational Instruction Tuning" and "A purely peer-to-peer version of electronic cash w" is: 0.112
Cosine similarity between "Generative Representational Instruction Tuning" and "All text-based language problems can be reduced to" is: 0.547

Oh, brave adventurer, who dared to climb
The lofty peak of Mt. Fuji in the night,
When shadows lurk and ghosts do roam,
And darkness reigns, a fearsome sight.

Thou didst set out, with heart aglow,
To conquer this mountain, so high,
And reach the summit, where the stars do glow,
And the moon shines bright, up in the sky.

Through the mist and fog, thou didst press on,
With steadfast courage, and a steadfast will,
Through the darkness, thou didst not be gone,
But didst climb on, with a steadfast skill.

At last, thou didst reach the summit's crest,
And gazed upon the world below,
And saw the beauty of the night's best,
And felt the peace, that only nature knows.

Oh, brave adventurer, who dared to climb
The lofty peak of Mt. Fuji in the night,
Thou art a hero, in the eyes of all,
For thou didst conquer this mountain, so bright.
```

----------------------------------------

TITLE: Basic Text Embedding Generation - Windows
DESCRIPTION: Basic command to generate embeddings for a simple text input 'Hello World!' using mean pooling on Windows systems. The output is space-separated float values with logging disabled.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/embedding/README.md#2025-04-22_snippet_1

LANGUAGE: powershell
CODE:
```
llama-embedding.exe -m ./path/to/model --pooling mean --log-disable -p "Hello World!" 2>$null
```

----------------------------------------

TITLE: Web UI Development Commands
DESCRIPTION: NPM commands for setting up and building the web UI, including development server launch and production build generation.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
# make sure you have nodejs installed
cd examples/server/webui
npm i

# to run the dev server
npm run dev

# to build the public/index.html.gz
npm run build
```

----------------------------------------

TITLE: JSONL Output Format Example for Benchmark Results
DESCRIPTION: Example of the JSONL output format when using --output-format jsonl flag, showing benchmark metrics including KV cache size, batch parameters, timing, and speed measurements.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/batched-bench/README.md#2025-04-22_snippet_1

LANGUAGE: json
CODE:
```
{"n_kv_max": 2048, "n_batch": 2048, "n_ubatch": 512, "flash_attn": 0, "is_pp_shared": 0, "n_gpu_layers": 99, "n_threads": 8, "n_threads_batch": 8, "pp": 128, "tg": 128, "pl": 1, "n_kv": 256, "t_pp": 0.233810, "speed_pp": 547.453064, "t_tg": 3.503684, "speed_tg": 36.532974, "t": 3.737494, "speed": 68.495094}
{"n_kv_max": 2048, "n_batch": 2048, "n_ubatch": 512, "flash_attn": 0, "is_pp_shared": 0, "n_gpu_layers": 99, "n_threads": 8, "n_threads_batch": 8, "pp": 128, "tg": 128, "pl": 2, "n_kv": 512, "t_pp": 0.422602, "speed_pp": 605.770935, "t_tg": 11.106112, "speed_tg": 23.050371, "t": 11.528713, "speed": 44.410854}
```

----------------------------------------

TITLE: Converting HuggingFace decoder model to GGUF format
DESCRIPTION: Python command to convert the WavTokenizer model from HuggingFace format to GGUF format with 16-bit floating point precision for use with llama.cpp.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_6

LANGUAGE: shell
CODE:
```
(venv) python convert_hf_to_gguf.py models/WavTokenizer-large-speech-75token \
    --outfile models/wavtokenizer-large-75-f16.gguf --outtype f16
```

----------------------------------------

TITLE: Installing GGUF Package
DESCRIPTION: Basic installation command for the GGUF package using pip
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/gguf-py/README.md#2025-04-22_snippet_0

LANGUAGE: sh
CODE:
```
pip install gguf
```

----------------------------------------

TITLE: Running llama.cpp with SYCL on Docker Container
DESCRIPTION: Docker commands to find available GPU devices and run the llama.cpp container with GPU passthrough for inference on Intel GPUs.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_1

LANGUAGE: sh
CODE:
```
# First, find all the DRI cards
ls -la /dev/dri
# Then, pick the card that you want to use (here for e.g. /dev/dri/card1).
docker run -it --rm -v "$(pwd):/app:Z" --device /dev/dri/renderD128:/dev/dri/renderD128 --device /dev/dri/card1:/dev/dri/card1 llama-cpp-sycl -m "/app/models/YOUR_MODEL_FILE" -p "Building a website can be done in 10 simple steps:" -n 400 -e -ngl 33
```

----------------------------------------

TITLE: Configuring CMake Build for llama.cpp Simple Example
DESCRIPTION: Sets up a CMake build configuration for a simple llama.cpp example. Requires CMake 3.12+, finds and links the Llama package, configures C++17 support, and sets up installation rules.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/simple-cmake-pkg/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
cmake_minimum_required(VERSION 3.12)
project(llama-simple-cmake-pkg)

set(TARGET llama-simple-cmake-pkg)

find_package(Llama REQUIRED)

add_executable(${TARGET} ${CMAKE_CURRENT_LIST_DIR}/../simple/simple.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE llama ggml::all ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Converting Visual Components to GGUF
DESCRIPTION: Command to convert visual encoder components to GGUF format with SigLIP parameters.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/granitevision.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
$ python convert_image_encoder_to_gguf.py \
    -m $ENCODER_PATH \
    --llava-projector $ENCODER_PATH/llava.projector \
    --output-dir $ENCODER_PATH \
    --clip-model-is-vision \
    --clip-model-is-siglip \
    --image-mean 0.5 0.5 0.5 \
    --image-std 0.5 0.5 0.5
```

----------------------------------------

TITLE: Setting CUDA Architecture Targets in CMake Configuration
DESCRIPTION: Determines which CUDA GPU architectures to target based on project configuration. Handles native builds, configurations requiring FP16 support, and default settings for maximum compatibility.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
if (NOT DEFINED CMAKE_CUDA_ARCHITECTURES)
    # native == GPUs available at build time
    # 50     == Maxwell, lowest CUDA 12 standard
    # 60     == P100, FP16 CUDA intrinsics
    # 61     == Pascal, __dp4a instruction (per-byte integer dot product)
    # 70     == V100, FP16 tensor cores
    # 75     == Turing, int8 tensor cores
    if (GGML_NATIVE AND CUDAToolkit_VERSION VERSION_GREATER_EQUAL "11.6" AND CMAKE_VERSION VERSION_GREATER_EQUAL "3.24")
        set(CMAKE_CUDA_ARCHITECTURES "native")
    elseif(GGML_CUDA_F16 OR GGML_CUDA_DMMV_F16)
        set(CMAKE_CUDA_ARCHITECTURES "60;61;70;75;80")
    else()
        set(CMAKE_CUDA_ARCHITECTURES "50;61;70;75;80")
    endif()
endif()
```

----------------------------------------

TITLE: Running llama-bench with Markdown output format
DESCRIPTION: Command to run the llama-bench tool with Markdown table output format. This displays benchmark results in a human-readable Markdown table showing model details and performance metrics.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_5

LANGUAGE: sh
CODE:
```
$ ./llama-bench -o md
```

----------------------------------------

TITLE: Building llama.cpp with Vulkan on MSYS2
DESCRIPTION: Commands to build llama.cpp with Vulkan support using CMake in MSYS2 environment.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_17

LANGUAGE: sh
CODE:
```
cmake -B build -DGGML_VULKAN=ON
cmake --build build --config Release
```

----------------------------------------

TITLE: UTF-8 Token Example Response
DESCRIPTION: Example tokenization response showing how UTF-8 characters are handled, using the character 'Ã¡' as an example.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_11

LANGUAGE: json
CODE:
```
{
  "tokens": [
    {"id": 198, "piece": [195]},
    {"id": 164, "piece": [161]}
  ]
}
```

----------------------------------------

TITLE: Advanced Text Embedding Generation - Unix
DESCRIPTION: Advanced command demonstrating multiple text inputs with custom separator, euclidean normalization, and specific output format for Unix-based systems. Uses GPU acceleration and disables logging.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/embedding/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
./llama-embedding -p 'Castle<#sep#>Stronghold<#sep#>Dog<#sep#>Cat' --pooling mean --embd-separator '<#sep#>' --embd-normalize 2  --embd-output-format '' -m './path/to/model.gguf' --n-gpu-layers 99 --log-disable 2>/dev/null
```

----------------------------------------

TITLE: Running Batched Text Generation with llama.cpp
DESCRIPTION: This command demonstrates how to run the llama-batched executable to generate multiple text sequences from a single prompt. The example uses a 7B parameter model and generates 4 parallel sequences from the prompt "Hello my name is". The output shows the generated texts and performance metrics.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/batched/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
./llama-batched -m ./models/llama-7b-v2/ggml-model-f16.gguf -p "Hello my name is" -np 4

...

main: n_len = 32, n_ctx = 2048, n_parallel = 4, n_kv_req = 113

 Hello my name is

main: generating 4 sequences ...

main: stream 0 finished
main: stream 1 finished
main: stream 2 finished
main: stream 3 finished

sequence 0:

Hello my name is Shirley. I am a 25-year-old female who has been working for over 5 years as a b

sequence 1:

Hello my name is Renee and I'm a 32 year old female from the United States. I'm looking for a man between

sequence 2:

Hello my name is Diana. I am looking for a housekeeping job. I have experience with children and have my own transportation. I am

sequence 3:

Hello my name is Cody. I am a 3 year old neutered male. I am a very friendly cat. I am very playful and

main: decoded 108 tokens in 3.57 s, speed: 30.26 t/s

llama_print_timings:        load time =   587.00 ms
llama_print_timings:      sample time =     2.56 ms /   112 runs   (    0.02 ms per token, 43664.72 tokens per second)
llama_print_timings: prompt eval time =  4089.11 ms /   118 tokens (   34.65 ms per token,    28.86 tokens per second)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =  4156.04 ms
```

----------------------------------------

TITLE: Installing GGUF in Development Mode
DESCRIPTION: Commands for installing GGUF package in editable mode for development purposes, including changing to the package directory
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/gguf-py/README.md#2025-04-22_snippet_2

LANGUAGE: sh
CODE:
```
cd /path/to/llama.cpp/gguf-py

pip install --editable .
```

----------------------------------------

TITLE: Building llama.cpp with Intel oneMKL
DESCRIPTION: Commands to build llama.cpp with Intel oneMKL acceleration using CMake. This is specific for Intel processors and requires the oneAPI environment.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
source /opt/intel/oneapi/setvars.sh
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=Intel10_64lp -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_NATIVE=ON
cmake --build build --config Release
```

----------------------------------------

TITLE: Running llama-mtmd-cli with GLMV-EDGE model
DESCRIPTION: Command to run the llama-mtmd-cli binary with a GLMV-EDGE model and its multimodal projector. It demonstrates how to specify the model path and the mmproj path.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/glmedge.md#2025-04-22_snippet_0

LANGUAGE: sh
CODE:
```
./llama-mtmd-cli -m model_path/ggml-model-f16.gguf --mmproj model_path/mmproj-model-f16.gguf
```

----------------------------------------

TITLE: Applying Custom Theme to LLaMA.cpp Server
DESCRIPTION: Command to run the LLaMA.cpp server with a custom theme that places chat buttons at the top of the page. This modification improves usability by making the Stop button more accessible.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/themes/buttons-top/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
--path=themes/buttons_top
```

----------------------------------------

TITLE: Complete Usage Documentation for llama-run Command-line Tool
DESCRIPTION: Comprehensive documentation of the llama-run command, including its usage syntax, available options, commands, and examples. It details parameters like context size, GPU layers, temperature settings, and various model source formats including local files, Ollama models, and Hugging Face repositories.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/run/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
Description:
  Runs a llm

Usage:
  llama-run [options] model [prompt]

Options:
  -c, --context-size <value>
      Context size (default: 2048)
  -n, -ngl, --ngl <value>
      Number of GPU layers (default: 0)
  --temp <value>
      Temperature (default: 0.8)
  -v, --verbose, --log-verbose
      Set verbosity level to infinity (i.e. log all messages, useful for debugging)
  -h, --help
      Show help message

Commands:
  model
      Model is a string with an optional prefix of
      huggingface:// (hf://), ollama://, https:// or file://.
      If no protocol is specified and a file exists in the specified
      path, file:// is assumed, otherwise if a file does not exist in
      the specified path, ollama:// is assumed. Models that are being
      pulled are downloaded with .partial extension while being
      downloaded and then renamed as the file without the .partial
      extension when complete.

Examples:
  llama-run llama3
  llama-run ollama://granite-code
  llama-run ollama://smollm:135m
  llama-run hf://QuantFactory/SmolLM-135M-GGUF/SmolLM-135M.Q2_K.gguf
  llama-run huggingface://bartowski/SmolLM-1.7B-Instruct-v0.2-GGUF/SmolLM-1.7B-Instruct-v0.2-IQ3_M.gguf
  llama-run https://example.com/some-file1.gguf
  llama-run some-file2.gguf
  llama-run file://some-file3.gguf
  llama-run --ngl 999 some-file4.gguf
  llama-run --ngl 999 some-file5.gguf Hello World
```

----------------------------------------

TITLE: Installing llama.cpp with Flox
DESCRIPTION: Command to install llama.cpp within a Flox environment on Mac and Linux systems. Flox follows the nixpkgs build of llama.cpp.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/install.md#2025-04-22_snippet_4

LANGUAGE: sh
CODE:
```
flox install llama-cpp
```

----------------------------------------

TITLE: Setting CUDA Compiler Flags and Options
DESCRIPTION: Configures CUDA compiler flags including architecture-specific optimizations, warning settings, and compiler version detection for cross-platform compatibility.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/CMakeLists.txt#2025-04-22_snippet_4

LANGUAGE: CMake
CODE:
```
set(CUDA_CXX_FLAGS "")

set(CUDA_FLAGS -use_fast_math)

if (CUDAToolkit_VERSION VERSION_GREATER_EQUAL "12.8")
    # Options are:
    # - none (not recommended)
    # - speed (nvcc's default)
    # - balance
    # - size
    list(APPEND CUDA_FLAGS -compress-mode=${GGML_CUDA_COMPRESSION_MODE})
endif()

if (GGML_FATAL_WARNINGS)
    list(APPEND CUDA_FLAGS -Werror all-warnings)
endif()

if (GGML_ALL_WARNINGS AND NOT MSVC)
    set(NVCC_CMD ${CMAKE_CUDA_COMPILER} .c)
    if (NOT CMAKE_CUDA_HOST_COMPILER STREQUAL "")
        list(APPEND NVCC_CMD -ccbin ${CMAKE_CUDA_HOST_COMPILER})
    endif()

    execute_process(
        COMMAND ${NVCC_CMD} -Xcompiler --version
        OUTPUT_VARIABLE CUDA_CCFULLVER
        ERROR_QUIET
    )

    if (NOT CUDA_CCFULLVER MATCHES clang)
        set(CUDA_CCID "GNU")
        execute_process(
            COMMAND ${NVCC_CMD} -Xcompiler "-dumpfullversion -dumpversion"
            OUTPUT_VARIABLE CUDA_CCVER
            ERROR_QUIET
        )
    else()
        if (CUDA_CCFULLVER MATCHES Apple)
            set(CUDA_CCID "AppleClang")
        else()
            set(CUDA_CCID "Clang")
        endif()
        string(REGEX REPLACE "^.* version ([0-9.]*).*$" "\\1" CUDA_CCVER ${CUDA_CCFULLVER})
    endif()

    message("-- CUDA host compiler is ${CUDA_CCID} ${CUDA_CCVER}")

    ggml_get_flags(${CUDA_CCID} ${CUDA_CCVER})
    list(APPEND CUDA_CXX_FLAGS ${CXX_FLAGS} ${GF_CXX_FLAGS})  # This is passed to -Xcompiler later
endif()

if (NOT MSVC)
    list(APPEND CUDA_CXX_FLAGS -Wno-pedantic)
endif()

list(JOIN   CUDA_CXX_FLAGS " " CUDA_CXX_FLAGS_JOINED)  # pass host compiler flags as a single argument

if (NOT CUDA_CXX_FLAGS_JOINED STREQUAL "")
    list(APPEND CUDA_FLAGS -Xcompiler ${CUDA_CXX_FLAGS_JOINED})
endif()

target_compile_options(ggml-cuda PRIVATE "$<$<COMPILE_LANGUAGE:CUDA>:${CUDA_FLAGS}>")
```

----------------------------------------

TITLE: Running Inference on Single Device with CANN Backend
DESCRIPTION: Command to run inference using a single device (GPU) with the CANN backend. It specifies device 0, disables layer splitting, and generates 400 tokens in response to the prompt about building a website.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CANN.md#2025-04-22_snippet_7

LANGUAGE: sh
CODE:
```
./build/bin/llama-cli -m path_to_model -p "Building a website can be done in 10 simple steps:" -n 400 -e -ngl 33 -sm none -mg 0
```

----------------------------------------

TITLE: Building llama.cpp with SYCL for AMD GPU
DESCRIPTION: CMake commands to build llama.cpp with SYCL support for AMD GPUs, using FP32.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_8

LANGUAGE: sh
CODE:
```
GGML_SYCL_DEVICE_ARCH=gfx90a # Example architecture
cmake -B build -DGGML_SYCL=ON -DGGML_SYCL_TARGET=AMD -DGGML_SYCL_DEVICE_ARCH=${GGML_SYCL_DEVICE_ARCH} -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx

# build all binary
cmake --build build --config Release -j -v
```

----------------------------------------

TITLE: JSON Response Structure Example - Token Probabilities
DESCRIPTION: Example structure showing the completion_probabilities array format in the API response, containing token IDs and probabilities.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_8

LANGUAGE: json
CODE:
```
{
    "content": "<the generated completion text>",
    "tokens": [ generated token ids if requested ],
    ...
    "probs": [
      {
        "id": <token id>
      }
    ]
}
```

----------------------------------------

TITLE: Running llama-cli with SYCL on Linux (Multiple Devices)
DESCRIPTION: Shell command to execute llama-cli inference using multiple SYCL devices on Linux with specific parameters.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_14

LANGUAGE: sh
CODE:
```
ZES_ENABLE_SYSMAN=1 ./build/bin/llama-cli -no-cnv -m models/llama-2-7b.Q4_0.gguf -p "Building a website can be done in 10 simple steps:" -n 400 -e -ngl 33 -sm layer
```

----------------------------------------

TITLE: Basic CPU-based Control Vector Generation Command
DESCRIPTION: A simple command to generate a control vector using CPU-only processing with a quantized GGUF model.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/cvector-generator/README.md#2025-04-22_snippet_0

LANGUAGE: sh
CODE:
```
./cvector-generator -m ./llama-3.Q4_K_M.gguf
```

----------------------------------------

TITLE: MUSA Static Build Configuration
DESCRIPTION: CMake commands for creating a static build of llama.cpp with MUSA support, including position-independent code.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_11

LANGUAGE: bash
CODE:
```
cmake -B build -DGGML_MUSA=ON \
  -DBUILD_SHARED_LIBS=OFF -DCMAKE_POSITION_INDEPENDENT_CODE=ON
cmake --build build --config Release
```

----------------------------------------

TITLE: Installing llama.cpp with MacPorts
DESCRIPTION: Command to install llama.cpp using MacPorts package manager. Requires sudo privileges.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/install.md#2025-04-22_snippet_1

LANGUAGE: sh
CODE:
```
sudo port install llama.cpp
```

----------------------------------------

TITLE: Specifying Python Dependencies for LLaMA Model Conversion
DESCRIPTION: This snippet defines the Python package requirements for converting Hugging Face models to GGUF format. It references an external requirements file and specifies an additional index URL for PyTorch CPU wheels.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-convert_lora_to_gguf.txt#2025-04-22_snippet_0

LANGUAGE: plaintext
CODE:
```
-r ./requirements-convert_hf_to_gguf.txt
--extra-index-url https://download.pytorch.org/whl/cpu
```

----------------------------------------

TITLE: HIP GPU Compilation for Windows
DESCRIPTION: CMake commands for building with HIP support on Windows, targeting specific AMD GPU architectures.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_14

LANGUAGE: bash
CODE:
```
set PATH=%HIP_PATH%\bin;%PATH%
cmake -S . -B build -G Ninja -DAMDGPU_TARGETS=gfx1100 -DGGML_HIP=ON -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ -DCMAKE_BUILD_TYPE=Release
cmake --build build
```

----------------------------------------

TITLE: Running llama-bench with CSV output format
DESCRIPTION: Command to run the llama-bench tool with CSV output format. This produces benchmark results as comma-separated values, suitable for importing into spreadsheet applications.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_6

LANGUAGE: sh
CODE:
```
$ ./llama-bench -o csv
```

----------------------------------------

TITLE: Cloning and Building llama.cpp with CMake
DESCRIPTION: Commands to clone the llama.cpp repository and build it using CMake. This is the prerequisite step before working with the MiniCPM-Llama3-V 2.5 model.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/minicpmv2.5.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

LANGUAGE: bash
CODE:
```
cmake -B build
cmake --build build --config Release
```

----------------------------------------

TITLE: Exporting LLM Component
DESCRIPTION: Python script to export the language model component for GGUF conversion.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/granitevision.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
import os
import transformers

MODEL_PATH = os.getenv("GRANITE_MODEL")
if not MODEL_PATH:
    raise ValueError("env var GRANITE_MODEL is unset!")

LLM_EXPORT_PATH = os.getenv("LLM_EXPORT_PATH")
if not LLM_EXPORT_PATH:
    raise ValueError("env var LLM_EXPORT_PATH is unset!")

tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_PATH)

model = transformers.AutoModelForImageTextToText.from_pretrained(MODEL_PATH, ignore_mismatched_sizes=True)

tokenizer.save_pretrained(LLM_EXPORT_PATH)
model.language_model.save_pretrained(LLM_EXPORT_PATH)
```

----------------------------------------

TITLE: Running a Quantized LLaMA Model with llama.cpp
DESCRIPTION: This code shows how to run inference on a quantized GGUF model using the llama-cli tool. It demonstrates starting a conversation with the model using a prompt.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/quantize/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
# start inference on a gguf model
./llama-cli -m ./models/mymodel/ggml-model-Q4_K_M.gguf -cnv -p "You are a helpful assistant"
```

----------------------------------------

TITLE: Running RPC Server with Specific CUDA Device
DESCRIPTION: Command to start the RPC server with a specific CUDA device using the CUDA_VISIBLE_DEVICES environment variable.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/rpc/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
$ CUDA_VISIBLE_DEVICES=0 bin/rpc-server -p 50052
```

----------------------------------------

TITLE: Visual Encoder Configuration
DESCRIPTION: JSON configuration for SiglipVisionModel with grid pinpoints and model parameters.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/granitevision.md#2025-04-22_snippet_4

LANGUAGE: json
CODE:
```
{
    "_name_or_path": "siglip-model",
    "architectures": [
      "SiglipVisionModel"
    ],
    "image_grid_pinpoints": [
        [384,384],
        [384,768],
        [384,1152],
        [384,1536],
        [384,1920],
        [384,2304],
        [384,2688],
        [384,3072],
        [384,3456],
        [384,3840],
        [768,384],
        [768,768],
        [768,1152],
        [768,1536],
        [768,1920],
        [1152,384],
        [1152,768],
        [1152,1152],
        [1536,384],
        [1536,768],
        [1920,384],
        [1920,768],
        [2304,384],
        [2688,384],
        [3072,384],
        [3456,384],
        [3840,384]
    ],
    "mm_patch_merge_type": "spatial_unpad",
    "hidden_size": 1152,
    "image_size": 384,
    "intermediate_size": 4304,
    "model_type": "siglip_vision_model",
    "num_attention_heads": 16,
    "num_hidden_layers": 27,
    "patch_size": 14,
    "layer_norm_eps": 1e-6,
    "hidden_act": "gelu_pytorch_tanh",
    "projection_dim": 0,
    "vision_feature_layer": [-24, -20, -12, -1]
}
```

----------------------------------------

TITLE: JSONL output from llama-bench
DESCRIPTION: Example of JSONL (JSON Lines) output generated by llama-bench. Each line is a valid JSON object representing a single benchmark test result, making it efficient for log processing and incremental loading.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_11

LANGUAGE: json
CODE:
```
{"build_commit":"3469684","build_number":1275,"cuda":true,"metal":false,"gpu_blas":true,"blas":true,"cpu_info":"13th Gen Intel(R) Core(TM) i9-13900K","gpu_info":"NVIDIA GeForce RTX 3090 Ti","model_filename":"models/7B/ggml-model-q4_0.gguf","model_type":"llama 7B mostly Q4_0","model_size":3825065984,"model_n_params":6738415616,"n_batch":512,"n_threads":16,"f16_kv":true,"n_gpu_layers":99,"main_gpu":0,"mul_mat_q":true,"tensor_split":"0.00","n_prompt":512,"n_gen":0,"test_time":"2023-09-23T12:09:57Z","avg_ns":212365953,"stddev_ns":985423,"avg_ts":2410.974041,"stddev_ts":11.163766,"samples_ns":[213837238,211635853,212328053,211329715,212698907],"samples_ts":[2394.34,2419.25,2411.36,2422.75,2407.16]}
{"build_commit":"3469684","build_number":1275,"cuda":true,"metal":false,"gpu_blas":true,"blas":true,"cpu_info":"13th Gen Intel(R) Core(TM) i9-13900K","gpu_info":"NVIDIA GeForce RTX 3090 Ti","model_filename":"models/7B/ggml-model-q4_0.gguf","model_type":"llama 7B mostly Q4_0","model_size":3825065984,"model_n_params":6738415616,"n_batch":512,"n_threads":16,"f16_kv":true,"n_gpu_layers":99,"main_gpu":0,"mul_mat_q":true,"tensor_split":"0.00","n_prompt":0,"n_gen":128,"test_time":"2023-09-23T12:09:59Z","avg_ns":977425219,"stddev_ns":9268593,"avg_ts":130.965708,"stddev_ts":1.238924,"samples_ns":[984472709,974901233,989474741,970729355,967548060],"samples_ts":[130.019,131.295,129.362,131.86,132.293]}
```

----------------------------------------

TITLE: Running Gemma 3 Vision with Custom Models
DESCRIPTION: Commands for running Gemma 3 vision models with custom GGUF files and image input.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/gemma3.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
# build
cmake -B build
cmake --build build --target llama-gemma3-cli

# run it
./build/bin/llama-gemma3-cli -m {text_model}.gguf --mmproj mmproj.gguf --image your_image.jpg
```

----------------------------------------

TITLE: Running MobileVLM on Android (Cat Image Example)
DESCRIPTION: Example command for running the MobileVLM model on Android to describe a cat image. Shows how to use the model for general image description tasks.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_9

LANGUAGE: sh
CODE:
```
/data/local/tmp/llama-mtmd-cli \
    -m /data/local/tmp/ggml-model-q4_k.gguf \
    --mmproj /data/local/tmp/mmproj-model-f16.gguf \
    -t 4 \
    --image /data/local/tmp/cat.jpeg \
    -p "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\nWhat is in the image? ASSISTANT:"
```

----------------------------------------

TITLE: Installing Dependencies with MSYS2 for Windows
DESCRIPTION: Installs required packages for building llama.cpp with Vulkan support on Windows using MSYS2's package manager.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_16

LANGUAGE: sh
CODE:
```
pacman -S git \
    mingw-w64-ucrt-x86_64-gcc \
    mingw-w64-ucrt-x86_64-cmake \
    mingw-w64-ucrt-x86_64-vulkan-devel \
    mingw-w64-ucrt-x86_64-shaderc
```

----------------------------------------

TITLE: Installing llama.cpp with Nix using non-flake installation
DESCRIPTION: Command to install llama.cpp using the Nix package manager for non-flake enabled installations on Mac and Linux systems.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/install.md#2025-04-22_snippet_3

LANGUAGE: sh
CODE:
```
nix-env --file '<nixpkgs>' --install --attr llama-cpp
```

----------------------------------------

TITLE: Building llama.cpp with CUDA and Specific Compute Capabilities
DESCRIPTION: Command to build llama.cpp with CUDA support for specific NVIDIA GPU compute capabilities using CMake.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_8

LANGUAGE: bash
CODE:
```
cmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES="86;89"
```

----------------------------------------

TITLE: GPU Layer Offloading Test Command
DESCRIPTION: Example command for testing performance with different numbers of layers offloaded to GPU.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_4

LANGUAGE: sh
CODE:
```
./llama-bench -ngl 10,20,30,31,32,33,34,35
```

----------------------------------------

TITLE: Basic Usage of llama-imatrix Command
DESCRIPTION: The basic command structure for using llama-imatrix tool, showing all available parameters. This command processes a model and text file to generate an importance matrix that can be used during quantization.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/imatrix/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
./llama-imatrix \
    -m model.gguf -f some-text.txt [-o imatrix.dat] [--process-output] [--verbosity 1] \
    [--no-ppl] [--chunk 123] [--output-frequency 10] [--save-frequency 0] \
    [--in-file imatrix-prev-0.dat --in-file imatrix-prev-1.dat ...]
```

----------------------------------------

TITLE: Compiling llama.cpp with BLIS Integration
DESCRIPTION: Builds llama.cpp with BLIS BLAS support using CMake configuration.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/BLIS.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
mkdir build
cd build
cmake -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=FLAME ..
make -j
```

----------------------------------------

TITLE: Running the Llama Retrieval Example in Bash
DESCRIPTION: Command to build and run the retrieval example with the BGE model. It processes README.md and License files with specified chunk size and separator, retrieving the top 3 most relevant chunks for queries.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/retrieval/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
make -j && ./llama-retrieval --model ./models/bge-base-en-v1.5-f16.gguf --top-k 3 --context-file README.md --context-file License --chunk-size 100 --chunk-separator .
```

----------------------------------------

TITLE: Configuring Compiler Cache for Build Acceleration
DESCRIPTION: Sets up ccache or sccache for faster compilation if GGML_CCACHE is enabled. This speeds up builds by caching previous compilation results.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_6

LANGUAGE: CMake
CODE:
```
if (GGML_CCACHE AND NOT CMAKE_C_COMPILER_LAUNCHER AND NOT CMAKE_CXX_COMPILER_LAUNCHER)
    find_program(GGML_CCACHE_FOUND ccache)
    find_program(GGML_SCCACHE_FOUND sccache)

    if (GGML_CCACHE_FOUND OR GGML_SCCACHE_FOUND)
        if(GGML_CCACHE_FOUND)
            set(GGML_CCACHE_VARIANT ccache)
        else()
            set(GGML_CCACHE_VARIANT sccache)
        endif()
        # TODO: should not be set globally
        if (GGML_SYCL AND GGML_CCACHE_FOUND AND WIN32)
            set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE "ccache compiler_type=icl")
        else ()
            set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE "${GGML_CCACHE_VARIANT}")
        endif ()
        set(ENV{CCACHE_SLOPPINESS} time_macros)
        message(STATUS "${GGML_CCACHE_VARIANT} found, compilation results will be cached. Disable with GGML_CCACHE=OFF.")
    else()
        message(STATUS "Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF")
    endif ()
endif()
```

----------------------------------------

TITLE: Cloning and Building llama.cpp with CMake
DESCRIPTION: Commands to clone the llama.cpp repository and build it using CMake. This is the initial setup required before working with MiniCPM-o 2.6.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/minicpmo2.6.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
```

LANGUAGE: bash
CODE:
```
cmake -B build
cmake --build build --config Release
```

----------------------------------------

TITLE: Downloading GRIT Model from Hugging Face with Script
DESCRIPTION: Command to download the GritLM-7B model from Hugging Face repository using the provided script. The model is saved to the models directory for later use in the example.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gritlm/README.md#2025-04-22_snippet_0

LANGUAGE: console
CODE:
```
$ scripts/hf.sh --repo cohesionet/GritLM-7B_gguf --file gritlm-7b_q4_1.gguf --outdir models
```

----------------------------------------

TITLE: Running llama-mtmd-cli with LLaVA 1.6 Model
DESCRIPTION: Command to run the llama-mtmd-cli using the LLaVA 1.6 model, specifying the model file and multimodal projector.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_11

LANGUAGE: sh
CODE:
```
./llama-mtmd-cli -m ../llava-v1.6-vicuna-7b/ggml-model-f16.gguf --mmproj vit/mmproj-model-f16.gguf
```

----------------------------------------

TITLE: Infinite Text Generation in Unix
DESCRIPTION: Command to generate continuous text from a starting prompt in Unix-based systems, requiring Ctrl-C to stop
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
./llama-cli -m models/gemma-1.1-7b-it.Q4_K_M.gguf --ignore-eos -n -1
```

----------------------------------------

TITLE: Smart Home JSON Response Structure
DESCRIPTION: Defines the required properties for different types of smart home control requests including commands, queries, answers, and clarification requests. Each response type has specific required fields and valid values.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/prompts/assistant.txt#2025-04-22_snippet_0

LANGUAGE: json
CODE:
```
{
  "action": "command",
  "service": "light.turn_on",
  "entity_id": "group.living_light",
  "target": "living",
  "comment": "I've turned on the lights in the living room to brighten up the space."
}
```

----------------------------------------

TITLE: Installing OpenCL Headers and Libraries for Android
DESCRIPTION: Commands to clone and install OpenCL headers and ICD loader for Android NDK, required for building llama.cpp with OpenCL support for Android devices.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/OPENCL.md#2025-04-22_snippet_2

LANGUAGE: sh
CODE:
```
mkdir -p ~/dev/llm
cd ~/dev/llm

git clone https://github.com/KhronosGroup/OpenCL-Headers && \
cd OpenCL-Headers && \
cp -r CL ~/android-sdk/ndk/26.3.11579264/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include

cd ~/dev/llm

git clone https://github.com/KhronosGroup/OpenCL-ICD-Loader && \
cd OpenCL-ICD-Loader && \
mkdir build_ndk26 && cd build_ndk26 && \
cmake .. -G Ninja -DCMAKE_BUILD_TYPE=Release \
  -DCMAKE_TOOLCHAIN_FILE=$HOME/android-sdk/ndk/26.3.11579264/build/cmake/android.toolchain.cmake \
  -DOPENCL_ICD_LOADER_HEADERS_DIR=$HOME/android-sdk/ndk/26.3.11579264/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include \
  -DANDROID_ABI=arm64-v8a \
  -DANDROID_PLATFORM=24 \
  -DANDROID_STL=c++_shared && \
ninja && \
cp libOpenCL.so ~/android-sdk/ndk/26.3.11579264/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/lib/aarch64-linux-android
```

----------------------------------------

TITLE: Running MobileVLM with Image Query
DESCRIPTION: Example command for using MobileVLM to analyze an image of llamas with custom model paths.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_15

LANGUAGE: sh
CODE:
```
-m /path/to/ggml-model-q4_k.gguf \
    --mmproj /path/to/mmproj-model-f16.gguf \
    --image /path/to/many_llamas.jpeg
    -p "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\nWhat's that? ASSISTANT:" \
```

----------------------------------------

TITLE: Running a Test with debug-test.sh in Bash
DESCRIPTION: This command executes a specific test and returns a PASS or FAIL message. It uses the debug-test.sh script from the scripts folder with a regular expression pattern to identify the test to run.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/debugging-tests.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
./scripts/debug-test.sh test-tokenizer
```

----------------------------------------

TITLE: Basic Usage of llama-run Command with Granite3-moe Model
DESCRIPTION: A minimal example showing how to invoke the llama-run command with a model called 'granite3-moe'.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/run/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
llama-run granite3-moe
```

----------------------------------------

TITLE: Linking CUDA Libraries Based on Static/Dynamic Configuration
DESCRIPTION: Configures the linking of CUDA libraries (cuBLAS, cuBLASLt, CUDA runtime) based on whether static or dynamic linking is preferred, with special handling for Windows platforms.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/CMakeLists.txt#2025-04-22_snippet_3

LANGUAGE: CMake
CODE:
```
if (GGML_STATIC)
    if (WIN32)
        # As of 12.3.1 CUDA Toolkit for Windows does not offer a static cublas library
        target_link_libraries(ggml-cuda PRIVATE CUDA::cudart_static CUDA::cublas CUDA::cublasLt)
    else ()
        target_link_libraries(ggml-cuda PRIVATE  CUDA::cudart_static CUDA::cublas_static CUDA::cublasLt_static)
    endif()
else()
    target_link_libraries(ggml-cuda PRIVATE CUDA::cudart CUDA::cublas CUDA::cublasLt)
endif()
```

----------------------------------------

TITLE: Example Llama2.c Model Conversion Command
DESCRIPTION: Example command demonstrating how to convert a TinyLlamas model to GGML format using the conversion tool with specific model paths.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/convert-llama2c-to-ggml/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
./llama-convert-llama2c-to-ggml --copy-vocab-from-model llama-2-7b-chat.gguf.q2_K.bin --llama2c-model stories42M.bin --llama2c-output-model stories42M.gguf.bin
```

----------------------------------------

TITLE: Starting LLM model server
DESCRIPTION: Command to start a server instance for the LLM model on port 8020, making it available for text processing in server-based TTS.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_9

LANGUAGE: shell
CODE:
```
./build/bin/llama-server -m ./models/outetts-0.2-0.5B-q8_0.gguf --port 8020
```

----------------------------------------

TITLE: Configuring llama-gguf-hash Build with Dependencies in CMake
DESCRIPTION: This CMake configuration sets up the llama-gguf-hash executable target, adds its required hash library dependencies (xxhash, sha1, and sha256), and links them together. It includes special handling for disabling warnings in third-party code for non-MSVC compilers and requires C++17 standard support.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-hash/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(TARGET llama-gguf-hash)
add_executable(${TARGET} gguf-hash.cpp)
install(TARGETS ${TARGET} RUNTIME)

# clibs dependencies
include_directories(deps/)

add_library(xxhash OBJECT deps/xxhash/xxhash.c deps/xxhash/xxhash.h)
target_link_libraries(${TARGET} PRIVATE xxhash)

add_library(sha1 OBJECT deps/sha1/sha1.c deps/sha1/sha1.h)
target_link_libraries(${TARGET} PRIVATE sha1)
if (NOT MSVC)
    # disable warnings in 3rd party code
    target_compile_options(sha1 PRIVATE -w)
endif()

add_library(sha256 OBJECT deps/sha256/sha256.c deps/sha256/sha256.h)
target_link_libraries(${TARGET} PRIVATE sha256)

target_link_libraries(${TARGET} PRIVATE ggml ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Prompt Cache Usage Command
DESCRIPTION: Command line option to specify a file for caching the model state after initial prompt to improve startup performance on subsequent runs.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_28

LANGUAGE: bash
CODE:
```
--prompt-cache FNAME
```

----------------------------------------

TITLE: Specifying Python Package Dependencies for llama.cpp
DESCRIPTION: This snippet lists the required Python packages and their version constraints for the llama.cpp project. It includes libraries for asynchronous HTTP requests, testing, machine learning model management, numerical operations, OpenAI API integration, monitoring, HTTP requests, and file downloading.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/tests/requirements.txt#2025-04-22_snippet_0

LANGUAGE: Text
CODE:
```
aiohttp~=3.9.3
pytest~=8.3.3
huggingface_hub~=0.23.2
numpy~=1.26.4
openai~=1.55.3
prometheus-client~=0.20.0
requests~=2.32.3
wget~=3.2
```

----------------------------------------

TITLE: Using GBNF Grammar with llama-cli
DESCRIPTION: Command line example showing how to use a GBNF grammar file with the llama-cli tool.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
./llama-cli -m <model> --grammar-file grammars/some-grammar.gbnf -p 'Some prompt'
```

----------------------------------------

TITLE: Infinite Text Generation in Windows
DESCRIPTION: Command to generate continuous text from a starting prompt in Windows, requiring Ctrl-C to stop
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_9

LANGUAGE: powershell
CODE:
```
llama-cli.exe -m models\gemma-1.1-7b-it.Q4_K_M.gguf --ignore-eos -n -1
```

----------------------------------------

TITLE: Running Unit Tests
DESCRIPTION: Command to run all unit tests for the GGUF package with verbose output
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/gguf-py/README.md#2025-04-22_snippet_9

LANGUAGE: bash
CODE:
```
python -m unittest discover ./gguf-py -v
```

----------------------------------------

TITLE: Generating Visual Studio Solution for llama.cpp with SYCL (Mixed Compilers)
DESCRIPTION: CMake command to generate a Visual Studio solution for llama.cpp with SYCL support using mixed compilers.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_23

LANGUAGE: powershell
CODE:
```
cmake -B build -G "Visual Studio 17 2022" -A x64 -DGGML_SYCL=ON -DCMAKE_BUILD_TYPE=Release \
      -DSYCL_INCLUDE_DIR="C:\Program Files (x86)\Intel\oneAPI\compiler\latest\include" \
      -DSYCL_LIBRARY_DIR="C:\Program Files (x86)\Intel\oneAPI\compiler\latest\lib"
```

----------------------------------------

TITLE: Running llama.cpp with MUSA-enabled Docker Images
DESCRIPTION: Commands to run the locally built MUSA-enabled Docker images. These examples assume mthreads is set as the default Docker runtime and use n-gpu-layers to enable GPU acceleration.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md#2025-04-22_snippet_7

LANGUAGE: bash
CODE:
```
docker run -v /path/to/models:/models local/llama.cpp:full-musa --run -m /models/7B/ggml-model-q4_0.gguf -p "Building a website can be done in 10 simple steps:" -n 512 --n-gpu-layers 1
docker run -v /path/to/models:/models local/llama.cpp:light-musa -m /models/7B/ggml-model-q4_0.gguf -p "Building a website can be done in 10 simple steps:" -n 512 --n-gpu-layers 1
docker run -v /path/to/models:/models local/llama.cpp:server-musa -m /models/7B/ggml-model-q4_0.gguf --port 8000 --host 0.0.0.0 -n 512 --n-gpu-layers 1
```

----------------------------------------

TITLE: Verifying a GGUF File Using the Faster XXH64 Hash
DESCRIPTION: This command verifies a GGUF file against its manifest using the faster XXH64 hash algorithm. The --xxh64 flag specifies to use XXH64 instead of the default SHA-256.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-hash/README.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
./llama-gguf-hash --check test.gguf.manifest --xxh64 test.gguf
```

----------------------------------------

TITLE: Running Inference on Multiple Devices with CANN Backend
DESCRIPTION: Command to run inference using multiple devices with layer splitting mode in the CANN backend. It automatically distributes the model across available devices to process the same prompt about building a website.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CANN.md#2025-04-22_snippet_8

LANGUAGE: sh
CODE:
```
./build/bin/llama-cli -m path_to_model -p "Building a website can be done in 10 simple steps:" -n 400 -e -ngl 33 -sm layer
```

----------------------------------------

TITLE: Usage Instructions for llama-export-lora Command-line Tool
DESCRIPTION: Displays the command-line options for the llama-export-lora tool. It shows how to specify the base model, LORA adapters, number of threads, and output file.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/export-lora/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
usage: llama-export-lora [options]

options:
  -m,    --model                  model path from which to load base model (default '')
         --lora FNAME             path to LoRA adapter  (can be repeated to use multiple adapters)
         --lora-scaled FNAME S    path to LoRA adapter with user defined scaling S  (can be repeated to use multiple adapters)
  -t,    --threads N              number of threads to use during computation (default: 4)
  -o,    --output FNAME           output file (default: 'ggml-lora-merged-f16.gguf')
```

----------------------------------------

TITLE: Building MUSA-enabled Docker Images for llama.cpp Locally
DESCRIPTION: Commands to build local Docker images with MUSA support for full, light, and server variants. Each command targets a specific build type from the MUSA Dockerfile.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
docker build -t local/llama.cpp:full-musa --target full -f .devops/musa.Dockerfile .
docker build -t local/llama.cpp:light-musa --target light -f .devops/musa.Dockerfile .
docker build -t local/llama.cpp:server-musa --target server -f .devops/musa.Dockerfile .
```

----------------------------------------

TITLE: Configuring BLIS Runtime Environment Variables
DESCRIPTION: Sets environment variables to control BLIS thread affinity and count for OpenMP execution.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/BLIS.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
export GOMP_CPU_AFFINITY="0-19"
export BLIS_NUM_THREADS=14
```

----------------------------------------

TITLE: Starting voice decoder model server
DESCRIPTION: Command to start a server instance for the voice decoder model on port 8021, with embeddings enabled and no pooling, for audio generation in server-based TTS.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_10

LANGUAGE: shell
CODE:
```
./build/bin/llama-server -m ./models/wavtokenizer-large-75-f16.gguf --port 8021 --embeddings --pooling none
```

----------------------------------------

TITLE: Compiling and Running llama-gguf-hash CLI Tool in Bash
DESCRIPTION: This snippet demonstrates how to compile the llama-gguf-hash tool using CMake and make, and then run it with various options. It includes commands for building in debug mode, cleaning the build directory, and executing the tool with different hashing algorithms.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-hash/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
cmake -B build -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON
make -C build clean
make -C build llama-gguf-hash VERBOSE=1
./build/bin/llama-gguf-hash test.gguf
./build/bin/llama-gguf-hash --xxh64 test.gguf
./build/bin/llama-gguf-hash --sha1 test.gguf
./build/bin/llama-gguf-hash --uuid test.gguf
./build/bin/llama-gguf-hash --sha256 test.gguf
```

----------------------------------------

TITLE: Building llama.cpp XCFramework for iOS integration
DESCRIPTION: Command to build the llama.cpp library as an XCFramework that can be included in iOS projects. This must be run from the root directory of the llama.cpp project to generate the necessary framework files for iOS integration.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama.swiftui/README.md#2025-04-22_snippet_0

LANGUAGE: console
CODE:
```
$ ./build-xcframework.sh
```

----------------------------------------

TITLE: Running llama.cpp with Docker and Vulkan
DESCRIPTION: Commands to build and run llama.cpp in a Docker container with Vulkan support, passing through GPU devices.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_18

LANGUAGE: sh
CODE:
```
# Build the image
docker build -t llama-cpp-vulkan --target light -f .devops/vulkan.Dockerfile .

# Then, use it:
docker run -it --rm -v "$(pwd):/app:Z" --device /dev/dri/renderD128:/dev/dri/renderD128 --device /dev/dri/card1:/dev/dri/card1 llama-cpp-vulkan -m "/app/models/YOUR_MODEL_FILE" -p "Building a website can be done in 10 simple steps:" -n 400 -e -ngl 33
```

----------------------------------------

TITLE: Converting HuggingFace model to GGUF format
DESCRIPTION: Python command to convert the OuteTTS HuggingFace model to GGUF format with 16-bit floating point precision for use with llama.cpp.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_2

LANGUAGE: shell
CODE:
```
(venv) python convert_hf_to_gguf.py models/OuteTTS-0.2-500M \
    --outfile models/outetts-0.2-0.5B-f16.gguf --outtype f16
```

----------------------------------------

TITLE: Defining JSON Schema with Pydantic in Python
DESCRIPTION: Creates Pydantic models for QA pairs and summaries with validation rules. Includes pattern matching for key facts and minimum item requirements for question answers.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
# pip install pydantic
import json
from typing import Annotated, List
from pydantic import BaseModel, Extra, Field
class QAPair(BaseModel):
    class Config:
        extra = 'allow'  # triggers additionalProperties: true in the JSON schema
    question: str
    concise_answer: str
    justification: str

class Summary(BaseModel):
    class Config:
        extra = 'allow'
    key_facts: List[Annotated[str, Field(pattern='- .{5,}')]]
    question_answers: List[Annotated[List[QAPair], Field(min_items=5)]]

print(json.dumps(Summary.model_json_schema(), indent=2))
```

----------------------------------------

TITLE: Compiling for Android
DESCRIPTION: Commands for building the project for Android 64-bit architecture by creating a build directory and executing the build script.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_7

LANGUAGE: sh
CODE:
```
mkdir examples/llava/android/build_64
cd examples/llava/android/build_64
../build_64.sh
```

----------------------------------------

TITLE: CSV output from llama-bench
DESCRIPTION: Example of CSV output generated by llama-bench. Each row represents a benchmark test with details including build information, hardware specs, model configuration, and performance metrics.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_7

LANGUAGE: csv
CODE:
```
build_commit,build_number,cuda,metal,gpu_blas,blas,cpu_info,gpu_info,model_filename,model_type,model_size,model_n_params,n_batch,n_threads,f16_kv,n_gpu_layers,main_gpu,mul_mat_q,tensor_split,n_prompt,n_gen,test_time,avg_ns,stddev_ns,avg_ts,stddev_ts
"3469684","1275","1","0","0","1","1","13th Gen Intel(R) Core(TM) i9-13900K","NVIDIA GeForce RTX 3090 Ti","models/7B/ggml-model-q4_0.gguf","llama 7B mostly Q4_0","3825065984","6738415616","512","16","1","99","0","1","0.00","512","0","2023-09-23T12:09:01Z","212155977","732372","2413.341687","8.305961"
"3469684","1275","1","0","0","1","1","13th Gen Intel(R) Core(TM) i9-13900K","NVIDIA GeForce RTX 3090 Ti","models/7B/ggml-model-q4_0.gguf","llama 7B mostly Q4_0","3825065984","6738415616","512","16","1","99","0","1","0.00","0","128","2023-09-23T12:09:02Z","969320879","2728399","132.052051","0.371342"
```

----------------------------------------

TITLE: Compiling for NVIDIA Jetson Orin
DESCRIPTION: Command for compiling the MobileVLM model with CUDA support specifically for the NVIDIA Jetson Orin platform, enabling hardware acceleration on this edge AI device.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_11

LANGUAGE: sh
CODE:
```
make GGML_CUDA=1 CUDA_DOCKER_ARCH=sm_87 GGML_CUDA_F16=1 -j 32
```

----------------------------------------

TITLE: GPU-accelerated Control Vector Generation Command
DESCRIPTION: Command to generate a control vector with GPU acceleration, where -ngl 99 indicates using all available GPU layers.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/cvector-generator/README.md#2025-04-22_snippet_1

LANGUAGE: sh
CODE:
```
./cvector-generator -m ./llama-3.Q4_K_M.gguf -ngl 99
```

----------------------------------------

TITLE: Installing llama.cpp with Nix using flake-enabled installation
DESCRIPTION: Command to install llama.cpp using the Nix package manager with flake-enabled installations on Mac and Linux systems.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/install.md#2025-04-22_snippet_2

LANGUAGE: sh
CODE:
```
nix profile install nixpkgs#llama-cpp
```

----------------------------------------

TITLE: Running MUSA CI Inside Docker Container
DESCRIPTION: Commands to execute inside the Docker container, including installing dependencies, configuring git for the workspace directory, and running the MUSA CI workflow.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ci/README.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
apt update -y && apt install -y bc cmake ccache git python3.10-venv time unzip wget
git config --global --add safe.directory /ws
GG_BUILD_MUSA=1 bash ./ci/run.sh /ci-results /ci-cache
```

----------------------------------------

TITLE: Running the Python TTS client script
DESCRIPTION: Command to run the Python TTS client script that connects to both server instances (LLM and voice decoder) to generate speech from text. The script generates a WAV file as output.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_12

LANGUAGE: shell
CODE:
```
(venv) python ./examples/tts/tts-outetts.py http://localhost:8020 http://localhost:8021 "Hello world"
```

----------------------------------------

TITLE: Running llama-bench with JSONL output format
DESCRIPTION: Command to run the llama-bench tool with JSON Lines (JSONL) output format. This produces benchmark results with each test as a separate line of JSON, suitable for streaming and log processing.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_10

LANGUAGE: sh
CODE:
```
$ ./llama-bench -o jsonl
```

----------------------------------------

TITLE: Compiling and Running Passkey Retrieval Task for llama.cpp
DESCRIPTION: This command compiles the project and runs the passkey retrieval task using a specified model file. It includes a parameter to add 250 junk tokens to the context.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/passkey/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
make -j && ./llama-passkey -m ./models/llama-7b-v2/ggml-model-f16.gguf --junk 250
```

----------------------------------------

TITLE: Building llama.cpp with SYCL on Windows (CMake Presets)
DESCRIPTION: CMake commands to build llama.cpp with SYCL support on Windows using CMake presets.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_21

LANGUAGE: sh
CODE:
```
cmake --preset x64-windows-sycl-release
cmake --build build-x64-windows-sycl-release -j --target llama-cli

cmake -DGGML_SYCL_F16=ON --preset x64-windows-sycl-release
cmake --build build-x64-windows-sycl-release -j --target llama-cli

cmake --preset x64-windows-sycl-debug
cmake --build build-x64-windows-sycl-debug -j --target llama-cli
```

----------------------------------------

TITLE: Building llama.cpp with SYCL Support Using Docker
DESCRIPTION: Docker command to build a container image for llama.cpp with SYCL support optimized for FP16 computation on Intel GPUs.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_0

LANGUAGE: sh
CODE:
```
# Using FP16
docker build -t llama-cpp-sycl --build-arg="GGML_SYCL_F16=ON" --target light -f .devops/intel.Dockerfile .
```

----------------------------------------

TITLE: Running Converted GGML Model Command
DESCRIPTION: Example command showing how to use the converted GGML model with llama-cli, including prompt generation parameters.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/convert-llama2c-to-ggml/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
./llama-cli -m stories42M.gguf.bin -p "One day, Lily met a Shoggoth" -n 500 -c 256
```

----------------------------------------

TITLE: Building llama.cpp with SYCL on Windows (CMake FP32)
DESCRIPTION: CMake commands to build llama.cpp with SYCL support on Windows using FP32.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_19

LANGUAGE: sh
CODE:
```
@call "C:\Program Files (x86)\Intel\oneAPI\setvars.bat" intel64 --force

cmake -B build -G "Ninja" -DGGML_SYCL=ON -DCMAKE_C_COMPILER=cl -DCMAKE_CXX_COMPILER=icx  -DCMAKE_BUILD_TYPE=Release

cmake --build build --config Release -j
```

----------------------------------------

TITLE: SQL output from llama-bench
DESCRIPTION: Example of SQL output generated by llama-bench. The output includes a CREATE TABLE statement defining the schema and INSERT statements for each benchmark test result, ready to be imported into a SQLite database.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_13

LANGUAGE: sql
CODE:
```
CREATE TABLE IF NOT EXISTS test (
  build_commit TEXT,
  build_number INTEGER,
  cuda INTEGER,
  metal INTEGER,
  gpu_blas INTEGER,
  blas INTEGER,
  cpu_info TEXT,
  gpu_info TEXT,
  model_filename TEXT,
  model_type TEXT,
  model_size INTEGER,
  model_n_params INTEGER,
  n_batch INTEGER,
  n_threads INTEGER,
  f16_kv INTEGER,
  n_gpu_layers INTEGER,
  main_gpu INTEGER,
  mul_mat_q INTEGER,
  tensor_split TEXT,
  n_prompt INTEGER,
  n_gen INTEGER,
  test_time TEXT,
  avg_ns INTEGER,
  stddev_ns INTEGER,
  avg_ts REAL,
  stddev_ts REAL
);

INSERT INTO test (build_commit, build_number, cuda, metal, gpu_blas, blas, cpu_info, gpu_info, model_filename, model_type, model_size, model_n_params, n_batch, n_threads, f16_kv, n_gpu_layers, main_gpu, mul_mat_q, tensor_split, n_prompt, n_gen, test_time, avg_ns, stddev_ns, avg_ts, stddev_ts) VALUES ('3469684', '1275', '1', '0', '0', '1', '1', '13th Gen Intel(R) Core(TM) i9-13900K', 'NVIDIA GeForce RTX 3090 Ti', 'models/7B/ggml-model-q4_0.gguf', 'llama 7B mostly Q4_0', '3825065984', '6738415616', '512', '16', '1', '99', '0', '1', '0.00', '512', '0', '2023-09-23T12:10:30Z', '212693772', '743623', '2407.240204', '8.409634');
INSERT INTO test (build_commit, build_number, cuda, metal, gpu_blas, blas, cpu_info, gpu_info, model_filename, model_type, model_size, model_n_params, n_batch, n_threads, f16_kv, n_gpu_layers, main_gpu, mul_mat_q, tensor_split, n_prompt, n_gen, test_time, avg_ns, stddev_ns, avg_ts, stddev_ts) VALUES ('3469684', '1275', '1', '0', '0', '1', '1', '13th Gen Intel(R) Core(TM) i9-13900K', 'NVIDIA GeForce RTX 3090 Ti', 'models/7B/ggml-model-q4_0.gguf', 'llama 7B mostly Q4_0', '3825065984', '6738415616', '512', '16', '1', '99', '0', '1', '0.00', '0', '128', '2023-09-23T12:10:31Z', '977925003', '4037361', '130.891159', '0.537692');
```

----------------------------------------

TITLE: Running MobileVLM on Android (Book Example)
DESCRIPTION: Example command for running the MobileVLM model on Android to identify the author of a book in an image. This demonstrates the model's visual question answering capability.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_8

LANGUAGE: sh
CODE:
```
/data/local/tmp/llama-mtmd-cli \
    -m /data/local/tmp/ggml-model-q4_k.gguf \
    --mmproj /data/local/tmp/mmproj-model-f16.gguf \
    -t 4 \
    --image /data/local/tmp/demo.jpg \
    -p "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\nWho is the author of this book? \nAnswer the question using a single word or phrase. ASSISTANT:"
```

----------------------------------------

TITLE: Downloading Model Files on Android via Termux
DESCRIPTION: Command to download a model file from a URL to the home directory on Android using curl in Termux.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/android.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
$ curl -L {model-url} -o ~/{model}.gguf
```

----------------------------------------

TITLE: Installing Dependencies for Llama.cpp Server Tests
DESCRIPTION: Command to install the required Python dependencies for running the server tests.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/tests/README.md#2025-04-22_snippet_0

LANGUAGE: shell
CODE:
```
pip install -r requirements.txt
```

----------------------------------------

TITLE: Debugging a Test in GDB with debug-test.sh
DESCRIPTION: These commands demonstrate how to run a test in GDB debugger mode. The -g flag enables GDB, and once in the debugger prompt, a breakpoint can be set on the main function.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/debugging-tests.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
./scripts/debug-test.sh -g test-tokenizer

# Once in the debugger, i.e. at the chevrons prompt, setting a breakpoint could be as follows:
>>> b main
```

----------------------------------------

TITLE: Configuring CMake Project Settings for GGML
DESCRIPTION: Sets up the basic project configuration for GGML, including minimum CMake version, project name, and default build settings.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
cmake_minimum_required(VERSION 3.14) # for add_link_options and implicit target directories.
project("ggml" C CXX)
include(CheckIncludeFileCXX)

set(CMAKE_EXPORT_COMPILE_COMMANDS ON)

if (NOT XCODE AND NOT MSVC AND NOT CMAKE_BUILD_TYPE)
    set(CMAKE_BUILD_TYPE Release CACHE STRING "Build type" FORCE)
    set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS "Debug" "Release" "MinSizeRel" "RelWithDebInfo")
endif()
```

----------------------------------------

TITLE: Configuring MUSA-specific Compilation Definitions for llama.cpp
DESCRIPTION: Sets up various compilation definitions for MUSA GPU support, including CUDA compatibility, memory management, and precision options. Links against MUSA libraries based on static or dynamic build configuration.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-musa/CMakeLists.txt#2025-04-22_snippet_2

LANGUAGE: CMake
CODE:
```
    ggml_add_backend_library(ggml-musa
                             ${GGML_HEADERS_MUSA}
                             ${GGML_SOURCES_MUSA}
                            )

    # TODO: do not use CUDA definitions for MUSA
    target_compile_definitions(ggml PUBLIC GGML_USE_CUDA)

    add_compile_definitions(GGML_USE_MUSA)
    add_compile_definitions(GGML_CUDA_PEER_MAX_BATCH_SIZE=${GGML_CUDA_PEER_MAX_BATCH_SIZE})

    if (GGML_CUDA_FORCE_MMQ)
        add_compile_definitions(GGML_CUDA_FORCE_MMQ)
    endif()

    if (GGML_CUDA_FORCE_CUBLAS)
        add_compile_definitions(GGML_CUDA_FORCE_CUBLAS)
    endif()

    if (GGML_CUDA_NO_VMM)
        add_compile_definitions(GGML_CUDA_NO_VMM)
    endif()

    if (NOT GGML_CUDA_FA)
        add_compile_definitions(GGML_CUDA_NO_FA)
    endif()

    if (GGML_CUDA_F16 OR GGML_CUDA_DMMV_F16)
        add_compile_definitions(GGML_CUDA_F16)
    endif()

    if (GGML_CUDA_NO_PEER_COPY)
        add_compile_definitions(GGML_CUDA_NO_PEER_COPY)
    endif()

    if (GGML_STATIC)
        target_link_libraries(ggml-musa PRIVATE MUSA::musart_static MUSA::mublas_static)
    else()
        target_link_libraries(ggml-musa PRIVATE MUSA::musart MUSA::mublas)
    endif()

    if (GGML_CUDA_NO_VMM)
        # No VMM requested, no need to link directly with the musa driver lib (libmusa.so)
    else()
        target_link_libraries(ggml-musa PRIVATE MUSA::musa_driver)
    endif()
else()
    message(FATAL_ERROR "MUSA Toolkit not found")
endif()
```

----------------------------------------

TITLE: Multi-line Prompt Formatting Example
DESCRIPTION: Example showing how to format prompts with multiple lines by escaping newline characters for use with the control vector generator.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/cvector-generator/README.md#2025-04-22_snippet_5

LANGUAGE: sh
CODE:
```
<|im_start|>system\nAct like a person who is extremely happy.<|im_end|>
<|im_start|>system\nYou are in a very good mood today<|im_end|>
```

----------------------------------------

TITLE: Building llama.cpp with SYCL on Windows (CMake FP16)
DESCRIPTION: CMake commands to build llama.cpp with SYCL support on Windows using FP16.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_20

LANGUAGE: sh
CODE:
```
@call "C:\Program Files (x86)\Intel\oneAPI\setvars.bat" intel64 --force

cmake -B build -G "Ninja" -DGGML_SYCL=ON -DCMAKE_C_COMPILER=cl -DCMAKE_CXX_COMPILER=icx  -DCMAKE_BUILD_TYPE=Release -DGGML_SYCL_F16=ON

cmake --build build --config Release -j
```

----------------------------------------

TITLE: Quantizing Models for Adreno GPU with Pure Q4_0
DESCRIPTION: Command for preparing models with Q4_0 quantization optimized for Adreno GPUs using the --pure flag to achieve best performance.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/OPENCL.md#2025-04-22_snippet_0

LANGUAGE: sh
CODE:
```
./llama-quantize --pure ggml-model-qwen2.5-3b-f16.gguf ggml-model-qwen-3b-Q4_0.gguf Q4_0
```

----------------------------------------

TITLE: Chat Template Format
DESCRIPTION: Template showing the structure of a chat conversation between a human user (placeholder [[USER_NAME]]) and an AI assistant (placeholder [[AI_NAME]]). Includes formatting for turn-taking, greetings, and Q&A.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/prompts/chat-with-vicuna-v1.txt#2025-04-22_snippet_0

LANGUAGE: text
CODE:
```
A chat between a curious human ("[[USER_NAME]]") and an artificial intelligence assistant ("[[AI_NAME]]"). The assistant gives helpful, detailed, and polite answers to the human's questions.

[[USER_NAME]]: Hello, [[AI_NAME]].
[[AI_NAME]]: Hello. How may I help you today?
[[USER_NAME]]: Please tell me the largest city in Europe.
[[AI_NAME]]: Sure. The largest city in Europe is Moscow, the capital of Russia.
[[USER_NAME]]:
```

----------------------------------------

TITLE: Setting Up Build Context for Testing
DESCRIPTION: This bash command creates a fresh build directory for debugging tests. It removes any existing build-ci-debug folder, creates a new one, and changes into it to prepare for compilation.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/debugging-tests.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
rm -rf build-ci-debug && mkdir build-ci-debug && cd build-ci-debug
```

----------------------------------------

TITLE: Running GDB on a Specific Test
DESCRIPTION: This command demonstrates how to run GDB with a specific test binary and model file as arguments. The command is constructed based on the output from the previous ctest command.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/debugging-tests.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
gdb --args ${Test Binary} ${Test GGUF Model}
```

----------------------------------------

TITLE: Generating Chat Templates Listing in Bash for llama.cpp
DESCRIPTION: This bash command generates a table of available Jinja templates for different AI models. It runs the test-chat binary from the build directory against all Jinja templates in the minja build tests directory, while suppressing error output.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/function-calling.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
./build/bin/test-chat ../minja/build/tests/*.jinja 2>/dev/null
```

----------------------------------------

TITLE: Configuring llama Library with CMake
DESCRIPTION: Defines the llama library with its source files, includes, linking requirements, and compilation features. When building as a shared library, it sets position-independent code and appropriate compilation definitions.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/src/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
llama_add_compile_flags()

#
# libraries
#

# llama

add_library(llama
            ../include/llama.h
            llama.cpp
            llama-adapter.cpp
            llama-arch.cpp
            llama-batch.cpp
            llama-chat.cpp
            llama-context.cpp
            llama-grammar.cpp
            llama-graph.cpp
            llama-hparams.cpp
            llama-impl.cpp
            llama-io.cpp
            llama-kv-cache.cpp
            llama-memory.cpp
            llama-mmap.cpp
            llama-model-loader.cpp
            llama-model.cpp
            llama-quant.cpp
            llama-sampling.cpp
            llama-vocab.cpp
            unicode-data.cpp
            unicode.cpp
            unicode.h
            )

target_include_directories(llama PUBLIC . ../include)
target_compile_features   (llama PUBLIC cxx_std_17) # don't bump

target_link_libraries(llama PUBLIC ggml)

if (BUILD_SHARED_LIBS)
    set_target_properties(llama PROPERTIES POSITION_INDEPENDENT_CODE ON)
    target_compile_definitions(llama PRIVATE LLAMA_BUILD)
    target_compile_definitions(llama PUBLIC  LLAMA_SHARED)
endif()
```

----------------------------------------

TITLE: Variable Naming Convention Examples in C++
DESCRIPTION: Examples showing proper variable naming using snake_case and organizing names to optimize for longest common prefix. This demonstrates how to structure related variable names in the project.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/CONTRIBUTING.md#2025-04-22_snippet_4

LANGUAGE: cpp
CODE:
```
// not OK
int small_number;
int big_number;

// OK
int number_small;
int number_big;
```

----------------------------------------

TITLE: MUSA Compute Capability Configuration
DESCRIPTION: CMake commands to specify custom compute capability for MUSA GPU compilation, targeting specific architectures like MTT S80.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_10

LANGUAGE: bash
CODE:
```
cmake -B build -DGGML_MUSA=ON -DMUSA_ARCHITECTURES="21"
cmake --build build --config Release
```

----------------------------------------

TITLE: Installing Development Tools
DESCRIPTION: Commands to install essential development packages and tools
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CUDA-FEDORA.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
sudo dnf distro-sync
sudo dnf install vim-default-editor --allowerasing
sudo dnf install @c-development @development-tools cmake
```

----------------------------------------

TITLE: Setting up Build Dependencies and Standards for GGML
DESCRIPTION: This snippet configures required dependencies, C/C++ standards, and build settings for the GGML library. It sets C11 and C++17 as the required standards and includes necessary packages like Threads.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_16

LANGUAGE: CMake
CODE:
```
#
# dependencies
#

set(CMAKE_C_STANDARD 11)
set(CMAKE_C_STANDARD_REQUIRED true)

set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED true)

set(THREADS_PREFER_PTHREAD_FLAG ON)

find_package(Threads REQUIRED)

include(GNUInstallDirs)
```

----------------------------------------

TITLE: LLaMA 2 70B Quantization Performance Table
DESCRIPTION: Markdown table showing model size and perplexity metrics for different quantization methods applied to LLaMA 2 70B model, including delta comparisons to FP16 baseline.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/perplexity/README.md#2025-04-22_snippet_3

LANGUAGE: markdown
CODE:
```
| Quantization | Model size (GiB) | Perplexity | Delta to fp16 |
|--------------|------------------|------------|---------------|
| Q4_0         | 36.20            | 3.5550     | 3.61%         |
| Q4_1         | 40.20            | 3.5125     | 2.37%         |
```

----------------------------------------

TITLE: Configuring CUDA Environment
DESCRIPTION: Commands to set up CUDA environment variables and path configuration
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CUDA-FEDORA.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
sudo sh -c 'echo "export PATH=\$PATH:/usr/local/cuda/bin" >> /etc/profile.d/cuda.sh'
sudo chmod +x /etc/profile.d/cuda.sh
source /etc/profile.d/cuda.sh
```

----------------------------------------

TITLE: Executing llama.cpp with SYCL on Linux (Single Device)
DESCRIPTION: Shell command to run llama.cpp inference using a single SYCL device (device 0) on Linux.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_11

LANGUAGE: sh
CODE:
```
./examples/sycl/run-llama2.sh 0
```

----------------------------------------

TITLE: Running Slow Tests with Llama.cpp Server
DESCRIPTION: Command to run slow tests that will download multiple models. The LLAMA_CACHE environment variable can be set to avoid re-downloading models.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/tests/README.md#2025-04-22_snippet_3

LANGUAGE: shell
CODE:
```
SLOW_TESTS=1 ./tests.sh
```

----------------------------------------

TITLE: Installing llama.cpp with Homebrew on Mac and Linux
DESCRIPTION: Command to install llama.cpp using the Homebrew package manager on Mac and Linux systems. The formula is automatically updated with new llama.cpp releases.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/install.md#2025-04-22_snippet_0

LANGUAGE: sh
CODE:
```
brew install llama.cpp
```

----------------------------------------

TITLE: Setting up llama-server CMake target with source files and dependencies
DESCRIPTION: Defines the llama-server target with its source files, includes required directories, and configures compiler settings. The script handles platform-specific requirements like MinGW Windows version definition and sets up asset file conversion from HTML files to C++ headers.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
set(TARGET llama-server)

option(LLAMA_SERVER_SSL "Build SSL support for the server" OFF)

include_directories(${CMAKE_CURRENT_SOURCE_DIR} ${CMAKE_CURRENT_BINARY_DIR})

if (MINGW)
    # fix: https://github.com/ggml-org/llama.cpp/actions/runs/9651004652/job/26617901362?pr=8006
    add_compile_definitions(_WIN32_WINNT=${GGML_WIN_VER})
endif()

set(TARGET_SRCS
    server.cpp
    utils.hpp
    httplib.h
)
set(PUBLIC_ASSETS
    index.html.gz
    loading.html
)

foreach(asset ${PUBLIC_ASSETS})
    set(input "${CMAKE_CURRENT_SOURCE_DIR}/public/${asset}")
    set(output "${CMAKE_CURRENT_BINARY_DIR}/${asset}.hpp")
    list(APPEND TARGET_SRCS ${output})
    add_custom_command(
        DEPENDS "${input}"
        OUTPUT "${output}"
        COMMAND "${CMAKE_COMMAND}" "-DINPUT=${input}" "-DOUTPUT=${output}" -P "${PROJECT_SOURCE_DIR}/scripts/xxd.cmake"
    )
    set_source_files_properties(${output} PROPERTIES GENERATED TRUE)
endforeach()
```

----------------------------------------

TITLE: Converting Image Encoder for MobileVLM_V2
DESCRIPTION: Command for converting the CLIP image encoder to GGUF format with LDPV2 projector type specifically for the MobileVLM_V2 model variant.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_4

LANGUAGE: sh
CODE:
```
python ./examples/llava/convert_image_encoder_to_gguf.py \
    -m path/to/clip-vit-large-patch14-336 \
    --llava-projector path/to/MobileVLM-1.7B_V2/llava.projector \
    --output-dir path/to/MobileVLM-1.7B_V2 \
    --projector-type ldpv2
```

----------------------------------------

TITLE: Configuring Common Tests for llama.cpp
DESCRIPTION: Defines test targets that are common across platforms, including log testing, chat template testing, and GGUF format testing. Some tests are conditionally included based on platform or backend configuration.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/tests/CMakeLists.txt#2025-04-22_snippet_4

LANGUAGE: CMake
CODE:
```
llama_target_and_test(test-log.cpp)
llama_target_and_test(test-chat-template.cpp)

# this fails on windows (github hosted runner) due to curl DLL not found (exit code 0xc0000135)
if (NOT WIN32)
    llama_target_and_test(test-arg-parser.cpp)
endif()

# llama_target_and_test(test-opt.cpp) # SLOW
llama_target_and_test(test-gguf.cpp)
llama_target_and_test(test-backend-ops.cpp)

llama_target_and_test(test-model-load-cancel.cpp  LABEL "model")
llama_target_and_test(test-autorelease.cpp        LABEL "model")

if (NOT GGML_BACKEND_DL)
    # these tests use the backends directly and cannot be built with dynamic loading
    llama_target_and_test(test-barrier.cpp)
    llama_target_and_test(test-quantize-fns.cpp)
    llama_target_and_test(test-quantize-perf.cpp)
    llama_target_and_test(test-rope.cpp)
endif()
```

----------------------------------------

TITLE: Verifying a GGUF File Using Default Settings (SHA-256)
DESCRIPTION: This command verifies a GGUF file against its manifest using the default settings, which checks the highest security strength hash (SHA-256) for all tensors and the overall file.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-hash/README.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
./llama-gguf-hash --check test.gguf.manifest test.gguf
```

----------------------------------------

TITLE: Converting PyTorch checkpoint to HuggingFace format
DESCRIPTION: Python command to convert the WavTokenizer PyTorch checkpoint (.ckpt) file to HuggingFace format, which is an intermediate step before GGUF conversion.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_5

LANGUAGE: shell
CODE:
```
(venv) python examples/tts/convert_pt_to_hf.py \
    models/WavTokenizer-large-speech-75token/wavtokenizer_large_speech_320_24k.ckpt
```

----------------------------------------

TITLE: Building Docker Image for llama.cpp CANN
DESCRIPTION: Command to build a Docker image containing llama.cpp with CANN support
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CANN.md#2025-04-22_snippet_0

LANGUAGE: shell
CODE:
```
docker build -t llama-cpp-cann -f .devops/llama-cli-cann.Dockerfile .
```

----------------------------------------

TITLE: Verifying a GGUF Manifest Using All Hash Algorithms
DESCRIPTION: This command verifies all hash entries in a GGUF manifest file against the manifest itself using the --all flag. It checks each hash type (XXH64, SHA-1, SHA-256) for all tensors and the overall file.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-hash/README.md#2025-04-22_snippet_7

LANGUAGE: bash
CODE:
```
./llama-gguf-hash --check test.gguf.manifest --all test.gguf.manifest
```

----------------------------------------

TITLE: Configuring llama-cli Executable Build with CMake
DESCRIPTION: Sets up the llama-cli executable target with required library dependencies and C++17 standard. Links against common and llama libraries along with system thread libraries. Configures installation of the runtime binary.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
set(TARGET llama-cli)
add_executable(${TARGET} main.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Embedding Metal Library for GGML in CMake
DESCRIPTION: Sets up the process for embedding the Metal library into the binary when GGML_METAL_EMBED_LIBRARY is enabled.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-metal/CMakeLists.txt#2025-04-22_snippet_4

LANGUAGE: CMake
CODE:
```
if (GGML_METAL_EMBED_LIBRARY)
    enable_language(ASM)

    add_compile_definitions(GGML_METAL_EMBED_LIBRARY)

    set(METALLIB_SOURCE "${CMAKE_CURRENT_SOURCE_DIR}/ggml-metal.metal")
    set(METALLIB_IMPL   "${CMAKE_CURRENT_SOURCE_DIR}/ggml-metal-impl.h")

    file(MAKE_DIRECTORY "${CMAKE_BINARY_DIR}/autogenerated")

    # merge ggml-common.h and ggml-metal.metal into a single file
    set(METALLIB_EMBED_ASM        "${CMAKE_BINARY_DIR}/autogenerated/ggml-metal-embed.s")
    set(METALLIB_SOURCE_EMBED     "${CMAKE_BINARY_DIR}/autogenerated/ggml-metal-embed.metal")
    set(METALLIB_SOURCE_EMBED_TMP "${CMAKE_BINARY_DIR}/autogenerated/ggml-metal-embed.metal.tmp")

    add_custom_command(
        OUTPUT ${METALLIB_EMBED_ASM}
        COMMAND echo "Embedding Metal library"
        COMMAND sed -e '/__embed_ggml-common.h__/r         ${METALLIB_COMMON}' -e '/__embed_ggml-common.h__/d'         < ${METALLIB_SOURCE}           > ${METALLIB_SOURCE_EMBED_TMP}
        COMMAND sed -e '/\#include \"ggml-metal-impl.h\"/r ${METALLIB_IMPL}'   -e '/\#include \"ggml-metal-impl.h\"/d' < ${METALLIB_SOURCE_EMBED_TMP} > ${METALLIB_SOURCE_EMBED}
        COMMAND echo ".section __DATA,__ggml_metallib"          >  ${METALLIB_EMBED_ASM}
        COMMAND echo ".globl _ggml_metallib_start"              >> ${METALLIB_EMBED_ASM}
        COMMAND echo "_ggml_metallib_start:"                    >> ${METALLIB_EMBED_ASM}
        COMMAND echo ".incbin \"${METALLIB_SOURCE_EMBED}\"" >> ${METALLIB_EMBED_ASM}
        COMMAND echo ".globl _ggml_metallib_end"                >> ${METALLIB_EMBED_ASM}
        COMMAND echo "_ggml_metallib_end:"                      >> ${METALLIB_EMBED_ASM}
        DEPENDS ../ggml-common.h ggml-metal.metal ggml-metal-impl.h
        COMMENT "Generate assembly for embedded Metal library"
    )

    target_sources(ggml-metal PRIVATE ${METALLIB_EMBED_ASM})
```

----------------------------------------

TITLE: Configuring CMake Build for llama-quantize Executable
DESCRIPTION: Defines and configures the llama-quantize executable target in CMake, setting its source files, linked libraries, include directories, and C++ standard. The target depends on the common and llama libraries, requires C++17, and will be installed during the install phase.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/quantize/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
set(TARGET llama-quantize)
add_executable(${TARGET} quantize.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_include_directories(${TARGET} PRIVATE ../../common)
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Listing SYCL Devices for llama.cpp
DESCRIPTION: Command to list available SYCL devices using the llama-ls-sycl-device utility.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_9

LANGUAGE: sh
CODE:
```
./build/bin/llama-ls-sycl-device
```

----------------------------------------

TITLE: Running llama.cpp CANN Docker Container
DESCRIPTION: Command to run the Docker container with proper device mappings and volume mounts for Ascend NPU access
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CANN.md#2025-04-22_snippet_1

LANGUAGE: shell
CODE:
```
docker run --name llamacpp --device /dev/davinci0  --device /dev/davinci_manager --device /dev/devmm_svm --device /dev/hisi_hdc -v /usr/local/dcmi:/usr/local/dcmi -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi -v /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/ -v /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info -v /PATH_TO_YOUR_MODELS/:/app/models -it llama-cpp-cann -m /app/models/MODEL_PATH -ngl 32 -p "Building a website can be done in 10 simple steps:"
```

----------------------------------------

TITLE: Cloning llama.cpp Repository
DESCRIPTION: Commands to clone the llama.cpp repository from GitHub and navigate to the project directory.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

----------------------------------------

TITLE: Configuring OpenCL and Python Dependencies for GGML in CMake
DESCRIPTION: Sets up the required packages (OpenCL and Python3) and defines the target name for the OpenCL backend library.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-opencl/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
find_package(OpenCL REQUIRED)
find_package(Python3 REQUIRED)

set(TARGET_NAME ggml-opencl)
```

----------------------------------------

TITLE: Adding CUDA Repository
DESCRIPTION: Commands to add and update the NVIDIA CUDA repository
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CUDA-FEDORA.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
sudo dnf config-manager addrepo --from-repofile=https://developer.download.nvidia.com/compute/cuda/repos/fedora41/x86_64/cuda-fedora41.repo
sudo dnf distro-sync
```

----------------------------------------

TITLE: Python Package Dependencies for llama.cpp
DESCRIPTION: A requirements.txt file specifying the Python packages required by the llama.cpp project. It includes numpy 1.26.4 or compatible, PySide6 6.9.0 or compatible, and gguf version 0.16.0 or newer.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-gguf_editor_gui.txt#2025-04-22_snippet_0

LANGUAGE: text
CODE:
```
numpy~=1.26.4
PySide6~=6.9.0
gguf>=0.16.0
```

----------------------------------------

TITLE: Adding OpenCL Backend Library in GGML CMake Configuration
DESCRIPTION: Adds the OpenCL backend library to the GGML project, specifying source files and linking OpenCL libraries.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-opencl/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: CMake
CODE:
```
ggml_add_backend_library(${TARGET_NAME}
                         ggml-opencl.cpp
                         ../../include/ggml-opencl.h)
target_link_libraries(${TARGET_NAME} PRIVATE ${OpenCL_LIBRARIES})
target_include_directories(${TARGET_NAME} PRIVATE ${OpenCL_INCLUDE_DIRS})
```

----------------------------------------

TITLE: Cloning the WavTokenizer voice decoder model
DESCRIPTION: Commands to clone the WavTokenizer voice decoder model from HuggingFace, setting up Git LFS to properly download the large model files.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_4

LANGUAGE: shell
CODE:
```
pushd models
git clone --branch main --single-branch --depth 1 https://huggingface.co/novateur/WavTokenizer-large-speech-75token
cd WavTokenizer-large-speech-75token && git lfs install && git lfs pull
popd
```

----------------------------------------

TITLE: Running llama.cpp on Android via Termux
DESCRIPTION: Command to execute the llama-cli binary on Android with a model file, context size, and prompt parameters.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/android.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
$ ./build/bin/llama-cli -m ~/{model}.gguf -c {context-size} -p "{your-prompt}"
```

----------------------------------------

TITLE: Creating Fedora Toolbox Container
DESCRIPTION: Commands to create and enter a Fedora 41 toolbox container for CUDA development
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CUDA-FEDORA.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
toolbox create --image registry.fedoraproject.org/fedora-toolbox:41 --container fedora-toolbox-41-cuda
toolbox enter --container fedora-toolbox-41-cuda
```

----------------------------------------

TITLE: Configuring RPC Server Build with CMake
DESCRIPTION: Sets up the build configuration for an RPC server executable. It defines the target name, specifies the source file, links against the ggml library, and sets the C++ standard to C++17.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/rpc/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(TARGET rpc-server)
add_executable(${TARGET} rpc-server.cpp)
target_link_libraries(${TARGET} PRIVATE ggml)
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Using llama-llava-clip-quantize-cli to Quantize CLIP Visual Projector Models
DESCRIPTION: This command demonstrates the basic syntax for quantizing a CLIP visual projector model using the llama-llava-clip-quantize-cli tool. It takes paths to the input and output model files along with a quantization type identifier.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llava/README-quantize.md#2025-04-22_snippet_0

LANGUAGE: sh
CODE:
```
./bin/llama-llava-clip-quantize-cli /path/to/ggml-model-f32.gguf /path/to/ggml-model-quantized.gguf <type>
```

----------------------------------------

TITLE: Configuring Installation of GGML Headers and Libraries
DESCRIPTION: This section sets up the installation configuration for GGML, including public headers, library files, and package information. It defines which files should be installed and where they should be placed.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_18

LANGUAGE: CMake
CODE:
```
#
# install
#

include(CMakePackageConfigHelpers)

# all public headers
set(GGML_PUBLIC_HEADERS
    include/ggml.h
    include/ggml-cpu.h
    include/ggml-alloc.h
    include/ggml-backend.h
    include/ggml-blas.h
    include/ggml-cann.h
    include/ggml-cpp.h
    include/ggml-cuda.h
    include/ggml-kompute.h
    include/ggml-opt.h
    include/ggml-metal.h
    include/ggml-rpc.h
    include/ggml-sycl.h
    include/ggml-vulkan.h
    include/gguf.h)

set_target_properties(ggml PROPERTIES PUBLIC_HEADER "${GGML_PUBLIC_HEADERS}")
#if (GGML_METAL)
#    set_target_properties(ggml PROPERTIES RESOURCE "${CMAKE_CURRENT_SOURCE_DIR}/src/ggml-metal.metal")
#endif()
install(TARGETS ggml LIBRARY PUBLIC_HEADER)
install(TARGETS ggml-base LIBRARY)

if (GGML_STANDALONE)
    configure_file(${CMAKE_CURRENT_SOURCE_DIR}/ggml.pc.in
        ${CMAKE_CURRENT_BINARY_DIR}/ggml.pc
        @ONLY)

    install(FILES ${CMAKE_CURRENT_BINARY_DIR}/ggml.pc
        DESTINATION share/pkgconfig)
endif()
```

----------------------------------------

TITLE: Downloading Test Dataset for llama.cpp Benchmarking
DESCRIPTION: Command to download the ShareGPT dataset originally used in vLLM benchmarks for testing llama.cpp server performance.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/bench/README.md#2025-04-22_snippet_1

LANGUAGE: shell
CODE:
```
wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json
```

----------------------------------------

TITLE: Text Generation Model Comparison Command
DESCRIPTION: Example command for comparing text generation performance between 7B and 13B models with different generation lengths.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_1

LANGUAGE: sh
CODE:
```
./llama-bench -m models/7B/ggml-model-q4_0.gguf -m models/13B/ggml-model-q4_0.gguf -p 0 -n 128,256,512
```

----------------------------------------

TITLE: Opaque Type Definition Example in C++
DESCRIPTION: Example showing how to properly define and use opaque types with the _t suffix. This demonstrates the recommendation for handling types that should be opaque to users.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/CONTRIBUTING.md#2025-04-22_snippet_2

LANGUAGE: cpp
CODE:
```
typedef struct llama_context * llama_context_t;

enum llama_pooling_type llama_pooling_type(const llama_context_t ctx);
```

----------------------------------------

TITLE: Configuring OpenCL Compilation Flags in GGML CMake
DESCRIPTION: Sets various compilation flags for OpenCL, including profiling, SOA Q, target version, and Adreno kernel optimization.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-opencl/CMakeLists.txt#2025-04-22_snippet_2

LANGUAGE: CMake
CODE:
```
if (GGML_OPENCL_PROFILING)
    message(STATUS "OpenCL profiling enabled (increases CPU overhead)")
    add_compile_definitions(GGML_OPENCL_PROFILING)
endif ()

add_compile_definitions(GGML_OPENCL_SOA_Q)
add_compile_definitions(GGML_OPENCL_TARGET_VERSION=${GGML_OPENCL_TARGET_VERSION})

if (GGML_OPENCL_USE_ADRENO_KERNELS)
    message(STATUS "OpenCL will use matmul kernels optimized for Adreno")
    add_compile_definitions(GGML_OPENCL_USE_ADRENO_KERNELS)
endif ()
```

----------------------------------------

TITLE: Slot Save Operation Response Format
DESCRIPTION: JSON response structure for saving a slot's prompt cache to a file, including slot ID, filename, and timing information.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_14

LANGUAGE: json
CODE:
```
{
    "id_slot": 0,
    "filename": "slot_save_file.bin",
    "n_saved": 1745,
    "n_written": 14309796,
    "timings": {
        "save_ms": 49.865
    }
}
```

----------------------------------------

TITLE: Python Dependencies for llama.cpp Project
DESCRIPTION: This requirements file lists the Python package dependencies with version constraints. It specifies docstring_parser version ~=0.15, pydantic version ~=2.6.3, and requests with no version constraint.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-pydantic.txt#2025-04-22_snippet_0

LANGUAGE: plaintext
CODE:
```
docstring_parser~=0.15
pydantic~=2.6.3
requests
```

----------------------------------------

TITLE: Exporting LLM from LLaVA Next Model
DESCRIPTION: Python script to export only the LLM from the LLaVA next model using the transformers library. This is useful when the legacy conversion script is incompatible with the language model.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
import os
import transformers

model_path = ...
llm_export_path = ...

tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)
model = transformers.AutoModelForImageTextToText.from_pretrained(model_path)

tokenizer.save_pretrained(llm_export_path)
model.language_model.save_pretrained(llm_export_path)
```

----------------------------------------

TITLE: Quantizing GGUF model to Q8_0 format
DESCRIPTION: Command to quantize the 16-bit floating point GGUF model to Q8_0 format, reducing its size while maintaining reasonable quality.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_3

LANGUAGE: shell
CODE:
```
build/bin/llama-quantize models/outetts-0.2-0.5B-f16.gguf \
    models/outetts-0.2-0.5B-q8_0.gguf q8_0
```

----------------------------------------

TITLE: Building llama.cpp with OpenCL for Windows Arm64
DESCRIPTION: PowerShell commands to build llama.cpp with OpenCL support for Windows Arm64 using CMake.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_28

LANGUAGE: powershell
CODE:
```
cmake .. -G Ninja `
  -DCMAKE_TOOLCHAIN_FILE="$HOME/dev/llm/llama.cpp/cmake/arm64-windows-llvm.cmake" `
  -DCMAKE_BUILD_TYPE=Release `
  -DCMAKE_PREFIX_PATH="$HOME/dev/llm/opencl" `
  -DBUILD_SHARED_LIBS=OFF `
  -DGGML_OPENCL=ON
ninja
```

----------------------------------------

TITLE: Finding Available Tests Matching a Pattern
DESCRIPTION: This command uses ctest to find all tests that match a specific regex pattern without executing them. It shows the test commands in verbose mode that can later be used with GDB.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/debugging-tests.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
ctest -R "test-tokenizer" -V -N
```

----------------------------------------

TITLE: Configuring MTMD Library Build in CMake
DESCRIPTION: Sets up the MTMD (multimodal) library as an object library with its dependencies, include directories, and creates static/shared versions. It links against ggml and llama libraries and requires C++17 support.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llava/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: CMake
CODE:
```
add_library(mtmd OBJECT
            mtmd.cpp
            mtmd.h
            clip.cpp
            clip.h
            clip-impl.h
            )

target_link_libraries(mtmd PRIVATE ggml llama ${CMAKE_THREAD_LIBS_INIT})

target_include_directories(mtmd PUBLIC .)
target_include_directories(mtmd PRIVATE ../..)
target_include_directories(mtmd PRIVATE ../../common) # for stb_image.h

target_compile_features(mtmd PRIVATE cxx_std_17)

add_library(mtmd_static STATIC $<TARGET_OBJECTS:mtmd>)
if (BUILD_SHARED_LIBS)
    set_target_properties(mtmd PROPERTIES POSITION_INDEPENDENT_CODE ON)
    target_compile_definitions(mtmd PRIVATE LLAMA_SHARED LLAMA_BUILD)
    add_library(mtmd_shared SHARED $<TARGET_OBJECTS:mtmd>)
    target_link_libraries(mtmd_shared PRIVATE ggml llama ${CMAKE_THREAD_LIBS_INIT})
    install(TARGETS mtmd_shared LIBRARY)
endif()
```

----------------------------------------

TITLE: Cloning MobileVLM and CLIP Models
DESCRIPTION: Commands for cloning the required MobileVLM-1.7B model and CLIP vision model from Hugging Face repositories. These are prerequisites for the model conversion process.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_1

LANGUAGE: sh
CODE:
```
git clone https://huggingface.co/mtgv/MobileVLM-1.7B

git clone https://huggingface.co/openai/clip-vit-large-patch14-336
```

----------------------------------------

TITLE: Configuring GGML Core Settings
DESCRIPTION: Sets core GGML settings including scheduler parameters and CPU backend enablement.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_14

LANGUAGE: CMake
CODE:
```
# ggml core
set(GGML_SCHED_MAX_COPIES  "4" CACHE STRING "ggml: max input copies for pipeline parallelism")
option(GGML_CPU                             "ggml: enable CPU backend"                        ON)
```

----------------------------------------

TITLE: Running k6 Benchmark with Default Settings
DESCRIPTION: Command to run the k6 benchmark script with specific parameters: 500 chat completions with 8 concurrent users for a maximum of 10 minutes.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/bench/README.md#2025-04-22_snippet_4

LANGUAGE: shell
CODE:
```
./k6 run script.js --duration 10m --iterations 500 --vus 8
```

----------------------------------------

TITLE: Building llama.cpp with Arm KleidiAI Support
DESCRIPTION: Commands to build llama.cpp with Arm KleidiAI optimization support using CMake.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_23

LANGUAGE: bash
CODE:
```
cmake -B build -DGGML_CPU_KLEIDIAI=ON
cmake --build build --config Release
```

----------------------------------------

TITLE: Mapping Original Tensor Names to GGUF Standardized Names
DESCRIPTION: Python code showing how to map model-specific tensor names to standardized GGUF tensor names using a dictionary structure. This example specifically maps attention normalization tensors from various model architectures.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/HOWTO-add-model.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
block_mappings_cfg: dict[MODEL_TENSOR, tuple[str, ...]] = {
        # Attention norm
        MODEL_TENSOR.ATTN_NORM: (
            "gpt_neox.layers.{bid}.input_layernorm",                # gptneox
            "transformer.h.{bid}.ln_1",                             # gpt2 gpt-j refact qwen
            "transformer.blocks.{bid}.norm_1",                      # mpt
            ...
        )
}
```

----------------------------------------

TITLE: Aggregating Python Requirements for llama.cpp
DESCRIPTION: Imports requirements from specialized dependency files for different llama.cpp tools and converters. Ensures consistent package versions across all Python scripts in the project.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements.txt#2025-04-22_snippet_0

LANGUAGE: plaintext
CODE:
```
-r ./requirements/requirements-convert_legacy_llama.txt

-r ./requirements/requirements-convert_hf_to_gguf.txt
-r ./requirements/requirements-convert_hf_to_gguf_update.txt
-r ./requirements/requirements-convert_llama_ggml_to_gguf.txt
-r ./requirements/requirements-convert_lora_to_gguf.txt
-r ./requirements/requirements-tool_bench.txt
```

----------------------------------------

TITLE: Displaying Binary Filename Migration Table in Markdown
DESCRIPTION: A markdown table showing the mapping between old and new binary filenames for the llama.cpp project. The table includes entries for various tools and libraries, with most new names prefixed with 'llama-'.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/deprecation-warning/README.md#2025-04-22_snippet_0

LANGUAGE: markdown
CODE:
```
| Old Filename | New Filename |
| ---- | ---- |
| main | llama-cli |
| server | llama-server |
| llama-bench | llama-bench |
| embedding | llama-embedding |
| quantize | llama-quantize |
| tokenize | llama-tokenize |
| export-lora | llama-export-lora |
| libllava.a | libllava.a |
| baby-llama | llama-baby-llama |
| batched | llama-batched |
| batched-bench | llama-batched-bench |
| benchmark-matmult | llama-benchmark-matmult |
| convert-llama2c-to-ggml | llama-convert-llama2c-to-ggml |
| eval-callback | llama-eval-callback |
| gbnf-validator | llama-gbnf-validator |
| gguf | llama-gguf |
| gguf-split | llama-gguf-split |
| gritlm | llama-gritlm |
| imatrix | llama-imatrix |
| infill | llama-infill |
| llava-cli | llama-llava-cli |
| lookahead | llama-lookahead |
| lookup | llama-lookup |
| lookup-create | llama-lookup-create |
| lookup-merge | llama-lookup-merge |
| lookup-stats | llama-lookup-stats |
| parallel | llama-parallel |
| passkey | llama-passkey |
| perplexity | llama-perplexity |
| q8dot | llama-q8dot |
| quantize-stats | llama-quantize-stats |
| retrieval | llama-retrieval |
| save-load-state | llama-save-load-state |
| simple | llama-simple |
| speculative | llama-speculative |
| vdot | llama-vdot |
| tests/test-c.o | tests/test-c.o |
```

----------------------------------------

TITLE: Configuring Vulkan Shader Generation in CMake
DESCRIPTION: Set up custom commands for generating Vulkan shaders, including handling cross-compilation scenarios and defining input/output paths.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-vulkan/CMakeLists.txt#2025-04-22_snippet_2

LANGUAGE: CMake
CODE:
```
set (_ggml_vk_host_suffix $<IF:$<STREQUAL:${CMAKE_HOST_SYSTEM_NAME},Windows>,.exe,>)
set (_ggml_vk_genshaders_cmd ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/vulkan-shaders-gen${_ggml_vk_host_suffix})
set (_ggml_vk_header     ${CMAKE_CURRENT_BINARY_DIR}/ggml-vulkan-shaders.hpp)
set (_ggml_vk_source     ${CMAKE_CURRENT_BINARY_DIR}/ggml-vulkan-shaders.cpp)
set (_ggml_vk_input_dir  ${CMAKE_CURRENT_SOURCE_DIR}/vulkan-shaders)
set (_ggml_vk_output_dir ${CMAKE_CURRENT_BINARY_DIR}/vulkan-shaders.spv)

file(GLOB _ggml_vk_shader_deps "${_ggml_vk_input_dir}/*.comp")
set (_ggml_vk_shader_deps ${_ggml_vk_shader_deps} vulkan-shaders-gen)

if (CMAKE_CROSSCOMPILING)
    set(_ggml_vk_shader_deps ${_ggml_vk_shader_deps} vulkan-shaders-gen-build vulkan-shaders-gen-install)
endif()

add_custom_command(
    OUTPUT ${_ggml_vk_header}
            ${_ggml_vk_source}

    COMMAND ${_ggml_vk_genshaders_cmd}
        --glslc      ${Vulkan_GLSLC_EXECUTABLE}
        --input-dir  ${_ggml_vk_input_dir}
        --output-dir ${_ggml_vk_output_dir}
        --target-hpp ${_ggml_vk_header}
        --target-cpp ${_ggml_vk_source}
        --no-clean

    DEPENDS ${_ggml_vk_shader_deps}
    COMMENT "Generate vulkan shaders"
)
```

----------------------------------------

TITLE: Converting GLMV-EDGE image encoder to GGUF
DESCRIPTION: Python command to convert the GLMV-EDGE image encoder to GGUF format using the glmedge-convert-image-encoder-to-gguf.py script. This step prepares the image encoder for use with llama.cpp.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/glmedge.md#2025-04-22_snippet_3

LANGUAGE: sh
CODE:
```
python ./examples/llava/glmedge-convert-image-encoder-to-gguf.py -m ../model_path --llava-projector ../model_path/glm.projector --output-dir ../model_path
```

----------------------------------------

TITLE: Setting up oneAPI environment for SYCL on Intel
DESCRIPTION: Sources the oneAPI environment variables needed for SYCL support on Intel GPUs. This should be executed before running any SYCL-enabled llama.cpp applications.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/sycl/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
source /opt/intel/oneapi/setvars.sh
```

----------------------------------------

TITLE: Installing Ascend Firmware
DESCRIPTION: Command for installing the Ascend NPU firmware
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CANN.md#2025-04-22_snippet_3

LANGUAGE: shell
CODE:
```
sudo sh Ascend-hdk-910b-npu-firmware_x.x.x.x.X.run --full
```

----------------------------------------

TITLE: Configuring Qwen2VL CLI Executable in CMake
DESCRIPTION: Sets up the llama-qwen2vl-cli executable, configuring its source, output name, installation, dependencies, and C++17 requirement.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llava/CMakeLists.txt#2025-04-22_snippet_4

LANGUAGE: CMake
CODE:
```
set(TARGET llama-qwen2vl-cli)
add_executable(${TARGET} qwen2vl-cli.cpp)
set_target_properties(${TARGET} PROPERTIES OUTPUT_NAME llama-qwen2vl-cli)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llava ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Setting up Python environment for TTS client
DESCRIPTION: Commands to create and activate a Python virtual environment, then install the required dependencies (requests and numpy) for the TTS client script.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_11

LANGUAGE: shell
CODE:
```
python3 -m venv venv
source venv/bin/activate
(venv) pip install requests numpy
```

----------------------------------------

TITLE: Core GGML Library Setup
DESCRIPTION: Configures the base GGML library components including header files and source files. Sets up the core library target with necessary include directories.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_10

LANGUAGE: cmake
CODE:
```
add_library(ggml-base
            ../include/ggml.h
            ../include/ggml-alloc.h
            ../include/ggml-backend.h
            ../include/ggml-cpp.h
            ../include/ggml-opt.h
            ../include/gguf.h
            ggml.c
            ggml-alloc.c
            ggml-backend.cpp
            ggml-opt.cpp
            ggml-threading.cpp
            ggml-threading.h
            ggml-quants.c
            ggml-quants.h
            gguf.cpp)
```

----------------------------------------

TITLE: Quantizing LLaMA Model from FP32 to Q4_K
DESCRIPTION: Command for quantizing the converted LLaMA model from FP32 to Q4_K format using the llama-quantize tool for more efficient storage and inference.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_6

LANGUAGE: sh
CODE:
```
./llama-quantize path/to/MobileVLM-1.7B/ggml-model-F32.gguf path/to/MobileVLM-1.7B/ggml-model-q4_k.gguf q4_k_s
```

----------------------------------------

TITLE: Building llama.cpp with OpenCL for Android
DESCRIPTION: Commands to build llama.cpp with OpenCL support for Android using CMake and NDK.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_26

LANGUAGE: sh
CODE:
```
cd ~/dev/llm

git clone https://github.com/ggml-org/llama.cpp && \
cd llama.cpp && \
mkdir build-android && cd build-android

cmake .. -G Ninja \
  -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \
  -DANDROID_ABI=arm64-v8a \
  -DANDROID_PLATFORM=android-28 \
  -DBUILD_SHARED_LIBS=OFF \
  -DGGML_OPENCL=ON

ninja
```

----------------------------------------

TITLE: Calculating Mathematical Expression in Python
DESCRIPTION: A mathematical calculation performed using Python, computing 4 multiplied by 7 divided by 3.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/prompts/reason-act.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
4 * 7 / 3
```

----------------------------------------

TITLE: Configuring SYCL Device Listing Executable in CMake
DESCRIPTION: This CMake script sets up the build configuration for a SYCL device listing utility. It defines the target name, adds the executable with its source file, specifies installation rules, and links necessary dependencies including the common and llama libraries. The code also sets C++17 as the required standard.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/sycl/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
#  MIT license
#  Copyright (C) 2024 Intel Corporation
#  SPDX-License-Identifier: MIT

set(TARGET llama-ls-sycl-device)
add_executable(${TARGET} ls-sycl-device.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Configuring llama-imatrix CMake Target
DESCRIPTION: Sets up a CMake executable target 'llama-imatrix' with dependencies on common and llama libraries. Configures C++17 standard requirement and installation rules.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/imatrix/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
set(TARGET llama-imatrix)
add_executable(${TARGET} imatrix.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Configuring llama-bench Executable in CMake
DESCRIPTION: Sets up the llama-bench target as an executable, specifies its source file, defines installation rules, links required libraries, and sets the C++ standard to C++17. This configuration is part of the llama.cpp project build system.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(TARGET llama-bench)
add_executable(${TARGET} llama-bench.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Downloading Code Llama Model with HF Script in Console
DESCRIPTION: Command for downloading a CodeLlama model from Hugging Face that supports infill functionality. This uses TheBloke's quantized version (Q5_K_S) of the 13B parameter model.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/infill/README.md#2025-04-22_snippet_0

LANGUAGE: console
CODE:
```
scripts/hf.sh --repo TheBloke/CodeLlama-13B-GGUF --file codellama-13b.Q5_K_S.gguf --outdir models
```

----------------------------------------

TITLE: Installing Ascend Driver and User Setup
DESCRIPTION: Commands for setting up the Ascend driver user group and installing the driver software
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CANN.md#2025-04-22_snippet_2

LANGUAGE: shell
CODE:
```
sudo groupadd -g HwHiAiUser
sudo useradd -g HwHiAiUser -d /home/HwHiAiUser -m HwHiAiUser -s /bin/bash
sudo usermod -aG HwHiAiUser $USER
sudo sh Ascend-hdk-910b-npu-driver_x.x.x_linux-{arch}.run --full --install-for-all
```

----------------------------------------

TITLE: SYCL Target Validation and Compiler Setup
DESCRIPTION: Validates the SYCL target platform (INTEL/NVIDIA/AMD) and sets up the appropriate SYCL compiler, with support for both oneAPI and open-source implementations.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-sycl/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
message(STATUS  "GGML_SYCL_TARGET=${GGML_SYCL_TARGET}")

if (NOT GGML_SYCL_TARGET MATCHES "^(INTEL|NVIDIA|AMD)$")
    message(FATAL_ERROR "Invalid backend chosen, supported options are INTEL, NVIDIA, or AMD")
endif()

check_cxx_compiler_flag("-fsycl" SUPPORTS_SYCL)

if (DEFINED ENV{ONEAPI_ROOT})
    message(STATUS "Using oneAPI Release SYCL compiler (icpx).")
elseif(SUPPORTS_SYCL)
    message(WARNING "Using open-source SYCL compiler (clang++). Didn't detect ENV {ONEAPI_ROOT}.
        If you expected the oneAPI Release compiler, please install oneAPI & source it, like:
        source /opt/intel/oneapi/setvars.sh")
else()
    message(FATAL_ERROR, "C++ compiler lacks SYCL support.")
endif()
```

----------------------------------------

TITLE: Example of a Generated GGUF Hash Manifest
DESCRIPTION: This is an example output of a generated manifest file that contains multiple hash types (xxh64, sha1, sha256) for each tensor in the GGUF file as well as for the overall file.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-hash/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
xxh64     f66e9cd66a4396a0  test.gguf:tensor_0
sha1      59f79ecefd8125a996fdf419239051a7e99e5f20  test.gguf:tensor_0
sha256    c0510d38fa060c46265e0160a85c7243096b01dd31c2f355bdbb5516b20de1bd  test.gguf:tensor_0
xxh64     7d3a1f9ac04d0537  test.gguf:tensor_1
sha1      4765f592eacf096df4628ba59476af94d767080a  test.gguf:tensor_1
sha256    8514cbcc73692a2c56bd7a33a022edd5ff819614bd23b19915d7224387f397a7  test.gguf:tensor_1
xxh64     a0af5d700049693b  test.gguf:tensor_2
sha1      25cbfbad4513cc348e2c95ebdee69d6ff2fd8753  test.gguf:tensor_2
sha256    947e6b36e20f2cc95e1d2ce1c1669d813d574657ac6b5ac5196158d454d35180  test.gguf:tensor_2
xxh64     e83fddf559d7b6a6  test.gguf:tensor_3
sha1      a9cba73e2d90f2ee3dae2548caa42bef3fe6a96c  test.gguf:tensor_3
sha256    423b044e016d8ac73c39f23f60bf01bedef5ecb03c0230accd824c91fe86f1a1  test.gguf:tensor_3
xxh64     1257733306b7992d  test.gguf:tensor_4
sha1      d7bc61db93bb685ce9d598da89717c66729b7543  test.gguf:tensor_4
sha256    79737cb3912d4201384cf7f16a1a37ff7823f23ea796cb205b6ca361ab9e3ebf  test.gguf:tensor_4
xxh64     d238d16ba4711e58  test.gguf:tensor_5
sha1      0706566c198fe1072f37e0a5135b4b5f23654c52  test.gguf:tensor_5
sha256    60949be8298eced0ecdde64487643d018407bd261691e061d9e9c3dbc9fd358b  test.gguf:tensor_5
xxh64     3fbc3b65ab8c7f39  test.gguf:tensor_6
sha1      73922a0727226a409049f6fc3172a52219ca6f00  test.gguf:tensor_6
sha256    574f4c46ff384a3b9a225eb955d2a871847a2e8b3fa59387a8252832e92ef7b0  test.gguf:tensor_6
xxh64     c22021c29854f093  test.gguf:tensor_7
sha1      efc39cece6a951188fc41e354c73bbfe6813d447  test.gguf:tensor_7
sha256    4c0410cd3c500f078ae5b21e8dc9eb79e29112713b2ab58a882f82a3868d4d75  test.gguf:tensor_7
xxh64     936df61f5d64261f  test.gguf:tensor_8
sha1      c2490296d789a4f34398a337fed8377d943d9f06  test.gguf:tensor_8
sha256    c4401313feeba0261275c3b25bd2d8fe40ce04e0f440c2980ed0e9674c30ff01  test.gguf:tensor_8
xxh64     93fd20c64421c081  test.gguf:tensor_9
sha1      7047ce1e78437a6884337a3751c7ee0421918a65  test.gguf:tensor_9
sha256    23d57cf0d7a6e90b0b3616b41300e0cd354781e812add854a5f95aa55f2bc514  test.gguf:tensor_9
xxh64     5a54d3aad816f302  test.gguf
sha1      d15be52c4ff213e823cb6dd13af7ee2f978e7042  test.gguf
sha256    7dd641b32f59b60dbd4b5420c4b0f6321ccf48f58f6ae201a3dbc4a58a27c6e4  test.gguf
```

----------------------------------------

TITLE: Configuring MTMD CLI Executable in CMake
DESCRIPTION: Sets up the llama-mtmd-cli executable, configuring its source, output name, installation, dependencies, and C++17 requirement.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llava/CMakeLists.txt#2025-04-22_snippet_5

LANGUAGE: CMake
CODE:
```
set(TARGET llama-mtmd-cli)
add_executable(${TARGET} mtmd-cli.cpp)
set_target_properties(${TARGET} PROPERTIES OUTPUT_NAME llama-mtmd-cli)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common mtmd ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Running llama-server with SimpleChat Frontend
DESCRIPTION: Command to start the llama server with the SimpleChat frontend from the build directory
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/public_simplechat/readme.md#2025-04-22_snippet_0

LANGUAGE: shell
CODE:
```
bin/llama-server -m path/model.gguf --path ../examples/server/public_simplechat
```

----------------------------------------

TITLE: Backend Library Function Definition
DESCRIPTION: Defines a function for adding GGML backend libraries with support for both static and dynamic loading. Handles library output directory and compilation definitions.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_11

LANGUAGE: cmake
CODE:
```
function(ggml_add_backend_library backend)
    if (GGML_BACKEND_DL)
        add_library(${backend} MODULE ${ARGN})
        set_target_properties(${backend} PROPERTIES LIBRARY_OUTPUT_DIRECTORY ${CMAKE_RUNTIME_OUTPUT_DIRECTORY})
        target_compile_definitions(${backend} PRIVATE GGML_BACKEND_DL)
        add_dependencies(ggml ${backend})
    else()
        add_library(${backend} ${ARGN})
        target_link_libraries(ggml PUBLIC ${backend})
        install(TARGETS ${backend} LIBRARY)
    endif()

    target_link_libraries(${backend} PRIVATE ggml-base)
    target_include_directories(${backend} PRIVATE ..)

    if (${BUILD_SHARED_LIBS})
        target_compile_definitions(${backend} PRIVATE GGML_BACKEND_BUILD)
        target_compile_definitions(${backend} PUBLIC  GGML_BACKEND_SHARED)
    endif()

    if(NOT GGML_AVAILABLE_BACKENDS)
        set(GGML_AVAILABLE_BACKENDS "${backend}"
            CACHE INTERNAL "List of backends for cmake package")
    else()
        list(FIND GGML_AVAILABLE_BACKENDS "${backend}" has_backend)
        if(has_backend EQUAL -1)
            set(GGML_AVAILABLE_BACKENDS "${GGML_AVAILABLE_BACKENDS};${backend}"
                CACHE INTERNAL "List of backends for cmake package")
        endif()
    endif()
endfunction()
```

----------------------------------------

TITLE: Converting LLaMA Model to GGUF
DESCRIPTION: Command for converting the LLaMA part of the LLaVA model to GGUF format using the legacy conversion script, skipping unknown parameters.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_5

LANGUAGE: sh
CODE:
```
python ./examples/convert_legacy_llama.py path/to/MobileVLM-1.7B --skip-unknown
```

----------------------------------------

TITLE: Building llama.cpp with OpenCL for Windows 11 Arm64
DESCRIPTION: PowerShell commands to clone and build llama.cpp with OpenCL support for Windows 11 Arm64, using CMake and Ninja with the ARM64 Windows LLVM toolchain.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/OPENCL.md#2025-04-22_snippet_5

LANGUAGE: powershell
CODE:
```
mkdir -p ~/dev/llm
cd ~/dev/llm

git clone https://github.com/ggml-org/llama.cpp && cd llama.cpp
mkdir build && cd build

cmake .. -G Ninja `
  -DCMAKE_TOOLCHAIN_FILE="$HOME/dev/llm/llama.cpp/cmake/arm64-windows-llvm.cmake" `
  -DCMAKE_BUILD_TYPE=Release `
  -DCMAKE_PREFIX_PATH="$HOME/dev/llm/opencl" `
  -DBUILD_SHARED_LIBS=OFF `
  -DGGML_OPENCL=ON
ninja
```

----------------------------------------

TITLE: CPU Backend Variant Configuration
DESCRIPTION: Sets up CPU-specific backend variants with different instruction set optimizations. Configures options for various CPU architectures including SSE, AVX, and AMX features.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_12

LANGUAGE: cmake
CODE:
```
function(ggml_add_cpu_backend_variant tag_name)
    set(GGML_CPU_TAG_NAME ${tag_name})
    foreach (feat NATIVE
                  SSE42
                  AVX AVX2 BMI2 AVX_VNNI FMA F16C
                  AVX512 AVX512_VBMI AVX512_VNNI AVX512_BF16
                  AMX_TILE AMX_INT8 AMX_BF16)
        set(GGML_${feat} OFF)
    endforeach()

    foreach (feat ${ARGN})
        set(GGML_${feat} ON)
    endforeach()

    ggml_add_cpu_backend_variant_impl(${tag_name})
endfunction()
```

----------------------------------------

TITLE: Configuring Build for llama-passkey Executable in CMake
DESCRIPTION: This CMake configuration sets up the llama-passkey executable build process. It defines the target name, source file (passkey.cpp), links necessary libraries (common, llama, and threading libraries), specifies C++17 as the required standard, and configures installation parameters.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/passkey/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(TARGET llama-passkey)
add_executable(${TARGET} passkey.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Compiling with MUSA GPU Support
DESCRIPTION: Basic compilation commands for building llama.cpp with MUSA GPU support using CMake. Enables GPU acceleration for Moore Threads GPUs.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_9

LANGUAGE: bash
CODE:
```
cmake -B build -DGGML_MUSA=ON
cmake --build build --config Release
```

----------------------------------------

TITLE: Configuring Sanitizers for Non-MSVC Compilers
DESCRIPTION: Sets up various sanitizers (thread, address, undefined behavior) for non-MSVC compilers. These help catch runtime errors during development.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_2

LANGUAGE: CMake
CODE:
```
if (NOT MSVC)
    if (GGML_SANITIZE_THREAD)
        add_compile_options(-fsanitize=thread)
        link_libraries     (-fsanitize=thread)
    endif()

    if (GGML_SANITIZE_ADDRESS)
        add_compile_options(-fsanitize=address -fno-omit-frame-pointer)
        link_libraries     (-fsanitize=address)
    endif()

    if (GGML_SANITIZE_UNDEFINED)
        add_compile_options(-fsanitize=undefined)
        link_libraries     (-fsanitize=undefined)
    endif()
endif()
```

----------------------------------------

TITLE: Windows-Specific Compiler Settings
DESCRIPTION: Configuration for MSVC compiler including UTF-8 support and large object file handling
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/CMakeLists.txt#2025-04-22_snippet_2

LANGUAGE: cmake
CODE:
```
if (MSVC)
    add_compile_options("$<$<COMPILE_LANGUAGE:C>:/utf-8>")
    add_compile_options("$<$<COMPILE_LANGUAGE:CXX>:/utf-8>")
    add_compile_options("$<$<COMPILE_LANGUAGE:C>:/bigobj>")
    add_compile_options("$<$<COMPILE_LANGUAGE:CXX>:/bigobj>")
endif()
```

----------------------------------------

TITLE: Configuring LLaVA CLIP Quantization CLI in CMake
DESCRIPTION: Sets up the llama-llava-clip-quantize-cli executable for CLIP model quantization, configuring its source, output name, installation, dependencies, and C++17 requirement.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llava/CMakeLists.txt#2025-04-22_snippet_6

LANGUAGE: CMake
CODE:
```
set(TARGET llama-llava-clip-quantize-cli)
add_executable(${TARGET} clip-quantize-cli.cpp)
set_target_properties(${TARGET} PROPERTIES OUTPUT_NAME llama-llava-clip-quantize-cli)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llava ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Build Type Configuration
DESCRIPTION: Sets default build type to Release if not specified, with options for Debug, Release, MinSizeRel, and RelWithDebInfo
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: cmake
CODE:
```
if (NOT XCODE AND NOT MSVC AND NOT CMAKE_BUILD_TYPE)
    set(CMAKE_BUILD_TYPE Release CACHE STRING "Build type" FORCE)
    set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS "Debug" "Release" "MinSizeRel" "RelWithDebInfo")
endif()
```

----------------------------------------

TITLE: Playing the generated audio file
DESCRIPTION: Linux command to play the generated WAV audio file using the aplay utility.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_8

LANGUAGE: shell
CODE:
```
aplay output.wav
```

----------------------------------------

TITLE: Converting LLaMA Part of LLaVA to GGUF
DESCRIPTION: Python command to convert the LLaMA part of the LLaVA model to GGUF format using the convert_legacy_llama.py script.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_5

LANGUAGE: sh
CODE:
```
python ./examples/convert_legacy_llama.py ../llava-v1.5-7b --skip-unknown
```

----------------------------------------

TITLE: Cloning the OuteTTS LLM model repository
DESCRIPTION: Commands to checkout the OuteTTS-0.2-500M model from HuggingFace, including proper Git LFS setup for downloading large files.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_1

LANGUAGE: shell
CODE:
```
pushd models
git clone --branch main --single-branch --depth 1 https://huggingface.co/OuteAI/OuteTTS-0.2-500M
cd OuteTTS-0.2-500M && git lfs install && git lfs pull
popd
```

----------------------------------------

TITLE: SYCL GPU Detection Log Example (Single GPU)
DESCRIPTION: Example log output showing detection of a single SYCL GPU with its compute units. This appears when running with SYCL backend and helps verify the device selection.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_28

LANGUAGE: sh
CODE:
```
detect 1 SYCL GPUs: [0] with top Max compute units:512
```

----------------------------------------

TITLE: Configuring llama-q8dot Executable Build in CMake
DESCRIPTION: Sets up the build configuration for the llama-q8dot executable, which likely performs quantized 8-bit dot product operations. It links against the common and llama libraries and requires C++17 support.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/pocs/vdot/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: CMake
CODE:
```
set(TARGET llama-q8dot)
add_executable(${TARGET} q8dot.cpp)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Configuring Swift Package with LLaMA XCFramework
DESCRIPTION: Demonstrates how to set up a Swift package that uses the precompiled LLaMA XCFramework. It shows the Package.swift configuration including how to reference a specific build of the framework via URL and checksum.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_15

LANGUAGE: swift
CODE:
```
// swift-tools-version: 5.10
// The swift-tools-version declares the minimum version of Swift required to build this package.

import PackageDescription

let package = Package(
    name: "MyLlamaPackage",
    targets: [
        .executableTarget(
            name: "MyLlamaPackage",
            dependencies: [
                "LlamaFramework"
            ]),
        .binaryTarget(
            name: "LlamaFramework",
            url: "https://github.com/ggml-org/llama.cpp/releases/download/b5046/llama-b5046-xcframework.zip",
            checksum: "c19be78b5f00d8d29a25da41042cb7afa094cbf6280a225abe614b03b20029ab"
        )
    ]
)
```

----------------------------------------

TITLE: Building llama.cpp for Windows ARM64
DESCRIPTION: Command to build llama.cpp for Windows on ARM (arm64) using LLVM compiler with CMake.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
cmake --preset arm64-windows-llvm-release -D GGML_OPENMP=OFF
cmake --build build-arm64-windows-llvm-release
```

----------------------------------------

TITLE: Using JSON Schema with llama-cli Command for Constrained Text Generation
DESCRIPTION: Example of using the llama-cli command with a JSON schema that defines an array of objects with name and age properties. The schema constrains the model to generate a JSON array with 10-100 items where each item has a name (string) and age (integer) field.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
llama-cli \
  -hfr bartowski/Phi-3-medium-128k-instruct-GGUF \
  -hff Phi-3-medium-128k-instruct-Q8_0.gguf \
  -j '{
    "type": "array",
    "items": {
        "type": "object",
        "properties": {
            "name": {
                "type": "string",
                "minLength": 1,
                "maxLength": 100
            },
            "age": {
                "type": "integer",
                "minimum": 0,
                "maximum": 150
            }
        },
        "required": ["name", "age"],
        "additionalProperties": false
    },
    "minItems": 10,
    "maxItems": 100
  }' \
  -p 'Generate a {name, age}[] JSON array with famous actors of all ages.'
```

----------------------------------------

TITLE: Defining GGUF Tensor Names for Falcon Model Architecture
DESCRIPTION: Python code demonstrating how to define the required tensor names for a Falcon model architecture in the GGUF constants file. This specifies which tensor types are needed for the model to function properly.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/HOWTO-add-model.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
    MODEL_ARCH.FALCON: [
        MODEL_TENSOR.TOKEN_EMBD,
        MODEL_TENSOR.OUTPUT_NORM,
        MODEL_TENSOR.OUTPUT,
        MODEL_TENSOR.ATTN_NORM,
        MODEL_TENSOR.ATTN_NORM_2,
        MODEL_TENSOR.ATTN_QKV,
        MODEL_TENSOR.ATTN_OUT,
        MODEL_TENSOR.FFN_DOWN,
        MODEL_TENSOR.FFN_UP,
    ]
```

----------------------------------------

TITLE: Implementing Chess Notation Grammar in GBNF
DESCRIPTION: Demonstrates a GBNF grammar for parsing chess moves, showing rule definitions, alternatives, and move sequences with comments.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md#2025-04-22_snippet_0

LANGUAGE: gbnf
CODE:
```
# `root` specifies the pattern for the overall output
root ::= (
    # it must start with the characters "1. " followed by a sequence
    # of characters that match the `move` rule, followed by a space, followed
    # by another move, and then a newline
    "1. " move " " move "\n"

    # it's followed by one or more subsequent moves, numbered with one or two digits
    ([1-9] [0-9]? ". " move " " move "\n")+
)

# `move` is an abstract representation, which can be a pawn, nonpawn, or castle.
# The `[+#]?` denotes the possibility of checking or mate signs after moves
move ::= (pawn | nonpawn | castle) [+#]?

pawn ::= ...
nonpawn ::= ...
castle ::= ...
```

----------------------------------------

TITLE: Basic CMake Project Configuration
DESCRIPTION: Initial CMake setup including minimum version requirement, project name, and basic build settings
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
cmake_minimum_required(VERSION 3.14)
project("llama.cpp" C CXX)
include(CheckIncludeFileCXX)

set(CMAKE_WARN_UNUSED_CLI YES)
set(CMAKE_EXPORT_COMPILE_COMMANDS ON)
```

----------------------------------------

TITLE: Example Output of All-Hash Verification
DESCRIPTION: This is the output of verifying all hash types in a GGUF manifest. It shows verification results for each hash algorithm (XXH64, SHA-1, SHA-256) for each tensor and the overall file.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-hash/README.md#2025-04-22_snippet_8

LANGUAGE: bash
CODE:
```
manifest  test.gguf.manifest  sha256  sha1  xxh64
xxh64     f66e9cd66a4396a0  test.gguf:tensor_0  -  Ok
sha1      59f79ecefd8125a996fdf419239051a7e99e5f20  test.gguf:tensor_0  -  Ok
sha256    c0510d38fa060c46265e0160a85c7243096b01dd31c2f355bdbb5516b20de1bd  test.gguf:tensor_0  -  Ok
xxh64     7d3a1f9ac04d0537  test.gguf:tensor_1  -  Ok
sha1      4765f592eacf096df4628ba59476af94d767080a  test.gguf:tensor_1  -  Ok
sha256    8514cbcc73692a2c56bd7a33a022edd5ff819614bd23b19915d7224387f397a7  test.gguf:tensor_1  -  Ok
xxh64     a0af5d700049693b  test.gguf:tensor_2  -  Ok
sha1      25cbfbad4513cc348e2c95ebdee69d6ff2fd8753  test.gguf:tensor_2  -  Ok
sha256    947e6b36e20f2cc95e1d2ce1c1669d813d574657ac6b5ac5196158d454d35180  test.gguf:tensor_2  -  Ok
xxh64     e83fddf559d7b6a6  test.gguf:tensor_3  -  Ok
sha1      a9cba73e2d90f2ee3dae2548caa42bef3fe6a96c  test.gguf:tensor_3  -  Ok
sha256    423b044e016d8ac73c39f23f60bf01bedef5ecb03c0230accd824c91fe86f1a1  test.gguf:tensor_3  -  Ok
xxh64     1257733306b7992d  test.gguf:tensor_4  -  Ok
sha1      d7bc61db93bb685ce9d598da89717c66729b7543  test.gguf:tensor_4  -  Ok
sha256    79737cb3912d4201384cf7f16a1a37ff7823f23ea796cb205b6ca361ab9e3ebf  test.gguf:tensor_4  -  Ok
xxh64     d238d16ba4711e58  test.gguf:tensor_5  -  Ok
sha1      0706566c198fe1072f37e0a5135b4b5f23654c52  test.gguf:tensor_5  -  Ok
sha256    60949be8298eced0ecdde64487643d018407bd261691e061d9e9c3dbc9fd358b  test.gguf:tensor_5  -  Ok
xxh64     3fbc3b65ab8c7f39  test.gguf:tensor_6  -  Ok
sha1      73922a0727226a409049f6fc3172a52219ca6f00  test.gguf:tensor_6  -  Ok
sha256    574f4c46ff384a3b9a225eb955d2a871847a2e8b3fa59387a8252832e92ef7b0  test.gguf:tensor_6  -  Ok
xxh64     c22021c29854f093  test.gguf:tensor_7  -  Ok
sha1      efc39cece6a951188fc41e354c73bbfe6813d447  test.gguf:tensor_7  -  Ok
sha256    4c0410cd3c500f078ae5b21e8dc9eb79e29112713b2ab58a882f82a3868d4d75  test.gguf:tensor_7  -  Ok
xxh64     936df61f5d64261f  test.gguf:tensor_8  -  Ok
sha1      c2490296d789a4f34398a337fed8377d943d9f06  test.gguf:tensor_8  -  Ok
sha256    c4401313feeba0261275c3b25bd2d8fe40ce04e0f440c2980ed0e9674c30ff01  test.gguf:tensor_8  -  Ok
xxh64     93fd20c64421c081  test.gguf:tensor_9  -  Ok
sha1      7047ce1e78437a6884337a3751c7ee0421918a65  test.gguf:tensor_9  -  Ok
sha256    23d57cf0d7a6e90b0b3616b41300e0cd354781e812add854a5f95aa55f2bc514  test.gguf:tensor_9  -  Ok
xxh64     5a54d3aad816f302  test.gguf  -  Ok
sha1      d15be52c4ff213e823cb6dd13af7ee2f978e7042  test.gguf  -  Ok
sha256    7dd641b32f59b60dbd4b5420c4b0f6321ccf48f58f6ae201a3dbc4a58a27c6e4  test.gguf  -  Ok

Verification results for test.gguf.manifest - Success
```

----------------------------------------

TITLE: Thread Scaling Performance Test Command
DESCRIPTION: Example command for testing performance with different numbers of CPU threads.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_3

LANGUAGE: sh
CODE:
```
./llama-bench -n 0 -n 16 -p 64 -t 1,2,4,8,16,32
```

----------------------------------------

TITLE: Compiling BLIS Framework with OpenMP and POSIX Threads
DESCRIPTION: Clones and compiles the BLIS framework with CBLAS compatibility and multi-threading support using OpenMP and POSIX threads.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/BLIS.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
git clone https://github.com/flame/blis
cd blis
./configure --enable-cblas -t openmp,pthreads auto
# will install to /usr/local/ by default.
make -j
```

----------------------------------------

TITLE: Configuring Architecture-Specific Optimizations for GGML
DESCRIPTION: Sets up options to enable or disable various CPU instruction set optimizations for different architectures including x86, ARM, RISC-V, and Loongarch.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_11

LANGUAGE: CMake
CODE:
```
option(GGML_CPU_HBM          "ggml: use memkind for CPU HBM" OFF)
option(GGML_CPU_AARCH64      "ggml: use runtime weight conversion of Q4_0 to Q4_X_X" ON)
option(GGML_CPU_KLEIDIAI     "ggml: use KleidiAI optimized kernels if applicable" OFF)
option(GGML_SSE42            "ggml: enable SSE 4.2"          ${INS_ENB})
option(GGML_AVX              "ggml: enable AVX"              ${INS_ENB})
option(GGML_AVX_VNNI         "ggml: enable AVX-VNNI"         OFF)
option(GGML_AVX2             "ggml: enable AVX2"             ${INS_ENB})
option(GGML_BMI2             "ggml: enable BMI2"             ${INS_ENB})
option(GGML_AVX512           "ggml: enable AVX512F"          OFF)
option(GGML_AVX512_VBMI      "ggml: enable AVX512-VBMI"      OFF)
option(GGML_AVX512_VNNI      "ggml: enable AVX512-VNNI"      OFF)
option(GGML_AVX512_BF16      "ggml: enable AVX512-BF16"      OFF)
if (NOT MSVC)
    # in MSVC F16C and FMA is implied with AVX2/AVX512
    option(GGML_FMA          "ggml: enable FMA"              ${INS_ENB})
    option(GGML_F16C         "ggml: enable F16C"             ${INS_ENB})
    # MSVC does not seem to support AMX
    option(GGML_AMX_TILE     "ggml: enable AMX-TILE"         OFF)
    option(GGML_AMX_INT8     "ggml: enable AMX-INT8"         OFF)
    option(GGML_AMX_BF16     "ggml: enable AMX-BF16"         OFF)
endif()
option(GGML_LASX             "ggml: enable lasx"             ON)
option(GGML_LSX              "ggml: enable lsx"              ON)
option(GGML_RVV              "ggml: enable rvv"              ON)
option(GGML_RV_ZFH           "ggml: enable riscv zfh"        OFF)
option(GGML_VXE              "ggml: enable vxe"              ON)
```

----------------------------------------

TITLE: GGML SYCL Library Configuration
DESCRIPTION: Configures the GGML SYCL backend library, including source files and compiler settings. Sets up Visual Studio solution settings for Windows builds.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-sycl/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: cmake
CODE:
```
ggml_add_backend_library(ggml-sycl
                         ggml-sycl.cpp
                         ../../include/ggml-sycl.h
                        )

file(GLOB   GGML_HEADERS_SYCL "*.hpp")
file(GLOB   GGML_SOURCES_SYCL "*.cpp")
target_sources(ggml-sycl PRIVATE ${GGML_HEADERS_SYCL} ${GGML_SOURCES_SYCL})

if (WIN32)
    if( ${CMAKE_GENERATOR} MATCHES "Visual Studio" AND NOT (${CMAKE_GENERATOR_TOOLSET} MATCHES "Intel C"))
        set_target_properties(ggml-sycl PROPERTIES VS_PLATFORM_TOOLSET "Intel C++ Compiler 2025")
        set(CMAKE_CXX_COMPILER "icx")
        set(CMAKE_CXX_COMPILER_ID "IntelLLVM")
    endif()
endif()
```

----------------------------------------

TITLE: Testing llama.cpp with KleidiAI
DESCRIPTION: Command to test llama.cpp with Arm KleidiAI optimizations enabled.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_24

LANGUAGE: bash
CODE:
```
./build/bin/llama-cli -m PATH_TO_MODEL -p "What is a car?"
```

----------------------------------------

TITLE: Mean Method for Control Vector Generation
DESCRIPTION: Command to generate a control vector using the mean value method instead of Principal Component Analysis (PCA).
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/cvector-generator/README.md#2025-04-22_snippet_3

LANGUAGE: sh
CODE:
```
./cvector-generator -m ./llama-3.Q4_K_M.gguf --method mean
```

----------------------------------------

TITLE: Configuring llama-cvector-generator Target in CMake
DESCRIPTION: This CMake snippet sets up the llama-cvector-generator executable target. It specifies the source files, links required libraries, sets C++17 as the compile standard, and configures installation rules.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/cvector-generator/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(TARGET llama-cvector-generator)
add_executable(${TARGET} cvector-generator.cpp pca.hpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Example of Q4_0 Quantization for CLIP Visual Projector
DESCRIPTION: This example shows how to quantize a CLIP visual projector model using the q4_0 quantization type (represented by the value 2). This quantization method uses 4-bit precision with a single scale value.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llava/README-quantize.md#2025-04-22_snippet_1

LANGUAGE: sh
CODE:
```
./bin/llama-llava-clip-quantize-cli /path/to/ggml-model-f32.gguf /path/to/ggml-model-quantized.gguf 2
```

----------------------------------------

TITLE: Running Tests from a Specific File
DESCRIPTION: Command to run all tests contained in a specific file rather than the entire test suite.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/tests/README.md#2025-04-22_snippet_5

LANGUAGE: shell
CODE:
```
./tests.sh unit/test_chat_completion.py -v -x
```

----------------------------------------

TITLE: Configuring CUDA Feature Flags via Compile Definitions
DESCRIPTION: Sets up conditional compilation flags based on various CUDA feature options, including CUDA graphs, matrix multiplication options, virtual memory management, and data type settings.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/CMakeLists.txt#2025-04-22_snippet_2

LANGUAGE: CMake
CODE:
```
add_compile_definitions(GGML_CUDA_PEER_MAX_BATCH_SIZE=${GGML_CUDA_PEER_MAX_BATCH_SIZE})

if (GGML_CUDA_GRAPHS)
    add_compile_definitions(GGML_CUDA_USE_GRAPHS)
endif()

if (GGML_CUDA_FORCE_MMQ)
    add_compile_definitions(GGML_CUDA_FORCE_MMQ)
endif()

if (GGML_CUDA_FORCE_CUBLAS)
    add_compile_definitions(GGML_CUDA_FORCE_CUBLAS)
endif()

if (GGML_CUDA_NO_VMM)
    add_compile_definitions(GGML_CUDA_NO_VMM)
endif()

if (NOT GGML_CUDA_FA)
    add_compile_definitions(GGML_CUDA_NO_FA)
endif()

if (GGML_CUDA_F16 OR GGML_CUDA_DMMV_F16)
    add_compile_definitions(GGML_CUDA_F16)
endif()

if (GGML_CUDA_NO_PEER_COPY)
    add_compile_definitions(GGML_CUDA_NO_PEER_COPY)
endif()
```

----------------------------------------

TITLE: Running Llama.cpp Server Tests in Debug Mode
DESCRIPTION: Command to run tests with verbose output in real-time, which is useful for debugging issues.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/tests/README.md#2025-04-22_snippet_4

LANGUAGE: shell
CODE:
```
DEBUG=1 ./tests.sh -s -v -x
```

----------------------------------------

TITLE: Executing the SYCL device listing tool
DESCRIPTION: Runs the llama-ls-sycl-device tool to display all available SYCL devices with their capabilities. This helps identify device IDs and specifications before running the main application.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/sycl/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
./build/bin/llama-ls-sycl-device
```

----------------------------------------

TITLE: Configuring CPU Backend Variants and Architecture Specifics
DESCRIPTION: Sets up options for building multiple CPU backend variants and specifying architecture-specific parameters for ARM and PowerPC.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_12

LANGUAGE: CMake
CODE:
```
option(GGML_CPU_ALL_VARIANTS "ggml: build all variants of the CPU backend (requires GGML_BACKEND_DL)" OFF)
set(GGML_CPU_ARM_ARCH        "" CACHE STRING "ggml: CPU architecture for ARM")
set(GGML_CPU_POWERPC_CPUTYPE "" CACHE STRING "ggml: CPU type for PowerPC")
```

----------------------------------------

TITLE: Creating Git Tag for Release
DESCRIPTION: Git command to create an annotated tag for version release
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/gguf-py/README.md#2025-04-22_snippet_4

LANGUAGE: sh
CODE:
```
git tag -a gguf-v1.0.0 -m "Version 1.0 release"
```

----------------------------------------

TITLE: JSON output from llama-bench
DESCRIPTION: Example of JSON output generated by llama-bench. The output is an array of test results with detailed information including hardware specs, model configuration, and performance metrics with sample data points.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_9

LANGUAGE: json
CODE:
```
[
  {
    "build_commit": "3469684",
    "build_number": 1275,
    "cuda": true,
    "metal": false,
    "gpu_blas": true,
    "blas": true,
    "cpu_info": "13th Gen Intel(R) Core(TM) i9-13900K",
    "gpu_info": "NVIDIA GeForce RTX 3090 Ti",
    "model_filename": "models/7B/ggml-model-q4_0.gguf",
    "model_type": "llama 7B mostly Q4_0",
    "model_size": 3825065984,
    "model_n_params": 6738415616,
    "n_batch": 512,
    "n_threads": 16,
    "f16_kv": true,
    "n_gpu_layers": 99,
    "main_gpu": 0,
    "mul_mat_q": true,
    "tensor_split": "0.00",
    "n_prompt": 512,
    "n_gen": 0,
    "test_time": "2023-09-23T12:09:57Z",
    "avg_ns": 212365953,
    "stddev_ns": 985423,
    "avg_ts": 2410.974041,
    "stddev_ts": 11.163766,
    "samples_ns": [ 213837238, 211635853, 212328053, 211329715, 212698907 ],
    "samples_ts": [ 2394.34, 2419.25, 2411.36, 2422.75, 2407.16 ]
  },
  {
    "build_commit": "3469684",
    "build_number": 1275,
    "cuda": true,
    "metal": false,
    "gpu_blas": true,
    "blas": true,
    "cpu_info": "13th Gen Intel(R) Core(TM) i9-13900K",
    "gpu_info": "NVIDIA GeForce RTX 3090 Ti",
    "model_filename": "models/7B/ggml-model-q4_0.gguf",
    "model_type": "llama 7B mostly Q4_0",
    "model_size": 3825065984,
    "model_n_params": 6738415616,
    "n_batch": 512,
    "n_threads": 16,
    "f16_kv": true,
    "n_gpu_layers": 99,
    "main_gpu": 0,
    "mul_mat_q": true,
    "tensor_split": "0.00",
    "n_prompt": 0,
    "n_gen": 128,
    "test_time": "2023-09-23T12:09:59Z",
    "avg_ns": 977425219,
    "stddev_ns": 9268593,
    "avg_ts": 130.965708,
    "stddev_ts": 1.238924,
    "samples_ns": [ 984472709, 974901233, 989474741, 970729355, 967548060 ],
    "samples_ts": [ 130.019, 131.295, 129.362, 131.86, 132.293 ]
  }
]
```

----------------------------------------

TITLE: Converting JSON Schema to GBNF Grammar Using Command-line Tool
DESCRIPTION: Command to convert a JSON schema file to a GBNF grammar using the provided Python script. This allows preprocessing schemas into grammars before inference time.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
examples/json_schema_to_grammar.py name-age-schema.json
```

----------------------------------------

TITLE: Downloading PHI-2 Model for llama.cpp Benchmarking
DESCRIPTION: Example command to download the PHI-2 model using the hf.sh script for benchmarking purposes.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/bench/README.md#2025-04-22_snippet_2

LANGUAGE: shell
CODE:
```
../../../scripts/hf.sh --repo ggml-org/models --file phi-2/ggml-model-q4_0.gguf
```

----------------------------------------

TITLE: Configuring and Building llama-gguf Executable with CMake
DESCRIPTION: This CMake script sets up the llama-gguf executable, specifying its source file, installation rules, library dependencies, and C++ standard requirement. It links against the ggml library and system thread libraries, and requires C++17 support.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(TARGET llama-gguf)
add_executable(${TARGET} gguf.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE ggml ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Configuring llama-parallel Executable Target in CMake
DESCRIPTION: Creates and configures the llama-parallel executable target in CMake. The target is built from parallel.cpp, links against common libraries and llama, and requires C++17 support. It also sets up installation instructions for the binary.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/parallel/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(TARGET llama-parallel)
add_executable(${TARGET} parallel.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Building llama.cpp with SYCL on Windows (Script)
DESCRIPTION: Batch script to build llama.cpp with SYCL support on Windows.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_18

LANGUAGE: sh
CODE:
```
.\examples\sycl\win-build-sycl.bat
```

----------------------------------------

TITLE: Configuring GGML Metal Backend Library in CMake
DESCRIPTION: Sets up the ggml-metal library, including source files and linked libraries.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-metal/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: CMake
CODE:
```
ggml_add_backend_library(ggml-metal
                         ggml-metal.m
                        )

target_link_libraries(ggml-metal PRIVATE
                      ${FOUNDATION_LIBRARY}
                      ${METAL_FRAMEWORK}
                      ${METALKIT_FRAMEWORK}
                      )
```

----------------------------------------

TITLE: Running a Single Test Case
DESCRIPTION: Command to run a specific test case by providing the file path and test name.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/tests/README.md#2025-04-22_snippet_6

LANGUAGE: shell
CODE:
```
./tests.sh unit/test_chat_completion.py::test_invalid_chat_completion_req
```

----------------------------------------

TITLE: Installing k6 with SSE Extension for llama.cpp Benchmarking
DESCRIPTION: Commands to install k6 with the xk6-sse extension required for server benchmarking. Requires golang 1.21 or higher to be installed.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/bench/README.md#2025-04-22_snippet_0

LANGUAGE: shell
CODE:
```
go install go.k6.io/xk6/cmd/xk6@latest
$GOPATH/bin/xk6 build master \
--with github.com/phymbert/xk6-sse
```

----------------------------------------

TITLE: Installing OpenCL Headers and Libraries for Windows 11 Arm64
DESCRIPTION: PowerShell commands to clone and install OpenCL headers and ICD loader for Windows 11 Arm64, required for building llama.cpp with OpenCL support on Snapdragon X Elite devices.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/OPENCL.md#2025-04-22_snippet_4

LANGUAGE: powershell
CODE:
```
mkdir -p ~/dev/llm

cd ~/dev/llm
git clone https://github.com/KhronosGroup/OpenCL-Headers && cd OpenCL-Headers
mkdir build && cd build
cmake .. -G Ninja `
  -DBUILD_TESTING=OFF `
  -DOPENCL_HEADERS_BUILD_TESTING=OFF `
  -DOPENCL_HEADERS_BUILD_CXX_TESTS=OFF `
  -DCMAKE_INSTALL_PREFIX="$HOME/dev/llm/opencl"
cmake --build . --target install

cd ~/dev/llm
git clone https://github.com/KhronosGroup/OpenCL-ICD-Loader && cd OpenCL-ICD-Loader
mkdir build && cd build
cmake .. -G Ninja `
  -DCMAKE_BUILD_TYPE=Release `
  -DCMAKE_PREFIX_PATH="$HOME/dev/llm/opencl" `
  -DCMAKE_INSTALL_PREFIX="$HOME/dev/llm/opencl"
cmake --build . --target install
```

----------------------------------------

TITLE: Single Line Pattern in GBNF
DESCRIPTION: Shows how to define a pattern that matches any characters except newline using negated character ranges.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md#2025-04-22_snippet_1

LANGUAGE: gbnf
CODE:
```
single-line ::= [^\n]+ "\n"
```

----------------------------------------

TITLE: Configuring CMake Build for llama-simple-chat Executable
DESCRIPTION: Sets up CMake build configuration for a simple chat application. Links against the llama library and pthread, requires C++17 support, and configures installation targets.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/simple-chat/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
set(TARGET llama-simple-chat)
add_executable(${TARGET} simple-chat.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Configuring and Compiling Test Binaries
DESCRIPTION: These commands set up the build environment with debug mode enabled and compile the test binaries. It uses CMake with specific flags for debugging, CUDA support, and fatal warnings, followed by a multi-threaded make command.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/debugging-tests.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_CUDA=1 -DLLAMA_FATAL_WARNINGS=ON ..
make -j
```

----------------------------------------

TITLE: Setting up Git-based Build Information for llama.cpp in CMake
DESCRIPTION: Detects Git repository information and configures automatic build information generation. Handles both regular Git repositories and Git submodules by locating the .git directory and index file.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/common/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
if(EXISTS "${CMAKE_CURRENT_SOURCE_DIR}/../.git")
    set(GIT_DIR "${CMAKE_CURRENT_SOURCE_DIR}/../.git")

    # Is git submodule
    if(NOT IS_DIRECTORY "${GIT_DIR}")
        file(READ ${GIT_DIR} REAL_GIT_DIR_LINK)
        string(REGEX REPLACE "gitdir: (.*)\n$" "\\1" REAL_GIT_DIR ${REAL_GIT_DIR_LINK})
        string(FIND "${REAL_GIT_DIR}" "/" SLASH_POS)
        if (SLASH_POS EQUAL 0)
            set(GIT_DIR "${REAL_GIT_DIR}")
        else()
            set(GIT_DIR "${CMAKE_CURRENT_SOURCE_DIR}/../${REAL_GIT_DIR}")
        endif()
    endif()

    if(EXISTS "${GIT_DIR}/index")
        set(GIT_INDEX "${GIT_DIR}/index")
    else()
        message(WARNING "Git index not found in git repository.")
        set(GIT_INDEX "")
    endif()
else()
    message(WARNING "Git repository not found; to enable automatic generation of build info, make sure Git is installed and the project is a Git repository.")
    set(GIT_INDEX "")
endif()
```

----------------------------------------

TITLE: Configuring CMake Build for LLaMA Quantization Stats Tool
DESCRIPTION: Configures CMake build settings for the llama-quantize-stats executable. Sets up target dependencies, links required libraries including llama and build_info, and configures include directories and C++17 standard requirement.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/quantize-stats/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
set(TARGET llama-quantize-stats)
add_executable(${TARGET} quantize-stats.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE llama build_info ${CMAKE_THREAD_LIBS_INIT})
target_include_directories(${TARGET} PRIVATE ../../common)
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Specifying CFFI Library Version Requirement
DESCRIPTION: Defines the dependency on the CFFI package with version 1.16.0 using the compatible release operator (~=). This ensures the installed version will be at least 1.16.0 but less than 1.17.0.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-test-tokenizer-random.txt#2025-04-22_snippet_0

LANGUAGE: plaintext
CODE:
```
cffi~=1.16.0
```

----------------------------------------

TITLE: Adding GGML Kompute Backend Library in CMake
DESCRIPTION: This snippet adds the GGML Kompute backend library to the project, linking it with the necessary dependencies and including the required headers.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-kompute/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: CMake
CODE:
```
ggml_add_backend_library(ggml-kompute
                         ggml-kompute.cpp
                         ../../include/ggml-kompute.h
                        )

target_link_libraries(ggml-kompute PRIVATE ggml-base kompute)
target_include_directories(ggml-kompute PRIVATE ${CMAKE_CURRENT_BINARY_DIR})

add_compile_definitions(VULKAN_HPP_DISPATCH_LOADER_DYNAMIC=1)
```

----------------------------------------

TITLE: Building the Llama.cpp Server
DESCRIPTION: Commands to build the llama-server target using CMake. This must be done before running the tests.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/tests/README.md#2025-04-22_snippet_1

LANGUAGE: shell
CODE:
```
cd ../../..
cmake -B build
cmake --build build --target llama-server
```

----------------------------------------

TITLE: Converting PyTorch Model to GGUF Format for MiniCPM-Llama3-V 2.5
DESCRIPTION: Script commands to convert the PyTorch model to GGUF format and quantize it. This includes extracting the projector, converting the image encoder, and creating a quantized int4 version of the model.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/minicpmv2.5.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
python ./examples/llava/minicpmv-surgery.py -m ../MiniCPM-Llama3-V-2_5
python ./examples/llava/minicpmv-convert-image-encoder-to-gguf.py -m ../MiniCPM-Llama3-V-2_5 --minicpmv-projector ../MiniCPM-Llama3-V-2_5/minicpmv.projector --output-dir ../MiniCPM-Llama3-V-2_5/ --image-mean 0.5 0.5 0.5 --image-std 0.5 0.5 0.5 --minicpmv_version 2
python ./convert_hf_to_gguf.py ../MiniCPM-Llama3-V-2_5/model

# quantize int4 version
./build/bin/llama-quantize ../MiniCPM-Llama3-V-2_5/model/model-8B-F16.gguf ../MiniCPM-Llama3-V-2_5/model/ggml-model-Q4_K_M.gguf Q4_K_M
```

----------------------------------------

TITLE: Detecting Buggy Apple Linker
DESCRIPTION: Checks for a specific version of Apple's ld64 linker that has known bugs and sets a compiler definition to handle it.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_7

LANGUAGE: CMake
CODE:
```
# this version of Apple ld64 is buggy
execute_process(
    COMMAND ${CMAKE_C_COMPILER} ${CMAKE_EXE_LINKER_FLAGS} -Wl,-v
    ERROR_VARIABLE output
    OUTPUT_QUIET
)

if (output MATCHES "dyld-1015\\.7")
    add_compile_definitions(HAVE_BUGGY_APPLE_LINKER)
endif()
```

----------------------------------------

TITLE: Starting Docker Container for MUSA CI
DESCRIPTION: Command to start a Docker container for running MUSA CI with necessary volume mounts for caching models, storing results, and accessing the workspace.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ci/README.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
docker run --privileged -it \
    -v $HOME/llama.cpp/ci-cache:/ci-cache \
    -v $HOME/llama.cpp/ci-results:/ci-results \
    -v $PWD:/ws -w /ws \
    mthreads/musa:rc3.1.1-devel-ubuntu22.04
```

----------------------------------------

TITLE: Building llama.cpp with CANN Support for Ascend NPU
DESCRIPTION: Commands to build llama.cpp with CANN support for Ascend NPU acceleration using CMake.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_21

LANGUAGE: bash
CODE:
```
cmake -B build -DGGML_CANN=on -DCMAKE_BUILD_TYPE=release
cmake --build build --config release
```

----------------------------------------

TITLE: Configuring and Building llama-run Executable with CMake
DESCRIPTION: Sets up the llama-run target by specifying source files, handling optional CURL dependency, configuring installation, linking required libraries, and setting C++17 as the required standard. Includes conditional compilation for CURL support.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/run/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(TARGET llama-run)
add_executable(${TARGET} run.cpp linenoise.cpp/linenoise.cpp)

# TODO: avoid copying this code block from common/CMakeLists.txt
set(LLAMA_RUN_EXTRA_LIBS "")
if (LLAMA_CURL)
    find_package(CURL REQUIRED)
    target_compile_definitions(${TARGET} PUBLIC LLAMA_USE_CURL)
    include_directories(${CURL_INCLUDE_DIRS})
    find_library(CURL_LIBRARY curl REQUIRED)
    set(LLAMA_RUN_EXTRA_LIBS ${LLAMA_RUN_EXTRA_LIBS} ${CURL_LIBRARY})
endif ()

install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT} ${LLAMA_RUN_EXTRA_LIBS})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Building GGML Library and Components in CMake
DESCRIPTION: This section builds the main GGML library and conditionally adds tests and examples based on build options. It includes necessary subdirectories for source files, tests, and examples.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_17

LANGUAGE: CMake
CODE:
```
#
# build the library
#

add_subdirectory(src)

#
# tests and examples
#

if (GGML_BUILD_TESTS)
    enable_testing()
    add_subdirectory(tests)
endif ()

if (GGML_BUILD_EXAMPLES)
    add_subdirectory(examples)
endif ()
```

----------------------------------------

TITLE: Configuring and Building llama-gen-docs Documentation Generator with CMake
DESCRIPTION: This CMake snippet defines the build configuration for the llama-gen-docs executable. It sets the target name, creates the executable from gen-docs.cpp, configures installation, links necessary libraries including common, llama, and threading libraries, and ensures C++17 standard compliance.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gen-docs/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(TARGET llama-gen-docs)
add_executable(${TARGET} gen-docs.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Configuring llama-lookahead Target in CMake
DESCRIPTION: This CMake configuration sets up the llama-lookahead executable target. It specifies lookahead.cpp as the source file, sets up installation rules, links required libraries (common, llama, and threading libraries), and specifies C++17 as the required standard.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/lookahead/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(TARGET llama-lookahead)
add_executable(${TARGET} lookahead.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Starting RPC Server with CUDA Backend
DESCRIPTION: Command to start the RPC server using the CUDA backend. It specifies the port number for the server to listen on.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/rpc/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
$ bin/rpc-server -p 50052
```

----------------------------------------

TITLE: Configuring RPC Backend in CMake
DESCRIPTION: Sets up the RPC backend library using CMake. Includes status message, library addition, and Windows-specific socket library linking.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-rpc/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
message(STATUS "Using RPC backend")

ggml_add_backend_library(ggml-rpc
                         ggml-rpc.cpp
                        )

if (WIN32)
    target_link_libraries(ggml-rpc PRIVATE ws2_32)
endif()
```

----------------------------------------

TITLE: Generating Visual Studio Solution for llama.cpp with SYCL (Intel C++ Compiler)
DESCRIPTION: CMake command to generate a Visual Studio solution for llama.cpp with SYCL support using the Intel C++ Compiler.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_22

LANGUAGE: powershell
CODE:
```
cmake -B build -G "Visual Studio 17 2022" -T "Intel C++ Compiler 2025" -A x64 -DGGML_SYCL=ON -DCMAKE_BUILD_TYPE=Release
```

----------------------------------------

TITLE: Downloading Granite Vision Model
DESCRIPTION: Clone the Granite Vision model repository and set environment variable.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/granitevision.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
$ git clone https://huggingface.co/ibm-granite/granite-vision-3.2-2b
$ export GRANITE_MODEL=./granite-vision-3.2-2b
```

----------------------------------------

TITLE: Setting Native Compilation Default Based on Environment
DESCRIPTION: Determines whether to enable native optimization based on cross-compilation status and source date epoch environment variable.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_5

LANGUAGE: CMake
CODE:
```
if (CMAKE_CROSSCOMPILING OR DEFINED ENV{SOURCE_DATE_EPOCH})
    message(STATUS "Setting GGML_NATIVE_DEFAULT to OFF")
    set(GGML_NATIVE_DEFAULT OFF)
else()
    set(GGML_NATIVE_DEFAULT ON)
endif()
```

----------------------------------------

TITLE: Finalizing Common Library Configuration in llama.cpp CMake
DESCRIPTION: Completes the common module configuration by setting include directories, C++ standard, and linking dependencies. This connects the common module with the main llama library and necessary threading support.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/common/CMakeLists.txt#2025-04-22_snippet_5

LANGUAGE: CMake
CODE:
```
target_include_directories(${TARGET} PUBLIC .)
target_compile_features   (${TARGET} PUBLIC cxx_std_17)
target_link_libraries     (${TARGET} PRIVATE ${LLAMA_COMMON_EXTRA_LIBS} PUBLIC llama Threads::Threads)
```

----------------------------------------

TITLE: Configuring llama-perplexity Target in CMake
DESCRIPTION: This CMake code snippet sets up the llama-perplexity executable target. It defines the target, adds the executable, configures installation, links required libraries, and sets the C++ standard to C++17.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/perplexity/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(TARGET llama-perplexity)
add_executable(${TARGET} perplexity.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Building simple-cmake-pkg Example Using llama.cpp Package
DESCRIPTION: Commands to build the simple-cmake-pkg example by specifying the location of the CMake package with CMAKE_PREFIX_PATH. This demonstrates how to use find_package() to locate the installed llama.cpp package.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/simple-cmake-pkg/README.md#2025-04-22_snippet_1

LANGUAGE: shell
CODE:
```
cd examples/simple-cmake-pkg
cmake -S . -B build -DCMAKE_PREFIX_PATH=../../inst/lib/cmake
cmake --build build
```

----------------------------------------

TITLE: Including Common CMake Modules for llama.cpp
DESCRIPTION: Includes the common CMake module and defines compilation constants. Sets up the scheduler maximum copies definition.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
include(CheckCXXCompilerFlag)
include("../cmake/common.cmake")

add_compile_definitions(GGML_SCHED_MAX_COPIES=${GGML_SCHED_MAX_COPIES})
```

----------------------------------------

TITLE: Accessing Command Line Arguments in Node.js
DESCRIPTION: Demonstrates how to access command line arguments in a Node.js program using the process.argv array. The array contains the executable path, script path, and any additional arguments passed to the script.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/prompts/chat.txt#2025-04-22_snippet_0

LANGUAGE: JavaScript
CODE:
```
argv[0] is the path to the Node. js executable.
argv[1] is the path to the script file.
argv[2] is the first argument passed to the script.
argv[3] is the second argument passed to the script and so on.
```

----------------------------------------

TITLE: CANN Library and Include Configuration
DESCRIPTION: Sets up CANN library dependencies, include directories, and platform compatibility checks. Configures compilation options based on SOC type.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cann/CMakeLists.txt#2025-04-22_snippet_2

LANGUAGE: cmake
CODE:
```
if (CANN_INSTALL_DIR)
    if (NOT UNIX)
        message(FATAL_ERROR "CANN: CANN toolkit supports unix but not ${CMAKE_SYSTEM_NAME}")
    endif()

    if (CMAKE_SYSTEM_PROCESSOR STREQUAL "aarch64")
    elseif (CMAKE_SYSTEM_PROCESSOR STREQUAL "x86_64" OR CMAKE_SYSTEM_PROCESSOR STREQUAL "amd64")
    else()
        message(FATAL_ERROR "CANN: CANN toolkit supports x86-64 and arm64 but not ${CMAKE_SYSTEM_PROCESSOR}")
    endif()

    set(CANN_INCLUDE_DIRS
        ${CANN_INSTALL_DIR}/include
        ${CANN_INSTALL_DIR}/include/aclnn
        ${CANN_INSTALL_DIR}/acllib/include
    )

    list(APPEND CANN_LIBRARIES
        ascendcl
        nnopbase
        opapi
        acl_op_compiler
    )

    file(GLOB GGML_SOURCES_CANN "*.cpp")

    ggml_add_backend_library(ggml-cann ${GGML_SOURCES_CANN})
    target_link_libraries(ggml-cann PRIVATE ${CANN_LIBRARIES})
    target_include_directories(ggml-cann PRIVATE ${CANN_INCLUDE_DIRS})
    target_link_directories(ggml-cann PRIVATE ${CANN_INSTALL_DIR}/lib64)

    target_compile_definitions(ggml-cann PRIVATE "-D${SOC_TYPE_COMPILE_OPTION}")

    message(STATUS "CANN: CANN_INCLUDE_DIRS =  ${CANN_INCLUDE_DIRS}")
    message(STATUS "CANN: CANN_LIBRARIES =  ${CANN_LIBRARIES}")
else()
    message(FATAL_ERROR "CANN: Can't find CANN_INSTALL_DIR, did you forget to source set_var.sh?")
endif()
```

----------------------------------------

TITLE: Configuring Platform-Specific Tests for llama.cpp
DESCRIPTION: Sets up conditional test compilation based on the platform. Includes grammar-related tests, tokenizer tests for BPE and SPM models, and other specialized tests. Some tests are disabled on Windows or specific architectures due to dependencies or internal function usage.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/tests/CMakeLists.txt#2025-04-22_snippet_3

LANGUAGE: CMake
CODE:
```
if (LLAMA_LLGUIDANCE)
    llama_target_and_test(test-grammar-llguidance.cpp ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-llama-bpe.gguf)
endif ()

if (NOT WIN32)
    # these tests are disabled on Windows because they use internal functions not exported with LLAMA_API
    llama_target_and_test(test-sampling.cpp)
    llama_target_and_test(test-grammar-parser.cpp)
    llama_target_and_test(test-grammar-integration.cpp)
    llama_target_and_test(test-llama-grammar.cpp)
    llama_target_and_test(test-chat.cpp)
    # TODO: disabled on loongarch64 because the ggml-ci node lacks Python 3.8
    if (NOT ${CMAKE_SYSTEM_PROCESSOR} MATCHES "loongarch64")
        llama_target_and_test(test-json-schema-to-grammar.cpp   WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}/..)
        target_include_directories(test-json-schema-to-grammar PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/../examples/server)
    endif()


    # build test-tokenizer-1-bpe target once and add many tests
    add_executable(test-tokenizer-1-bpe test-tokenizer-1-bpe.cpp)
    target_link_libraries(test-tokenizer-1-bpe PRIVATE common)
    install(TARGETS test-tokenizer-1-bpe RUNTIME)

    # TODO: disabled due to slowness
    #llama_test(test-tokenizer-1-bpe NAME test-tokenizer-1-aquila    ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-aquila.gguf)
    #llama_test(test-tokenizer-1-bpe NAME test-tokenizer-1-falcon    ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-falcon.gguf)
    #llama_test(test-tokenizer-1-bpe NAME test-tokenizer-1-gpt-2     ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-gpt-2.gguf)
    #llama_test(test-tokenizer-1-bpe NAME test-tokenizer-1-gpt-neox  ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-gpt-neox.gguf)
    #llama_test(test-tokenizer-1-bpe NAME test-tokenizer-1-llama-bpe ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-llama-bpe.gguf --ignore-merges)
    #llama_test(test-tokenizer-1-bpe NAME test-tokenizer-1-mpt       ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-mpt.gguf)
    #llama_test(test-tokenizer-1-bpe NAME test-tokenizer-1-refact    ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-refact.gguf)
    #llama_test(test-tokenizer-1-bpe NAME test-tokenizer-1-starcoder ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-starcoder.gguf)

    # build test-tokenizer-1-spm target once and add many tests
    add_executable(test-tokenizer-1-spm test-tokenizer-1-spm.cpp)
    target_link_libraries(test-tokenizer-1-spm PRIVATE common)
    install(TARGETS test-tokenizer-1-spm RUNTIME)

    llama_test(test-tokenizer-1-spm  NAME test-tokenizer-1-llama-spm ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-llama-spm.gguf)
    #llama_test(test-tokenizer-1-spm  NAME test-tokenizer-1-baichuan  ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-baichuan.gguf)

    # llama_target_and_test(test-double-float.cpp) # SLOW
endif()
```

----------------------------------------

TITLE: Verifying SYCL Installation and Environment
DESCRIPTION: Commands to source oneAPI environment variables and list available SYCL devices.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_5

LANGUAGE: sh
CODE:
```
source /opt/intel/oneapi/setvars.sh
sycl-ls
```

----------------------------------------

TITLE: Configuring llama-batched Executable in CMake
DESCRIPTION: Sets up the llama-batched executable target in CMake. It specifies the source file, configures installation, links required libraries including common, llama, and threading libraries, and sets the C++ standard to C++17.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/batched/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(TARGET llama-batched)
add_executable(${TARGET} batched.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Building and Running llama-batched-swift
DESCRIPTION: Commands to compile and execute the Swift version of the batched llama example. The program takes a model path as required argument, with optional prompt and parallel processing parameters.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/batched.swift/README.md#2025-04-22_snippet_0

LANGUAGE: shell
CODE:
```
make
```

LANGUAGE: shell
CODE:
```
./llama-batched-swift MODEL_PATH [PROMPT] [PARALLEL]
```

----------------------------------------

TITLE: Configuring LLaMA Lookup Tools in CMake
DESCRIPTION: This CMake configuration defines four targets related to lookup functionality in the llama.cpp project. Each target is set up with the same dependencies (common, llama, and thread libraries) and requires C++17 support. The targets are also configured for installation.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/lookup/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(TARGET llama-lookup)
add_executable(${TARGET} lookup.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)

set(TARGET llama-lookup-create)
add_executable(${TARGET} lookup-create.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)

set(TARGET llama-lookup-merge)
add_executable(${TARGET} lookup-merge.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)

set(TARGET llama-lookup-stats)
add_executable(${TARGET} lookup-stats.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Configuring GGML CPU Backend Sources and Compilation
DESCRIPTION: Sets up source files, compiler flags, and build configurations for the GGML CPU backend. Handles special cases for KLEIDIAI sources, architecture-specific flags, and feature detection compilation. Includes support for dynamic loading and Emscripten compilation.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/CMakeLists.txt#2025-04-22_snippet_2

LANGUAGE: cmake
CODE:
```
list(APPEND GGML_KLEIDIAI_SOURCES ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p1vlx4_qsi4c32p4vlx4_1vlx4vl_sme2_mopa.c)
list(APPEND GGML_KLEIDIAI_SOURCES ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4vlx4_1x4vl_sme2_sdot.c)
set(PRIVATE_ARCH_FLAGS "${PRIVATE_ARCH_FLAGS}+sve+sve2")

set_source_files_properties(${GGML_KLEIDIAI_SOURCES} PROPERTIES COMPILE_OPTIONS "${PRIVATE_ARCH_FLAGS}")
list(APPEND GGML_CPU_SOURCES ${GGML_KLEIDIAI_SOURCES})

message(STATUS "Adding CPU backend variant ${GGML_CPU_NAME}: ${ARCH_FLAGS} ${ARCH_DEFINITIONS}")
target_sources(${GGML_CPU_NAME} PRIVATE ${GGML_CPU_SOURCES})
target_compile_options(${GGML_CPU_NAME} PRIVATE ${ARCH_FLAGS})
target_compile_definitions(${GGML_CPU_NAME} PRIVATE ${ARCH_DEFINITIONS})

if (GGML_BACKEND_DL)
    if (GGML_NATIVE)
        message(FATAL_ERROR "GGML_NATIVE is not compatible with GGML_BACKEND_DL, consider using GGML_CPU_ALL_VARIANTS")
    endif()

    set(GGML_CPU_FEATS_NAME ${GGML_CPU_NAME}-feats)
    add_library(${GGML_CPU_FEATS_NAME} OBJECT ggml-cpu/cpu-feats-x86.cpp)
    target_include_directories(${GGML_CPU_FEATS_NAME} PRIVATE . .. ../include)
    target_compile_definitions(${GGML_CPU_FEATS_NAME} PRIVATE ${ARCH_DEFINITIONS})
    target_compile_definitions(${GGML_CPU_FEATS_NAME} PRIVATE GGML_BACKEND_DL GGML_BACKEND_BUILD GGML_BACKEND_SHARED)
    set_target_properties(${GGML_CPU_FEATS_NAME} PROPERTIES POSITION_INDEPENDENT_CODE ON)
    target_link_libraries(${GGML_CPU_NAME} PRIVATE ${GGML_CPU_FEATS_NAME})
endif()

if (EMSCRIPTEN)
    set_target_properties(${GGML_CPU_NAME} PROPERTIES COMPILE_FLAGS "-msimd128")
endif()
```

----------------------------------------

TITLE: Enabling Fatal Warnings Across Different Compilers
DESCRIPTION: Configures compiler flags to treat warnings as errors when GGML_FATAL_WARNINGS is enabled. Works with GCC, Clang, and MSVC.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_3

LANGUAGE: CMake
CODE:
```
if (GGML_FATAL_WARNINGS)
    if (CMAKE_CXX_COMPILER_ID MATCHES "GNU" OR CMAKE_CXX_COMPILER_ID MATCHES "Clang")
        list(APPEND C_FLAGS   -Werror)
        list(APPEND CXX_FLAGS -Werror)
    elseif (CMAKE_CXX_COMPILER_ID STREQUAL "MSVC")
        add_compile_options(/WX)
    endif()
endif()
```

----------------------------------------

TITLE: Cloning LLaVA 1.6 Model
DESCRIPTION: Git command to clone the LLaVA 1.6 model repository from Hugging Face.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_6

LANGUAGE: sh
CODE:
```
git clone https://huggingface.co/liuhaotian/llava-v1.6-vicuna-7b
```

----------------------------------------

TITLE: CANN Installation Directory Detection
DESCRIPTION: Checks for CANN installation directory using environment variables CANN_INSTALL_DIR or ASCEND_TOOLKIT_HOME.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cann/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
if ("cann${CANN_INSTALL_DIR}" STREQUAL "cann" AND DEFINED ENV{ASCEND_TOOLKIT_HOME})
    set(CANN_INSTALL_DIR $ENV{ASCEND_TOOLKIT_HOME})
    message(STATUS "CANN: updated CANN_INSTALL_DIR from ASCEND_TOOLKIT_HOME=$ENV{ASCEND_TOOLKIT_HOME}")
endif()
```

----------------------------------------

TITLE: Configuring and Building llama-tts Target in CMake
DESCRIPTION: Sets up a CMake target for the text-to-speech (llama-tts) executable. It defines the target name, specifies the source file, configures installation, links required libraries including llama and common components, and sets C++17 as the required standard.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(TARGET llama-tts)
add_executable(${TARGET} tts.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE llama common ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Running Cross-compiled llama.cpp on Android
DESCRIPTION: Commands to execute the cross-compiled llama-simple binary on the Android device with the proper library path and parameters.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/android.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
$ cd /data/local/tmp/llama.cpp
$ LD_LIBRARY_PATH=lib ./bin/llama-simple -m {model}.gguf -c {context-size} -p "{your-prompt}"
```

----------------------------------------

TITLE: Configuring CMake Build for llama-batched-bench
DESCRIPTION: Defines and configures a CMake target for building a batched benchmark executable. Sets up the target with required dependencies including common library, llama library, and threading libraries. Specifies C++17 as the required standard and configures installation rules.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/batched-bench/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
set(TARGET llama-batched-bench)
add_executable(${TARGET} batched-bench.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Installing NVIDIA Driver Libraries
DESCRIPTION: Commands to install NVIDIA driver libraries and update RPM database
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CUDA-FEDORA.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
sudo dnf install nvidia-driver-cuda nvidia-driver-libs nvidia-driver-cuda-libs nvidia-persistenced
sudo dnf download --destdir=/tmp/nvidia-driver-libs --resolve --arch x86_64 nvidia-driver-cuda nvidia-driver-libs nvidia-driver-cuda-libs nvidia-persistenced
sudo rpm --install --verbose --hash --justdb /tmp/nvidia-driver-libs/*
```

----------------------------------------

TITLE: Configuring cURL Integration in llama.cpp CMake
DESCRIPTION: Adds cURL support for downloading models if LLAMA_CURL is enabled. This conditional block finds the cURL package, adds necessary compiler definitions, and links against the cURL library.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/common/CMakeLists.txt#2025-04-22_snippet_3

LANGUAGE: CMake
CODE:
```
# Use curl to download model url
if (LLAMA_CURL)
    find_package(CURL)
    if (NOT CURL_FOUND)
        message(FATAL_ERROR "Could NOT find CURL. Hint: to disable this feature, set -DLLAMA_CURL=OFF")
    endif()
    target_compile_definitions(${TARGET} PUBLIC LLAMA_USE_CURL)
    include_directories(${CURL_INCLUDE_DIRS})
    find_library(CURL_LIBRARY curl REQUIRED)
    set(LLAMA_COMMON_EXTRA_LIBS ${LLAMA_COMMON_EXTRA_LIBS} ${CURL_LIBRARY})
endif ()
```

----------------------------------------

TITLE: Setting POSIX Conformance Definitions
DESCRIPTION: Configures POSIX conformance levels and system-specific extensions for various operating systems to ensure availability of required functions.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_9

LANGUAGE: CMake
CODE:
```
#
# POSIX conformance
#

# clock_gettime came in POSIX.1b (1993)
# CLOCK_MONOTONIC came in POSIX.1-2001 / SUSv3 as optional
# posix_memalign came in POSIX.1-2001 / SUSv3
# M_PI is an XSI extension since POSIX.1-2001 / SUSv3, came in XPG1 (1985)

# Somehow in OpenBSD whenever POSIX conformance is specified
# some string functions rely on locale_t availability,
# which was introduced in POSIX.1-2008, forcing us to go higher
if (CMAKE_SYSTEM_NAME MATCHES "OpenBSD")
    add_compile_definitions(_XOPEN_SOURCE=700)
else()
    add_compile_definitions(_XOPEN_SOURCE=600)
endif()

# Data types, macros and functions related to controlling CPU affinity and
# some memory allocation are available on Linux through GNU extensions in libc
if (CMAKE_SYSTEM_NAME MATCHES "Linux" OR CMAKE_SYSTEM_NAME MATCHES "Android")
    add_compile_definitions(_GNU_SOURCE)
endif()

# RLIMIT_MEMLOCK came in BSD, is not specified in POSIX.1,
# and on macOS its availability depends on enabling Darwin extensions
# similarly on DragonFly, enabling BSD extensions is necessary
if (
    CMAKE_SYSTEM_NAME MATCHES "Darwin" OR
    CMAKE_SYSTEM_NAME MATCHES "iOS"    OR
    CMAKE_SYSTEM_NAME MATCHES "tvOS"   OR
    CMAKE_SYSTEM_NAME MATCHES "DragonFly"
)
    add_compile_definitions(_DARWIN_C_SOURCE)
endif()

# alloca is a non-standard interface that is not visible on BSDs when
# POSIX conformance is specified, but not all of them provide a clean way
# to enable it in such cases
if (CMAKE_SYSTEM_NAME MATCHES "FreeBSD")
    add_compile_definitions(__BSD_VISIBLE)
endif()
if (CMAKE_SYSTEM_NAME MATCHES "NetBSD")
    add_compile_definitions(_NETBSD_SOURCE)
endif()
if (CMAKE_SYSTEM_NAME MATCHES "OpenBSD")
    add_compile_definitions(_BSD_SOURCE)
endif()

if (WIN32)
    add_compile_definitions(_CRT_SECURE_NO_WARNINGS)
endif()
```

----------------------------------------

TITLE: Installing Dependencies in Termux for Android
DESCRIPTION: Commands to update the Termux environment and install necessary build dependencies for llama.cpp on Android.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/android.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
$ apt update && apt upgrade -y
$ apt install git cmake
```

----------------------------------------

TITLE: Configuring Sanitizer Options for GGML
DESCRIPTION: Sets up options for enabling various sanitizers (thread, address, undefined behavior) during compilation.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_9

LANGUAGE: CMake
CODE:
```
# sanitizers
option(GGML_SANITIZE_THREAD    "ggml: enable thread sanitizer"    OFF)
option(GGML_SANITIZE_ADDRESS   "ggml: enable address sanitizer"   OFF)
option(GGML_SANITIZE_UNDEFINED "ggml: enable undefined sanitizer" OFF)
```

----------------------------------------

TITLE: Configuring CMake Build for Speculative Simple Target
DESCRIPTION: Sets up a CMake target for the speculative-simple executable, linking it with the common and llama libraries. Configures C++17 as the required standard and sets up installation rules.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/speculative-simple/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
set(TARGET llama-speculative-simple)
add_executable(${TARGET} speculative-simple.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Running k6 Benchmark with Custom Environment Variables
DESCRIPTION: Example of running the benchmark with custom environment variables to override default settings like number of prompts.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/bench/README.md#2025-04-22_snippet_5

LANGUAGE: shell
CODE:
```
SERVER_BENCH_N_PROMPTS=500 k6 run script.js --duration 10m --iterations 500 --vus 8
```

----------------------------------------

TITLE: Configuring llama-retrieval target in CMake
DESCRIPTION: Sets up the llama-retrieval executable target with proper linking, installation configuration, and compiler feature requirements. The target depends on common and llama libraries, along with platform-specific threading libraries.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/retrieval/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
set(TARGET llama-retrieval)
add_executable(${TARGET} retrieval.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Building llama.cpp with Debug Configuration
DESCRIPTION: Commands to build llama.cpp in debug mode using CMake. This includes examples for both single-config and multi-config generators.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

LANGUAGE: bash
CODE:
```
cmake -B build -G "Xcode"
cmake --build build --config Debug
```

----------------------------------------

TITLE: Setting Up Visual Encoder Directory
DESCRIPTION: Create directory and copy required files for visual encoder.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/granitevision.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
$ ENCODER_PATH=$PWD/visual_encoder
$ mkdir $ENCODER_PATH

$ cp $GRANITE_MODEL/llava.clip $ENCODER_PATH/pytorch_model.bin
$ cp $GRANITE_MODEL/llava.projector $ENCODER_PATH/
```

----------------------------------------

TITLE: Finding Vulkan Package and GLSLC Compiler in CMake
DESCRIPTION: This snippet locates the Vulkan package and the GLSLC shader compiler. It's essential for compiling shaders in the project.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-kompute/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
find_package(Vulkan COMPONENTS glslc REQUIRED)
find_program(glslc_executable NAMES glslc HINTS Vulkan::glslc)

if (NOT glslc_executable)
    message(FATAL_ERROR "glslc not found")
endif()
```

----------------------------------------

TITLE: Collecting CUDA Source Files for Compilation
DESCRIPTION: Gathers CUDA source files for compilation, including header files, core CUDA files, and specialized template instances based on configuration options.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: CMake
CODE:
```
file(GLOB   GGML_HEADERS_CUDA "*.cuh")
list(APPEND GGML_HEADERS_CUDA "../../include/ggml-cuda.h")

file(GLOB   GGML_SOURCES_CUDA "*.cu")
file(GLOB   SRCS "template-instances/fattn-mma*.cu")
list(APPEND GGML_SOURCES_CUDA ${SRCS})
file(GLOB   SRCS "template-instances/mmq*.cu")
list(APPEND GGML_SOURCES_CUDA ${SRCS})

if (GGML_CUDA_FA_ALL_QUANTS)
    file(GLOB   SRCS "template-instances/fattn-vec*.cu")
    list(APPEND GGML_SOURCES_CUDA ${SRCS})
    add_compile_definitions(GGML_CUDA_FA_ALL_QUANTS)
else()
    file(GLOB   SRCS "template-instances/fattn-vec*q4_0-q4_0.cu")
    list(APPEND GGML_SOURCES_CUDA ${SRCS})
    file(GLOB   SRCS "template-instances/fattn-vec*q8_0-q8_0.cu")
    list(APPEND GGML_SOURCES_CUDA ${SRCS})
    file(GLOB   SRCS "template-instances/fattn-vec*f16-f16.cu")
    list(APPEND GGML_SOURCES_CUDA ${SRCS})
endif()
```

----------------------------------------

TITLE: Enum Definition Example for Vocab Types in C++
DESCRIPTION: Example of proper enum naming convention where enum values are in uppercase and prefixed with the enum name. This demonstrates the proper pattern for defining enumerations in the llama.cpp project.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/CONTRIBUTING.md#2025-04-22_snippet_0

LANGUAGE: cpp
CODE:
```
enum llama_vocab_type {
    LLAMA_VOCAB_TYPE_NONE = 0,
    LLAMA_VOCAB_TYPE_SPM  = 1,
    LLAMA_VOCAB_TYPE_BPE  = 2,
    LLAMA_VOCAB_TYPE_WPM  = 3,
    LLAMA_VOCAB_TYPE_UGM  = 4,
    LLAMA_VOCAB_TYPE_RWKV = 5,
};
```

----------------------------------------

TITLE: Configuring Debug and Warning Options for GGML
DESCRIPTION: Sets up debugging options including compiler warnings and profiling.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_8

LANGUAGE: CMake
CODE:
```
# debug
option(GGML_ALL_WARNINGS           "ggml: enable all compiler warnings"                   ON)
option(GGML_ALL_WARNINGS_3RD_PARTY "ggml: enable all compiler warnings in 3rd party libs" OFF)
option(GGML_GPROF                  "ggml: enable gprof"                                   OFF)

# build
option(GGML_FATAL_WARNINGS    "ggml: enable -Werror flag"    OFF)
```

----------------------------------------

TITLE: Configuring llama-save-load-state Executable in CMake
DESCRIPTION: This CMake code snippet sets up the llama-save-load-state executable. It defines the target, adds the executable, sets up installation, links required libraries, and specifies C++17 as the compilation standard.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/save-load-state/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(TARGET llama-save-load-state)
add_executable(${TARGET} save-load-state.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Setting Ascend Environment Variables
DESCRIPTION: Commands to set up Ascend environment variables in the shell configuration
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CANN.md#2025-04-22_snippet_5

LANGUAGE: shell
CODE:
```
echo "source ~/Ascend/ascend-toolkit/set_env.sh" >> ~/.bashrc
source ~/.bashrc
```

----------------------------------------

TITLE: Configuring llama2c to GGML Converter Build
DESCRIPTION: CMake configuration that sets up the build process for a converter utility. It creates an executable target that links against common and llama libraries, requires C++17 support, and sets up installation rules.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/convert-llama2c-to-ggml/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
set(TARGET llama-convert-llama2c-to-ggml)
add_executable(${TARGET} convert-llama2c-to-ggml.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Setting Up ggml-blas Library and Vendor-Specific Configurations in CMake
DESCRIPTION: This snippet handles the setup of the ggml-blas library and applies vendor-specific configurations. It includes special handling for Apple Accelerate, OpenBLAS, BLIS, ATLAS, FlexiBLAS, Intel MKL, and NVHPC.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-blas/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: CMake
CODE:
```
if (BLAS_FOUND)
    message(STATUS "BLAS found, Libraries: ${BLAS_LIBRARIES}")

    ggml_add_backend_library(ggml-blas
                             ggml-blas.cpp
                            )

    if (${GGML_BLAS_VENDOR} MATCHES "Apple")
        add_compile_definitions(ACCELERATE_NEW_LAPACK)
        add_compile_definitions(ACCELERATE_LAPACK_ILP64)
        add_compile_definitions(GGML_BLAS_USE_ACCELERATE)
    elseif ("${BLAS_INCLUDE_DIRS}" STREQUAL "")
        # BLAS_INCLUDE_DIRS is missing in FindBLAS.cmake.
        # see https://gitlab.kitware.com/cmake/cmake/-/issues/20268
        find_package(PkgConfig REQUIRED)
        if (${GGML_BLAS_VENDOR} MATCHES "Generic")
            pkg_check_modules(DepBLAS blas)
        elseif (${GGML_BLAS_VENDOR} MATCHES "OpenBLAS")
            # As of openblas v0.3.22, the 64-bit is named openblas64.pc
            pkg_check_modules(DepBLAS openblas64)
            if (NOT DepBLAS_FOUND)
                pkg_check_modules(DepBLAS openblas)
            endif()
        elseif (${GGML_BLAS_VENDOR} MATCHES "FLAME")
            add_compile_definitions(GGML_BLAS_USE_BLIS)
            pkg_check_modules(DepBLAS blis)
        elseif (${GGML_BLAS_VENDOR} MATCHES "ATLAS")
            pkg_check_modules(DepBLAS blas-atlas)
        elseif (${GGML_BLAS_VENDOR} MATCHES "FlexiBLAS")
            pkg_check_modules(DepBLAS flexiblas_api)
        elseif (${GGML_BLAS_VENDOR} MATCHES "Intel")
            add_compile_definitions(GGML_BLAS_USE_MKL)
            # all Intel* libraries share the same include path
            pkg_check_modules(DepBLAS mkl-sdl)
        elseif (${GGML_BLAS_VENDOR} MATCHES "NVHPC")
            # this doesn't provide pkg-config
            # suggest to assign BLAS_INCLUDE_DIRS on your own
            if ("${NVHPC_VERSION}" STREQUAL "")
                message(WARNING "Better to set NVHPC_VERSION")
            else()
                set(DepBLAS_FOUND ON)
                set(DepBLAS_INCLUDE_DIRS "/opt/nvidia/hpc_sdk/${CMAKE_SYSTEM_NAME}_${CMAKE_SYSTEM_PROCESSOR}/${NVHPC_VERSION}/math_libs/include")
            endif()
        endif()
        if (DepBLAS_FOUND)
            set(BLAS_INCLUDE_DIRS ${DepBLAS_INCLUDE_DIRS})
        else()
            message(WARNING "BLAS_INCLUDE_DIRS neither been provided nor been automatically"
            " detected by pkgconfig, trying to find cblas.h from possible paths...")
            find_path(BLAS_INCLUDE_DIRS
                NAMES cblas.h
                HINTS
                    /usr/include
                    /usr/local/include
                    /usr/include/openblas
                    /opt/homebrew/opt/openblas/include
                    /usr/local/opt/openblas/include
                    /usr/include/x86_64-linux-gnu/openblas/include
            )
        endif()
    endif()

    message(STATUS "BLAS found, Includes: ${BLAS_INCLUDE_DIRS}")

    target_compile_options(ggml-blas PRIVATE ${BLAS_LINKER_FLAGS})

    if (${BLAS_INCLUDE_DIRS} MATCHES "mkl" AND (${GGML_BLAS_VENDOR} MATCHES "Generic" OR ${GGML_BLAS_VENDOR} MATCHES "Intel"))
        add_compile_definitions(GGML_BLAS_USE_MKL)
    endif()

    target_link_libraries     (ggml-blas PRIVATE ${BLAS_LIBRARIES})
    target_include_directories(ggml-blas PRIVATE ${BLAS_INCLUDE_DIRS})
else()
    message(ERROR "BLAS not found, please refer to "
                  "https://cmake.org/cmake/help/latest/module/FindBLAS.html#blas-lapack-vendors"
                  " to set correct GGML_BLAS_VENDOR")
endif()
```

----------------------------------------

TITLE: Configuring Kompute and Compiling Shaders in CMake
DESCRIPTION: This section checks for the presence of Kompute, sets up its configuration, and compiles a list of shaders using the previously defined compile_shader function. It also creates custom targets for generated shaders and ensures proper build order.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-kompute/CMakeLists.txt#2025-04-22_snippet_3

LANGUAGE: CMake
CODE:
```
if (EXISTS "${CMAKE_CURRENT_SOURCE_DIR}/kompute/CMakeLists.txt")
    message(STATUS "Kompute found")
    set(KOMPUTE_OPT_LOG_LEVEL Error CACHE STRING "Kompute log level")
    add_subdirectory(kompute)

    # Compile our shaders
    compile_shader(SOURCES
        kompute-shaders/op_scale.comp
        kompute-shaders/op_scale_8.comp
        kompute-shaders/op_add.comp
        kompute-shaders/op_addrow.comp
        kompute-shaders/op_mul.comp
        kompute-shaders/op_silu.comp
        kompute-shaders/op_relu.comp
        kompute-shaders/op_gelu.comp
        kompute-shaders/op_softmax.comp
        kompute-shaders/op_norm.comp
        kompute-shaders/op_rmsnorm.comp
        kompute-shaders/op_diagmask.comp
        kompute-shaders/op_mul_mat_mat_f32.comp
        kompute-shaders/op_mul_mat_f16.comp
        kompute-shaders/op_mul_mat_q8_0.comp
        kompute-shaders/op_mul_mat_q4_0.comp
        kompute-shaders/op_mul_mat_q4_1.comp
        kompute-shaders/op_mul_mat_q4_k.comp
        kompute-shaders/op_mul_mat_q6_k.comp
        kompute-shaders/op_getrows_f32.comp
        kompute-shaders/op_getrows_f16.comp
        kompute-shaders/op_getrows_q4_0.comp
        kompute-shaders/op_getrows_q4_1.comp
        kompute-shaders/op_getrows_q6_k.comp
        kompute-shaders/op_rope_norm_f16.comp
        kompute-shaders/op_rope_norm_f32.comp
        kompute-shaders/op_rope_neox_f16.comp
        kompute-shaders/op_rope_neox_f32.comp
        kompute-shaders/op_cpy_f16_f16.comp
        kompute-shaders/op_cpy_f16_f32.comp
        kompute-shaders/op_cpy_f32_f16.comp
        kompute-shaders/op_cpy_f32_f32.comp
    )

    # Create a custom target for our generated shaders
    add_custom_target(generated_shaders DEPENDS
        shaderop_scale.h
        shaderop_scale_8.h
        shaderop_add.h
        shaderop_addrow.h
        shaderop_mul.h
        shaderop_silu.h
        shaderop_relu.h
        shaderop_gelu.h
        shaderop_softmax.h
        shaderop_norm.h
        shaderop_rmsnorm.h
        shaderop_diagmask.h
        shaderop_mul_mat_mat_f32.h
        shaderop_mul_mat_f16.h
        shaderop_mul_mat_q8_0.h
        shaderop_mul_mat_q4_0.h
        shaderop_mul_mat_q4_1.h
        shaderop_mul_mat_q4_k.h
        shaderop_mul_mat_q6_k.h
        shaderop_getrows_f32.h
        shaderop_getrows_f16.h
        shaderop_getrows_q4_0.h
        shaderop_getrows_q4_1.h
        shaderop_getrows_q6_k.h
        shaderop_rope_norm_f16.h
        shaderop_rope_norm_f32.h
        shaderop_rope_neox_f16.h
        shaderop_rope_neox_f32.h
        shaderop_cpy_f16_f16.h
        shaderop_cpy_f16_f32.h
        shaderop_cpy_f32_f16.h
        shaderop_cpy_f32_f32.h
    )

    # Create a custom command that depends on the generated_shaders
    add_custom_command(
        OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/ggml-kompute.stamp
        COMMAND ${CMAKE_COMMAND} -E touch ${CMAKE_CURRENT_BINARY_DIR}/ggml-kompute.stamp
        DEPENDS generated_shaders
        COMMENT "Ensuring shaders are generated before compiling ggml-kompute.cpp"
    )

    # Add the stamp to the main sources to ensure dependency tracking
    target_sources(ggml-kompute PRIVATE ${CMAKE_CURRENT_BINARY_DIR}/ggml-kompute.stamp)
else()
    message(WARNING "Kompute not found")
endif()
```

----------------------------------------

TITLE: Highlighting Security Note in Markdown
DESCRIPTION: Uses Markdown syntax to create a highlighted note emphasizing the importance of assessing model trustworthiness based on specific use cases and risk tolerance.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/SECURITY.md#2025-04-22_snippet_0

LANGUAGE: markdown
CODE:
```
> [!NOTE]
> The trustworthiness of a model is not binary. You must always determine the proper level of caution depending on the specific model and how it matches your use case and risk tolerance.
```

----------------------------------------

TITLE: Finding Required HIP and ROCm Packages in CMake
DESCRIPTION: Locates necessary HIP and ROCm packages including hip, hipblas, and rocblas. It also checks for the rocwmma library if GGML_HIP_ROCWMMA_FATTN is enabled.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-hip/CMakeLists.txt#2025-04-22_snippet_2

LANGUAGE: cmake
CODE:
```
find_package(hip     REQUIRED)
find_package(hipblas REQUIRED)
find_package(rocblas REQUIRED)
if (GGML_HIP_ROCWMMA_FATTN)
    CHECK_INCLUDE_FILE_CXX("rocwmma/rocwmma.hpp" FOUND_ROCWMMA)
    if (NOT ${FOUND_ROCWMMA})
        message(FATAL_ERROR "rocwmma has not been found")
    endif()
endif()

if (${hip_VERSION} VERSION_LESS 5.5)
    message(FATAL_ERROR "At least ROCM/HIP V5.5 is required")
endif()

message(STATUS "HIP and hipBLAS found")

# Workaround old compilers
set(CMAKE_HIP_FLAGS "${CMAKE_HIP_FLAGS} --gpu-max-threads-per-block=1024")
```

----------------------------------------

TITLE: Configuring Library Prefixes for Windows
DESCRIPTION: Removes the default 'lib' prefix from libraries when building on Windows with MinGW.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_3

LANGUAGE: CMake
CODE:
```
# remove the lib prefix on win32 mingw
if (WIN32)
    set(CMAKE_STATIC_LIBRARY_PREFIX "")
    set(CMAKE_SHARED_LIBRARY_PREFIX "")
    set(CMAKE_SHARED_MODULE_PREFIX  "")
endif()
```

----------------------------------------

TITLE: Setting up CMake Configuration for llama-gbnf-validator in llama.cpp
DESCRIPTION: This CMake script defines the llama-gbnf-validator executable target with its source file, installation configuration, linked libraries, and C++17 requirement. The target depends on common, llama, and system thread libraries.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gbnf-validator/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(TARGET llama-gbnf-validator)
add_executable(${TARGET} gbnf-validator.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Installing Android NDK for llama.cpp OpenCL Development
DESCRIPTION: Commands to download and install Android command line tools and NDK 26.3.11579264, which are prerequisites for building llama.cpp for Android with OpenCL support.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/OPENCL.md#2025-04-22_snippet_1

LANGUAGE: sh
CODE:
```
cd ~
wget https://dl.google.com/android/repository/commandlinetools-linux-8512546_latest.zip && \
unzip commandlinetools-linux-8512546_latest.zip && \
mkdir -p ~/android-sdk/cmdline-tools && \
mv cmdline-tools latest && \
mv latest ~/android-sdk/cmdline-tools/ && \
rm -rf commandlinetools-linux-8512546_latest.zip

yes | ~/android-sdk/cmdline-tools/latest/bin/sdkmanager "ndk;26.3.11579264"
```

----------------------------------------

TITLE: Creating CMake Package and Version Info for GGML
DESCRIPTION: This section generates version information based on Git commit data and creates a CMake package configuration. It captures GGML-specific variables, sets up installation paths, and configures package version information for downstream projects.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_19

LANGUAGE: CMake
CODE:
```
#
# Create CMake package
#

# Generate version info based on git commit.

if(NOT DEFINED GGML_BUILD_NUMBER)
    find_program(GIT_EXE NAMES git git.exe REQUIRED NO_CMAKE_FIND_ROOT_PATH)
    execute_process(COMMAND ${GIT_EXE} rev-list --count HEAD
        WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}
        OUTPUT_VARIABLE GGML_BUILD_NUMBER
        OUTPUT_STRIP_TRAILING_WHITESPACE
    )

    if(GGML_BUILD_NUMBER EQUAL 1)
        message(WARNING "GGML build version fixed at 1 likely due to a shallow clone.")
    endif()

    execute_process(COMMAND ${GIT_EXE} rev-parse --short HEAD
        WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}
        OUTPUT_VARIABLE GGML_BUILD_COMMIT
        OUTPUT_STRIP_TRAILING_WHITESPACE
    )
endif()


# Capture variables prefixed with GGML_.

set(variable_set_statements
"
####### Expanded from @GGML_VARIABLES_EXPANED@ by configure_package_config_file() #######
####### Any changes to this file will be overwritten by the next CMake run        #######

")

set(GGML_SHARED_LIB ${BUILD_SHARED_LIBS})

get_cmake_property(all_variables VARIABLES)
foreach(variable_name IN LISTS all_variables)
    if(variable_name MATCHES "^GGML_")
        string(REPLACE ";" "\\;"
               variable_value "${${variable_name}}")

        set(variable_set_statements
            "${variable_set_statements}set(${variable_name} \"${variable_value}\")\n")
    endif()
endforeach()

set(GGML_VARIABLES_EXPANDED ${variable_set_statements})

# Create the CMake package and set install location.

set(GGML_INSTALL_VERSION 0.0.${GGML_BUILD_NUMBER})
set(GGML_INCLUDE_INSTALL_DIR ${CMAKE_INSTALL_INCLUDEDIR} CACHE PATH "Location of header  files")
set(GGML_LIB_INSTALL_DIR     ${CMAKE_INSTALL_LIBDIR}     CACHE PATH "Location of library files")
set(GGML_BIN_INSTALL_DIR     ${CMAKE_INSTALL_BINDIR}     CACHE PATH "Location of binary  files")

configure_package_config_file(
        ${CMAKE_CURRENT_SOURCE_DIR}/cmake/ggml-config.cmake.in
        ${CMAKE_CURRENT_BINARY_DIR}/ggml-config.cmake
    INSTALL_DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/ggml
    PATH_VARS GGML_INCLUDE_INSTALL_DIR
              GGML_LIB_INSTALL_DIR
              GGML_BIN_INSTALL_DIR)

write_basic_package_version_file(
        ${CMAKE_CURRENT_BINARY_DIR}/ggml-version.cmake
    VERSION ${GGML_INSTALL_VERSION}
    COMPATIBILITY SameMajorVersion)

install(FILES ${CMAKE_CURRENT_BINARY_DIR}/ggml-config.cmake
              ${CMAKE_CURRENT_BINARY_DIR}/ggml-version.cmake
        DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/ggml)
```

----------------------------------------

TITLE: Applying Single LORA Adapter to LLaMA Model
DESCRIPTION: Demonstrates how to use the llama-export-lora tool to apply a single LORA adapter to a base LLaMA model. It specifies the input model, output file, and the LORA adapter file.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/export-lora/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
./bin/llama-export-lora \
    -m open-llama-3b-v2.gguf \
    -o open-llama-3b-v2-english2tokipona-chat.gguf \
    --lora lora-open-llama-3b-v2-english2tokipona-chat-LATEST.gguf
```

----------------------------------------

TITLE: Configuring BLAS Vendor and Finding BLAS Package in CMake
DESCRIPTION: This snippet sets up BLAS vendor configuration and attempts to find the BLAS package. It handles static linking and sets vendor-specific options.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-blas/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
if (GGML_STATIC)
    set(BLA_STATIC ON)
endif()
#if (CMAKE_VERSION VERSION_GREATER_EQUAL 3.22)
#    set(BLA_SIZEOF_INTEGER 8)
#endif()

set(BLA_VENDOR ${GGML_BLAS_VENDOR})
find_package(BLAS)
```

----------------------------------------

TITLE: Compiling Metal Shaders for GGML in CMake
DESCRIPTION: Sets up the compilation process for Metal shaders when GGML_METAL_EMBED_LIBRARY is not enabled, including debug options and version flags.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-metal/CMakeLists.txt#2025-04-22_snippet_5

LANGUAGE: CMake
CODE:
```
if (GGML_METAL_SHADER_DEBUG)
    set(XC_FLAGS -fno-fast-math -fno-inline -g)
else()
    set(XC_FLAGS -O3)
endif()

if (GGML_METAL_MACOSX_VERSION_MIN)
    message(STATUS "Adding  -mmacosx-version-min=${GGML_METAL_MACOSX_VERSION_MIN} flag to metal compilation")
    list   (APPEND XC_FLAGS -mmacosx-version-min=${GGML_METAL_MACOSX_VERSION_MIN})
endif()

if (GGML_METAL_STD)
    message(STATUS "Adding  -std=${GGML_METAL_STD} flag to metal compilation")
    list   (APPEND XC_FLAGS -std=${GGML_METAL_STD})
endif()

add_custom_command(
    OUTPUT ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/default.metallib
    COMMAND xcrun -sdk macosx metal ${XC_FLAGS} -c ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-metal.metal -o - |
        xcrun -sdk macosx metallib - -o ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/default.metallib
    COMMAND rm -f ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-common.h
    COMMAND rm -f ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-metal.metal
    DEPENDS ggml-metal.metal ${METALLIB_COMMON}
    COMMENT "Compiling Metal kernels"
    )

add_custom_target(
    ggml-metal-lib ALL
    DEPENDS ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/default.metallib
    )
```

----------------------------------------

TITLE: Setting Compile Definitions for GGML Metal Backend in CMake
DESCRIPTION: Adds compile definitions for debug mode and BF16 support in the Metal backend.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-metal/CMakeLists.txt#2025-04-22_snippet_2

LANGUAGE: CMake
CODE:
```
if (GGML_METAL_NDEBUG)
    add_compile_definitions(GGML_METAL_NDEBUG)
endif()

if (GGML_METAL_USE_BF16)
    add_compile_definitions(GGML_METAL_USE_BF16)
endif()
```

----------------------------------------

TITLE: Whitespace Definition in GBNF
DESCRIPTION: Demonstrates how to define optional whitespace including spaces, tabs, and newlines with comments.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md#2025-04-22_snippet_2

LANGUAGE: gbnf
CODE:
```
# defines optional whitespace
ws ::= [ \t\n]+
```

----------------------------------------

TITLE: Configuring Shared Libraries for Different Platforms
DESCRIPTION: Sets the default behavior for building shared or static libraries based on the target platform, with special handling for Emscripten and MinGW.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_2

LANGUAGE: CMake
CODE:
```
if (EMSCRIPTEN)
    set(BUILD_SHARED_LIBS_DEFAULT OFF)

    option(GGML_WASM_SINGLE_FILE "ggml: embed WASM inside the generated ggml.js" ON)
else()
    if (MINGW)
        set(BUILD_SHARED_LIBS_DEFAULT OFF)
    else()
        set(BUILD_SHARED_LIBS_DEFAULT ON)
    endif()
endif()
```

----------------------------------------

TITLE: Configuring Dependencies and Third-party Libraries for llama.cpp
DESCRIPTION: This CMake snippet configures the necessary dependencies for the llama.cpp project. It requires the Threads package, sets up include directories, and conditionally includes the vdot subdirectory based on the build environment and configuration options.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/pocs/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
find_package(Threads REQUIRED)

# third-party

include_directories(${CMAKE_CURRENT_SOURCE_DIR})

if (EMSCRIPTEN)
else()
    if (NOT GGML_BACKEND_DL)
        add_subdirectory(vdot)
    endif()
endif()
```

----------------------------------------

TITLE: Running the Llama.cpp Server Tests
DESCRIPTION: Basic command to start the server tests using the provided test script.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/tests/README.md#2025-04-22_snippet_2

LANGUAGE: shell
CODE:
```
./tests.sh
```

----------------------------------------

TITLE: Linking Libraries to Android JNI Interface
DESCRIPTION: Links the required libraries to the Android JNI interface, including the llama and common libraries from llama.cpp, as well as the Android system libraries needed for logging and native functionality.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama.android/llama/src/main/cpp/CMakeLists.txt#2025-04-22_snippet_4

LANGUAGE: cmake
CODE:
```
target_link_libraries(${CMAKE_PROJECT_NAME}
        # List libraries link to the target library
        llama
        common
        android
        log)
```

----------------------------------------

TITLE: Defining OpenCL Kernel Addition Function in GGML CMake
DESCRIPTION: Creates a function to add OpenCL kernels, either by embedding them or copying to the output directory based on configuration.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-opencl/CMakeLists.txt#2025-04-22_snippet_4

LANGUAGE: CMake
CODE:
```
function(ggml_opencl_add_kernel KNAME)
    set(KERN_HDR ${CMAKE_CURRENT_BINARY_DIR}/autogenerated/${KNAME}.cl.h)
    set(KERN_SRC ${CMAKE_CURRENT_SOURCE_DIR}/kernels/${KNAME}.cl)

    if (GGML_OPENCL_EMBED_KERNELS)
        message(STATUS "opencl: embedding kernel ${KNAME}")

        # Python must be accessible from command line
        add_custom_command(
            OUTPUT ${KERN_HDR}
            COMMAND ${Python3_EXECUTABLE} ${EMBED_KERNEL_SCRIPT} ${KERN_SRC} ${KERN_HDR}
            DEPENDS ${KERN_SRC} ${EMBED_KERNEL_SCRIPT}
            COMMENT "Generate ${KERN_HDR}"
        )

        target_sources(${TARGET_NAME} PRIVATE ${KERN_HDR})
    else ()
        message(STATUS "opencl: adding kernel ${KNAME}")
        configure_file(${KERN_SRC} ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/${KNAME}.cl COPYONLY)
    endif ()
endfunction()
```

----------------------------------------

TITLE: Running LLaMA.cpp Server with Custom Theme
DESCRIPTION: Command example showing how to start the LLaMA.cpp server with a custom theme from the 'wild' directory by using the --path parameter.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/themes/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
server --path=wild
```

----------------------------------------

TITLE: Vendor-Specific SYCL Configurations
DESCRIPTION: Sets up vendor-specific configurations for Intel, NVIDIA, and AMD targets, including warp sizes and performance optimizations.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-sycl/CMakeLists.txt#2025-04-22_snippet_3

LANGUAGE: cmake
CODE:
```
if (GGML_SYCL_TARGET STREQUAL "NVIDIA")
    add_compile_definitions(GGML_SYCL_WARP_SIZE=32)
elseif (GGML_SYCL_TARGET STREQUAL "AMD")
    add_compile_definitions(GGML_SYCL_WARP_SIZE=32)
else()
    add_compile_definitions(GGML_SYCL_WARP_SIZE=16)
endif()
```

----------------------------------------

TITLE: Reverse Prompt with Input Prefix and Suffix
DESCRIPTION: Command demonstrating the setup of reverse prompts with both input prefix and suffix for structured conversations
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_11

LANGUAGE: sh
CODE:
```
./llama-cli -r "User:" --in-prefix " " --in-suffix "Assistant:"
```

----------------------------------------

TITLE: Setting Instruction Set Enablement Logic
DESCRIPTION: Sets up a variable to control whether instruction set optimizations should be enabled by default based on GGML_NATIVE setting.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_10

LANGUAGE: CMake
CODE:
```
# instruction set specific
if (GGML_NATIVE OR NOT GGML_NATIVE_DEFAULT)
    set(INS_ENB OFF)
else()
    set(INS_ENB ON)
endif()

message(DEBUG "GGML_NATIVE         : ${GGML_NATIVE}")
message(DEBUG "GGML_NATIVE_DEFAULT : ${GGML_NATIVE_DEFAULT}")
message(DEBUG "INS_ENB             : ${INS_ENB}")
```

----------------------------------------

TITLE: Compiling LLaMA Multi-Modal
DESCRIPTION: Command for compiling the project using make with 32 parallel jobs.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_14

LANGUAGE: sh
CODE:
```
make -j32
```

----------------------------------------

TITLE: Running a Specific Test by Number
DESCRIPTION: This command runs a specific test by its number (23 in this example) to speed up the testing loop when the test number is already known.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/debugging-tests.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
./scripts/debug-test.sh test 23
```

----------------------------------------

TITLE: Setting up CMake Project for Android llama.cpp Integration
DESCRIPTION: Configures the CMake project for building llama.cpp on Android. It sets the minimum required CMake version and defines the project name that will be used throughout the build process.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama.android/llama/src/main/cpp/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
cmake_minimum_required(VERSION 3.22.1)

project("llama-android")
```

----------------------------------------

TITLE: Building and Installing Cross-compiled llama.cpp
DESCRIPTION: Commands to build and install the cross-compiled llama.cpp project for Android, with multi-threaded compilation support.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/android.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
$ cmake --build build-android --config Release -j{n}
$ cmake --install build-android --prefix {install-dir} --config Release
```

----------------------------------------

TITLE: Checking Vulkan Shader Extension Support in CMake
DESCRIPTION: Execute process to compile test shaders and determine support for Vulkan extensions like cooperative matrix and integer dot product.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-vulkan/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: CMake
CODE:
```
execute_process(COMMAND ${Vulkan_GLSLC_EXECUTABLE} -o - -fshader-stage=compute --target-env=vulkan1.3 "${CMAKE_CURRENT_SOURCE_DIR}/vulkan-shaders/test_coopmat_support.comp"
                OUTPUT_VARIABLE glslc_output
                ERROR_VARIABLE glslc_error)

if (${glslc_error} MATCHES ".*extension not supported: GL_KHR_cooperative_matrix.*")
    message(STATUS "GL_KHR_cooperative_matrix not supported by glslc")
    set(GGML_VULKAN_COOPMAT_GLSLC_SUPPORT OFF)
else()
    message(STATUS "GL_KHR_cooperative_matrix supported by glslc")
    set(GGML_VULKAN_COOPMAT_GLSLC_SUPPORT ON)
    add_compile_definitions(GGML_VULKAN_COOPMAT_GLSLC_SUPPORT)
endif()
```

----------------------------------------

TITLE: Installing CANN Toolkit and Dependencies
DESCRIPTION: Commands for installing CANN toolkit dependencies and setting up the environment
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CANN.md#2025-04-22_snippet_4

LANGUAGE: shell
CODE:
```
pip3 install attrs numpy decorator sympy cffi pyyaml pathlib2 psutil protobuf scipy requests absl-py wheel typing_extensions
sh Ascend-cann-toolkit_8.0.RC2.alpha002_linux-aarch64.run --install
sh Ascend-cann-kernels-910b_8.0.RC2.alpha002_linux.run --install
```

----------------------------------------

TITLE: Detecting Host Compiler in CMake
DESCRIPTION: Function to detect the host compiler on Windows and non-Windows systems, setting variables for C and C++ compilers.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-vulkan/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
function(detect_host_compiler)
    if (CMAKE_HOST_SYSTEM_NAME STREQUAL "Windows")
        find_program(HOST_C_COMPILER NAMES cl gcc clang NO_CMAKE_FIND_ROOT_PATH)
        find_program(HOST_CXX_COMPILER NAMES cl g++ clang++ NO_CMAKE_FIND_ROOT_PATH)
    else()
        find_program(HOST_C_COMPILER NAMES gcc clang NO_CMAKE_FIND_ROOT_PATH)
        find_program(HOST_CXX_COMPILER NAMES g++ clang++ NO_CMAKE_FIND_ROOT_PATH)
    endif()
    set(HOST_C_COMPILER "${HOST_C_COMPILER}" PARENT_SCOPE)
    set(HOST_CXX_COMPILER "${HOST_CXX_COMPILER}" PARENT_SCOPE)
endfunction()
```

----------------------------------------

TITLE: SYCL and OneDNN Integration
DESCRIPTION: Configures SYCL compiler options and integrates with OneDNN library if available. Includes vendor-specific configurations and compiler flags.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-sycl/CMakeLists.txt#2025-04-22_snippet_2

LANGUAGE: cmake
CODE:
```
find_package(IntelSYCL)
if (IntelSYCL_FOUND)
    target_link_libraries(ggml-sycl PRIVATE IntelSYCL::SYCL_CXX)
else()
    target_compile_options(ggml-sycl PRIVATE "-fsycl")
    target_link_options(ggml-sycl PRIVATE "-fsycl")
endif()

target_compile_options(ggml-sycl PRIVATE "-Wno-narrowing")
```

----------------------------------------

TITLE: Commented FetchContent Example for llama.cpp
DESCRIPTION: Shows a commented-out alternative approach to integrate llama.cpp using CMake's FetchContent module, which would download the repository directly instead of using a local copy.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama.android/llama/src/main/cpp/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: cmake
CODE:
```
#include(FetchContent)
#FetchContent_Declare(
#        llama
#        GIT_REPOSITORY https://github.com/ggml-org/llama.cpp
#        GIT_TAG        master
#)

# Also provides "common"
#FetchContent_MakeAvailable(llama)
```

----------------------------------------

TITLE: GGUF Command Line Options
DESCRIPTION: Command line parameters for controlling GGUF file splitting and merging operations. Allows specifying maximum split size in MB/GB, maximum number of tensors per split, and merge functionality.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-split/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
--split
--split-max-size
--split-max-tensors
--merge
```

----------------------------------------

TITLE: Accessing Global Settings Object in JavaScript
DESCRIPTION: Shows how to access the global settings object 'gMe' which contains various configuration options for the llama.cpp web interface. This object is attached to the document and can be modified using the browser's developer tools.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/public_simplechat/readme.md#2025-04-22_snippet_3

LANGUAGE: JavaScript
CODE:
```
gMe.baseURL
```

LANGUAGE: JavaScript
CODE:
```
gMe.bStream
```

LANGUAGE: JavaScript
CODE:
```
gMe.apiEP
```

LANGUAGE: JavaScript
CODE:
```
gMe.bCompletionFreshChatAlways
```

LANGUAGE: JavaScript
CODE:
```
gMe.bCompletionInsertStandardRolePrefix
```

LANGUAGE: JavaScript
CODE:
```
gMe.bTrimGarbage
```

LANGUAGE: JavaScript
CODE:
```
gMe.apiRequestOptions
```

LANGUAGE: JavaScript
CODE:
```
gMe.headers
```

LANGUAGE: JavaScript
CODE:
```
gMe.iRecentUserMsgCnt
```

----------------------------------------

TITLE: Sanitizer Configuration
DESCRIPTION: Setup for various sanitizer options including thread, address, and undefined behavior sanitizers
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/CMakeLists.txt#2025-04-22_snippet_3

LANGUAGE: cmake
CODE:
```
if (NOT MSVC)
    if (LLAMA_SANITIZE_THREAD)
        message(STATUS "Using -fsanitize=thread")
        add_compile_options(-fsanitize=thread)
        link_libraries     (-fsanitize=thread)
    endif()

    if (LLAMA_SANITIZE_ADDRESS)
        message(STATUS "Using -fsanitize=address")
        add_compile_options(-fsanitize=address -fno-omit-frame-pointer)
        link_libraries     (-fsanitize=address)
    endif()

    if (LLAMA_SANITIZE_UNDEFINED)
        message(STATUS "Using -fsanitize=undefined")
        add_compile_options(-fsanitize=undefined)
        link_libraries     (-fsanitize=undefined)
    endif()
endif()
```

----------------------------------------

TITLE: Installing Metal Files for GGML in CMake
DESCRIPTION: Sets up the installation process for Metal-related files when GGML_METAL_EMBED_LIBRARY is not enabled.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-metal/CMakeLists.txt#2025-04-22_snippet_6

LANGUAGE: CMake
CODE:
```
if (NOT GGML_METAL_EMBED_LIBRARY)
    install(
        FILES src/ggml-metal/ggml-metal.metal
        PERMISSIONS
            OWNER_READ
            OWNER_WRITE
            GROUP_READ
            WORLD_READ
        DESTINATION ${CMAKE_INSTALL_BINDIR})

        install(
            FILES ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/default.metallib
            DESTINATION ${CMAKE_INSTALL_BINDIR}
        )
endif()
```

----------------------------------------

TITLE: Defining CMake Function for Building and Testing in llama.cpp
DESCRIPTION: Function that builds an executable from a source file and registers it as a test. It handles configuration of the test name, label, working directory, and arguments. The function links with the 'common' library and includes 'get-model.cpp'.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/tests/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: CMake
CODE:
```
function(llama_target_and_test source)
    include(CMakeParseArguments)
    set(options)
    set(oneValueArgs NAME LABEL WORKING_DIRECTORY)
    set(multiValueArgs ARGS)
    cmake_parse_arguments(LLAMA_TEST "${options}" "${oneValueArgs}" "${multiValueArgs}" ${ARGN})

    if (NOT DEFINED LLAMA_TEST_LABEL)
        set(LLAMA_TEST_LABEL "main")
    endif()
    if (NOT DEFINED LLAMA_TEST_WORKING_DIRECTORY)
        set(LLAMA_TEST_WORKING_DIRECTORY .)
    endif()
    if (DEFINED LLAMA_TEST_NAME)
        set(TEST_TARGET ${LLAMA_TEST_NAME})
    else()
        get_filename_component(TEST_TARGET ${source} NAME_WE)
    endif()

    add_executable(${TEST_TARGET} ${source} get-model.cpp)
    install(TARGETS ${TEST_TARGET} RUNTIME)
    target_link_libraries(${TEST_TARGET} PRIVATE common)
    add_test(
        NAME ${TEST_TARGET}
        WORKING_DIRECTORY ${LLAMA_TEST_WORKING_DIRECTORY}
        COMMAND $<TARGET_FILE:${TEST_TARGET}>
        ${LLAMA_TEST_ARGS})

    set_property(TEST ${TEST_TARGET} PROPERTY LABELS ${LLAMA_TEST_LABEL})
endfunction()
```

----------------------------------------

TITLE: Configuring Vulkan Shaders Generator Project in CMake
DESCRIPTION: Sets up a CMake project for a Vulkan shaders generator. It defines the project, sets compiler definitions based on GLSLC support, specifies the target executable, and configures compilation features and library dependencies.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-vulkan/vulkan-shaders/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
cmake_minimum_required(VERSION 3.19)
project("vulkan-shaders-gen" C CXX)

find_package (Threads REQUIRED)

if (GGML_VULKAN_COOPMAT_GLSLC_SUPPORT)
    add_compile_definitions(GGML_VULKAN_COOPMAT_GLSLC_SUPPORT)
endif()
if (GGML_VULKAN_COOPMAT2_GLSLC_SUPPORT)
    add_compile_definitions(GGML_VULKAN_COOPMAT2_GLSLC_SUPPORT)
endif()
if (GGML_VULKAN_INTEGER_DOT_GLSLC_SUPPORT)
    add_compile_definitions(GGML_VULKAN_INTEGER_DOT_GLSLC_SUPPORT)
endif()
set(TARGET vulkan-shaders-gen)
add_executable(${TARGET} vulkan-shaders-gen.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_compile_features(${TARGET} PRIVATE cxx_std_17)
target_link_libraries(vulkan-shaders-gen PUBLIC Threads::Threads)
```

----------------------------------------

TITLE: Advanced PCA Options for Control Vector Generation
DESCRIPTION: Command with advanced PCA (Principal Component Analysis) options, specifying iteration count and batch size for more refined control vector generation.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/cvector-generator/README.md#2025-04-22_snippet_2

LANGUAGE: sh
CODE:
```
./cvector-generator -m ./llama-3.Q4_K_M.gguf -ngl 99 --pca-iter 2000 --pca-batch 100
```

----------------------------------------

TITLE: Preprocessor Directive Formatting in C++
DESCRIPTION: Example showing the proper format for preprocessor directives with comments that include the directive name for clarity and readability.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/CONTRIBUTING.md#2025-04-22_snippet_5

LANGUAGE: cpp
CODE:
```
#ifdef FOO
#endif // FOO
```

----------------------------------------

TITLE: Example Output of XXH64 Verification
DESCRIPTION: This is the output of verifying a GGUF file against its manifest using the XXH64 hash algorithm. This method is faster but provides less cryptographic security than SHA-256.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-hash/README.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
manifest  test.gguf.manifest  sha256  sha1  xxh64
xxh64     f66e9cd66a4396a0  test.gguf:tensor_0  -  Ok
xxh64     7d3a1f9ac04d0537  test.gguf:tensor_1  -  Ok
xxh64     a0af5d700049693b  test.gguf:tensor_2  -  Ok
xxh64     e83fddf559d7b6a6  test.gguf:tensor_3  -  Ok
xxh64     1257733306b7992d  test.gguf:tensor_4  -  Ok
xxh64     d238d16ba4711e58  test.gguf:tensor_5  -  Ok
xxh64     3fbc3b65ab8c7f39  test.gguf:tensor_6  -  Ok
xxh64     c22021c29854f093  test.gguf:tensor_7  -  Ok
xxh64     936df61f5d64261f  test.gguf:tensor_8  -  Ok
xxh64     93fd20c64421c081  test.gguf:tensor_9  -  Ok
xxh64     5a54d3aad816f302  test.gguf  -  Ok

Verification results for test.gguf.manifest - Success
```

----------------------------------------

TITLE: Sample output from SYCL device listing
DESCRIPTION: Example output from the llama-ls-sycl-device tool showing two Intel GPUs with their specifications including compute units, work group sizes, and memory size.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/sycl/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
found 2 SYCL devices:
|  |                   |                                       |       |Max    |        |Max  |Global |                     |
|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |
|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|
|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|
| 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|    1.3|    512|    1024|   32| 16225M|            1.3.29138|
| 1| [level_zero:gpu:1]|                 Intel UHD Graphics 750|    1.3|     32|     512|   32| 62631M|            1.3.29138|
```

----------------------------------------

TITLE: Configuring Debug Build Settings for Linux
DESCRIPTION: Enables libstdc++ assertions for debug builds on Linux systems. This helps with identifying issues during development.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: CMake
CODE:
```
# enable libstdc++ assertions for debug builds
if (CMAKE_SYSTEM_NAME MATCHES "Linux")
    add_compile_definitions($<$<CONFIG:Debug>:_GLIBCXX_ASSERTIONS>)
endif()
```

----------------------------------------

TITLE: Running llama-cli with SYCL on Linux (Single Device)
DESCRIPTION: Shell command to execute llama-cli inference using a single SYCL device (device 0) on Linux with specific parameters.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_13

LANGUAGE: sh
CODE:
```
ZES_ENABLE_SYSMAN=1 ./build/bin/llama-cli -no-cnv -m models/llama-2-7b.Q4_0.gguf -p "Building a website can be done in 10 simple steps:" -n 400 -e -ngl 33 -sm none -mg 0
```

----------------------------------------

TITLE: Detecting HIP Compiler and Enabling HIP Language in CMake
DESCRIPTION: Checks if the C++ compiler is hipcc and enables HIP language support. It also handles Windows-specific configuration and forwards AMDGPU_TARGETS to CMAKE_HIP_ARCHITECTURES.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-hip/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: cmake
CODE:
```
if (WIN32)
    set(CXX_IS_HIPCC TRUE)
else()
    string(REGEX MATCH "hipcc(\.bat)?$" CXX_IS_HIPCC "${CMAKE_CXX_COMPILER}")
endif()

if (CXX_IS_HIPCC)
    if (LINUX)
        if (NOT ${CMAKE_CXX_COMPILER_ID} MATCHES "Clang")
            message(WARNING "Only LLVM is supported for HIP, hint: CXX=/opt/rocm/llvm/bin/clang++")
        endif()

        message(WARNING "Setting hipcc as the C++ compiler is legacy behavior."
                " Prefer setting the HIP compiler directly. See README for details.")
    endif()
else()
    # Forward AMDGPU_TARGETS to CMAKE_HIP_ARCHITECTURES.
    if (AMDGPU_TARGETS AND NOT CMAKE_HIP_ARCHITECTURES)
        set(CMAKE_HIP_ARCHITECTURES ${AMDGPU_TARGETS})
    endif()
    cmake_minimum_required(VERSION 3.21)
    enable_language(HIP)
endif()
```

----------------------------------------

TITLE: Declaring Python Dependencies
DESCRIPTION: This snippet defines the Python packages required for the project. It includes matplotlib for creating graphs and visualizations, and requests for making HTTP requests.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/bench/requirements.txt#2025-04-22_snippet_0

LANGUAGE: plaintext
CODE:
```
matplotlib
requests
```

----------------------------------------

TITLE: Updating Chat Templates for Multiple AI Models using Bash Commands
DESCRIPTION: This bash script contains multiple commands to update chat templates for various AI models. Each command runs the get_chat_template.py script with specific model parameters and saves the output to a .jinja file in the models/templates directory. The models include variants from CohereForAI, deepseek-ai, fireworks-ai, Google, MeetKai, Meta-Llama, Microsoft, MistralAI, NousResearch, and Qwen.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/models/templates/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
./scripts/get_chat_template.py CohereForAI/c4ai-command-r-plus tool_use      > models/templates/CohereForAI-c4ai-command-r-plus-tool_use.jinja
./scripts/get_chat_template.py CohereForAI/c4ai-command-r7b-12-2024 default  > models/templates/CohereForAI-c4ai-command-r7b-12-2024-default.jinja
./scripts/get_chat_template.py CohereForAI/c4ai-command-r7b-12-2024 rag      > models/templates/CohereForAI-c4ai-command-r7b-12-2024-rag.jinja
./scripts/get_chat_template.py CohereForAI/c4ai-command-r7b-12-2024 tool_use > models/templates/CohereForAI-c4ai-command-r7b-12-2024-tool_use.jinja
./scripts/get_chat_template.py deepseek-ai/DeepSeek-R1-Distill-Llama-8B      > models/templates/deepseek-ai-DeepSeek-R1-Distill-Llama-8B.jinja
./scripts/get_chat_template.py deepseek-ai/DeepSeek-R1-Distill-Qwen-32B      > models/templates/deepseek-ai-DeepSeek-R1-Distill-Qwen-32B.jinja
./scripts/get_chat_template.py fireworks-ai/llama-3-firefunction-v2          > models/templates/fireworks-ai-llama-3-firefunction-v2.jinja
./scripts/get_chat_template.py google/gemma-2-2b-it                          > models/templates/google-gemma-2-2b-it.jinja
./scripts/get_chat_template.py meetkai/functionary-medium-v3.1               > models/templates/meetkai-functionary-medium-v3.1.jinja
./scripts/get_chat_template.py meetkai/functionary-medium-v3.2               > models/templates/meetkai-functionary-medium-v3.2.jinja
./scripts/get_chat_template.py meta-llama/Llama-3.1-8B-Instruct              > models/templates/meta-llama-Llama-3.1-8B-Instruct.jinja
./scripts/get_chat_template.py meta-llama/Llama-3.2-3B-Instruct              > models/templates/meta-llama-Llama-3.2-3B-Instruct.jinja
./scripts/get_chat_template.py meta-llama/Llama-3.3-70B-Instruct             > models/templates/meta-llama-Llama-3.3-70B-Instruct.jinja
./scripts/get_chat_template.py microsoft/Phi-3.5-mini-instruct               > models/templates/microsoft-Phi-3.5-mini-instruct.jinja
./scripts/get_chat_template.py mistralai/Mistral-Nemo-Instruct-2407          > models/templates/mistralai-Mistral-Nemo-Instruct-2407.jinja
./scripts/get_chat_template.py NousResearch/Hermes-2-Pro-Llama-3-8B tool_use > models/templates/NousResearch-Hermes-2-Pro-Llama-3-8B-tool_use.jinja
./scripts/get_chat_template.py NousResearch/Hermes-3-Llama-3.1-8B tool_use   > models/templates/NousResearch-Hermes-3-Llama-3.1-8B-tool_use.jinja
./scripts/get_chat_template.py Qwen/Qwen2.5-7B-Instruct                      > models/templates/Qwen-Qwen2.5-7B-Instruct.jinja
```

----------------------------------------

TITLE: Checking LLaVA Files
DESCRIPTION: List generated LLaVA files after surgery script execution.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/granitevision.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
$ ls $GRANITE_MODEL | grep -i llava
llava.clip
llava.projector
```

----------------------------------------

TITLE: Setting Warning Flags for Non-MSVC Compilers
DESCRIPTION: Configures comprehensive warning flags for C and C++ when GGML_ALL_WARNINGS is enabled. Different flags are set for C vs C++ files.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_4

LANGUAGE: CMake
CODE:
```
if (GGML_ALL_WARNINGS)
    if (NOT MSVC)
        list(APPEND WARNING_FLAGS -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function)
        list(APPEND C_FLAGS       -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes
                                  -Werror=implicit-int -Werror=implicit-function-declaration)
        list(APPEND CXX_FLAGS     -Wmissing-declarations -Wmissing-noreturn)

        list(APPEND C_FLAGS   ${WARNING_FLAGS})
        list(APPEND CXX_FLAGS ${WARNING_FLAGS})

        ggml_get_flags(${CMAKE_CXX_COMPILER_ID} ${CMAKE_CXX_COMPILER_VERSION})

        add_compile_options("$<$<COMPILE_LANGUAGE:C>:${C_FLAGS};${GF_C_FLAGS}>"
                            "$<$<COMPILE_LANGUAGE:CXX>:${CXX_FLAGS};${GF_CXX_FLAGS}>")
    else()
        # todo : msvc
        set(C_FLAGS   "")
        set(CXX_FLAGS "")
    endif()
endif()
```

----------------------------------------

TITLE: Selecting SYCL Devices for Inference
DESCRIPTION: Environment variable settings to choose specific SYCL devices for inference.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_10

LANGUAGE: sh
CODE:
```
export ONEAPI_DEVICE_SELECTOR="level_zero:0"
export ONEAPI_DEVICE_SELECTOR="level_zero:1"
export ONEAPI_DEVICE_SELECTOR="level_zero:0;level_zero:1"
```

----------------------------------------

TITLE: Implementing JSON Schema with Zod in JavaScript
DESCRIPTION: Demonstrates using Zod to create a strictly validated object schema with specific rules for age and email fields. Shows conversion to JSON schema format.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md#2025-04-22_snippet_9

LANGUAGE: javascript
CODE:
```
import { z } from 'zod';
import { zodToJsonSchema } from 'zod-to-json-schema';

const Foo = z.object({
  age: z.number().positive(),
  email: z.string().email(),
}).strict();

console.log(zodToJsonSchema(Foo));
```

----------------------------------------

TITLE: Defining CMake Test Registration Function for llama.cpp
DESCRIPTION: Function that registers a test for a pre-built target. It handles test naming, working directory configuration, and command arguments. The function is configurable through parameters and sets appropriate test properties.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/tests/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
function(llama_test target)
    include(CMakeParseArguments)
    set(options)
    set(oneValueArgs NAME LABEL WORKING_DIRECTORY)
    set(multiValueArgs ARGS)
    cmake_parse_arguments(LLAMA_TEST "${options}" "${oneValueArgs}" "${multiValueArgs}" ${ARGN})

    if (NOT DEFINED LLAMA_TEST_LABEL)
        set(LLAMA_TEST_LABEL "main")
    endif()
    if (NOT DEFINED LLAMA_TEST_WORKING_DIRECTORY)
        set(LLAMA_TEST_WORKING_DIRECTORY .)
    endif()
    if (DEFINED LLAMA_TEST_NAME)
        set(TEST_NAME ${LLAMA_TEST_NAME})
    else()
        set(TEST_NAME ${target})
    endif()

    set(TEST_TARGET ${target})

    add_test(
        NAME ${TEST_NAME}
        WORKING_DIRECTORY ${LLAMA_TEST_WORKING_DIRECTORY}
        COMMAND $<TARGET_FILE:${TEST_TARGET}>
        ${LLAMA_TEST_ARGS})

    set_property(TEST ${TEST_NAME} PROPERTY LABELS ${LLAMA_TEST_LABEL})
endfunction()
```

----------------------------------------

TITLE: Configuring Architecture-Specific Settings
DESCRIPTION: Sets up architecture-specific compiler and linker options including static linking, profiling, and Windows version targeting.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_8

LANGUAGE: CMake
CODE:
```
# architecture specific
# TODO: probably these flags need to be tweaked on some architectures
#       feel free to update the Makefile for your architecture and send a pull request or issue
message(STATUS "CMAKE_SYSTEM_PROCESSOR: ${CMAKE_SYSTEM_PROCESSOR}")
if (MSVC)
    string(TOLOWER "${CMAKE_GENERATOR_PLATFORM}" CMAKE_GENERATOR_PLATFORM_LWR)
    message(STATUS "CMAKE_GENERATOR_PLATFORM: ${CMAKE_GENERATOR_PLATFORM}")
else ()
    set(CMAKE_GENERATOR_PLATFORM_LWR "")
endif ()

if (NOT MSVC)
    if (GGML_STATIC)
        add_link_options(-static)
        if (MINGW)
            add_link_options(-static-libgcc -static-libstdc++)
        endif()
    endif()
    if (GGML_GPROF)
        add_compile_options(-pg)
    endif()
endif()

if (MINGW)
    # Target Windows 8 for PrefetchVirtualMemory
    add_compile_definitions(_WIN32_WINNT=${GGML_WIN_VER})
endif()
```

----------------------------------------

TITLE: Python Package Dependencies with Version Constraints
DESCRIPTION: Defines Python package dependencies with version constraints using the tilde operator, which allows patch version updates while maintaining minor version stability. Contains core libraries for HTTP requests, testing, data analysis, visualization, and API integrations.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-tool_bench.txt#2025-04-22_snippet_0

LANGUAGE: plaintext
CODE:
```
aiohttp~=3.9.3
pytest~=8.3.3
huggingface_hub~=0.23.2
matplotlib~=3.10.0
numpy~=1.26.4
openai~=1.55.3
pandas~=2.2.3
prometheus-client~=0.20.0
requests~=2.32.3
wget~=3.2
typer~=0.15.1
seaborn~=0.13.2
```

----------------------------------------

TITLE: Installing Python Dependencies with Version Constraints
DESCRIPTION: Requirements file specifying exact version constraints for two Python packages - tabulate v0.9.0 and GitPython v3.1.43. Uses the tilde (~=) operator to indicate version compatibility.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-compare-llama-bench.txt#2025-04-22_snippet_0

LANGUAGE: text
CODE:
```
tabulate~=0.9.0
GitPython~=3.1.43
```

----------------------------------------

TITLE: Creating Deprecation Warning Executables in CMake
DESCRIPTION: Creates executables that show deprecation warnings for older CLI applications, reusing the same deprecation-warning.cpp source file.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llava/CMakeLists.txt#2025-04-22_snippet_3

LANGUAGE: CMake
CODE:
```
add_executable(llama-llava-cli    deprecation-warning.cpp)
add_executable(llama-gemma3-cli   deprecation-warning.cpp)
add_executable(llama-minicpmv-cli deprecation-warning.cpp)
```

----------------------------------------

TITLE: List Grammar in GBNF
DESCRIPTION: Shows how to create a grammar for basic lists with items prefixed by hyphens.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md#2025-04-22_snippet_3

LANGUAGE: gbnf
CODE:
```
# a grammar for lists
root ::= ("- " item)+
item ::= [^\n]+ "\n"
```

----------------------------------------

TITLE: Building Package Distribution
DESCRIPTION: Command to build the package distribution archives
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/gguf-py/README.md#2025-04-22_snippet_7

LANGUAGE: sh
CODE:
```
python -m build
```

----------------------------------------

TITLE: Listing and Adding OpenCL Kernels in GGML CMake
DESCRIPTION: Defines a list of OpenCL kernels and adds them to the project using the previously defined function.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-opencl/CMakeLists.txt#2025-04-22_snippet_5

LANGUAGE: CMake
CODE:
```
set(GGML_OPENCL_KERNELS
    add
    clamp
    cpy
    cvt
    diag_mask_inf
    gelu
    gemv_noshuffle_general
    gemv_noshuffle
    get_rows
    im2col_f32
    im2col_f16
    mul_mat_Ab_Bi_8x4
    mul_mv_f16_f16
    mul_mv_f16_f32_1row
    mul_mv_f16_f32_l4
    mul_mv_f16_f32
    mul_mv_f32_f32
    mul_mv_q4_0_f32
    mul_mv_q4_0_f32_v
    mul_mv_q4_0_f32_8x_flat
    mul_mv_q4_0_f32_1d_8x_flat
    mul_mv_q4_0_f32_1d_16x_flat
    mul_mv_q6_k
    mul
    norm
    relu
    rms_norm
    rope
    scale
    silu
    softmax_4_f32
    softmax_4_f16
    softmax_f32
    softmax_f16
    transpose
)

foreach (K ${GGML_OPENCL_KERNELS})
    ggml_opencl_add_kernel(${K})
endforeach()
```

----------------------------------------

TITLE: Creating Visual GGUF Model for LLaVA 1.6
DESCRIPTION: Python command to create the visual GGUF model for LLaVA 1.6 using the convert_image_encoder_to_gguf.py script.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_9

LANGUAGE: sh
CODE:
```
python ./examples/llava/convert_image_encoder_to_gguf.py -m vit --llava-projector vit/llava.projector --output-dir vit --clip-model-is-vision
```

----------------------------------------

TITLE: Setting up LLGuidance Integration in llama.cpp CMake
DESCRIPTION: Configures LLGuidance integration by setting up an external project. This creates a Git-based dependency on LLGuidance, builds it with Cargo, and links it to the main project with platform-specific libraries.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/common/CMakeLists.txt#2025-04-22_snippet_4

LANGUAGE: CMake
CODE:
```
if (LLAMA_LLGUIDANCE)
    include(ExternalProject)
    set(LLGUIDANCE_SRC ${CMAKE_BINARY_DIR}/llguidance/source)
    set(LLGUIDANCE_PATH ${LLGUIDANCE_SRC}/target/release)

    # Set the correct library file extension based on platform
    if (WIN32)
        set(LLGUIDANCE_LIB_NAME "llguidance.lib")
        # Add Windows-specific libraries
        set(LLGUIDANCE_PLATFORM_LIBS
            ws2_32    # Windows Sockets API
            userenv   # For GetUserProfileDirectoryW
            ntdll     # For NT functions
            bcrypt    # For BCryptGenRandom
        )
    else()
        set(LLGUIDANCE_LIB_NAME "libllguidance.a")
        set(LLGUIDANCE_PLATFORM_LIBS "")
    endif()

    ExternalProject_Add(llguidance_ext
        GIT_REPOSITORY https://github.com/guidance-ai/llguidance
        # v0.7.10:
        GIT_TAG 0309d2a6bf40abda35344a362edc71e06d5009f8
        PREFIX ${CMAKE_BINARY_DIR}/llguidance
        SOURCE_DIR ${LLGUIDANCE_SRC}
        BUILD_IN_SOURCE TRUE
        CONFIGURE_COMMAND ""
        BUILD_COMMAND cargo build --release
        INSTALL_COMMAND ""
        BUILD_BYPRODUCTS ${LLGUIDANCE_PATH}/${LLGUIDANCE_LIB_NAME} ${LLGUIDANCE_PATH}/llguidance.h
        UPDATE_COMMAND ""
    )
    target_compile_definitions(${TARGET} PUBLIC LLAMA_USE_LLGUIDANCE)

    add_library(llguidance STATIC IMPORTED)
    set_target_properties(llguidance PROPERTIES IMPORTED_LOCATION ${LLGUIDANCE_PATH}/${LLGUIDANCE_LIB_NAME})
    add_dependencies(llguidance llguidance_ext)

    target_include_directories(${TARGET} PRIVATE ${LLGUIDANCE_PATH})
    # Add platform libraries to the main target
    set(LLAMA_COMMON_EXTRA_LIBS ${LLAMA_COMMON_EXTRA_LIBS} llguidance ${LLGUIDANCE_PLATFORM_LIBS})
endif ()
```

----------------------------------------

TITLE: Configuring CPU Backend Variant Implementation in CMake
DESCRIPTION: CMake function that sets up CPU backend library configuration with platform-specific optimizations and feature detection. Handles both ARM and x86 architectures, configures compiler flags, and manages dependencies like OpenMP and Accelerate framework.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
function(ggml_add_cpu_backend_variant_impl tag_name)
    if (tag_name)
        set(GGML_CPU_NAME ggml-cpu-${tag_name})
    else()
        set(GGML_CPU_NAME ggml-cpu)
    endif()

    ggml_add_backend_library(${GGML_CPU_NAME})

    list (APPEND GGML_CPU_SOURCES
        ggml-cpu/ggml-cpu.c
        ggml-cpu/ggml-cpu.cpp
        ggml-cpu/ggml-cpu-aarch64.cpp
        ggml-cpu/ggml-cpu-aarch64.h
        ggml-cpu/ggml-cpu-hbm.cpp
        ggml-cpu/ggml-cpu-hbm.h
        ggml-cpu/ggml-cpu-quants.c
        ggml-cpu/ggml-cpu-quants.h
        ggml-cpu/ggml-cpu-traits.cpp
        ggml-cpu/ggml-cpu-traits.h
        ggml-cpu/amx/amx.cpp
        ggml-cpu/amx/amx.h
        ggml-cpu/amx/mmq.cpp
        ggml-cpu/amx/mmq.h
        ggml-cpu/ggml-cpu-impl.h
        ggml-cpu/common.h
        ggml-cpu/binary-ops.h
        ggml-cpu/binary-ops.cpp
        ggml-cpu/unary-ops.h
        ggml-cpu/unary-ops.cpp
        ggml-cpu/simd-mappings.h
        ggml-cpu/vec.h
        ggml-cpu/vec.cpp
        ggml-cpu/ops.h
        ggml-cpu/ops.cpp
        )

    target_compile_features(${GGML_CPU_NAME} PRIVATE c_std_11 cxx_std_17)
    target_include_directories(${GGML_CPU_NAME} PRIVATE . ggml-cpu)

    if (APPLE AND GGML_ACCELERATE)
        find_library(ACCELERATE_FRAMEWORK Accelerate)
        if (ACCELERATE_FRAMEWORK)
            message(STATUS "Accelerate framework found")

            target_compile_definitions(${GGML_CPU_NAME} PRIVATE GGML_USE_ACCELERATE)
            target_compile_definitions(${GGML_CPU_NAME} PRIVATE ACCELERATE_NEW_LAPACK)
            target_compile_definitions(${GGML_CPU_NAME} PRIVATE ACCELERATE_LAPACK_ILP64)

            target_link_libraries(${GGML_CPU_NAME} PRIVATE ${ACCELERATE_FRAMEWORK})
        else()
            message(WARNING "Accelerate framework not found")
        endif()
    endif()

    if (GGML_OPENMP)
        find_package(OpenMP)
        if (OpenMP_FOUND)
            target_compile_definitions(${GGML_CPU_NAME} PRIVATE GGML_USE_OPENMP)

            target_link_libraries(${GGML_CPU_NAME} PRIVATE OpenMP::OpenMP_C OpenMP::OpenMP_CXX)
        else()
            message(WARNING "OpenMP not found")
        endif()
    endif()

    if (GGML_LLAMAFILE)
        target_compile_definitions(${GGML_CPU_NAME} PRIVATE GGML_USE_LLAMAFILE)

        list(APPEND GGML_CPU_SOURCES
                    ggml-cpu/llamafile/sgemm.cpp
                    ggml-cpu/llamafile/sgemm.h)
    endif()

    if (GGML_CPU_HBM)
        find_library(memkind memkind REQUIRED)

        message(STATUS "Using memkind for CPU HBM")

        target_compile_definitions(${GGML_CPU_NAME} PRIVATE GGML_USE_CPU_HBM)

        target_link_libraries(${GGML_CPU_NAME} PUBLIC memkind)
    endif()

    if (CMAKE_OSX_ARCHITECTURES      STREQUAL "arm64" OR
        CMAKE_GENERATOR_PLATFORM_LWR STREQUAL "arm64" OR
        (NOT CMAKE_OSX_ARCHITECTURES AND NOT CMAKE_GENERATOR_PLATFORM_LWR AND
            CMAKE_SYSTEM_PROCESSOR MATCHES "^(aarch64|arm.*|ARM64)$"))

        message(STATUS "ARM detected")

        if (MSVC AND NOT CMAKE_C_COMPILER_ID STREQUAL "Clang")
            message(FATAL_ERROR "MSVC is not supported for ARM, use clang")
        else()
            check_cxx_compiler_flag(-mfp16-format=ieee GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E)
            if (NOT "${GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E}" STREQUAL "")
                list(APPEND ARCH_FLAGS -mfp16-format=ieee)
            endif()

            if (GGML_NATIVE)
                execute_process(
                    COMMAND ${CMAKE_C_COMPILER} -mcpu=native -E -v -
                    INPUT_FILE "/dev/null"
                    OUTPUT_QUIET
                    ERROR_VARIABLE ARM_MCPU
                    RESULT_VARIABLE ARM_MCPU_RESULT
                )
                if (NOT ARM_MCPU_RESULT)
                    string(REGEX MATCH "-mcpu=[^ ']+" ARM_MCPU_FLAG "${ARM_MCPU}")
                endif()
                if ("${ARM_MCPU_FLAG}" STREQUAL "")
                    set(ARM_MCPU_FLAG -mcpu=native)
                    message(STATUS "ARM -mcpu not found, -mcpu=native will be used")
                endif()

                include(CheckCXXSourceRuns)

                function(check_arm_feature tag code)
                    set(CMAKE_REQUIRED_FLAGS_SAVE ${CMAKE_REQUIRED_FLAGS})
                    set(CMAKE_REQUIRED_FLAGS "${ARM_MCPU_FLAG}+${tag}")
                    check_cxx_source_runs("${code}" GGML_MACHINE_SUPPORTS_${tag})
                    if (GGML_MACHINE_SUPPORTS_${tag})
                        set(ARM_MCPU_FLAG_FIX "${ARM_MCPU_FLAG_FIX}+${tag}" PARENT_SCOPE)
                    else()
                        set(CMAKE_REQUIRED_FLAGS "${ARM_MCPU_FLAG}+no${tag}")
                        check_cxx_source_compiles("int main() { return 0; }" GGML_MACHINE_SUPPORTS_no${tag})
                        if (GGML_MACHINE_SUPPORTS_no${tag})
                            set(ARM_MCPU_FLAG_FIX "${ARM_MCPU_FLAG_FIX}+no${tag}" PARENT_SCOPE)
                        endif()
                    endif()
                    set(CMAKE_REQUIRED_FLAGS ${CMAKE_REQUIRED_FLAGS_SAVE})
                endfunction()

                check_arm_feature(dotprod "#include <arm_neon.h>\nint main() { int8x16_t _a, _b; volatile int32x4_t _s = vdotq_s32(_s, _a, _b); return 0; }")
                check_arm_feature(i8mm    "#include <arm_neon.h>\nint main() { int8x16_t _a, _b; volatile int32x4_t _s = vmmlaq_s32(_s, _a, _b); return 0; }")
                check_arm_feature(sve     "#include <arm_sve.h>\nint main()  { svfloat32_t _a, _b; volatile svfloat32_t _c = svadd_f32_z(svptrue_b8(), _a, _b); return 0; }")
                check_arm_feature(sme     "#include <arm_sme.h>\n__arm_locally_streaming int main() { __asm__ volatile(\"smstart; smstop;\"); return 0; }")

                list(APPEND ARCH_FLAGS "${ARM_MCPU_FLAG}${ARM_MCPU_FLAG_FIX}")
            else()
                if (GGML_CPU_ARM_ARCH)
                    list(APPEND ARCH_FLAGS -march=${GGML_CPU_ARM_ARCH})
                endif()
            endif()

            if (CMAKE_HOST_SYSTEM_NAME STREQUAL "Windows")
                set(FEAT_INPUT_FILE "NUL")
            else()
                set(FEAT_INPUT_FILE "/dev/null")
            endif()

            execute_process(
                COMMAND ${CMAKE_C_COMPILER} ${ARCH_FLAGS} -dM -E -
                INPUT_FILE ${FEAT_INPUT_FILE}
                OUTPUT_VARIABLE ARM_FEATURE
                RESULT_VARIABLE ARM_FEATURE_RESULT
            )
            if (ARM_FEATURE_RESULT)
                message(WARNING "Failed to get ARM features")
            else()
                foreach(feature DOTPROD SVE MATMUL_INT8 FMA FP16_VECTOR_ARITHMETIC SME)
                    string(FIND "${ARM_FEATURE}" "__ARM_FEATURE_${feature} 1" feature_pos)
                    if (NOT ${feature_pos} EQUAL -1)
                        message(STATUS "ARM feature ${feature} enabled")
                    endif()
                endforeach()
            endif()
        endif()
    elseif (CMAKE_OSX_ARCHITECTURES STREQUAL "x86_64" OR CMAKE_GENERATOR_PLATFORM_LWR MATCHES "^(x86_64|i686|amd64|x64|win32)$" OR
            (NOT CMAKE_OSX_ARCHITECTURES AND NOT CMAKE_GENERATOR_PLATFORM_LWR AND
            CMAKE_SYSTEM_PROCESSOR MATCHES "^(x86_64|i686|AMD64|amd64)$"))

        message(STATUS "x86 detected")

        if (MSVC)
            if (GGML_NATIVE)
                include(ggml-cpu/cmake/FindSIMD.cmake)
            endif ()
            if (GGML_AVX512)
                list(APPEND ARCH_FLAGS /arch:AVX512)
                list(APPEND ARCH_DEFINITIONS GGML_AVX512)
                if (GGML_AVX512_VBMI)
                    list(APPEND ARCH_DEFINITIONS __AVX512VBMI__)
                    if (CMAKE_C_COMPILER_ID STREQUAL "Clang")
                        list(APPEND ARCH_FLAGS -mavx512vbmi)
                    endif()
                endif()
                if (GGML_AVX512_VNNI)
                    list(APPEND ARCH_DEFINITIONS __AVX512VNNI__ GGML_AVX512_VNNI)
                    if (CMAKE_C_COMPILER_ID STREQUAL "Clang")
                        list(APPEND ARCH_FLAGS -mavx512vnni)
                    endif()
                endif()
                if (GGML_AVX512_BF16)
                    list(APPEND ARCH_DEFINITIONS __AVX512BF16__ GGML_AVX512_BF16)
                    if (CMAKE_C_COMPILER_ID STREQUAL "Clang")
                        list(APPEND ARCH_FLAGS -mavx512bf16)
                    endif()
                endif()
                if (GGML_AMX_TILE)
                    list(APPEND ARCH_DEFINITIONS __AMX_TILE__ GGML_AMX_TILE)
                endif()
                if (GGML_AMX_INT8)
                    list(APPEND ARCH_DEFINITIONS __AMX_INT8__ GGML_AMX_INT8)
                endif()
                if (GGML_AMX_BF16)
                    list(APPEND ARCH_DEFINITIONS __AMX_BF16__ GGML_AMX_BF16)
                endif()
            elseif (GGML_AVX2)
                list(APPEND ARCH_FLAGS /arch:AVX2)
                list(APPEND ARCH_DEFINITIONS GGML_AVX2 GGML_FMA GGML_F16C)
            elseif (GGML_AVX)
                list(APPEND ARCH_FLAGS /arch:AVX)
                list(APPEND ARCH_DEFINITIONS GGML_AVX)
            elseif (GGML_SSE42)
                list(APPEND ARCH_FLAGS /arch:SSE4.2)
                list(APPEND ARCH_DEFINITIONS GGML_SSE42)
            endif()
            if (GGML_AVX_VNNI)
                list(APPEND ARCH_DEFINITIONS __AVXVNNI__ GGML_AVX_VNNI)
            endif()
```

----------------------------------------

TITLE: Configuring PyTorch CPU Dependencies
DESCRIPTION: Requirements file that specifies PyTorch version 2.2.1 for CPU and includes additional requirements from a separate file for legacy Llama conversion. Uses PyTorch's custom package index for CPU-specific wheels.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-convert_hf_to_gguf.txt#2025-04-22_snippet_0

LANGUAGE: txt
CODE:
```
-r ./requirements-convert_legacy_llama.txt
--extra-index-url https://download.pytorch.org/whl/cpu
torch~=2.2.1
```

----------------------------------------

TITLE: Setting ROCM_PATH in CMake for HIP/ROCm Configuration
DESCRIPTION: Determines the ROCM_PATH based on environment variables or default locations. This path is crucial for locating HIP and ROCm libraries.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-hip/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
if (NOT EXISTS $ENV{ROCM_PATH})
    if (NOT EXISTS /opt/rocm)
        set(ROCM_PATH /usr)
    else()
        set(ROCM_PATH /opt/rocm)
    endif()
else()
    set(ROCM_PATH $ENV{ROCM_PATH})
endif()

list(APPEND CMAKE_PREFIX_PATH  ${ROCM_PATH})
list(APPEND CMAKE_PREFIX_PATH "${ROCM_PATH}/lib64/cmake")
```

----------------------------------------

TITLE: MUSA Runtime Device Configuration
DESCRIPTION: Example of using MUSA environmental variables to control GPU device visibility at runtime.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_12

LANGUAGE: bash
CODE:
```
MUSA_VISIBLE_DEVICES="-0" ./build/bin/llama-server --model /srv/models/llama.gguf
```

----------------------------------------

TITLE: Installing BLIS System-wide
DESCRIPTION: Installs the compiled BLIS framework to the system directory.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/BLIS.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
sudo make install
```

----------------------------------------

TITLE: Verifying SYCL Installation on Windows
DESCRIPTION: Command to list available SYCL devices on Windows after oneAPI installation.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_17

LANGUAGE: sh
CODE:
```
sycl-ls.exe
```

----------------------------------------

TITLE: Configuring Tokenizer Tests for Various Model Vocabularies in llama.cpp
DESCRIPTION: Creates a tokenizer test executable and registers multiple tests using it with different model vocabulary files. Each test uses the same executable but with different GGUF vocabulary model files as arguments.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/tests/CMakeLists.txt#2025-04-22_snippet_2

LANGUAGE: CMake
CODE:
```
add_executable(test-tokenizer-0 test-tokenizer-0.cpp)
target_link_libraries(test-tokenizer-0 PRIVATE common)
install(TARGETS test-tokenizer-0 RUNTIME)

llama_test(test-tokenizer-0 NAME test-tokenizer-0-bert-bge          ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-bert-bge.gguf)
llama_test(test-tokenizer-0 NAME test-tokenizer-0-command-r         ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-command-r.gguf)
llama_test(test-tokenizer-0 NAME test-tokenizer-0-deepseek-coder    ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-deepseek-coder.gguf)
llama_test(test-tokenizer-0 NAME test-tokenizer-0-deepseek-llm      ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-deepseek-llm.gguf)
llama_test(test-tokenizer-0 NAME test-tokenizer-0-falcon            ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-falcon.gguf)
llama_test(test-tokenizer-0 NAME test-tokenizer-0-gpt-2             ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-gpt-2.gguf)
llama_test(test-tokenizer-0 NAME test-tokenizer-0-llama-bpe         ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-llama-bpe.gguf)
llama_test(test-tokenizer-0 NAME test-tokenizer-0-llama-spm         ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-llama-spm.gguf)
llama_test(test-tokenizer-0 NAME test-tokenizer-0-mpt               ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-mpt.gguf)
llama_test(test-tokenizer-0 NAME test-tokenizer-0-phi-3             ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-phi-3.gguf)
llama_test(test-tokenizer-0 NAME test-tokenizer-0-qwen2             ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-qwen2.gguf)
llama_test(test-tokenizer-0 NAME test-tokenizer-0-refact            ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-refact.gguf)
llama_test(test-tokenizer-0 NAME test-tokenizer-0-starcoder         ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-starcoder.gguf)
```

----------------------------------------

TITLE: Setting up Node.js Client Directory
DESCRIPTION: Commands to create and navigate to a new directory for the llama client implementation
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
mkdir llama-client
cd llama-client
```

----------------------------------------

TITLE: Project Contributors
DESCRIPTION: List of contributors to the project.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_16

LANGUAGE: sh
CODE:
```
zhangjidong05, yangyang260, huyiming03, chenxiaotao03, ZiangWu-77
```

----------------------------------------

TITLE: Loading Local llama.cpp Library in Android Project
DESCRIPTION: Adds the local llama.cpp source directory to the build process, allowing the Android project to compile and link against it.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama.android/llama/src/main/cpp/CMakeLists.txt#2025-04-22_snippet_2

LANGUAGE: cmake
CODE:
```
add_subdirectory(../../../../../../ build-llama)
```

----------------------------------------

TITLE: Ascend SOC Type Detection Function
DESCRIPTION: Function to automatically detect Ascend SOC type using npu-smi tool. Fails build if detection unsuccessful.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cann/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: cmake
CODE:
```
function(detect_ascend_soc_type SOC_VERSION)
    execute_process(
        COMMAND bash -c "npu-smi info|awk -F' ' 'NF > 0 && NR==7 {print $3}'"
        OUTPUT_VARIABLE npu_info
        RESULT_VARIABLE npu_result
        OUTPUT_STRIP_TRAILING_WHITESPACE
    )
    if("${npu_info}" STREQUAL "" OR ${npu_result})
        message(FATAL_ERROR "Auto-detech ascend soc type failed, please specify manually or check ascend device working normally.")
    endif()
    set(${SOC_VERSION} "Ascend${npu_info}" PARENT_SCOPE)
endfunction()
```

----------------------------------------

TITLE: Container Management
DESCRIPTION: Command for restarting containers when troubleshooting NVIDIA driver issues
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CUDA-FEDORA.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
podman container restart --all
```

----------------------------------------

TITLE: Configuring llama-simple Executable Build in CMake
DESCRIPTION: Sets up the build configuration for the llama-simple executable which compiles simple.cpp. It links against the llama library and pthread, requires C++17 standard, and configures installation of the runtime binary.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/simple/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(TARGET llama-simple)
add_executable(${TARGET} simple.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Specifying Python Package Dependencies for Legacy LLAMA Conversion
DESCRIPTION: This requirements file lists the necessary Python packages and their versions for converting legacy LLAMA models. It includes Pillow for image processing, PyTorch for deep learning operations, and TorchVision for computer vision tasks. The file also references an external requirements file for additional dependencies.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llava/requirements.txt#2025-04-22_snippet_0

LANGUAGE: Text
CODE:
```
-r ../../requirements/requirements-convert_legacy_llama.txt
--extra-index-url https://download.pytorch.org/whl/cpu
pillow~=10.2.0
torch~=2.2.1
torchvision~=0.17.1
```

----------------------------------------

TITLE: Configuring llama-eval-callback Executable Build and Test in CMake
DESCRIPTION: This CMake configuration sets up the llama-eval-callback executable, linking it with necessary libraries and setting C++17 as the standard. It also configures a test that runs the executable with a small LLaMA model from the Hugging Face repository.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/eval-callback/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
set(TARGET llama-eval-callback)
add_executable(${TARGET} eval-callback.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)

set(TEST_TARGET test-eval-callback)
add_test(NAME ${TEST_TARGET}
        COMMAND llama-eval-callback --hf-repo ggml-org/models --hf-file tinyllamas/stories260K.gguf --model stories260K.gguf --prompt hello --seed 42 -ngl 0)
set_property(TEST ${TEST_TARGET} PROPERTY LABELS eval-callback curl)
```

----------------------------------------

TITLE: Creating Kanji Mnemonics in Markdown
DESCRIPTION: This snippet demonstrates the format used for creating mnemonics for kanji characters. It includes the kanji, its meaning, components, and a mnemonic that incorporates the meanings of the components.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/prompts/mnemonics.txt#2025-04-22_snippet_0

LANGUAGE: Markdown
CODE:
```
Kanji: æ¬  (lack of)
Components: ð ‚Š (hook claw), äºº (person)
Mnemonic: This **person** is a pirate. He lost his hand to a crocodile many years ago. Nowadays, the ***lack of*** a hand does not bother him too much. In fact, the **hook claw** that replaces it is the mark of a true pirate, so he is quite proud of it!
```

----------------------------------------

TITLE: Example GDB Command for Tokenizer Test
DESCRIPTION: This is a concrete example of running GDB with the test-tokenizer-0 binary and the llama-spm vocabulary model. It shows how to apply the abstract GDB command with actual file paths.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/debugging-tests.md#2025-04-22_snippet_7

LANGUAGE: bash
CODE:
```
gdb --args ~/llama.cpp/build-ci-debug/bin/test-tokenizer-0 "~/llama.cpp/tests/../models/ggml-vocab-llama-spm.gguf"
```

----------------------------------------

TITLE: Creating C Test Executable for llama.cpp
DESCRIPTION: Creates a test executable from a C source file, linking it with the llama library. This is marked as a dummy executable that won't be installed.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/tests/CMakeLists.txt#2025-04-22_snippet_5

LANGUAGE: CMake
CODE:
```
# dummy executable - not installed
get_filename_component(TEST_TARGET test-c.c NAME_WE)
add_executable(${TEST_TARGET} test-c.c)
target_link_libraries(${TEST_TARGET} PRIVATE llama)
```

----------------------------------------

TITLE: Setting Platform-Specific Default Options
DESCRIPTION: Configures platform-specific defaults for Metal and BLAS backends, with special handling for Apple platforms.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_4

LANGUAGE: CMake
CODE:
```
if (APPLE)
    set(GGML_METAL_DEFAULT ON)
    set(GGML_BLAS_DEFAULT ON)
    set(GGML_BLAS_VENDOR_DEFAULT "Apple")
else()
    set(GGML_METAL_DEFAULT OFF)
    set(GGML_BLAS_DEFAULT OFF)
    set(GGML_BLAS_VENDOR_DEFAULT "Generic")
endif()
```

----------------------------------------

TITLE: Setting MUSA Path in CMake for llama.cpp
DESCRIPTION: Determines the MUSA installation path based on environment variables or default locations. Sets the C and C++ compilers to use MUSA's clang and clang++.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-musa/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
if (NOT EXISTS $ENV{MUSA_PATH})
    if (NOT EXISTS /opt/musa)
        set(MUSA_PATH /usr/local/musa)
    else()
        set(MUSA_PATH /opt/musa)
    endif()
else()
    set(MUSA_PATH $ENV{MUSA_PATH})
endif()

set(CMAKE_C_COMPILER "${MUSA_PATH}/bin/clang")
set(CMAKE_C_EXTENSIONS OFF)
set(CMAKE_CXX_COMPILER "${MUSA_PATH}/bin/clang++")
set(CMAKE_CXX_EXTENSIONS OFF)

list(APPEND CMAKE_MODULE_PATH "${MUSA_PATH}/cmake")

find_package(MUSAToolkit)
```

----------------------------------------

TITLE: Configuring llama-embedding CMake Target
DESCRIPTION: Sets up the CMake build configuration for the llama-embedding executable. Links against common and llama libraries, requires C++17, and configures installation settings.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/embedding/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
set(TARGET llama-embedding)
add_executable(${TARGET} embedding.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Referencing Legacy Llama Conversion Requirements
DESCRIPTION: References an external requirements file that contains dependencies needed for converting legacy llama models. Uses relative path reference to another requirements file.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-convert_llama_ggml_to_gguf.txt#2025-04-22_snippet_0

LANGUAGE: text
CODE:
```
-r ./requirements-convert_legacy_llama.txt
```

----------------------------------------

TITLE: Example Output of Default Verification
DESCRIPTION: This is the output of verifying a GGUF file against its manifest using the default SHA-256 hash algorithm. It shows the verification status for each tensor and the overall file.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-hash/README.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
manifest  test.gguf.manifest  sha256  sha1  xxh64
sha256    c0510d38fa060c46265e0160a85c7243096b01dd31c2f355bdbb5516b20de1bd  test.gguf:tensor_0  -  Ok
sha256    8514cbcc73692a2c56bd7a33a022edd5ff819614bd23b19915d7224387f397a7  test.gguf:tensor_1  -  Ok
sha256    947e6b36e20f2cc95e1d2ce1c1669d813d574657ac6b5ac5196158d454d35180  test.gguf:tensor_2  -  Ok
sha256    423b044e016d8ac73c39f23f60bf01bedef5ecb03c0230accd824c91fe86f1a1  test.gguf:tensor_3  -  Ok
sha256    79737cb3912d4201384cf7f16a1a37ff7823f23ea796cb205b6ca361ab9e3ebf  test.gguf:tensor_4  -  Ok
sha256    60949be8298eced0ecdde64487643d018407bd261691e061d9e9c3dbc9fd358b  test.gguf:tensor_5  -  Ok
sha256    574f4c46ff384a3b9a225eb955d2a871847a2e8b3fa59387a8252832e92ef7b0  test.gguf:tensor_6  -  Ok
sha256    4c0410cd3c500f078ae5b21e8dc9eb79e29112713b2ab58a882f82a3868d4d75  test.gguf:tensor_7  -  Ok
sha256    c4401313feeba0261275c3b25bd2d8fe40ce04e0f440c2980ed0e9674c30ff01  test.gguf:tensor_8  -  Ok
sha256    23d57cf0d7a6e90b0b3616b41300e0cd354781e812add854a5f95aa55f2bc514  test.gguf:tensor_9  -  Ok
sha256    7dd641b32f59b60dbd4b5420c4b0f6321ccf48f58f6ae201a3dbc4a58a27c6e4  test.gguf  -  Ok

Verification results for test.gguf.manifest - Success
```

----------------------------------------

TITLE: Accessing SimpleChat Frontend
DESCRIPTION: URL format to access the SimpleChat web interface through local browser
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/public_simplechat/readme.md#2025-04-22_snippet_2

LANGUAGE: http
CODE:
```
http://127.0.0.1:PORT/index.html
```

----------------------------------------

TITLE: Uploading Package Distribution
DESCRIPTION: Command to upload the built distribution archives to PyPI
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/gguf-py/README.md#2025-04-22_snippet_8

LANGUAGE: sh
CODE:
```
python -m twine upload dist/*
```

----------------------------------------

TITLE: Installing GGUF with GUI Support
DESCRIPTION: Installation command for GGUF package with additional GUI editor functionality
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/gguf-py/README.md#2025-04-22_snippet_1

LANGUAGE: sh
CODE:
```
pip install gguf[gui]
```

----------------------------------------

TITLE: Configuring MUSA Architectures and Source Files for llama.cpp
DESCRIPTION: Sets up MUSA architectures, defines MUSA-specific header and source files, and configures compilation flags for MUSA sources. Includes conditional compilation for different quantization options.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-musa/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: CMake
CODE:
```
if (MUSAToolkit_FOUND)
    message(STATUS "MUSA Toolkit found")

    if (NOT DEFINED MUSA_ARCHITECTURES)
        set(MUSA_ARCHITECTURES "21;22;31")
    endif()
    message(STATUS "Using MUSA architectures: ${MUSA_ARCHITECTURES}")

    file(GLOB   GGML_HEADERS_MUSA "../ggml-cuda/*.cuh")
    list(APPEND GGML_HEADERS_MUSA "../../include/ggml-cuda.h")

    file(GLOB   GGML_SOURCES_MUSA "../ggml-cuda/*.cu")
    file(GLOB   SRCS "../ggml-cuda/template-instances/fattn-mma*.cu")
    list(APPEND GGML_SOURCES_MUSA ${SRCS})
    file(GLOB   SRCS "../ggml-cuda/template-instances/mmq*.cu")
    list(APPEND GGML_SOURCES_MUSA ${SRCS})

    if (GGML_CUDA_FA_ALL_QUANTS)
        file(GLOB   SRCS "../ggml-cuda/template-instances/fattn-vec*.cu")
        list(APPEND GGML_SOURCES_MUSA ${SRCS})
        add_compile_definitions(GGML_CUDA_FA_ALL_QUANTS)
    else()
        file(GLOB   SRCS "../ggml-cuda/template-instances/fattn-vec*q4_0-q4_0.cu")
        list(APPEND GGML_SOURCES_MUSA ${SRCS})
        file(GLOB   SRCS "../ggml-cuda/template-instances/fattn-vec*q8_0-q8_0.cu")
        list(APPEND GGML_SOURCES_MUSA ${SRCS})
        file(GLOB   SRCS "../ggml-cuda/template-instances/fattn-vec*f16-f16.cu")
        list(APPEND GGML_SOURCES_MUSA ${SRCS})
    endif()

    set_source_files_properties(${GGML_SOURCES_MUSA} PROPERTIES LANGUAGE CXX)
    foreach(SOURCE ${GGML_SOURCES_MUSA})
        set(COMPILE_FLAGS "-fsigned-char -x musa -mtgpu")
        foreach(ARCH ${MUSA_ARCHITECTURES})
            set(COMPILE_FLAGS "${COMPILE_FLAGS} --cuda-gpu-arch=mp_${ARCH}")
        endforeach()
        set_property(SOURCE ${SOURCE} PROPERTY COMPILE_FLAGS ${COMPILE_FLAGS})
    endforeach()
```

----------------------------------------

TITLE: Configuring llama-speculative Build in CMake
DESCRIPTION: Sets up the build configuration for the llama-speculative executable. It defines the target, specifies the source file, sets up installation, links required libraries including common, llama, and threading libraries, and sets the C++ standard to C++17.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/speculative/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(TARGET llama-speculative)
add_executable(${TARGET} speculative.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Upgrading Pip
DESCRIPTION: Command to upgrade pip to the latest version to support editable installation
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/gguf-py/README.md#2025-04-22_snippet_3

LANGUAGE: sh
CODE:
```
pip install --upgrade pip
```

----------------------------------------

TITLE: Configuring CMake Build System for llama.cpp Examples
DESCRIPTION: This CMake configuration file manages the build process for multiple examples in the llama.cpp project. It sets up required dependencies like Threads, adds compile flags, and conditionally includes subdirectories based on the target platform and build configuration. The file differentiates between Emscripten, Windows, and other platforms, and handles special cases for certain examples.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
# dependencies

find_package(Threads REQUIRED)

# third-party

# ...

# flags

llama_add_compile_flags()

# examples

include_directories(${CMAKE_CURRENT_SOURCE_DIR})

if (EMSCRIPTEN)
else()
    add_subdirectory(batched-bench)
    add_subdirectory(batched)
    add_subdirectory(embedding)
    add_subdirectory(eval-callback)

    if (NOT WIN32)
        # disabled on Windows because it uses internal functions not exported with LLAMA_API
        add_subdirectory(gbnf-validator)
    endif()

    add_subdirectory(gguf-hash)
    add_subdirectory(gguf-split)
    add_subdirectory(gguf)
    add_subdirectory(gritlm)
    add_subdirectory(imatrix)
    add_subdirectory(infill)
    add_subdirectory(llama-bench)
    add_subdirectory(lookahead)
    add_subdirectory(lookup)
    add_subdirectory(main)
    add_subdirectory(parallel)
    add_subdirectory(passkey)
    add_subdirectory(perplexity)
    add_subdirectory(quantize)
    add_subdirectory(retrieval)
    if (LLAMA_BUILD_SERVER)
        add_subdirectory(server)
    endif()
    add_subdirectory(save-load-state)
    add_subdirectory(run)
    add_subdirectory(simple)
    add_subdirectory(simple-chat)
    add_subdirectory(speculative)
    add_subdirectory(speculative-simple)
    add_subdirectory(tokenize)
    add_subdirectory(tts)
    add_subdirectory(gen-docs)
    if (NOT GGML_BACKEND_DL)
        # these examples use the backends directly and cannot be built with dynamic loading
        add_subdirectory(convert-llama2c-to-ggml)
        add_subdirectory(cvector-generator)
        add_subdirectory(export-lora)
        if (NOT WIN32)
            # disabled on Windows because it uses internal functions not exported with LLAMA_API
            add_subdirectory(quantize-stats)
        endif()
        add_subdirectory(llava)
        if (GGML_RPC)
            add_subdirectory(rpc)
        endif()
        if (GGML_SYCL)
            add_subdirectory(sycl)
        endif()
    endif()
endif()
```

----------------------------------------

TITLE: Setting Additional Default Values for GGML Features
DESCRIPTION: Sets default values for additional GGML features if they haven't been defined elsewhere.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_6

LANGUAGE: CMake
CODE:
```
# defaults
if (NOT GGML_LLAMAFILE_DEFAULT)
    set(GGML_LLAMAFILE_DEFAULT OFF)
endif()

if (NOT GGML_CUDA_GRAPHS_DEFAULT)
    set(GGML_CUDA_GRAPHS_DEFAULT OFF)
endif()
```

----------------------------------------

TITLE: Pushing Git Tags
DESCRIPTION: Git command to push tags to remote repository
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/gguf-py/README.md#2025-04-22_snippet_5

LANGUAGE: sh
CODE:
```
git push origin --tags
```

----------------------------------------

TITLE: Configuring and Building llama-tokenize Executable with CMake
DESCRIPTION: Defines the llama-tokenize target from tokenize.cpp source file, configures its installation, links required libraries including common, llama, and thread libraries, and sets C++17 as the required standard.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tokenize/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(TARGET llama-tokenize)
add_executable(${TARGET} tokenize.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Installing OpenCL Headers and ICD Loader for Android
DESCRIPTION: Commands to install OpenCL headers and ICD loader library for Android development with NDK.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_25

LANGUAGE: sh
CODE:
```
mkdir -p ~/dev/llm
cd ~/dev/llm

git clone https://github.com/KhronosGroup/OpenCL-Headers && \
cd OpenCL-Headers && \
cp -r CL $ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include

cd ~/dev/llm

git clone https://github.com/KhronosGroup/OpenCL-ICD-Loader && \
cd OpenCL-ICD-Loader && \
mkdir build_ndk && cd build_ndk && \
cmake .. -G Ninja -DCMAKE_BUILD_TYPE=Release \
  -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \
  -DOPENCL_ICD_LOADER_HEADERS_DIR=$ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include \
  -DANDROID_ABI=arm64-v8a \
  -DANDROID_PLATFORM=24 \
  -DANDROID_STL=c++_shared && \
ninja && \
cp libOpenCL.so $ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/lib/aarch64-linux-android
```

----------------------------------------

TITLE: Sample Output from Model Inference Callback
DESCRIPTION: Example output produced by the callback during model inference. It shows tensor operations and their data values, including embedding lookups, normalization, attention mechanisms, and tensor transformations.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/eval-callback/README.md#2025-04-22_snippet_1

LANGUAGE: shell
CODE:
```
llm_load_tensors: offloaded 33/33 layers to GPU
...
llama_new_context_with_model: n_ctx      = 512
...
llama_new_context_with_model:      CUDA0 compute buffer size =   105.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     6.01 MiB
llama_new_context_with_model: graph nodes  = 1225
llama_new_context_with_model: graph splits = 2
ggml_debug:                 inp_embd = (f32)   GET_ROWS(token_embd.weight{2560, 51200, 1, 1}, inp_tokens{1, 1, 1, 1}}) = {2560, 1, 1, 1}
                                     [
                                      [
                                       [ -0.0181,   0.0272,   0.0272, ...],
                                      ],
                                     ]
ggml_debug:                   norm-0 = (f32)       NORM(CUDA0#inp_embd#0{2560, 1, 1, 1}, }) = {2560, 1, 1, 1}
                                     [
                                      [
                                       [ -0.6989,   1.0636,   1.0636, ...],
                                      ],
                                     ]
ggml_debug:                 norm_w-0 = (f32)        MUL(norm-0{2560, 1, 1, 1}, blk.0.attn_norm.weight{2560, 1, 1, 1}}) = {2560, 1, 1, 1}
                                     [
                                      [
                                       [ -0.1800,   0.2817,   0.2632, ...],
                                      ],
                                     ]
ggml_debug:              attn_norm-0 = (f32)        ADD(norm_w-0{2560, 1, 1, 1}, blk.0.attn_norm.bias{2560, 1, 1, 1}}) = {2560, 1, 1, 1}
                                     [
                                      [
                                       [ -0.1863,   0.2970,   0.2604, ...],
                                      ],
                                     ]
ggml_debug:                   wqkv-0 = (f32)    MUL_MAT(blk.0.attn_qkv.weight{2560, 7680, 1, 1}, attn_norm-0{2560, 1, 1, 1}}) = {7680, 1, 1, 1}
                                     [
                                      [
                                       [ -1.1238,   1.2876,  -1.8086, ...],
                                      ],
                                     ]
ggml_debug:                   bqkv-0 = (f32)        ADD(wqkv-0{7680, 1, 1, 1}, blk.0.attn_qkv.bias{7680, 1, 1, 1}}) = {7680, 1, 1, 1}
                                     [
                                      [
                                       [ -1.1135,   1.4604,  -1.9226, ...],
                                      ],
                                     ]
ggml_debug:            bqkv-0 (view) = (f32)       VIEW(bqkv-0{7680, 1, 1, 1}, }) = {2560, 1, 1, 1}
                                     [
                                      [
                                       [ -1.1135,   1.4604,  -1.9226, ...],
                                      ],
                                     ]
ggml_debug:                   Qcur-0 = (f32)       CONT(bqkv-0 (view){2560, 1, 1, 1}, }) = {2560, 1, 1, 1}
                                     [
                                      [
                                       [ -1.1135,   1.4604,  -1.9226, ...],
                                      ],
                                     ]
ggml_debug:        Qcur-0 (reshaped) = (f32)    RESHAPE(Qcur-0{2560, 1, 1, 1}, }) = {80, 32, 1, 1}
                                     [
                                      [
                                       [ -1.1135,   1.4604,  -1.9226, ...],
                                       [ -0.3608,   0.5076,  -1.8866, ...],
                                       [  1.7643,   0.0273,  -2.1065, ...],
                                       ...
                                      ],
                                     ]
ggml_debug:                   Qcur-0 = (f32)       ROPE(Qcur-0 (reshaped){80, 32, 1, 1}, CUDA0#inp_pos#0{1, 1, 1, 1}}) = {80, 32, 1, 1}
                                     [
                                      [
                                       [ -1.1135,   1.4604,  -1.9226, ...],
                                       [ -0.3608,   0.5076,  -1.8866, ...],
                                       [  1.7643,   0.0273,  -2.1065, ...],
                                       ...
                                      ],
                                     ]
```

----------------------------------------

TITLE: Installing Publishing Dependencies
DESCRIPTION: Command to install required packages for manual publishing
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/gguf-py/README.md#2025-04-22_snippet_6

LANGUAGE: sh
CODE:
```
pip install build twine
```

----------------------------------------

TITLE: Configuring Standalone Mode for GGML
DESCRIPTION: Determines if GGML is being built as a standalone project or integrated as a subproject. Sets the output directory for binaries in standalone mode.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: CMake
CODE:
```
if (CMAKE_SOURCE_DIR STREQUAL CMAKE_CURRENT_SOURCE_DIR)
    set(GGML_STANDALONE ON)

    set(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)

    # configure project version
    # TODO
else()
    set(GGML_STANDALONE OFF)
endif()
```

----------------------------------------

TITLE: Vulkan Build Configuration for Windows
DESCRIPTION: CMake commands for building llama.cpp with Vulkan support on Windows.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_15

LANGUAGE: bash
CODE:
```
cmake -B build -DGGML_VULKAN=ON
cmake --build build --config Release
```

----------------------------------------

TITLE: Creating Local Directories for MUSA CI Docker Setup
DESCRIPTION: Commands to create local directories for storing cached models, configuration files, virtual environments, and CI run results when running MUSA CI in a Docker container.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ci/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
mkdir -p $HOME/llama.cpp/ci-cache
```

----------------------------------------

TITLE: Aggregating Python Dependencies for llama.cpp Project
DESCRIPTION: This requirements file imports multiple specialized requirements files for different components of the llama.cpp project, including examples, server components, conversion utilities, benchmarking tools, and GUI applications.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-all.txt#2025-04-22_snippet_0

LANGUAGE: plaintext
CODE:
```
-r ../examples/llava/requirements.txt
-r ../examples/server/bench/requirements.txt
-r ../examples/server/tests/requirements.txt

-r ./requirements-compare-llama-bench.txt
-r ./requirements-pydantic.txt
-r ./requirements-test-tokenizer-random.txt

-r ./requirements-convert_hf_to_gguf.txt
-r ./requirements-convert_hf_to_gguf_update.txt
-r ./requirements-convert_legacy_llama.txt
-r ./requirements-convert_llama_ggml_to_gguf.txt
-r ./requirements-tool_bench.txt

-r ./requirements-gguf_editor_gui.txt
```

----------------------------------------

TITLE: Defining Shader Compilation Function in CMake
DESCRIPTION: This function compiles shader files, generates header files from the compiled shaders, and sets up custom commands for the build process. It handles different compilation steps for Visual Studio and other generators.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-kompute/CMakeLists.txt#2025-04-22_snippet_2

LANGUAGE: CMake
CODE:
```
function(compile_shader)
    set(options)
    set(oneValueArgs)
    set(multiValueArgs SOURCES)
    cmake_parse_arguments(compile_shader "${options}" "${oneValueArgs}" "${multiValueArgs}" ${ARGN})
    foreach(source ${compile_shader_SOURCES})
        get_filename_component(filename ${source} NAME)
        set(spv_file ${filename}.spv)
        add_custom_command(
            OUTPUT ${spv_file}
            DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/${source}
            ${CMAKE_CURRENT_SOURCE_DIR}/kompute-shaders/common.comp
            ${CMAKE_CURRENT_SOURCE_DIR}/kompute-shaders/op_getrows.comp
            ${CMAKE_CURRENT_SOURCE_DIR}/kompute-shaders/op_mul_mv_q_n_pre.comp
            ${CMAKE_CURRENT_SOURCE_DIR}/kompute-shaders/op_mul_mv_q_n.comp
            COMMAND ${glslc_executable} --target-env=vulkan1.2 -o ${spv_file} ${CMAKE_CURRENT_SOURCE_DIR}/${source}
            COMMENT "Compiling ${source} to ${spv_file}"
            )

        get_filename_component(RAW_FILE_NAME ${spv_file} NAME)
        set(FILE_NAME "shader${RAW_FILE_NAME}")
        string(REPLACE ".comp.spv" ".h" HEADER_FILE ${FILE_NAME})
        string(TOUPPER ${HEADER_FILE} HEADER_FILE_DEFINE)
        string(REPLACE "." "_" HEADER_FILE_DEFINE "${HEADER_FILE_DEFINE}")
        set(OUTPUT_HEADER_FILE "${HEADER_FILE}")
        message(STATUS "${HEADER_FILE} generating ${HEADER_FILE_DEFINE}")
        if(CMAKE_GENERATOR MATCHES "Visual Studio")
            add_custom_command(
                OUTPUT ${OUTPUT_HEADER_FILE}
                COMMAND ${CMAKE_COMMAND} -E echo "/*THIS FILE HAS BEEN AUTOMATICALLY GENERATED - DO NOT EDIT*/" > ${OUTPUT_HEADER_FILE}
                COMMAND ${CMAKE_COMMAND} -E echo \"\#ifndef ${HEADER_FILE_DEFINE}\" >> ${OUTPUT_HEADER_FILE}
                COMMAND ${CMAKE_COMMAND} -E echo \"\#define ${HEADER_FILE_DEFINE}\" >> ${OUTPUT_HEADER_FILE}
                COMMAND ${CMAKE_COMMAND} -E echo "namespace kp {" >> ${OUTPUT_HEADER_FILE}
                COMMAND ${CMAKE_COMMAND} -E echo "namespace shader_data {" >> ${OUTPUT_HEADER_FILE}
                COMMAND ${CMAKE_BINARY_DIR}/bin/$<CONFIG>/xxd -i ${RAW_FILE_NAME} >> ${OUTPUT_HEADER_FILE}
                COMMAND ${CMAKE_COMMAND} -E echo "}}" >> ${OUTPUT_HEADER_FILE}
                COMMAND ${CMAKE_COMMAND} -E echo \"\#endif // define ${HEADER_FILE_DEFINE}\" >> ${OUTPUT_HEADER_FILE}
                DEPENDS ${spv_file} xxd
                COMMENT "Converting to hpp: ${FILE_NAME} ${CMAKE_BINARY_DIR}/bin/$<CONFIG>/xxd"
                )
        else()
            add_custom_command(
                OUTPUT ${OUTPUT_HEADER_FILE}
                COMMAND ${CMAKE_COMMAND} -E echo "/*THIS FILE HAS BEEN AUTOMATICALLY GENERATED - DO NOT EDIT*/" > ${OUTPUT_HEADER_FILE}
                COMMAND ${CMAKE_COMMAND} -E echo \"\#ifndef ${HEADER_FILE_DEFINE}\" >> ${OUTPUT_HEADER_FILE}
                COMMAND ${CMAKE_COMMAND} -E echo \"\#define ${HEADER_FILE_DEFINE}\" >> ${OUTPUT_HEADER_FILE}
                COMMAND ${CMAKE_COMMAND} -E echo "namespace kp {" >> ${OUTPUT_HEADER_FILE}
                COMMAND ${CMAKE_COMMAND} -E echo "namespace shader_data {" >> ${OUTPUT_HEADER_FILE}
                COMMAND ${CMAKE_BINARY_DIR}/bin/xxd -i ${RAW_FILE_NAME} >> ${OUTPUT_HEADER_FILE}
                COMMAND ${CMAKE_COMMAND} -E echo "}}" >> ${OUTPUT_HEADER_FILE}
                COMMAND ${CMAKE_COMMAND} -E echo \"\#endif // define ${HEADER_FILE_DEFINE}\" >> ${OUTPUT_HEADER_FILE}
                DEPENDS ${spv_file} xxd
                COMMENT "Converting to hpp: ${FILE_NAME} ${CMAKE_BINARY_DIR}/bin/xxd"
                )
        endif()
    endforeach()
endfunction()
```

----------------------------------------

TITLE: Setting Up OpenCL Kernel Embedding in GGML CMake
DESCRIPTION: Configures the embedding of OpenCL kernels, including directory setup and Python script for kernel embedding.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-opencl/CMakeLists.txt#2025-04-22_snippet_3

LANGUAGE: CMake
CODE:
```
if (GGML_OPENCL_EMBED_KERNELS)
    add_compile_definitions(GGML_OPENCL_EMBED_KERNELS)

    set(EMBED_KERNEL_SCRIPT "${CMAKE_CURRENT_SOURCE_DIR}/kernels/embed_kernel.py")
    file(MAKE_DIRECTORY     "${CMAKE_CURRENT_BINARY_DIR}/autogenerated")

    target_include_directories(${TARGET_NAME} PRIVATE "${CMAKE_CURRENT_BINARY_DIR}/autogenerated")
endif ()
```

----------------------------------------

TITLE: Building oneDNN with NVIDIA Backend
DESCRIPTION: Commands to clone and build oneDNN from source with NVIDIA GPU support.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_4

LANGUAGE: sh
CODE:
```
git clone https://github.com/oneapi-src/oneDNN.git
cd oneDNN
cmake -GNinja -Bbuild-nvidia -DDNNL_CPU_RUNTIME=DPCPP -DDNNL_GPU_RUNTIME=DPCPP -DDNNL_GPU_VENDOR=NVIDIA -DONEDNN_BUILD_GRAPH=OFF -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx
cmake --build build-nvidia --config Release
```

----------------------------------------

TITLE: Finding Required Libraries for Metal Backend in CMake
DESCRIPTION: Locates the Foundation, Metal, and MetalKit libraries required for the Metal backend implementation.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-metal/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
find_library(FOUNDATION_LIBRARY Foundation REQUIRED)
find_library(METAL_FRAMEWORK    Metal      REQUIRED)
find_library(METALKIT_FRAMEWORK MetalKit   REQUIRED)

message(STATUS "Metal framework found")
```

----------------------------------------

TITLE: Copying Metal Files in CMake for GGML
DESCRIPTION: Copies necessary Metal-related files to the binary output directory.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-metal/CMakeLists.txt#2025-04-22_snippet_3

LANGUAGE: CMake
CODE:
```
configure_file(../ggml-common.h  ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-common.h     COPYONLY)
configure_file(ggml-metal.metal  ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-metal.metal  COPYONLY)
configure_file(ggml-metal-impl.h ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-metal-impl.h COPYONLY)
```

----------------------------------------

TITLE: Converting Llama2.c Model Usage Help
DESCRIPTION: Command-line help output showing available options for the llama2.c to GGML conversion tool, including required and optional parameters for model conversion.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/convert-llama2c-to-ggml/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
usage: ./llama-convert-llama2c-to-ggml [options]

options:
  -h, --help                       show this help message and exit
  --copy-vocab-from-model FNAME    path of gguf llama model or llama2.c vocabulary from which to copy vocab (default 'models/7B/ggml-model-f16.gguf')
  --llama2c-model FNAME            [REQUIRED] model path from which to load Karpathy's llama2.c model
  --llama2c-output-model FNAME     model path to save the converted llama2.c model (default ak_llama_model.bin')
```

----------------------------------------

TITLE: Configuring llama-vdot Executable Build in CMake
DESCRIPTION: Sets up the build configuration for the llama-vdot executable, which likely performs vector dot product operations. It links against the common and llama libraries and requires C++17 support.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/pocs/vdot/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(TARGET llama-vdot)
add_executable(${TARGET} vdot.cpp)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Specifying Python Dependencies for LLaMA Model Conversion
DESCRIPTION: This snippet defines the Python package dependencies needed for converting legacy LLaMA models. It includes a reference to another requirements file and specifies a PyTorch version with CPU support.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-convert_hf_to_gguf_update.txt#2025-04-22_snippet_0

LANGUAGE: plaintext
CODE:
```
-r ./requirements-convert_legacy_llama.txt
--extra-index-url https://download.pytorch.org/whl/cpu
torch~=2.2.1
```

----------------------------------------

TITLE: Running LLaMA Multi-Modal CLI with Book Image
DESCRIPTION: Example command for running llama-mtmd-cli to identify a book author from an image. Uses GGUF model format with GPU acceleration.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_12

LANGUAGE: sh
CODE:
```
./llama-mtmd-cli \
    -m /data/local/tmp/ggml-model-q4_k.gguf \
    --mmproj /data/local/tmp/mmproj-model-f16.gguf \
    --image /data/local/tmp/demo.jpeg \
    -p "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\nWho is the author of this book? \nAnswer the question using a single word or phrase. ASSISTANT:" \
    --n-gpu-layers 999
```

----------------------------------------

TITLE: Configuring Windows-Specific Version Flag
DESCRIPTION: Sets up a Windows-specific version flag when building on Windows platforms.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_13

LANGUAGE: CMake
CODE:
```
if (WIN32)
    set(GGML_WIN_VER "0x602" CACHE STRING   "ggml: Windows version")
endif()
```

----------------------------------------

TITLE: Generating a GGUF Hash Manifest with Multiple Hash Types
DESCRIPTION: This command generates a manifest file containing multiple hash types (xxh64, sha1, sha256) for a GGUF file and all its internal tensors. The --all flag ensures all supported hash algorithms are used.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-hash/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
./llama-gguf-hash --all test.gguf > test.gguf.manifest
```

----------------------------------------

TITLE: Cloning GLMV-EDGE model repository
DESCRIPTION: Git command to clone either the 2B or 5B version of the GLMV-EDGE model from Hugging Face. This is the first step in the GGUF conversion process.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/glmedge.md#2025-04-22_snippet_1

LANGUAGE: sh
CODE:
```
git clone https://huggingface.co/THUDM/glm-edge-v-5b or https://huggingface.co/THUDM/glm-edge-v-2b
```

----------------------------------------

TITLE: Testing llama.cpp with CANN Backend
DESCRIPTION: Command to test llama.cpp with the CANN backend for Ascend NPU acceleration.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_22

LANGUAGE: bash
CODE:
```
./build/bin/llama-cli -m PATH_TO_MODEL -p "Building a website can be done in 10 steps:" -ngl 32
```

----------------------------------------

TITLE: Transferring Files to Android Device with ADB
DESCRIPTION: ADB commands to create a directory on the Android device and push the compiled llama.cpp binaries and model files to it.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/android.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
$ adb shell "mkdir /data/local/tmp/llama.cpp"
$ adb push {install-dir} /data/local/tmp/llama.cpp/
$ adb push {model}.gguf /data/local/tmp/llama.cpp/
$ adb shell
```

----------------------------------------

TITLE: Running llama-bench with SQL output format
DESCRIPTION: Command to run the llama-bench tool with SQL output format. The output contains SQL statements suitable for importing into a SQLite database, including table creation and data insertion commands.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_12

LANGUAGE: sh
CODE:
```
$ ./llama-bench -o sql
```

----------------------------------------

TITLE: Configuring llama-gguf-split Executable with CMake
DESCRIPTION: This snippet defines and configures the llama-gguf-split executable build target. It specifies the source file, installation rules, required libraries (common, llama, and threading libraries), and sets C++17 as the standard.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-split/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
set(TARGET llama-gguf-split)
add_executable(${TARGET} gguf-split.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Installing Python Requirements for LLaVA
DESCRIPTION: Command to install the required Python packages for working with LLaVA models.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_2

LANGUAGE: sh
CODE:
```
pip install -r examples/llava/requirements.txt
```

----------------------------------------

TITLE: Adding User to GPU Groups for Intel GPUs
DESCRIPTION: Commands to add the current user to 'video' and 'render' groups, which is necessary for Intel GPU access.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_2

LANGUAGE: sh
CODE:
```
sudo usermod -aG render $USER
sudo usermod -aG video $USER
```

----------------------------------------

TITLE: Creating Shared Library for Android JNI Integration
DESCRIPTION: Defines a shared library target that includes the JNI interface code. This library will be packaged with the APK and loaded through System.loadLibrary() from Java/Kotlin code.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama.android/llama/src/main/cpp/CMakeLists.txt#2025-04-22_snippet_3

LANGUAGE: cmake
CODE:
```
add_library(${CMAKE_PROJECT_NAME} SHARED
        # List C/C++ source files with relative paths to this CMakeLists.txt.
        llama-android.cpp)
```

----------------------------------------

TITLE: Verifying GPU Installation with clinfo
DESCRIPTION: Commands to install clinfo and list available OpenCL platforms and devices.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_3

LANGUAGE: sh
CODE:
```
sudo apt install clinfo
sudo clinfo -l
```

----------------------------------------

TITLE: Installing OpenCL Headers and ICD Loader for Windows Arm64
DESCRIPTION: PowerShell commands to install OpenCL headers and ICD loader library for Windows Arm64 development.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_27

LANGUAGE: powershell
CODE:
```
mkdir -p ~/dev/llm

cd ~/dev/llm
git clone https://github.com/KhronosGroup/OpenCL-Headers && cd OpenCL-Headers
mkdir build && cd build
cmake .. -G Ninja `
  -DBUILD_TESTING=OFF `
  -DOPENCL_HEADERS_BUILD_TESTING=OFF `
  -DOPENCL_HEADERS_BUILD_CXX_TESTS=OFF `
  -DCMAKE_INSTALL_PREFIX="$HOME/dev/llm/opencl"
cmake --build . --target install

cd ~/dev/llm
git clone https://github.com/KhronosGroup/OpenCL-ICD-Loader && cd OpenCL-ICD-Loader
mkdir build && cd build
cmake .. -G Ninja `
  -DCMAKE_BUILD_TYPE=Release `
  -DCMAKE_PREFIX_PATH="$HOME/dev/llm/opencl" `
  -DCMAKE_INSTALL_PREFIX="$HOME/dev/llm/opencl"
cmake --build . --target install
```

----------------------------------------

TITLE: Measuring KL Divergence with llama-perplexity
DESCRIPTION: Placeholder for measuring Kullback-Leibler divergence between model distributions, which is currently marked as TODO in the documentation.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_11

LANGUAGE: bash
CODE:
```
# TODO
```

----------------------------------------

TITLE: Creating Build Information Generator Command in CMake
DESCRIPTION: Sets up a custom command to generate build-info.cpp when Git index changes. This command passes compiler information to a CMake script that generates build details from Git repository data.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/common/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: CMake
CODE:
```
add_custom_command(
    OUTPUT "${CMAKE_CURRENT_SOURCE_DIR}/build-info.cpp"
    COMMENT "Generating build details from Git"
    COMMAND ${CMAKE_COMMAND} -DMSVC=${MSVC} -DCMAKE_C_COMPILER_VERSION=${CMAKE_C_COMPILER_VERSION}
            -DCMAKE_C_COMPILER_ID=${CMAKE_C_COMPILER_ID} -DCMAKE_VS_PLATFORM_NAME=${CMAKE_VS_PLATFORM_NAME}
            -DCMAKE_C_COMPILER=${CMAKE_C_COMPILER} -P "${CMAKE_CURRENT_SOURCE_DIR}/cmake/build-info-gen-cpp.cmake"
    WORKING_DIRECTORY "${CMAKE_CURRENT_SOURCE_DIR}/.."
    DEPENDS "${CMAKE_CURRENT_SOURCE_DIR}/build-info.cpp.in" ${GIT_INDEX}
    VERBATIM
)
set(TARGET build_info)
add_library(${TARGET} OBJECT build-info.cpp)
if (BUILD_SHARED_LIBS)
    set_target_properties(${TARGET} PROPERTIES POSITION_INDEPENDENT_CODE ON)
endif()
```

----------------------------------------

TITLE: Listing SYCL Devices for llama.cpp on Windows
DESCRIPTION: Command to list available SYCL devices for llama.cpp on Windows.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_24

LANGUAGE: sh
CODE:
```
build\bin\llama-ls-sycl-device.exe
```

----------------------------------------

TITLE: Enabling oneAPI Runtime Environment on Windows
DESCRIPTION: Command to enable the oneAPI runtime environment on Windows using the Intel oneAPI command prompt.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_15

LANGUAGE: sh
CODE:
```
"C:\Program Files (x86)\Intel\oneAPI\setvars.bat" intel64
```

----------------------------------------

TITLE: Enabling oneAPI Runtime Environment on Windows (PowerShell)
DESCRIPTION: PowerShell command to enable the oneAPI runtime environment on Windows.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_16

LANGUAGE: powershell
CODE:
```
cmd.exe "/K" '"C:\Program Files (x86)\Intel\oneAPI\setvars.bat" && powershell'
```

----------------------------------------

TITLE: SYCL GPU Usage Log Example
DESCRIPTION: Example log output showing which SYCL GPU is being used for model execution, including the device's compute units. This appears during model initialization.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_29

LANGUAGE: sh
CODE:
```
use 1 SYCL GPUs: [0] with Max compute units:512
```

----------------------------------------

TITLE: Configuring CMake Build for LLAMA.cpp Lora Export Executable
DESCRIPTION: Sets up CMake configuration to build the llama-export-lora executable. Links against common and llama libraries, requires C++17, and configures installation targets.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/export-lora/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
set(TARGET llama-export-lora)
add_executable(${TARGET} export-lora.cpp)
install(TARGETS ${TARGET} RUNTIME)
target_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})
target_compile_features(${TARGET} PRIVATE cxx_std_17)
```

----------------------------------------

TITLE: Creating Directory for CI Results in MUSA Docker Setup
DESCRIPTION: Command to create a local directory for storing CI run results when setting up MUSA CI in a Docker container.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ci/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
mkdir -p $HOME/llama.cpp/ci-results
```

----------------------------------------

TITLE: Proper Pointer and Reference Syntax Examples in C++
DESCRIPTION: Examples showing the proper syntax for pointers and references in C++ code, demonstrating the preferred style of placing the asterisk or ampersand next to the type rather than the variable name.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/CONTRIBUTING.md#2025-04-22_snippet_3

LANGUAGE: cpp
CODE:
```
// OK
llama_context * ctx;
const llama_rope_type rope_type;

// not OK
struct llama_context * ctx;
const enum llama_rope_type rope_type;
```

----------------------------------------

TITLE: Help Command for Control Vector Generator
DESCRIPTION: Command to display the help message with all available options for the cvector-generator tool.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/cvector-generator/README.md#2025-04-22_snippet_4

LANGUAGE: sh
CODE:
```
./cvector-generator -h
# Then, have a look at "cvector" section
```

----------------------------------------

TITLE: Using Top-nÏƒ Sampling in LLaMA.cpp
DESCRIPTION: Example command for enabling Top-nÏƒ sampling with a value of 1. This sampling method selects tokens based on a statistical threshold in pre-softmax logits, maintaining a stable sampling space regardless of temperature scaling.
SOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_26

LANGUAGE: shell
CODE:
```
--top-nsigma 1
```