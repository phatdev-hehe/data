TITLE: Basic Optimization Loop with PyTorch
DESCRIPTION: Standard training loop showing how to use an optimizer with backward pass. The loop zeros gradients, computes the forward pass, calculates loss, performs backpropagation, and updates parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
for input, target in dataset:
    optimizer.zero_grad()
    output = model(input)
    loss = loss_fn(output, target)
    loss.backward()
    optimizer.step()
```

----------------------------------------

TITLE: Allocating Tensors on Specific CUDA Devices (Python)
DESCRIPTION: Demonstrates how to create `torch.device` objects for CUDA devices, allocate tensors directly on a specified device, transfer tensors using `.cuda()` and `.to()`, and manage device context using `torch.cuda.device` for temporary device changes. Shows tensor device affiliation after operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_0

LANGUAGE: python
CODE:
```
cuda = torch.device('cuda')     # Default CUDA device
cuda0 = torch.device('cuda:0')
cuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)

x = torch.tensor([1., 2.], device=cuda0)
# x.device is device(type='cuda', index=0)
y = torch.tensor([1., 2.]).cuda()
# y.device is device(type='cuda', index=0)

with torch.cuda.device(1):
    # allocates a tensor on GPU 1
    a = torch.tensor([1., 2.], device=cuda)

    # transfers a tensor from CPU to GPU 1
    b = torch.tensor([1., 2.]).cuda()
    # a.device and b.device are device(type='cuda', index=1)

    # You can also use ``Tensor.to`` to transfer a tensor:
    b2 = torch.tensor([1., 2.]).to(device=cuda)
    # b.device and b2.device are device(type='cuda', index=1)

    c = a + b
    # c.device is device(type='cuda', index=1)

    z = x + y
    # z.device is device(type='cuda', index=0)

    # even within a context, you can specify the device
    # (or give a GPU index to the .cuda call)
    d = torch.randn(2, device=cuda2)
    e = torch.randn(2).to(cuda2)
    f = torch.randn(2).cuda(cuda2)
    # d.device, e.device, and f.device are all device(type='cuda', index=2)
```

----------------------------------------

TITLE: Importing Libraries and Setting Seed in PyTorch
DESCRIPTION: Imports essential PyTorch modules (`torch`, `nn`, `F`), `partial` from `functools`, and sets the random seed for reproducibility using `torch.manual_seed(0)`. These imports are prerequisites for subsequent model definition, data generation, and gradient computation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
import torch.nn as nn
import torch.nn.functional as F
from functools import partial

torch.manual_seed(0);
```

----------------------------------------

TITLE: ResNet50 Optimization Example
DESCRIPTION: Shows how to apply torch.compile to optimize a pre-trained ResNet50 model from PyTorch hub using the inductor backend.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_get_started.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch
model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)
opt_model = torch.compile(model, backend="inductor")
opt_model(torch.randn(1,3,64,64))
```

----------------------------------------

TITLE: Creating Typed Tensors
DESCRIPTION: Shows how to create tensors with specific data types and device placement using dtype and device parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensors.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> torch.zeros([2, 4], dtype=torch.int32)
tensor([[ 0,  0,  0,  0],
        [ 0,  0,  0,  0]], dtype=torch.int32)
>>> cuda0 = torch.device('cuda:0')
>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)
tensor([[ 1.0000,  1.0000,  1.0000,  1.0000],
        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')
```

----------------------------------------

TITLE: ReLU Activation in PyTorch
DESCRIPTION: This snippet shows the application of ReLU (Rectified Linear Unit) activation function on tensors of various shapes. It demonstrates in-place operations on 16-bit float tensors across different layers of a neural network.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2next50_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
Operator: aten.relu_.default
cnt: 1, ((T([128, 64, 112, 112], f16),), {})
cnt: 3, ((T([128, 128, 56, 56], f16),), {})
cnt: 9, ((T([128, 32, 56, 56], f16),), {})
cnt: 4, ((T([128, 256, 56, 56], f16),), {})
cnt: 12, ((T([128, 64, 28, 28], f16),), {})
```

----------------------------------------

TITLE: Applying Custom Initialization to PyTorch Module Parameters
DESCRIPTION: Shows how to define a custom initialization function and apply it recursively to a PyTorch module and its submodules using the 'apply' method.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
# Define a function to initialize Linear weights.
# Note that no_grad() is used here to avoid tracking this computation in the autograd graph.
@torch.no_grad()
def init_weights(m):
  if isinstance(m, nn.Linear):
    nn.init.xavier_normal_(m.weight)
    m.bias.fill_(0.0)

# Apply the function recursively on the module and its submodules.
dynamic_net.apply(init_weights)
```

----------------------------------------

TITLE: Adding Tensors in PyTorch - Python
DESCRIPTION: This snippet shows the usage of PyTorch's aten.add.Tensor to perform tensor addition on tensors of different dimensions utilizing half precision (f16). Parameters include tensor shapes and data types. The operation is constrained by compatibility in dimensions for addition.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v3_large_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten.add.Tensor
cnt: 2, ((T([32, 960, 7, 7], f16), T([32, 960, 7, 7], f16)), {})
cnt: 2, ((T([32, 160, 7, 7], f16), T([32, 160, 7, 7], f16)), {})
cnt: 1, ((T([32, 672, 7, 7], f16), T([32, 672, 7, 7], f16)), {})
cnt: 1, ((T([32, 672, 14, 14], f16), T([32, 672, 14, 14], f16)), {})
cnt: 1, ((T([32, 112, 14, 14], f16), T([32, 112, 14, 14], f16)), {})
cnt: 1, ((T([32, 480, 14, 14], f16), T([32, 480, 14, 14], f16)), {})
cnt: 3, ((T([32, 80, 14, 14], f16), T([32, 80, 14, 14], f16)), {})
cnt: 2, ((T([32, 120, 28, 28], f16), T([32, 120, 28, 28], f16)), {})
cnt: 2, ((T([32, 40, 28, 28], f16), T([32, 40, 28, 28], f16)), {})
cnt: 1, ((T([32, 72, 28, 28], f16), T([32, 72, 28, 28], f16)), {})
cnt: 1, ((T([32, 24, 56, 56], f16), T([32, 24, 56, 56], f16)), {})
cnt: 1, ((T([32, 16, 112, 112], f16), T([32, 16, 112, 112], f16)), {})

```

----------------------------------------

TITLE: Saving and Restoring Module State Dicts - PyTorch - Python
DESCRIPTION: Demonstrates extracting the state_dict of a torch.nn.Module (here, BatchNorm1d), which includes both parameters and buffers, saving it to disk, and restoring it to a freshly initialized module. Relies on torch and its nn submodule. Inputs include a module with learnable parameters and persistent buffers; outputs are a file containing the state_dict and a restored module state. This method is more robust and compatible than saving entire modules directly. Inputs and outputs are dictionaries mapping parameter/buffer names to tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> bn = torch.nn.BatchNorm1d(3, track_running_stats=True)
>>> list(bn.named_parameters())
[('weight', Parameter containing: tensor([1., 1., 1.], requires_grad=True)),
 ('bias', Parameter containing: tensor([0., 0., 0.], requires_grad=True))]

>>> list(bn.named_buffers())
[('running_mean', tensor([0., 0., 0.])),
 ('running_var', tensor([1., 1., 1.])),
 ('num_batches_tracked', tensor(0))]

>>> bn.state_dict()
OrderedDict([('weight', tensor([1., 1., 1.])),
             ('bias', tensor([0., 0., 0.])),
             ('running_mean', tensor([0., 0., 0.])),
             ('running_var', tensor([1., 1., 1.])),
             ('num_batches_tracked', tensor(0))])
```

----------------------------------------

TITLE: Importing Core PyTorch and Utility Modules - Python
DESCRIPTION: This snippet imports PyTorch modules, the neural network functional interface, and the partial function utility from functools. It sets a manual random seed for reproducibility. No external dependencies beyond PyTorch and standard library. Ensures code reproducibility and provides necessary functionality for deep learning computations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
import torch.nn as nn
import torch.nn.functional as F
from functools import partial
_ = torch.manual_seed(0)
```

----------------------------------------

TITLE: Vectorizing a Model with torch.func.vmap in Python
DESCRIPTION: This snippet illustrates how `torch.func.vmap` can automatically vectorize a function designed for single inputs. A simple linear `model` function, which expects a 1D feature vector, is defined. `vmap(model)` is then used to apply this model across a batch of `examples` (a 2D tensor) without modifying the original `model` function to handle batches explicitly. The `vmap` transform effectively adds a batch dimension to the operations within the `model`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.whirlwind_tour.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
from torch.func import vmap
batch_size, feature_size = 3, 5
weights = torch.randn(feature_size, requires_grad=True)

def model(feature_vec):
    # Very simple linear model with activation
    assert feature_vec.dim() == 1
    return feature_vec.dot(weights).relu()

examples = torch.randn(batch_size, feature_size)
result = vmap(model)(examples)
```

----------------------------------------

TITLE: Compiling PyTorch Model for Inference
DESCRIPTION: Example demonstrating how to apply torch.compile to a model for inference optimization in a training loop
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
# inference
model = ...
opt_model = torch.compile(model)

for _ in range(N_ITERS):
    inp = ...
    out = opt_model(inp)
```

----------------------------------------

TITLE: Installing PyTorch on Linux Bash
DESCRIPTION: Sets the CMAKE_PREFIX_PATH environment variable to include the conda environment path (if active) and then runs the standard Python setup script with the 'develop' flag to build and install PyTorch in editable mode on Linux.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_12

LANGUAGE: Bash
CODE:
```
export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
python setup.py develop
```

----------------------------------------

TITLE: Calculating Standard Batch Gradient in PyTorch
DESCRIPTION: Demonstrates the standard PyTorch workflow for calculating gradients averaged over a mini-batch. It instantiates the `SimpleCNN` model, moves it to the specified device, performs a forward pass with the entire batch of `data`, calculates the loss using the defined `loss_fn`, and then calls `loss.backward()` to compute gradients for all model parameters, averaged across the batch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
model = SimpleCNN().to(device=device)
predictions = model(data) # move the entire mini-batch through the model

loss = loss_fn(predictions, targets)
loss.backward() # back propogate the 'average' gradient of this mini-batch
```

----------------------------------------

TITLE: Calculating Broadcasted Tensor Size in Python
DESCRIPTION: Illustrates how the size of the resulting tensor is determined when two broadcastable tensors are operated on. It shows examples where dimensions are prepended with 1s for alignment and the final dimension size is the maximum of the input sizes along that dimension. It also includes an example that results in a RuntimeError because the tensors are not broadcastable.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/broadcasting.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
# can line up trailing dimensions to make reading easier
>>> x=torch.empty(5,1,4,1)
>>> y=torch.empty(  3,1,1)
>>> (x+y).size()
torch.Size([5, 3, 4, 1])

# but not necessary:
>>> x=torch.empty(1)
>>> y=torch.empty(3,1,7)
>>> (x+y).size()
torch.Size([3, 1, 7])

>>> x=torch.empty(5,2,4,1)
>>> y=torch.empty(3,1,1)
>>> (x+y).size()
RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1
```

----------------------------------------

TITLE: Complete Distributed Training Example with PyTorch
DESCRIPTION: End-to-end example demonstrating distributed autograd and optimizer implementation. Includes RPC initialization, remote tensor operations, distributed backward pass, and optimizer step execution.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/distributed_autograd.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
import torch
import torch.multiprocessing as mp
import torch.distributed.autograd as dist_autograd
from torch.distributed import rpc
from torch import optim
from torch.distributed.optim import DistributedOptimizer

def random_tensor():
    return torch.rand((3, 3), requires_grad=True)

def _run_process(rank, dst_rank, world_size):
    name = "worker{}".format(rank)
    dst_name = "worker{}".format(dst_rank)

    # Initialize RPC.
    rpc.init_rpc(
        name=name,
        rank=rank,
        world_size=world_size
    )

    # Use a distributed autograd context.
    with dist_autograd.context() as context_id:
        # Forward pass (create references on remote nodes).
        rref1 = rpc.remote(dst_name, random_tensor)
        rref2 = rpc.remote(dst_name, random_tensor)
        loss = rref1.to_here() + rref2.to_here()

        # Backward pass (run distributed autograd).
        dist_autograd.backward(context_id, [loss.sum()])

        # Build DistributedOptimizer.
        dist_optim = DistributedOptimizer(
        optim.SGD,
        [rref1, rref2],
        lr=0.05,
        )

        # Run the distributed optimizer step.
        dist_optim.step(context_id)

def run_process(rank, world_size):
    dst_rank = (rank + 1) % world_size
    _run_process(rank, dst_rank, world_size)
    rpc.shutdown()

if __name__ == '__main__':
  # Run world_size workers
  world_size = 2
  mp.spawn(run_process, args=(world_size,), nprocs=world_size)
```

----------------------------------------

TITLE: Defining a Custom Network Module Composed of Submodules
DESCRIPTION: Defines a custom neural network module `Net` that internally uses two instances of the `MyLinear` module (`l0`, `l1`) as layers. The `forward` method explicitly defines the data flow: input passes through `l0`, then a ReLU activation (using `torch.nn.functional.relu`), and finally through `l1`. This demonstrates building complex modules by composing simpler ones.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import torch.nn.functional as F

class Net(nn.Module):
  def __init__(self):
    super().__init__()
    self.l0 = MyLinear(4, 3)
    self.l1 = MyLinear(3, 1)
  def forward(self, x):
    x = self.l0(x)
    x = F.relu(x)
    x = self.l1(x)
    return x
```

----------------------------------------

TITLE: Typical Mixed Precision Training with torch.amp (Python)
DESCRIPTION: This snippet demonstrates a standard workflow for mixed precision training in PyTorch using torch.amp.autocast and torch.amp.GradScaler. It initializes a model and optimizer, runs forward and backward passes with autocasting for float16 precision on CUDA, and uses a GradScaler to handle gradient scaling and updating across training epochs. Dependencies: PyTorch, CUDA or XPU device support, torch.amp (Python). Inputs are data batches (input, target); outputs are model parameter updates. Ensure GradScaler and autocast regions are used as shown for proper mixed precision handling.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/amp_examples.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
# Creates model and optimizer in default precision
model = Net().cuda()
optimizer = optim.SGD(model.parameters(), ...)

# Creates a GradScaler once at the beginning of training.
scaler = GradScaler()

for epoch in epochs:
    for input, target in data:
        optimizer.zero_grad()

        # Runs the forward pass with autocasting.
        with autocast(device_type='cuda', dtype=torch.float16):
            output = model(input)
            loss = loss_fn(output, target)

        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.
        # Backward passes under autocast are not recommended.
        # Backward ops run in the same dtype autocast chose for corresponding forward ops.
        scaler.scale(loss).backward()

        # scaler.step() first unscales the gradients of the optimizer's assigned params.
        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,
        # otherwise, optimizer.step() is skipped.
        scaler.step(optimizer)

        # Updates the scale for next iteration.
        scaler.update()

```

----------------------------------------

TITLE: Initializing and Using DistributedDataParallel in Python
DESCRIPTION: Provides a complete example of setting up and using `torch.nn.parallel.DistributedDataParallel` (DDP) for distributed training. It initializes a process group (`gloo` backend), creates a simple linear model, wraps it with DDP, performs a forward and backward pass, and updates parameters using an SGD optimizer. The example uses `torch.multiprocessing.spawn` to launch multiple processes and requires setting `MASTER_ADDR` and `MASTER_PORT` environment variables for the default `env://` initialization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/ddp.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import torch.nn as nn
import torch.optim as optim
import os
from torch.nn.parallel import DistributedDataParallel as DDP


def example(rank, world_size):
    # create default process group
    dist.init_process_group("gloo", rank=rank, world_size=world_size)
    # create local model
    model = nn.Linear(10, 10).to(rank)
    # construct DDP model
    ddp_model = DDP(model, device_ids=[rank])
    # define loss function and optimizer
    loss_fn = nn.MSELoss()
    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)

    # forward pass
    outputs = ddp_model(torch.randn(20, 10).to(rank))
    labels = torch.randn(20, 10).to(rank)
    # backward pass
    loss_fn(outputs, labels).backward()
    # update parameters
    optimizer.step()

def main():
    world_size = 2
    mp.spawn(example,
        args=(world_size,),
        nprocs=world_size,
        join=True)

if __name__=="__main__":
    # Environment variables which need to be
    # set when using c10d's default "env"
    # initialization mode.
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "29500"
    main()
```

----------------------------------------

TITLE: Exporting a PyTorch Model Using torch.onnx.export (Python)
DESCRIPTION: This snippet demonstrates exporting a custom PyTorch nn.Module to the ONNX format using torch.onnx.export. It defines a simple convolutional model, generates a random input tensor, and invokes the export function with model, input, and output filename specified. Key parameters include input_names for naming ONNX inputs and dynamo for selecting the exporter backend. The code requires torch to be installed and outputs an ONNX file named 'my_model.onnx'; model inputs and outputs should be tensor-like and match the model signature. The example is self-contained, but the exported model may depend on ONNX specification version and runtime compatibility.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch

class MyModel(torch.nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.conv1 = torch.nn.Conv2d(1, 128, 5)

    def forward(self, x):
        return torch.relu(self.conv1(x))

input_tensor = torch.rand((1, 1, 128, 128), dtype=torch.float32)

model = MyModel()

torch.onnx.export(
    model,                  # model to export
    (input_tensor,),        # inputs of the model,
    "my_model.onnx",        # filename of the ONNX model
    input_names=["input"],  # Rename inputs for the ONNX model
    dynamo=True             # True or False to select the exporter to use
)

```

----------------------------------------

TITLE: Checking MPS Availability and Using the 'mps' Device in PyTorch (Python)
DESCRIPTION: This Python snippet demonstrates checking for the availability of the PyTorch MPS backend using `torch.backends.mps.is_available()` and `torch.backends.mps.is_built()`. If MPS is available, it shows how to create a `torch.device('mps')`, allocate tensors directly on the MPS device, perform tensor operations, and move a PyTorch model to the MPS device using `.to()` for GPU acceleration on compatible macOS hardware. Requires the PyTorch library.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/mps.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
# Check that MPS is available
if not torch.backends.mps.is_available():
    if not torch.backends.mps.is_built():
        print("MPS not available because the current PyTorch install was not "
              "built with MPS enabled.")
    else:
        print("MPS not available because the current MacOS version is not 12.3+ "
              "and/or you do not have an MPS-enabled device on this machine.")

else:
    mps_device = torch.device("mps")

    # Create a Tensor directly on the mps device
    x = torch.ones(5, device=mps_device)
    # Or
    x = torch.ones(5, device="mps")

    # Any operation happens on the GPU
    y = x * 2

    # Move your model to mps just like any other device
    model = YourFavoriteNet()
    model.to(mps_device)

    # Now every call runs on the GPU
    pred = model(x)
```

----------------------------------------

TITLE: Gradient Accumulation with torch.amp and GradScaler (Python)
DESCRIPTION: This snippet demonstrates using AMP with gradient accumulation across multiple batches. Each mini-batch performs forward/backward passes with scaled gradients, but updates (step, scale update, and zero_grad) are only performed every iters_to_accumulate batches. This pattern is critical for consistent scaling and step granularity during accumulation. Requires PyTorch, CUDA device, and established model and data iteration with accumulation logic.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/amp_examples.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
scaler = GradScaler()

for epoch in epochs:
    for i, (input, target) in enumerate(data):
        with autocast(device_type='cuda', dtype=torch.float16):
            output = model(input)
            loss = loss_fn(output, target)
            loss = loss / iters_to_accumulate

        # Accumulates scaled gradients.
        scaler.scale(loss).backward()

        if (i + 1) % iters_to_accumulate == 0:
            # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)

            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

```

----------------------------------------

TITLE: Defining SimpleCNN Model and NLL Loss Function in PyTorch
DESCRIPTION: Defines a standard PyTorch `nn.Module` implementing a simple Convolutional Neural Network (CNN) named `SimpleCNN` with two convolutional layers and two fully connected layers, using ReLU activation and max pooling. It also defines a `loss_fn` using PyTorch's negative log-likelihood loss (`F.nll_loss`) intended for classification tasks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
# Here's a simple CNN and loss function:

class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        x = F.log_softmax(x, dim=1)
        output = x
        return output

def loss_fn(predictions, targets):
    return F.nll_loss(predictions, targets)
```

----------------------------------------

TITLE: Creating a Custom PyTorch Module with Different Training and Evaluation Behavior
DESCRIPTION: Demonstrates how to create a custom PyTorch module that behaves differently in training and evaluation modes using the 'training' attribute.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
class ModalModule(nn.Module):
  def __init__(self):
    super().__init__()

  def forward(self, x):
    if self.training:
      # Add a constant only in training mode.
      return x + 1.
    else:
      return x


m = ModalModule()
x = torch.randn(4)

print('training mode output: {}'.format(m(x)))
: tensor([1.6614, 1.2669, 1.0617, 1.6213, 0.5481])

m.eval()
print('evaluation mode output: {}'.format(m(x)))
: tensor([ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519])
```

----------------------------------------

TITLE: Moving PyTorch Module Parameters to Different Devices
DESCRIPTION: Demonstrates how to move all parameters of a PyTorch module to a CUDA device and change their precision using the 'to' method.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
# Move all parameters to a CUDA device
dynamic_net.to(device='cuda')

# Change precision of all parameters
dynamic_net.to(dtype=torch.float64)

dynamic_net(torch.randn(5, device='cuda', dtype=torch.float64))
: tensor([6.5166], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
```

----------------------------------------

TITLE: Training a PyTorch Neural Network with Optimizer
DESCRIPTION: Illustrates a basic training loop for a PyTorch neural network, including creating an optimizer, computing loss, and updating parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
# Create the network (from previous section) and optimizer
net = Net()
optimizer = torch.optim.SGD(net.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)

# Run a sample training loop that "teaches" the network
# to output the constant zero function
for _ in range(10000):
  input = torch.randn(4)
  output = net(input)
  loss = torch.abs(output)
  net.zero_grad()
  loss.backward()
  optimizer.step()

# After training, switch the module to eval mode to do inference, compute performance metrics, etc.
# (see discussion below for a description of training and evaluation modes)
...
net.eval()
...
```

----------------------------------------

TITLE: Defining a Simple Custom PyTorch Module (MyLinear)
DESCRIPTION: Defines a custom linear layer module named `MyLinear` that inherits from `torch.nn.Module`. It initializes learnable weight and bias parameters using `nn.Parameter` and defines the forward computation (affine transformation) in the `forward` method. This demonstrates the fundamental structure of a PyTorch module.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from torch import nn

class MyLinear(nn.Module):
  def __init__(self, in_features, out_features):
    super().__init__()
    self.weight = nn.Parameter(torch.randn(in_features, out_features))
    self.bias = nn.Parameter(torch.randn(out_features))

  def forward(self, input):
    return (input @ self.weight) + self.bias
```

----------------------------------------

TITLE: Adding Tensors with aten.add in PyTorch
DESCRIPTION: Executes element-wise addition of tensors with options for scalar addition. It aids in tensor arithmetic using PyTorch, critical for operations in neural networks and numerical calculations. Requires tensor inputs and optionally a scalar for addition, outputting the resultant tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/Speech2Text2ForCausalLM_training.txt#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
aten.add.Tensor(T([128], i64), 1)
```

LANGUAGE: Python
CODE:
```
aten.add.Tensor(T([64, 128], i32), 0)
```

LANGUAGE: Python
CODE:
```
aten.add.Tensor(T([64, 128], i64), 1)
```

LANGUAGE: Python
CODE:
```
aten.add.Tensor(T([64, 128, 256], f16), T([64, 128, 256], f16))
```

LANGUAGE: Python
CODE:
```
aten.add.Tensor(T([64, 4, 128, 128], f16), T([64, 1, 128, 128], f16))
```

LANGUAGE: Python
CODE:
```
aten.add.Tensor(T([10000, 256], f16), T([10000, 256], f16))
```

----------------------------------------

TITLE: Saving and Loading PyTorch Tensors - PyTorch - Python
DESCRIPTION: Demonstrates the basic usage of torch.save and torch.load to serialize and deserialize individual tensors. Assumes torch is imported; depends on the torch package. Takes a PyTorch tensor, saves it to a .pt file, and loads it back, preserving the tensor's data. File extensions such as .pt or .pth are conventionally used. The loaded tensor should be identical to the original; limitations relate to file permissions and format compatibility.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> t = torch.tensor([1., 2.])
>>> torch.save(t, 'tensor.pt')
>>> torch.load('tensor.pt')
tensor([1., 2.])
```

----------------------------------------

TITLE: Saving and Loading PyTorch Module State
DESCRIPTION: Shows how to save a trained PyTorch module's state to disk and load it into a new instance of the same module.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
# Save the module
torch.save(net.state_dict(), 'net.pt')

...

# Load the module later on
new_net = Net()
new_net.load_state_dict(torch.load('net.pt'))
: <All keys matched successfully>
```

----------------------------------------

TITLE: Creating and Using Basic Tensor Views in PyTorch
DESCRIPTION: Demonstrates creating a tensor view with t.view() and shows how views share the same underlying data with the base tensor. When a view tensor is modified, the changes are reflected in the base tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_view.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> t = torch.rand(4, 4)
>>> b = t.view(2, 8)
>>> t.storage().data_ptr() == b.storage().data_ptr()  # `t` and `b` share the same underlying data.
True
# Modifying view tensor changes base tensor as well.
>>> b[0][0] = 3.14
>>> t[0][0]
tensor(3.14)
```

----------------------------------------

TITLE: Basic torch.compile Usage with Trigonometric Functions
DESCRIPTION: Demonstrates basic usage of torch.compile with simple trigonometric operations (cos and sin) on GPU. Shows how to compile and run a function with the inductor backend.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_get_started.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
def fn(x):
   a = torch.cos(x)
   b = torch.sin(a)
   return b
new_fn = torch.compile(fn, backend="inductor")
input_tensor = torch.randn(10000).to(device="cuda:0")
a = new_fn(input_tensor)
```

----------------------------------------

TITLE: Computing Gradients using torch.func.grad and functional_call in Python
DESCRIPTION: Shows the PyTorch 2.0+ approach for computing model parameter gradients using `torch.func`. Parameters are extracted using `model.named_parameters()` into a dictionary. A loss function `compute_loss` uses `torch.func.functional_call` to execute the original `model` with the provided `params` dictionary and inputs. `torch.func.grad` is then used to compute gradients with respect to the `params` dictionary. Requires the `torch` library. This is the recommended replacement for the `functorch.make_functional` pattern.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.migrating.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
# ------------------------------------
# using torch.func (as of PyTorch 2.0)
# ------------------------------------
import torch
inputs = torch.randn(64, 3)
targets = torch.randn(64, 3)
model = torch.nn.Linear(3, 3)

params = dict(model.named_parameters())

def compute_loss(params, inputs, targets):
    prediction = torch.func.functional_call(model, params, (inputs,))
    return torch.nn.functional.mse_loss(prediction, targets)

grads = torch.func.grad(compute_loss)(params, inputs, targets)
```

----------------------------------------

TITLE: Disabling Gradient Computation in PyTorch
DESCRIPTION: Demonstrates how to locally disable gradient computation using context managers like torch.no_grad() and torch.set_grad_enabled(). These are useful for temporarily turning off gradients during evaluation or inference.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
>>> x = torch.zeros(1, requires_grad=True)
>>> with torch.no_grad():
...     y = x * 2
>>> y.requires_grad
False

>>> is_train = False
>>> with torch.set_grad_enabled(is_train):
...     y = x * 2
>>> y.requires_grad
False

>>> torch.set_grad_enabled(True)  # this can also be used as a function
>>> y = x * 2
>>> y.requires_grad
True

>>> torch.set_grad_enabled(False)
>>> y = x * 2
>>> y.requires_grad
False
```

----------------------------------------

TITLE: Computing First and Second Order Gradients with torch.func.grad in Python
DESCRIPTION: This snippet demonstrates the basic usage of `torch.func.grad` to compute the gradient of a function. It first calculates the first derivative of `torch.sin(x)` with respect to `x`. It then shows how `grad` can be composed with itself to compute second-order gradients, calculating the second derivative of `torch.sin(x)`. Assertions verify the results against analytical derivatives (`cos(x)` and `-sin(x)`).
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.whirlwind_tour.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from torch.func import grad
x = torch.randn([])
cos_x = grad(lambda x: torch.sin(x))(x)
assert torch.allclose(cos_x, x.cos())

# Second-order gradients
neg_sin_x = grad(grad(lambda x: torch.sin(x)))(x)
assert torch.allclose(neg_sin_x, -x.sin())
```

----------------------------------------

TITLE: Implementing Basic TorchDynamo Optimization
DESCRIPTION: Demonstrates how to use TorchDynamo's optimize decorator to compile and optimize a PyTorch function. Shows a basic example with tensor operations and conditional logic, including a custom compiler function that prints the FX graph.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_overview.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from typing import List
import torch
from torch import _dynamo as torchdynamo
def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):
    print("my_compiler() called with FX graph:")
    gm.graph.print_tabular()
    return gm.forward  # return a python callable

@torchdynamo.optimize(my_compiler)
def toy_example(a, b):
    x = a / (torch.abs(a) + 1)
    if b.sum() < 0:
        b = b * -1
    return x * b
for _ in range(100):
    toy_example(torch.randn(10), torch.randn(10))
```

----------------------------------------

TITLE: Constructing Basic Optimizers in PyTorch
DESCRIPTION: Examples of creating SGD and Adam optimizers with model parameters. The first example shows optimization of all model parameters, while the second optimizes specific variables.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
optimizer = optim.Adam([var1, var2], lr=0.0001)
```

----------------------------------------

TITLE: Implementing a Custom Linear Function in PyTorch
DESCRIPTION: This Python snippet demonstrates how to create a custom linear function by subclassing torch.autograd.Function. Dependencies include PyTorch's autograd module. The implementation includes a forward method for computation, setup_context for managing forward pass tensors, and backward for computing gradients. Ensure ctx is used to correctly mark gradients and input changes. Inputs include tensors for input, weight, and optional bias, while outputs are tensors resulting from matrix multiplication and addition.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
# Inherit from Function
class LinearFunction(Function):

    # Note that forward, setup_context, and backward are @staticmethods
    @staticmethod
    def forward(input, weight, bias):
        output = input.mm(weight.t())
        if bias is not None:
            output += bias.unsqueeze(0).expand_as(output)
        return output

    @staticmethod
    # inputs is a Tuple of all of the inputs passed to forward.
    # output is the output of the forward().
    def setup_context(ctx, inputs, output):
        input, weight, bias = inputs
        ctx.save_for_backward(input, weight, bias)

    # This function has only a single output, so it gets only one gradient
    @staticmethod
    def backward(ctx, grad_output):
        # This is a pattern that is very convenient - at the top of backward
        # unpack saved_tensors and initialize all gradients w.r.t. inputs to
        # None. Thanks to the fact that additional trailing Nones are
        # ignored, the return statement is simple even when the function has
        # optional inputs.
        input, weight, bias = ctx.saved_tensors
        grad_input = grad_weight = grad_bias = None

        # These needs_input_grad checks are optional and there only to
        # improve efficiency. If you want to make your code simpler, you can
        # skip them. Returning gradients for inputs that don't require it is
        # not an error.
        if ctx.needs_input_grad[0]:
            grad_input = grad_output.mm(weight)
        if ctx.needs_input_grad[1]:
            grad_weight = grad_output.t().mm(input)
        if bias is not None and ctx.needs_input_grad[2]:
            grad_bias = grad_output.sum(0)

        return grad_input, grad_weight, grad_bias
```

----------------------------------------

TITLE: Using torch.Size to Get and Access Tensor Dimensions in Python
DESCRIPTION: This example demonstrates how to obtain a torch.Size object from a tensor and how to interact with it using common sequence operations. It shows creating a tensor, getting its size, displaying the full dimensions, accessing individual dimensions, and checking the length of the Size object.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/size.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> x = torch.ones(10, 20, 30)
>>> s = x.size()
>>> s
torch.Size([10, 20, 30])
>>> s[1]
20
>>> len(s)
3
```

----------------------------------------

TITLE: Using torch.compile with Different Backends in Python
DESCRIPTION: Demonstrates how to use torch.compile with various backends for training and inference optimization. Each example shows the syntax for specifying a different backend.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
torch.compile(m, backend="inductor")
torch.compile(m, backend="cudagraphs")
torch.compile(m, backend="ipex")
torch.compile(m, backend="onnxrt")
torch.compile(m, backend="tensorrt")
torch.compile(m, backend="ipex")
torch.compile(m, backend="tvm")
torch.compile(m, backend="openvino")
```

----------------------------------------

TITLE: Calling aten._softmax.default (Python)
DESCRIPTION: Performs the softmax operation on a tensor, converting values into probabilities that sum to 1 along a specified dimension. The examples show usage on 3D float16 tensors, often used in attention mechanisms or output layers.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_2

LANGUAGE: Python
CODE:
```
((T([128, 128, 128], f16), -1, False), {})
```

----------------------------------------

TITLE: Map and Filter Operations with Nesting
DESCRIPTION: Examples of applying map and filter operations with nesting levels on both DataFrame and regular DataPipes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/dataframes_pipes.ipynb#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
dp = get_dataframes_pipe(range = 30, dataframe_size = 3)
dp = dp.filter(lambda x: x.i > 5)
dp = dp.batch(5).filter(lambda x: x.i < 13, nesting_level = 1)
print("Iterate over DataFrame batches")
for i in dp:
    print(i)
```

----------------------------------------

TITLE: Protecting Multiprocessing Code on Windows (Python)
DESCRIPTION: This Python snippet demonstrates the necessary `if __name__ == '__main__':` protection idiom for using `multiprocessing` on Windows. Because Windows uses 'spawn' instead of 'fork' to start processes, the entry point must be guarded to prevent code from being executed multiple times in child processes, avoiding `RuntimeError` during bootstrapping.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#_snippet_6

LANGUAGE: python
CODE:
```
import torch

def main()
    for i, data in enumerate(dataloader):
        # do something here

if __name__ == '__main__':
    main()
```

----------------------------------------

TITLE: Saving Module State Dict and Loading State Into New Module - PyTorch - Python
DESCRIPTION: Provides a concise example of saving a module's state_dict to disk and using torch.load and load_state_dict to apply the saved state to a new, freshly constructed module. This practice is essential for transferring learned parameters to models with identical architecture. Inputs are the module and filename; outputs are a state_dict loaded from disk and a module whose weights now match the previously saved state. Works only when module architectures align.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> torch.save(bn.state_dict(), 'bn.pt')
>>> bn_state_dict = torch.load('bn.pt')
>>> new_bn = torch.nn.BatchNorm1d(3, track_running_stats=True)
>>> new_bn.load_state_dict(bn_state_dict)
<All keys matched successfully>
```

----------------------------------------

TITLE: Training ResNet50 on CIFAR10 with Automatic Mixed Precision on XPU
DESCRIPTION: Sets up and trains a ResNet50 model on the CIFAR10 dataset using automatic mixed precision (AMP) with GradScaler targeting an Intel XPU device. The code includes data preparation, model initialization, and a training loop with periodic loss reporting, ending with model checkpoint saving.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/get_start_xpu.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
use_amp=True

transform = torchvision.transforms.Compose(
    [
        torchvision.transforms.Resize((224, 224)),
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
    ]
)
train_dataset = torchvision.datasets.CIFAR10(
    root=DATA,
    train=True,
    transform=transform,
    download=DOWNLOAD,
)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128)
train_len = len(train_loader)

model = torchvision.models.resnet50()
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)
scaler = torch.amp.GradScaler(device="xpu", enabled=use_amp)

model.train()
model = model.to("xpu")
criterion = criterion.to("xpu")

print(f"Initiating training")
for batch_idx, (data, target) in enumerate(train_loader):
    data = data.to("xpu")
    target = target.to("xpu")
    # set dtype=torch.bfloat16 for BF16
    with torch.autocast(device_type="xpu", dtype=torch.float16, enabled=use_amp):
        output = model(data)
        loss = criterion(output, target)
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
    optimizer.zero_grad()
    if (batch_idx + 1) % 10 == 0:
         iteration_loss = loss.item()
         print(f"Iteration [{batch_idx+1}/{train_len}], Loss: {iteration_loss:.4f}")

torch.save(
    {
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
    },
    "checkpoint.pth",
)

print("Execution finished")
```

----------------------------------------

TITLE: Demonstrating Graph Break in PyTorch Compilation
DESCRIPTION: Example showing how a graph break occurs when using Python builtin functions (open) within a torch.compile decorated function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch

@torch.compile
def fn(x):
    x = x + 1
    with open("test.txt", "r") as f:
        return x + len(f.read())

fn(torch.ones(3, 3))
```

----------------------------------------

TITLE: Checking Tensor Broadcastability in Python
DESCRIPTION: Demonstrates the rules for determining if two PyTorch tensors are broadcastable. It shows examples of tensors with the same shape (always broadcastable), tensors where one lacks dimensions (not broadcastable), and tensors where trailing dimensions align according to the rules (broadcastable and not broadcastable cases).
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/broadcasting.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> x=torch.empty(5,7,3)
>>> y=torch.empty(5,7,3)
# same shapes are always broadcastable (i.e. the above rules always hold)

>>> x=torch.empty((0,))
>>> y=torch.empty(2,2)
# x and y are not broadcastable, because x does not have at least 1 dimension

# can line up trailing dimensions
>>> x=torch.empty(5,3,4,1)
>>> y=torch.empty(  3,1,1)
# x and y are broadcastable.
# 1st trailing dimension: both have size 1
# 2nd trailing dimension: y has size 1
# 3rd trailing dimension: x size == y size
# 4th trailing dimension: y dimension doesn't exist

# but:
>>> x=torch.empty(5,2,4,1)
>>> y=torch.empty(  3,1,1)
# x and y are not broadcastable, because in the 3rd trailing dimension 2 != 3
```

----------------------------------------

TITLE: Gradient Clipping with torch.amp and GradScaler (Python)
DESCRIPTION: This snippet shows how to perform gradient clipping with AMP in PyTorch by first unscaling gradients via GradScaler.unscale_ before calling torch.nn.utils.clip_grad_norm_. This ensures clipping is performed using unscaled gradients, preserving correct numerical thresholds. Requires PyTorch, model/optimizer set up for mixed precision, and clip_grad_norm_ utility. The scaler tracks if unscale_ was already called to avoid redundant unscaling.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/amp_examples.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
scaler = GradScaler()

for epoch in epochs:
    for input, target in data:
        optimizer.zero_grad()
        with autocast(device_type='cuda', dtype=torch.float16):
            output = model(input)
            loss = loss_fn(output, target)
        scaler.scale(loss).backward()

        # Unscales the gradients of optimizer's assigned params in-place
        scaler.unscale_(optimizer)

        # Since the gradients of optimizer's assigned params are unscaled, clips as usual:
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)

        # optimizer's gradients are already unscaled, so scaler.step does not unscale them,
        # although it still skips optimizer.step() if the gradients contain infs or NaNs.
        scaler.step(optimizer)

        # Updates the scale for next iteration.
        scaler.update()

```

----------------------------------------

TITLE: Implementing Gradient Penalty with AMP and GradScaler (Python)
DESCRIPTION: This example illustrates combining gradient penalty computation with automatic mixed precision and gradient scaling. The penalty's autograd.grad outputs are scaled, so manual unscaling is performed before penalty calculation. All penalty operations are wrapped in autocast, and GradScaler manages scaling for numeric stability. This pattern is used for regularization techniques requiring gradient norms. Assumes model, optimizer, and torch.amp are available.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/amp_examples.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
scaler = GradScaler()

for epoch in epochs:
    for input, target in data:
        optimizer.zero_grad()
        with autocast(device_type='cuda', dtype=torch.float16):
            output = model(input)
            loss = loss_fn(output, target)

        # Scales the loss for autograd.grad's backward pass, producing scaled_grad_params
        scaled_grad_params = torch.autograd.grad(outputs=scaler.scale(loss),
                                                 inputs=model.parameters(),
                                                 create_graph=True)

        # Creates unscaled grad_params before computing the penalty. scaled_grad_params are
        # not owned by any optimizer, so ordinary division is used instead of scaler.unscale_:
        inv_scale = 1./scaler.get_scale()
        grad_params = [p * inv_scale for p in scaled_grad_params]

        # Computes the penalty term and adds it to the loss
        with autocast(device_type='cuda', dtype=torch.float16):
            grad_norm = 0
            for grad in grad_params:
                grad_norm += grad.pow(2).sum()
            grad_norm = grad_norm.sqrt()
            loss = loss + grad_norm

        # Applies scaling to the backward call as usual.
        # Accumulates leaf gradients that are correctly scaled.
        scaler.scale(loss).backward()

```

----------------------------------------

TITLE: Releasing Shared CUDA Tensor Memory Promptly in Consumer Process (Python)
DESCRIPTION: Demonstrates the recommended practice for handling shared CUDA tensors received via a multiprocessing queue. The tensor `x` obtained from `queue.get()` should be explicitly deleted using `del x` as soon as it's no longer needed to release the underlying shared memory. This prevents the producer process from unnecessarily holding onto the memory.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/multiprocessing.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
## Good
x = queue.get()
# do somethings with x
del x
```

----------------------------------------

TITLE: Exporting with Optional Input Provided using torch.export (Python)
DESCRIPTION: This snippet demonstrates how `torch.export.export` handles optional arguments when they *are* provided during tracing. The module `M` has an optional argument `y`. When exported with both `x` and `y` provided, the resulting graph includes the code path where `y` is used (`return y * x`), and the exported function signature requires both `x` and `y` as inputs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.programming_model.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
class M(torch.nn.Module):
    def forward(self, x, y=None):
        if y is not None:
            return y * x
        return x + x

# Optional input is passed in
ep = torch.export.export(M(), (torch.randn(3, 3), torch.randn(3, 3)))
print(ep)
"""
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "f32[3, 3]", y: "f32[3, 3]"):
            # File: /data/users/angelayi/pytorch/moo.py:15 in forward, code: return y * x
            mul: "f32[3, 3]" = torch.ops.aten.mul.Tensor(y, x);  y = x = None
            return (mul,)
"""
```

----------------------------------------

TITLE: Rewriting Data-Dependent Branching with torch.cond in PyTorch (Python)
DESCRIPTION: These snippets compare two approaches to handling an if-statement with a data-dependent (dynamic) condition in a torch.nn.Module: using native Python branching vs. explicitly using torch.cond. The first module relies on x.sum() > 0 for branching, which is not permitted for dynamic shapes when tracing/exporting; the second rewrites the logic using torch.cond, allowing both branches to be traced. Both expect the PyTorch framework and proper import of torch.cond. The input x must be a Tensor, and the outputs are the result of either x.sin() or x.cos(), depending on the sum. No additional dependencies are required beyond PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.programming_model.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
class M_old(torch.nn.Module):
    def forward(self, x):
        if x.sum() > 0:
            return x.sin()
        else:
            return x.cos()
```

LANGUAGE: python
CODE:
```
class M_new(torch.nn.Module):
    def forward(self, x):
        return torch.cond(
            pred=x.sum() > 0,
            true_fn=lambda x: x.sin(),
            false_fn=lambda x: x.cos(),
            operands=(x,),
        )
```

----------------------------------------

TITLE: Compiling PyTorch Training Loop
DESCRIPTION: Example showing how to optimize a training step with torch.compile, including gradient computation and optimization
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
# training
model = ...
opt = torch.optim.Adam(model.parameters())

@torch.compile
def train(mod, data):
    opt.zero_grad(True)
    pred = mod(data[0])
    loss = torch.nn.CrossEntropyLoss()(pred, data[1])
    loss.backward()
    opt.step()

for _ in range(N_ITERS):
    inp = ...
    train(model, inp)
```

----------------------------------------

TITLE: Constructing PyTorch Tensor
DESCRIPTION: Documents the constructor behavior for torch.Tensor class. When provided with a torch.Size argument, it creates an empty tensor of the specified size. This constructor does not support explicit dtype or device specification.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensors.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
data (array_like)  # The tensor to construct from
device (torch.device, optional)  # The desired device of returned tensor. Default: if None, uses same device as source tensor
```

----------------------------------------

TITLE: Exporting Model with Dynamic Input Shapes - PyTorch - Python
DESCRIPTION: This snippet illustrates creating a PyTorch module with two branches and a persistent buffer, preparing example inputs, and specifying dynamic input dimension using torch.export.Dim and the export dynamic_shapes argument. Demonstrates setting the first input dimension (typically batch) as dynamic for flexible exported models. Dependencies: PyTorch; inputs required: appropriately shaped tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
import torch
from torch.export import Dim, export

class M(torch.nn.Module):
    def __init__(self):
        super().__init__()

        self.branch1 = torch.nn.Sequential(
            torch.nn.Linear(64, 32), torch.nn.ReLU()
        )
        self.branch2 = torch.nn.Sequential(
            torch.nn.Linear(128, 64), torch.nn.ReLU()
        )
        self.buffer = torch.ones(32)

    def forward(self, x1, x2):
        out1 = self.branch1(x1)
        out2 = self.branch2(x2)
        return (out1 + self.buffer, out2)

example_args = (torch.randn(32, 64), torch.randn(32, 128))

# Create a dynamic batch size
batch = Dim("batch")
# Specify that the first dimension of each input is that batch size
dynamic_shapes = {"x1": {0: batch}, "x2": {0: batch}}
```

----------------------------------------

TITLE: Element-wise Operations in PyTorch
DESCRIPTION: Element-wise multiplication and division operations on tensors. These are common in various parts of neural networks, including activation functions and normalization layers.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
cnt: 4, ((T([128, 64, 64, 64], f16), T([128, 64, 64, 64], f16, stride=(64, 1, 0, 0))), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([128, 2048, 8, 8], f16, stride=(2048, 1, 0, 0)), 64), {})
```

----------------------------------------

TITLE: Defining a Dynamic Network using ModuleList and ModuleDict
DESCRIPTION: Defines a `DynamicNet` module that dynamically creates a variable number of linear layers using `nn.ModuleList` and stores different activation functions in `nn.ModuleDict`. The `forward` method demonstrates how to iterate through the `ModuleList` and select an activation from the `ModuleDict` based on an input argument (`act`).
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
class DynamicNet(nn.Module):
  def __init__(self, num_layers):
    super().__init__()
    self.linears = nn.ModuleList(
      [MyLinear(4, 4) for _ in range(num_layers)])
    self.activations = nn.ModuleDict({
      'relu': nn.ReLU(),
      'lrelu': nn.LeakyReLU()
    })
    self.final = MyLinear(4, 1)
  def forward(self, x, act):
    for linear in self.linears:
      x = linear(x)
      x = self.activations[act](x)
    x = self.final(x)
    return x

dynamic_net = DynamicNet(3)
sample_input = torch.randn(4)
output = dynamic_net(sample_input, 'relu')
```

----------------------------------------

TITLE: Iterating Through Named Parameters of a Module
DESCRIPTION: Shows how to access and print the registered parameters (weight and bias) of the `MyLinear` module instance `m` using the `named_parameters()` method. This method yields tuples containing the parameter name and the parameter tensor itself, indicating they require gradients.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
for parameter in m.named_parameters():
  print(parameter)
: ('weight', Parameter containing:
tensor([[ 1.0597,  1.1796,  0.8247],
        [-0.5080, -1.2635, -1.1045],
        [ 0.0593,  0.2469, -1.4299],
        [-0.4926, -0.5457,  0.4793]], requires_grad=True))
('bias', Parameter containing:
tensor([ 0.3634,  0.2015, -0.8525], requires_grad=True))
```

----------------------------------------

TITLE: Initializing PyTorch Module State Management
DESCRIPTION: Demonstrates state management in PyTorch modules including parameter dictionaries, persistent and non-persistent buffers, and state saving/loading.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
self.param_dict = nn.ParameterDict({
  'foo': nn.Parameter(torch.randn(3)),
  'bar': nn.Parameter(torch.randn(4))
})

self.register_buffer('buffer1', torch.randn(4), persistent=True)
self.register_buffer('buffer2', torch.randn(5), persistent=False)
self.register_buffer('buffer3', None)
self.linear = nn.Linear(2, 3)

m = StatefulModule()
torch.save(m.state_dict(), 'state.pt')
m_loaded = StatefulModule()
m_loaded.load_state_dict(torch.load('state.pt'))
```

----------------------------------------

TITLE: Tensor Indexing and Modification
DESCRIPTION: Demonstrates accessing and modifying tensor elements using Python indexing notation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensors.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])
>>> print(x[1][2])
tensor(6)
>>> x[0][1] = 8
>>> print(x)
tensor([[ 1,  8,  3],
        [ 4,  5,  6]])
```

----------------------------------------

TITLE: Defining MLP Model and Exporting to ONNX (Python)
DESCRIPTION: Defines a simple Multilayer Perceptron (MLP) model using `torch.nn.Module` and demonstrates how to export it to an ONNX graph using `torch.onnx.export` with the `dynamo=True` flag. Requires a PyTorch model instance and sample input data. Outputs an `ONNXProgram` object.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_dynamo.rst#_snippet_1

LANGUAGE: python
CODE:
```
import torch
import torch.nn as nn

class MLPModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc0 = nn.Linear(8, 8, bias=True)
        self.fc1 = nn.Linear(8, 4, bias=True)
        self.fc2 = nn.Linear(4, 2, bias=True)
        self.fc3 = nn.Linear(2, 2, bias=True)

    def forward(self, tensor_x: torch.Tensor):
        tensor_x = self.fc0(tensor_x)
        tensor_x = torch.sigmoid(tensor_x)
        tensor_x = self.fc1(tensor_x)
        tensor_x = torch.sigmoid(tensor_x)
        tensor_x = self.fc2(tensor_x)
        tensor_x = torch.sigmoid(tensor_x)
        output = self.fc3(tensor_x)
        return output

model = MLPModel()
tensor_x = torch.rand((97, 8), dtype=torch.float32)
onnx_program = torch.onnx.export(model, (tensor_x,), dynamo=True)
```

----------------------------------------

TITLE: Creating and Using torch.device Objects in PyTorch
DESCRIPTION: This code snippet illustrates how to create torch.device objects using different constructors, including string-based and combined string/ordinal approaches. It covers various device types like CUDA, CPU, and MPS.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> torch.device('cuda:0')
device(type='cuda', index=0)

>>> torch.device('cpu')
device(type='cpu')

>>> torch.device('mps')
device(type='mps')

>>> torch.device('cuda')  # current cuda device
device(type='cuda')
```

----------------------------------------

TITLE: Configuring PyTorch Module Initialization
DESCRIPTION: Shows different ways to initialize PyTorch modules with specific device placement and data types, including custom initialization techniques.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
# Initialize module directly onto GPU.
m = nn.Linear(5, 3, device='cuda')

# Initialize module with 16-bit floating point parameters.
m = nn.Linear(5, 3, dtype=torch.half)

# Skip default parameter initialization and perform custom initialization.
m = torch.nn.utils.skip_init(nn.Linear, 5, 3)
nn.init.orthogonal_(m.weight)
```

----------------------------------------

TITLE: Batch Matrix Multiplication with aten.bmm in PyTorch
DESCRIPTION: Executes batch matrix-matrix product over tensors of varying shapes characteristic of minibatch operations, emphasizing f16 processing. PyTorch is required, with CUDA facilitating rapid execution.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MobileBertForMaskedLM_training.txt#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
aten.bmm.default, ((T([64, 128, 32], f16), T([64, 32, 128], f16)), {})
```

----------------------------------------

TITLE: Capturing Full Neural Network Iteration with CUDA Graph PyTorch
DESCRIPTION: This example illustrates capturing a complete neural network training step (forward pass, loss calculation, backward pass, and optimizer step) into a CUDA graph. It defines a simple linear model, loss function, and optimizer. The snippet includes a warmup loop using placeholder data and the capture block using `torch.cuda.graph`. It then demonstrates replaying the captured graph on multiple batches of real data by copying them into the static input and target tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_41

LANGUAGE: python
CODE:
```
N, D_in, H, D_out = 640, 4096, 2048, 1024
model = torch.nn.Sequential(torch.nn.Linear(D_in, H),
                            torch.nn.Dropout(p=0.2),
                            torch.nn.Linear(H, D_out),
                            torch.nn.Dropout(p=0.1)).cuda()
loss_fn = torch.nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

# Placeholders used for capture
static_input = torch.randn(N, D_in, device='cuda')
static_target = torch.randn(N, D_out, device='cuda')

# warmup
# Uses static_input and static_target here for convenience,
# but in a real setting, because the warmup includes optimizer.step()
# you must use a few batches of real data.
s = torch.cuda.Stream()
s.wait_stream(torch.cuda.current_stream())
with torch.cuda.stream(s):
    for i in range(3):
        optimizer.zero_grad(set_to_none=True)
        y_pred = model(static_input)
        loss = loss_fn(y_pred, static_target)
        loss.backward()
        optimizer.step()
torch.cuda.current_stream().wait_stream(s)

# capture
g = torch.cuda.CUDAGraph()
# Sets grads to None before capture, so backward() will create
# .grad attributes with allocations from the graph's private pool
optimizer.zero_grad(set_to_none=True)
with torch.cuda.graph(g):
    static_y_pred = model(static_input)
    static_loss = loss_fn(static_y_pred, static_target)
    static_loss.backward()
    optimizer.step()

real_inputs = [torch.rand_like(static_input) for _ in range(10)]
real_targets = [torch.rand_like(static_target) for _ in range(10)]

for data, target in zip(real_inputs, real_targets):
    # Fills the graph's input memory with new data to compute on
    static_input.copy_(data)
    static_target.copy_(target)
    # replay() includes forward, backward, and step.
    # You don't even need to call optimizer.zero_grad() between iterations
    # because the captured backward refills static .grad tensors in place.
    g.replay()
    # Params have been updated. static_y_pred, static_loss, and .grad
    # attributes hold values from computing on this iteration's data.
```

----------------------------------------

TITLE: Calling aten.bmm.default (Python)
DESCRIPTION: Performs a batch matrix multiplication (batch1 @ batch2). Essential for operations involving batches of matrices, such as in attention mechanisms or transformer layers. Examples show various batch and matrix dimensions with float16 data.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_9

LANGUAGE: Python
CODE:
```
((T([128, 128, 64], f16), T([128, 64, 128], f16, stride=(8192, 1, 64))), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 128, 128], f16), T([128, 128, 64], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 128, 128], f16, stride=(16384, 1, 128)), T([128, 128, 64], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 64, 128], f16, stride=(8192, 1, 64)), T([128, 128, 128], f16)), {})
```

----------------------------------------

TITLE: Correctly Passing Received Shared Tensors via Cloning (Python)
DESCRIPTION: Shows the correct way to pass the data content of a received shared tensor to another part of the application or another process via a queue. Instead of passing the shared tensor `x` directly, a process-local copy `x_clone` is created using `x.clone()`, and this independent copy is then put into `queue_2`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/multiprocessing.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
# you need to create a process-local copy
x = queue.get()
x_clone = x.clone()
queue_2.put(x_clone)
```

----------------------------------------

TITLE: Constructing Optimizers with Named Parameters in PyTorch
DESCRIPTION: Examples of creating SGD and Adam optimizers using named parameters. This approach allows for more explicit parameter tracking during optimization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
optimizer = optim.SGD(model.named_parameters(), lr=0.01, momentum=0.9)
optimizer = optim.Adam([('layer0', var1), ('layer1', var2)], lr=0.0001)
```

----------------------------------------

TITLE: Seeding PyTorch Random Number Generator in Python
DESCRIPTION: Seeds the global random number generator for all PyTorch devices (CPU and CUDA) to a specific value (0 in this case) using `torch.manual_seed()`. This helps control randomness in PyTorch operations for reproducible results, ensuring the same sequence of random numbers is generated each time the application runs in the same environment, provided other sources of nondeterminism are also addressed.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/randomness.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
torch.manual_seed(0)
```

----------------------------------------

TITLE: Training Loop Memory Management in PyTorch
DESCRIPTION: This snippet demonstrates a common pitfall in accumulating history across a training loop that can lead to excessive memory usage in PyTorch. It shows how to prevent memory buildup by converting a differentiable loss variable before aggregation to avoid retaining unnecessary computation history.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/faq.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
    total_loss = 0
    for i in range(10000):
        optimizer.zero_grad()
        output = model(input)
        loss = criterion(output)
        loss.backward()
        optimizer.step()
        total_loss += loss
```

----------------------------------------

TITLE: Implementing Elastic Training Script in PyTorch
DESCRIPTION: This code snippet demonstrates the structure of a PyTorch training script compatible with elastic training. It includes checkpoint loading and saving, process group initialization, and a training loop that resumes from the last saved epoch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/train_script.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
def main():
     args = parse_args(sys.argv[1:])
     state = load_checkpoint(args.checkpoint_path)
     initialize(state)

     # torch.distributed.run ensures that this will work
     # by exporting all the env vars needed to initialize the process group
     torch.distributed.init_process_group(backend=args.backend)

     for i in range(state.epoch, state.total_num_epochs)
          for batch in iter(state.dataset)
              train(batch, state.model)

          state.epoch += 1
          save_checkpoint(state)
```

----------------------------------------

TITLE: Autograd with Tensors
DESCRIPTION: Demonstrates creating a tensor with gradient tracking and performing automatic differentiation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensors.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)
>>> out = x.pow(2).sum()
>>> out.backward()
>>> x.grad
tensor([[ 2.0000, -2.0000],
        [ 2.0000,  2.0000]])
```

----------------------------------------

TITLE: Importing PyTorch CUDA Module
DESCRIPTION: This code snippet shows how to import the torch.cuda module, which provides CUDA-related functionality in PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cuda.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
import torch.cuda
```

----------------------------------------

TITLE: Configuring PyTorch DataLoader for Reproducibility in Python
DESCRIPTION: Provides a complete example for configuring a PyTorch `DataLoader` to achieve reproducible data loading, especially when using multiple workers. It defines a `worker_init_fn` (`seed_worker`) to individually seed each worker based on the main process's initial seed, ensuring consistent randomness (e.g., for augmentations) within each worker. It also uses a pre-seeded `torch.Generator` passed to the `DataLoader` to control shuffling and sampling randomness.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/randomness.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
def seed_worker(worker_id):
    worker_seed = torch.initial_seed() % 2**32
    numpy.random.seed(worker_seed)
    random.seed(worker_seed)

g = torch.Generator()
g.manual_seed(0)

DataLoader(
    train_dataset,
    batch_size=batch_size,
    num_workers=num_workers,
    worker_init_fn=seed_worker,
    generator=g,
)
```

----------------------------------------

TITLE: Chaining Multiple Learning Rate Schedulers in PyTorch
DESCRIPTION: Example demonstrating how to chain multiple schedulers together. The schedulers are applied sequentially, with each one modifying the learning rate produced by the previous scheduler.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
scheduler1 = ExponentialLR(optimizer, gamma=0.9)
scheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)

for epoch in range(20):
    for input, target in dataset:
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
    scheduler1.step()
    scheduler2.step()
```

----------------------------------------

TITLE: Creating Basic Tensor from List
DESCRIPTION: Demonstrates creating a tensor from a Python list using torch.tensor constructor. Shows both float and numpy array inputs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensors.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> torch.tensor([[1., -1.], [1., -1.]])
tensor([[ 1.0000, -1.0000],
        [ 1.0000, -1.0000]])
>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))
tensor([[ 1,  2,  3],
        [ 4,  5,  6]])
```

----------------------------------------

TITLE: BERT Model Optimization with HuggingFace
DESCRIPTION: Demonstrates how to optimize a pre-trained BERT model from HuggingFace using torch.compile with the inductor backend for GPU inference.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_get_started.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import torch
from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained("bert-base-uncased").to(device="cuda:0")
model = torch.compile(model, backend="inductor")
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt').to(device="cuda:0")
output = model(**encoded_input)
```

----------------------------------------

TITLE: Create Tensor and Move Module to Specific Device PyTorch
DESCRIPTION: Demonstrates how to create new tensors on a specified device and move existing modules to that device using the `device` attribute. This pattern is key for writing device-agnostic code.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_35

LANGUAGE: python
CODE:
```
x = torch.empty((8, 42), device=args.device)
net = Network().to(device=args.device)
```

----------------------------------------

TITLE: MyPy-style Type Annotation in TorchScript
DESCRIPTION: Example of MyPy-style type annotation in TorchScript using a comment to specify types. All parameters must be annotated in this style, even if they use the default type.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
import torch

def f(a, b):
    # type: (torch.Tensor, int) → torch.Tensor
    return a+b

m = torch.jit.script(f)
print("TorchScript:", m(torch.ones([6]), 100))
```

----------------------------------------

TITLE: Registering Parameters and Buffers in a PyTorch Module
DESCRIPTION: Showcases various ways to register parameters and buffers within a PyTorch module, including direct attribute assignment, string-based registration, and using ParameterList.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
class StatefulModule(nn.Module):
  def __init__(self):
    super().__init__()
    # Setting a nn.Parameter as an attribute of the module automatically registers the tensor
    # as a parameter of the module.
    self.param1 = nn.Parameter(torch.randn(2))

    # Alternative string-based way to register a parameter.
    self.register_parameter('param2', nn.Parameter(torch.randn(3)))

    # Reserves the "param3" attribute as a parameter, preventing it from being set to anything
    # except a parameter. "None" entries like this will not be present in the module's state_dict.
    self.register_parameter('param3', None)

    # Registers a list of parameters.
    self.param_list = nn.ParameterList([nn.Parameter(torch.randn(2)) for i in range(3)])

    # Registers a dictionary of parameters.
```

----------------------------------------

TITLE: Determine Device based on CUDA Availability PyTorch
DESCRIPTION: Uses `argparse` to check for a `--disable-cuda` flag and `torch.cuda.is_available()` to determine if a CUDA device should be used. Sets a `torch.device` object (`args.device`) accordingly (cuda or cpu) for device-agnostic code.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_34

LANGUAGE: python
CODE:
```
import argparse
import torch

parser = argparse.ArgumentParser(description='PyTorch Example')
parser.add_argument('--disable-cuda', action='store_true',
                    help='Disable CUDA')
args = parser.parse_args()
args.device = None
if not args.disable_cuda and torch.cuda.is_available():
    args.device = torch.device('cuda')
else:
    args.device = torch.device('cpu')
```

----------------------------------------

TITLE: Demonstrating PyTorch Type Promotion with Different Tensor Types
DESCRIPTION: This code snippet shows examples of PyTorch's type promotion rules when performing arithmetic operations between tensors of different data types. It includes examples with various tensor types including floating point, integer, boolean, and complex tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> float_tensor = torch.ones(1, dtype=torch.float)
>>> double_tensor = torch.ones(1, dtype=torch.double)
>>> complex_float_tensor = torch.ones(1, dtype=torch.complex64)
>>> complex_double_tensor = torch.ones(1, dtype=torch.complex128)
>>> int_tensor = torch.ones(1, dtype=torch.int)
>>> long_tensor = torch.ones(1, dtype=torch.long)
>>> uint_tensor = torch.ones(1, dtype=torch.uint8)
>>> bool_tensor = torch.ones(1, dtype=torch.bool)
# zero-dim tensors
>>> long_zerodim = torch.tensor(1, dtype=torch.long)
>>> int_zerodim = torch.tensor(1, dtype=torch.int)

>>> torch.add(5, 5).dtype
torch.int64
# 5 is an int64, but does not have higher category than int_tensor so is not considered.
>>> (int_tensor + 5).dtype
torch.int32
>>> (int_tensor + long_zerodim).dtype
torch.int32
>>> (long_tensor + int_tensor).dtype
torch.int64
>>> (bool_tensor + long_tensor).dtype
torch.int64
>>> (bool_tensor + uint_tensor).dtype
torch.uint8
>>> (float_tensor + double_tensor).dtype
torch.float64
>>> (complex_float_tensor + complex_double_tensor).dtype
torch.complex128
>>> (bool_tensor + int_tensor).dtype
torch.int32
# Since long is a different kind than float, result dtype only needs to be large enough
# to hold the float.
>>> torch.add(long_tensor, float_tensor).dtype
torch.float32
```

----------------------------------------

TITLE: Exporting AlexNet to ONNX using TorchScript Tracing in Python
DESCRIPTION: This snippet demonstrates how to export a pre-trained AlexNet model from PyTorch to an ONNX file named `alexnet.onnx`. It uses tracing by executing the model once with dummy input. Dependencies include `torch` and `torchvision`. The output is an `alexnet.onnx` file containing the model graph and parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_0

LANGUAGE: python
CODE:
```
import torch
import torchvision

dummy_input = torch.randn(10, 3, 224, 224, device="cuda")
model = torchvision.models.alexnet(pretrained=True).cuda()

# Providing input and output names sets the display names for values
# within the model's graph. Setting these does not change the semantics
# of the graph; it is only for readability.
#
# The inputs to the network consist of the flat list of inputs (i.e.
# the values you would pass to the forward() method) followed by the
# flat list of parameters. You can partially specify names, i.e. provide
# a list here shorter than the number of inputs to the model, and we will
# only set that subset of names, starting from the beginning.
input_names = [ "actual_input_1" ] + [ "learned_%d" % i for i in range(16) ]
output_names = [ "output1" ]

torch.onnx.export(model, dummy_input, "alexnet.onnx", verbose=True, input_names=input_names, output_names=output_names)
```

----------------------------------------

TITLE: Disabling PyTorch Compilation for Specific Functions
DESCRIPTION: Example showing how to use the torch.compiler.disable decorator to exclude problematic functions from compilation
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
def bad1_inner(...):
    # skipped

@torch.compiler.disable
def bad1_outer(...):
    # skipped
    bad1_inner(...)

def bad2_inner(...)
    # traced

@torch.compiler.disable(recursive=False)
def bad2_outer(...):
    # skipped
    bad2_inner(...)

@torch.compile
def fn(...):
    # graph break
    bad1_outer(...)
    ...
    # graph break
    bad2_outer(...)
```

----------------------------------------

TITLE: Installing PyTorch on MacOS Bash
DESCRIPTION: Runs the standard Python setup script using the 'python3' command with the 'develop' flag to build and install PyTorch in editable mode on macOS systems.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_13

LANGUAGE: Bash
CODE:
```
python3 setup.py develop
```

----------------------------------------

TITLE: Working with Saved Tensors in PyTorch Autograd
DESCRIPTION: This Python code snippet demonstrates how autograd saves intermediary tensor states during the forward pass using PyTorch's mechanism. It illustrates that the attribute '_saved_self' in 'grad_fn' refers to the original tensor, while in some cases, the saved tensor might be a different object but sharing the same storage. Requires 'torch' library.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/autograd.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
x = torch.randn(5, requires_grad=True)
y = x.pow(2)
print(x.equal(y.grad_fn._saved_self))  # True
print(x is y.grad_fn._saved_self)  # True
```

----------------------------------------

TITLE: Loading Pretrained Weights for PyTorch Models in Python
DESCRIPTION: This snippet shows how to load pretrained weights for a model from a local checkpoint or a URL. It highlights the use of the 'torch' library for model weight management and provides a choice between local and web-hosted weights.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/hub.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
if pretrained:
    # For checkpoint saved in local GitHub repo, e.g. <RELATIVE_PATH_TO_CHECKPOINT>=weights/save.pth
    dirname = os.path.dirname(__file__)
    checkpoint = os.path.join(dirname, <RELATIVE_PATH_TO_CHECKPOINT>)
    state_dict = torch.load(checkpoint)
    model.load_state_dict(state_dict)

    # For checkpoint saved elsewhere
    checkpoint = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'
    model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))
```

----------------------------------------

TITLE: Creating and Operating on Tensors with ATen C++
DESCRIPTION: This snippet demonstrates fundamental tensor operations using the ATen C++ API. It shows how to create tensors (`at::ones`, `at::randn`), perform type casting (`to`), and execute basic arithmetic operations (`+`). The ATen API provides the core tensor class and operations without automatic differentiation by default. Requires including the `ATen/ATen.h` header.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/index.rst#_snippet_0

LANGUAGE: cpp
CODE:
```
#include <ATen/ATen.h>

at::Tensor a = at::ones({2, 2}, at::kInt);
at::Tensor b = at::randn({2, 2});
auto c = a + b.to(at::kInt);
```

----------------------------------------

TITLE: Filtering Data with PyTorch DataPipes
DESCRIPTION: Demonstrates the use of the filter() method to selectively include or exclude data based on a condition. Shows filtering at different nesting levels and handling of empty batches.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(10).filter(lambda x: x % 2 == 0)
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(10)
dp = dp.batch(3).filter(lambda x: len(x) > 2)
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(10)
dp = dp.batch(3).filter(lambda x: x > 4, nesting_level = 1)
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(10)
dp = dp.batch(3).filter(lambda x: x > 4, nesting_level = -1, drop_empty_batches = False)
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(20)
dp = dp.batch(3).batch(2).batch(2).filter(lambda x: x < 4 or x > 9 , nesting_level = -1, drop_empty_batches = False)
for i in dp:
    print(i)
```

----------------------------------------

TITLE: Creating a Tensor with LibTorch in C++
DESCRIPTION: Minimal C++ example that creates a random tensor using LibTorch and prints it. This demonstrates basic usage of the PyTorch C++ API.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/installing.rst#2025-04-22_snippet_2

LANGUAGE: cpp
CODE:
```
#include <torch/torch.h>
#include <iostream>

int main() {
  torch::Tensor tensor = torch::rand({2, 3});
  std::cout << tensor << std::endl;
}
```

----------------------------------------

TITLE: Creating a Multi-Dimensional Tensor with PyTorch C++
DESCRIPTION: This snippet shows how to create a three-dimensional tensor filled with values from a unit normal distribution using the randn() factory function. It also demonstrates how to verify the tensor's size.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_2

LANGUAGE: cpp
CODE:
```
torch::Tensor tensor = torch::randn({3, 4, 5});
assert(tensor.sizes() == std::vector<int64_t>{3, 4, 5});
```

----------------------------------------

TITLE: Correct Concurrent Stream Usage with Synchronization - PyTorch Python
DESCRIPTION: This snippet corrects the previous example by adding necessary synchronization when using a non-default stream. `s.wait_stream(torch.cuda.default_stream(cuda))` ensures that operations on the custom stream `s` wait for the default stream (where `A` was initialized) to complete. `A.record_stream(s)` ensures that memory for `A` is not deallocated before operations on stream `s` that use `A` are finished.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_13

LANGUAGE: Python
CODE:
```
cuda = torch.device('cuda')
s = torch.cuda.Stream()  # Create a new stream.
A = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)
s.wait_stream(torch.cuda.default_stream(cuda))  # NEW!
with torch.cuda.stream(s):
    B = torch.sum(A)
A.record_stream(s)  # NEW!
```

----------------------------------------

TITLE: Equivalent Methods for Device Specification in PyTorch Tensor Creation
DESCRIPTION: This code snippet illustrates three equivalent ways to specify a device when creating a random tensor: using a torch.device object, a properly formatted string, or a legacy integer device ordinal.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> torch.randn((2,3), device=torch.device('cuda:1'))
>>> torch.randn((2,3), device='cuda:1')
>>> torch.randn((2,3), device=1)  # legacy
```

----------------------------------------

TITLE: Implementing Custom Linear Module in PyTorch
DESCRIPTION: Implementation of a custom Linear module that inherits from nn.Module. Includes initialization of weights and bias parameters, forward pass implementation, and optional string representation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
class Linear(nn.Module):
    def __init__(self, input_features, output_features, bias=True):
        super().__init__()
        self.input_features = input_features
        self.output_features = output_features

        # nn.Parameter is a special kind of Tensor, that will get
        # automatically registered as Module's parameter once it's assigned
        # as an attribute. Parameters and buffers need to be registered, or
        # they won't appear in .parameters() (doesn't apply to buffers), and
        # won't be converted when e.g. .cuda() is called. You can use
        # .register_buffer() to register buffers.
        # nn.Parameters require gradients by default.
        self.weight = nn.Parameter(torch.empty(output_features, input_features))
        if bias:
            self.bias = nn.Parameter(torch.empty(output_features))
        else:
            # You should always register all possible parameters, but the
            # optional ones can be None if you want.
            self.register_parameter('bias', None)

        # Not a very smart way to initialize weights
        nn.init.uniform_(self.weight, -0.1, 0.1)
        if self.bias is not None:
            nn.init.uniform_(self.bias, -0.1, 0.1)

    def forward(self, input):
        # See the autograd section for explanation of what happens here.
        return LinearFunction.apply(input, self.weight, self.bias)

    def extra_repr(self):
        # (Optional)Set the extra information about this module. You can test
        # it by printing an object of this class.
        return 'input_features={}, output_features={}, bias={}'.format(
            self.input_features, self.output_features, self.bias is not None
        )
```

----------------------------------------

TITLE: Batch Matrix Multiplication via aten.bmm
DESCRIPTION: The aten.bmm.default performs batch matrix multiplication on sets of 3D tensors. PyTorch dependency ensures that inputs comprising compatible tensor shapes perform batched matrix multiplications with resulting output tensors having batched products.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForConditionalGeneration_training.txt#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
Operator: aten.bmm.default
cnt: 36, ((T([96, 128, 64], f16), T([96, 64, 128], f16, stride=(8192, 1, 64))), {})
cnt: 36, ((T([96, 128, 128], f16), T([96, 128, 64], f16)), {})
cnt: 18, ((T([96, 128, 128], f16, stride=(16384, 1, 128)), T([96, 128, 64], f16)), {})
cnt: 18, ((T([96, 64, 128], f16, stride=(8192, 1, 64)), T([96, 128, 128], f16)), {})
```

----------------------------------------

TITLE: Ignoring Functions in TorchScript with @torch.jit.ignore Decorator
DESCRIPTION: Decorator that indicates to the compiler that a function or method should be ignored and left as a Python function. Allows non-TorchScript compatible code in models.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_41

LANGUAGE: python
CODE:
```
@torch.jit.ignore
```

----------------------------------------

TITLE: Diagnosing Collective Shape Mismatch Errors in PyTorch Distributed Python
DESCRIPTION: This example illustrates how setting `TORCH_DISTRIBUTED_DEBUG` to "DETAIL" helps catch inconsistencies in collective operations, specifically a shape mismatch in `torch.distributed.all_reduce`. It sets up a distributed environment and attempts `all_reduce` on tensors with different sizes on different ranks. With detailed debugging enabled, this will result in a `RuntimeError` providing a clear report of the shape mismatch, preventing potential hangs and aiding in root cause analysis.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.rst#_snippet_6

LANGUAGE: python
CODE:
```
import torch
import torch.distributed as dist
import torch.multiprocessing as mp


def worker(rank):
    dist.init_process_group("nccl", rank=rank, world_size=2)
    torch.cuda.set_device(rank)
    tensor = torch.randn(10 if rank == 0 else 20).cuda()
    dist.all_reduce(tensor)
    torch.cuda.synchronize(device=rank)


if __name__ == "__main__":
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "29501"
    os.environ["TORCH_CPP_LOG_LEVEL"]="INFO"
    os.environ["TORCH_DISTRIBUTED_DEBUG"] = "DETAIL"
    mp.spawn(worker, nprocs=2, args=())
```

----------------------------------------

TITLE: Implementing Post Training Static Quantization in PyTorch
DESCRIPTION: Shows implementation of static quantization where both weights and activations are quantized to int8. Includes model preparation, calibration, and conversion steps with observer attachment for activation tensor calibration.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch

# define a floating point model where some layers could be statically quantized
class M(torch.nn.Module):
    def __init__(self):
        super().__init__()
        # QuantStub converts tensors from floating point to quantized
        self.quant = torch.ao.quantization.QuantStub()
        self.conv = torch.nn.Conv2d(1, 1, 1)
        self.relu = torch.nn.ReLU()
        # DeQuantStub converts tensors from quantized to floating point
        self.dequant = torch.ao.quantization.DeQuantStub()

    def forward(self, x):
        x = self.quant(x)
        x = self.conv(x)
        x = self.relu(x)
        x = self.dequant(x)
        return x

model_fp32 = M()
model_fp32.eval()
model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('x86')
model_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32, [['conv', 'relu']])
model_fp32_prepared = torch.ao.quantization.prepare(model_fp32_fused)

input_fp32 = torch.randn(4, 1, 4, 4)
model_fp32_prepared(input_fp32)

model_int8 = torch.ao.quantization.convert(model_fp32_prepared)
res = model_int8(input_fp32)
```

----------------------------------------

TITLE: Defining a simple PyTorch module (Python)
DESCRIPTION: Defines a basic PyTorch `nn.Module` containing a `Linear` layer followed by a `ReLU` activation. This serves as an initial floating-point model example used to illustrate the subsequent quantization steps.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
class LinearReLUModule(torch.nn.Module):
   def __init__(self) -> None:
       super().__init__()
       self.linear = torch.nn.Linear(5, 10).float()
       self.relu = torch.nn.ReLU()

   def forward(self, x):
       return self.relu(self.linear(x))
```

----------------------------------------

TITLE: Configuring Per-Layer Learning Rates in PyTorch
DESCRIPTION: Example of specifying different learning rates for different parameter groups. The base model uses a higher learning rate (1e-2) while the classifier uses the default rate (1e-3).
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
optim.SGD([
                {'params': model.base.parameters(), 'lr': 1e-2},
                {'params': model.classifier.parameters()}
            ], lr=1e-3, momentum=0.9)

optim.SGD([
                {'params': model.base.named_parameters(), 'lr': 1e-2},
                {'params': model.classifier.named_parameters()}
            ], lr=1e-3, momentum=0.9)
```

----------------------------------------

TITLE: Integrating DistributedDataParallel with TorchDynamo in Python
DESCRIPTION: Demonstrates the recommended way to use `DistributedDataParallel` (DDP) with `torch.compile` (TorchDynamo). The DDP wrapper should be applied to the model *before* compiling it. This allows TorchDynamo to apply `DDPOptimizer` graph-break optimizations based on the DDP bucket configuration, potentially improving performance.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/ddp.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
    ddp_model = DDP(model, device_ids=[rank])
    ddp_model = torch.compile(ddp_model)
```

----------------------------------------

TITLE: Multiple Models and Optimizers with PyTorch AMP
DESCRIPTION: Demonstrates using GradScaler with multiple models, losses, and optimizers in a training loop, including proper scaling, unscaling, and update steps.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/amp_examples.rst#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
scaler = torch.amp.GradScaler()

for epoch in epochs:
    for input, target in data:
        optimizer0.zero_grad()
        optimizer1.zero_grad()
        with autocast(device_type='cuda', dtype=torch.float16):
            output0 = model0(input)
            output1 = model1(input)
            loss0 = loss_fn(2 * output0 + 3 * output1, target)
            loss1 = loss_fn(3 * output0 - 5 * output1, target)

        scaler.scale(loss0).backward(retain_graph=True)
        scaler.scale(loss1).backward()

        scaler.unscale_(optimizer0)

        scaler.step(optimizer0)
        scaler.step(optimizer1)

        scaler.update()
```

----------------------------------------

TITLE: Scripting, Serializing, and Loading a PyTorch Module
DESCRIPTION: Demonstrates how to script a PyTorch module, save it to a file, and then load it back.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
>>> scripted_module = torch.jit.script(MyModule())
>>> torch.jit.save(scripted_module, 'mymodule.pt')
>>> torch.jit.load('mymodule.pt')
RecursiveScriptModule( original_name=MyModule
                      (l0): RecursiveScriptModule(original_name=Linear)
                      (l1): RecursiveScriptModule(original_name=Linear) )
```

----------------------------------------

TITLE: Saving Dictionaries of Tensors with Pickle - PyTorch - Python
DESCRIPTION: Shows how torch.save and torch.load, which use Python's pickle module by default, can serialize dictionaries containing multiple tensors. Requires that both the keys and the values in the dictionary are pickle-able and that torch is imported. The example creates a dictionary of two tensors, saves it to a file, and reconstructs the dictionary upon loading. Inputs are the dictionary and a filename; the output is a reconstituted dictionary of tensors. Limitations include ensuring all dictionary elements are serializable with pickle.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> d = {'a': torch.tensor([1., 2.]), 'b': torch.tensor([3., 4.])}
>>> torch.save(d, 'tensor_dict.pt')
>>> torch.load('tensor_dict.pt')
{'a': tensor([1., 2.]), 'b': tensor([3., 4.])}
```

----------------------------------------

TITLE: TIMM Model Optimization Example
DESCRIPTION: Shows how to optimize a pre-trained ResNeXt model from the TIMM library using torch.compile with the inductor backend.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_get_started.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import timm
import torch
model = timm.create_model('resnext101_32x8d', pretrained=True, num_classes=2)
opt_model = torch.compile(model, backend="inductor")
opt_model(torch.randn(64,3,7,7))
```

----------------------------------------

TITLE: Out of Memory Exception Handling in PyTorch
DESCRIPTION: This example demonstrates a strategy to overcome issues in handling out of memory exceptions due to Python's exception handling mechanism retaining stack frames. It suggests moving recovery code outside of the exception block to properly free memory before retrying the operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/faq.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
    oom = False
    try:
        run_model(batch_size)
    except RuntimeError: # Out of memory
        oom = True

    if oom:
        for _ in range(batch_size):
            run_model(1)
```

----------------------------------------

TITLE: Tensor Division in PyTorch
DESCRIPTION: Performs element-wise division of a tensor by a scalar value.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
aten.div.Tensor((T([], f16), 64000), {})
```

----------------------------------------

TITLE: Demonstrating Numerical Overflow in PyTorch Norm Computation (Python)
DESCRIPTION: This Python code snippet illustrates how the .norm() method in PyTorch can produce overflow (inf) results when using FP32 precision for large input values, and how converting the tensor to double precision (float64) increases the range and avoids the overflow. The snippet requires the torch library and demonstrates the effect of numerical precision on operations. The main input parameter is a torch tensor with large numbers and outputs are the results of .norm() in fp32 and float64. Limitations involve possible overflows in fp32 for large values.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/numerical_accuracy.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
a=torch.tensor([1e20, 1e20]) # fp32 type by default
a.norm() # produces tensor(inf)
a.double().norm() # produces tensor(1.4142e+20, dtype=torch.float64), representable in fp32
```

----------------------------------------

TITLE: Logging Hierarchical Metrics to TensorBoard
DESCRIPTION: Shows how to log multiple metrics with hierarchical naming for better organization in TensorBoard. Demonstrates logging of train/test metrics for loss and accuracy over multiple iterations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensorboard.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from torch.utils.tensorboard import SummaryWriter
import numpy as np

writer = SummaryWriter()

for n_iter in range(100):
    writer.add_scalar('Loss/train', np.random.random(), n_iter)
    writer.add_scalar('Loss/test', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)
```

----------------------------------------

TITLE: Analyzing ATen Batched Matrix Multiplication in PyTorch
DESCRIPTION: This snippet illustrates the application of the ATen bmm (batched matrix multiplication) operator, reflecting its extensive use with batches of matrices having diverse configurations. It is pivotal in batch processing in machine learning, enabling parallel, efficient calculations typically used in sequence operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistilBertForMaskedLM_training.txt#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
Operator: aten.bmm.default
cnt: 6, ((T([192, 128, 64], f16), T([192, 64, 128], f16)), {})
cnt: 6, ((T([192, 128, 128], f16), T([192, 128, 64], f16)), {})
cnt: 6, ((T([192, 128, 128], f16, stride=(16384, 1, 128)), T([192, 128, 64], f16)), {})
cnt: 6, ((T([192, 128, 64], f16), T([192, 64, 128], f16, stride=(8192, 1, 64))), {})
cnt: 6, ((T([192, 64, 128], f16, stride=(8192, 1, 64)), T([192, 128, 128], f16)), {})
cnt: 6, ((T([192, 128, 128], f16), T([192, 128, 64], f16, stride=(8192, 1, 128))), {})
```

----------------------------------------

TITLE: Refactoring a Function to Return Auxiliary Outputs (Preferred with torch.func) - PyTorch - Python
DESCRIPTION: This example shows the suggested approach for writing functions to be used with torch.func transforms in Python. Instead of relying on global state, all necessary outputs are returned from the function. The function 'f' returns both the main value and any intermediate results, making it compatible with torch.func (including auxiliary outputs via has_aux=True). Dependencies are PyTorch and torch.func. Inputs and outputs are tensors, and this pattern avoids mutation or use of global state.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
def f(x):
  intermediate = x.sin()
  z = intermediate.sin()
  return z, intermediate

grad_x, intermediate = grad(f, has_aux=True)(x)
```

----------------------------------------

TITLE: Using torch.compiler.disable with Decorator Syntax
DESCRIPTION: Example showing how to disable TorchDynamo compilation on a specific function and its recursive calls using the decorator syntax. This is recommended when you can modify the source code.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_fine_grain_apis.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
@torch.compiler.disable
def problematic_function():
    # Function implementation
    pass
```

----------------------------------------

TITLE: Implementing Method Export and Compilation in TorchScript Modules
DESCRIPTION: Demonstrates how to use the @torch.jit.export decorator to explicitly mark methods for compilation. By default, TorchScript recursively compiles methods starting from forward().
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_11

LANGUAGE: Python
CODE:
```
import torch
import torch.nn as nn
import torchvision

class MyModule(nn.Module):
    def __init__(self):
        super().__init__()
        means = torch.tensor([103.939, 116.779, 123.68])
        self.means = torch.nn.Parameter(means.resize_(1, 3, 1, 1))
        resnet = torchvision.models.resnet18()
        self.resnet = torch.jit.trace(resnet, torch.rand(1, 3, 224, 224))

    def helper(self, input):
        return self.resnet(input - self.means)

    def forward(self, input):
        return self.helper(input)

    # Since nothing in the model calls `top_level_method`, the compiler
    # must be explicitly told to compile this method
    @torch.jit.export
    def top_level_method(self, input):
        return self.other_helper(input)

    def other_helper(self, input):
        return input + 10

# `my_script_module` will have the compiled methods `forward`, `helper`,
# `top_level_method`, and `other_helper`
my_script_module = torch.jit.script(MyModule())
```

----------------------------------------

TITLE: Tracing a Simple PyTorch Function with Dynamo
DESCRIPTION: Demonstrates how Dynamo traces a basic mean squared error function, converting it into a linear sequence of PyTorch operations without control flow.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch

@torch.compile
def mse(x, y):
    z = (x - y) ** 2
    return z.sum()

x = torch.randn(200)
y = torch.randn(200)
mse(x, y)
```

LANGUAGE: python
CODE:
```
def forward(l_x_: torch.Tensor, l_y_: torch.Tensor):
    # File: example.py:5, code: z = (x - y) ** 2
    sub = l_x_ - l_y_
    z = sub ** 2
    # File: example.py:6, code: return z.sum()
    sum_1 = z.sum()
    return (sum_1,)
```

----------------------------------------

TITLE: Manually Splitting a Transformer Model for Pipeline Parallelism in Python
DESCRIPTION: This code demonstrates how to manually split a Transformer model into two stages for pipeline parallelism. It initializes the full model on a meta device, then selectively deletes parts for each stage before wrapping in a PipelineStage.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.pipelining.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
with torch.device("meta"):
    assert num_stages == 2, "This is a simple 2-stage example"

    # we construct the entire model, then delete the parts we do not need for this stage
    # in practice, this can be done using a helper function that automatically divides up layers across stages.
    model = Transformer()

    if stage_index == 0:
        # prepare the first stage model
        del model.layers["1"]
        model.norm = None
        model.output = None

    elif stage_index == 1:
        # prepare the second stage model
        model.tok_embeddings = None
        del model.layers["0"]

    from torch.distributed.pipelining import PipelineStage
    stage = PipelineStage(
        model,
        stage_index,
        num_stages,
        device,
    )
```

----------------------------------------

TITLE: Using Learning Rate Scheduler in PyTorch Training Loop
DESCRIPTION: This snippet demonstrates the typical usage of a learning rate scheduler within a PyTorch training loop. It shows how to initialize the scheduler and call its step() method after each epoch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
scheduler = ...
for epoch in range(100):
    train(...)
    validate(...)
    scheduler.step()
```

----------------------------------------

TITLE: Custom Gradient Rule with PyTorch autograd.Function (Python)
DESCRIPTION: Defines a torch.autograd.Function (MyCube) for y = x ** 3 that moves part of the gradient computation (dx) to forward, allowing customized performance for backward passes. The backward method uses both output gradients and saved Tensors for higher-order differentiation, compatible with torch.func transforms. The example includes a my_cube convenience function and a test for correctness of second derivatives. Requires only torch. Inputs: single scalar tensor. Output: cubed tensor. Edge cases and higher-order gradients are supported by design.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.func.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
class MyCube(torch.autograd.Function):
    @staticmethod
    def forward(x):
        result = x ** 3
        # In regular PyTorch, if we had just run y = x ** 3, then the backward
        # pass computes dx = 3 * x ** 2. In this autograd.Function, we've done
        # that computation here in the forward pass instead.
        dx = 3 * x ** 2
        return result, dx

    @staticmethod
    def setup_context(ctx, inputs, output):
        x, = inputs
        result, dx = output
        ctx.save_for_backward(x, dx)

    @staticmethod
    def backward(ctx, grad_output, grad_dx):
        x, dx = ctx.saved_tensors
        # In order for the autograd.Function to work with higher-order
        # gradients, we must add the gradient contribution of `dx`.
        result = grad_output * dx + grad_dx * 6 * x
        return result

```

LANGUAGE: python
CODE:
```
def my_cube(x):
    result, _ = MyCube.apply(x)
    return result
```

LANGUAGE: python
CODE:
```
x = torch.randn([])
ggx = torch.func.grad(torch.func.grad(my_cube))(x)
assert torch.allclose(ggx, 6 * x)
```

----------------------------------------

TITLE: Saving and Loading Scripted Quantized Models in PyTorch
DESCRIPTION: Shows how to save and load a scripted quantized model using torch.jit.save and torch.jit.load. This method involves preparing the model, converting it, scripting it, and then using BytesIO for serialization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
# Note: using the same model M from previous example
m = M().eval()
prepare_orig = prepare_fx(m, {'' : default_qconfig})
prepare_orig(torch.rand(5, 5))
quantized_orig = convert_fx(prepare_orig)

# save/load using scripted model
scripted = torch.jit.script(quantized_orig)
b = io.BytesIO()
torch.jit.save(scripted, b)
b.seek(0)
scripted_quantized = torch.jit.load(b)
```

----------------------------------------

TITLE: Performing Asynchronous All-Reduce with Stream Synchronization in PyTorch Distributed (Python)
DESCRIPTION: This snippet initializes a PyTorch distributed process group using NCCL and demonstrates an asynchronous `all_reduce` operation on a CUDA tensor. It highlights the importance of explicitly synchronizing a non-default stream (`s.wait_stream`) with the default stream before using the tensor after the asynchronous collective has been enqueued to ensure correct and deterministic results, preventing potential data races.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.rst#_snippet_2

LANGUAGE: Python
CODE:
```
dist.init_process_group("nccl", rank=rank, world_size=2)
output = torch.tensor([rank]).cuda(rank)
s = torch.cuda.Stream()
handle = dist.all_reduce(output, async_op=True)
# Wait ensures the operation is enqueued, but not necessarily complete.
handle.wait()
# Using result on non-default stream.
with torch.cuda.stream(s):
    s.wait_stream(torch.cuda.default_stream())
    output.add_(100)
if rank == 0:
    # if the explicit call to wait_stream was omitted, the output below will be
    # non-deterministically 1 or 101, depending on whether the allreduce overwrote
    # the value after the add completed.
    print(output)
```

----------------------------------------

TITLE: Exporting Conv2D and BatchNorm Module for Training - PyTorch - Python
DESCRIPTION: This snippet defines a neural network module combining Conv2D and BatchNorm layers, creates input data, and exports the model for training using torch.export.export_for_training. It demonstrates capturing general ATen ops (including non-functional batch_norm) in the produced IR suitable for eager training. Requires PyTorch 2.5+, and expects module definitions and tensor inputs; output is an ExportedProgram with IR as shown.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
class ConvBatchnorm(torch.nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.conv = torch.nn.Conv2d(1, 3, 1, 1)
        self.bn = torch.nn.BatchNorm2d(3)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        return (x,)

mod = ConvBatchnorm()
inp = torch.randn(1, 1, 3, 3)

ep_for_training = torch.export.export_for_training(mod, (inp,))
print(ep_for_training)
```

----------------------------------------

TITLE: Analyzing PyTorch Tensor Copy Operations
DESCRIPTION: This snippet demonstrates the usage of the aten.copy_.default operator for copying tensor data between tensors with various shapes and strides.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
Operator: aten.copy_.default
cnt: 2, ((T([3, 3, 512, 512], f16), T([3, 3, 512, 512], f16)), {})
cnt: 1, ((T([3, 1, 512, 512], f16), T([3, 1, 512, 512], f16)), {})
cnt: 1, ((T([3, 4, 512, 512], f16), T([3, 4, 512, 512], f16)), {})
cnt: 1, ((T([256, 128, 3, 3], f16), T([256, 128, 3, 3], f16, stride=(1152, 1, 384, 128))), {})
cnt: 1, ((T([128, 64, 3, 3], f16), T([128, 64, 3, 3], f16, stride=(576, 1, 192, 64))), {})
```

----------------------------------------

TITLE: Handling Missing Parameters in Optimizer State Dict
DESCRIPTION: This example demonstrates how to handle missing parameters when loading an optimizer state dict for a model with a modified structure. It uses a custom hook to adapt the state dict, mapping existing parameters and ignoring new ones.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
def adapt_state_dict_missing_param(optimizer, state_dict):
    adapted_state_dict = deepcopy(optimizer.state_dict())
    # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict.
    for k, v in state_dict['param_groups'][0].items():
        if k not in ['params', 'param_names']:
            adapted_state_dict['param_groups'][0][k] = v

    lookup_dict = {
        'fc.weight': 'fc.weight',
        'fc.bias': 'fc.bias',
        'bypass.weight': None,
    }

    clone_deepcopy = lambda d: {k: (v.clone() if isinstance(v, torch.Tensor) else deepcopy(v)) for k, v in d.items()}
    for param_id, param_name in zip(
            optimizer.state_dict()['param_groups'][0]['params'],
            optimizer.state_dict()['param_groups'][0]['param_names']):
        name_in_loaded = lookup_dict[param_name]
        if name_in_loaded in state_dict['param_groups'][0]['param_names']:
            index_in_loaded_list = state_dict['param_groups'][0]['param_names'].index(name_in_loaded)
            id_in_loaded = state_dict['param_groups'][0]['params'][index_in_loaded_list]
            # Copy the state of the corresponding parameter
            if id_in_loaded in state_dict['state']:
                adapted_state_dict['state'][param_id] = clone_deepcopy(state_dict['state'][id_in_loaded])

    return adapted_state_dict

optimizer2.register_load_state_dict_pre_hook(adapt_state_dict_ids)
optimizer2.load_state_dict(torch.load(PATH)) # The previous optimizer saved state_dict
```

----------------------------------------

TITLE: Calling aten.addmm.default (Python)
DESCRIPTION: Performs a matrix multiplication followed by an addition (out = beta*input + alpha*(mat1 @ mat2)). Commonly used in linear layers. Examples show various matrix dimensions and float16 data type.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_7

LANGUAGE: Python
CODE:
```
((T([1024], f16), T([1024, 1024], f16), T([1024, 1024], f16, stride=(1, 1024))), {})
```

LANGUAGE: Python
CODE:
```
((T([4096], f16), T([1024, 1024], f16), T([1024, 4096], f16, stride=(1, 1024))), {})
```

LANGUAGE: Python
CODE:
```
((T([1024], f16), T([1024, 4096], f16), T([4096, 1024], f16, stride=(1, 4096))), {})
```

----------------------------------------

TITLE: Integrating Traced Modules in Scripted nn.Module with torch.jit (Python)
DESCRIPTION: Illustrates use of a traced torchvision model (ResNet18) within a custom nn.Module scripted by TorchScript. Dependencies are torch and torchvision. The 'MyScriptModule' class preprocesses input images using mean normalization and invokes the traced ResNet. Demonstrates submodule composition, parameter management, and scripting of modules; inputs are images (torch.Tensor), outputs are model predictions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch\nimport torchvision\n\nclass MyScriptModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.means = torch.nn.Parameter(torch.tensor([103.939, 116.779, 123.68])\n                                        .resize_(1, 3, 1, 1))\n        self.resnet = torch.jit.trace(torchvision.models.resnet18(),\n                                      torch.rand(1, 3, 224, 224))\n\n    def forward(self, input):\n        return self.resnet(input - self.means)\n\nmy_script_module = torch.jit.script(MyScriptModule())\n
```

----------------------------------------

TITLE: Exporting a PyTorch Model with Dynamic Shapes
DESCRIPTION: Example showing how to export a PyTorch model using torch.export API with dynamic shapes specified through the dynamic_shapes argument. The resulting ExportedProgram contains symbolic shapes for inputs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
exported_program: torch.export.ExportedProgram = export(
    M(), args=example_args, dynamic_shapes=dynamic_shapes
)
print(exported_program)
```

----------------------------------------

TITLE: Seeding Python's Standard Random Module in Python
DESCRIPTION: Sets the seed for Python's built-in `random` module using `random.seed(0)`. This step is necessary for achieving reproducibility if custom operators or other parts of the application code rely on Python's standard library for random number generation, complementing PyTorch's own seeding mechanism.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/randomness.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import random
random.seed(0)
```

----------------------------------------

TITLE: Matrix Multiplication Using ATen (aten.mm.default) in PyTorch (Python)
DESCRIPTION: Each snippet shows usage of aten.mm (dense matrix multiplication), applied between pairs of 2D float16 tensors of various shapes. All operands require matched inner channel dimensions (e.g., [32,1000] x [1000,1280]), returned result is a matrix of leading batch size. This is used in fully connected neural network or classification layers.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mnasnet1_0_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
Operator: aten.mm.default
cnt: 1, ((T([32, 1000], f16, stride=(0, 0)), T([1000, 1280], f16)), {})
cnt: 1, ((T([1000, 32], f16, stride=(0, 0)), T([32, 1280], f16)), {})
```

----------------------------------------

TITLE: Calling aten.native_layer_norm.default (Python)
DESCRIPTION: Performs layer normalization on an input tensor. Applies normalization over a specified dimension, typically the last dimension of the input, using mean and variance calculated over that dimension. Includes learnable weights and biases.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_24

LANGUAGE: Python
CODE:
```
((T([8, 128, 1024], f16), [1024], T([1024], f16), T([1024], f16), 1e-05), {})
```

----------------------------------------

TITLE: Analyzing PyTorch Operator Usage
DESCRIPTION: This code snippet demonstrates the usage of various PyTorch operators in a neural network model. It includes operations like log_softmax, tensor addition, matrix multiplication, concatenation, cloning, padding, and convolution. The snippet shows the frequency of each operation and the tensor shapes involved.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([64, 1000], f16), 1, False), {})
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})
Operator: aten.add.Tensor
cnt: 58, ((T([], i64), 1), {})
cnt: 2, ((T([64, 32, 112, 112], f16), T([64, 32, 112, 112], f16)), {})
# ... (truncated for brevity)
Operator: aten.convolution.default
cnt: 1, ((T([64, 3, 225, 225], f16), T([32, 3, 3, 3], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([64, 32, 112, 112], f16), T([32, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 32), {})
# ... (truncated for brevity)
```

----------------------------------------

TITLE: Computing Per-Sample Gradients using torch.func.vmap and torch.func.grad in Python
DESCRIPTION: This snippet demonstrates the composition of `vmap` and `grad` to compute per-sample gradients, a task challenging in standard PyTorch. A `compute_loss` function calculates the MSE loss for a single example. `grad(compute_loss)` creates a function to compute the gradient of the loss with respect to the first argument (weights). `vmap` is then applied, specifying that the `weights` argument should not be mapped (`None`) while the `example` and `target` arguments should be mapped along their 0th dimension. This results in computing the gradient of the loss with respect to the weights for each example in the batch independently.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.whirlwind_tour.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from torch.func import vmap
batch_size, feature_size = 3, 5

def model(weights,feature_vec):
    # Very simple linear model with activation
    assert feature_vec.dim() == 1
    return feature_vec.dot(weights).relu()

def compute_loss(weights, example, target):
    y = model(weights, example)
    return ((y - target) ** 2).mean()  # MSELoss

weights = torch.randn(feature_size, requires_grad=True)
examples = torch.randn(batch_size, feature_size)
targets = torch.randn(batch_size)
inputs = (weights,examples, targets)
grad_weight_per_example = vmap(grad(compute_loss), in_dims=(None, 0, 0))(*inputs)
```

----------------------------------------

TITLE: Convolution Operations in PyTorch
DESCRIPTION: Multiple convolution operations with various input shapes, kernel sizes, and strides. These operations are typical in convolutional neural networks, with some likely being part of residual blocks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
cnt: 2, ((T([128, 1, 128], f16), T([1, 1, 5], f16), None, [1], [2], [1], False, [0], 1), {})
```

LANGUAGE: Python
CODE:
```
cnt: 2, ((T([128, 128, 32, 32], f16), T([512, 128, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

----------------------------------------

TITLE: Scalar and Tensor Addition in PyTorch Python
DESCRIPTION: The 'aten.add' operator is utilized for element-wise addition either between a tensor and a scalar or two tensors of matching shape. The operands must be compatible in dimensions, and it requires PyTorch. Inputs include the tensors and/or scalar, producing a tensor of the accumulated results. Efficient data handling hinges on appropriate tensor alignment.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
Operator: aten.add.Scalar
cnt: 1, ((T([100, 1], i64), 1), {})
Operator: aten.add.Tensor
cnt: 3, ((T([1, 16, 320, 320], f16), T([1, 16, 320, 320], f16)), {})
```

----------------------------------------

TITLE: Implementing Custom Batch with Memory Pinning in PyTorch
DESCRIPTION: Demonstrates implementing a custom batch class with methods for memory pinning to speed up data transfer to CUDA devices. Utilizes PyTorch's DataLoader and includes a collate function that wraps batches into the custom class, enabling data transfer optimizations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/data.rst#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
class SimpleCustomBatch:
    def __init__(self, data):
        transposed_data = list(zip(*data))
        self.inp = torch.stack(transposed_data[0], 0)
        self.tgt = torch.stack(transposed_data[1], 0)

    # custom memory pinning method on custom type
    def pin_memory(self):
        self.inp = self.inp.pin_memory()
        self.tgt = self.tgt.pin_memory()
        return self

def collate_wrapper(batch):
    return SimpleCustomBatch(batch)

inps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)
tgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)
dataset = TensorDataset(inps, tgts)

loader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,
                    pin_memory=True)

for batch_ndx, sample in enumerate(loader):
    print(sample.inp.is_pinned())
    print(sample.tgt.is_pinned())
```

----------------------------------------

TITLE: Training ResNet50 on CIFAR10 with torch.compile on XPU
DESCRIPTION: Demonstrates training a ResNet50 model on the CIFAR10 dataset using torch.compile optimization on an Intel XPU device. The example includes data preparation, model setup with compilation for performance acceleration, and a training loop with regular loss reporting.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/get_start_xpu.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
import torch
import torchvision

LR = 0.001
DOWNLOAD = True
DATA = "datasets/cifar10/"

transform = torchvision.transforms.Compose(
    [
        torchvision.transforms.Resize((224, 224)),
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
    ]
)
train_dataset = torchvision.datasets.CIFAR10(
    root=DATA,
    train=True,
    transform=transform,
    download=DOWNLOAD,
)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128)
train_len = len(train_loader)

model = torchvision.models.resnet50()
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)
model.train()
model = model.to("xpu")
criterion = criterion.to("xpu")
model = torch.compile(model)

print(f"Initiating training with torch compile")
for batch_idx, (data, target) in enumerate(train_loader):
    data = data.to("xpu")
    target = target.to("xpu")
    optimizer.zero_grad()
    output = model(data)
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()
    if (batch_idx + 1) % 10 == 0:
         iteration_loss = loss.item()
         print(f"Iteration [{batch_idx+1}/{train_len}], Loss: {iteration_loss:.4f}")
torch.save(
    {
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
    },
    "checkpoint.pth",
)

print("Execution finished")
```

----------------------------------------

TITLE: Initializing Distributed Process Group with File Method (Python)
DESCRIPTION: Initializes the PyTorch distributed process group using a shared file system as the synchronization mechanism. It requires specifying the backend, a unique file URI (`file://`) on a shared filesystem, the total number of processes (`world_size`), and the specific rank of the current process. The file must be non-existent or empty before initialization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.rst#_snippet_1

LANGUAGE: python
CODE:
```
import torch.distributed as dist

# rank should always be specified
dist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',
 world_size=4, rank=args.rank)
```

----------------------------------------

TITLE: Batching Data with PyTorch DataPipes
DESCRIPTION: Demonstrates the use of the batch() method on a DataPipe to create batches of data. Shows examples of classic batching, dropping incomplete batches, and nested batching.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(10).batch(3)
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(10).batch(3, drop_last = True)
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(30).batch(3).batch(2)
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(30).batch(3).batch(2).batch(10, unbatch_level=-1)
for i in dp:
    print(i)
```

----------------------------------------

TITLE: Bad Practice: Using NumPy Arrays in Traced Code (Avoid)
DESCRIPTION: This snippet illustrates a common pitfall: using NumPy arrays and functions within code that will be traced by `torch.onnx.export`. NumPy variables are converted to constants during tracing, which is incorrect if their values should vary based on input. Use PyTorch tensors and operations instead.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_3

LANGUAGE: python
CODE:
```
# Bad! Will be replaced with constants during tracing.
x, y = np.random.rand(1, 2), np.random.rand(1, 2)
np.concatenate((x, y), axis=1)
```

----------------------------------------

TITLE: Avoiding Data-Dependent Control Flow in PyTorch
DESCRIPTION: Example showing how to refactor code to avoid data-dependent control flow that causes graph breaks. The new version evaluates the condition outside the compiled function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
# old
x = torch.randn(3, 3)
@torch.compile
def fn(y):
    if x.sum() > 0:
        return y + x
    else:
        return y - x

# new
x = torch.randn(3, 3)
cond = (x.sum() > 0).item()
@torch.compile
def fn(y):
    if cond:
        return y + x
    else:
        return y - x
```

----------------------------------------

TITLE: Retrieving and Inspecting Dynamo Cache Entries in Python
DESCRIPTION: This snippet demonstrates how to retrieve compiled code and guards from a function using Dynamo's debug API. It shows how to access the first cache entry and examine both the guard function and compiled code using Python's disassembler.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_overview.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from torch._dynamo.eval_frame import _debug_get_cache_entry_list, innermost_fn
cache_entries = _debug_get_cache_entry_list(innermost_fn(toy_example))
cache_entry = cache_entries[0]
guard, code = cache_entry.check_fn, cache_entry.code
# the guard takes the local variables of an input frame, and tells whether a re-compilation should be triggered.
import dis
dis.dis(guard)
dis.dis(code)
```

----------------------------------------

TITLE: Tracing Conditional Logic with Dynamo
DESCRIPTION: Shows how Dynamo handles conditional statements by tracing only the executed path based on the given inputs, demonstrating its input-dependent behavior.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch

@torch.compile
def fn(x, n):
    y = x ** 2
    if n >= 0:
        return (n + 1) * y
    else:
        return y / n

x = torch.randn(200)
fn(x, 2)
```

LANGUAGE: python
CODE:
```
def forward(l_x_: torch.Tensor):
    # File: example.py:5, code: y = x ** 2
    y = l_x_ ** 2
    # File: example.py:7, code: return (n + 1) * y
    mul = 3 * y
    return (mul,)
```

----------------------------------------

TITLE: Loading Batched Data from Map-style Dataset
DESCRIPTION: Illustrates the equivalent operation of loading batched samples from a map-style dataset when automatic batching is enabled.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/data.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
for indices in batch_sampler:
    yield collate_fn([dataset[i] for i in indices])
```

----------------------------------------

TITLE: Complete Structured Pruning Implementation Example
DESCRIPTION: A comprehensive example showing how to prune 50% of rows in linear layers of a neural network using SaliencyPruner. Includes model definition, pruning configuration, and pruning execution.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/pruner/README.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from torch.ao.pruning._experimental.pruner import SaliencyPruner

# Define model
class Model(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.seq = nn.Sequential(
            nn.Linear(700, 500, bias=True),
            nn.ReLU(),
            nn.Linear(500, 800, bias=False),
            nn.ReLU(),
            nn.Linear(800, 600, bias=True),
            nn.ReLU(),
        )
        self.linear = nn.Linear(600, 4, bias=False)

    def forward(self, x):
        x = self.seq(x)
        x = self.linear(x)
        return x

# Define pruning_config, which specifies which tensors you wish to prune.
# The SaliencyPruner also needs a sparsity_level parameter to specify what % of rows to prune.
pruning_config = [
    {"tensor_fqn": "seq.0.weight", "sparsity_level": 0.5},
    {"tensor_fqn": "seq.2.weight", "sparsity_level": 0.5},
    {"tensor_fqn": "seq.4.weight", "sparsity_level": 0.5},
    {"tensor_fqn": "linear.weight", "sparsity_level": 0.5},
]

original = Model()
# define defaults
# for structured pruning, we also prune biases by default.
defaults = {"prune_bias": True}
# any configs passed in here are defaults that are propagated
# Your selection criteria is decided by which pruner you use
pruner = SaliencyPruner(defaults, patterns=patterns)

# Next we call `prepare`, which will attach `FakeStructuredSparsity` parameterizations
# to the tensors specified in the config. These parameterizations will zero out
# the appropriate weights in order to make the model behave as if it has been pruned.
pruner.prepare(original, sparse_config)

# take one pruning step. This will update the masks
pruner.enable_mask_update = True
pruner.step()

# pruner.prune() will find patterns and apply that patterns pruning function to it's matching nodes.
# The output of pruner.prune() is a model with resized weights and the masks / parametrizations removed.
pruned_model = pruner.prune()
```

----------------------------------------

TITLE: Backward Pass Stream Semantics - Safe Use Across Streams with Sync - PyTorch Python
DESCRIPTION: This snippet demonstrates how to safely use gradients obtained from a backward pass performed on a custom stream (`s`) when the usage occurs outside that stream context. Explicit synchronization (`torch.cuda.current_stream().wait_stream(s)`) is added before using the gradients (`use grads`) to ensure the backward pass on stream `s` has completed.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_17

LANGUAGE: Python
CODE:
```
with torch.cuda.stream(s):
    loss.backward()
torch.cuda.current_stream().wait_stream(s)
use grads
```

----------------------------------------

TITLE: Backward Pass Stream Semantics - Safe Initial Grad Across Streams with Sync - PyTorch Python
DESCRIPTION: This snippet provides a safe pattern for using an initial gradient created outside the target stream context. Synchronization (`s.wait_stream(torch.cuda.current_stream())`) ensures the initial gradient is ready. `initial_grad.record_stream(s)` associates the gradient tensor with the target stream `s`, and then `loss.backward(gradient=initial_grad)` is safely called within stream `s`'s context.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_20

LANGUAGE: Python
CODE:
```
initial_grad = torch.ones_like(loss)
s.wait_stream(torch.cuda.current_stream())
with torch.cuda.stream(s):
    initial_grad.record_stream(s)
    loss.backward(gradient=initial_grad)
```

----------------------------------------

TITLE: Training Loop with PyTorch Graphed Callables
DESCRIPTION: Demonstrates a training loop utilizing modules previously converted into graphed callables via `torch.cuda.make_graphed_callables`. The loop performs optimizer zeroing, calls the graphed forward passes (potentially conditionally), calculates loss, performs graphed backward passes, and executes the non-graphed optimizer step.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_43

LANGUAGE: python
CODE:
```
optimizer = torch.optim.SGD(chain(module1.parameters(),
                                      module2.parameters(),
                                      module3.parameters()),
                                lr=0.1)

    # Sample inputs used for capture
    # requires_grad state of sample inputs must match
    # requires_grad state of real inputs each callable will see.
    x = torch.randn(N, D_in, device='cuda')
    h = torch.randn(N, H, device='cuda', requires_grad=True)

    module1 = torch.cuda.make_graphed_callables(module1, (x,))
    module2 = torch.cuda.make_graphed_callables(module2, (h,))
    module3 = torch.cuda.make_graphed_callables(module3, (h,))

    real_inputs = [torch.rand_like(x) for _ in range(10)]
    real_targets = [torch.randn(N, D_out, device="cuda") for _ in range(10)]

    for data, target in zip(real_inputs, real_targets):
        optimizer.zero_grad(set_to_none=True)

        tmp = module1(data)  # forward ops run as a graph

        if tmp.sum().item() > 0:
            tmp = module2(tmp)  # forward ops run as a graph
        else:
            tmp = module3(tmp)  # forward ops run as a graph

        loss = loss_fn(tmp, target)
        # module2's or module3's (whichever was chosen) backward ops,
        # as well as module1's backward ops, run as graphs
        loss.backward()
        optimizer.step()
```

----------------------------------------

TITLE: Launching Fault-Tolerant Distributed Training Job with torchrun in Bash
DESCRIPTION: Command to launch a fault-tolerant distributed training job using torchrun. It specifies the number of nodes, processes per node, maximum restarts, rendezvous ID, backend, and endpoint. The command should be run on all participating nodes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/quickstart.rst#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
torchrun
   --nnodes=NUM_NODES
   --nproc-per-node=TRAINERS_PER_NODE
   --max-restarts=NUM_ALLOWED_FAILURES
   --rdzv-id=JOB_ID
   --rdzv-backend=c10d
   --rdzv-endpoint=HOST_NODE_ADDR
   YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)
```

----------------------------------------

TITLE: Compiling PyTorch Model with AOTInductor
DESCRIPTION: Demonstrates how to export a PyTorch model using torch.export and compile it using AOTInductor. The example includes a simple neural network model with dynamic batch dimension support.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_aot_inductor.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import os
import torch

class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = torch.nn.Linear(10, 16)
        self.relu = torch.nn.ReLU()
        self.fc2 = torch.nn.Linear(16, 1)
        self.sigmoid = torch.nn.Sigmoid()

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.sigmoid(x)
        return x

with torch.no_grad():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = Model().to(device=device)
    example_inputs=(torch.randn(8, 10, device=device),)
    batch_dim = torch.export.Dim("batch", min=1, max=1024)
    exported = torch.export.export(model, example_inputs, dynamic_shapes={"x": {0: batch_dim}})
    output_path = torch._inductor.aoti_compile_and_package(
        exported,
        package_path=os.path.join(os.getcwd(), "model.pt2"),
    )
```

----------------------------------------

TITLE: Implementing Quantization-Aware Training in PyTorch
DESCRIPTION: Demonstrates the process of implementing quantization-aware training (QAT) including model preparation, fusing modules, and converting to quantized format. Uses torch.ao.quantization APIs to prepare and convert the model.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
model_fp32.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')

model_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32,
    [['conv', 'bn', 'relu']])

model_fp32_prepared = torch.ao.quantization.prepare_qat(model_fp32_fused.train())

training_loop(model_fp32_prepared)

model_fp32_prepared.eval()
model_int8 = torch.ao.quantization.convert(model_fp32_prepared)

res = model_int8(input_fp32)
```

----------------------------------------

TITLE: Implementing torch.cond in Python
DESCRIPTION: A logical implementation of torch.cond showing its basic structure and parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cond.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
def cond(
    pred: Union[bool, torch.Tensor],
    true_fn: Callable,
    false_fn: Callable,
    operands: Tuple[torch.Tensor]
):
    if pred:
        return true_fn(*operands)
    else:
        return false_fn(*operands)
```

----------------------------------------

TITLE: Running Hugging Face Benchmark with PyTorch Inductor
DESCRIPTION: This command runs the Hugging Face benchmark suite using PyTorch Inductor backend with specific settings for performance testing. It includes options for cold-start latency, inference, AMP, and CUDA device.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_performance_dashboard.rst#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python benchmarks/dynamo/huggingface.py --performance --cold-start-latency --inference --amp --backend inductor --disable-cudagraphs --device cuda
```

----------------------------------------

TITLE: Recursively Iterating Through All Modules in a Nested Network
DESCRIPTION: Defines a `BigNet` module which contains another module (`Net`) as a submodule. It then demonstrates using `named_modules()` to recursively iterate through all modules within `big_net`, including the top-level module itself, its direct children (`l1`, `net`), and the children of nested modules (`net.l0`, `net.l1`).
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
class BigNet(nn.Module):
  def __init__(self):
    super().__init__()
    self.l1 = MyLinear(5, 4)
    self.net = Net()
  def forward(self, x):
    return self.net(self.l1(x))

big_net = BigNet()
for module in big_net.named_modules():
  print(module)
: ('', BigNet(
  (l1): MyLinear()
  (net): Net(
    (l0): MyLinear()
    (l1): MyLinear()
  )
))
('l1', MyLinear())
('net', Net(
  (l0): MyLinear()
  (l1): MyLinear()
))
('net.l0', MyLinear())
('net.l1', MyLinear())
```

----------------------------------------

TITLE: Calculating NLL Loss in PyTorch
DESCRIPTION: This snippet shows the tensor shapes and parameters for calculating Negative Log Likelihood (NLL) loss and its backward pass in a neural network.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/adv_inception_v3_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
Operator: aten.nll_loss_backward.default
cnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})

Operator: aten.nll_loss_forward.default
cnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})
```

----------------------------------------

TITLE: Matrix Multiplication with Bias Addition in PyTorch
DESCRIPTION: This code shows the addmm operation which performs matrix multiplication followed by addition with a bias vector. This operation is typically used in fully connected layers of neural networks, combining weights, input, and bias in one operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
Operator: aten.addmm.default
cnt: 1, ((T([65], f16), T([96, 512], f16), T([512, 65], f16, stride=(1, 512))), {})
```

----------------------------------------

TITLE: Using DataPipes for CSV Processing
DESCRIPTION: Example usage of DataPipes to create a pipeline for processing CSV files, demonstrating chaining of multiple DataPipes using functional API.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/datapipes/README.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import torch.utils.data.datapipes as dp

FOLDER = 'path/2/csv/folder'
datapipe = dp.iter.FileLister([FOLDER]).filter(fn=lambda filename: filename.endswith('.csv'))
datapipe = dp.iter.FileOpener(datapipe, mode='rt')
datapipe = datapipe.parse_csv_files(delimiter=' ')

for d in datapipe: # Start loading data
    pass
```

----------------------------------------

TITLE: Launching Elastic Distributed Training Job with torchrun in Bash
DESCRIPTION: Command to launch an elastic distributed training job using torchrun. It specifies a range of nodes, processes per node, maximum restarts, rendezvous ID, backend, and endpoint. The command should be run on at least MIN_SIZE and at most MAX_SIZE nodes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/quickstart.rst#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
torchrun
    --nnodes=MIN_SIZE:MAX_SIZE
    --nproc-per-node=TRAINERS_PER_NODE
    --max-restarts=NUM_ALLOWED_FAILURES_OR_MEMBERSHIP_CHANGES
    --rdzv-id=JOB_ID
    --rdzv-backend=c10d
    --rdzv-endpoint=HOST_NODE_ADDR
    YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)
```

----------------------------------------

TITLE: Good Practice: Using Torch Tensors in Traced Code
DESCRIPTION: This snippet demonstrates the recommended approach: using PyTorch tensors and operations instead of NumPy. Torch tensor operations will be captured correctly during tracing, preserving dynamic behavior where applicable (within the limits of tracing).
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_4

LANGUAGE: python
CODE:
```
# Good! Tensor operations will be captured during tracing.
x, y = torch.randn(1, 2), torch.randn(1, 2)
torch.cat((x, y), dim=1)
```

----------------------------------------

TITLE: Element-Wise Multiplication of Tensors in PyTorch
DESCRIPTION: The mul.Tensor operator performs element-wise multiplication between two tensors. This operation is prominently used in scaling, gating mechanisms, or pointwise transformations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/legacy_senet154_training.txt#2025-04-22_snippet_5

LANGUAGE: text
CODE:
```
cnt: 6, ((T([32, 256, 56, 56], f16), T([32, 256, 1, 1], f16)), {})
```

LANGUAGE: text
CODE:
```
cnt: 16, ((T([32, 512, 28, 28], f16), T([32, 512, 1, 1], f16)), {})
```

LANGUAGE: text
CODE:
```
cnt: 72, ((T([32, 1024, 14, 14], f16), T([32, 1024, 1, 1], f16)), {})
```

LANGUAGE: text
CODE:
```
cnt: 6, ((T([32, 2048, 7, 7], f16), T([32, 2048, 1, 1], f16)), {})
```

----------------------------------------

TITLE: Convolution Operations in PyTorch - Python
DESCRIPTION: This snippet uses aten.convolution.default to perform convolutions typical in CNN architectures. Parameters like stride, padding, and dilation control the operation. Essential for extracting features from input data in machine learning models.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v3_large_training.txt#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
Operator: aten.convolution.default
cnt: 1, ((T([32, 3, 224, 224], f16), T([16, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 16, 112, 112], f16), T([16, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 16), {})
cnt: 1, ((T([32, 16, 112, 112], f16), T([16, 16, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 16, 112, 112], f16), T([64, 16, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 64, 112, 112], f16), T([64, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 64), {})
cnt: 1, ((T([32, 64, 56, 56], f16), T([24, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([32, 24, 56, 56], f16), T([72, 24, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 72, 56, 56], f16), T([72, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 72), {})
cnt: 1, ((T([32, 72, 56, 56], f16), T([24, 72, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 72, 56, 56], f16), T([72, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 72), {})
cnt: 1, ((T([32, 72, 1, 1], f16), T([24, 72, 1, 1], f16), T([24], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 24, 1, 1], f16), T([72, 24, 1, 1], f16), T([72], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 72, 28, 28], f16), T([40, 72, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([32, 40, 28, 28], f16), T([120, 40, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([32, 120, 28, 28], f16), T([120, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 120), {})
cnt: 2, ((T([32, 120, 1, 1], f16), T([32, 120, 1, 1], f16), T([32], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([32, 32, 1, 1], f16), T([120, 32, 1, 1], f16), T([120], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([32, 120, 28, 28], f16), T([40, 120, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 40, 28, 28], f16), T([240, 40, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 240, 28, 28], f16), T([240, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 240), {})
cnt: 1, ((T([32, 240, 14, 14], f16), T([80, 240, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 80, 14, 14], f16), T([200, 80, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 200, 14, 14], f16), T([200, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 200), {})
cnt: 1, ((T([32, 200, 14, 14], f16), T([80, 200, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([32, 80, 14, 14], f16), T([184, 80, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([32, 184, 14, 14], f16), T([184, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 184), {})
cnt: 2, ((T([32, 184, 14, 14], f16), T([80, 184, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 80, 14, 14], f16), T([480, 80, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 480, 14, 14], f16), T([480, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 480), {})
cnt: 1, ((T([32, 480, 1, 1], f16), T([120, 480, 1, 1], f16), T([120], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 120, 1, 1], f16), T([480, 120, 1, 1], f16), T([480], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 480, 14, 14], f16), T([112, 480, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([32, 112, 14, 14], f16), T([672, 112, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 672, 14, 14], f16), T([672, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 672), {})
cnt: 2, ((T([32, 672, 1, 1], f16), T([168, 672, 1, 1], f16), T([168], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([32, 168, 1, 1], f16), T([672, 168, 1, 1], f16), T([672], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 672, 14, 14], f16), T([112, 672, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 672, 14, 14], f16), T([672, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 672), {})
cnt: 1, ((T([32, 672, 7, 7], f16), T([160, 672, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([32, 160, 7, 7], f16), T([960, 160, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([32, 960, 7, 7], f16), T([960, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 960), {})
cnt: 2, ((T([32, 960, 1, 1], f16), T([240, 960, 1, 1], f16), T([240], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([32, 240, 1, 1], f16), T([960, 240, 1, 1], f16), T([960], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([32, 960, 7, 7], f16), T([160, 960, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})

```

----------------------------------------

TITLE: Cloning Tensors with clone in PyTorch (Python)
DESCRIPTION: Executes aten.clone, which creates a copy of a tensor. Cloning is particularly relevant when modifications to the tensor are needed without affecting the original tensor data—a regular requirement in neural network training where weight updates occur.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
aten.clone.default
cnt: 1, ((T([16, 512], i64),), {})
cnt: 1, ((T([16], i64),), {})
```

----------------------------------------

TITLE: Installing PyTorch and Related Packages on Intel GPU
DESCRIPTION: Facilitates the installation of PyTorch, Torchvision, and Torchaudio using pip commands, specifically tailored for Intel GPUs with both release and nightly wheels. Prerequisites include having the Intel GPU driver installed.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/get_start_xpu.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/xpu
```

LANGUAGE: Python
CODE:
```
pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/xpu
```

----------------------------------------

TITLE: Creating Stateless Functional Model with functorch
DESCRIPTION: Imports necessary `functorch` functions (`make_functional_with_buffers`, `vmap`, `grad`). Uses `make_functional_with_buffers(model)` to transform the existing stateful `nn.Module` (`model`) into a stateless version. This process extracts the model's parameters and buffers into separate tuples (`params`, `buffers`) and returns a pure function `fmodel` representing the model's forward pass logic. This separation is required for using `functorch` transforms.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from functorch import make_functional_with_buffers, vmap, grad

fmodel, params, buffers = make_functional_with_buffers(model)
```

----------------------------------------

TITLE: Computing Jacobian-Vector Product (JVP) with torch.func.jvp in Python
DESCRIPTION: This snippet demonstrates `torch.func.jvp` for computing Jacobian-vector products (forward-mode automatic differentiation). `jvp` takes the function `f`, the primal inputs `(x, y)`, and tangent vectors `(torch.ones(5), torch.ones(5))` corresponding to the inputs. It returns the original function's output (ignored here with `_`) and the JVP result (`out_tangent`). An assertion confirms the JVP for the product function `x*y` is `x+y` when tangents are ones.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.whirlwind_tour.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from torch.func import jvp
x = torch.randn(5)
y = torch.randn(5)
f = lambda x, y: (x * y)
_, out_tangent = jvp(f, (x, y), (torch.ones(5), torch.ones(5)))
assert torch.allclose(out_tangent, x + y)
```

----------------------------------------

TITLE: Using Learning Rate Schedulers in PyTorch
DESCRIPTION: Example showing how to use a learning rate scheduler with an optimizer. The scheduler adjusts the learning rate after each epoch based on a predefined strategy.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
scheduler = ExponentialLR(optimizer, gamma=0.9)

for epoch in range(20):
    for input, target in dataset:
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
    scheduler.step()
```

----------------------------------------

TITLE: Exporting Model with Context Manager - PyTorch - Python
DESCRIPTION: This snippet defines a simple neural network module using PyTorch, which includes a custom context manager in the forward pass. It shows usage of strict and non-strict tracing modes with torch.export, highlighting that non-strict mode can bypass TorchDynamo limitations around context managers. Requires PyTorch (torch), and demonstrates exporting with and without strict mode by passing a strict flag.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import contextlib
import torch

class ContextManager():
    def __init__(self):
        self.count = 0
    def __enter__(self):
        self.count += 1
    def __exit__(self, exc_type, exc_value, traceback):
        self.count -= 1

class M(torch.nn.Module):
    def forward(self, x):
        with ContextManager():
            return x.sin() + x.cos()

export(M(), (torch.ones(3, 3),), strict=False)  # Non-strict traces successfully
export(M(), (torch.ones(3, 3),))  # Strict mode fails with torch._dynamo.exc.Unsupported: ContextManager
```

----------------------------------------

TITLE: Profiling ATen Operator Calls in PyTorch (Python)
DESCRIPTION: This snippet logs counts and input/output patterns for a wide variety of ATen operators within a Python program, likely via PyTorch's tracing or profiling mechanisms. Dependencies include PyTorch, and familiarity with tensor shapes, strides, and data types is required. Each entry groups calls by the operator, lists the number of occurrences (cnt), and details argument examples and optional keyword settings. The expected input is a model execution trace, and the output enumerates how operators such as softmax, addmm, bmm, embedding, and loss-related operations are being used, which is crucial for optimization and debugging efforts.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistillGPT2_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([511, 50257], f16), 1, False), {})
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([511, 50257], f16), T([511, 50257], f16), 1, f16), {})
Operator: aten._softmax.default
cnt: 6, ((T([1, 12, 512, 512], f16), -1, False), {})
Operator: aten._softmax_backward_data.default
cnt: 6, ((T([1, 12, 512, 512], f16), T([1, 12, 512, 512], f16), -1, f16), {})
Operator: aten._to_copy.default
cnt: 6, ((T([1, 1, 512, 512], u8, stride=(1048576, 1048576, 1024, 1)),), {'dtype': torch.bool})
cnt: 6, ((T([], f16),), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})
Operator: aten._unsafe_view.default
cnt: 6, ((T([12, 512, 512], f16), [1, 12, 512, 512]), {})
cnt: 6, ((T([12, 512, 64], f16), [1, 12, 512, 64]), {})
cnt: 1, ((T([512, 50257], f16), [1, 512, 50257]), {})
cnt: 12, ((T([1, 512, 12, 64], f16), [1, 512, 768]), {})
Operator: aten.add.Tensor
cnt: 25, ((T([1, 512, 768], f16), T([1, 512, 768], f16)), {})
cnt: 18, ((T([1, 512, 3072], f16), T([1, 512, 3072], f16)), {})
cnt: 6, ((T([1, 512, 3072], f16), 1.0), {})
cnt: 1, ((T([50257, 768], f16), T([50257, 768], f16)), {})
Operator: aten.addmm.default
cnt: 6, ((T([2304], f16), T([512, 768], f16), T([768, 2304], f16)), {})
cnt: 6, ((T([768], f16), T([512, 768], f16), T([768, 768], f16)), {})
cnt: 6, ((T([3072], f16), T([512, 768], f16), T([768, 3072], f16)), {})
cnt: 6, ((T([768], f16), T([512, 3072], f16), T([3072, 768], f16)), {})
Operator: aten.bmm.default
cnt: 6, ((T([12, 512, 64], f16, stride=(64, 2304, 1)), T([12, 64, 512], f16, stride=(64, 1, 2304))), {})
cnt: 12, ((T([12, 512, 512], f16), T([12, 512, 64], f16, stride=(64, 2304, 1))), {})
cnt: 6, ((T([12, 512, 512], f16, stride=(262144, 1, 512)), T([12, 512, 64], f16, stride=(64, 768, 1))), {})
cnt: 6, ((T([12, 512, 64], f16, stride=(64, 768, 1)), T([12, 64, 512], f16, stride=(64, 1, 2304))), {})
cnt: 6, ((T([12, 64, 512], f16, stride=(64, 1, 2304)), T([12, 512, 512], f16)), {})
Operator: aten.cat.default
cnt: 6, (([T([1, 512, 768], f16), T([1, 512, 768], f16, stride=(512, 1, 512)), T([1, 512, 768], f16)], 2), {})
Operator: aten.clone.default
cnt: 2, ((T([1, 512], i64),), {})
Operator: aten.copy_.default
cnt: 2, ((T([1, 512], i64), T([1, 512], i64)), {})
Operator: aten.div.Tensor
cnt: 12, ((T([1, 12, 512, 512], f16), T([], f16)), {})
Operator: aten.embedding.default
cnt: 1, ((T([50257, 768], f16), T([1, 512], i64)), {})
cnt: 1, ((T([1024, 768], f16), T([1, 512], i64)), {})
Operator: aten.embedding_dense_backward.default
cnt: 1, ((T([1, 512, 768], f16), T([1, 512], i64), 1024, -1, False), {})
cnt: 1, ((T([1, 512, 768], f16), T([1, 512], i64), 50257, -1, False), {})
Operator: aten.mm.default
cnt: 1, ((T([512, 768], f16), T([768, 50257], f16, stride=(1, 768))), {})
cnt: 1, ((T([50257, 512], f16, stride=(1, 50257)), T([512, 768], f16)), {})
cnt: 1, ((T([512, 50257], f16), T([50257, 768], f16)), {})
cnt: 6, ((T([512, 768], f16), T([768, 3072], f16, stride=(1, 768))), {})
cnt: 6, ((T([3072, 512], f16, stride=(1, 3072)), T([512, 768], f16)), {})
cnt: 6, ((T([512, 3072], f16), T([3072, 768], f16, stride=(1, 3072))), {})
cnt: 6, ((T([768, 512], f16, stride=(1, 768)), T([512, 3072], f16)), {})
cnt: 6, ((T([512, 768], f16), T([768, 768], f16, stride=(1, 768))), {})
cnt: 6, ((T([768, 512], f16, stride=(1, 768)), T([512, 768], f16)), {})
cnt: 6, ((T([512, 2304], f16), T([2304, 768], f16, stride=(1, 2304))), {})
cnt: 6, ((T([768, 512], f16, stride=(1, 768)), T([512, 2304], f16)), {})
Operator: aten.mul.Scalar
cnt: 6, ((T([1, 512, 3072], f16), 3.0), {})
Operator: aten.mul.Tensor
cnt: 12, ((T([1, 512, 3072], f16), 0.5), {})
cnt: 12, ((T([1, 512, 3072], f16), 0.044715), {})
cnt: 12, ((T([1, 512, 3072], f16), 0.7978845608028654), {})
cnt: 24, ((T([1, 512, 3072], f16), T([1, 512, 3072], f16)), {})
Operator: aten.native_layer_norm.default
cnt: 13, ((T([1, 512, 768], f16), [768], T([768], f16), T([768], f16), 1e-05), {})
Operator: aten.native_layer_norm_backward.default
cnt: 13, ((T([1, 512, 768], f16), T([1, 512, 768], f16), [768], T([1, 512, 1], f32), T([1, 512, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})
Operator: aten.nll_loss_backward.default
cnt: 1, ((T([], f16), T([511, 50257], f16), T([511], i64), None, 1, -100, T([], f16)), {})
Operator: aten.nll_loss_forward.default
cnt: 1, ((T([511, 50257], f16), T([511], i64), None, 1, -100), {})
Operator: aten.pow.Tensor_Scalar
cnt: 6, ((T([1, 512, 3072], f16), 3.0), {})
cnt: 6, ((T([1, 512, 3072], f16), 2.0), {})
Operator: aten.slice_backward.default
cnt: 1, ((T([1, 511, 50257], f16), [1, 511, 50257], 2, 0, 9223372036854775807, 1), {})
cnt: 1, ((T([1, 511, 50257], f16), [1, 512, 50257], 1, 0, -1, 1), {})
Operator: aten.split.Tensor
cnt: 6, ((T([1, 512, 2304], f16), 768, 2), {})
Operator: aten.sum.SymInt
cnt: 12, ((T([512, 768], f16), [0], True), {})
cnt: 6, ((T([512, 3072], f16), [0], True), {})
cnt: 6, ((T([512, 2304], f16), [0], True), {})
Operator: aten.tanh.default
cnt: 6, ((T([1, 512, 3072], f16),), {})
Operator: aten.tanh_backward.default
cnt: 6, ((T([1, 512, 3072], f16), T([1, 512, 3072], f16)), {})
Operator: aten.where.self
cnt: 12, ((T([1, 1, 512, 512], b8), T([1, 12, 512, 512], f16), T([], f16)), {})
```

----------------------------------------

TITLE: Implementing AOT Autograd with Debug Compiler
DESCRIPTION: Demonstrates AOT Autograd usage with a simple compiler function that prints the generated graph
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/aot_autograd_optimizations.ipynb#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from functorch.compile import aot_function

def compiler_fn(fx_module: torch.fx.GraphModule, _):
    print(fx_module.code)
    return fx_module

aot_print_fn = aot_function(fn, fw_compiler=compiler_fn, bw_compiler=compiler_fn)

cloned_inputs = [x.clone().detach().requires_grad_(True) for x in (a, b, c, d)]
cloned_a, cloned_b, cloned_c, cloned_d = cloned_inputs
res = aot_print_fn(cloned_a, cloned_b, cloned_c, cloned_d)
res.sum().backward()
assert torch.allclose(ref, res)
```

----------------------------------------

TITLE: Executing aten.add.Tensor operation in PyTorch
DESCRIPTION: Performs element-wise addition on two input tensors using the Aten backend in PyTorch with half-precision tensors. Key parameters include tensor shapes and data types. The output is a tensor with the same shape as the inputs. No additional dependencies are needed beyond PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten.add.Tensor
cnt: 3, ((T([128, 256, 56, 56], f16), T([128, 256, 56, 56], f16)), {})
```

----------------------------------------

TITLE: PyTorch Tensor Addition Operations
DESCRIPTION: Various tensor addition operations with different shapes and dimensions, primarily using float16 dtype
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
aten.add.Tensor(T([], i64), 1)
aten.add.Tensor(T([64, 32, 112, 112], f16), T([64, 32, 112, 112], f16))
```

----------------------------------------

TITLE: Applying aten.relu_ Activation in PyTorch
DESCRIPTION: This function uses aten.relu_.default, a PyTorch operator applying the Rectified Linear Unit (ReLU) activation function on tensors. Key dependencies include PyTorch library for deep learning operations. Inputs are multi-dimensional tensors, and the output is the same tensor with ReLU applied, yielding non-negative values, suitable for introducing non-linearity in neural networks. Input tensors must be of compatible type and dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_11

LANGUAGE: Python
CODE:
```
cnt: 2, ((T([64, 32, 112, 112], f16),), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 192, 112, 112], f16),), {})
```

----------------------------------------

TITLE: Shape Specialization in torch.export with Branching Logic
DESCRIPTION: Demonstration of how torch.export specializes on input tensor shapes by default. This example shows how shape-dependent control flow is specialized based on the example inputs, resulting in only one branch being included in the exported program.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
import torch
from torch.export import export

class Mod(torch.nn.Module):
    def forward(self, x):
        if x.shape[0] > 5:
            return x + 1
        else:
            return x - 1

example_inputs = (torch.rand(10, 2),)
exported_program = export(Mod(), example_inputs)
print(exported_program)
```

----------------------------------------

TITLE: Defining and Compiling a Function with torch.compile in Python
DESCRIPTION: This snippet defines a simple Python function `fn` that performs element-wise multiplication scaled by the first dimension of the input tensor 'a'. It then uses `torch.compile` to optimize this function. The function is called twice with tensors of different shapes to demonstrate Dynamo's tracing behavior.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import torch

@torch.compile
def fn(a, b):
    return a.shape[0] * a * b

fn(torch.randn(4, 3), torch.randn(4, 3))
fn(torch.randn(8, 3), torch.randn(8, 3))
```

----------------------------------------

TITLE: Backward Pass Stream Semantics - Safe Use in Same Stream - PyTorch Python
DESCRIPTION: This snippet illustrates a safe pattern for backward pass stream semantics. When `loss.backward()` is called within a specific stream context (`with torch.cuda.stream(s)`), using the resulting gradients (`use grads`) within the *same* stream context is safe because all operations are serialized within that stream.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_15

LANGUAGE: Python
CODE:
```
s = torch.cuda.Stream()

# Safe, grads are used in the same stream context as backward()
with torch.cuda.stream(s):
    loss.backward()
    use grads
```

----------------------------------------

TITLE: Move Dataloader Batch to CUDA Device PyTorch
DESCRIPTION: Shows how to move tensors from a dataloader batch to a specific CUDA device during iteration. This is necessary when the dataloader loads data onto the CPU by default but needs to be processed on the GPU.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_36

LANGUAGE: python
CODE:
```
cuda0 = torch.device('cuda:0')  # CUDA GPU 0
for i, x in enumerate(train_loader):
    x = x.to(cuda0)
```

----------------------------------------

TITLE: Data-Dependent Operations Example with Graph Breaks
DESCRIPTION: Example demonstrating how data-dependent operations cause graph breaks in torch.compile
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
import torch

@torch.compile
def fn(x):
    y = x.sum()
    if y > 0:
        return x + y.item()
    return x - y.item()

fn(torch.ones(3, 3))
```

----------------------------------------

TITLE: Configuring TF32 Matmul and cuDNN Precision (Python)
DESCRIPTION: Shows how to explicitly set the `allow_tf32` flags for CUDA matrix multiplications (`torch.backends.cuda.matmul`) and cuDNN operations (`torch.backends.cudnn`) via the `torch.backends` interface in Python. These flags control whether PyTorch is allowed to use TensorFloat32 tensor cores.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_1

LANGUAGE: python
CODE:
```
# The flag below controls whether to allow TF32 on matmul. This flag defaults to False
# in PyTorch 1.12 and later.
torch.backends.cuda.matmul.allow_tf32 = True

# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.
torch.backends.cudnn.allow_tf32 = True
```

----------------------------------------

TITLE: Create Tensor Like Another with Ones or Zeros PyTorch
DESCRIPTION: Shows the use of `torch.ones_like()` and `torch.zeros_like()` as convenient functions to create new tensors with the same size, device, and dtype as an input tensor, filled with ones or zeros.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_39

LANGUAGE: python
CODE:
```
x_cpu = torch.empty(2, 3)
x_gpu = torch.empty(2, 3)

y_cpu = torch.ones_like(x_cpu)
y_gpu = torch.zeros_like(x_gpu)
```

----------------------------------------

TITLE: Exporting a Simple PyTorch Module
DESCRIPTION: This code snippet demonstrates the usage of the torch.export function to export a basic PyTorch module by capturing a fully traced graph. Requires PyTorch and torch.export package. Sample inputs include random tensors which are used to execute and validate the exported program.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from torch.export import export

class Mod(torch.nn.Module):
    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        a = torch.sin(x)
        b = torch.cos(y)
        return a + b

example_args = (torch.randn(10, 10), torch.randn(10, 10))

exported_program: torch.export.ExportedProgram = export(
    Mod(), args=example_args
)
print(exported_program)
```

----------------------------------------

TITLE: Generating Dummy Data and Targets for CNN in PyTorch
DESCRIPTION: Sets the computation device to 'cuda' if available (otherwise defaults to CPU implicitly later). Initializes parameters like `num_models` (unused in the snippet) and `batch_size`. Creates a batch of random tensor data (`data`) simulating 28x28 images and corresponding random integer targets (`targets`) for a batch size of 64, moving both tensors to the specified GPU device.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
device = 'cuda'

num_models = 10
batch_size = 64
data = torch.randn(batch_size, 1, 28, 28, device=device)

targets = torch.randint(10, (64,), device=device)
```

----------------------------------------

TITLE: Initializing torch.distributed with TCP Method (Python)
DESCRIPTION: This snippet shows the basic syntax for initializing a PyTorch distributed process group using the TCP initialization method. It demonstrates importing `torch.distributed` and calling `init_process_group`, specifying a `backend` (like 'gloo' or 'nccl') and a `tcp://` URL for `init_method` which points to the address of one of the participating processes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.rst#_snippet_0

LANGUAGE: python
CODE:
```
import torch.distributed as dist

# Use address of one of the machines
dist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',
```

----------------------------------------

TITLE: Working with Non-Contiguous Tensor Views in PyTorch
DESCRIPTION: Shows how operations like transpose() create views that might be non-contiguous. Demonstrates checking contiguity with is_contiguous() and converting to a contiguous tensor with contiguous() method when needed.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_view.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> base = torch.tensor([[0, 1],[2, 3]])
>>> base.is_contiguous()
True
>>> t = base.transpose(0, 1)  # `t` is a view of `base`. No data movement happened here.
# View tensors might be non-contiguous.
>>> t.is_contiguous()
False
# To get a contiguous tensor, call `.contiguous()` to enforce
# copying data when `t` is not contiguous.
>>> c = t.contiguous()
```

----------------------------------------

TITLE: Performing Backpropagation with PyTorch C++ Autograd
DESCRIPTION: This example illustrates how to use the Autograd API in PyTorch C++ to enable automatic differentiation. It creates tensors (`torch::ones`, `torch::randn`) using the `torch::` namespace, with `a` explicitly marked to require gradients (`torch::requires_grad()`). After a simple operation (`a + b`), calling `c.backward()` computes the gradient of `c` with respect to `a`, which is then stored in `a.grad()`. Requires specific autograd headers.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/index.rst#_snippet_1

LANGUAGE: cpp
CODE:
```
#include <torch/csrc/autograd/variable.h>
#include <torch/csrc/autograd/function.h>

torch::Tensor a = torch::ones({2, 2}, torch::requires_grad());
torch::Tensor b = torch::randn({2, 2});
auto c = a + b;
c.backward(); // a.grad() will now hold the gradient of c w.r.t. a.
```

----------------------------------------

TITLE: Basic GradScaler Usage with PyTorch AMP
DESCRIPTION: Shows the basic usage of GradScaler for stepping and updating optimizers in mixed precision training.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/amp_examples.rst#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
scaler.step(optimizer)
scaler.update()
```

----------------------------------------

TITLE: Running PyTorch Bottleneck Profiler Command
DESCRIPTION: Command line instruction for running the bottleneck profiler on a Python script. The utility accepts a source script path and optional arguments.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/bottleneck.rst#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python -m torch.utils.bottleneck /path/to/source/script.py [args]
```

----------------------------------------

TITLE: In-Place Operation Causing vmap Error - PyTorch - Python
DESCRIPTION: This snippet demonstrates how using in-place PyTorch operations in batch transforms (vmap) can cause errors due to shape mismatch. The function 'f' attempts an in-place addition on tensor 'x' using 'y'. If the tensor shapes are incompatible under vmap, this results in a runtime error. Dependencies are PyTorch and torch.func. Inputs are singleton and batched tensors; output is a tensor (if successful). Limitation: Do not use in-place operations with mismatched shapes in vmap.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
def f(x, y):
  x.add_(y)
  return x

x = torch.randn(1)
y = torch.randn(3, 1)  # When vmapped over, looks like it has shape [1]

# Raises an error because `x` has fewer elements than `y`.
vmap(f, in_dims=(None, 0))(x, y)
```

----------------------------------------

TITLE: Capturing with torch.cuda.amp and Partial Exclusion
DESCRIPTION: Illustrates how to capture forward, loss, and backward operations using `torch.cuda.graph` alongside `torch.cuda.amp.autocast` and `scaler.scale`, while explicitly omitting `scaler.step(optimizer)` and `scaler.update()` from the captured graph. This allows integrating CUDA graphs with typical AMP usage by running the scaler steps eagerly outside the graph replay.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_44

LANGUAGE: python
CODE:
```
# warmup
    # In a real setting, use a few batches of real data.
    s = torch.cuda.Stream()
    s.wait_stream(torch.cuda.current_stream())
    with torch.cuda.stream(s):
        for i in range(3):
            optimizer.zero_grad(set_to_none=True)
            with torch.cuda.amp.autocast():
                y_pred = model(static_input)
                loss = loss_fn(y_pred, static_target)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
    torch.cuda.current_stream().wait_stream(s)

    # capture
    g = torch.cuda.CUDAGraph()
    optimizer.zero_grad(set_to_none=True)
    with torch.cuda.graph(g):
        with torch.cuda.amp.autocast():
            static_y_pred = model(static_input)
            static_loss = loss_fn(static_y_pred, static_target)
        scaler.scale(static_loss).backward()
        # don't capture scaler.step(optimizer) or scaler.update()

    real_inputs = [torch.rand_like(static_input) for _ in range(10)]
    real_targets = [torch.rand_like(static_target) for _ in range(10)]

    for data, target in zip(real_inputs, real_targets):
        static_input.copy_(data)
        static_target.copy_(target)
        g.replay()
        # Runs scaler.step and scaler.update eagerly
        scaler.step(optimizer)
        scaler.update()
```

----------------------------------------

TITLE: Python Inference with Compiled Model
DESCRIPTION: Shows how to load and run inference using the compiled model artifact in Python using the aoti_load_package utility function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_aot_inductor.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import os
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"
model = torch._inductor.aoti_load_package(os.path.join(os.getcwd(), "model.pt2"))
print(model(torch.randn(8, 10, device=device)))
```

----------------------------------------

TITLE: Basic Resnet18 Profiling with torch.compile
DESCRIPTION: Example showing how to profile a compiled ResNet18 model using torch.profiler, including warm-up run and chrome trace export.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_profiling_torch_compile.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from torchvision.models import resnet18

device = 'cuda'      # or 'cpu', 'xpu', etc.
model = resnet18().to(device)

inputs = [torch.randn((5, 3, 224, 224), device=device) for _ in range(10)]

model_c = torch.compile(model)

def fwd_bwd(inp):
    out = model_c(inp)
    out.sum().backward()

# warm up
fwd_bwd(inputs[0])

with torch.profiler.profile() as prof:
    for i in range(1, 4):
        fwd_bwd(inputs[i])
        prof.step()

prof.export_chrome_trace("trace.json")
```

----------------------------------------

TITLE: Creating a Nested Tensor with torch.jagged Layout in Python
DESCRIPTION: This snippet demonstrates constructing a nested tensor using the `torch.nested.nested_tensor` method with a `torch.jagged` layout. It highlights the requirement for input tensors to have the same number of dimensions when constructing a nested tensor, and showcases how to specify dtype, device, and autograd parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> a, b = torch.arange(3), torch.arange(5) + 3
>>> a
tensor([0, 1, 2])
>>> b
tensor([3, 4, 5, 6, 7])
>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)
>>> print([component for component in nt])
[tensor([0, 1, 2]), tensor([3, 4, 5, 6, 7])]
```

LANGUAGE: python
CODE:
```
>>> a = torch.randn(50, 128) # 2D tensor
>>> b = torch.randn(2, 50, 128) # 3D tensor
>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)
...
RuntimeError: When constructing a nested tensor, all tensors in list must have the same dim
```

LANGUAGE: python
CODE:
```
>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32, device='cuda', requires_grad=True)
>>> print([component for component in nt])
[tensor([0., 1., 2.], device='cuda:0', grad_fn=<UnbindBackwardAutogradNestedTensor0>), tensor([3., 4., 5., 6., 7.], device='cuda:0', grad_fn=<UnbindBackwardAutogradNestedTensor0>)]
```

----------------------------------------

TITLE: Layer Normalization Operations
DESCRIPTION: Layer normalization operations with different tensor shapes and epsilon values. Includes both forward and backward passes with corresponding gradient computations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/twins_pcpvt_base_training.txt#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
aten.native_layer_norm.default((T([32, 3136, 64], f16), [64], T([64], f16), T([64], f16), 1e-05))
```

----------------------------------------

TITLE: Cloning Tensors with aten.clone in Python
DESCRIPTION: The aten.clone.default operator is frequently used to create a duplicate tensor while preserving its dimensions and data types. This operation allows for separate processing paths without altering the original data, ensuring data integrity across parallel processes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/nvidia_deeprecommender_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
Operator: aten.clone.default
cnt: 1, ((T([256, 197951], f16),), {})
```

----------------------------------------

TITLE: Quantizing Tensors in PyTorch
DESCRIPTION: Functions for converting floating point tensors to quantized format with specified scale, zero point, and data type.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
torch.quantize_per_tensor(x, scale, zero_point, dtype)
torch.quantize_per_channel(x, scales, zero_points, axis, dtype)
torch.quantize_per_tensor_dynamic(x, dtype, reduce_range)
to(torch.float16)
```

----------------------------------------

TITLE: Freezing Compiled Functions in PyTorch
DESCRIPTION: Example showing how to freeze compiled functions to prevent recompilation in production environments using torch.dynamo.run()
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
frozen_toy_example = dynamo.run(toy_example)
frozen_toy_example(torch.randn(10), torch.randn(10))
```

----------------------------------------

TITLE: Ensembling Models using torch.func.stack_module_state and vmap in Python
DESCRIPTION: Presents the PyTorch 2.0+ approach for model ensembling using `torch.func`. A stateless 'meta' device version of the base model is created. `torch.func.stack_module_state` stacks the parameters and buffers from the list of `models` into separate dictionaries. A helper function `call_single_model` wraps `torch.func.functional_call` to apply the `base_model` logic using a single set of parameters/buffers. `torch.vmap` maps this function over the stacked `params` and `buffers`, applying each model instance to the same `data`. Requires `torch`, `copy`, and `torch.func`. This replaces the `functorch.combine_state_for_ensemble` pattern.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.migrating.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
import torch
num_models = 5
batch_size = 64
in_features, out_features = 3, 3
models = [torch.nn.Linear(in_features, out_features) for i in range(num_models)]
data = torch.randn(batch_size, 3)

# ------------------------------------
# using torch.func (as of PyTorch 2.0)
# ------------------------------------
import copy

# Construct a version of the model with no memory by putting the Tensors on
# the meta device.
base_model = copy.deepcopy(models[0])
base_model.to('meta')

params, buffers = torch.func.stack_module_state(models)

# It is possible to vmap directly over torch.func.functional_call,
# but wrapping it in a function makes it clearer what is going on.
def call_single_model(params, buffers, data):
    return torch.func.functional_call(base_model, (params, buffers), (data,))

output = torch.vmap(call_single_model, (0, 0, None))(params, buffers, data)
assert output.shape == (num_models, batch_size, out_features)
```

----------------------------------------

TITLE: Demonstrating Broadcasting Backward Compatibility Changes in Python
DESCRIPTION: Highlights a backward compatibility change introduced by broadcasting. Previously, operations on tensors with different shapes but the same number of elements might have worked by viewing them as 1D. Now, broadcasting rules apply. The example shows `torch.add(torch.ones(4,1), torch.randn(4))` resulting in a `[4,4]` tensor due to broadcasting, whereas it might have previously resulted in `[4,1]`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/broadcasting.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> torch.add(torch.ones(4,1), torch.randn(4))
```

----------------------------------------

TITLE: Using torch.use_deterministic_algorithms() in PyTorch
DESCRIPTION: This code snippet shows how to enable deterministic algorithms in PyTorch using the torch.use_deterministic_algorithms() method. When set to True, it activates the fill_uninitialized_memory feature if it's enabled.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/deterministic.rst#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
torch.use_deterministic_algorithms(True)
```

----------------------------------------

TITLE: Computing Per-sample Gradients with vmap and grad in PyTorch
DESCRIPTION: Demonstrates combining vmap and grad transforms to compute per-sample gradients in a simple linear model with MSE loss.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/whirlwind_tour.ipynb#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from functorch import vmap
batch_size, feature_size = 3, 5

def model(weights,feature_vec):
    # Very simple linear model with activation
    assert feature_vec.dim() == 1
    return feature_vec.dot(weights).relu()

def compute_loss(weights, example, target):
    y = model(weights, example)
    return ((y - target) ** 2).mean()  # MSELoss

weights = torch.randn(feature_size, requires_grad=True)
examples = torch.randn(batch_size, feature_size)
targets = torch.randn(batch_size)
inputs = (weights,examples, targets)
grad_weight_per_example = vmap(grad(compute_loss), in_dims=(None, 0, 0))(*inputs)
```

----------------------------------------

TITLE: Creating Tensor Views in TorchScript (Python)
DESCRIPTION: Demonstrates how indexing a PyTorch Tensor in TorchScript creates a 'view' that shares the underlying data with the original Tensor. Changes to the view can affect the original Tensor due to TorchScript's reference semantics for Tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_16

LANGUAGE: python
CODE:
```
t = torch.rand(3, 4)
t2 =  t[0] # view of one slice of t
```

----------------------------------------

TITLE: Importing PyTorch C++ Extension Module
DESCRIPTION: This snippet shows the import statement for the torch.utils.cpp_extension module, which provides utilities for working with C++ extensions in PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cpp_extension.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
torch.utils.cpp_extension
```

----------------------------------------

TITLE: Creating torch.device Objects Using String and Device Ordinal
DESCRIPTION: This code snippet shows how to create torch.device objects by passing a string for the device type and an integer for the device ordinal. This approach works for CUDA, MPS, and CPU devices.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> torch.device('cuda', 0)
device(type='cuda', index=0)

>>> torch.device('mps', 0)
device(type='mps', index=0)

>>> torch.device('cpu', 0)
device(type='cpu', index=0)
```

----------------------------------------

TITLE: Backward Pass Stream Semantics - BC Note Safe Pattern Post-1.9 - PyTorch Python
DESCRIPTION: This snippet repeats the safe pattern (required in PyTorch versions > 1.9) for using gradients on the default stream when `loss.backward()` was called on a different stream `s`. Explicitly calling `torch.cuda.current_stream().wait_stream(s)` synchronizes the default stream with stream `s` before the gradients are used, ensuring the backward pass is complete.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_22

LANGUAGE: Python
CODE:
```
with torch.cuda.stream(s):
    loss.backward()
torch.cuda.current_stream().wait_stream(s)
use grads
```

----------------------------------------

TITLE: Loading Optimizer State Dict with Custom Parameter Mapping
DESCRIPTION: This example shows how to implement a custom hook for loading an optimizer state dict when the model architecture has changed. It demonstrates adapting the state dict to map parameters from a single-layer model to a two-layer model.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
def adapt_state_dict_ids(optimizer, state_dict):
    adapted_state_dict = deepcopy(optimizer.state_dict())
    # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict.
    for k, v in state_dict['param_groups'][0].items():
        if k not in ['params', 'param_names']:
            adapted_state_dict['param_groups'][0][k] = v

    lookup_dict = {
        'fc1.weight': 'fc.weight',
        'fc1.bias': 'fc.bias',
        'fc2.weight': 'fc.weight',
        'fc2.bias': 'fc.bias'
    }
    clone_deepcopy = lambda d: {k: (v.clone() if isinstance(v, torch.Tensor) else deepcopy(v)) for k, v in d.items()}
    for param_id, param_name in zip(
            optimizer.state_dict()['param_groups'][0]['params'],
            optimizer.state_dict()['param_groups'][0]['param_names']):
        name_in_loaded = lookup_dict[param_name]
        index_in_loaded_list = state_dict['param_groups'][0]['param_names'].index(name_in_loaded)
        id_in_loaded = state_dict['param_groups'][0]['params'][index_in_loaded_list]
        # Copy the state of the corresponding parameter
        if id_in_loaded in state_dict['state']:
            adapted_state_dict['state'][param_id] = clone_deepcopy(state_dict['state'][id_in_loaded])

    return adapted_state_dict

optimizer2.register_load_state_dict_pre_hook(adapt_state_dict_ids)
optimizer2.load_state_dict(torch.load(PATH)) # The previous optimizer saved state_dict
```

----------------------------------------

TITLE: FX Graph Mode Quantization Example
DESCRIPTION: Shows implementation of post-training quantization and quantization-aware training using FX Graph Mode. Includes dynamic, static, and QAT approaches with proper configuration setup.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
import torch
from torch.ao.quantization import (
  get_default_qconfig_mapping,
  get_default_qat_qconfig_mapping,
  QConfigMapping,
)
import torch.ao.quantization.quantize_fx as quantize_fx
import copy

model_fp = UserModel()

model_to_quantize = copy.deepcopy(model_fp)
model_to_quantize.eval()
qconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_dynamic_qconfig)
example_inputs = (input_fp32)
model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)
model_quantized = quantize_fx.convert_fx(model_prepared)
```

----------------------------------------

TITLE: Layer Normalization in Transformer Blocks
DESCRIPTION: Shows layer normalization operations in transformer blocks for stabilizing network activations. These normalize tensors along the embedding dimension (768) with learned scale and bias parameters, using epsilon 1e-06 for numerical stability.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
Operator: aten.native_layer_norm.default
cnt: 72, ((T([2, 576, 768], f16, stride=(442368, 1, 576)), [768], T([768], f16), T([768], f16), 1e-06), {})
cnt: 3, ((T([2, 577, 768], f16), [768], T([768], f16), T([768], f16), 1e-06), {})
cnt: 2, ((T([2, 1, 768], f16), [768], T([768], f16), T([768], f16), 1e-06), {})
Operator: aten.native_layer_norm_backward.default
cnt: 3, ((T([2, 577, 768], f16), T([2, 577, 768], f16), [768], T([2, 577, 1], f32), T([2, 577, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})
cnt: 2, ((T([2, 1, 768], f16), T([2, 1, 768], f16), [768], T([2, 1, 1], f32), T([2, 1, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})
cnt: 72, ((T([2, 576, 768], f16), T([2, 576, 768], f16, stride=(442368, 1, 576)), [768], T([2, 576, 1], f32), T([2, 576, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})
```

----------------------------------------

TITLE: Registering Custom Dataclass as PyTree Input in torch.export (Python)
DESCRIPTION: This example shows how to use a custom dataclass as an input type for a module being exported with `torch.export.export`. The `Input` dataclass is registered using `torch.export.register_dataclass`, allowing instances of it to be treated as PyTrees. The module `M` accepts an instance of `Input`, and the export process correctly handles this custom input type.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.programming_model.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from dataclasses import dataclass
import torch

@dataclass
class Input:
    f: torch.Tensor
    p: torch.Tensor

torch.export.register_dataclass(Input)

class M(torch.nn.Module):
    def forward(self, x: Input):
        return x.f + 1

torch.export.export(M(), (Input(f=torch.ones(10, 4), p=torch.zeros(10, 4)),))
```

----------------------------------------

TITLE: Configuring Different Penalization for Bias Parameters in PyTorch
DESCRIPTION: Example of how to apply different weight decay settings to different parameter groups. This approach prevents bias terms from being penalized by setting their weight_decay to 0.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
bias_params = [p for name, p in self.named_parameters() if 'bias' in name]
others = [p for name, p in self.named_parameters() if 'bias' not in name]

optim.SGD([
                {'params': others},
                {'params': bias_params, 'weight_decay': 0}
            ], weight_decay=1e-2, lr=1e-2)
```

----------------------------------------

TITLE: Computing Batch Jacobian with vmap in PyTorch
DESCRIPTION: Demonstrates how to use vmap with jacrev to compute Jacobians for a batch of inputs. This approach applies jacrev to each input in the batch and stacks the results efficiently.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
compute_batch_jacobian = vmap(jacrev(predict, argnums=2), in_dims=(None, None, 0))
batch_jacobian0 = compute_batch_jacobian(weight, bias, x)
```

----------------------------------------

TITLE: Free Intermediate Tensors in PyTorch
DESCRIPTION: This code example illustrates how intermediate variables in PyTorch can unnecessarily extend their scope and retain memory. The snippet provides a way to free memory earlier by explicitly deleting intermediate objects once they are no longer needed within a loop.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/faq.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
    for i in range(5):
        intermediate = f(input[i])
        result += g(intermediate)
    output = h(result)
    return output
```

----------------------------------------

TITLE: Cloning Tensors with ATen Clone Operator
DESCRIPTION: Covers `aten.clone`, enabling tensor copying to prevent in-place modifications. Supports tensors with shapes such as [64, 3, 256, 256]. Essential for operations requiring tensor duplication before transformations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_11

LANGUAGE: plaintext
CODE:
```
Operator: aten.clone.default
cnt: 1, ((T([64, 3, 256, 256], f16),), {})
```

----------------------------------------

TITLE: Performing Inference with FP32 on Intel GPU
DESCRIPTION: An example of executing an FP32 inference workflow using ResNet50 model on Intel GPU. The code transfers the model and data to XPU and performs inference within a no-grad context.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/get_start_xpu.rst#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
import torch
import torchvision.models as models

model = models.resnet50(weights="ResNet50_Weights.DEFAULT")
model.eval()
data = torch.rand(1, 3, 224, 224)

model = model.to("xpu")
data = data.to("xpu")

with torch.no_grad():
    model(data)

print("Execution finished")
```

----------------------------------------

TITLE: Computing Mean of Tensor Dimensions in Python
DESCRIPTION: Calculates the mean of tensor elements along specified dimensions. It requires a tensor input, with dimensions like [16, 64] or [16, 256], and a list of dimensions for reduction, such as [0]. The output is a tensor with reduced dimensions, providing average values crucial for statistical normalizations in data analysis.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
Operator: aten.mean.dim
cnt: 4, ((T([16, 64], f16), [0]), {})
cnt: 4, ((T([16, 128], f16), [0]), {})
cnt: 26, ((T([16, 256], f16), [0]), {})
```

----------------------------------------

TITLE: Copying Tensor Data with copy_ in PyTorch (Python)
DESCRIPTION: Performs aten.copy_ which replicates the content from one tensor to another. It's commonly employed when data from a calculated tensor needs to overwrite the contents of a destination tensor, often used in custom model implementations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_10

LANGUAGE: Python
CODE:
```
aten.copy_.default
cnt: 1, ((T([16, 512], i64), T([16, 512], i64)), {})
cnt: 1, ((T([16], i64), T([16], i64)), {})
```

----------------------------------------

TITLE: PyTorch Tensor Operations with Shape Information
DESCRIPTION: Collection of PyTorch tensor operations showing operator type, count, tensor shapes, and parameters. Operations include batch normalization, leaky ReLU, convolutions, and loss calculations. All operations use half precision (f16) tensors with various dimensions and strides.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cspdarknet53_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
# Batch normalization operation examples
cnt: 1, ((T([64, 32, 256, 256], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})

# Leaky ReLU operation examples
cnt: 1, ((T([64, 32, 256, 256], f16),), {})

# Matrix multiplication examples
cnt: 1, ((T([64, 1000], f16), T([1000, 1024], f16)), {})

# Loss calculation examples
cnt: 1, ((T([64, 1000], f16), T([64], i64), None, 1, -100), {})
```

----------------------------------------

TITLE: Auto-vectorization with vmap Transform in PyTorch
DESCRIPTION: Shows how to use vmap transform to add batch dimensions to tensor operations in a simple linear model.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/whirlwind_tour.ipynb#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
from functorch import vmap
batch_size, feature_size = 3, 5
weights = torch.randn(feature_size, requires_grad=True)

def model(feature_vec):
    # Very simple linear model with activation
    assert feature_vec.dim() == 1
    return feature_vec.dot(weights).relu()

examples = torch.randn(batch_size, feature_size)
result = vmap(model)(examples)
```

----------------------------------------

TITLE: Converting Tensor to Python Number
DESCRIPTION: Shows how to extract a Python number from a single-value tensor using the item() method.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensors.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> x = torch.tensor([[1]])
>>> x
tensor([[ 1]])
>>> x.item()
1
>>> x = torch.tensor(2.5)
>>> x
tensor(2.5000)
>>> x.item()
2.5
```

----------------------------------------

TITLE: Recompilation Trigger Example
DESCRIPTION: Shows how changing tensor shapes triggers recompilation in torch.compile when guards fail.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch

@torch.compile
def fn(x):
    return x + 1

fn(torch.ones(3, 3))
fn(torch.ones(4, 4))
```

----------------------------------------

TITLE: Invoking aten.native_batch_norm.default in PyTorch ATen
DESCRIPTION: Logs calls to the `aten.native_batch_norm.default` operator, which performs the forward pass of batch normalization. The arguments include the input tensor (4D, f16), weight (1D, f16), bias (1D, f16), running mean (1D, f16), running variance (1D, f16), a boolean indicating training mode (`True`), momentum factor (`0.1`), and epsilon (`1e-05`). The `cnt` shows how many times each specific combination of tensor shapes and parameters was called.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/lcnet_050_training.txt#2025-04-22_snippet_3

LANGUAGE: plaintext
CODE:
```
Operator: aten.native_batch_norm.default
cnt: 2, ((T([128, 8, 112, 112], f16), T([8], f16), T([8], f16), T([8], f16), T([8], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 16, 56, 56], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), True, 0.1, 1e-05), {})
cnt: 3, ((T([128, 32, 56, 56], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 32, 28, 28], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})
cnt: 3, ((T([128, 64, 28, 28], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 64, 14, 14], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})
cnt: 11, ((T([128, 128, 14, 14], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 128, 7, 7], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})
cnt: 3, ((T([128, 256, 7, 7], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), True, 0.1, 1e-05), {})
```

----------------------------------------

TITLE: Handling Weights Only torch.load Failure and Whitelisting Classes - PyTorch - Python
DESCRIPTION: Shows the error message and steps necessary to recover from a torch.load(weights_only=True) failure, including re-running with weights_only=False or allowlisting required classes and functions using torch.serialization.safe_globals. No execution context; relevant for debugging safe loading of PyTorch model checkpoints and managing deserialization security. No execution dependencies.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
_pickle.UnpicklingError: Weights only load failed. This file can still be loaded,
to do so you have two options, do those steps only if you trust the source of the checkpoint.
    1. Re-running `torch.load` with `weights_only` set to `False` will likely succeed,
        but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
    2. Alternatively, to load with `weights_only=True` please check the recommended
       steps in the following error message.
       WeightsUnpickler error: Unsupported global: GLOBAL {__module__}.{__name__} was not an allowed global by
       default. Please use `torch.serialization.add_safe_globals([{__name__}])` or the
       `torch.serialization.safe_globals([{__name__}])` context manager to allowlist this global
       if you trust this class/function.
```

----------------------------------------

TITLE: Alternative Ways to Specify Devices in PyTorch Functions
DESCRIPTION: This code snippet demonstrates different ways to specify devices when creating tensors, including using torch.device objects, device strings, and legacy integer device ordinals, which are all equivalent approaches.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> # Example of a function that takes in a torch.device
>>> cuda1 = torch.device('cuda:1')
>>> torch.randn((2,3), device=cuda1)

>>> # You can substitute the torch.device with a string
>>> torch.randn((2,3), device='cuda:1')
```

----------------------------------------

TITLE: Saving and Loading Quantized Models using state_dict in PyTorch
DESCRIPTION: Demonstrates how to save and load a quantized model using state_dict. This method involves preparing the model, converting it, and then using torch.save and torch.load with BytesIO for serialization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
class M(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(5, 5)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.linear(x)
        x = self.relu(x)
        return x

m = M().eval()
prepare_orig = prepare_fx(m, {'' : default_qconfig})
prepare_orig(torch.rand(5, 5))
quantized_orig = convert_fx(prepare_orig)

# Save/load using state_dict
b = io.BytesIO()
torch.save(quantized_orig.state_dict(), b)

m2 = M().eval()
prepared = prepare_fx(m2, {'' : default_qconfig})
quantized = convert_fx(prepared)
b.seek(0)
quantized.load_state_dict(torch.load(b))
```

----------------------------------------

TITLE: Preserving Tensor View Relationships When Saving - PyTorch - Python
DESCRIPTION: Illustrates that torch.save preserves view relationships between tensors when multiple related tensors are saved together. If tensors share underlying storage (e.g., views of the same tensor), loading them reconstructs the sharing. Requires torch. Two tensors (a base tensor and a strided view) are saved as a list; after loading, modifying the view also changes the base tensor, indicating correct storage sharing is preserved. Inputs are the original tensor and its view; output is a list with restored relationship; caveats arise with storage size (discussed later).
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> numbers = torch.arange(1, 10)
>>> evens = numbers[1::2]
>>> torch.save([numbers, evens], 'tensors.pt')
>>> loaded_numbers, loaded_evens = torch.load('tensors.pt')
>>> loaded_evens *= 2
>>> loaded_numbers
tensor([ 1,  4,  3,  8,  5, 12,  7, 16,  9])
```

----------------------------------------

TITLE: Configuring PyTorch C++ Build Using CMake - CMake
DESCRIPTION: This CMake file sets up a C++ project that depends on the PyTorch library and supports C++17 standards. It locates the required Torch and threading packages, establishes include directories, and sets up the build and linking process for an executable named 'Predictor'. Essential parameters include definitions of C++ standards, linkage options, and external dependencies such as system threads and dynamic libraries. The file is intended for users compiling PyTorch C++ projects using CMake 3.15 or above.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/mobile/custom_build/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
cmake_minimum_required(VERSION 3.15)

project(custom_build_project)

set(CMAKE_CXX_STANDARD 17 CACHE STRING "The C++ standard whose features are requested to build this target.")

# Find torch library
find_package(Torch REQUIRED)

# Main executable
add_executable(Predictor predictor.cpp)
target_include_directories(Predictor PUBLIC ${TORCH_INCLUDE_DIRS})

find_package(Threads REQUIRED)

target_link_libraries(Predictor
  -Wl,-s
  -Wl,--gc-sections
  -Wl,--whole-archive
  ${TORCH_LIBRARIES}
  -Wl,--no-whole-archive
  Threads::Threads
  ${CMAKE_DL_LIBS}
)
```

----------------------------------------

TITLE: Implementing Multi-Headed Attention with First-Class Dimensions in Python
DESCRIPTION: This snippet shows how to implement multi-headed attention using first-class dimensions in PyTorch. It demonstrates reshaping inputs, batching over attention heads, and using matrix products for attention scores.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
from torchdim import softmax
def multiheadattention(q, k, v, num_attention_heads, dropout_prob, use_positional_embedding):
    batch, query_sequence, key_sequence, heads, features = dims(5)
    heads.size = num_attention_heads

    # binding dimensions, and unflattening the heads from the feature dimension
    q = q[batch, query_sequence, [heads, features]]
    k = k[batch, key_sequence, [heads, features]]
    v = v[batch, key_sequence, [heads, features]]

    # einsum-style operators to calculate scores,
    attention_scores = (q*k).sum(features) * (features.size ** -0.5)

    # use first-class dim to specify dimension for softmax
    attention_probs = softmax(attention_scores, dim=key_sequence)

    # dropout work pointwise, following Rule #1
    attention_probs = torch.nn.functional.dropout(attention_probs, p=dropout_prob)

    # another matrix product
    context_layer = (attention_probs*v).sum(key_sequence)

    # flatten heads back into features
    return context_layer.order(batch, query_sequence, [heads, features])
```

----------------------------------------

TITLE: Layer Normalization Operations in PyTorch
DESCRIPTION: This snippet shows layer normalization operations and their backward passes. It includes operations on tensors with different shapes and strides.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
Operator: aten.native_layer_norm.default
cnt: 8, ((T([64, 28, 28, 192], f16, stride=(150528, 28, 1, 784)), [192], T([192], f16), T([192], f16), 1e-05), {})
cnt: 28, ((T([64, 14, 14, 384], f16, stride=(75264, 14, 1, 196)), [384], T([384], f16), T([384], f16), 1e-05), {})
cnt: 3, ((T([64, 197, 384], f16), [384], T([384], f16), T([384], f16), 1e-05), {})
cnt: 2, ((T([64, 1, 384], f16), [384], T([384], f16), T([384], f16), 1e-05), {})

Operator: aten.native_layer_norm_backward.default
cnt: 3, ((T([64, 197, 384], f16), T([64, 197, 384], f16), [384], T([64, 197, 1], f32), T([64, 197, 1], f32), T([384], f16), T([384], f16), [True, True, True]), {})
cnt: 2, ((T([64, 1, 384], f16), T([64, 1, 384], f16), [384], T([64, 1, 1], f32), T([64, 1, 1], f32), T([384], f16), T([384], f16), [True, True, True]), {})
cnt: 28, ((T([64, 14, 14, 384], f16), T([64, 14, 14, 384], f16, stride=(75264, 14, 1, 196)), [384], T([64, 14, 14, 1], f32), T([64, 14, 14, 1], f32), T([384], f16), T([384], f16), [True, True, True]), {})
cnt: 8, ((T([64, 28, 28, 192], f16), T([64, 28, 28, 192], f16, stride=(150528, 28, 1, 784)), [192], T([64, 28, 28, 1], f32), T([64, 28, 28, 1], f32), T([192], f16), T([192], f16), [True, True, True]), {})
```

----------------------------------------

TITLE: Convolution Operation Parameters - PyTorch
DESCRIPTION: Collection of convolution operation specifications with tensor shapes, strides, padding and other parameters in PyTorch's internal format. Each entry shows input tensor, weight tensor, bias tensor (if any), stride, padding, dilation, groups and other configuration parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
((T([1, 80, 40, 40], f16), T([480, 80, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

----------------------------------------

TITLE: Executing aten.clone operation in PyTorch
DESCRIPTION: Utilizes PyTorch Aten to clone tensors, preserving data type and shape. Input is the tensor to be cloned. Output is a new tensor identical to the input. Make sure to have PyTorch installed.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
Operator: aten.clone.default
cnt: 1, ((T([128, 3, 224, 224], f16),), {})
```

----------------------------------------

TITLE: Copying Tensors with aten.copy_ in PyTorch
DESCRIPTION: The aten.copy_.default operator facilitates in-place copying of tensor data. It is indispensable when one needs to transfer data between tensors of the same shape and data type efficiently, without introducing new allocations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/nvidia_deeprecommender_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
Operator: aten.copy_.default
cnt: 1, ((T([256, 197951], f16), T([256, 197951], f16)), {})
```

----------------------------------------

TITLE: Analyzing PyTorch Tensor Cloning Operations
DESCRIPTION: This snippet shows the usage of the aten.clone.default operator for creating copies of tensors with various shapes and data types.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
Operator: aten.clone.default
cnt: 2, ((T([3, 3, 512, 512], f16),), {})
cnt: 1, ((T([3, 1, 512, 512], f16),), {})
cnt: 1, ((T([3, 4, 512, 512], f16),), {})
```

----------------------------------------

TITLE: Backward Pass Stream Semantics - BC Note Unsafe Pattern Post-1.9 - PyTorch Python
DESCRIPTION: This snippet repeats the unsafe pattern (originally safe in PyTorch 1.9 and earlier when using the default stream) where `loss.backward()` runs on a custom stream `s`, but gradients are used immediately outside that stream context on the default stream without synchronization. This is now explicitly unsafe and requires synchronization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_21

LANGUAGE: Python
CODE:
```
with torch.cuda.stream(s):
    loss.backward()
use grads
```

----------------------------------------

TITLE: Diagnosing Graph Breaks with torch._dynamo.explain
DESCRIPTION: Example showing how to use dynamo.explain() to identify and analyze graph breaks in a PyTorch function. The explanation output includes graph count, break reasons, and operation details.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch
import torch._dynamo as dynamo
def toy_example(a, b):
    x = a / (torch.abs(a) + 1)
    print("woo")
    if b.sum() < 0:
        b = b * -1
    return x * b
explanation = dynamo.explain(toy_example)(torch.randn(10), torch.randn(10))
print(explanation)
```

----------------------------------------

TITLE: Creating a One-Dimensional Tensor with PyTorch C++
DESCRIPTION: This example demonstrates how to create a one-dimensional tensor (vector) with 5 components, all set to 1, using the ones() factory function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_1

LANGUAGE: cpp
CODE:
```
torch::Tensor tensor = torch::ones(5);
```

----------------------------------------

TITLE: Tuning Intra-op Threads for Matrix Multiplication in Python
DESCRIPTION: Illustrates how to measure the performance impact of varying the number of intra-op threads for a PyTorch operation (matrix multiplication). It iterates through different thread counts, sets the number of threads using `torch.set_num_threads(t)`, and times the execution of `torch.mm` using `timeit.timeit`. This script helps in finding the optimal number of threads for CPU-bound operations by measuring runtimes. Dependencies include the `torch` and `timeit` libraries.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cpu_threading_torchscript_inference.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import timeit
runtimes = []
threads = [1] + [t for t in range(2, 49, 2)]
for t in threads:
    torch.set_num_threads(t)
    r = timeit.timeit(setup = "import torch; x = torch.randn(1024, 1024); y = torch.randn(1024, 1024)", stmt="torch.mm(x, y)", number=100)
    runtimes.append(r)
# ... plotting (threads, runtimes) ...
```

----------------------------------------

TITLE: Creating Named Tensors Using Factory Functions in PyTorch
DESCRIPTION: Demonstrates how to create a tensor with named dimensions using the torch.zeros factory function. The 'names' parameter associates names 'N' and 'C' with the first and second dimensions respectively.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/name_inference.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> torch.zeros(2, 3, names=('N', 'C'))
tensor([[0., 0., 0.],
        [0., 0., 0.]], names=('N', 'C'))
```

----------------------------------------

TITLE: Transforming Modules using PyTorch FX in Python
DESCRIPTION: This function illustrates the process of transforming a neural network module in PyTorch FX by acquiring its graph, modifying it, and returning a new module. Dependencies include the 'torch' and 'torch.fx' packages. The main parameter is 'm', a torch.nn.Module, and the function returns a modified module with the same interface. This process allows for custom tracing behaviors and is constrained by input and output types being torch.nn.Module.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
import torch.fx

def transform(m: nn.Module,
              tracer_class : type = torch.fx.Tracer) -> torch.nn.Module:
    # Step 1: Acquire a Graph representing the code in `m`

    # NOTE: torch.fx.symbolic_trace is a wrapper around a call to
    # fx.Tracer.trace and constructing a GraphModule. We'll
    # split that out in our transform to allow the caller to
    # customize tracing behavior.
    graph : torch.fx.Graph = tracer_class().trace(m)

    # Step 2: Modify this Graph or create a new one
    graph = ...

    # Step 3: Construct a Module to return
    return torch.fx.GraphModule(m, graph)
```

----------------------------------------

TITLE: Implementing BackendConfig and Pattern Configs - PyTorch Python
DESCRIPTION: This snippet demonstrates the core structure for defining backend configurations in PyTorch quantization. It shows how to specify data type configurations (`DTypeConfig`), define custom fusion functions (`fuse_conv2d_relu`), configure individual operator patterns (`BackendPatternConfig`) with observation types, dtype configs, root/qat/reference modules, and fuser methods, and finally aggregate these into a `BackendConfig` instance. This setup is essential for tailoring quantization behavior for different hardware backends.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/backend_config/README.md#_snippet_0

LANGUAGE: python
CODE:
```
import torch
from torch.ao.quantization.backend_config import (
    BackendConfig,
    BackendPatternConfig,
    DTypeConfig,
    ObservationType,
)

weighted_int8_dtype_config = DTypeConfig(
    input_dtype=torch.quint8,
    output_dtype=torch.quint8,
    weight_dtype=torch.qint8,
    bias_dtype=torch.float)

def fuse_conv2d_relu(is_qat, conv, relu):
    """Return a fused ConvReLU2d from individual conv and relu modules."""
    return torch.ao.nn.intrinsic.ConvReLU2d(conv, relu)

# For quantizing Linear
linear_config = BackendPatternConfig(torch.nn.Linear) \
    .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT) \
    .add_dtype_config(weighted_int8_dtype_config) \
    .set_root_module(torch.nn.Linear) \
    .set_qat_module(torch.ao.nn.qat.Linear) \
    .set_reference_quantized_module(torch.ao.nn.quantized.reference.Linear)

# For fusing Conv2d + ReLU into ConvReLU2d
conv_relu_config = BackendPatternConfig((torch.nn.Conv2d, torch.nn.ReLU)) \
    .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT) \
    .add_dtype_config(weighted_int8_dtype_config) \
    .set_fused_module(torch.ao.nn.intrinsic.ConvReLU2d) \
    .set_fuser_method(fuse_conv2d_relu)

# For quantizing ConvReLU2d
fused_conv_relu_config = BackendPatternConfig(torch.ao.nn.intrinsic.ConvReLU2d) \
    .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT) \
    .add_dtype_config(weighted_int8_dtype_config) \
    .set_root_module(torch.nn.Conv2d) \
    .set_qat_module(torch.ao.nn.intrinsic.qat.ConvReLU2d) \
    .set_reference_quantized_module(torch.ao.nn.quantized.reference.Conv2d)

backend_config = BackendConfig("my_backend") \
    .set_backend_pattern_config(linear_config) \
    .set_backend_pattern_config(conv_relu_config) \
    .set_backend_pattern_config(fused_conv_relu_config)
```

----------------------------------------

TITLE: Computing Jacobians using torch.func.jacrev and functional_call in Python
DESCRIPTION: Demonstrates the PyTorch 2.0+ method for Jacobian computation using `torch.func`. Parameters are obtained via `model.named_parameters()`. `torch.func.jacrev` is applied directly to `torch.func.functional_call`. `argnums=1` specifies that the Jacobian should be computed with respect to the second argument passed to `functional_call` (the `params` dictionary), not the default first argument (`model`). Requires `torch` and functions from `torch.func`. This replaces the `functorch.jacrev` pattern shown previously.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.migrating.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
# ------------------------------------
# using torch.func (as of PyTorch 2.0)
# ------------------------------------
import torch
from torch.func import jacrev, functional_call
inputs = torch.randn(64, 3)
model = torch.nn.Linear(3, 3)

params = dict(model.named_parameters())
# jacrev computes jacobians of argnums=0 by default.
# We set it to 1 to compute jacobians of params
jacobians = jacrev(functional_call, argnums=1)(model, params, (inputs,))
```

----------------------------------------

TITLE: Running Training Processes Using Bash
DESCRIPTION: Execute a training script with multiple processes using a Bash command. This snippet illustrates how to initiate the Python script 'main.py' with 4 subprocesses intended for CPU training. This requires a Python environment with PyTorch and expected inputs are command-line arguments for specifying the number of processes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/multiprocessing.rst#2025-04-22_snippet_1

LANGUAGE: Bash
CODE:
```
python main.py --num-processes 4
```

----------------------------------------

TITLE: Registering Overloaded PyTorch Operators in C++
DESCRIPTION: This snippet demonstrates how to register multiple kernels under the same operator name using overloads. It defines two CPU kernels with different signatures and registers them as separate overloads of the same operator.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_10

LANGUAGE: cpp
CODE:
```
namespace {
  Tensor my_kernel_cpu_1(const Tensor& a) {...}
  Tensor my_kernel_cpu_2(const Tensor& a, const Tensor& b) {...}
}

static auto registry = torch::RegisterOperators()
   .op("my_namespace::my_op.overload1(Tensor a) -> Tensor",
       torch::RegisterOperators::options()
         .kernel<decltype(my_kernel_cpu_1), &my_kernel_cpu>(CPU()))
   .op("my_namespace::my_op.overload2(Tensor a, Tensor b) -> Tensor",
       torch::RegisterOperators::options()
         .kernel<decltype(my_kernel_cpu_2), &my_kernel_cpu>(CPU()));
```

----------------------------------------

TITLE: Creating a DTensor from a logical torch.Tensor
DESCRIPTION: Uses the distribute_tensor function to create a DTensor from a logical torch.Tensor on each rank. This is useful for sharding model parameters, buffers, and inputs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.tensor.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
distribute_tensor(tensor, device_mesh, placements)
```

----------------------------------------

TITLE: Profiling Triton and CPU Kernels in PyTorch
DESCRIPTION: Explains three key components visible in profiling traces: CPU-side events (prefixed with 'triton_'), kernel launches (cuLaunchKernel), and GPU-side events. Covers both Inductor-generated and non-Inductor Triton kernels, as well as CPU kernels and performance considerations around launch overhead.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_profiling_torch_compile.rst#2025-04-22_snippet_4



----------------------------------------

TITLE: PyTorch Convolution Operations
DESCRIPTION: Forward convolution operations with various kernel sizes, strides and channels using float16 precision
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
aten.convolution.default(T([128, 3, 224, 224], f16), T([16, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1)
```

----------------------------------------

TITLE: Calling aten.copy_.default (Python)
DESCRIPTION: Copies the elements of one tensor to another in-place. This operation modifies the destination tensor. Examples show copying int64 tensors of different shapes and strides.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_11

LANGUAGE: Python
CODE:
```
((T([8, 128], i64), T([8, 128], i64)), {})
```

LANGUAGE: Python
CODE:
```
((T([8, 127], i64, stride=(128, 1)), T([8, 127], i64)), {})
```

LANGUAGE: Python
CODE:
```
((T([8], i64, stride=(128,)), T([8], i64)), {})
```

----------------------------------------

TITLE: Demonstrating Storage Bloat from Shared Tensor Storage - PyTorch - Python
DESCRIPTION: Highlights an issue that can occur when saving a view of a larger tensor: torch.save preserves the original storage, so the resulting file contains all values from the shared storage, not just the elements in the view. Inputs are a large tensor and a view; output demonstrates how loading the view yields a storage size equal to the original large tensor. Useful for advanced users considering file size and efficiency when saving tensor subsets. No additional dependencies required except torch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> large = torch.arange(1, 1000)
>>> small = large[0:5]
>>> torch.save(small, 'small.pt')
>>> loaded_small = torch.load('small.pt')
>>> loaded_small.storage().size()
999
```

----------------------------------------

TITLE: Iterating Through Named Children of a Module
DESCRIPTION: Demonstrates iterating through the immediate submodules (children) of the `Net` instance using `named_children()`. This method yields tuples containing the attribute name assigned to the child module (`'l0'`, `'l1'`) and the child module instance itself.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
net = Net()
for child in net.named_children():
  print(child)
: ('l0', MyLinear())
('l1', MyLinear())
```

----------------------------------------

TITLE: Compiling PyTorch Model with DistributedDataParallel
DESCRIPTION: Example of applying torch.compile with DistributedDataParallel wrapper for distributed training
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
# DistributedDataParallel
model = ...
opt_model = torch.compile(model)
model_ddp = DistributedDataParallel(opt_model, ...)

for _ in range(N_ITERS):
    inp = ...
    out = model_ddp(inp)
```

----------------------------------------

TITLE: Executing aten.avg_pool2d operation in PyTorch
DESCRIPTION: Averages two-dimensional pooling on a tensor via the Aten backend using specified pooling window, stride, and padding, with outputs a reduced tensor. Dependencies include PyTorch's tensor framework, input tensor shape, and pooling parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
Operator: aten.avg_pool2d.default
cnt: 1, ((T([128, 256, 56, 56], f16), [2, 2], [2, 2], [0, 0], True, False), {})
```

----------------------------------------

TITLE: Creating and Inspecting Multi-dimensional Sparse COO Tensor in PyTorch
DESCRIPTION: This snippet shows how to create a multi-dimensional sparse COO tensor and inspect its properties such as sparsity, layout, and dimensions. It also demonstrates accessing COO format data.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_13

LANGUAGE: Python
CODE:
```
>>> i = [[0, 1, 1],
         [2, 0, 2]]
>>> v =  [[3, 4], [5, 6], [7, 8]]
>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))

>>> isinstance(s, torch.Tensor)
True
>>> s.is_sparse
True
>>> s.layout == torch.sparse_coo
True

>>> s.sparse_dim(), s.dense_dim()
(2, 1)

>>> s._indices()
tensor([[0, 1, 1],
        [2, 0, 2]])
```

----------------------------------------

TITLE: Batch Normalization Operations in PyTorch
DESCRIPTION: This snippet shows batch normalization operations at different stages of the network. These operations normalize activations using running statistics (mean and variance) and learnable parameters (scale and shift), improving training stability.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
Operator: aten.native_batch_norm.default
cnt: 1, ((T([96, 64, 64, 64], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})
cnt: 5, ((T([96, 64, 32, 32], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})
cnt: 5, ((T([96, 128, 16, 16], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), False, 0.1, 1e-05), {})
cnt: 5, ((T([96, 256, 8, 8], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), False, 0.1, 1e-05), {})
cnt: 5, ((T([96, 512, 4, 4], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), False, 0.1, 1e-05), {})
```

----------------------------------------

TITLE: Using torch.func.grad with torch.compile
DESCRIPTION: Demonstrates compiling a gradient computation using torch.func.grad wrapped in torch.compile for optimization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import torch

def wrapper_fn(x):
    return torch.func.grad(lambda x: x.sin().sum())(x)

x = torch.randn(3, 3, 3)
grad_x = torch.compile(wrapper_fn)(x)
```

----------------------------------------

TITLE: Implementing PyTorch Module Hooks
DESCRIPTION: Demonstrates the implementation of forward and backward hooks in PyTorch modules for custom behavior during forward and backward passes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
def forward_pre_hook(m, inputs):
    input = inputs[0]
    return input + 1.

def forward_hook(m, inputs, output):
    return output + inputs[0]

def backward_hook(m, grad_inputs, grad_outputs):
    new_grad_inputs = [torch.ones_like(gi) * 42. for gi in grad_inputs]
    return new_grad_inputs

m = nn.Linear(3, 3)
x = torch.randn(2, 3, requires_grad=True)

forward_pre_hook_handle = m.register_forward_pre_hook(forward_pre_hook)
forward_hook_handle = m.register_forward_hook(forward_hook)
m.register_full_backward_hook(backward_hook)
```

----------------------------------------

TITLE: Computing Jacobian over Model Parameters in PyTorch
DESCRIPTION: This example shows how to compute the Jacobian with respect to the parameters of a PyTorch neural network model using the functional_call utility. The function f takes the model's parameters and inputs, and computes the forward pass. Dependencies include torch.func.functional_call, and the code uses torch.nn.Linear to define the model.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.api.rst#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
model = torch.nn.Linear(3, 3)

def f(params, x):
    return torch.func.functional_call(model, params, x)

x = torch.randn(3)
jacobian = jacrev(f)(dict(model.named_parameters()), x)
```

----------------------------------------

TITLE: C++ Neural Network Components
DESCRIPTION: Core C++ components for neural network development in PyTorch, including model architecture, functional operations, and optimization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cpp_index.rst#2025-04-22_snippet_1

LANGUAGE: C++
CODE:
```
torch::nn
torch::nn::functional
torch::optim
```

----------------------------------------

TITLE: Performing Embedding Lookup - PyTorch Aten
DESCRIPTION: Looks up embeddings for given indices from an embedding table (weight tensor). This internal operator is fundamental for processing categorical inputs like words or tokens. It takes the embedding table (weight), an index tensor, and optionally a padding index.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BartForConditionalGeneration_training.txt#_snippet_12

LANGUAGE: Python
CODE:
```
import torch

weight = torch.randn(50265, 1024, dtype=torch.float16)
indices = torch.randint(0, 50265, (2, 1024), dtype=torch.int64)
embeddings = torch.embedding(weight, indices, padding_idx=1)
```

----------------------------------------

TITLE: Defining PyTorch Tensor Operations
DESCRIPTION: The code snippets define tensor operations in PyTorch using half-precision floating-point (f16) format. Each operation specifies tensor dimensions, types, and additional parameters, such as stride and convolution settings. These operations likely correspond to configurations in a neural network model.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 52, 1, 1], f16), T([624, 52, 1, 1], f16), T([624], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 624, 14, 14], f16), T([160, 624, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

LANGUAGE: Python
CODE:
```
cnt: 6, ((T([64, 80, 14, 14], f16, stride=(31360, 196, 14, 1)), T([240, 80, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([120, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 120), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([120, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 120), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([120, 1, 7, 7], f16), None, [1, 1], [3, 3], [1, 1], False, [0, 0], 120), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([120, 1, 9, 9], f16), None, [1, 1], [4, 4], [1, 1], False, [0, 0], 120), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 480, 1, 1], f16), T([80, 480, 1, 1], f16), T([80], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 80, 1, 1], f16), T([480, 80, 1, 1], f16), T([480], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

LANGUAGE: Python
CODE:
```
cnt: 6, ((T([64, 240, 14, 14], f16, stride=(94080, 196, 14, 1)), T([80, 240, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 160, 14, 14], f16), T([960, 160, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 240, 14, 14], f16, stride=(188160, 196, 14, 1)), T([240, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 240), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 240, 14, 14], f16, stride=(188160, 196, 14, 1)), T([240, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 240), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 240, 14, 14], f16, stride=(188160, 196, 14, 1)), T([240, 1, 7, 7], f16), None, [2, 2], [3, 3], [1, 1], False, [0, 0], 240), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 240, 14, 14], f16, stride=(188160, 196, 14, 1)), T([240, 1, 9, 9], f16), None, [2, 2], [4, 4], [1, 1], False, [0, 0], 240), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 960, 1, 1], f16), T([80, 960, 1, 1], f16), T([80], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 80, 1, 1], f16), T([960, 80, 1, 1], f16), T([960], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 960, 7, 7], f16), T([264, 960, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 264, 7, 7], f16), T([1584, 264, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([396, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 396), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([396, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 396), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([396, 1, 7, 7], f16), None, [1, 1], [3, 3], [1, 1], False, [0, 0], 396), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([396, 1, 9, 9], f16), None, [1, 1], [4, 4], [1, 1], False, [0, 0], 396), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 1584, 1, 1], f16), T([132, 1584, 1, 1], f16), T([132], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 132, 1, 1], f16), T([1584, 132, 1, 1], f16), T([1584], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

LANGUAGE: Python
CODE:
```
cnt: 6, ((T([64, 792, 7, 7], f16, stride=(77616, 49, 7, 1)), T([132, 792, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 264, 7, 7], f16), T([1536, 264, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

LANGUAGE: Python
CODE:
```
Operator: aten.convolution_backward.default
cnt: 1, ((T([64, 1536, 7, 7], f16), T([64, 264, 7, 7], f16), T([1536, 264, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 6, ((T([64, 132, 7, 7], f16, stride=(12936, 49, 7, 1)), T([64, 792, 7, 7], f16, stride=(77616, 49, 7, 1)), T([132, 792, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 1584, 1, 1], f16), T([64, 132, 1, 1], f16), T([1584, 132, 1, 1], f16), [1584], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 132, 1, 1], f16), T([64, 1584, 1, 1], f16), T([132, 1584, 1, 1], f16), [132], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([396, 1, 9, 9], f16), [0], [1, 1], [4, 4], [1, 1], False, [0, 0], 396, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([396, 1, 7, 7], f16), [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 396, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([396, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 396, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([396, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 396, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 1584, 7, 7], f16), T([64, 264, 7, 7], f16), T([1584, 264, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 264, 7, 7], f16), T([64, 960, 7, 7], f16), T([264, 960, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 960, 1, 1], f16), T([64, 80, 1, 1], f16), T([960, 80, 1, 1], f16), [960], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 80, 1, 1], f16), T([64, 960, 1, 1], f16), T([80, 960, 1, 1], f16), [80], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 240, 7, 7], f16, stride=(47040, 49, 7, 1)), T([64, 240, 14, 14], f16, stride=(188160, 196, 14, 1)), T([240, 1, 9, 9], f16), [0], [2, 2], [4, 4], [1, 1], False, [0, 0], 240, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 240, 7, 7], f16, stride=(47040, 49, 7, 1)), T([64, 240, 14, 14], f16, stride=(188160, 196, 14, 1)), T([240, 1, 7, 7], f16), [0], [2, 2], [3, 3], [1, 1], False, [0, 0], 240, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 240, 7, 7], f16, stride=(47040, 49, 7, 1)), T([64, 240, 14, 14], f16, stride=(188160, 196, 14, 1)), T([240, 1, 5, 5], f16), [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 240, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 240, 7, 7], f16, stride=(47040, 49, 7, 1)), T([64, 240, 14, 14], f16, stride=(188160, 196, 14, 1)), T([240, 1, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 240, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 960, 14, 14], f16), T([64, 160, 14, 14], f16), T([960, 160, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 6, ((T([64, 80, 14, 14], f16, stride=(31360, 196, 14, 1)), T([64, 240, 14, 14], f16, stride=(94080, 196, 14, 1)), T([80, 240, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 480, 1, 1], f16), T([64, 80, 1, 1], f16), T([480, 80, 1, 1], f16), [480], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 80, 1, 1], f16), T([64, 480, 1, 1], f16), T([80, 480, 1, 1], f16), [80], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([120, 1, 9, 9], f16), [0], [1, 1], [4, 4], [1, 1], False, [0, 0], 120, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([120, 1, 7, 7], f16), [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 120, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([120, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 120, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([120, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 120, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 6, ((T([64, 240, 14, 14], f16, stride=(94080, 196, 14, 1)), T([64, 80, 14, 14], f16, stride=(31360, 196, 14, 1)), T([240, 80, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 160, 14, 14], f16), T([64, 624, 14, 14], f16), T([160, 624, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 624, 1, 1], f16), T([64, 52, 1, 1], f16), T([624, 52, 1, 1], f16), [624], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 52, 1, 1], f16), T([64, 624, 1, 1], f16), T([52, 624, 1, 1], f16), [52], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 624, 14, 14], f16), T([64, 624, 14, 14], f16), T([624, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 624, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 624, 14, 14], f16), T([64, 104, 14, 14], f16), T([624, 104, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 6, ((T([64, 52, 14, 14], f16, stride=(20384, 196, 14, 1)), T([64, 312, 14, 14], f16, stride=(122304, 196, 14, 1)), T([52, 312, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 624, 1, 1], f16), T([64, 26, 1, 1], f16), T([624, 26, 1, 1], f16), [624], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 26, 1, 1], f16), T([64, 624, 1, 1], f16), T([26, 624, 1, 1], f16), [26], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
```

----------------------------------------

TITLE: In-place Tensor Addition in PyTorch ResNet Skip Connections
DESCRIPTION: This snippet shows in-place tensor addition operations (add_) used for residual/skip connections in a ResNet model. The operations work with half-precision (f16) tensors of various sizes that correspond to different network stages.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
Operator: aten.add_.Tensor
cnt: 3, ((T([4, 256, 296, 304], f16), T([4, 256, 296, 304], f16)), {})
cnt: 4, ((T([4, 512, 148, 152], f16), T([4, 512, 148, 152], f16)), {})
cnt: 6, ((T([4, 1024, 74, 76], f16), T([4, 1024, 74, 76], f16)), {})
cnt: 3, ((T([4, 2048, 37, 38], f16), T([4, 2048, 37, 38], f16)), {})
```

----------------------------------------

TITLE: Demonstrating Tensor Strides in PyTorch
DESCRIPTION: This code snippet demonstrates how to create a tensor and inspect its strides. It also shows how the strides change when the tensor is transposed. This illustrates the concept of strided memory layout in PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])
>>> x.stride()
(5, 1)

>>> x.t().stride()
(1, 5)
```

----------------------------------------

TITLE: Executing Batch Norm Backward Pass (aten.native_batch_norm_backward.default) in PyTorch (Python)
DESCRIPTION: This operator snippet covers backward propagation through a batch normalization layer, as used for gradient calculation during training. It includes input, grad_output, parameter tensors (gamma, beta), running stats, Booleans to indicate flags, and reduction axes. Usage presumes prior batch norm context and uses both float16 and float32 types, requiring PyTorch's autograd engine.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mnasnet1_0_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
Operator: aten.native_batch_norm_backward.default
cnt: 1, ((T([32, 1280, 7, 7], f16), T([32, 1280, 7, 7], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f32), T([1280], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([32, 320, 7, 7], f16), T([32, 320, 7, 7], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f32), T([320], f32), False, 1e-05, [True, True, True]), {})
cnt: 8, ((T([32, 1152, 7, 7], f16), T([32, 1152, 7, 7], f16), T([1152], f16), T([1152], f16), T([1152], f16), T([1152], f32), T([1152], f32), False, 1e-05, [True, True, True]), {})
cnt: 4, ((T([32, 192, 7, 7], f16), T([32, 192, 7, 7], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([32, 576, 7, 7], f16), T([32, 576, 7, 7], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f32), T([576], f32), False, 1e-05, [True, True, True]), {})
cnt: 3, ((T([32, 576, 14, 14], f16), T([32, 576, 14, 14], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f32), T([576], f32), False, 1e-05, [True, True, True]), {})
cnt: 2, ((T([32, 96, 14, 14], f16), T([32, 96, 14, 14], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), False, 1e-05, [True, True, True]), {})
cnt: 6, ((T([32, 480, 14, 14], f16), T([32, 480, 14, 14], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f32), T([480], f32), False, 1e-05, [True, True, True]), {})
cnt: 3, ((T([32, 80, 14, 14], f16), T([32, 80, 14, 14], f16), T([80], f16), T([80], f16), T([80], f16), T([80], f32), T([80], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([32, 240, 14, 14], f16), T([32, 240, 14, 14], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f32), T([240], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([32, 240, 28, 28], f16), T([32, 240, 28, 28], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f32), T([240], f32), False, 1e-05, [True, True, True]), {})
cnt: 3, ((T([32, 40, 28, 28], f16), T([32, 40, 28, 28], f16), T([40], f16), T([40], f16), T([40], f16), T([40], f32), T([40], f32), False, 1e-05, [True, True, True]), {})
cnt: 4, ((T([32, 120, 28, 28], f16), T([32, 120, 28, 28], f16), T([120], f16), T([120], f16), T([120], f16), T([120], f32), T([120], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([32, 72, 28, 28], f16), T([32, 72, 28, 28], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f32), T([72], f32), False, 1e-05, [True, True, True]), {})
cnt: 5, ((T([32, 72, 56, 56], f16), T([32, 72, 56, 56], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f32), T([72], f32), False, 1e-05, [True, True, True]), {})
cnt: 3, ((T([32, 24, 56, 56], f16), T([32, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f32), T([24], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([32, 48, 56, 56], f16), T([32, 48, 56, 56], f16), T([48], f16), T([48], f16), T([48], f16), T([48], f32), T([48], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([32, 48, 112, 112], f16), T([32, 48, 112, 112], f16), T([48], f16), T([48], f16), T([48], f16), T([48], f32), T([48], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([32, 16, 112, 112], f16), T([32, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f32), T([16], f32), False, 1e-05, [True, True, True]), {})
cnt: 2, ((T([32, 32, 112, 112], f16), T([32, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), False, 1e-05, [True, True, True]), {})
```

----------------------------------------

TITLE: DataParallel with PyTorch Autocast
DESCRIPTION: Shows how to use autocast with DataParallel for multi-GPU training within a single process.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/amp_examples.rst#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
model = MyModel()
dp_model = nn.DataParallel(model)

with autocast(device_type='cuda', dtype=torch.float16):
    output = dp_model(input)
    loss = loss_fn(output)
```

----------------------------------------

TITLE: Repeating Tensor Elements with ATen
DESCRIPTION: This operator replicates the elements of a tensor along specified dimensions, expanding its size. Requires an input tensor, e.g., [16, 5, 1, 1], and a tuple of repeat counts like [1, 1, 128, 128]. Outputs are expanded tensors used for broadcasting in model operations, relieving manual tensor reshaping constraints.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_13

LANGUAGE: Python
CODE:
```
Operator: aten.repeat.default
cnt: 1, ((T([16, 5, 1, 1], f16), [1, 1, 128, 128]), {})
cnt: 8, ((T([64], f16), [16]), {})
cnt: 8, ((T([128], f16), [16]), {})
cnt: 52, ((T([256], f16), [16]), {})
```

----------------------------------------

TITLE: Vectorizing a Function Using vmap - Pure Function Example - PyTorch - Python
DESCRIPTION: This snippet illustrates how vmap in torch.func is conceptually similar to manually mapping a function over input tensors in a for-loop, provided the function has no side effects. It demonstrates using torch.stack and Python's list comprehension to achieve the same behavior as vmap, highlighting that under the hood, vmap(f)(x) ~ torch.stack([f(x_i) for x_i in x.unbind(0)]). Required dependencies: PyTorch. Functionality focuses on batch mapping of computations over tensor entries.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
torch.stack([f(x_i) for x_i in x.unbind(0)])
```

----------------------------------------

TITLE: Initializing TensorPipe RPC Backend in PyTorch
DESCRIPTION: Example demonstrating how to initialize the TensorPipe RPC backend in PyTorch with custom options like worker threads and timeout settings. This is used for setting up distributed training environments.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> import os
>>> from torch.distributed import rpc
>>> os.environ['MASTER_ADDR'] = 'localhost'
>>> os.environ['MASTER_PORT'] = '29500'
>>>
>>> rpc.init_rpc(
>>>     "worker1",
>>>     rank=0,
>>>     world_size=2,
>>>     rpc_backend_options=rpc.TensorPipeRpcBackendOptions(
>>>         num_worker_threads=8,
>>>         rpc_timeout=20 # 20 second timeout
>>>     )
>>> )
>>>
>>> # omitting init_rpc invocation on worker2
```

----------------------------------------

TITLE: Computing Gradients through NumPy Code
DESCRIPTION: Shows how to compute gradients through NumPy code using torch.compile and wrap_numpy decorator with backpropagation support.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
@torch.compile(fullgraph=True)
@torch.compiler.wrap_numpy
def numpy_fn(X, Y):
    return np.mean(np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1)))

X = torch.randn(1024, 64, device="cuda", requires_grad=True)
Y = torch.randn(1024, 64, device="cuda")
Z = numpy_fn(X, Y)
assert isinstance(Z, torch.Tensor)
Z.backward()
print(X.grad)
```

----------------------------------------

TITLE: Loss Functions and Masking Utilities in PyTorch (Python)
DESCRIPTION: Usage patterns for negative log-likelihood (nll_loss), scalar comparisons (ne, rsub), and slicing backward are shown, designed for tasks with masking, class indexing, and gradient flow. Inputs are either probability or label tensors; some take ignore-index or reduction arguments. Slicing/cumsum operate on high-dimensional tensors for sequential models.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
Operator: aten.ne.Scalar
cnt: 1, ((T([4, 128], i64), 0), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.nll_loss_backward.default
cnt: 1, ((T([], f16), T([508, 30522], f16), T([508], i64), None, 1, -100, T([], f16)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.nll_loss_forward.default
cnt: 1, ((T([508, 30522], f16), T([508], i64), None, 1, -100), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.rsub.Scalar
cnt: 1, ((T([4, 1, 1, 128], f16), 1.0), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.slice_backward.default
cnt: 1, ((T([4, 127, 30522], f16), [4, 127, 30522], 2, 0, 9223372036854775807, 1), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.slice_backward.default
cnt: 1, ((T([4, 127, 30522], f16), [4, 128, 30522], 1, 0, -1, 1), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.slice_backward.default
cnt: 1, ((T([4, 128, 30522], f16), [4, 128, 30522], 0, 0, 9223372036854775807, 1), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.cumsum.default
cnt: 1, ((T([4, 128], i32), 1), {})
```

----------------------------------------

TITLE: Good Practice: Using Implicit Casting for Single-Element Tensors
DESCRIPTION: This snippet illustrates the recommended practice: using implicit casting for single-element tensors where possible. PyTorch allows using a single-element tensor directly in operations like `reshape` where a scalar is expected, preserving the tensor as a variable during tracing.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_6

LANGUAGE: python
CODE:
```
def forward(self, x, y):
    return x.reshape(y, -1)
```

----------------------------------------

TITLE: Basic MulConstant Autograd Function Implementation
DESCRIPTION: Implements a basic multiplication constant operation with forward and backward passes using PyTorch's Function class.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
class MulConstant(Function):
    @staticmethod
    def forward(tensor, constant):
        return tensor * constant

    @staticmethod
    def setup_context(ctx, inputs, output):
        # ctx is a context object that can be used to stash information
        # for backward computation
        tensor, constant = inputs
        ctx.constant = constant

    @staticmethod
    def backward(ctx, grad_output):
        # We return as many input gradients as there were arguments.
        # Gradients of non-Tensor arguments to forward must be None.
        return grad_output * ctx.constant, None
```

----------------------------------------

TITLE: Guards Generated for Static Shape Input in PyTorch Dynamo
DESCRIPTION: These are the guards generated by Dynamo for the first execution of the compiled function. They check specific properties of the input tensors `a` and `b`, including their dtype, device, grad status, and importantly, their exact fixed size `[4, 3]` and stride `[3, 1]`. If these conditions hold on a subsequent call, the compiled static graph can be reused.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
# Guards first call
check_tensor(L['a'], torch.float32, device=None, requires_grad=False, size=[4, 3], stride=[3, 1])
check_tensor(L['b'], torch.float32, device=None, requires_grad=False, size=[4, 3], stride=[3, 1])
```

----------------------------------------

TITLE: PyTorch Activation Functions
DESCRIPTION: LeakyReLU and ReLU activation operations with different tensor shapes and alpha values.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Super_SloMo_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
((T([6, 32, 352, 352], f16), 0.1), {})
```

----------------------------------------

TITLE: PyTorch Tensor Addition Operations
DESCRIPTION: Documents tensor addition operations with various shapes and stride patterns.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/twins_pcpvt_base_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
aten.add.Tensor((T([32, 3136, 64], f16), T([32, 3136, 64], f16)))
aten.add.Tensor((T([32, 784, 128], f16), T([32, 784, 128], f16)))
aten.add_.Tensor((T([32, 64, 56, 56], f16, stride=(200704, 1, 3584, 64)), T([32, 64, 56, 56], f16, stride=(200704, 1, 3584, 64))))
```

----------------------------------------

TITLE: Initializing Simple PyTorch Model Function
DESCRIPTION: Defines a basic PyTorch function that performs element-wise operations on four input tensors
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/aot_autograd_optimizations.ipynb#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch

def fn(a, b, c, d):
    x = a + b + c + d
    return x.cos().cos()
```

----------------------------------------

TITLE: Enforcing Deterministic cuDNN Convolution Algorithms in Python
DESCRIPTION: Specifies two ways to ensure cuDNN uses deterministic convolution algorithms, which is necessary even if benchmarking is disabled. Setting `torch.use_deterministic_algorithms(True)` enforces determinism globally for many PyTorch operations, while `torch.backends.cudnn.deterministic = True` specifically targets cuDNN convolutions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/randomness.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
torch.use_deterministic_algorithms(True)
```

LANGUAGE: python
CODE:
```
torch.backends.cudnn.deterministic = True
```

----------------------------------------

TITLE: PyTorch 2 Export Quantization Implementation
DESCRIPTION: Demonstrates implementation of PyTorch 2 export quantization with XNNPACK backend. Includes model definition, calibration setup, and quantization process.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
import torch
from torch.ao.quantization.quantize_pt2e import prepare_pt2e
from torch.export import export_for_training
from torch.ao.quantization.quantizer import (
    XNNPACKQuantizer,
    get_symmetric_quantization_config,
)

class M(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(5, 10)

    def forward(self, x):
        return self.linear(x)

float_model = M().eval()

def calibrate(model, data_loader):
    model.eval()
    with torch.no_grad():
        for image, target in data_loader:
            model(image)

m = export_for_training(m, *example_inputs).module()
quantizer = XNNPACKQuantizer().set_global(get_symmetric_quantization_config())
m = prepare_pt2e(m, quantizer)
m = convert_pt2e(m)
```

----------------------------------------

TITLE: Function using Tensor.item() under vmap (Unsupported) - PyTorch - Python
DESCRIPTION: This snippet shows a function calling .item() on a batched tensor inside a vmap transform, which is not supported because .item() squeezes the batch dimension into a scalar, breaking batching. Dependencies: PyTorch, torch.func. Limitation: Do not use .item() on tensors inside functions passed to vmap.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
def f(x):
  return x.item()

x = torch.randn(3)
vmap(f)(x)
```

----------------------------------------

TITLE: Applying FSDP2 to a PyTorch Module
DESCRIPTION: Demonstrates how to use the fully_shard function to apply Fully Sharded Data Parallelism to a PyTorch module. This function dynamically constructs a new class that subclasses the original module type and FSDPModule.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.fsdp.fully_shard.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
fully_shard(module)
```

----------------------------------------

TITLE: Performing Inplace Tensor Operations (Python)
DESCRIPTION: Illustrates how TorchScript supports inplace operations using methods ending in `_` (like `relu_()`) or the `out` parameter in functions (`torch.add`). These operations modify the Tensor directly, reflecting TorchScript's mutable Tensor semantics and allowing for memory optimization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_17

LANGUAGE: python
CODE:
```
t2.relu_() # inplace relu operator, note t is modified as well!
torch.add(t, t, out=t) # update t, without using temporary memory if possible
```

----------------------------------------

TITLE: Setting Thread Count for Optimal PyTorch Multiprocessing Performance in Python
DESCRIPTION: Demonstrates how to set the optimal number of threads for each PyTorch process using torch.set_num_threads(). The formula divides available vCPUs by the number of processes to avoid CPU oversubscription.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/multiprocessing.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
torch.set_num_threads(floor(N/M))
```

----------------------------------------

TITLE: Compiling Function with Shape-Dependent Control Flow in Python
DESCRIPTION: This snippet demonstrates a function compiled with `torch.compile(dynamic=True)` where the control flow (`if/else`) depends on a calculation involving a tensor's shape (`a.shape[0]`). Dynamo traces this condition, resulting in a guard (`2*L['a'].size()[0] >= 16`) that involves symbolic integers and is checked during execution to select the correct path or trigger a recompile if the condition's outcome changes relative to the trace.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
import torch

@torch.compile(dynamic=True)
def fn(a):
    if a.shape[0] * 2 < 16:
        return a
    else:
        return a + 1

fn(torch.randn(8))
```

----------------------------------------

TITLE: Illustrating In-place Broadcasting Constraints in Python
DESCRIPTION: Shows the limitation of in-place operations (like `add_`) regarding broadcasting. The in-place tensor cannot change its shape as a result of the broadcast. The first example succeeds because the shape of `x` matches the broadcasted result shape. The second example fails with a RuntimeError because the broadcast would require changing the shape of `x`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/broadcasting.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> x=torch.empty(5,3,4,1)
>>> y=torch.empty(3,1,1)
>>> (x.add_(y)).size()
torch.Size([5, 3, 4, 1])

# but:
>>> x=torch.empty(1,3,1)
>>> y=torch.empty(3,1,7)
>>> (x.add_(y)).size()
RuntimeError: The expanded size of the tensor (1) must match the existing size (7) at non-singleton dimension 2.
```

----------------------------------------

TITLE: Defining PyTorch CUDA Environment Variables in reStructuredText
DESCRIPTION: This snippet defines a table of PyTorch-specific CUDA environment variables using reStructuredText syntax. It includes variables for memory caching, allocation configuration, CUDA checks, cuDNN settings, and NCCL behavior.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cuda_environment_variables.rst#2025-04-22_snippet_0

LANGUAGE: reStructuredText
CODE:
```
.. list-table::
  :header-rows: 1

  * - Variable
    - Description
  * - ``PYTORCH_NO_CUDA_MEMORY_CACHING``
    - If set to ``1``, disables caching of memory allocations in CUDA. This can be useful for debugging.
  * - ``PYTORCH_CUDA_ALLOC_CONF``
    - For a more in depth explanation of this environment variable, see :ref:`cuda-memory-management`.
  * - ``PYTORCH_NVML_BASED_CUDA_CHECK``
    - If set to ``1``, before importing PyTorch modules that check if CUDA is available, PyTorch will use NVML to check if the CUDA driver is functional instead of using the CUDA runtime. This can be helpful if forked processes fail with a CUDA initialization error.
  * - ``TORCH_CUDNN_V8_API_LRU_CACHE_LIMIT``
    - The cache limit for the cuDNN v8 API. This is used to limit the memory used by the cuDNN v8 API. The default value is 10000, which roughly corresponds to 2GiB assuming 200KiB per ExecutionPlan. Set to ``0`` for no limit or a negative value for no caching.
  * - ``TORCH_CUDNN_V8_API_DISABLED``
    - If set to ``1``, disables the cuDNN v8 API. And will fall back to the cuDNN v7 API.
  * - ``TORCH_ALLOW_TF32_CUBLAS_OVERRIDE``
    - If set to ``1``, forces TF32 enablement, overrides ``set_float32_matmul_precision`` setting.
  * - ``TORCH_NCCL_USE_COMM_NONBLOCKING``
    - If set to ``1``, enables non-blocking error handling in NCCL.
  * - ``TORCH_NCCL_AVOID_RECORD_STREAMS``
    - If set to ``0``, enables fallback to record streams-based synchronization behavior in NCCL.
  * - ``TORCH_CUDNN_V8_API_DEBUG``
    - If set to ``1``, sanity check whether cuDNN V8 is being used.
```

----------------------------------------

TITLE: Custom Float32 Autograd Function with AMP
DESCRIPTION: Implementation of a custom autograd function that requires float32 inputs, using custom_fwd and custom_bwd decorators with specific dtype requirements.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/amp_examples.rst#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
class MyFloat32Func(torch.autograd.Function):
    @staticmethod
    @custom_fwd(device_type='cuda', cast_inputs=torch.float32)
    def forward(ctx, input):
        ctx.save_for_backward(input)
        ...
        return fwd_output
    @staticmethod
    @custom_bwd(device_type='cuda')
    def backward(ctx, grad):
        ...
```

----------------------------------------

TITLE: PyTorch Matrix Multiplication Operations in Neural Network
DESCRIPTION: Shows matrix multiplication operations (aten.mm.default) used in fully connected layers and projections. These operations transform features between different dimensionalities across various stages of the network.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convnext_base_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
Operator: aten.mm.default
cnt: 3, ((T([100352, 128], f16), T([128, 512], f16, stride=(1, 128))), {})
cnt: 3, ((T([100352, 512], f16), T([512, 128], f16, stride=(1, 512))), {})
cnt: 3, ((T([25088, 256], f16), T([256, 1024], f16, stride=(1, 256))), {})
cnt: 3, ((T([25088, 1024], f16), T([1024, 256], f16, stride=(1, 1024))), {})
cnt: 27, ((T([6272, 512], f16), T([512, 2048], f16, stride=(1, 512))), {})
cnt: 27, ((T([6272, 2048], f16), T([2048, 512], f16, stride=(1, 2048))), {})
cnt: 3, ((T([1568, 1024], f16), T([1024, 4096], f16, stride=(1, 1024))), {})
cnt: 3, ((T([1568, 4096], f16), T([4096, 1024], f16, stride=(1, 4096))), {})
```

----------------------------------------

TITLE: Starting Multiple Workers in PyTorch Distributed Processing
DESCRIPTION: This function is used to start multiple worker processes in PyTorch's distributed processing framework. It's part of the elastic multiprocessing module.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/multiprocessing.rst#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
torch.distributed.elastic.multiprocessing.start_processes
```

----------------------------------------

TITLE: Creating a DTensor using factory functions
DESCRIPTION: Uses DTensor factory functions like zeros, ones, empty, full, rand, and randn to create DTensors directly, specifying the DeviceMesh and Placement.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.tensor.rst#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
zeros(size, *, dtype=None, layout=None, requires_grad=False, device_mesh=None, placements=None)
```

LANGUAGE: Python
CODE:
```
ones(size, *, dtype=None, layout=None, requires_grad=False, device_mesh=None, placements=None)
```

LANGUAGE: Python
CODE:
```
empty(size, *, dtype=None, layout=None, requires_grad=False, device_mesh=None, placements=None)
```

LANGUAGE: Python
CODE:
```
full(size, fill_value, *, dtype=None, layout=None, requires_grad=False, device_mesh=None, placements=None)
```

LANGUAGE: Python
CODE:
```
rand(*size, *, generator=None, dtype=None, layout=None, requires_grad=False, device_mesh=None, placements=None)
```

LANGUAGE: Python
CODE:
```
randn(*size, *, generator=None, dtype=None, layout=None, requires_grad=False, device_mesh=None, placements=None)
```

----------------------------------------

TITLE: Training Model with Hogwild in PyTorch (Python)
DESCRIPTION: This snippet demonstrates the setup of parallel training using the Hogwild method in PyTorch. It involves sharing a model's parameters across multiple processes with each process performing updates to shared parameters asynchronously. The shared memory setting is crucial for using the fork method. Dependencies include torch.multiprocessing, and the proper setup of data loaders and optimizers. The expected input is a model with shared memory, and outputs are updates to the model parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/multiprocessing.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
import torch.multiprocessing as mp
from model import MyModel

def train(model):
    # Construct data_loader, optimizer, etc.
    for data, labels in data_loader:
        optimizer.zero_grad()
        loss_fn(model(data), labels).backward()
        optimizer.step()  # This will update the shared parameters

if __name__ == '__main__':
    num_processes = 4
    model = MyModel()
    # NOTE: this is required for the ``fork`` method to work
    model.share_memory()
    processes = []
    for rank in range(num_processes):
        p = mp.Process(target=train, args=(model,))
        p.start()
        processes.append(p)
    for p in processes:
        p.join()
```

----------------------------------------

TITLE: Gradient Checking Example
DESCRIPTION: Shows how to verify custom gradient implementations using PyTorch's gradcheck utility with finite difference approximations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
from torch.autograd import gradcheck

# gradcheck takes a tuple of tensors as input, check if your gradient
# evaluated with these tensors are close enough to numerical
# approximations and returns True if they all verify this condition.
input = (torch.randn(20,20,dtype=torch.double,requires_grad=True), torch.randn(30,20,dtype=torch.double,requires_grad=True))
test = gradcheck(linear, input, eps=1e-6, atol=1e-4)
print(test)
```

----------------------------------------

TITLE: Creating Named Tensors with PyTorch
DESCRIPTION: This example demonstrates how to create a tensor with named dimensions using torch.zeros. The factory functions such as torch.empty, torch.rand, torch.randn, torch.ones, and torch.tensor support the names argument to associate names with tensor dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/named_tensor.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
torch.zeros(2, 3, names=('N', 'C'))
```

----------------------------------------

TITLE: Decomposing BatchNorm for Inference - PyTorch - Python
DESCRIPTION: Here, the previously exported training program is converted to an inference IR by decomposing operators (such as batch_norm) using a decomposition table. The decomposition table is modified to keep conv2d but decompose other ops to their functional equivalents. Requires a previously exported ExportedProgram and PyTorch's torch.export.default_decompositions API.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
# Lower to core aten inference IR, but keep conv2d
decomp_table = torch.export.default_decompositions()
del decomp_table[torch.ops.aten.conv2d.default]
ep_for_inference = ep_for_training.run_decompositions(decomp_table)

print(ep_for_inference)
```

----------------------------------------

TITLE: Implementing CSV Parser DataPipe
DESCRIPTION: Complete implementation of CSVParserIterDataPipe that processes CSV file streams and yields rows. Shows how to handle file streams and use external libraries.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/datapipes/README.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
@functional_datapipe("parse_csv_files")
class CSVParserIterDataPipe(IterDataPipe):
    def __init__(self, dp, **fmtparams):
        self.dp = dp
        self.fmtparams = fmtparams

    def __iter__(self):
        for filename, stream in self.dp:
            reader = csv.reader(stream, **self.fmtparams)
            for row in reader:
                yield filename, row
```

----------------------------------------

TITLE: Recurrent Network Data Parallelism in PyTorch
DESCRIPTION: This example demonstrates handling data parallelism issues when using packed and unpacked sequence operations in recurrent networks on PyTorch. It highlights using the `total_length` argument in `pad_packed_sequence` to ensure consistent output sequence lengths across devices.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/faq.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
    from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

    class MyModule(nn.Module):
        # ... __init__, other methods, etc.

        # padded_input is of shape [B x T x *] (batch_first mode) and contains
        # the sequences sorted by lengths
        #   B is the batch size
        #   T is max sequence length
        def forward(self, padded_input, input_lengths):
            total_length = padded_input.size(1)  # get the max sequence length
            packed_input = pack_padded_sequence(padded_input, input_lengths,
                                                batch_first=True)
            packed_output, _ = self.my_lstm(packed_input)
            output, _ = pad_packed_sequence(packed_output, batch_first=True,
                                            total_length=total_length)
            return output

    
    m = MyModule().cuda()
    dp_m = nn.DataParallel(m)
```

----------------------------------------

TITLE: Tensor Addition and Convolution Operations
DESCRIPTION: Multiple tensor addition operations and convolution operations with various configurations, including 7x7 and 1x1 convolutions with different stride and padding settings.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convmixer_768_32_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
aten.add.Tensor((T([32, 768, 32, 32], f16), T([32, 768, 32, 32], f16)), {})
aten.convolution.default((T([32, 3, 224, 224], f16), T([768, 3, 7, 7], f16), T([768], f16), [7, 7], [0, 0], [1, 1], False, [0, 0], 1), {})
```

----------------------------------------

TITLE: Defining If Statements in TorchScript
DESCRIPTION: Specifies the syntax for if statements in TorchScript, including both basic if/else and ternary if/else forms. Shows how tensors are handled in conditionals.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_27

LANGUAGE: python
CODE:
```
if_stmt ::= "if" assignment_expression ":" suite
            ("elif" assignment_expression ":" suite)
            ["else" ":" suite]

if_stmt ::= return [expression_list] "if" assignment_expression "else" [expression_list]
```

----------------------------------------

TITLE: Defining a Constant in TorchScript Using Python Module Attribute
DESCRIPTION: This snippet demonstrates how to use a constant value from a Python module (math.pi) in a TorchScript function. Values looked up as attributes of a module are assumed to be constant in TorchScript.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
import math
import torch

@torch.jit.script
def fn():
    return math.pi
```

----------------------------------------

TITLE: Mapping Functions to Data with PyTorch DataPipes
DESCRIPTION: Illustrates the use of the map() method to apply functions to data in DataPipes. Shows examples of mapping at different nesting levels.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(10).map(lambda x: x * 2)
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(10).batch(3).map(lambda x: x * 2)
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(10).batch(3).batch(2).map(lambda x: x * 2, nesting_level = 2)
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(10).batch(3).batch(2).batch(2).map(lambda x: x * 2, nesting_level = -1)
for i in dp:
    print(i)
```

----------------------------------------

TITLE: Implementing Quantization Aware Training in PyTorch
DESCRIPTION: Demonstrates quantization-aware training setup with fake quantization modules to model quantization effects during training. Includes model definition with quantization stubs and batch normalization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch

# define a floating point model where some layers could benefit from QAT
class M(torch.nn.Module):
    def __init__(self):
        super().__init__()
        # QuantStub converts tensors from floating point to quantized
        self.quant = torch.ao.quantization.QuantStub()
        self.conv = torch.nn.Conv2d(1, 1, 1)
        self.bn = torch.nn.BatchNorm2d(1)
        self.relu = torch.nn.ReLU()
        # DeQuantStub converts tensors from quantized to floating point
        self.dequant = torch.ao.quantization.DeQuantStub()

    def forward(self, x):
        x = self.quant(x)
        x = self.conv(x)
        x = self.bn(x)
        x = self.relu(x)
        x = self.dequant(x)
        return x

# create a model instance
model_fp32 = M()

# model must be set to eval for fusion to work
model_fp32.eval()
```

----------------------------------------

TITLE: Generating CUDA Memory Snapshot in PyTorch
DESCRIPTION: This snippet demonstrates how to enable memory history recording, run code to be observed, and save a pickled snapshot of CUDA memory usage in PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch_cuda_memory.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
# enable memory history, which will
# add tracebacks and event history to snapshots
torch.cuda.memory._record_memory_history()

run_your_code()
torch.cuda.memory._dump_snapshot("my_snapshot.pickle")
```

----------------------------------------

TITLE: Profiling PyTorch Distributed Collective Operation with torch.profiler (Python)
DESCRIPTION: This code demonstrates how to use the `torch.profiler` context manager to profile a collective communication function from the `torch.distributed` package. It initializes a tensor and performs an `all_reduce` operation within the profiler context. This allows users to capture performance data for distributed operations, which will be included in the profiling output.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.rst#_snippet_3

LANGUAGE: Python
CODE:
```
import torch
import torch.distributed as dist
with torch.profiler():
    tensor = torch.randn(20, 10)
    dist.all_reduce(tensor)
```

----------------------------------------

TITLE: Implementing __torch_dispatch__ Method
DESCRIPTION: Defines the signature and calling convention for the __torch_dispatch__ method used to override PyTorch's native operations. This method receives normalized arguments and can intercept all calls into the aten native API.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
@classmethod
def __torch_dispatch__(cls, func, types, args=(), kwargs=None):
    pass
```

----------------------------------------

TITLE: Setting up Environment and Installing PyTorch for CUDA on Windows CMD
DESCRIPTION: Configures environment variables necessary for a CUDA-enabled build on Windows, including paths for MKL libraries, setting up the MSVC build environment using vcvarsall, optionally overriding toolset versions or CUDA host compiler, and finally running the PyTorch setup script.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_15

LANGUAGE: CMD
CODE:
```
cmd

:: Set the environment variables after you have downloaded and unzipped the mkl package,
:: else CMake would throw an error as `Could NOT find OpenMP`.
set CMAKE_INCLUDE_PATH={Your directory}\mkl\include
set LIB={Your directory}\mkl\lib;%LIB%

:: Read the content in the previous section carefully before you proceed.
:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.
:: "Visual Studio 2019 Developer Command Prompt" will be run automatically.
:: Make sure you have CMake >= 3.12 before you do this when you use the Visual Studio generator.
set CMAKE_GENERATOR_TOOLSET_VERSION=14.27
set DISTUTILS_USE_SDK=1
for /f "usebackq tokens=*" %i in (`"%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe" -version [15^,17^) -products * -latest -property installationPath`) do call "%i\VC\Auxiliary\Build\vcvarsall.bat" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%

:: [Optional] If you want to override the CUDA host compiler
set CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe

python setup.py develop

```

----------------------------------------

TITLE: Using InferenceMode for Inference Workloads in C++
DESCRIPTION: Example of using c10::InferenceMode guard to optimize performance for inference workload in C++. The guard wraps the entire workflow from model loading to inference execution and post-processing.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/inference_mode.rst#2025-04-22_snippet_0

LANGUAGE: cpp
CODE:
```
c10::InferenceMode guard;
model.load_jit(saved_model);
auto inputs = preprocess_tensors(data);
auto out = model.forward(inputs);
auto outputs = postprocess_tensors(out);
```

----------------------------------------

TITLE: Installing PyTorch Environment Setup
DESCRIPTION: Instructions for setting up PyTorch environment with CUDA, torchvision, and installing latest PyTorch from source. Includes verification of installation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
# Install torchvision. It comes with the pytorch stable release binary
conda install pytorch torchvision -c pytorch

# Install the latest pytorch master from source.
# It should supersede the installation from the release binary.
cd $PYTORCH_HOME
python setup.py build develop

# Check the pytorch installation version
python -c "import torch; print(torch.__version__)"
```

----------------------------------------

TITLE: Error When Changing Allocator After First Allocation in Python
DESCRIPTION: This Python snippet illustrates a common error scenario when using `torch.cuda.memory.change_current_allocator`. It shows that attempting to change the allocator after the default allocator has already been implicitly instantiated (e.g., by the first tensor allocation on a CUDA device) will result in an error. `change_current_allocator` must be called before any CUDA memory operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_25

LANGUAGE: python
CODE:
```
import torch

# Do an initial memory allocator
b = torch.zeros(10, device='cuda')
# Load the allocator
new_alloc = torch.cuda.memory.CUDAPluggableAllocator(
    'alloc.so', 'my_malloc', 'my_free')
# This will error since the current allocator was already instantiated
torch.cuda.memory.change_current_allocator(new_alloc)
```

----------------------------------------

TITLE: AOT Module with ResNet Example
DESCRIPTION: Demonstrates how to apply AOT compilation to a complete neural network module using ResNet18 as an example.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/COMPILE_README.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from torchvision.models import resnet18
aot_module(resnet18(), print_graph("forward"), print_graph("backward"))(torch.randn(1,3,200,200))
```

----------------------------------------

TITLE: Analyzing PyTorch Convolution Operations
DESCRIPTION: This snippet demonstrates the usage of the aten.convolution.default operator for performing convolutions with various input shapes, kernel sizes, and parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
Operator: aten.convolution.default
cnt: 2, ((T([3, 3, 518, 518], f16), T([64, 3, 7, 7], f16), T([64], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([3, 64, 512, 512], f16), T([128, 64, 3, 3], f16), T([128], f16), [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([3, 128, 256, 256], f16), T([256, 128, 3, 3], f16), T([256], f16), [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
# ... (truncated for brevity)
```

----------------------------------------

TITLE: Measuring CUDA Operation Time with Events - PyTorch Python
DESCRIPTION: This snippet demonstrates how to accurately measure the time elapsed between two CUDA operations using `torch.cuda.Event`. Events are recorded at the start and end points, and `torch.cuda.synchronize()` is called before querying the elapsed time to ensure all preceding operations on the device have completed.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_11

LANGUAGE: Python
CODE:
```
start_event = torch.cuda.Event(enable_timing=True)
end_event = torch.cuda.Event(enable_timing=True)
start_event.record()

# Run some things here

end_event.record()
torch.cuda.synchronize()  # Wait for the events to be recorded!
elapsed_time_ms = start_event.elapsed_time(end_event)
```

----------------------------------------

TITLE: Stream Synchronization Without Read Dependency - PyTorch Python
DESCRIPTION: This example demonstrates that synchronization (`s.wait_stream`) is still required when using a non-default stream even if there is no explicit read dependency between operations on different streams. This is necessary because the CUDA caching allocator might reuse memory that has pending operations associated with it on another stream.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_14

LANGUAGE: Python
CODE:
```
cuda = torch.device('cuda')
s = torch.cuda.Stream()  # Create a new stream.
A = torch.empty((100, 100), device=cuda)
s.wait_stream(torch.cuda.default_stream(cuda))  # STILL REQUIRED!
with torch.cuda.stream(s):
    A.normal_(0.0, 1.0)
    A.record_stream(s)
```

----------------------------------------

TITLE: Basic __torch_function__ Implementation for ScalarTensor
DESCRIPTION: Example implementation of __torch_function__ that falls back to original torch functions when no override is available. Converts ScalarTensor arguments to regular tensors before calling the original function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
@classmethod
def __torch_function__(cls, func, types, args=(), kwargs=None):
    if kwargs is None:
        kwargs = {}
    if func not in HANDLED_FUNCTIONS or not all(
            issubclass(t, (torch.Tensor, ScalarTensor))
            for t in types
        ):
        args = [a.tensor() if hasattr(a, 'tensor') else a for a in args]
        return func(*args, **kwargs)
    return HANDLED_FUNCTIONS[func](*args, **kwargs)
```

----------------------------------------

TITLE: Demonstrating Tensor Operations with GPU and CPU in PyTorch
DESCRIPTION: This code snippet shows examples of tensor operations that fail due to automatic transfer limitations between GPU and CPU in PyTorch. It illustrates that scalar tensors are not automatically transferred between devices.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> torch.ones(()).cuda() + torch.ones(1)  # Fail, scalar not auto-transferred from GPU to CPU and non-scalar not auto-transferred from CPU to GPU
>>> torch.ones(1) + torch.ones(()).cuda()  # Fail, scalar not auto-transferred from GPU to CPU and non-scalar not auto-transferred from CPU to GPU
```

----------------------------------------

TITLE: PyTorch Tensor Addition Operations
DESCRIPTION: Multiple tensor addition operations across various shapes and dimensions, primarily working with half-precision (fp16) tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cspdarknet53_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
aten.add.Tensor((T([], i64), 1), {})
aten.add.Tensor((T([64, 64, 128, 128], f16), T([64, 64, 128, 128], f16, stride=(2097152, 16384, 128, 1))), {})
```

----------------------------------------

TITLE: Importing PyTorch, functorch, and Setting Random Seed - Python
DESCRIPTION: This snippet imports essential PyTorch and functorch modules, including neural network layers and functional tools, and sets a manual random seed for reproducibility. Dependencies required include PyTorch and functorch. This is the setup required for all subsequent model, data, and transformation operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/ensembling.ipynb#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
import torch.nn as nn
import torch.nn.functional as F
from functools import partial
torch.manual_seed(0);
```

----------------------------------------

TITLE: Benchmarking Naive vs. functorch Per-Sample Gradient Calculation
DESCRIPTION: Uses `torch.utils.benchmark.Timer` to measure the execution time of the naive per-sample gradient calculation (`compute_sample_grads`) and the `functorch`-based calculation (`ft_compute_sample_grad`). `Timer` objects are created for each function call, and `.timeit(100)` runs each statement 100 times to get reliable timing measurements. The results (median times) are stored in `no_vmap_timing` and `with_vmap_timing` and printed.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
from torch.utils.benchmark import Timer

without_vmap = Timer( stmt="compute_sample_grads(data, targets)", globals=globals())
with_vmap = Timer(stmt="ft_compute_sample_grad(params, buffers, data, targets)",globals=globals())
no_vmap_timing = without_vmap.timeit(100)
with_vmap_timing = with_vmap.timeit(100)

print(f'Per-sample-grads without vmap {no_vmap_timing}')
print(f'Per-sample-grads with vmap {with_vmap_timing}')
```

----------------------------------------

TITLE: Computing Jacobian with Reverse-Mode AD using torch.func.jacrev in Python
DESCRIPTION: This snippet showcases `torch.func.jacrev` to compute the Jacobian of a function using reverse-mode automatic differentiation. It calculates the Jacobian of `torch.sin` applied element-wise to a vector `x`. The result is compared against the expected Jacobian, which is a diagonal matrix with `torch.cos(x)` on the diagonal.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.whirlwind_tour.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from torch.func import jacrev
x = torch.randn(5)
jacobian = jacrev(torch.sin)(x)
expected = torch.diag(torch.cos(x))
assert torch.allclose(jacobian, expected)
```

----------------------------------------

TITLE: Batch Matrix Multiplication with bmm in PyTorch (Python)
DESCRIPTION: The aten.bmm operation conducts a batch matrix-matrix product of matrices stored in one tensor and of matrices stored in another. This is a fundamental operation in handling mini-batches of data in neural networks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
aten.bmm.default
cnt: 12, ((T([192, 512, 64], f16), T([192, 64, 512], f16)), {})
cnt: 12, ((T([192, 512, 512], f16), T([192, 512, 64], f16)), {})
cnt: 12, ((T([192, 512, 512], f16, stride=(262144, 1, 512)), T([192, 512, 64], f16)), {})
cnt: 12, ((T([192, 512, 64], f16), T([192, 64, 512], f16, stride=(32768, 1, 64))), {})
cnt: 12, ((T([192, 64, 512], f16, stride=(32768, 1, 64)), T([192, 512, 512], f16)), {})
cnt: 12, ((T([192, 512, 512], f16), T([192, 512, 64], f16, stride=(32768, 1, 512))), {})
```

----------------------------------------

TITLE: Synchronizing Producer Process Using Events in PyTorch Multiprocessing (Python)
DESCRIPTION: Shows how a producer process can use an `event` object (presumably a `torch.multiprocessing.Event` or standard `multiprocessing.Event`) to wait until consumer processes have finished processing shared tensors. Calling `event.wait()` blocks the producer until the event is set by the consumers, ensuring resources aren't released prematurely.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/multiprocessing.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
## producer
# send tensors, do something
event.wait()
```

----------------------------------------

TITLE: Layer Normalization Operations - PyTorch
DESCRIPTION: Layer normalization operations for transformer layers, including both forward and backward passes. Uses epsilon value of 1e-05 for numerical stability.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_GPT2_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
aten.native_layer_norm.default((T([4, 512, 768], f16), [768], T([768], f16), T([768], f16), 1e-05), {})
aten.native_layer_norm_backward.default((T([4, 512, 768], f16), T([4, 512, 768], f16), [768], T([4, 512, 1], f32), T([4, 512, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})
```

----------------------------------------

TITLE: Tensor Subtraction with sub in PyTorch (Python)
DESCRIPTION: Utilizes aten.sub for subtracting one tensor from another element-wise, essential during operations involving differential calculations where pairwise differences across tensors are needed.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_25

LANGUAGE: Python
CODE:
```
aten.sub.Tensor
cnt: 2, ((T([16, 512], i64, stride=(2048, 4)), T([16, 512], i64, stride=(2048, 4))), {})
```

----------------------------------------

TITLE: Linear Model with vmap Transform
DESCRIPTION: Example showing how to use vmap to handle batch dimensions in a simple linear model.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/README.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from functorch import vmap
batch_size, feature_size = 3, 5
weights = torch.randn(feature_size, requires_grad=True)

def model(feature_vec):
    # Very simple linear model with activation
    assert feature_vec.dim() == 1
    return feature_vec.dot(weights).relu()

examples = torch.randn(batch_size, feature_size)
result = vmap(model)(examples)
```

----------------------------------------

TITLE: Training Model with FP32 on Intel GPU
DESCRIPTION: Provides an example of training a ResNet50 model on Intel GPU using CIFAR-10 dataset. It includes data preprocessing, model setup, and training loop with step-by-step optimization and feedback.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/get_start_xpu.rst#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
import torch
import torchvision

LR = 0.001
DOWNLOAD = True
DATA = "datasets/cifar10/"

transform = torchvision.transforms.Compose(
    [
        torchvision.transforms.Resize((224, 224)),
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
    ]
)
train_dataset = torchvision.datasets.CIFAR10(
    root=DATA,
    train=True,
    transform=transform,
    download=DOWNLOAD,
)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128)
train_len = len(train_loader)

model = torchvision.models.resnet50()
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)
model.train()
model = model.to("xpu")
criterion = criterion.to("xpu")

print(f"Initiating training")
for batch_idx, (data, target) in enumerate(train_loader):
    data = data.to("xpu")
    target = target.to("xpu")
    optimizer.zero_grad()
    output = model(data)
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()
    if (batch_idx + 1) % 10 == 0:
        iteration_loss = loss.item()
        print(f"Iteration [{batch_idx+1}/{train_len}], Loss: {iteration_loss:.4f}")
torch.save(
    {
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
    },
    "checkpoint.pth",
)

print("Execution finished")
```

----------------------------------------

TITLE: PyTorch Sum Operations
DESCRIPTION: Log entries for sum reduction operations, including symbolic and default implementations. Shows operations on classification output tensors with 1000 classes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet50_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([32, 1000], f16, stride=(0, 0)), [0], True), {})

Operator: aten.sum.default
cnt: 1, ((T([32, 1000], f16),), {})
```

----------------------------------------

TITLE: Logging PyTorch Model and Images to TensorBoard
DESCRIPTION: Demonstrates basic TensorBoard integration by logging MNIST dataset images and a ResNet model graph. Shows initialization of SummaryWriter, data loading with transforms, model setup, and logging images and model graph.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensorboard.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
import torchvision
from torch.utils.tensorboard import SummaryWriter
from torchvision import datasets, transforms

# Writer will output to ./runs/ directory by default
writer = SummaryWriter()

transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
trainset = datasets.MNIST('mnist_train', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)
model = torchvision.models.resnet50(False)
# Have ResNet model take in grayscale rather than RGB
model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
images, labels = next(iter(trainloader))

grid = torchvision.utils.make_grid(images)
writer.add_image('images', grid, 0)
writer.add_graph(model, images)
writer.close()
```

----------------------------------------

TITLE: Create New Tensor with Same Properties PyTorch
DESCRIPTION: Illustrates using `torch.Tensor.new_*` methods (`new_full`, `new_tensor`) to create new tensors that automatically inherit the device and dtype of an existing tensor. This is the recommended way to create internal tensors within modules.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_38

LANGUAGE: python
CODE:
```
cuda = torch.device('cuda')
x_cpu = torch.empty(2)
x_gpu = torch.empty(2, device=cuda)
x_cpu_long = torch.empty(2, dtype=torch.int64)

y_cpu = x_cpu.new_full([3, 2], fill_value=0.3)
y_gpu = x_gpu.new_full([3, 2], fill_value=-5)
y_cpu_long = x_cpu_long.new_tensor([[1, 2, 3]])
```

----------------------------------------

TITLE: Specifying Non-Tensor Argument Types With Type Annotations - PyTorch TorchScript - Python
DESCRIPTION: This snippet demonstrates how to specify argument types for TorchScript functions using MyPy-style type annotations in a comment. The function foo takes an int and a tuple of two Tensors, returning their sum plus the int. This facilitates proper type checking and compilation in TorchScript. Requires torch and a valid PyTorch runtime. Expected inputs are an integer and a tuple of two tensors; the output is a tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch

@torch.jit.script
def foo(x, tup):
    # type: (int, Tuple[Tensor, Tensor]) -> Tensor
    t0, t1 = tup
    return t0 + t1 + x

print(foo(3, (torch.rand(3), torch.rand(3))))
```

----------------------------------------

TITLE: Bad Practice: In-place Operations with Traced Tensor Shapes (Avoid)
DESCRIPTION: This snippet demonstrates a potential pitfall in tracing mode: using in-place operations on variables derived from traced tensor shapes (e.g., `tensor.shape`). In tracing, these derived variables might share memory, causing unexpected results. Avoid inplace modifications like `+=` in such cases.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_7

LANGUAGE: python
CODE:
```
class Model(torch.nn.Module):
  def forward(self, states):
      batch_size, seq_length = states.shape[:2]
      real_seq_length = seq_length
      real_seq_length += 2
      return real_seq_length + seq_length
```

----------------------------------------

TITLE: Iterating Through Named Parameters of a Composite Module
DESCRIPTION: Demonstrates iterating through all named parameters of the `DynamicNet` instance using `named_parameters()`. This shows that the method recursively collects parameters from the module itself (`final`) and all its submodules, including those within `ModuleList` (`linears.0`, `linears.1`, etc.) and `ModuleDict` (if they had parameters).
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
for parameter in dynamic_net.named_parameters():
  print(parameter)
: ('linears.0.weight', Parameter containing:
tensor([[-1.2051,  0.7601,  1.1065,  0.1963],
        [ 3.0592,  0.4354,  1.6598,  0.9828],
        [-0.4446,  0.4628,  0.8774,  1.6848],
        [-0.1222,  1.5458,  1.1729,  1.4647]], requires_grad=True))
('linears.0.bias', Parameter containing:
tensor([ 1.5310,  1.0609, -2.0940,  1.1266], requires_grad=True))
('linears.1.weight', Parameter containing:
tensor([[ 2.1113, -0.0623, -1.0806,  0.3508],
        [-0.0550,  1.5317,  1.1064, -0.5562],
        [-0.4028, -0.6942,  1.5793, -1.0140],
        [-0.0329,  0.1160, -1.7183, -1.0434]], requires_grad=True))
('linears.1.bias', Parameter containing:
tensor([ 0.0361, -0.9768, -0.3889,  1.1613], requires_grad=True))
('linears.2.weight', Parameter containing:
tensor([[-2.6340, -0.3887, -0.9979,  0.0767],
        [-0.3526,  0.8756, -1.5847, -0.6016],
        [-0.3269, -0.1608,  0.2897, -2.0829],
        [ 2.6338,  0.9239,  0.6943, -1.5034]], requires_grad=True))
('linears.2.bias', Parameter containing:
tensor([ 1.0268,  0.4489, -0.9403,  0.1571], requires_grad=True))
('final.weight', Parameter containing:
```

----------------------------------------

TITLE: Implementing Eager Mode Quantization in PyTorch
DESCRIPTION: Demonstrates how to implement eager mode quantization using QuantStub and DeQuantStub. The example shows a custom module with manual quantization and dequantization steps.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
class M(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.quant = torch.ao.quantization.QuantStub()
        self.conv1 = torch.nn.Conv2d(1, 1, 1)
        # this module will not be quantized (see `qconfig = None` logic below)
        self.conv2 = torch.nn.Conv2d(1, 1, 1)
        self.dequant = torch.ao.quantization.DeQuantStub()

    def forward(self, x):
        # during the convert step, this will be replaced with a
        # `quantize_per_tensor` call
        x = self.quant(x)
        x = self.conv1(x)
        # during the convert step, this will be replaced with a
        # `dequantize` call
        x = self.dequant(x)
        x = self.conv2(x)
        return x

m = M()
m.qconfig = some_qconfig
# turn off quantization for conv2
m.conv2.qconfig = None
```

----------------------------------------

TITLE: Configuring Quantization with QConfig and BackendConfig in PyTorch (Python)
DESCRIPTION: Provides simplified examples of `qconfig_mapping` and `backend_config` dictionaries used to specify quantization parameters and supported data type configurations for a given pattern like `nnqat.LinearReLU`. It shows how observers and target dtypes are defined for activations and weights.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#_snippet_5

LANGUAGE: Python
CODE:
```
# qconfig_mapping (simplified, shown as dict)
{'qat_linear_relu': QConfig(
  weight=MinMaxObserver.with_args(dtype=torch.qint8),
  input_activation=HistogramObserver.with_args(dtype=torch.quint8),
  output_activation=PlaceholderObserver.with_args(dtype=torch.float32),
)}

# backend_config (simplified)
{
  'pattern': nnqat.LinearReLU,
  'dtype_configs': [{input: torch.quint8, output: torch.float32, weight: torch.qint8}],
}
```

----------------------------------------

TITLE: Registering PyTorch Kernels for Multiple Backends (C++)
DESCRIPTION: Shows how to register different kernel implementations for the same operator (`my_namespace::my_op`) targeting different backends (CPU and CUDA). Two separate `.op()` calls (or chained calls) are used, each specifying a different kernel function and the corresponding dispatch key (`CPU()` or `CUDA()`).
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_6

LANGUAGE: cpp
CODE:
```
namespace {
Tensor my_kernel_cpu(const Tensor& a, const Tensor& b) {...}
Tensor my_kernel_cuda(const Tensor& a, const Tensor& b) {...}
}

static auto registry = torch::RegisterOperators()
   .op("my_namespace::my_op",  torch::RegisterOperators::options()
       .kernel<decltype(my_kernel_cpu), &my_kernel_cpu>(CPU()))
   .op("my_namespace::my_op",  torch::RegisterOperators::options()
       .kernel<decltype(my_kernel_cuda), &my_kernel_cuda>(CUDA()));
```

----------------------------------------

TITLE: Backend Registration in setup.py
DESCRIPTION: Shows how to register a backend through Python package entry points in setup.py configuration.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_custom_backends.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
setup(
    ...
    'torch_dynamo_backends': [
        'my_compiler = your_module.submodule:my_compiler',
    ]
    ...
)
```

----------------------------------------

TITLE: Tensor Summation Operations in PyTorch (Default)
DESCRIPTION: This snippet shows a tensor summation operation that reduces all dimensions to a scalar value. This is typically used for computing total loss or other global statistics from tensor values.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
Operator: aten.sum.default
cnt: 1, ((T([96, 65], f16),), {})
```

----------------------------------------

TITLE: Building a Stage Runtime
DESCRIPTION: This snippet shows the construction of a distributed stage runtime on a specified device using a ProcessGroup. It requires a pipe object, a stage index, device, and group as inputs and outputs a stage object.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.pipelining.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
stage = pipe.build_stage(stage_idx, device, group)
```

----------------------------------------

TITLE: Exporting with Optional Input Omitted using torch.export (Python)
DESCRIPTION: This snippet shows the behavior of `torch.export.export` when an optional argument is *not* provided during tracing. The module `M` has an optional argument `y`. When exported with only `x` provided, `torch.export.export` traces the default path (where `y` is `None`, leading to `return x + x`). The resulting exported graph is specialized for this case, and its function signature only requires `x`, effectively losing the optional defaulting behavior.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.programming_model.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
class M(torch.nn.Module):
    def forward(self, x, y=None):
        if y is not None:
            return y * x
        return x + x

# Optional input is not passed in
ep = torch.export.export(M(), (torch.randn(3, 3),))
print(ep)
"""
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "f32[3, 3]", y):
            # File: /data/users/angelayi/pytorch/moo.py:16 in forward, code: return x + x
            add: "f32[3, 3]" = torch.ops.aten.add.Tensor(x, x);  x = None
            return (add,)
"""
```

----------------------------------------

TITLE: Building LibTorch Project with CMake
DESCRIPTION: Commands to create a build directory, configure CMake with the LibTorch path, and build the project. This is the final step in compiling the LibTorch example application.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/installing.rst#2025-04-22_snippet_3

LANGUAGE: sh
CODE:
```
mkdir build
cd build
cmake -DCMAKE_PREFIX_PATH=/absolute/path/to/libtorch ..
cmake --build . --config Release
```

----------------------------------------

TITLE: Loading Optimizer State Dict by Parameter Names
DESCRIPTION: This example shows how to implement a custom hook for loading an optimizer state dict based on parameter names instead of their order. It ensures that parameters are correctly mapped even if their order changes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
def names_matching(optimizer, state_dict):
    assert len(state_dict['param_groups']) == len(optimizer.state_dict()['param_groups'])
    adapted_state_dict = deepcopy(optimizer.state_dict())
    for g_ind in range(len(state_dict['param_groups'])):
        assert len(state_dict['param_groups'][g_ind]['params']) == len(
            optimizer.state_dict()['param_groups'][g_ind]['params'])

        for k, v in state_dict['param_groups'][g_ind].items():
            if k not in ['params', 'param_names']:
                adapted_state_dict['param_groups'][g_ind][k] = v

        for param_id, param_name in zip(
                optimizer.state_dict()['param_groups'][g_ind]['params'],
                optimizer.state_dict()['param_groups'][g_ind]['param_names']):
            index_in_loaded_list = state_dict['param_groups'][g_ind]['param_names'].index(param_name)
            id_in_loaded = state_dict['param_groups'][g_ind]['params'][index_in_loaded_list]
            # Copy the state of the corresponding parameter
            if id_in_loaded in state_dict['state']:
                adapted_state_dict['state'][param_id] = deepcopy(state_dict['state'][id_in_loaded])

    return adapted_state_dict
```

----------------------------------------

TITLE: Signaling Completion from Consumer Process Using Events (Python)
DESCRIPTION: Demonstrates how a consumer process, after receiving and using shared tensors, signals its completion to the producer process by calling `event.set()` on a shared event object. This allows the waiting producer process (using `event.wait()`) to proceed.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/multiprocessing.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
## consumer
# receive tensors and use them
event.set()
```

----------------------------------------

TITLE: Embedding Operations - PyTorch
DESCRIPTION: Token embedding operations for input sequences, including both forward and backward passes. Handles vocabulary size of 50257 and embedding dimension of 768.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_GPT2_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
aten.embedding.default((T([50257, 768], f16), T([4, 512], i64)), {})
aten.embedding_dense_backward.default((T([4, 512, 768], f16), T([4, 512], i64), 50257, -1, False), {})
```

----------------------------------------

TITLE: Python Model Device Conversion Example
DESCRIPTION: Demonstrates best practices for training models on GPU and performing inference on CPU using TorchScript.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
cpu_model = gpu_model.cpu()
sample_input_cpu = sample_input_gpu.cpu()
traced_cpu = torch.jit.trace(cpu_model, sample_input_cpu)
torch.jit.save(traced_cpu, "cpu.pt")

traced_gpu = torch.jit.trace(gpu_model, sample_input_gpu)
torch.jit.save(traced_gpu, "gpu.pt")

# ... later, when using the model:

if use_gpu:
  model = torch.jit.load("gpu.pt")
else:
  model = torch.jit.load("cpu.pt")

model(input)
```

----------------------------------------

TITLE: Using torch.cond with Dynamic Shape Predicate
DESCRIPTION: An example of using torch.cond to branch based on input shape, demonstrating its ability to handle dynamic shape predicates.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cond.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch

def true_fn(x: torch.Tensor):
    return x.cos() + x.sin()

def false_fn(x: torch.Tensor):
    return x.sin()

class DynamicShapeCondPredicate(torch.nn.Module):
    """
    A basic usage of cond based on dynamic shape predicate.
    """

    def __init__(self):
        super().__init__()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        def true_fn(x: torch.Tensor):
            return x.cos()

        def false_fn(x: torch.Tensor):
            return x.sin()

        return torch.cond(x.shape[0] > 4, true_fn, false_fn, (x,))

dyn_shape_mod = DynamicShapeCondPredicate()
```

----------------------------------------

TITLE: Integrating optimize_for_inference as a TorchDynamo Backend
DESCRIPTION: This snippet shows how to create a custom TorchDynamo backend that uses torch.jit.optimize_for_inference for improved performance. It takes a GraphModule and example inputs, scripts the module, and then applies optimization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_custom_backends.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
def optimize_for_inference_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):
    scripted = torch.jit.script(gm)
    return torch.jit.optimize_for_inference(scripted)
```

----------------------------------------

TITLE: Dividing Tensors with div Tensor in PyTorch (Python)
DESCRIPTION: Involves the aten.div operation to divide a tensor by a scalar. This operation is essential for normalizing data or scaling down tensor values, especially necessary in adjusting activation outputs or gradients.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_11

LANGUAGE: Python
CODE:
```
aten.div.Tensor
cnt: 24, ((T([16, 12, 512, 512], f16), 8.0), {})
```

----------------------------------------

TITLE: Graph Breaks Profiling Example
DESCRIPTION: Shows how to identify graph breaks using torch.profiler by creating synthetic breaks in a model's execution.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_profiling_torch_compile.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import torch
import torch._dynamo

device = 'cuda'

class ModelWithBreaks(torch.nn.Module):
    def __init__(self):
        super().__init__()
        def create_sequential():
            return torch.nn.Sequential(
                torch.nn.Linear(128, 128),
                torch.nn.ReLU(),
                torch.nn.Linear(128, 128),
                torch.nn.ReLU(),
            )
        self.mod1 = create_sequential()
        self.mod2 = create_sequential()
        self.mod3 = create_sequential()
        self.mod4 = create_sequential()

    def forward(self, inp):
        mod1 = self.mod1(inp)
        torch._dynamo.graph_break()
        mod2 = self.mod2(mod1)
        torch._dynamo.graph_break()
        mod3 = self.mod3(mod2)
        torch._dynamo.graph_break()
        mod4 = self.mod4(mod3)
        return mod4

model = ModelWithBreaks().to(device)
inputs = [torch.randn((128, 128), device=device) for _ in range(10)]

model_c = torch.compile(model)

def fwd_bwd(inp):
    out = model_c(inp)
    out.sum().backward()

# warm up
fwd_bwd(inputs[0])

with torch.profiler.profile() as prof:
    for i in range(1, 4):
        fwd_bwd(inputs[i])
        prof.step()

prof.export_chrome_trace("trace_break.json")
```

----------------------------------------

TITLE: Matrix Multiplication and Addition with addmm in PyTorch (Python)
DESCRIPTION: The aten.addmm function combines matrix multiplication with addition in a single operation, typically optimizing performance for layers in neural networks such as dense layers or transformations involving bias addition.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
aten.addmm.default
cnt: 48, ((T([768], f16), T([8192, 768], f16), T([768, 768], f16, stride=(1, 768))), {})
cnt: 12, ((T([3072], f16), T([8192, 768], f16), T([768, 3072], f16, stride=(1, 768))), {})
cnt: 12, ((T([768], f16), T([8192, 3072], f16), T([3072, 768], f16, stride=(1, 3072))), {})
cnt: 1, ((T([768], f16), T([16, 768], f16, stride=(393216, 1)), T([768, 768], f16, stride=(1, 768))), {})
cnt: 1, ((T([2], f16), T([16, 768], f16), T([768, 2], f16, stride=(1, 768))), {})
```

----------------------------------------

TITLE: Run All PyTorch Python Tests (bash)
DESCRIPTION: Executes the main test runner script `./test/run_test.py` to run the entire PyTorch Python test suite. This command runs all unit tests located within the `test` folder.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_13

LANGUAGE: bash
CODE:
```
python test/run_test.py
```

----------------------------------------

TITLE: ATen Tensor API Examples in C++
DESCRIPTION: Examples of tensor operations available in ATen's API including mathematical functions, in-place operations (denoted by underscore suffix), and various tensor manipulation methods.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_basics.rst#2025-04-22_snippet_0

LANGUAGE: cpp
CODE:
```
Tensor atan2(const Tensor & other) const;
Tensor & atan2_(const Tensor & other);
Tensor pow(Scalar exponent) const;
Tensor pow(const Tensor & exponent) const;
Tensor & pow_(Scalar exponent);
Tensor & pow_(const Tensor & exponent);
Tensor lerp(const Tensor & end, Scalar weight) const;
Tensor & lerp_(const Tensor & end, Scalar weight);
Tensor histc() const;
Tensor histc(int64_t bins) const;
Tensor histc(int64_t bins, Scalar min) const;
Tensor histc(int64_t bins, Scalar min, Scalar max) const;
```

----------------------------------------

TITLE: Implementing Data-Dependent Control Flow with torch.cond
DESCRIPTION: An example showcasing how to express data-dependent control flow using torch.cond in a PyTorch module.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cond.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
class DataDependentCondPredicate(torch.nn.Module):
    """
    A basic usage of cond based on data dependent predicate.
    """
    def __init__(self):
        super().__init__()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return torch.cond(x.sum() > 4.0, true_fn, false_fn, (x,))
```

----------------------------------------

TITLE: Manual Gradient Layout Control Example - Python
DESCRIPTION: Example demonstrating how to reset gradients to None before accumulation phase in training loop. This approach can improve performance for some networks by allowing autograd to recreate gradients with optimal layout.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/autograd.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
for iterations...
    ...
    for param in model.parameters():
        param.grad = None
    loss.backward()
```

----------------------------------------

TITLE: Hessian Computation with functorch.hessian and Comparison - Python
DESCRIPTION: Demonstrates use of functorch.hessian to compute second-order derivatives. Reduces matrix size to prevent excessive memory use. Computes the Hessian both via hessian() and via explicit composition jacfwd(jacfwd()), and notes jacrev(jacrev()) as an alternative (commented out). Requires functorch, torch, and initialized variables. Used to compare equivalence and API convenience.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
from functorch import hessian

# lets reduce the size in order not to blow out colab. Hessians require significant memory:
Din = 512
Dout = 32
weight = torch.randn(Dout, Din)
bias = torch.randn(Dout)
x = torch.randn(Din)

hess_api = hessian(predict, argnums=2)(weight, bias, x)
hess_fwdfwd = jacfwd(jacfwd(predict, argnums=2), argnums=2)(weight, bias, x)
#hess_revrev = jacrev(jacrev(predict, argnums=2), argnums=2)(weight, bias, x)

```

----------------------------------------

TITLE: Optional Type Refinement in TorchScript With Conditionals and Asserts - PyTorch - Python
DESCRIPTION: This code demonstrates how TorchScript refines Optional types based on None checks within conditionals and asserts. An nn.Module subclass defines a forward method using Optional[int] parameters; TorchScript infers variable types when conditions or asserts validate non-None values. Shows refinement for local variables and copying attributes to locals for refinement. Dependencies include torch, torch.nn, and typing. Inputs include Optional[int] types and torch.Tensor, returning an int.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
import torch
import torch.nn as nn
from typing import Optional

class M(nn.Module):
    z: Optional[int]

    def __init__(self, z):
        super().__init__()
        # If `z` is None, its type cannot be inferred, so it must
        # be specified (above)
        self.z = z

    def forward(self, x, y, z):
        # type: (Optional[int], Optional[int], Optional[int]) -> int
        if x is None:
            x = 1
            x = x + 1

        # Refinement for an attribute by assigning it to a local
        z = self.z
        if y is not None and z is not None:
            x = y + z

        # Refinement via an `assert`
        assert z is not None
        x += z
        return x

module = torch.jit.script(M(2))
module = torch.jit.script(M(None))
```

----------------------------------------

TITLE: Saving ONNX Model to File (Python)
DESCRIPTION: Uses the `save()` method of the `ONNXProgram` object to serialize the optimized ONNX model into a Protobuf file on disk. Takes the desired file path as an argument. Requires an `ONNXProgram` object.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_dynamo.rst#_snippet_3

LANGUAGE: python
CODE:
```
onnx_program.save("mlp.onnx")
```

----------------------------------------

TITLE: Defining a Function with Global State in torch.func (Discouraged Practice) - PyTorch - Python
DESCRIPTION: This Python snippet demonstrates an incorrect pattern for users of torch.func transforms: the function 'f' assigns intermediate results to a global variable, which is not compatible with torch.func's execution model. The pattern showcases the risks of using mutation or side effects inside functions being transformed. It requires PyTorch and torch.func. The function takes a tensor 'x' as input and relies on global state for intermediate values, which can lead to unexpected results or errors under function transformations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from torch.func import grad

# Don't do this
intermediate = None

def f(x):
  global intermediate
  intermediate = x.sin()
  z = intermediate.sin()
  return z

x = torch.randn([])
grad_x = grad(f)(x)
```

----------------------------------------

TITLE: Using ATen Tanh and Its Backward Operator in PyTorch: Python
DESCRIPTION: This code exemplifies the application of ATen's 'tanh' and 'tanh_backward' operators to process tensors with specific dimensions and data types (f16). It focuses on evaluating the tanh function and its backward pass, crucial for neural network activation and backpropagation tasks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_14

LANGUAGE: Python
CODE:
```
Operator: aten.tanh.default
cnt: 1, ((T([3, 1, 512, 512], f16),), {})
Operator: aten.tanh_backward.default
cnt: 1, ((T([3, 1, 512, 512], f16, stride=(0, 0, 0, 0)), T([3, 1, 512, 512], f16)), {})
```

----------------------------------------

TITLE: Basic functorch Sanity Check Example
DESCRIPTION: Simple example demonstrating basic functorch usage with vmap transform on sine function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/README.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from functorch import vmap
x = torch.randn(3)
y = vmap(torch.sin)(x)
assert torch.allclose(y, x.sin())
```

----------------------------------------

TITLE: Creating Empty Tensors in PyTorch
DESCRIPTION: These code snippets show various methods to create empty tensors in PyTorch. When fill_uninitialized_memory is True and deterministic algorithms are enabled, these operations will fill uninitialized memory with known values.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/deterministic.rst#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
torch.empty()
torch.empty_strided()
torch.empty_permuted()
torch.empty_like()
```

----------------------------------------

TITLE: Combined Forward and Context Linear Function Implementation
DESCRIPTION: Demonstrates how to implement a custom Linear function with combined forward and context setup, including full backward pass implementation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
class LinearFunction(Function):
    @staticmethod
    # ctx is the first argument to forward
    def forward(ctx, input, weight, bias=None):
        # The forward pass can use ctx.
        ctx.save_for_backward(input, weight, bias)
        output = input.mm(weight.t())
        if bias is not None:
            output += bias.unsqueeze(0).expand_as(output)
        return output

    @staticmethod
    def backward(ctx, grad_output):
        input, weight, bias = ctx.saved_tensors
        grad_input = grad_weight = grad_bias = None

        if ctx.needs_input_grad[0]:
            grad_input = grad_output.mm(weight)
        if ctx.needs_input_grad[1]:
            grad_weight = grad_output.t().mm(input)
        if bias is not None and ctx.needs_input_grad[2]:
            grad_bias = grad_output.sum(0)

        return grad_input, grad_weight, grad_bias
```

----------------------------------------

TITLE: Backward Pass Stream Semantics - Unsafe Use Across Streams - PyTorch Python
DESCRIPTION: This snippet shows an unsafe pattern. `loss.backward()` is called within a custom stream context (`with torch.cuda.stream(s)`), but the resulting gradients (`use grads`) are used outside that context, presumably on the default stream, without explicit synchronization. This can lead to errors as the default stream might attempt to use gradients before the backward pass on stream `s` is complete.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_16

LANGUAGE: Python
CODE:
```
with torch.cuda.stream(s):
    loss.backward()
use grads
```

----------------------------------------

TITLE: Conducting Convolutions in ATen with PyTorch
DESCRIPTION: Performs 2D convolutions on input tensors, essential for feature extraction in CNNs. Dependencies involve input and weight tensors with specific shapes, strides, padding options, and other parameters. Outputs are processed tensors based on the input and kernel configurations. Limitations include the requirement for structured input shapes and parameter configurations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
Operator: aten.convolution.default
cnt: 1, ((T([16, 8, 128, 128], f16), T([64, 8, 7, 7], f16), None, [1, 1], [3, 3], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([16, 64, 128, 128], f16), T([128, 64, 4, 4], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([16, 128, 64, 64], f16), T([256, 128, 4, 4], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 12, ((T([16, 256, 32, 32], f16), T([256, 256, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([16, 256, 32, 32], f16), T([256, 128, 4, 4], f16), None, [2, 2], [1, 1], [1, 1], True, [0, 0], 1), {})
cnt: 1, ((T([16, 128, 64, 64], f16), T([128, 64, 4, 4], f16), None, [2, 2], [1, 1], [1, 1], True, [0, 0], 1), {})
cnt: 1, ((T([16, 64, 128, 128], f16), T([3, 64, 7, 7], f16), None, [1, 1], [3, 3], [1, 1], False, [0, 0], 1), {})
```

----------------------------------------

TITLE: Analyzing ReLU Activation Operations in PyTorch
DESCRIPTION: This snippet demonstrates the usage of ReLU (Rectified Linear Unit) activation operations on tensors with various shapes. It shows in-place ReLU operations on 4D tensors with different channel and spatial dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/shufflenet_v2_x1_0_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
Operator: aten.relu_.default
cnt: 1, ((T([128, 24, 112, 112], f16),), {})
cnt: 8, ((T([128, 58, 28, 28], f16),), {})
cnt: 1, ((T([128, 58, 56, 56], f16),), {})
cnt: 16, ((T([128, 116, 14, 14], f16),), {})
cnt: 1, ((T([128, 116, 28, 28], f16),), {})
cnt: 8, ((T([128, 232, 7, 7], f16),), {})
cnt: 1, ((T([128, 232, 14, 14], f16),), {})
cnt: 1, ((T([128, 1024, 7, 7], f16),), {})
```

----------------------------------------

TITLE: Dynamic Shapes Usage in PyTorch
DESCRIPTION: Demonstrates using dynamic shapes to handle varying tensor dimensions without recompilation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import torch

@torch.compile(dynamic=True)
def fn(x):
    return x + 1

fn(torch.ones(3, 3))
fn(torch.ones(4, 4))
```

----------------------------------------

TITLE: Preparing Data and Computing Jacobian Using compute_jac - Python
DESCRIPTION: Clones and sets requirement for gradient on input vector xp, defines unit vectors as identity matrix, and computes Jacobian using 'compute_jac'. Prints shape and first row of the resulting Jacobian for inspection. Dependencies: torch, predict, compute_jac, weight, bias. Outputs: Jacobian matrix and sample row.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
xp = x.clone().requires_grad_()
unit_vectors = torch.eye(D)

jacobian = compute_jac(xp)

print(jacobian.shape)
print(jacobian[0])  # show first row
```

----------------------------------------

TITLE: Stacking Tensors using ATen in Python
DESCRIPTION: Demonstrates the usage of the ATen \"aten.stack.default\" operator to stack multiple tensors along a new dimension. The example works with f16 tensors of shape [128, 32, 112, 112]. It requires ATen to be available within the PyTorch environment.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/visformer_small_training.txt#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([128, 32, 112, 112], f16),), {})
Operator: aten.stack.default
```

----------------------------------------

TITLE: Decomposing PyTorch Operations using FX Proxy
DESCRIPTION: This code demonstrates how to decompose PyTorch operations into smaller constituent operations using FX Proxy. It specifically shows how to transform F.relu(x) into (x > 0) * x by creating decomposition rules and applying them during graph transformation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
# Note that this decomposition rule can be read as regular Python
def relu_decomposition(x):
    return (x > 0) * x

decomposition_rules = {}
decomposition_rules[F.relu] = relu_decomposition

def decompose(model: torch.nn.Module,
              tracer_class : type = fx.Tracer) -> torch.nn.Module:
    """
    Decompose `model` into smaller constituent operations.
    Currently,this only supports decomposing ReLU into its
    mathematical definition: (x > 0) * x
    """
    graph : fx.Graph = tracer_class().trace(model)
    new_graph = fx.Graph()
    env = {}
    tracer = torch.fx.proxy.GraphAppendingTracer(new_graph)
    for node in graph.nodes:
        if node.op == 'call_function' and node.target in decomposition_rules:
            # By wrapping the arguments with proxies,
            # we can dispatch to the appropriate
            # decomposition rule and implicitly add it
            # to the Graph by symbolically tracing it.
            proxy_args = [
                fx.Proxy(env[x.name], tracer) if isinstance(x, fx.Node) else x for x in node.args]
            output_proxy = decomposition_rules[node.target](*proxy_args)

            # Operations on `Proxy` always yield new `Proxy`s, and the
            # return value of our decomposition rule is no exception.
            # We need to extract the underlying `Node` from the `Proxy`
            # to use it in subsequent iterations of this transform.
            new_node = output_proxy.node
            env[node.name] = new_node
        else:
            # Default case: we don't have a decomposition rule for this
            # node, so just copy the node over into the new graph.
            new_node = new_graph.node_copy(node, lambda x: env[x.name])
            env[node.name] = new_node
    return fx.GraphModule(model, new_graph)
```

----------------------------------------

TITLE: Disabling TF32 Matmul and cuDNN (Python)
DESCRIPTION: Shows the Python code to turn off the `allow_tf32` flags for both CUDA matrix multiplication (`torch.backends.cuda.matmul`) and cuDNN operations (`torch.backends.cudnn`). Disabling these ensures that full FP32 precision is used for these computations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_3

LANGUAGE: python
CODE:
```
torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.allow_tf32 = False
```

----------------------------------------

TITLE: PyTorch Conv-BN Fusion Example
DESCRIPTION: Example showing how to use torch.ao.quantization.fuse_modules API for fusing Conv-BN or Linear-BN patterns in Eager mode quantization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization-accuracy-debugging.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
torch.ao.quantization.fuse_modules
```

----------------------------------------

TITLE: Forward Pass of a Reference Quantized Model in PyTorch (Python)
DESCRIPTION: Shows the structure and `forward` method of a `GraphModule` representing a reference quantized model in PyTorch. It demonstrates how the conversion process replaces QDQStubs with explicit `torch.quantize_per_tensor` and `dequantize` operations and uses reference quantized modules like `QuantizedLinear`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#_snippet_8

LANGUAGE: Python
CODE:
```
quantized: GraphModule(
  (linear): LinearReLU(
    (0): QuantizedLinear(Reference)(in_features=5, out_features=10, bias=True)
    (1): ReLU()
  )
)

def forward(self, x):
    linear_input_scale_0 = self.linear_input_scale_0
    linear_input_zero_point_0 = self.linear_input_zero_point_0
    quantize_per_tensor = torch.quantize_per_tensor(x, linear_input_scale_0, linear_input_zero_point_0, torch.quint8);  x = linear_input_scale_0 = linear_input_zero_point_0 = None
    dequantize = quantize_per_tensor.dequantize();  quantize_per_tensor = None
    linear = self.linear(dequantize);  dequantize = None
    linear_scale_0 = self.linear_scale_0
    linear_zero_point_0 = self.linear_zero_point_0
    quantize_per_tensor_1 = torch.quantize_per_tensor(linear, linear_scale_0, linear_zero_point_0, torch.quint8);  linear = linear_scale_0 = linear_zero_point_0 = None
    dequantize_1 = quantize_per_tensor_1.dequantize();  quantize_per_tensor_1 = None
    return dequantize_1
```

----------------------------------------

TITLE: Implementing aten.split_with_sizes in PyTorch
DESCRIPTION: The aten.split_with_sizes.default is a PyTorch operation utilized for partitioning tensors into smaller parts along a specified axis, based on given chunk sizes in the snippet. Dependencies include PyTorch. Inputs involve tensor and designated sizes list with dimension index. The result includes smaller tensors as per partitions. Ensure inputs comply with total dimension size constraints.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_16

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 32, 112, 112], f16), [16, 16], 1), {})
```

----------------------------------------

TITLE: Defining CNN Model Structure in PyTorch
DESCRIPTION: Implementation of a simple Convolutional Neural Network (CNN) with 3 conv layers and 1 fully connected layer to demonstrate NTK computation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/neural_tangent_kernels.ipynb#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
class CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, (3, 3))
        self.conv2 = nn.Conv2d(32, 32, (3, 3))
        self.conv3 = nn.Conv2d(32, 32, (3, 3))
        self.fc = nn.Linear(21632, 10)
        
    def forward(self, x):
        x = self.conv1(x)
        x = x.relu()
        x = self.conv2(x)
        x = x.relu()
        x = self.conv3(x)
        x = x.flatten(1)
        x = self.fc(x)
        return x
```

----------------------------------------

TITLE: Using a Custom Backend with torch.compile
DESCRIPTION: This snippet demonstrates how to apply a custom backend (in this case, the optimize_for_inference_compiler) to accelerate existing code using the torch.compile decorator.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_custom_backends.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
@torch.compile(backend=optimize_for_inference_compiler)
def code_to_accelerate():
    ...
```

----------------------------------------

TITLE: Launching Multiple Processes for Pipeline Parallelism in Bash
DESCRIPTION: This bash command demonstrates how to use torchrun to launch multiple processes for pipeline parallelism, specifically launching 2 processes per node to run the example script.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.pipelining.rst#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
torchrun --nproc_per_node=2 example.py
```

----------------------------------------

TITLE: Temporarily Set Default CUDA Device PyTorch
DESCRIPTION: Uses the `torch.cuda.device` context manager to temporarily change the default CUDA device for operations within the `with` block. Operations outside the block revert to the previous default device.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_37

LANGUAGE: python
CODE:
```
print("Outside device is 0")  # On device 0 (default in most scenarios)
with torch.cuda.device(1):
    print("Inside device is 1")  # On device 1
print("Outside device is still 0")  # On device 0
```

----------------------------------------

TITLE: Defining a Partitionable Transformer Model in Python
DESCRIPTION: This code snippet shows how to define a Transformer model that can be easily partitioned for pipeline parallelism. It uses a ModuleDict for layers and handles None values to enable easy splitting.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.pipelining.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
class Transformer(nn.Module):
    def __init__(self, model_args: ModelArgs):
        super().__init__()

        self.tok_embeddings = nn.Embedding(...)

        # Using a ModuleDict lets us delete layers without affecting names,
        # ensuring checkpoints will correctly save and load.
        self.layers = torch.nn.ModuleDict()
        for layer_id in range(model_args.n_layers):
            self.layers[str(layer_id)] = TransformerBlock(...)

        self.output = nn.Linear(...)

    def forward(self, tokens: torch.Tensor):
        # Handling layers being 'None' at runtime enables easy pipeline splitting
        h = self.tok_embeddings(tokens) if self.tok_embeddings else tokens

        for layer in self.layers.values():
            h = layer(h, self.freqs_cis)

        h = self.norm(h) if self.norm else h
        output = self.output(h).float() if self.output else h
        return output
```

----------------------------------------

TITLE: Backward Pass Stream Semantics - Unsafe Initial Grad Across Streams - PyTorch Python
DESCRIPTION: This snippet illustrates an unsafe pattern where the initial gradient tensor (`initial_grad`) is created outside the custom stream context, likely on the default stream, and then immediately used in a `backward()` call within the custom stream context (`with torch.cuda.stream(s)`), without synchronization. The `backward` call might attempt to read `initial_grad` before its creation on the default stream is complete.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_19

LANGUAGE: Python
CODE:
```
initial_grad = torch.ones_like(loss)
with torch.cuda.stream(s):
    loss.backward(gradient=initial_grad)
```

----------------------------------------

TITLE: Creating a PyTorch Module with Persistent Buffer
DESCRIPTION: Demonstrates how to create a PyTorch module with a persistent buffer using 'register_buffer', which is included in the state dictionary but not learnable.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
class RunningMean(nn.Module):
  def __init__(self, num_features, momentum=0.9):
    super().__init__()
    self.momentum = momentum
    self.register_buffer('mean', torch.zeros(num_features))
  def forward(self, x):
    self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x
    return self.mean

m = RunningMean(4)
for _ in range(10):
  input = torch.randn(4)
  m(input)

print(m.state_dict())
: OrderedDict([('mean', tensor([ 0.1041, -0.1113, -0.0647,  0.1515]))])))

# Serialized form will contain the 'mean' tensor
torch.save(m.state_dict(), 'mean.pt')

m_loaded = RunningMean(4)
m_loaded.load_state_dict(torch.load('mean.pt'))
assert(torch.all(m.mean == m_loaded.mean))
```

----------------------------------------

TITLE: Incorrectly Delaying Shared CUDA Tensor Memory Release (Python)
DESCRIPTION: Illustrates a bad practice where a shared CUDA tensor `x` received from a queue is not deleted promptly after use. Keeping the reference `x` alive while performing other tasks forces the producer process to maintain the original tensor in memory unnecessarily long, potentially leading to higher memory consumption.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/multiprocessing.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
## Bad
x = queue.get()
# do somethings with x
# do everything else (producer have to keep x in memory)
```

----------------------------------------

TITLE: Execute aten.native_batch_norm_backward in Python
DESCRIPTION: This code snippet represents the backward pass of the native batch normalization operation in PyTorch, indicating derivative calculations for the backward propagation through neural networks. Requirements include compatible versions of PyTorch. Inputs consist of tensors for data, weights, and additional configuration settings for training purposes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 1536, 7, 7], f16), T([64, 1536, 7, 7], f16), T([1536], f16), T([1536], f16), T([1536], f16), T([1536], f32), T([1536], f32), True, 0.001, [True, True, True]), {})
```

----------------------------------------

TITLE: Python Primitive Specialization in torch.export
DESCRIPTION: Example showing how torch.export specializes on Python primitive values like integers. The loop count and constant value are baked into the exported program rather than being treated as dynamic inputs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
import torch
from torch.export import export

class Mod(torch.nn.Module):
    def forward(self, x: torch.Tensor, const: int, times: int):
        for i in range(times):
            x = x + const
        return x

example_inputs = (torch.rand(2, 2), 1, 3)
exported_program = export(Mod(), example_inputs)
print(exported_program)
```

----------------------------------------

TITLE: Conditional Branch Example with Dynamic Shapes in PyTorch
DESCRIPTION: Example demonstrating how PyTorch handles conditional branching based on dynamic tensor sizes. Shows a function that performs different operations (multiplication or addition) based on the size of a concatenated tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamic_shapes.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
def f(x, y):
    z = torch.cat([x, y])
    if z.size(0) > 2:
        return z.mul(2)
    else:
        return z.add(2)
```

----------------------------------------

TITLE: Sum Operations in PyTorch
DESCRIPTION: This snippet shows sum operations on tensors with various shapes and dimensions. It includes operations that reduce tensors along specified dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_10

LANGUAGE: Python
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([64, 196, 1000], f16), [0, 1], True), {})
cnt: 1, ((T([64, 1000], f16), [0], True), {})
cnt: 2, ((T([64, 384], f16, stride=(75648, 1)), [0], True), {})
cnt: 2, ((T([64, 1152], f16), [0], True), {})
cnt: 2, ((T([64, 384], f16), [0], True), {})
cnt: 1, ((T([64, 1, 384], f16, stride=(75648, 384, 1)), [0], True), {})
cnt: 1, ((T([64, 14, 14, 384], f16, stride=(75648, 5376, 384, 1)), [0, 1, 2], True), {})
cnt: 14, ((T([64, 14, 14, 1152], f16), [0, 1, 2], True), {})
cnt: 27, ((T([64, 14, 14, 384], f16), [0, 1, 2], True), {})
cnt: 1, ((T([64, 14, 14, 384], f16), [0], True), {})
cnt: 8, ((T([64, 28, 28, 192], f16, stride=(150528, 28, 1, 784)), [0, 1, 2], True), {})
cnt: 4, ((T([64, 28, 28, 576], f16), [0, 1, 2], True), {})
cnt: 4, ((T([64, 14, 14, 486], f16), [0, 1, 2], True), {})
```

----------------------------------------

TITLE: Demonstrating Copy Epilogue in Python for FX Graph Pass
DESCRIPTION: This code snippet shows a function with a clone operation followed by a copy operation. It illustrates that the clone operation cannot be eliminated due to the copy epilogue, as it would cause unintended aliasing between input and output.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/_inductor/fx_passes/README.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
def f(x: Tensor):
    y = x.clone()
    x.copy_(y)
    return y
```

----------------------------------------

TITLE: Automatic Model Splitting for Pipeline Parallelism in Python
DESCRIPTION: This snippet demonstrates how to use the pipeline API to automatically split a model for pipeline parallelism. It specifies a split point before a specific layer and creates a pipeline object.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.pipelining.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from torch.distributed.pipelining import pipeline, SplitPoint

# An example micro-batch input
x = torch.LongTensor([1, 2, 4, 5])

pipe = pipeline(
    module=mod,
    mb_args=(x,),
    split_spec={
        "layers.1": SplitPoint.BEGINNING,
    }
)
```

----------------------------------------

TITLE: Adding Data-Dependent Shape Assertions with torch._check in PyTorch (Python)
DESCRIPTION: These code snippets illustrate handling a data-dependent dynamic shape (result of torch.nonzero) when tracing a torch.nn.Module. The old version uses a standard Python if-statement to branch based on nz.shape[0] > 0, which is not export-safe. The new version introduces torch._check(nz.shape[0] > 0) as an explicit assertion, making the branch traceable under the condition checked. Both modules require PyTorch, and the input x is a Tensor. The output is x.sin() if the assertion passes, otherwise x.cos(). torch._check is required for the assertion. Inputs should be tensors suitable for nonzero().
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.programming_model.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
class M_old(torch.nn.Module):
    def forward(self, x):
        nz = x.nonzero()
        if nz.shape[0] > 0:
            return x.sin()
        else:
            return x.cos()
```

LANGUAGE: python
CODE:
```
class M_new(torch.nn.Module):
    def forward(self, x):
        nz = x.nonzero()
        torch._check(nz.shape[0] > 0)
        if nz.shape[0] > 0:
            return x.sin()
        else:
            return x.cos()
```

----------------------------------------

TITLE: Applying aten._to_copy with dtype Conversion in PyTorch
DESCRIPTION: The aten._to_copy.default operator example demonstrates converting a tensor of shape [16, 1, 1, 128] from f32 to f16, leveraging PyTorch functionalities. Prerequisites include correct installation of PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MobileBertForMaskedLM_training.txt#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
aten._to_copy.default, ((T([16, 1, 1, 128], f32),), {'dtype': f16})
```

----------------------------------------

TITLE: Demonstrating CUDAGraph Tree Execution Flow in PyTorch
DESCRIPTION: This code snippet illustrates how CUDAGraph Trees handle different execution paths and graph breaks in a PyTorch function. It shows the process of graph warming, recording, and replay for optimized execution.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_cudagraph_trees.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
@torch.compile(mode="reduce-overhead")
def foo(x):
    # GRAPH 1
    y = x * x * x
    # graph break triggered here
    if y.sum() > 0:
        # GRAPH 2
        z = y ** y
    else:
        # GRAPH 3
        z = (y.abs() ** y.abs())
    torch._dynamo.graph_break()
    # GRAPH 4
    return z * torch.rand_like(z)

# the first run warms up each graph, which does things like CuBlas or Triton benchmarking
foo(torch.arange(0, 10, device="cuda"))
# The second run does a CUDA Graph recording, and replays it
foo(torch.arange(0, 10, device="cuda"))
# Finally we hit the optimized, CUDA Graph replay path
foo(torch.arange(0, 10, device="cuda"))
```

----------------------------------------

TITLE: Guard Generation Example in PyTorch
DESCRIPTION: Demonstrates how guards are generated for a simple torch.compile function to check input tensor properties.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch

@torch.compile
def fn(x):
    return x + 1

fn(torch.ones(3, 3))
```

----------------------------------------

TITLE: Tracing ATen Tensor Arithmetic Operators in PyTorch Model (Python)
DESCRIPTION: Captures all invocations of arithmetic-related ATen operators (add, add_, addmm, div) as used in a typical PyTorch model, logging argument tensor shapes, dtypes, and optional operator options. Dependencies: PyTorch (ATen backend), model code with mixed-precision tensors. Accepts input tensor pairs or tensors with scalars and returns the arithmetic result in specified dtype and shape; scalar variants work elementwise. Limitations: Only includes tensor shapes/types, not the actual computational graph or code.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet18_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
Operator: aten.add.Tensor
cnt: 1, ((T([16, 512, 7, 7], f16), T([16, 512, 7, 7], f16)), {})
cnt: 2, ((T([16, 256, 14, 14], f16), T([16, 256, 14, 14], f16)), {})
cnt: 2, ((T([16, 128, 28, 28], f16), T([16, 128, 28, 28], f16)), {})
cnt: 3, ((T([16, 64, 56, 56], f16), T([16, 64, 56, 56], f16)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.add_.Tensor
cnt: 2, ((T([16, 64, 56, 56], f16), T([16, 64, 56, 56], f16)), {})
cnt: 2, ((T([16, 128, 28, 28], f16), T([16, 128, 28, 28], f16)), {})
cnt: 2, ((T([16, 256, 14, 14], f16), T([16, 256, 14, 14], f16)), {})
cnt: 2, ((T([16, 512, 7, 7], f16), T([16, 512, 7, 7], f16)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.addmm.default
cnt: 1, ((T([1000], f16), T([16, 512], f16), T([512, 1000], f16, stride=(1, 512))), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.div.Scalar
cnt: 1, ((T([16, 512, 7, 7], f16, stride=(512, 1, 0, 0)), 49), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.div.Tensor
cnt: 2, ((T([], f16), 16000), {})
```

----------------------------------------

TITLE: Unsafe View Reshaping of Tensors in PyTorch (Python)
DESCRIPTION: Executes aten._unsafe_view for changing the shape of tensors without copying the data, which is preferred when transforming intermediate tensors within a computational graph requiring specific input sizes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
aten._unsafe_view.default
cnt: 36, ((T([16, 12, 512, 64], f16), [192, 512, 64]), {})
cnt: 12, ((T([16, 12, 64, 512], f16), [192, 64, 512]), {})
cnt: 12, ((T([192, 512, 512], f16), [16, 12, 512, 512]), {})
cnt: 12, ((T([192, 512, 64], f16), [16, 12, 512, 64]), {})
cnt: 24, ((T([16, 512, 12, 64], f16), [16, 512, 768]), {})
cnt: 12, ((T([16, 512, 768], f16), [8192, 768]), {})
```

----------------------------------------

TITLE: Benchmarking TF32 Matmul Precision (Python)
DESCRIPTION: Provides a Python code example to benchmark the performance and numerical error of matrix multiplication comparing double precision, float32 with TF32 enabled, and float32 with TF32 disabled. It demonstrates the potential speedup on compatible hardware (like A100) versus the trade-off in numerical precision.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_2

LANGUAGE: python
CODE:
```
a_full = torch.randn(10240, 10240, dtype=torch.double, device='cuda')
b_full = torch.randn(10240, 10240, dtype=torch.double, device='cuda')
ab_full = a_full @ b_full
mean = ab_full.abs().mean()  # 80.7277

a = a_full.float()
b = b_full.float()

# Do matmul at TF32 mode.
torch.backends.cuda.matmul.allow_tf32 = True
ab_tf32 = a @ b  # takes 0.016s on GA100
error = (ab_tf32 - ab_full).abs().max()  # 0.1747
relative_error = error / mean  # 0.0022

# Do matmul with TF32 disabled.
torch.backends.cuda.matmul.allow_tf32 = False
ab_fp32 = a @ b  # takes 0.11s on GA100
error = (ab_fp32 - ab_full).abs().max()  # 0.0031
relative_error = error / mean  # 0.000039
```

----------------------------------------

TITLE: Example of Structured Pruned Weight Tensor
DESCRIPTION: Demonstrates structured pruning where entire rows of a weight tensor are zeroed out, which allows for matrix resizing and computational efficiency gains.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/pruner/README.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
W_pruned = [[0 0 0] = [[4, 5, 6],
            [4 5 6]    [7, 1, 9]]
            [7 1 9]]
```

----------------------------------------

TITLE: GELU Activation Computation in PyTorch (Python)
DESCRIPTION: Involves the computation of the Gaussian Error Linear Unit (GELU) activation function using aten.gelu, preferred in transformer models for its better performance in comparison to other activations like ReLU.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_14

LANGUAGE: Python
CODE:
```
aten.gelu.default
cnt: 12, ((T([16, 512, 3072], f16),), {})
```

----------------------------------------

TITLE: Set Up Conda Environment on Windows for Source Build
DESCRIPTION: This code snippet provides the Command Prompt commands to set up a Conda environment and configure the necessary Visual Studio build tools on Windows. These steps are prerequisites for building PyTorch from source on a Windows system.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_1

LANGUAGE: Batch
CODE:
```
$ source <CONDA_INSTALL_DIR>\Scripts\activate.bat
$ conda create -y -n <CONDA_NAME>
$ conda activate <CONDA_NAME>
$ call "C:\Program Files\Microsoft Visual Studio\<VERSION>\Community\VC\Auxiliary\Build\vcvarsall.bat" x64
```

----------------------------------------

TITLE: Complete SWA Training Loop Implementation
DESCRIPTION: Full example of implementing SWA training including model creation, learning rate scheduling, and batch norm updates.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_16

LANGUAGE: Python
CODE:
```
loader, optimizer, model, loss_fn = ...
swa_model = torch.optim.swa_utils.AveragedModel(model)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)
swa_start = 160
swa_scheduler = SWALR(optimizer, swa_lr=0.05)

for epoch in range(300):
      for input, target in loader:
          optimizer.zero_grad()
          loss_fn(model(input), target).backward()
          optimizer.step()
      if epoch > swa_start:
          swa_model.update_parameters(model)
          swa_scheduler.step()
      else:
          scheduler.step()

# Update bn statistics for the swa_model at the end
torch.optim.swa_utils.update_bn(loader, swa_model)
# Use swa_model to make predictions on test data
preds = swa_model(test_input)
```

----------------------------------------

TITLE: Distributed Autograd Context Example in PyTorch
DESCRIPTION: Shows basic distributed autograd usage including context creation, remote computation, backward pass and gradient retrieval. Uses RPC for remote operations and demonstrates core distributed autograd concepts.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/distributed_autograd.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
with dist_autograd.context() as context_id:
    t1 = torch.rand((3, 3), requires_grad=True)
    t2 = torch.rand((3, 3), requires_grad=True)

    # Perform some computation remotely.
    t3 = rpc.rpc_sync("worker1", my_add, args=(t1, t2))

    # Perform some computation locally based on remote result.
    t4 = torch.rand((3, 3), requires_grad=True)
    t5 = torch.mul(t3, t4)

    # Compute some loss.
    loss = t5.sum()

    # Run the backward pass.
    dist_autograd.backward(context_id, [loss])

    # Retrieve the gradients from the context.
    dist_autograd.get_gradients(context_id)
```

----------------------------------------

TITLE: Dynamic Shapes Support with Nested Jagged Tensors in PyTorch
DESCRIPTION: Demonstrates how NJTs support dynamic shapes with torch.compile to avoid unnecessary recompiles when the ragged structure changes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> a = torch.randn(2, 3)
>>> b = torch.randn(4, 3)
>>> c = torch.randn(5, 3)
>>> d = torch.randn(6, 3)
>>> nt1 = torch.nested.nested_tensor([a, b], layout=torch.jagged)
>>> nt2 = torch.nested.nested_tensor([c, d], layout=torch.jagged)
>>> def f(x): return x.sin() + 1
...
>>> compiled_f = torch.compile(f, fullgraph=True)
>>> output1 = compiled_f(nt1)
>>> output2 = compiled_f(nt2)  # NB: No recompile needed even though ragged structure differs
```

----------------------------------------

TITLE: NLL Loss Operations
DESCRIPTION: Negative Log Likelihood Loss calculations in both forward and backward passes, operating on classification outputs with 1000 classes and batch size of 64.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
((T([], f16), T([64, 1000], f16), T([64], i64), None, 1, -100, T([], f16)), {})
((T([64, 1000], f16), T([64], i64), None, 1, -100), {})
```

----------------------------------------

TITLE: Using allow_in_graph for Unsupported Functions
DESCRIPTION: Example of using torch._dynamo.allow_in_graph as an escape hatch for functions that don't work directly with torch.compile but can be symbolically traced.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
import torch

@torch.compile
def f(x):
    return torch._dynamo.allow_in_graph(torch.vmap(torch.sum))(x)

x = torch.randn(2, 3)
f(x)
```

----------------------------------------

TITLE: Defining Module Attributes with Type Annotations in TorchScript
DESCRIPTION: This snippet illustrates how to define module attributes in TorchScript, including empty lists and dictionaries that require explicit type annotations. It also shows how to use these attributes in the forward method.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
from typing import List, Dict

class Foo(nn.Module):
    # `words` is initialized as an empty list, so its type must be specified
    words: List[str]

    # The type could potentially be inferred if `a_dict` (below) was not
    # empty, but this annotation ensures `some_dict` will be made into the
    # proper type
    some_dict: Dict[str, int]

    def __init__(self, a_dict):
        super().__init__()
        self.words = []
        self.some_dict = a_dict

        # `int`s can be inferred
        self.my_int = 10

    def forward(self, input):
        # type: (str) -> int
        self.words.append(input)
        return self.some_dict[input] + self.my_int

f = torch.jit.script(Foo({'hi': 2}))
```

----------------------------------------

TITLE: Registering and Implementing a Custom PyTorch Operator with Fake (Meta) Implementation (Python)
DESCRIPTION: These snippets define a custom sin operator in the mylib namespace using torch.library.custom_op, with a NumPy-backed implementation for actual tensor data. A fake (meta) implementation is also registered with torch.library.register_fake for tracing with FakeTensor, returning an empty tensor of the correct shape. Dependencies: PyTorch and NumPy are required for the main operator. The primary input x is a Tensor. Outputs are tensors resulting from applying np.sin or torch.empty_like to x. The fake implementation should match the shape and metadata of the main operator, supporting tracing/export and custom op integration.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.programming_model.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
@torch.library.custom_op("mylib::sin", mutates_args=())
def sin(x: Tensor) -> Tensor:
    x_np = x.numpy()
    y_np = np.sin(x_np)
    return torch.from_numpy(y_np)
```

LANGUAGE: python
CODE:
```
@torch.library.register_fake("mylib::sin")
def _(x: Tensor) -> Tensor:
    return torch.empty_like(x)
```

----------------------------------------

TITLE: Running Multithreading in PyTorch with Autograd
DESCRIPTION: Demonstrates using Python threads to run training function in parallel leveraging PyTorch's autograd. The train_fn defines forward and backward operations, and threading is used to manage multiple executions. Key considerations include avoiding shared state issues and understanding concurrency constraints.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/autograd.rst#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
# Define a train function to be used in different threads
def train_fn():
    x = torch.ones(5, 5, requires_grad=True)
    # forward
    y = (x + 3) * (x + 4) * 0.5
    # backward
    y.sum().backward()
    # potential optimizer update


# User write their own threading code to drive the train_fn
threads = []
for _ in range(10):
    p = threading.Thread(target=train_fn, args=())
    p.start()
    threads.append(p)

for p in threads:
    p.join()
```

----------------------------------------

TITLE: Shuffling Data with PyTorch DataPipes
DESCRIPTION: Shows how to use the shuffle() method to randomize the order of data in DataPipes. Demonstrates shuffling at different levels and across batches.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(10).shuffle()
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(10).batch(3).shuffle()
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(10).batch(3).shuffle(unbatch_level = -1).batch(3)
for i in dp:
    print(i)
```

----------------------------------------

TITLE: Compiling torch.vmap with torch.compile
DESCRIPTION: Shows how to use torch.compile with torch.vmap for vectorized operations on tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import torch

def my_fn(x):
    return torch.vmap(lambda x: x.sum(1))(x)

x = torch.randn(3, 3, 3)
output = torch.compile(my_fn)(x)
```

----------------------------------------

TITLE: Incorrect Concurrent Stream Usage - PyTorch Python
DESCRIPTION: This snippet shows an incorrect pattern when using a non-default CUDA stream (`s`). Operations on the default stream (creating and populating `A`) and the custom stream (computing `sum(A)`) can execute concurrently. Without explicit synchronization, the `sum()` operation may start before `normal_()` on `A` finishes, leading to incorrect results or errors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_12

LANGUAGE: Python
CODE:
```
cuda = torch.device('cuda')
s = torch.cuda.Stream()  # Create a new stream.
A = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)
with torch.cuda.stream(s):
    # sum() may start execution before normal_() finishes!
    B = torch.sum(A)
```

----------------------------------------

TITLE: Defining and Printing Graphs in PyTorch FX in Python
DESCRIPTION: This snippet shows how to define a PyTorch nn.Module, trace it to acquire its graph, and print the graph's tabular structure. It relies on 'torch' and 'torch.fx', and inputs an instance of a PyTorch module without any constraints or transformations. This operation returns a printed representation of the graph with node details, aiding in graph analysis and debugging.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch
import torch.fx

class MyModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.param = torch.nn.Parameter(torch.rand(3, 4))
        self.linear = torch.nn.Linear(4, 5)

    def forward(self, x):
        return torch.topk(torch.sum(
            self.linear(x + self.linear.weight).relu(), dim=-1), 3)

m = MyModule()
gm = torch.fx.symbolic_trace(m)

gm.graph.print_tabular()
```

----------------------------------------

TITLE: Optimizing ONNX Program (Python)
DESCRIPTION: Calls the `optimize()` method on the exported `ONNXProgram` object. This performs in-place optimizations on the captured ONNX graph, such as constant folding and elimination of redundant operators. Requires an `ONNXProgram` object obtained from `torch.onnx.export`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_dynamo.rst#_snippet_2

LANGUAGE: python
CODE:
```
onnx_program.optimize()
```

----------------------------------------

TITLE: PyTorch Neural Network Operations - Embeddings and Activation
DESCRIPTION: Neural network specific operations including embedding lookups, GELU activation functions, and their backward passes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_Bert_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
aten.embedding.default((T([30522, 768], f16), T([4, 512], i64), 0), {})
aten.gelu.default((T([4, 512, 3072], f16),), {})
aten.native_layer_norm.default((T([4, 512, 768], f16), [768], T([768], f16), T([768], f16), 1e-12), {})
```

----------------------------------------

TITLE: Bad Practice: Using Tensor.item() in Traced Code (Avoid)
DESCRIPTION: This snippet shows a bad practice: using `Tensor.item()` within a model intended for tracing. `Tensor.item()` converts a tensor to a Python built-in number, which is then treated as a constant during tracing, leading to incorrect behavior if the tensor value should change.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_5

LANGUAGE: python
CODE:
```
def forward(self, x, y):
    return x.reshape(y.item(), -1)
```

----------------------------------------

TITLE: Tensor Mean Operation
DESCRIPTION: Calculation of mean values across specific dimensions of tensors with various shapes. Operations are performed on half-precision (f16) tensors with dimension parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
cnt: 1, ((T([64, 240, 28, 28], f16), [2, 3], True), {})
```

----------------------------------------

TITLE: Initializing Modules on Meta Device with Context Manager in Python
DESCRIPTION: Shows how the `torch.device('meta')` context manager affects module initialization. When a `torch.nn.Module` is created within this context, its parameters are placed on the 'meta' device, allowing model structure inspection without memory usage.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/meta.rst#_snippet_2

LANGUAGE: Python
CODE:
```
>>> from torch.nn.modules import Linear
>>> with torch.device('meta'):
...     print(Linear(20, 30))
...
Linear(in_features=20, out_features=30, bias=True)
```

----------------------------------------

TITLE: Install Python Dependencies (bash)
DESCRIPTION: Installs Python packages listed in the `requirements.txt` file using the `pip` package manager. This command is necessary to install required dependencies for running PyTorch tests and linting checks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_12

LANGUAGE: bash
CODE:
```
pip install -r requirements.txt
```

----------------------------------------

TITLE: Computing Hessians by Composing Jacobian Transforms in Python
DESCRIPTION: This snippet demonstrates how to compute the Hessian matrix (matrix of second-order partial derivatives) by composing Jacobian transforms. It defines a function `f` that sums the sine of input elements. The Hessian is computed in two ways: first by composing `jacrev` with itself (`jacrev(jacrev(f))`), and second by composing `jacfwd` with `jacrev` (`jacfwd(jacrev(f))`). Both methods yield the Hessian matrix for the function `f` evaluated at `x`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.whirlwind_tour.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
def f(x):
    return x.sin().sum()

x = torch.randn(5)
hessian0 = jacrev(jacrev(f))(x)
hessian1 = jacfwd(jacrev(f))(x)
```

----------------------------------------

TITLE: Python In-place Operation Tracing Example
DESCRIPTION: Shows how to handle in-place tensor operations during tracing by replacing them with out-of-place alternatives.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
def fill_row_zero(x):
    x[0] = torch.rand(*x.shape[1:2])
    return x

traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))
print(traced.graph)

# Fixed version:
def fill_row_zero(x):
    x = torch.cat((torch.rand(1, *x.shape[1:2]), x[1:2]), dim=0)
    return x
```

----------------------------------------

TITLE: PyTorch Tensor Operations Log
DESCRIPTION: Log of PyTorch operator calls showing operation type, count of executions, and tensor shapes/parameters. Includes core operations like convolutions, pooling, normalizations used in ConvNeXt architecture.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/poolformer_m36_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([64, 1000], f16), 1, False), {})
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})
Operator: aten.add.Tensor
cnt: 30, ((T([64, 96, 56, 56], f16), T([64, 96, 56, 56], f16)), {})
cnt: 30, ((T([64, 192, 28, 28], f16), T([64, 192, 28, 28], f16)), {})
cnt: 90, ((T([64, 384, 14, 14], f16), T([64, 384, 14, 14], f16)), {})
cnt: 30, ((T([64, 768, 7, 7], f16), T([64, 768, 7, 7], f16)), {})
```

----------------------------------------

TITLE: Constant Folding with Static Inputs in torch.export (Python)
DESCRIPTION: This snippet demonstrates how `torch.export.export` handles static primitive inputs. When the module `MyMod` is exported with a static integer `3` for `y`, the operation `y + 7` is constant-folded during tracing, resulting in the value `10` being directly embedded into the exported graph. The `add` operation in the final graph uses this constant `10`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.programming_model.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch

class MyMod(torch.nn.Module):
    def forward(self, x, y):
        z = y + 7
        return x + z

m = torch.export.export(MyMod(), (torch.randn(1), 3))
print(m.graph_module.code)

"""
def forward(self, arg0_1, arg1_1):
    add = torch.ops.aten.add.Tensor(arg0_1, 10);  arg0_1 = None
    return (add,)

"""
```

----------------------------------------

TITLE: Using torch.compiler.allow_in_graph for Complex Features
DESCRIPTION: Example showing how to mark functions containing hard-to-support features (like hooks or autograd.Function) to be included as-is in the TorchDynamo graph.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_fine_grain_apis.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
@torch.compiler.allow_in_graph
def function_with_hooks():
    # Function implementation with hooks
    pass
```

----------------------------------------

TITLE: Enumerating BatchNorm, Matrix Multiplication, and Reduction Cases - Python
DESCRIPTION: This bundle includes sample arguments for N-dimensional batch normalization, matrix multiplication (mm), and reduction operations (mean, mul) typical in deep network pipelines. Each tuple provides tensor shapes meant for functionally significant bottlenecks in training and inference, with batchnorm requiring parallel scale/bias/mean/variance vectors of size matching the normalized axis. Supported operations utilize scalar or per-element broadcasting and reduce dimensions as needed.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
Operator: aten.native_batch_norm.default
cnt: 3, ((T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), True, 0.1, 1e-05), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.mm.default
cnt: 1, ((T([128, 1000], f16), T([1000, 1280], f16)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.mean.dim
cnt: 1, ((T([128, 72, 28, 28], f16), [2, 3], True), {})
```

----------------------------------------

TITLE: Using Custom Quantization with Eager Mode
DESCRIPTION: Example showing how to use custom quantization modules with PyTorch's eager mode quantization workflow including prepare and convert steps.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_11

LANGUAGE: Python
CODE:
```
m = torch.nn.Sequential(CustomModule()).eval()
prepare_custom_config_dict = {
    "float_to_observed_custom_module_class": {
        CustomModule: ObservedCustomModule
    }
}
convert_custom_config_dict = {
    "observed_to_quantized_custom_module_class": {
        ObservedCustomModule: StaticQuantCustomModule
    }
}
m.qconfig = torch.ao.quantization.default_qconfig
mp = torch.ao.quantization.prepare(
    m, prepare_custom_config_dict=prepare_custom_config_dict)
# calibration (not shown)
mq = torch.ao.quantization.convert(
    mp, convert_custom_config_dict=convert_custom_config_dict)
```

----------------------------------------

TITLE: Initializing DDP in Side-Stream for CUDA Graph Capture (NCCL >= 2.9.6)
DESCRIPTION: Provides the required setup step for using DistributedDataParallel (DDP) with full-backward CUDA graph capture when using NCCL version 2.9.6 or later. It shows that the DDP wrapper must be constructed within the context of a side-stream.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_46

LANGUAGE: python
CODE:
```
with torch.cuda.stream(s):
        model = DistributedDataParallel(model)
```

----------------------------------------

TITLE: Analyzing Batch Normalization Operations in PyTorch
DESCRIPTION: This snippet shows the usage of batch normalization operations with different tensor shapes and data types. It includes input tensors, running mean and variance, and normalization parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/shufflenet_v2_x1_0_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 24, 28, 28], f16), T([128, 24, 28, 28], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f32), T([24], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 24, 112, 112], f16), T([128, 24, 112, 112], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f32), T([24], f32), False, 1e-05, [True, True, True]), {})
```

----------------------------------------

TITLE: Vectorized Jacobian Computation Using functorch vjp and vmap - Python
DESCRIPTION: Uses functorch's 'vjp' for reverse-mode vector-Jacobian product and 'vmap' for vectorization to efficiently compute the full Jacobian matrix at once. Applies vjp to the partially-applied predict function and maps over identity vectors. Asserts equivalence with the manual row-by-row method. Requires functorch, predict, weight, bias, x, unit_vectors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from functorch import vmap, vjp

_, vjp_fn = vjp(partial(predict, weight, bias), x)

ft_jacobian, = vmap(vjp_fn)(unit_vectors)

# lets confirm both methods compute the same result
assert torch.allclose(ft_jacobian, jacobian)
```

----------------------------------------

TITLE: Initializing and Sharding a Distributed Tensor in PyTorch
DESCRIPTION: This snippet demonstrates how to create a device mesh, initialize a large tensor, and shard it across multiple devices using DTensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import os
import torch
from torch.distributed.tensor import init_device_mesh, Shard, distribute_tensor

mesh = init_device_mesh("cuda", (int(os.environ["WORLD_SIZE"]),))
big_tensor = torch.randn(100000, 88)
my_dtensor = distribute_tensor(big_tensor, mesh, [Shard(dim=0)])
```

----------------------------------------

TITLE: Converting a Dense Tensor to CSR Format in PyTorch
DESCRIPTION: Shows how to convert a dense tensor to a sparse CSR tensor using the to_sparse_csr() method. The example converts a 3x4 dense tensor into CSR format, where zeros are treated as missing values.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
a = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype=torch.float64)
sp = a.to_sparse_csr()
sp
```

----------------------------------------

TITLE: Using Nested Jagged Tensors with torch.compile in PyTorch
DESCRIPTION: Shows how to use nested jagged tensors with torch.compile for optimal performance. Demonstrates compiling functions that work with NJTs directly or create them from values and offsets.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
>>> import torch
>>> a = torch.randn(2, 3)
>>> b = torch.randn(4, 3)
>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)
>>> def f(x): return x.sin() + 1
...
>>> compiled_f = torch.compile(f, fullgraph=True)
>>> output = compiled_f(nt)
>>> output.shape
torch.Size([2, j1, 3])
>>> def g(values, offsets): return torch.nested.nested_tensor_from_jagged(values, offsets) * 2.
...
>>> compiled_g = torch.compile(g, fullgraph=True)
>>> output2 = compiled_g(nt.values(), nt.offsets())
>>> output2.shape
torch.Size([2, j1, 3])
```

----------------------------------------

TITLE: Computing Batched Jacobians using torch.func.vmap and torch.func.jacrev in Python
DESCRIPTION: This snippet demonstrates how to compute Jacobians for a batch of inputs by composing `torch.func.vmap` with `torch.func.jacrev`. `jacrev(torch.sin)` creates a function that computes the Jacobian for a single input vector. `vmap` then maps this Jacobian computation over a batch of input vectors `x` (shape 64x5), producing a batch of Jacobians with the expected shape (64, 5, 5).
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.whirlwind_tour.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
x = torch.randn(64, 5)
jacobian = vmap(jacrev(torch.sin))(x)
assert jacobian.shape == (64, 5, 5)
```

----------------------------------------

TITLE: Evaluating Model in Eval Mode for vmap Compatibility - PyTorch (Python)
DESCRIPTION: Runs a model in evaluation mode (model.eval()), which disables updates to running_mean and running_var, thereby enabling safe use of vmap with BatchNorm layers. This usage pattern is important for scenarios where inference or deterministic statistics are necessary. After evaluation, the model can be set back to training mode with model.train().
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.batch_norm.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
model.eval()
vmap(model)(x)
model.train()
```

----------------------------------------

TITLE: Executing aten.sum Operator on Tensors in PyTorch
DESCRIPTION: The aten.sum.SymInt operation is employed for aggregating tensor elements over specified dimensions, significant for computed data summation in PyTorch. Dependents comprise PyTorch’s versatile tensor functionalities. Inputs involve target tensors and dimensions for reduction. Outputs result in reduced-dimension tensors summarizing values in listed axes, and inputs must match the expected format.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_17

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 1000], f16), [0], True), {})
```

----------------------------------------

TITLE: Sigmoid Activation Operations in PyTorch
DESCRIPTION: This snippet shows the sigmoid activation function applied to a tensor, which squashes values to the range [0,1]. This is typically used for binary classification or as a gate mechanism in neural networks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
Operator: aten.sigmoid.default
cnt: 1, ((T([96, 65], f16),), {})
```

----------------------------------------

TITLE: Configuring BackendConfig for LinearReLU Pattern in PyTorch
DESCRIPTION: This snippet shows how to instantiate and configure a `BackendConfig` object for handling the `nniqat.LinearReLU` pattern. It specifies the root float module (`nn.Linear`), the reference quantized module it should be swapped with (`nnqr.Linear`), and the corresponding fused float module (`nni.LinearReLU`). This configuration guides the FX Graph Mode Quantization process.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#_snippet_9

LANGUAGE: Python
CODE:
```
BackendConfig(nniqat.LinearReLU)
    .set_root_module(nn.Linear)
    .set_reference_quantized_module_for_root(nnqr.Linear)
    .set_fused_module(nni.LinearReLU)
```

----------------------------------------

TITLE: Analyzing PyTorch Sum Operations with Dimension Reduction
DESCRIPTION: Logs of aten.sum.SymInt operations showing tensor summation along specified dimensions. Records include tensor shapes, dimensions to sum over, and whether to keep the dimension in the output (True).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([128, 1000], f16), [0], True), {})
cnt: 1, ((T([128, 1044, 7, 7], f16), [2, 3], True), {})
cnt: 1, ((T([128, 972, 7, 7], f16), [2, 3], True), {})
cnt: 1, ((T([128, 906, 7, 7], f16), [2, 3], True), {})
cnt: 1, ((T([128, 840, 7, 7], f16), [2, 3], True), {})
cnt: 1, ((T([128, 768, 7, 7], f16), [2, 3], True), {})
cnt: 1, ((T([128, 702, 14, 14], f16), [2, 3], True), {})
cnt: 1, ((T([128, 636, 14, 14], f16), [2, 3], True), {})
cnt: 1, ((T([128, 570, 14, 14], f16), [2, 3], True), {})
cnt: 1, ((T([128, 504, 14, 14], f16), [2, 3], True), {})
cnt: 1, ((T([128, 432, 14, 14], f16), [2, 3], True), {})
cnt: 1, ((T([128, 366, 14, 14], f16), [2, 3], True), {})
cnt: 1, ((T([128, 300, 28, 28], f16), [2, 3], True), {})
cnt: 1, ((T([128, 228, 28, 28], f16), [2, 3], True), {})
```

----------------------------------------

TITLE: Enumerating PyTorch Tensor Patterns for BatchNorm Operators - Python
DESCRIPTION: These code snippets enumerate argument combinations for PyTorch batch normalization or similar operators using half- and single-precision FloatTensors. Each tuple specifies shapes and types of spatial tensors, bias, running_mean/var, and flags suitable for module testing. Key parameters include lists of tensor shapes and dtype annotations, batch size, and normalization axes. Outputs are primarily for test case or coverage matrix preparation. Depend on PyTorch's tensor construction and operator APIs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/resnest101e_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
cnt: 25, ((T([32, 1024, 16, 16], f16), T([32, 1024, 16, 16], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), True, 1e-05, [True, True, True]), {})
cnt: 23, ((T([32, 512, 16, 16], f16), T([32, 512, 16, 16], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), True, 1e-05, [True, True, True]), {})
cnt: 23, ((T([32, 128, 1, 1], f16), T([32, 128, 1, 1], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})
cnt: 22, ((T([32, 256, 16, 16], f16), T([32, 256, 16, 16], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), True, 1e-05, [True, True, True]), {})
cnt: 6, ((T([32, 512, 32, 32], f16), T([32, 512, 32, 32], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), True, 1e-05, [True, True, True]), {})
cnt: 4, ((T([32, 256, 32, 32], f16), T([32, 256, 32, 32], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), True, 1e-05, [True, True, True]), {})
cnt: 4, ((T([32, 64, 1, 1], f16), T([32, 64, 1, 1], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})
cnt: 3, ((T([32, 128, 32, 32], f16), T([32, 128, 32, 32], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})
cnt: 5, ((T([32, 256, 64, 64], f16), T([32, 256, 64, 64], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), True, 1e-05, [True, True, True]), {})
cnt: 4, ((T([32, 128, 64, 64], f16), T([32, 128, 64, 64], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})
cnt: 3, ((T([32, 32, 1, 1], f16), T([32, 32, 1, 1], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), True, 1e-05, [True, True, True]), {})
cnt: 3, ((T([32, 64, 64, 64], f16), T([32, 64, 64, 64], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([32, 128, 128, 128], f16), T([32, 128, 128, 128], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})
cnt: 2, ((T([32, 64, 128, 128], f16), T([32, 64, 128, 128], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})
```

----------------------------------------

TITLE: Setting up Custom Backend with CMake
DESCRIPTION: This CMake script sets up a custom backend for a PyTorch project. It checks for and includes ROCm support, finds the required Torch package, and configures the build settings for the custom backend library and an associated test executable. The script specifies the C++ standard to be used and links the necessary Torch libraries.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/custom_backend/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
# Basic CMake setup
cmake_minimum_required(VERSION 3.15 FATAL_ERROR)
project(custom_backend)

if(USE_ROCM)
include(utils)
include(LoadHIP)
endif()
find_package(Torch REQUIRED)

add_library(custom_backend SHARED custom_backend.cpp)
set_property(TARGET custom_backend PROPERTY CXX_STANDARD 17)
target_link_libraries(custom_backend "${TORCH_LIBRARIES}")

add_executable(test_custom_backend test_custom_backend.cpp)
set_property(TARGET test_custom_backend PROPERTY CXX_STANDARD 17)
target_link_libraries(test_custom_backend custom_backend)
```

----------------------------------------

TITLE: Demonstrating Allowed and Disallowed Casting in PyTorch Operations
DESCRIPTION: This code snippet demonstrates the casting rules in PyTorch when the output tensor of an arithmetic operation is specified. It shows which operations are allowed (such as casting integers to float) and which are disallowed (such as casting float to integer).
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
# allowed:
>>> float_tensor *= float_tensor
>>> float_tensor *= int_tensor
>>> float_tensor *= uint_tensor
>>> float_tensor *= bool_tensor
>>> float_tensor *= double_tensor
>>> int_tensor *= long_tensor
>>> int_tensor *= uint_tensor
>>> uint_tensor *= int_tensor

# disallowed (RuntimeError: result type can't be cast to the desired output type):
>>> int_tensor *= float_tensor
>>> bool_tensor *= int_tensor
>>> bool_tensor *= uint_tensor
>>> float_tensor *= complex_float_tensor
```

----------------------------------------

TITLE: Custom Autograd Function with AMP Support
DESCRIPTION: Example of implementing a custom autograd function with proper AMP support using custom_fwd and custom_bwd decorators.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/amp_examples.rst#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
class MyMM(torch.autograd.Function):
    @staticmethod
    @custom_fwd
    def forward(ctx, a, b):
        ctx.save_for_backward(a, b)
        return a.mm(b)
    @staticmethod
    @custom_bwd
    def backward(ctx, grad):
        a, b = ctx.saved_tensors
        return grad.mm(b.t()), a.t().mm(grad)
```

----------------------------------------

TITLE: Checking for PyTorch HIP vs. CUDA Support - Python
DESCRIPTION: Provides a conditional logic example to check whether the PyTorch build has HIP (ROCm) support (`torch.version.hip`) or CUDA support (`torch.version.cuda`), after confirming GPU availability with `torch.cuda.is_available()`. Useful for writing backend-specific code paths.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/hip.rst#_snippet_1

LANGUAGE: Python
CODE:
```
if torch.cuda.is_available() and torch.version.hip:
    # do something specific for HIP
elif torch.cuda.is_available() and torch.version.cuda:
    # do something specific for CUDA
```

----------------------------------------

TITLE: Configuring and Building Custom Torch Operators - CMake - CMake
DESCRIPTION: This snippet sets up a CMake-based build environment for compiling a shared library of custom PyTorch operators (op.cpp) and an associated executable (test_custom_ops.cpp). It conditionally includes GPU-specific dependencies, finds the required Torch package, sets compiler features (such as C++17 and range-based for), and links the respective targets against necessary Torch libraries. To use this setup, CMake version 3.15 or later and PyTorch C++ libraries must be installed. Input source files are defined as op.cpp and test_custom_ops.cpp, and output artifacts are the custom_ops shared library and the test_custom_ops executable. Limitations include a dependency on specific CMake and PyTorch versions, and this CMakeLists assumes the presence of relevant files in the working directory.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/custom_operator/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
# Basic CMake setup
cmake_minimum_required(VERSION 3.15 FATAL_ERROR)
project(custom_ops)

if(USE_ROCM)
include(utils)
include(LoadHIP)
endif()
find_package(Torch REQUIRED)

add_library(custom_ops SHARED op.cpp)
set_property(TARGET custom_ops PROPERTY CXX_STANDARD 17)

target_compile_features(custom_ops PUBLIC cxx_range_for)
target_link_libraries(custom_ops "${TORCH_LIBRARIES}")
target_compile_definitions(custom_ops PRIVATE custom_ops_EXPORTS)

add_executable(test_custom_ops test_custom_ops.cpp)
set_property(TARGET test_custom_ops PROPERTY CXX_STANDARD 17)
target_link_libraries(test_custom_ops custom_ops)

```

----------------------------------------

TITLE: Module-based Debugging Backend Example
DESCRIPTION: Shows how to use a custom debugging backend with torch.nn.Module implementations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_custom_backends.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from typing import List
import torch
def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):
    print("my_compiler() called with FX graph:")
    gm.graph.print_tabular()
    return gm.forward  # return a python callable
class MockModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.relu = torch.nn.ReLU()
    def forward(self, x):
        return self.relu(torch.cos(x))
mod = MockModule()
optimized_mod = torch.compile(mod, backend=my_compiler)
optimized_mod(torch.randn(10))
```

----------------------------------------

TITLE: Implementing Naive Per-Sample Gradient Calculation in PyTorch
DESCRIPTION: Defines two functions for calculating per-sample gradients manually. `compute_grad` takes a single sample and target, unsqueezes them to add a batch dimension, computes the loss, and returns the gradients for model parameters using `torch.autograd.grad`. `compute_sample_grads` iterates through the input batch `data` and `targets`, calls `compute_grad` for each pair, and aggregates the gradients for each parameter across all samples using `zip` and `torch.stack`. This approach is functionally correct but inefficient.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
def compute_grad(sample, target):
    
    sample = sample.unsqueeze(0)  # prepend batch dimension for processing
    target = target.unsqueeze(0)

    prediction = model(sample)
    loss = loss_fn(prediction, target)

    return torch.autograd.grad(loss, list(model.parameters()))


def compute_sample_grads(data, targets):
    """ manually process each sample with per sample gradient """
    sample_grads = [compute_grad(data[i], targets[i]) for i in range(batch_size)]
    sample_grads = zip(*sample_grads)
    sample_grads = [torch.stack(shards) for shards in sample_grads]
    return sample_grads

per_sample_grads = compute_sample_grads(data, targets)
```

----------------------------------------

TITLE: Replacing Add with Mul Using PyTorch FX Transformer
DESCRIPTION: This example uses the PyTorch FX Transformer class to replace torch.ops.aten.add.Tensor calls with torch.ops.aten.mul.Tensor calls.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
class ReplaceAddWithMul(torch.fx.Transformer):
    def call_function(self, target, args, kwargs):
        if target != torch.ops.aten.add.Tensor:
            return super().call_function(target, args, kwargs)
        return super().call_function(torch.ops.aten.mul.Tensor, args, kwargs)

transformed_graph_module = ReplaceAddWithMul(graph_module).transform()
```

----------------------------------------

TITLE: Applying Tensor Normalization and Clamping in PyTorch
DESCRIPTION: Performs normalization and clamping operations on tensors. These operations are used for stabilizing neural network training and preventing numerical issues with small values.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/tts_angular_training.txt#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
aten.norm.ScalarOpt_dim(T([64, 256], f16, stride=(12800, 1)), 2, [1], True)
```

LANGUAGE: Python
CODE:
```
aten.clamp_min.default(T([64, 1], f16), 1e-12)
```

----------------------------------------

TITLE: Installing ONNX Export Dependencies (Bash)
DESCRIPTION: Installs the required Python packages 'onnx' and 'onnxscript' using pip, which are necessary for the TorchDynamo-based ONNX exporter. This is a prerequisite for exporting PyTorch models to ONNX format.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_dynamo.rst#_snippet_0

LANGUAGE: bash
CODE:
```
pip install --upgrade onnx onnxscript
```

----------------------------------------

TITLE: Executing NumPy Function on CUDA using torch.compile
DESCRIPTION: Example showing how to compile and execute a NumPy function on CUDA by using torch.device context manager. The function performs matrix multiplication and summation operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
import torch
import numpy as np

@torch.compile
def numpy_fn(X: np.ndarray, Y: np.ndarray) -> np.ndarray:
    return np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1))

X = np.random.randn(1024, 64)
Y = np.random.randn(1024, 64)
with torch.device("cuda"):
    Z = numpy_fn(X, Y)
assert isinstance(Z, np.ndarray)
```

----------------------------------------

TITLE: Dimension Permutation Operations in PyTorch
DESCRIPTION: Demonstrates how operations that permute dimensions handle named tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/name_inference.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> x = torch.randn(3, 3, names=('N', 'C'))
>>> x.transpose('N', 'C').names
('C', 'N')
```

----------------------------------------

TITLE: Slicing and Indexing Sparse COO Tensor in PyTorch
DESCRIPTION: This example demonstrates slicing and indexing operations on a sparse COO tensor. It shows that slicing is supported only for dense dimensions, while indexing is supported for both sparse and dense dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_14

LANGUAGE: Python
CODE:
```
>>> s[1]
tensor(indices=tensor([[0, 2]]),
       values=tensor([[5, 6],
                      [7, 8]]),
       size=(3, 2), nnz=2, layout=torch.sparse_coo)
>>> s[1, 0, 1]
tensor(6)
>>> s[1, 0, 1:]
tensor([6])
```

----------------------------------------

TITLE: Using CUDAPluggableAllocator to Change Current Allocator in Python
DESCRIPTION: This Python snippet demonstrates how to load a custom C++ allocator from a shared library (`alloc.so`) using `torch.cuda.memory.CUDAPluggableAllocator` and then swap the current PyTorch CUDA allocator with the loaded custom one using `torch.cuda.memory.change_current_allocator`. A subsequent CUDA tensor allocation will then use the new custom allocator.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_24

LANGUAGE: python
CODE:
```
import torch

# Load the allocator
new_alloc = torch.cuda.memory.CUDAPluggableAllocator(
    'alloc.so', 'my_malloc', 'my_free')
# Swap the current allocator
torch.cuda.memory.change_current_allocator(new_alloc)
# This will allocate memory in the device using the new allocator
b = torch.zeros(10, device='cuda')
```

----------------------------------------

TITLE: Compiling NumPy Code with torch.compile
DESCRIPTION: Demonstrates how to use torch.compile with native NumPy code, showing integration between PyTorch compilation and NumPy operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
import torch
import numpy as np

@torch.compile
def numpy_fn(X: np.ndarray, Y: np.ndarray) -> np.ndarray:
    return np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1))

X = np.random.randn(1024, 64)
Y = np.random.randn(1024, 64)
Z = numpy_fn(X, Y)
assert isinstance(Z, np.ndarray)
```

----------------------------------------

TITLE: Tensor Property Conversion in C++
DESCRIPTION: Shows how to convert tensors between different data types and devices.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_13

LANGUAGE: cpp
CODE:
```
torch::Tensor source_tensor = torch::randn({2, 3}, torch::kInt64);
```

LANGUAGE: cpp
CODE:
```
torch::Tensor float_tensor = source_tensor.to(torch::kFloat32);
```

LANGUAGE: cpp
CODE:
```
torch::Tensor gpu_tensor = float_tensor.to(torch::kCUDA);
```

LANGUAGE: cpp
CODE:
```
torch::Tensor gpu_two_tensor = float_tensor.to(torch::Device(torch::kCUDA, 1));
```

LANGUAGE: cpp
CODE:
```
torch::Tensor async_cpu_tensor = gpu_tensor.to(torch::kCPU, /*non_blocking=*/true);
```

----------------------------------------

TITLE: Direct Graph Manipulation in PyTorch FX using Python
DESCRIPTION: This snippet demonstrates direct manipulation of a PyTorch FX Graph by replacing torch.add operations with torch.mul within a traced module's graph. Dependencies are 'torch' and 'torch.fx'. The input is a PyTorch module that performs addition, and the output is a module modified to perform multiplication instead. Constraints include using suitable function replacements and ensuring graph consistency via 'graph.lint()'.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import torch
import torch.fx

# Sample module
class M(torch.nn.Module):
    def forward(self, x, y):
        return torch.add(x, y)

def transform(m: torch.nn.Module,
              tracer_class : type = fx.Tracer) -> torch.nn.Module:
    graph : fx.Graph = tracer_class().trace(m)
    # FX represents its Graph as an ordered list of
    # nodes, so we can iterate through them.
    for node in graph.nodes:
        # Checks if we're calling a function (i.e:
        # torch.add)
        if node.op == 'call_function':
            # The target attribute is the function
            # that call_function calls.
            if node.target == torch.add:
                node.target = torch.mul

    graph.lint() # Does some checks to make sure the
                 # Graph is well-formed.

    return fx.GraphModule(m, graph)
```

----------------------------------------

TITLE: Matrix Multiplication Operations
DESCRIPTION: Matrix multiplication operations between tensors of different shapes, including operations with stride configurations and half-precision (f16) format.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
cnt: 1, ((T([64, 1000], f16), T([1000, 1536], f16)), {})
```

----------------------------------------

TITLE: Setting Up Pre-commit Hook Bash
DESCRIPTION: Presents a bash command using `ln -s` to create a symbolic link from the central PyTorch pre-commit script (`../../tools/git-pre-commit`) located relative to the repository root to the `.git/hooks/pre-commit` file in the local repository clone. This activates the pre-commit checks for the user.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_62

LANGUAGE: Bash
CODE:
```
ln -s ../../tools/git-pre-commit .git/hooks/pre-commit
```

----------------------------------------

TITLE: TorchScript Custom Class Example
DESCRIPTION: Basic example of creating a custom class in TorchScript with initialization and method definition.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
@torch.jit.script
class MyClass:
    def __init__(self, x: int):
        self.x = x

    def inc(self, val: int):
        self.x += val
```

----------------------------------------

TITLE: Max Pooling Operations in PyTorch
DESCRIPTION: This snippet demonstrates max pooling operations in PyTorch, including both forward and backward passes. It shows the use of 2D max pooling with specific kernel sizes, strides, and padding.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2next50_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
Operator: aten.max_pool2d_with_indices.default
cnt: 1, ((T([128, 64, 112, 112], f16), [3, 3], [2, 2], [1, 1]), {})

Operator: aten.max_pool2d_with_indices_backward.default
cnt: 1, ((T([128, 64, 56, 56], f16), T([128, 64, 112, 112], f16), [3, 3], [2, 2], [1, 1], [1, 1], False, T([128, 64, 56, 56], i64)), {})
```

----------------------------------------

TITLE: Implementing Custom Nonzero Operation in PyTorch
DESCRIPTION: Example implementation of a custom nonzero operation using PyTorch's library registration system. The function creates a dynamic-sized tensor with shape [nnz, x.dim()] where nnz is determined at runtime.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.programming_model.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
@torch.library.register_fake("mylib::custom_nonzero")
def _(x):
    nnz = torch.library.get_ctx().new_dynamic_size()
    shape = [nnz, x.dim()]
    return x.new_empty(shape, dtype=torch.int64)
```

----------------------------------------

TITLE: Reduction Function Implementation in PyTorch
DESCRIPTION: Example implementation of a reduction function that computes the mean along dimension 0.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/activation_sparsifier/README.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
def reduce_fn(agg_tensor):
    return agg_tensor.mean(dim=0)
```

----------------------------------------

TITLE: Using Autocast in JIT Scripted Functions with CUDA
DESCRIPTION: Demonstrates how to use an autocast context manager within a JIT scripted function to automatically convert operations from float32 to float16 on CUDA devices for performance optimization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/JIT-AUTOCAST.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from torch.cuda.amp import autocast

@torch.jit.script
def func(a, b):
    with autocast():
        return torch.mm(a, b)

a_float32 = torch.rand((8, 8), dtype=torch.float32, device="cuda")
b_float32 = torch.rand((8, 8), dtype=torch.float32, device="cuda")
result = func(a_float32, b_float32)
print(result.dtype) # expecting torch.float16
```

----------------------------------------

TITLE: Implementing Basic AOT Function with Graph Printing
DESCRIPTION: Demonstrates the basic usage of aot_function with custom compiler functions that print the forward and backward FX graphs of a simple cosine operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/COMPILE_README.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from functorch.compile import aot_function, aot_module, draw_graph
import torch.fx as fx
import torch

def print_graph(name):
    def f(fx_g: fx.GraphModule, inps):
        print(name)
        print(fx_g.code)
        return fx_g
    return f

def f(x):
    return x.cos().cos()

nf = aot_function(f, fw_compiler=print_graph("forward"), bw_compiler=print_graph("backward"))
nf(torch.randn(3, requires_grad=True))
```

----------------------------------------

TITLE: Illustrating Error with Nondeterministic Operation in Deterministic Mode (Python)
DESCRIPTION: Demonstrates the effect of enabling deterministic algorithms using `torch.use_deterministic_algorithms(True)`. When a nondeterministic operation like `index_add_` on a CUDA tensor (which lacks a deterministic CUDA implementation) is called, PyTorch raises a `RuntimeError`, enforcing the deterministic requirement.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/randomness.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> import torch
>>> torch.use_deterministic_algorithms(True)
>>> torch.randn(2, 2).cuda().index_add_(0, torch.tensor([0, 1]), torch.randn(2, 2))
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
RuntimeError: index_add_cuda_ does not have a deterministic implementation, but you set
'torch.use_deterministic_algorithms(True)'. ...
```

----------------------------------------

TITLE: Creating an Integer Tensor with Custom Range in PyTorch C++
DESCRIPTION: This snippet demonstrates how to create a 5x5 square matrix with integers between 3 and 10 using the randint() factory function with both lower and upper bounds specified.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_4

LANGUAGE: cpp
CODE:
```
torch::Tensor tensor = torch::randint(/*low=*/3, /*high=*/10, {5, 5});
```

----------------------------------------

TITLE: Perform aten.native_batch_norm Operation in Python
DESCRIPTION: This snippet involves the native batch normalization operation using PyTorch, providing input tensors of varying dimensions. Dependencies include the PyTorch library for tensor operations. Inputs are multidimensional arrays shaped for specific use cases, generating normalized outputs. The parameters include flags and coefficients that manage the normalization process' behavior.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 0.001), {})
```

----------------------------------------

TITLE: Usage Examples for aten.relu.default
DESCRIPTION: Logs Rectified Linear Unit (ReLU) activation function calls (`aten.relu.default`). These are out-of-place operations applied to 4D float16 tensors of varying shapes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/regnety_002_training.txt#2025-04-22_snippet_8

LANGUAGE: plaintext
CODE:
```
Operator: aten.relu.default
cnt: 1, ((T([128, 24, 56, 56], f16),), {})
cnt: 1, ((T([128, 56, 28, 28], f16),), {})
cnt: 4, ((T([128, 152, 14, 14], f16),), {})
cnt: 7, ((T([128, 368, 7, 7], f16),), {})
```

----------------------------------------

TITLE: Defining Example Inputs for BatchNorm, Pooling, Activations in PyTorch (Python)
DESCRIPTION: Defines serialized tuples representing the input and parameter specifications for ATen operators, such as native_batch_norm, native_batch_norm_backward, max_pool2d_with_indices, and various activation functions. These inputs are typically used for automated test generation, tracing, or benchmarking, where the tuples encode tensor shapes, types, hyperparameters like kernel sizes, strides, paddings, and boolean flags. All tensors predominantly use the 'f16' dtype; scalar and list inputs are used to set dimensions, flags, and operation settings. Each tuple can be parsed to instantiate test tensors and call the corresponding operator; limitations include highly fixed shapes and a focus on batch sizes of 32 and standard CNN feature map layouts.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dpn107_training.txt#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 296, 56, 56], f16), T([32, 128, 56, 56], f16), T([296, 128, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 128, 112, 112], f16), T([32, 3, 224, 224], f16), T([128, 3, 7, 7], f16), [0], [2, 2], [3, 3], [1, 1], False, [0, 0], 1, [False, True, False]), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.copy_.default
cnt: 1, ((T([32, 3, 224, 224], f16), T([32, 3, 224, 224], f16)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.div.Scalar
cnt: 1, ((T([32, 2688, 7, 7], f16, stride=(2688, 1, 0, 0)), 49), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.elu.default
cnt: 1, ((T([32, 2688, 7, 7], f16), 1.0), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.elu_backward.default
cnt: 1, ((T([32, 2688, 7, 7], f16), 1.0, 1, 1, False, T([32, 2688, 7, 7], f16)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.lift_fresh_copy.default
cnt: 1, ((T([32], i64),), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.max_pool2d_with_indices.default
cnt: 1, ((T([32, 128, 112, 112], f16), [3, 3], [2, 2], [1, 1]), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.max_pool2d_with_indices_backward.default
cnt: 1, ((T([32, 128, 56, 56], f16), T([32, 128, 112, 112], f16), [3, 3], [2, 2], [1, 1], [1, 1], False, T([32, 128, 56, 56], i64)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.mean.dim
cnt: 1, ((T([32, 2688, 7, 7], f16), [-1, -2], True), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.native_batch_norm.default
cnt: 1, ((T([32, 128, 112, 112], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 2, ((T([32, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 8, ((T([32, 200, 56, 56], f16), T([200], f16), T([200], f16), T([200], f16), T([200], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 316, 56, 56], f16), T([316], f16), T([316], f16), T([316], f16), T([316], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 336, 56, 56], f16), T([336], f16), T([336], f16), T([336], f16), T([336], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 356, 56, 56], f16), T([356], f16), T([356], f16), T([356], f16), T([356], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 2, ((T([32, 376, 56, 56], f16), T([376], f16), T([376], f16), T([376], f16), T([376], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 400, 56, 56], f16), T([400], f16), T([400], f16), T([400], f16), T([400], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 15, ((T([32, 400, 28, 28], f16), T([400], f16), T([400], f16), T([400], f16), T([400], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 704, 28, 28], f16), T([704], f16), T([704], f16), T([704], f16), T([704], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 768, 28, 28], f16), T([768], f16), T([768], f16), T([768], f16), T([768], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 832, 28, 28], f16), T([832], f16), T([832], f16), T([832], f16), T([832], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 896, 28, 28], f16), T([896], f16), T([896], f16), T([896], f16), T([896], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 960, 28, 28], f16), T([960], f16), T([960], f16), T([960], f16), T([960], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 1024, 28, 28], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 1088, 28, 28], f16), T([1088], f16), T([1088], f16), T([1088], f16), T([1088], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 2, ((T([32, 1152, 28, 28], f16), T([1152], f16), T([1152], f16), T([1152], f16), T([1152], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 800, 28, 28], f16), T([800], f16), T([800], f16), T([800], f16), T([800], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 39, ((T([32, 800, 14, 14], f16), T([800], f16), T([800], f16), T([800], f16), T([800], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 1216, 14, 14], f16), T([1216], f16), T([1216], f16), T([1216], f16), T([1216], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 1280, 14, 14], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 1344, 14, 14], f16), T([1344], f16), T([1344], f16), T([1344], f16), T([1344], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 1408, 14, 14], f16), T([1408], f16), T([1408], f16), T([1408], f16), T([1408], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 1472, 14, 14], f16), T([1472], f16), T([1472], f16), T([1472], f16), T([1472], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 1536, 14, 14], f16), T([1536], f16), T([1536], f16), T([1536], f16), T([1536], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 2, ((T([32, 1600, 14, 14], f16), T([1600], f16), T([1600], f16), T([1600], f16), T([1600], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 1664, 14, 14], f16), T([1664], f16), T([1664], f16), T([1664], f16), T([1664], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 1728, 14, 14], f16), T([1728], f16), T([1728], f16), T([1728], f16), T([1728], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 1792, 14, 14], f16), T([1792], f16), T([1792], f16), T([1792], f16), T([1792], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 1856, 14, 14], f16), T([1856], f16), T([1856], f16), T([1856], f16), T([1856], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 1920, 14, 14], f16), T([1920], f16), T([1920], f16), T([1920], f16), T([1920], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 1984, 14, 14], f16), T([1984], f16), T([1984], f16), T([1984], f16), T([1984], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 2048, 14, 14], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 2112, 14, 14], f16), T([2112], f16), T([2112], f16), T([2112], f16), T([2112], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 2176, 14, 14], f16), T([2176], f16), T([2176], f16), T([2176], f16), T([2176], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 2240, 14, 14], f16), T([2240], f16), T([2240], f16), T([2240], f16), T([2240], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 2304, 14, 14], f16), T([2304], f16), T([2304], f16), T([2304], f16), T([2304], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 2368, 14, 14], f16), T([2368], f16), T([2368], f16), T([2368], f16), T([2368], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 2, ((T([32, 2432, 14, 14], f16), T([2432], f16), T([2432], f16), T([2432], f16), T([2432], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 5, ((T([32, 1600, 7, 7], f16), T([1600], f16), T([1600], f16), T([1600], f16), T([1600], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 2432, 7, 7], f16), T([2432], f16), T([2432], f16), T([2432], f16), T([2432], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 2560, 7, 7], f16), T([2560], f16), T([2560], f16), T([2560], f16), T([2560], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 2688, 7, 7], f16), T([2688], f16), T([2688], f16), T([2688], f16), T([2688], f16), True, 0.1, 0.001), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.native_batch_norm_backward.default
cnt: 1, ((T([32, 2688, 7, 7], f16), T([32, 2688, 7, 7], f16), T([2688], f16), T([2688], f16), T([2688], f16), T([2688], f32), T([2688], f32), True, 0.001, [True, True, True]), {})
```

LANGUAGE: python
CODE:
```
cnt: 5, ((T([32, 1600, 7, 7], f16), T([32, 1600, 7, 7], f16), T([1600], f16), T([1600], f16), T([1600], f16), T([1600], f32), T([1600], f32), True, 0.001, [True, True, True]), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 2560, 7, 7], f16), T([32, 2560, 7, 7], f16), T([2560], f16), T([2560], f16), T([2560], f16), T([2560], f32), T([2560], f32), True, 0.001, [True, True, True]), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 2432, 7, 7], f16), T([32, 2432, 7, 7], f16), T([2432], f16), T([2432], f16), T([2432], f16), T([2432], f32), T([2432], f32), True, 0.001, [True, True, True]), {})
```

LANGUAGE: python
CODE:
```
cnt: 2, ((T([32, 1600, 14, 14], f16), T([32, 1600, 14, 14], f16), T([1600], f16), T([1600], f16), T([1600], f16), T([1600], f32), T([1600], f32), True, 0.001, [True, True, True]), {})
```

LANGUAGE: python
CODE:
```
cnt: 2, ((T([32, 2432, 14, 14], f16), T([32, 2432, 14, 14], f16), T([2432], f16), T([2432], f16), T([2432], f16), T([2432], f32), T([2432], f32), True, 0.001, [True, True, True]), {})
```

LANGUAGE: python
CODE:
```
cnt: 39, ((T([32, 800, 14, 14], f16), T([32, 800, 14, 14], f16), T([800], f16), T([800], f16), T([800], f16), T([800], f32), T([800], f32), True, 0.001, [True, True, True]), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 2368, 14, 14], f16), T([32, 2368, 14, 14], f16), T([2368], f16), T([2368], f16), T([2368], f16), T([2368], f32), T([2368], f32), True, 0.001, [True, True, True]), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 2304, 14, 14], f16), T([32, 2304, 14, 14], f16), T([2304], f16), T([2304], f16), T([2304], f16), T([2304], f32), T([2304], f32), True, 0.001, [True, True, True]), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 2240, 14, 14], f16), T([32, 2240, 14, 14], f16), T([2240], f16), T([2240], f16), T([2240], f16), T([2240], f32), T([2240], f32), True, 0.001, [True, True, True]), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 2176, 14, 14], f16), T([32, 2176, 14, 14], f16), T([2176], f16), T([2176], f16), T([2176], f16), T([2176], f32), T([2176], f32), True, 0.001, [True, True, True]), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 2112, 14, 14], f16), T([32, 2112, 14, 14], f16), T([2112], f16), T([2112], f16), T([2112], f16), T([2112], f32), T([2112], f32), True, 0.001, [True, True, True]), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 2048, 14, 14], f16), T([32, 2048, 14, 14], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f32), T([2048], f32), True, 0.001, [True, True, True]), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 1984, 14, 14], f16), T([32, 1984, 14, 14], f16), T([1984], f16), T([1984], f16), T([1984], f16), T([1984], f32), T([1984], f32), True, 0.001, [True, True, True]), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 1920, 14, 14], f16), T([32, 1920, 14, 14], f16), T([1920], f16), T([1920], f16), T([1920], f16), T([1920], f32), T([1920], f32), True, 0.001, [True, True, True]), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 1856, 14, 14], f16), T([32, 1856, 14, 14], f16), T([1856], f16), T([1856], f16), T([1856], f16), T([1856], f32), T([1856], f32), True, 0.001, [True, True, True]), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 1792, 14, 14], f16), T([32, 1792, 14, 14], f16), T([1792], f16), T([1792], f16), T([1792], f16), T([1792], f32), T([1792], f32), True, 0.001, [True, True, True]), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 1728, 14, 14], f16), T([32, 1728, 14, 14], f16), T([1728], f16), T([1728], f16), T([1728], f16), T([1728], f32), T([1728], f32), True, 0.001, [True, True, True]), {})
```

----------------------------------------

TITLE: Using CUDAStreamGuard for Stream Management in PyTorch C++
DESCRIPTION: This example shows how to use CUDAStreamGuard to manage CUDA streams within a scope, automatically handling stream switching and restoration.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_3

LANGUAGE: cpp
CODE:
```
// create a tensor on device 0
torch::Tensor tensor0 = torch::ones({2, 2}, torch::device(torch::kCUDA));
// get a new stream from CUDA stream pool on device 0
at::cuda::CUDAStream myStream = at::cuda::getStreamFromPool(false, 0);
// set the current CUDA stream to `myStream` within the scope using CUDA stream guard
{
  at::cuda::CUDAStreamGuard guard(myStream);
  // current CUDA stream is `myStream` from here till the end of bracket.
  // sum() on tensor0 uses `myStream` as current CUDA stream
  tensor0.sum();
}
// current CUDA stream is reset to default CUDA stream after CUDA stream guard is destroyed
// sum() on tensor0 uses default CUDA stream on device 0 as current CUDA stream
tensor0.sum();
```

----------------------------------------

TITLE: TorchScript Compilation with AOT Function
DESCRIPTION: Shows how to use TorchScript as a compiler backend for AOT Autograd, including scripting and freezing the model for inference optimization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/COMPILE_README.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
def f(x):
    return x.cos().cos()

def ts_compiler(fx_g: fx.GraphModule, inps):
    f = torch.jit.script(fx_g)
    print(f.graph)
    f = torch.jit.freeze(f.eval())
    return f

aot_function(f, ts_compiler, ts_compiler)(torch.randn(3, requires_grad=True))
```

----------------------------------------

TITLE: Basic TensorOptions Usage in PyTorch C++
DESCRIPTION: Shows the basic and verbose way of specifying tensor options using TensorOptions class.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_8

LANGUAGE: cpp
CODE:
```
torch::ones(10, torch::TensorOptions().dtype(torch::kFloat32))
```

----------------------------------------

TITLE: Custom Cube Function with Intermediate Value Handling
DESCRIPTION: Implements a custom cube operation that demonstrates how to handle intermediate values and support higher-order gradients.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
class MyCube(torch.autograd.Function):
    @staticmethod
    def forward(x):
        # We wish to save dx for backward. In order to do so, it must
        # be returned as an output.
        dx = 3 * x ** 2
        result = x ** 3
        return result, dx

    @staticmethod
    def setup_context(ctx, inputs, output):
        x, = inputs
        result, dx = output
        ctx.save_for_backward(x, dx)

    @staticmethod
    def backward(ctx, grad_output, grad_dx):
        x, dx = ctx.saved_tensors
        # In order for the autograd.Function to work with higher-order
        # gradients, we must add the gradient contribution of `dx`,
        # which is grad_dx * 6 * x.
        result = grad_output * dx + grad_dx * 6 * x
        return result

# Wrap MyCube in a function so that it is clearer what the output is
def my_cube(x):
    result, dx = MyCube.apply(x)
    return result
```

----------------------------------------

TITLE: Verifying an Exported ONNX Model in Python
DESCRIPTION: This snippet shows how to load an exported ONNX model file and check its well-formedness using the ONNX library. It requires the `onnx` library installed (`pip install onnx`). The output is a confirmation if the model is well-formed and a printable representation of the ONNX graph.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_1

LANGUAGE: python
CODE:
```
import onnx

# Load the ONNX model
model = onnx.load("alexnet.onnx")

# Check that the model is well formed
onnx.checker.check_model(model)

# Print a human readable representation of the graph
print(onnx.helper.printable_graph(model.graph))
```

----------------------------------------

TITLE: Asynchronous Execution with TorchScript Fork/Wait in Python
DESCRIPTION: Demonstrates inter-op parallelism in PyTorch using TorchScript. The `forward` function uses `torch.jit._fork` to launch the `compute_z` function asynchronously, allowing parallel execution with the subsequent matrix multiplication (`torch.mm`). `torch.jit._wait` is used to synchronize and retrieve the result of the asynchronous task before combining it with the other result. This requires the `torch` library and utilizes TorchScript JIT compilation (`@torch.jit.script`).
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cpu_threading_torchscript_inference.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
@torch.jit.script
def compute_z(x):
    return torch.mm(x, self.w_z)

@torch.jit.script
def forward(x):
    # launch compute_z asynchronously:
    fut = torch.jit._fork(compute_z, x)
    # execute the next operation in parallel to compute_z:
    y = torch.mm(x, self.w_y)
    # wait for the result of compute_z:
    z = torch.jit._wait(fut)
    return y + z
```

----------------------------------------

TITLE: Downloading LibTorch Distribution using wget and unzip
DESCRIPTION: Commands to download and extract the LibTorch ZIP archive for CPU-only version. This is the first step in setting up a C++ project with PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/installing.rst#2025-04-22_snippet_0

LANGUAGE: sh
CODE:
```
wget https://download.pytorch.org/libtorch/nightly/cpu/libtorch-shared-with-deps-latest.zip
unzip libtorch-shared-with-deps-latest.zip
```

----------------------------------------

TITLE: Creating an Integer Tensor with Range in PyTorch C++
DESCRIPTION: This example illustrates how to create a 5x5 square matrix with integers between 0 and 10 using the randint() factory function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_3

LANGUAGE: cpp
CODE:
```
torch::Tensor tensor = torch::randint(/*high=*/10, {5, 5});
```

----------------------------------------

TITLE: Fixing CUDA Stream Synchronization Error
DESCRIPTION: Corrected version of the code that properly synchronizes streams by making the new stream wait for the default stream to complete.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cuda._sanitizer.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
with torch.cuda.stream(torch.cuda.Stream()):
    torch.cuda.current_stream().wait_stream(torch.cuda.default_stream())
    torch.mul(a, 5, out=a)
```

----------------------------------------

TITLE: Interpreting torch.save Serialized File Structure - PyTorch - text
DESCRIPTION: Explains, in a code-block (not executable code), the directory structure and files contained within a torch.save-generated ZIP64 archive. No dependencies or code execution; purely referential. Important for understanding what data and metadata are stored, including object pickles, storages, versioning, and system byteorder. Useful for advanced users inspecting or manipulating checkpoints at the file level.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_8

LANGUAGE: text
CODE:
```
checkpoint.pth
├── data.pkl
├── byteorder  # added in PyTorch 2.1.0
├── data/
│   ├── 0
│   ├── 1
│   ├── 2
│   └── …
└── version
```

----------------------------------------

TITLE: Concatenating DataPipes in PyTorch
DESCRIPTION: Demonstrates the use of the concat() method to combine multiple DataPipes sequentially, yielding elements from each DataPipe in order.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(4)
dp2 = ExampleIterPipe(3)
dp = dp.concat(dp2)
for i in dp:
    print(i)
```

----------------------------------------

TITLE: Minimal TensorOptions Syntax
DESCRIPTION: Demonstrates the most concise way to specify tensor options for a single property.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_11

LANGUAGE: cpp
CODE:
```
torch::ones(10, torch::kFloat32)
```

----------------------------------------

TITLE: Per-Sample Gradients Computation
DESCRIPTION: Example showing how to compute per-sample gradients using vmap and grad transforms together.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/README.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from functorch import vmap
batch_size, feature_size = 3, 5

def model(weights,feature_vec):
    # Very simple linear model with activation
    assert feature_vec.dim() == 1
    return feature_vec.dot(weights).relu()

def compute_loss(weights, example, target):
    y = model(weights, example)
    return ((y - target) ** 2).mean()  # MSELoss

weights = torch.randn(feature_size, requires_grad=True)
examples = torch.randn(batch_size, feature_size)
targets = torch.randn(batch_size)
inputs = (weights,examples, targets)
grad_weight_per_example = vmap(grad(compute_loss), in_dims=(None, 0, 0))(*inputs)
```

----------------------------------------

TITLE: Executing PyTorch Operators with Tensor Inputs
DESCRIPTION: This code snippet shows the execution of various PyTorch operators with different tensor inputs. It includes operations like batch normalization, NLL loss, ReLU activation, and threshold backward pass.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gernet_l_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 32, 128, 128], f16), T([128, 32, 128, 128], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), True, 1e-05, [True, True, True]), {})
Operator: aten.nll_loss_backward.default
cnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})
Operator: aten.nll_loss_forward.default
cnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})
Operator: aten.relu_.default
cnt: 1, ((T([128, 32, 128, 128], f16),), {})
cnt: 2, ((T([128, 128, 64, 64], f16),), {})
cnt: 4, ((T([128, 192, 32, 32], f16),), {})
cnt: 1, ((T([128, 160, 32, 32], f16),), {})
cnt: 11, ((T([128, 160, 16, 16], f16),), {})
cnt: 6, ((T([128, 640, 16, 16], f16),), {})
cnt: 1, ((T([128, 1920, 16, 16], f16),), {})
cnt: 17, ((T([128, 1920, 8, 8], f16),), {})
cnt: 9, ((T([128, 640, 8, 8], f16),), {})
cnt: 1, ((T([128, 2560, 8, 8], f16),), {})
Operator: aten.sum.SymInt
cnt: 1, ((T([128, 1000], f16), [0], True), {})
Operator: aten.threshold_backward.default
cnt: 1, ((T([128, 2560, 8, 8], f16), T([128, 2560, 8, 8], f16), 0), {})
cnt: 9, ((T([128, 640, 8, 8], f16), T([128, 640, 8, 8], f16), 0), {})
cnt: 17, ((T([128, 1920, 8, 8], f16), T([128, 1920, 8, 8], f16), 0), {})
cnt: 1, ((T([128, 1920, 16, 16], f16), T([128, 1920, 16, 16], f16), 0), {})
cnt: 6, ((T([128, 640, 16, 16], f16), T([128, 640, 16, 16], f16), 0), {})
cnt: 11, ((T([128, 160, 16, 16], f16), T([128, 160, 16, 16], f16), 0), {})
cnt: 1, ((T([128, 160, 32, 32], f16), T([128, 160, 32, 32], f16), 0), {})
cnt: 4, ((T([128, 192, 32, 32], f16), T([128, 192, 32, 32], f16), 0), {})
cnt: 2, ((T([128, 128, 64, 64], f16), T([128, 128, 64, 64], f16), 0), {})
cnt: 1, ((T([128, 32, 128, 128], f16), T([128, 32, 128, 128], f16), 0), {})
```

----------------------------------------

TITLE: Capturing and Replaying Simple Operation with CUDA Graph PyTorch
DESCRIPTION: This snippet demonstrates capturing a simple computation (`static_input * 2`) into a CUDA graph using the `torch.cuda.graph` context manager. It includes the necessary warmup phase on a separate stream and shows how to replay the graph with new input data by copying it to the static input tensor used during capture. The graph reuses the same memory addresses for input and output.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_40

LANGUAGE: python
CODE:
```
g = torch.cuda.CUDAGraph()

# Placeholder input used for capture
static_input = torch.empty((5,), device="cuda")

# Warmup before capture
s = torch.cuda.Stream()
s.wait_stream(torch.cuda.current_stream())
with torch.cuda.stream(s):
    for _ in range(3):
        static_output = static_input * 2
torch.cuda.current_stream().wait_stream(s)

# Captures the graph
# To allow capture, automatically sets a side stream as the current stream in the context
with torch.cuda.graph(g):
    static_output = static_input * 2

# Fills the graph's input memory with new data to compute on
static_input.copy_(torch.full((5,), 3, device="cuda"))
g.replay()
# static_output holds the results
print(static_output)  # full of 3 * 2 = 6

# Fills the graph's input memory with more data to compute on
static_input.copy_(torch.full((5,), 4, device="cuda"))
g.replay()
print(static_output)  # full of 4 * 2 = 8
```

----------------------------------------

TITLE: Checkout PyTorch Nightly Branch with conda (bash)
DESCRIPTION: Uses the `./tools/nightly.py` script to checkout or create a new PyTorch development branch (`-b`) and installs dependencies into a specified `conda` environment (`-p`). This approach reuses an existing `conda` environment; a subsequent `source` command activates the environment (use `& .\my-env\Scripts\Activate.ps1` on Windows).
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_8

LANGUAGE: bash
CODE:
```
./tools/nightly.py checkout -b my-nightly-branch -p my-env
source my-env/bin/activate
```

----------------------------------------

TITLE: Identifying Graph Breaks with torch._dynamo.explain
DESCRIPTION: Demonstrates how to use `torch._dynamo.explain` to analyze a function for graph breaks. It runs the function with Dynamo and aggregates reasons for compilation interruptions, providing details like graph count, break count, reasons, and user stack traces.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting_old.rst#_snippet_6

LANGUAGE: python
CODE:
```
import torch
import torch._dynamo as dynamo
def toy_example(a, b):
    x = a / (torch.abs(a) + 1)
    print("woo")
    if b.sum() < 0:
        b = b * -1
    return x * b
explanation = dynamo.explain(toy_example)(torch.randn(10), torch.randn(10))
print(explanation_verbose)
```

----------------------------------------

TITLE: Defining MNIST MLP Model Architecture in PyTorch
DESCRIPTION: Defines a multi-level perceptron model with two convolutions, two linear layers, and activations for MNIST digit classification.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
class Net(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output
```

----------------------------------------

TITLE: Batch and Summation Reduction (aten.sum.SymInt, aten.sum.default) in PyTorch (Python)
DESCRIPTION: These snippets describe sum reductions over batch outputs, one with a symbolic int axis and keepdim parameter, the other summing the entire tensor. Intended for reducing over class or batch axes, accepting float16 input and, in the first case, supporting symbolic/static size semantics. Output is a scalar or reduced-dim tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mnasnet1_0_training.txt#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([32, 1000], f16, stride=(0, 0)), [0], True), {})
Operator: aten.sum.default
cnt: 1, ((T([32, 1000], f16),), {})
```

----------------------------------------

TITLE: Moving Module to Empty State on Another Device in Python
DESCRIPTION: Shows using the `torch.nn.Module.to_empty()` method. This method moves a module (initially on the 'meta' device) to a specified data-carrying device (like CPU) but leaves its parameters uninitialized. This is useful for preparing a module structure on a target device before manually initializing the parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/meta.rst#_snippet_4

LANGUAGE: Python
CODE:
```
>>> from torch.nn.modules import Linear
>>> with torch.device('meta'):
...     m = Linear(20, 30)
>>> m.to_empty(device="cpu")
Linear(in_features=20, out_features=30, bias=True)
```

----------------------------------------

TITLE: Demonstrating Named Tensor Operations in PyTorch
DESCRIPTION: Shows how pointwise unary functions preserve tensor names from input to output.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/name_inference.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> x = torch.randn(3, 3, names=('N', 'C'))
>>> x.abs().names
('N', 'C')
```

----------------------------------------

TITLE: Using torch.jit.annotate for Type Annotation
DESCRIPTION: Shows how to use torch.jit.annotate to explicitly specify a type for an expression. This is useful when the default type inference would be incorrect, like initializing an empty list of a specific type.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
import torch
from typing import List

def g(l: List[int], val: int):
    l.append(val)
    return l

def f(val: int):
    l = g(torch.jit.annotate(List[int], []), val)
    return l

m = torch.jit.script(f)
print("Eager:", f(3))
print("TorchScript:", m(3))
```

----------------------------------------

TITLE: Describing PyTorch threshold_backward Operator Arguments - Python
DESCRIPTION: This snippet lists call patterns for the threshold_backward operator, which is commonly used for implementing backward passes of threshold or ReLU-like operations. Input arguments are pairs of float16 tensors representing inputs and gradients, as well as threshold values as constants. The variety of tensor shapes demonstrates different spatial and channel dimensions for batch-wise processing. Intended for operator kernel and autograd validation within PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/resnest101e_training.txt#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
Operator: aten.threshold_backward.default
cnt: 3, ((T([32, 2048, 8, 8], f16), T([32, 2048, 8, 8], f16), 0), {})
cnt: 3, ((T([32, 256, 1, 1], f16), T([32, 256, 1, 1], f16), 0), {})
cnt: 2, ((T([32, 1024, 8, 8], f16), T([32, 1024, 8, 8], f16), 0), {})
cnt: 2, ((T([32, 512, 8, 8], f16), T([32, 512, 8, 8], f16), 0), {})
cnt: 24, ((T([32, 1024, 16, 16], f16), T([32, 1024, 16, 16], f16), 0), {})
cnt: 23, ((T([32, 512, 16, 16], f16), T([32, 512, 16, 16], f16), 0), {})
cnt: 23, ((T([32, 128, 1, 1], f16), T([32, 128, 1, 1], f16), 0), {})
cnt: 22, ((T([32, 256, 16, 16], f16), T([32, 256, 16, 16], f16), 0), {})
cnt: 5, ((T([32, 512, 32, 32], f16), T([32, 512, 32, 32], f16), 0), {})
cnt: 4, ((T([32, 256, 32, 32], f16), T([32, 256, 32, 32], f16), 0), {})
cnt: 4, ((T([32, 64, 1, 1], f16), T([32, 64, 1, 1], f16), 0), {})
cnt: 3, ((T([32, 128, 32, 32], f16), T([32, 128, 32, 32], f16), 0), {})
cnt: 4, ((T([32, 256, 64, 64], f16), T([32, 256, 64, 64], f16), 0), {})
cnt: 4, ((T([32, 128, 64, 64], f16), T([32, 128, 64, 64], f16), 0), {})
cnt: 3, ((T([32, 32, 1, 1], f16), T([32, 32, 1, 1], f16), 0), {})
cnt: 3, ((T([32, 64, 64, 64], f16), T([32, 64, 64, 64], f16), 0), {})
cnt: 1, ((T([32, 128, 128, 128], f16), T([32, 128, 128, 128], f16), 0), {})
cnt: 2, ((T([32, 64, 128, 128], f16), T([32, 64, 128, 128], f16), 0), {})
```

----------------------------------------

TITLE: Saving and Reloading PowerSGD State and Hook in PyTorch
DESCRIPTION: This code demonstrates how to save and reload PowerSGD state and hook for DDP models in PyTorch. It includes setting up a distributed environment, creating a simple model, applying PowerSGD hook, saving the state, and reloading it.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/ddp_comm_hooks.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
import os
import sys
import tempfile
import torch
import torch.distributed as dist
import torch.nn as nn
import torch.optim as optim
import torch.multiprocessing as mp

from torch.nn.parallel import DistributedDataParallel
from torch.distributed.algorithms.ddp_comm_hooks import powerSGD_hook as powerSGD

class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(24,24)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(24,12)

    def forward(self, x):
        return self.fc2(self.relu(self.fc1(x)))

def setup(rank, world_size):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'

    # initialize the process group
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

def cleanup():
    dist.destroy_process_group()

def run_demo(demo_fn, world_size):
    mp.spawn(
        demo_fn,
        args=(world_size,),
        nprocs=world_size,
        join=True)

def demo_serialization(rank, world_size):
    setup(rank, world_size)

    CHECKPOINT = tempfile.gettempdir() + "/checkpoint.pt"

    model = SimpleModel().to(rank)
    ddp_model = DistributedDataParallel(model, device_ids=[rank])

    powersgd_hook = powerSGD.powerSGD_hook
    powersgd_state = powerSGD.PowerSGDState(process_group=None)

    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)
    ddp_model.register_comm_hook(powersgd_state, powersgd_hook)

    state = {
        'state_dict': ddp_model.state_dict(),
        'comm_hook': powersgd_hook,
        'comm_hook_state': powersgd_state}

    if rank == 0:
        torch.save(state, CHECKPOINT)

    dist.barrier()
    map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}
    checkpoint = torch.load(CHECKPOINT, map_location=map_location)

    new_ddp_model = DistributedDataParallel(SimpleModel().to(rank), device_ids=[rank])
    new_ddp_model.load_state_dict(checkpoint['state_dict'])
    powersgd_hook = checkpoint['comm_hook']
    powersgd_state = checkpoint['comm_hook_state']

    new_ddp_model.register_comm_hook(powersgd_state, powersgd_hook)

    if rank == 0:
        os.remove(CHECKPOINT)

    cleanup()

if __name__ == "__main__":
    n_gpus = torch.cuda.device_count()
    assert n_gpus >= 2, f"Requires at least 2 GPUs to run, but got {n_gpus}"
    world_size = n_gpus
    run_demo(demo_serialization, world_size)
```

----------------------------------------

TITLE: Dequantizing Tensors in PyTorch
DESCRIPTION: Methods for converting quantized tensors back to floating point format.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
quantized_tensor.dequantize()
torch.dequantize(x)
```

----------------------------------------

TITLE: Modifying Existing GraphModule in PyTorch FX using Python
DESCRIPTION: This code snippet shows how to modify an existing PyTorch FX GraphModule by tracing a given module, altering its graph, and recompiling it. Dependencies include 'torch' and 'torch.fx'. The input is a torch.nn.Module, and it returns a modified GraphModule. The main constraint is the requirement to recompile to synchronize the 'forward()' method.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
import torch.fx

def transform(m : nn.Module) -> nn.Module:
    gm : torch.fx.GraphModule = torch.fx.symbolic_trace(m)

    # Modify gm.graph
    # <...>

    # Recompile the forward() method of `gm` from its Graph
    gm.recompile()

    return gm
```

----------------------------------------

TITLE: Convolution Operations in PyTorch Neural Network
DESCRIPTION: Summary of convolution operations (aten.convolution.default) in the model, showing counts, tensor shapes, strides, padding, and other parameters. These represent the various convolutional layers throughout the neural network.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_regnet_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
Operator: aten.convolution.default
cnt: 1, ((T([32, 3, 224, 224], f16), T([32, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 32, 112, 112], f16), T([224, 32, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 224, 112, 112], f16), T([224, 112, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 2), {})
cnt: 1, ((T([32, 224, 1, 1], f16), T([8, 224, 1, 1], f16), T([8], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 8, 1, 1], f16), T([224, 8, 1, 1], f16), T([224], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([32, 224, 56, 56], f16), T([224, 224, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 32, 112, 112], f16), T([224, 32, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 224, 56, 56], f16), T([224, 112, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 2), {})
cnt: 1, ((T([32, 224, 1, 1], f16), T([56, 224, 1, 1], f16), T([56], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 56, 1, 1], f16), T([224, 56, 1, 1], f16), T([224], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 224, 56, 56], f16), T([448, 224, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 448, 56, 56], f16), T([448, 112, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 4), {})
cnt: 1, ((T([32, 448, 1, 1], f16), T([56, 448, 1, 1], f16), T([56], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 56, 1, 1], f16), T([448, 56, 1, 1], f16), T([448], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 9, ((T([32, 448, 28, 28], f16), T([448, 448, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 224, 56, 56], f16), T([448, 224, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([32, 448, 28, 28], f16), T([448, 112, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 4), {})
cnt: 4, ((T([32, 448, 1, 1], f16), T([112, 448, 1, 1], f16), T([112], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([32, 112, 1, 1], f16), T([448, 112, 1, 1], f16), T([448], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 448, 28, 28], f16), T([896, 448, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 896, 28, 28], f16), T([896, 112, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 8), {})
cnt: 1, ((T([32, 896, 1, 1], f16), T([112, 896, 1, 1], f16), T([112], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 112, 1, 1], f16), T([896, 112, 1, 1], f16), T([896], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 21, ((T([32, 896, 14, 14], f16), T([896, 896, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 448, 28, 28], f16), T([896, 448, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 10, ((T([32, 896, 14, 14], f16), T([896, 112, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 8), {})
cnt: 10, ((T([32, 896, 1, 1], f16), T([224, 896, 1, 1], f16), T([224], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 10, ((T([32, 224, 1, 1], f16), T([896, 224, 1, 1], f16), T([896], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 896, 14, 14], f16), T([2240, 896, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 2240, 14, 14], f16), T([2240, 112, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 20), {})
cnt: 1, ((T([32, 2240, 1, 1], f16), T([224, 2240, 1, 1], f16), T([224], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 224, 1, 1], f16), T([2240, 224, 1, 1], f16), T([2240], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 2240, 7, 7], f16), T([2240, 2240, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 896, 14, 14], f16), T([2240, 896, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
```

----------------------------------------

TITLE: Analyzing ATen Softmax Backward Operations in PyTorch
DESCRIPTION: This snippet provides an example of the ATen softmax backward data operator, showing its application for back-propagating errors in deep learning. It shows tensor specifications and suggests managing derivatives during calculation through certain options such as data type and dimensional settings.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistilBertForMaskedLM_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
Operator: aten._softmax_backward_data.default
cnt: 6, ((T([16, 12, 128, 128], f16), T([16, 12, 128, 128], f16), -1, f16), {})
```

----------------------------------------

TITLE: Demonstrating In-Place Tensor Mutation in PyTorch Python
DESCRIPTION: Illustrates how to modify a Tensor directly using methods with a trailing underscore (`_`) or by using the `out` argument in functions like `torch.add`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_33

LANGUAGE: Python
CODE:
```
a = torch.zeros(2, 3)
b = torch.ones(2, 3)
a.add_(b)  # in-place add, so `a` is modified.
torch.add(a, b, out=a) # another way to express the same thing
```

----------------------------------------

TITLE: Using Data Sparsification Callbacks with PyTorch Lightning
DESCRIPTION: Example showing how to use data sparsification callbacks with PyTorch Lightning Trainer. It demonstrates the workflow of creating a Lightning module, adding the callback to the trainer, and accessing the sparsified model after training.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/lightning/callbacks/README.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
pl_module = SomePLModule()  # pl_module.model should specify the pytorch model

ds_callback = SomeDataSparsifierCallback(data_sparsifier_class=..., data_sparsifier_args=..., ...)  # add scheduler if TrainingAwareDataSparsifier
trainer = Trainer(callbacks=[ds_callback])

trainer.fit(pl_module, train_data_loader, val_data_loader)

# NOTE: pl_module.model is not sparsified

# access sparsified model
sparsified_model = ds_callback.sparsified
```

----------------------------------------

TITLE: Basic RPC Model Computation with PyTorch
DESCRIPTION: Example of a simple distributed model using PyTorch's RPC framework where computation is split across two worker nodes. This demonstrates the basic pattern that distributed autograd needs to support.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/distributed_autograd.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
import torch.distributed.rpc as rpc

def my_add(t1, t2):
  return torch.add(t1, t2)

# On worker 0:
t1 = torch.rand((3, 3), requires_grad=True)
t2 = torch.rand((3, 3), requires_grad=True)

# Perform some computation remotely.
t3 = rpc.rpc_sync("worker1", my_add, args=(t1, t2))

# Perform some computation locally based on remote result.
t4 = torch.rand((3, 3), requires_grad=True)
t5 = torch.mul(t3, t4)

# Compute some loss.
loss = t5.sum()
```

----------------------------------------

TITLE: Acquiring CUDA Streams in PyTorch C++
DESCRIPTION: This snippet demonstrates three methods to acquire CUDA streams: from a pool, getting the default stream, and getting the current stream. It includes options for specifying priority and device index.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_0

LANGUAGE: cpp
CODE:
```
CUDAStream getStreamFromPool(const bool isHighPriority = false, DeviceIndex device = -1);
```

LANGUAGE: cpp
CODE:
```
CUDAStream getDefaultCUDAStream(DeviceIndex device_index = -1);
```

LANGUAGE: cpp
CODE:
```
CUDAStream getCurrentCUDAStream(DeviceIndex device_index = -1);
```

----------------------------------------

TITLE: Checking Intel GPU Availability
DESCRIPTION: Demonstrates how to check if an Intel GPU is available using PyTorch's API. If unavailable, users should double-check their Intel GPU driver installation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/get_start_xpu.rst#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
import torch
torch.xpu.is_available()  # torch.xpu is the API for Intel GPU support
```

----------------------------------------

TITLE: C++ Tensor Operation Reference
DESCRIPTION: Example showing basic tensor operations available in the C++ API, such as add, reshape, and clone using torch::Tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cpp_index.rst#2025-04-22_snippet_0

LANGUAGE: C++
CODE:
```
torch::Tensor::add
torch::Tensor::reshape
torch::Tensor::clone
```

----------------------------------------

TITLE: Defining and Compiling a PyTorch Model with torch.compile
DESCRIPTION: Defines a simple sequential PyTorch model and compiles it using `@torch.compile()`. This demonstrates the basic usage of the compilation decorator for a standard PyTorch model.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting_old.rst#_snippet_5

LANGUAGE: python
CODE:
```
import torch

@torch.compile()
def test_model(x):
    model = torch.nn.Sequential(
        torch.nn.Linear(10, 10),
        torch.nn.LayerNorm(10),
        torch.nn.ReLU(),
    )
    return model(x)

y = test_model(torch.ones(10, 10))
```

----------------------------------------

TITLE: Vectorizing Gradient Computation with functorch.vmap
DESCRIPTION: Applies `functorch.vmap` to the single-sample gradient function `ft_compute_grad` to create a vectorized version `ft_compute_sample_grad`. `vmap` maps the function over batches of inputs. The `in_dims=(None, None, 0, 0)` argument specifies that the transformation should map over the 0th dimension (the batch dimension) of the `sample` (3rd arg) and `target` (4th arg), while the `params` (1st arg) and `buffers` (2nd arg) should be broadcasted (not mapped over, used as constants for each sample).
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))
```

----------------------------------------

TITLE: AOTAutograd Backend Implementation
DESCRIPTION: Example of implementing a custom backend with AOTAutograd support for model training, using boxed functions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_custom_backends.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from torch._dynamo.backends.common import aot_autograd
from functorch.compile import make_boxed_func

def my_compiler(gm, example_inputs):
    return make_boxed_func(gm.forward)

my_backend = aot_autograd(fw_compiler=my_compiler)  # bw_compiler=my_compiler

model_opt = torch.compile(model, backend=my_backend)
```

----------------------------------------

TITLE: Minimal Repro for TorchInductor Error (Python)
DESCRIPTION: This code is an example of a minified reproduction script generated by PyTorch's debugging tools. It defines a simple `Repro` module containing the problematic operation (`torch.ops.aten._foobar`) and uses `make_fx` and `compile_fx_inner` to compile and run it, demonstrating the minimal code required to trigger the original TorchInductor error. Requires `torch`, `torch.fx`, `torch._dynamo.testing`, `torch.fx.experimental.proxy_tensor`, and `torch._inductor.compile_fx`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting_old.rst#_snippet_2

LANGUAGE: python
CODE:
```
import torch
from torch import tensor, device
import torch.fx as fx
from torch._dynamo.testing import rand_strided
from math import inf
from torch.fx.experimental.proxy_tensor import make_fx

# torch version: 1.13.0a0+gitfddfc44
# torch cuda version: 11.6
# torch git version: fddfc4488afb207971c54ad4bf58130fdc8a4dc5


# CUDA Info:
# nvcc: NVIDIA (R) Cuda compiler driver
# Copyright (c) 2005-2022 NVIDIA Corporation
# Built on Thu_Feb_10_18:23:41_PST_2022
# Cuda compilation tools, release 11.6, V11.6.112
# Build cuda_11.6.r11.6/compiler.30978841_0

# GPU Hardware Info:
# NVIDIA A100-SXM4-40GB : 8

from torch.nn import *

class Repro(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, add):
        _foobar = torch.ops.aten._foobar.default(add);  add = None
        return (_foobar,)

args = [((200, 200), (200, 1), torch.float32, 'cpu')]
args = [rand_strided(shape, stride, dtype, device) for shape, stride, dtype, device in args]
mod = make_fx(Repro())(*args)
from torch._inductor.compile_fx import compile_fx_inner

compiled = compile_fx_inner(mod, args)
compiled(*args)
```

----------------------------------------

TITLE: Implementing Custom Cube Function with Auto-generated vmap
DESCRIPTION: Example implementation of a custom autograd Function that computes the cube of a tensor with automatic vmap rule generation. Includes forward, backward, and setup_context methods with proper tensor handling.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.func.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
class MyCube(torch.autograd.Function):
    generate_vmap_rule = True

    @staticmethod
    def forward(x):
        result = x ** 3
        dx = 3 * x ** 2
        return result, dx

    @staticmethod
    def setup_context(ctx, inputs, output):
        x, = inputs
        result, dx = output
        ctx.save_for_backward(x, dx)

    @staticmethod
    def backward(ctx, grad_output, grad_dx):
        x, dx = ctx.saved_tensors
        result = grad_output * dx + grad_dx * 6 * x
        return result

def my_cube(x):
    result, dx = MyCube.apply(x)
    return result

x = torch.randn(3)
result = torch.vmap(my_cube)(x)
assert torch.allclose(result, x ** 3)
```

----------------------------------------

TITLE: Executing Log Softmax Operation in PyTorch
DESCRIPTION: Performs the log softmax operation on a tensor of shape [256, 256008] with data type f16 and a specified dimension. No additional configurations are applied. Ensures numerical stability in softmax operations for large values.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/XGLMForCausalLM_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
"""Operator: aten._log_softmax.default\ncnt: 1, ((T([256, 256008], f16), 1, False), {})"""
```

----------------------------------------

TITLE: Sharing Local RRef using RPC (Python)
DESCRIPTION: This snippet demonstrates how an owner worker creates a local RRef and shares it directly with another worker using `dist.rpc_async`. This is the simplest owner-to-user sharing case where the owner manages the initial reference counting locally before sending.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/rref.rst#_snippet_2

LANGUAGE: Python
CODE:
```
import torch
import torch.distributed.rpc as RRef, rpc

# on worker B and worker C
def func(rref):
  pass

# on worker B, creating a local RRef
rref = RRef("data")
# say the rref has RRefId 100
dist.rpc_async('C', func, args=(rref, ))
```

----------------------------------------

TITLE: Accessing Saved Tensors in PyTorch Autograd
DESCRIPTION: Demonstrates how to access intermediary results saved during forward pass through the grad_fn attribute. Shows tensor creation, exponential operation, and verification of saved results using torch.allclose().
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/autograd.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> a = torch.tensor([0., 0., 0.], requires_grad=True)
>>> b = a.exp()
>>> print(isinstance(b.grad_fn, torch.autograd.graph.Node))
True
>>> print(dir(b.grad_fn))
['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_raw_saved_result', '_register_hook_dict', '_saved_result', 'metadata', 'name', 'next_functions', 'register_hook', 'register_prehook', 'requires_grad']
>>> print(torch.allclose(b.grad_fn._saved_result, b))
True
```

----------------------------------------

TITLE: Ragged Dimension Operations for PyTorch NJT
DESCRIPTION: This snippet outlines a strategy for operations on the ragged dimension of Nested Jagged Tensors. It involves converting to padded dense format, running the operation, and converting back to NJT. It also mentions the potential for fusion in torch.compile to avoid materializing padded intermediates.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_13

LANGUAGE: Python
CODE:
```
# For operations on ragged dimension
# 1. Convert to padded dense with appropriate padding value
# 2. Run the operation
# 3. Convert back to NJT
# Note: In torch.compile, these conversions can be fused
```

----------------------------------------

TITLE: Defining TorchScript Function - Python
DESCRIPTION: Demonstrates defining a TorchScript function using the `@torch.jit.script` decorator in Python. The function `f` takes two tensor inputs `a` and `b`, performs several arithmetic operations and a hyperbolic tangent (`torch.tanh`), and returns a computed tensor. This snippet is used as an example of Python code that gets converted into the JIT's internal Graph representation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_0

LANGUAGE: python
CODE:
```
@torch.jit.script
def f(a, b):
  c = a + b
  d = c * c
  e = torch.tanh(d * c)
  return d + (e + e)
```

----------------------------------------

TITLE: Using torch.cond for Control Flow in PyTorch
DESCRIPTION: This snippet shows how to use torch.cond for expressing if-else like control flow in PyTorch when dealing with data-dependent control flow.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_17

LANGUAGE: reStructuredText
CODE:
```
:ref:`torch.cond <cond>`
```

----------------------------------------

TITLE: Implementing AOT Autograd with NNC Compiler
DESCRIPTION: Uses PyTorch Neural Network Compiler (NNC) for operator fusion optimization
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/aot_autograd_optimizations.ipynb#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from functorch.compile import ts_compile

aot_nnc_fn = aot_function(fn, fw_compiler=ts_compile, bw_compiler=ts_compile)

cloned_inputs = [x.clone().detach().requires_grad_(True) for x in (a, b, c, d)]
cloned_a, cloned_b, cloned_c, cloned_d = cloned_inputs

res = aot_nnc_fn(*cloned_inputs)
loss = res.sum()
loss.backward()
assert torch.allclose(ref, res)
assert torch.allclose(a.grad, cloned_a.grad)
assert torch.allclose(b.grad, cloned_b.grad)
assert torch.allclose(c.grad, cloned_c.grad)
assert torch.allclose(d.grad, cloned_d.grad)
```

----------------------------------------

TITLE: Implementing MNIST Training Loop with Lazy Tensor mark_step()
DESCRIPTION: Defines the training loop function using Lazy Tensor, including the mark_step() call to break up the current trace and start asynchronous execution after each iteration.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
def train(log_interval, model, device, train_loader, optimizer, epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad(set_to_none=True)
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        torch._lazy.mark_step()

        if batch_idx % log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))
```

----------------------------------------

TITLE: Creating Composable Backends with Fallback Support
DESCRIPTION: This snippet shows how to create a composable backend that tries multiple backends in sequence with fallback support. It first attempts to use TensorRT, then falls back to Inductor if TensorRT fails, and finally returns the original module if both backends fail.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_custom_backends.rst#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from torch._dynamo import lookup_backend
def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):
    try:
        trt_compiled = lookup_backend("tensorrt")(gm, example_inputs)
        if trt_compiled is not None:
            return trt_compiled
    except Exception:
        pass
    # first backend failed, try something else...
    try:
        inductor_compiled = lookup_backend("inductor")(gm, example_inputs)
        if inductor_compiled is not None:
            return inductor_compiled
    except Exception:
        pass
    return gm.forward
```

----------------------------------------

TITLE: Creating EMA Model in PyTorch
DESCRIPTION: Example showing how to create an Exponential Moving Average (EMA) model with a specified decay rate.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_13

LANGUAGE: Python
CODE:
```
decay = 0.999
averaged_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(decay))
```

----------------------------------------

TITLE: Simple PyTorch Function Compilation Example
DESCRIPTION: A basic example showing a function decorated with torch.compile that contains a graph break (the print statement). This demonstrates how Dynamo handles operations before and after a non-PyTorch operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
import torch

@torch.compile
def fn(a):
    b = a + 2
    print("Hi")
    return b + a

fn(torch.randn(4))
```

----------------------------------------

TITLE: Demonstrating Various DTensor Distribution Strategies in PyTorch
DESCRIPTION: This code snippet shows different ways to construct DTensors, including row-wise sharding, column-wise sharding, replication, and combined sharding and replication strategies.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
from torch.distributed.tensor import DTensor, Shard, Replicate, distribute_tensor, distribute_module, init_device_mesh

device_mesh = init_device_mesh("cuda", (4,))
rowwise_placement=[Shard(0)]
colwise_placement=[Shard(1)]

big_tensor = torch.randn(888, 12)
rowwise_tensor = distribute_tensor(big_tensor, device_mesh=device_mesh, placements=rowwise_placement)

replica_placement = [Replicate()]
replica_tensor = distribute_tensor(big_tensor, device_mesh=device_mesh, placements=replica_placement)

device_mesh = init_device_mesh("cuda", (2, 2))
spec=[Replicate(), Shard(0)]
partial_replica = distribute_tensor(big_tensor, device_mesh=device_mesh, placements=spec)

local_tensor = torch.randn((8, 8), requires_grad=True)
rowwise_tensor = DTensor.from_local(local_tensor, device_mesh, rowwise_placement)

colwise_tensor = rowwise_tensor.redistribute(device_mesh, colwise_placement)
replica_tensor = colwise_tensor.redistribute(device_mesh, replica_placement)
```

----------------------------------------

TITLE: PyTorch Matrix Multiplication Operations
DESCRIPTION: Batch matrix multiplication operations between tensors of various shapes using half precision floating point.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
aten.bmm.default((T([4096, 64, 16], f16), T([4096, 16, 144], f16)))
aten.bmm.default((T([4096, 64, 144], f16), T([4096, 144, 32], f16)))
```

----------------------------------------

TITLE: Initializing GroupNorm in Place of BatchNorm2d - PyTorch (Python)
DESCRIPTION: Replaces a BatchNorm2d layer with a GroupNorm layer to avoid complications with running statistics during batched operations in Functorch. The first argument "C" is the number of channels, while "G" is the number of groups, which must evenly divide C. Set track_running_stats to False to ensure running means are not updated. No external dependencies are required beyond PyTorch's GroupNorm.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.batch_norm.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
BatchNorm2d(C, G, track_running_stats=False)
```

----------------------------------------

TITLE: New API for Module Attributes in TorchScript
DESCRIPTION: Shows the newer approach for defining module attributes in TorchScript using Python type annotations instead of torch.jit.Attribute.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
from typing import Dict

class MyModule(torch.nn.Module):
    my_dict: Dict[str, int]

    def __init__(self):
        super().__init__()
        # This type cannot be inferred and must be specified
        self.my_dict = {}

        # The attribute type here is inferred to be `int`
        self.my_int = 20

    def forward(self):
        pass

m = torch.jit.script(MyModule())
```

----------------------------------------

TITLE: Python ScriptModule Attribute Example
DESCRIPTION: Shows an example of defining attributes on a ScriptModule that will be handled by the TorchScript compiler.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
import torch

class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.x = 2

    def forward(self):
        return self.x

m = torch.jit.script(Model())
```

----------------------------------------

TITLE: Avoiding CPU Oversubscription in PyTorch Training (Python)
DESCRIPTION: This snippet shows how to adjust the number of threads in each subprocess to avoid CPU oversubscription during model training in PyTorch. It requires setting the number of threads using torch.set_num_threads() within each training process. Inputs include the number of vCPUs available (N) and the number of subprocesses (M), while outputs are efficient CPU utilization without oversubscription.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/multiprocessing.rst#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
def train(rank, args, model, device, dataset, dataloader_kwargs):
    torch.manual_seed(args.seed + rank)

    #### define the num threads used in current sub-processes
    torch.set_num_threads(floor(N/M))

    train_loader = torch.utils.data.DataLoader(dataset, **dataloader_kwargs)

    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)
    for epoch in range(1, args.epochs + 1):
        train_epoch(epoch, args, model, device, train_loader, optimizer)
```

----------------------------------------

TITLE: Converting Between Nested Jagged Tensors and Padded Tensors in PyTorch
DESCRIPTION: Shows how to convert a nested jagged tensor to a padded dense tensor using to_padded_tensor() with a specified padding value, and how to convert back using nested.narrow() with sequence lengths.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
>>> import torch
>>> a = torch.randn(2, 3)
>>> b = torch.randn(6, 3)
>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)
>>> padded = torch.nested.to_padded_tensor(nt, padding=4.2)
>>> padded
tensor([[[ 1.6107,  0.5723,  0.3913],
         [ 0.0700, -0.4954,  1.8663],
         [ 4.2000,  4.2000,  4.2000],
         [ 4.2000,  4.2000,  4.2000],
         [ 4.2000,  4.2000,  4.2000],
         [ 4.2000,  4.2000,  4.2000]],
        [[-0.0479, -0.7610, -0.3484],
         [ 1.1345,  1.0556,  0.3634],
         [-1.7122, -0.5921,  0.0540],
         [-0.5506,  0.7608,  2.0606],
         [ 1.5658, -1.1934,  0.3041],
         [ 0.1483, -1.1284,  0.6957]]])
```

LANGUAGE: python
CODE:
```
>>> padded = torch.randn(3, 5, 4)
>>> seq_lens = torch.tensor([3, 2, 5], dtype=torch.int64)
>>> nt = torch.nested.narrow(padded, dim=1, length=seq_lens, layout=torch.jagged)
>>> nt.shape
torch.Size([3, j1, 4])
>>> nt = nt.contiguous()
>>> nt.shape
torch.Size([3, j2, 4])
```

----------------------------------------

TITLE: Loading Batched Data from Iterable-style Dataset
DESCRIPTION: Demonstrates the equivalent operation of loading batched samples from an iterable-style dataset when automatic batching is enabled.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/data.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
dataset_iter = iter(dataset)
for indices in batch_sampler:
    yield collate_fn([next(dataset_iter) for _ in indices])
```

----------------------------------------

TITLE: Creating a Tensor with Custom Options in PyTorch C++
DESCRIPTION: This snippet demonstrates how to create a tensor using the full() factory function with custom TensorOptions. It also shows how to verify the tensor's properties.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_6

LANGUAGE: cpp
CODE:
```
torch::Tensor tensor = torch::full({3, 4}, /*value=*/123, options);

assert(tensor.dtype() == torch::kFloat32);
assert(tensor.layout() == torch::kStrided);
assert(tensor.device().type() == torch::kCUDA); // or device().is_cuda()
assert(tensor.device().index() == 1);
assert(tensor.requires_grad());
```

----------------------------------------

TITLE: Replacing Add with Mul in PyTorch FX Graph
DESCRIPTION: This snippet demonstrates how to replace torch.ops.aten.add.Tensor calls with torch.ops.aten.mul.Tensor calls in a PyTorch FX graph.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch

def replace_add_with_mul(gm: torch.fx.GraphModule) -> torch.fx.GraphModule:
    for node in gm.graph.nodes:
        if node.op == "call_function" and node.target == torch.ops.aten.add.Tensor:
            node.target = torch.ops.aten.mul.Tensor
```

----------------------------------------

TITLE: Discovering All Unconvertible ATen Ops (Part 1: Calling Utility)
DESCRIPTION: Demonstrates how to use torch.onnx.utils.unconvertible_ops to identify ATen operators within a PyTorch model that cannot be converted to ONNX operators for a given opset version. It takes the model, arguments, and opset version as input.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_34

LANGUAGE: Python
CODE:
```
# prepare model, args, opset_version
...

torch_script_graph, unconvertible_ops = torch.onnx.utils.unconvertible_ops(
    model, args, opset_version=opset_version
)
```

----------------------------------------

TITLE: Batch Matrix Multiplication (bmm) for 3D Tensors in PyTorch (Python)
DESCRIPTION: Usage of batched matrix-matrix (bmm) multiplications for sequences of matrices (3D tensors), commonly used in transformer models for attention score computation. Requires matching batch and inner dimensions; supports explicit stride specifications for efficiency. Inputs/outputs are FP16 tensors, and improper dimensions will raise runtime exceptions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
Operator: aten.bmm.default
cnt: 12, ((T([48, 128, 64], f16), T([48, 64, 128], f16)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.bmm.default
cnt: 12, ((T([48, 128, 128], f16), T([48, 128, 64], f16)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.bmm.default
cnt: 12, ((T([48, 128, 128], f16, stride=(16384, 1, 128)), T([48, 128, 64], f16)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.bmm.default
cnt: 12, ((T([48, 128, 64], f16), T([48, 64, 128], f16, stride=(8192, 1, 64))), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.bmm.default
cnt: 12, ((T([48, 64, 128], f16, stride=(8192, 1, 64)), T([48, 128, 128], f16)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.bmm.default
cnt: 12, ((T([48, 128, 128], f16), T([48, 128, 64], f16, stride=(8192, 1, 128))), {})
```

----------------------------------------

TITLE: Analyzing ATen Copy Operations in PyTorch
DESCRIPTION: Records instances of the \"aten._to_copy.default\" operator, specifying how tensors are cast to different data types like float16 from float32 with potential layout and device considerations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_4

LANGUAGE: pseudocode
CODE:
```
Operator: aten._to_copy.default
cnt: 1, ((T([128, 128], f32),), {'dtype': f16})
cnt: 1, ((T([8, 1, 128, 128], f16, stride=(0, 16384, 128, 1)),), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})
```

----------------------------------------

TITLE: Usage Log: aten.nll_loss_forward.default Operator (Text)
DESCRIPTION: Logs a call to the `aten.nll_loss_forward.default` operator, used for computing the negative log likelihood loss. The log shows the arguments including the input tensor shape ([128, 1000] f16), target tensor shape ([128] i64), reduction type (1, likely 'mean'), and ignore index (-100).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dm_nfnet_f0_training.txt#2025-04-22_snippet_4

LANGUAGE: text
CODE:
```
Operator: aten.nll_loss_forward.default
cnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})
```

----------------------------------------

TITLE: Copying Tensor Content Using aten.copy_
DESCRIPTION: Using aten.copy_.default facilitates copying data from one tensor to another. It maintains dependencies on PyTorch and executes in-place operations altering target tensors with content parallel to the source tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForConditionalGeneration_training.txt#2025-04-22_snippet_10

LANGUAGE: Python
CODE:
```
Operator: aten.copy_.default
cnt: 2, ((T([8, 128], i64), T([8, 128], i64)), {})
cnt: 1, ((T([8, 127], i64, stride=(128, 1)), T([8, 127], i64)), {})
cnt: 1, ((T([8], i64, stride=(128,)), T([8], i64)), {})
```

----------------------------------------

TITLE: Loss Calculation Operations
DESCRIPTION: NLL (Negative Log Likelihood) loss forward and backward calculations, typically used with classification tasks. Includes operations on tensors with shape [32, 1000].
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convmixer_768_32_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
aten.nll_loss_forward.default((T([32, 1000], f16), T([32], i64), None, 1, -100), {})
aten.nll_loss_backward.default((T([], f16), T([32, 1000], f16), T([32], i64), None, 1, -100, T([], f16)), {})
```

----------------------------------------

TITLE: Installing Decompilation Hooks with Depyf
DESCRIPTION: Shows how to install decompilation hooks using the depyf library to convert Python bytecode into human-readable source code for better debugging of TorchDynamo's behavior.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_overview.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import depyf
depyf.install()
```

----------------------------------------

TITLE: Saving Only Tensor Data by Cloning to New Storage - PyTorch - Python
DESCRIPTION: Describes a technique to reduce saved file size by cloning a tensor before saving, breaking its view relationship and storing only its raw data. Suitable when the view relationship is not required after loading. Inputs are a tensor view; output is a smaller file and a loaded tensor whose storage contains only its own elements. Uses torch's clone() method; same requirements as other snippets.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> large = torch.arange(1, 1000)
>>> small = large[0:5]
>>> torch.save(small.clone(), 'small.pt')  # saves a clone of small
>>> loaded_small = torch.load('small.pt')
>>> loaded_small.storage().size()
5
```

----------------------------------------

TITLE: Performing Matrix-Vector Multiplication with CSR Tensor in PyTorch
DESCRIPTION: Demonstrates matrix-vector multiplication operation with a CSR tensor using the matmul method. The example multiplies a previously defined CSR tensor with a random vector.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
vec = torch.randn(4, 1, dtype=torch.float64)
sp.matmul(vec)
```

----------------------------------------

TITLE: Building an FX Graph for Tensor Addition in PyTorch (Python)
DESCRIPTION: This Python snippet contains both a functional example and its corresponding FX intermediate graph. The code defines a function performing tensor addition, while the graph block illustrates how placeholders and call_function nodes are used and returned in FX. This demonstrates basic graph tracing for simple tensor operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
def add_one(x):
  return torch.ops.aten(x, 1)
```

LANGUAGE: python
CODE:
```
graph():
  %ph_0 : [#users=1] = placeholder[target=ph_0]
  %add_tensor : [#users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%ph_0, 1), kwargs = {})
  return [add_tensor]
```

----------------------------------------

TITLE: Using torch.distributed.rpc.rpc_sync() in TorchScript
DESCRIPTION: Makes a blocking RPC call to run a function on a remote worker. RPC messages are sent and received in parallel to execution of Python code.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_33

LANGUAGE: python
CODE:
```
torch.distributed.rpc.rpc_sync()
```

----------------------------------------

TITLE: Inspecting TorchScript IR Graphs Using .graph Attribute (Python)
DESCRIPTION: Shows how to print and inspect the TorchScript intermediate representation (IR) graph for a scripted function. Requires PyTorch. Uses a loop and conditional in the scripted function 'foo', and prints 'foo.graph' to observe the static single assignment (SSA) IR comprised of ATen and primitive operators. Useful for low-level debugging and performance analysis.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
@torch.jit.script\ndef foo(len):\n    # type: (int) -> torch.Tensor\n    rv = torch.zeros(3, 4)\n    for i in range(len):\n        if i < 10:\n            rv = rv - 1.0\n        else:\n            rv = rv + 1.0\n    return rv\n\nprint(foo.graph)\n
```

----------------------------------------

TITLE: Migrating to PyTorch 1.2 Recursive Scripting API Example
DESCRIPTION: Demonstrates how to convert a standard nn.Module to a ScriptModule using the PyTorch 1.2 recursive scripting API with torch.jit.script().
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

my_model = Model()
my_scripted_model = torch.jit.script(my_model)
```

----------------------------------------

TITLE: Implement Autograd Function with Static Symbolic Method (Python)
DESCRIPTION: Demonstrates how to implement a `torch.autograd.Function` subclass (`MyRelu`) that includes a static `symbolic` method. This method defines the ONNX representation for the function's forward pass, allowing it to be exported without needing a separate registered symbolic function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_19

LANGUAGE: python
CODE:
```
class MyRelu(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input: torch.Tensor) -> torch.Tensor:
        ctx.save_for_backward(input)
        return input.clamp(min=0)

    @staticmethod
    def symbolic(g: torch.Graph, input: torch.Value) -> torch.Value:
        return g.op("Clip", input, g.op("Constant", value_t=torch.tensor(0, dtype=torch.float)))
```

----------------------------------------

TITLE: Building libtorch Using CMake
DESCRIPTION: This snippet provides a complete example of building libtorch using CMake. It clones the PyTorch repository, creates a build directory, configures the build with CMake, and installs the library.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/libtorch.rst#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
git clone -b main --recurse-submodule https://github.com/pytorch/pytorch.git
mkdir pytorch-build
cd pytorch-build
cmake -DBUILD_SHARED_LIBS:BOOL=ON -DCMAKE_BUILD_TYPE:STRING=Release -DPYTHON_EXECUTABLE:PATH=`which python3` -DCMAKE_INSTALL_PREFIX:PATH=../pytorch-install ../pytorch
cmake --build . --target install
```

----------------------------------------

TITLE: Seeding Global NumPy Random Number Generator in Python
DESCRIPTION: Seeds the global random number generator provided by the NumPy library using `np.random.seed(0)`. This is crucial for reproducibility if the application or any of its dependencies utilize `numpy.random` functions. Note that this does not affect NumPy `Generator` objects, which require separate seeding.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/randomness.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import numpy as np
np.random.seed(0)
```

----------------------------------------

TITLE: Applying Average Pooling with ATen AvgPool2D Operator
DESCRIPTION: Demonstrates `aten.avg_pool2d`, used in downsampling in neural networks. Tensors reduced using pool size [2, 2], with strides [2, 2]. Critical for maintaining feature maps while reducing spatial dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_7

LANGUAGE: plaintext
CODE:
```
Operator: aten.avg_pool2d.default
cnt: 1, ((T([64, 512, 16, 16], f16), [2, 2], [2, 2]), {})
```

----------------------------------------

TITLE: Replacing Nodes in PyTorch FX Graph in Python
DESCRIPTION: This example provides a method to replace node operations in a PyTorch FX graph using the 'inserting_after' context. It depends on 'torch' and 'torch.fx'. The snippet replaces node uses with a new operation, like adding a ReLU function, using the 'replace_all_uses_with' API. This is useful for modifying computation flow in traced graphs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
# Specifies the insertion point. Any nodes added to the
# Graph within this scope will be inserted after `node`
with traced.graph.inserting_after(node):
    # Insert a new `call_function` node calling `torch.relu`
    new_node = traced.graph.call_function(
        torch.relu, args=(node,))

    # We want all places that used the value of `node` to
    # now use that value after the `relu` call we've added.
    # We use the `replace_all_uses_with` API to do this.
    node.replace_all_uses_with(new_node)
```

----------------------------------------

TITLE: Correct Use of In-Place Operations under vmap - PyTorch - Python
DESCRIPTION: This snippet gives the correct pattern for using in-place operations under a vmap transform. Here, both tensors input to 'f' are batched, ensuring matching shapes for in-place modification. The code verifies correctness by comparing to expected output. Dependencies: PyTorch, torch.func. All tensors are batched consistently. Use this approach rather than applying in-place ops to non-batched tensors in vmap.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
def f(x, y):
  x.add_(y)
  return x

x = torch.randn(3, 1)
y = torch.randn(3, 1)
expected = x + y

# Does not raise an error because x is being vmapped over.
vmap(f, in_dims=(0, 0))(x, y)
assert torch.allclose(x, expected)
```

----------------------------------------

TITLE: Vectorized Ensemble Prediction with vmap (Shared Minibatch) - functorch/PyTorch - Python
DESCRIPTION: Uses vmap with custom in_dims to apply the same minibatch across the stacked ensemble, enabling fast prediction for multiple models using identical inputs. An assertion checks numerical equivalence with for-loop results. Requires parameters and buffers to be stacked appropriately and minibatch dimensions to match model input.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/ensembling.ipynb#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
predictions2_vmap = vmap(fmodel, in_dims=(0, 0, None))(params, buffers, minibatch)

assert torch.allclose(predictions2_vmap, torch.stack(predictions2), atol=1e-3, rtol=1e-5)
```

----------------------------------------

TITLE: Handling Tensor Gradients in PyTorch
DESCRIPTION: Performs operations related to gradient computation and backpropagation, including selecting and slicing tensors for gradient calculations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/tts_angular_training.txt#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
aten.select_backward.default(T([64, 256], f16), [64, 50, 256], 1, -1)
```

LANGUAGE: Python
CODE:
```
aten.slice_backward.default(T([64, 50, 256], f16), [64, 50, 256], 0, 0, 9223372036854775807, 1)
```

----------------------------------------

TITLE: Grouping Data with PyTorch DataPipes
DESCRIPTION: Illustrates the use of the groupby() method to group data based on a key function. Shows examples of grouping with different parameters and handling of buffer sizes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(10).shuffle().groupby(lambda x: x % 3)
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(10).batch(3).groupby(lambda x: len(x))
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(10).batch(3).groupby(lambda x: x % 3, unbatch_level = 1)
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(15).shuffle().groupby(lambda x: x % 3, buffer_size = 5)
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(18).shuffle().groupby(lambda x: x % 3, group_size = 3)
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(15).shuffle().groupby(lambda x: x % 3, group_size = 3, guaranteed_group_size = 2)
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(15).shuffle().groupby(lambda x: x % 3, guaranteed_group_size = 2)
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(15).groupby(lambda x: x % 3, guaranteed_group_size = 2, buffer_size = 6)
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(15).shuffle().groupby(lambda x: x % 3, guaranteed_group_size = 2, buffer_size = 6)
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(15).shuffle().groupby(lambda x: x % 3, guaranteed_group_size = 2, buffer_size = 6, drop_remaining = True)
for i in dp:
    print(i)
```

----------------------------------------

TITLE: Defining a TorchScript Class with Initialization Constraints
DESCRIPTION: Example showing how TorchScript classes require member declaration in __init__(). Attempting to assign to self outside of __init__() will result in a runtime error.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
@torch.jit.script
class Foo:
  def assign_x(self):
    self.x = torch.rand(2, 3)
```

----------------------------------------

TITLE: Using torch.device as a Context Manager for Default Device Selection
DESCRIPTION: This code snippet demonstrates how to use torch.device as a context manager to change the default device that tensors are allocated on within a specific code block, and verifies the device assignment with the device property.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> with torch.device('cuda:1'):
...     r = torch.randn(2, 3)
>>> r.device
device(type='cuda', index=1)
```

----------------------------------------

TITLE: Computing Hessian using torch.func.hessian in Python
DESCRIPTION: This snippet shows the usage of the convenience function `torch.func.hessian`. This function simplifies Hessian computation by internally combining forward-mode (`jacfwd`) and reverse-mode (`jacrev`) automatic differentiation. It calculates the Hessian of the function `f` (sum of sines) at the input `x` directly.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.whirlwind_tour.rst#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from torch.func import hessian

def f(x):
    return x.sin().sum()

x = torch.randn(5)
hess = hessian(f)(x)
```

----------------------------------------

TITLE: Summarizing Tensor Multiplication and Elementwise Calls (aten.mul) - PyTorch - Python
DESCRIPTION: The snippet lists argument patterns for the aten.mul (Tensor) operator, spanning scalar-tensor, tensor-tensor, and broadcasting multiplication combinations across numerous tensor ranks and shapes. Includes half precision types and a variety of scales, as seen in normalization and scale operations. Inputs are tensors and/or scalars, only operator config and usage sampled.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_12

LANGUAGE: Python
CODE:
```
Operator: aten.mul.Tensor
cnt: 2, ((T([16, 1, 1, 1], f16), 0.34412564994580647), {})
cnt: 2, ((T([32, 1, 1, 1], f16), 0.1490107774734497), {})
cnt: 2, ((T([64, 1, 1, 1], f16), 0.10536653122135592), {})
cnt: 10, ((T([128, 1, 1, 1], f16), 0.07450538873672485), {})
cnt: 2, ((T([128, 128, 56, 56], f16), 1.0), {})
cnt: 2, ((T([256, 1, 1, 1], f16), 0.1580497968320339), {})
cnt: 2, ((T([64, 1, 1, 1], f16), 0.1580497968320339), {})
cnt: 4, ((T([64, 1, 1, 1], f16), 0.07450538873672485), {})
cnt: 2, ((T([256, 1, 1, 1], f16), 0.22351616621017456), {})
cnt: 2, ((T([128, 256, 56, 56], f16), T([128, 256, 1, 1], f16)), {})
cnt: 2, ((T([128, 256, 56, 56], f16), 2.0), {})
cnt: 2, ((T([128, 256, 56, 56], f16), 0.2), {})
cnt: 2, ((T([128, 256, 56, 56], f16), 0.9805806756909201), {})
cnt: 2, ((T([512, 1, 1, 1], f16), 0.11175808310508728), {})
cnt: 2, ((T([128, 1, 1, 1], f16), 0.11175808310508728), {})
cnt: 4, ((T([512, 1, 1, 1], f16), 0.1580497968320339), {})
cnt: 4, ((T([128, 512, 28, 28], f16), T([128, 512, 1, 1], f16)), {})
cnt: 4, ((T([128, 512, 28, 28], f16), 2.0), {})
cnt: 4, ((T([128, 512, 28, 28], f16), 0.2), {})
cnt: 2, ((T([128, 512, 28, 28], f16), 0.9805806756909201), {})
cnt: 2, ((T([128, 1, 1, 1], f16), 0.07902489841601695), {})
cnt: 2, ((T([128, 512, 28, 28], f16), 0.9622504486493761), {})
cnt: 2, ((T([1536, 1, 1, 1], f16), 0.07902489841601695), {})
cnt: 2, ((T([384, 1, 1, 1], f16), 0.07902489841601695), {})
cnt: 36, ((T([384, 1, 1, 1], f16), 0.07450538873672485), {})
cnt: 18, ((T([1536, 1, 1, 1], f16), 0.09125009274634042), {})
cnt: 12, ((T([128, 1536, 14, 14], f16), T([128, 1536, 1, 1], f16)), {})
cnt: 12, ((T([128, 1536, 14, 14], f16), 2.0), {})
cnt: 12, ((T([128, 1536, 14, 14], f16), 0.2), {})
cnt: 2, ((T([128, 1536, 14, 14], f16), 0.9805806756909201), {})
cnt: 16, ((T([384, 1, 1, 1], f16), 0.04562504637317021), {})
cnt: 2, ((T([128, 1536, 14, 14], f16), 0.9622504486493761), {})
cnt: 2, ((T([128, 1536, 14, 14], f16), 0.9449111825230679), {})
cnt: 2, ((T([128, 1536, 14, 14], f16), 0.9284766908852592), {})
cnt: 2, ((T([128, 1536, 14, 14], f16), 0.9128709291752768), {})
cnt: 2, ((T([128, 1536, 14, 14], f16), 0.8980265101338745), {})
cnt: 2, ((T([1536, 1, 1, 1], f16), 0.04562504637317021), {})
cnt: 6, ((T([128, 1536, 7, 7], f16), T([128, 1536, 1, 1], f16)), {})
cnt: 6, ((T([128, 1536, 7, 7], f16), 2.0), {})
cnt: 6, ((T([128, 1536, 7, 7], f16), 0.2), {})
cnt: 2, ((T([128, 1536, 7, 7], f16), 0.9805806756909201), {})
cnt: 2, ((T([128, 1536, 7, 7], f16), 0.9622504486493761), {})
cnt: 2, ((T([2304, 1, 1, 1], f16), 0.04562504637317021), {})
cnt: 3, ((T([128, 1536, 7, 7], f16), T([128, 1536, 7, 7], f16)), {})
cnt: 6, ((T([128, 1536, 14, 14], f16), T([128, 1536, 14, 14], f16)), {})
cnt: 2, ((T([128, 512, 28, 28], f16), T([128, 512, 28, 28], f16)), {})
cnt: 1, ((T([128, 256, 56, 56], f16), T([128, 256, 56, 56], f16)), {})
```

----------------------------------------

TITLE: Using collections.namedtuple and typing.NamedTuple with TorchScript (Python)
DESCRIPTION: This snippet shows usage of both typing.NamedTuple and collections.namedtuple to define tuple-like data structures for TorchScript functions. The function inc takes an _AnnotatedNamedTuple and returns a tuple of incremented values. It demonstrates compatibility between annotated and unannotated named tuples in TorchScript. Prerequisites: torch, typing.NamedTuple, collections.namedtuple. Expects appropriate namedtuple input; outputs incremented tuple.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import torch
from typing import NamedTuple
from typing import Tuple
from collections import namedtuple

_AnnotatedNamedTuple = NamedTuple('_NamedTupleAnnotated', [('first', int), ('second', int)])
_UnannotatedNamedTuple = namedtuple('_NamedTupleAnnotated', ['first', 'second'])

def inc(x: _AnnotatedNamedTuple) -> Tuple[int, int]:
    return (x.first+1, x.second+1)

m = torch.jit.script(inc)
print(inc(_UnannotatedNamedTuple(1,2)))
```

----------------------------------------

TITLE: Utilizing PyTorch aten.add.Tensor Operator in Python
DESCRIPTION: This code snippet demonstrates the use of the 'aten.add.Tensor' operator in PyTorch for element-wise tensor addition. It involves several tensor shapes and datatypes (e.g., f16), highlighting the use of broadcasting for different tensor sizes. This operation is a fundamental part of many neural networks for combining inputs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnext50_32x4d_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
cnt: 2, ((T([8, 2048, 7, 7], f16), T([8, 2048, 7, 7], f16)), {})
cnt: 6, ((T([8, 1024, 14, 14], f16), T([8, 1024, 14, 14], f16)), {})
cnt: 4, ((T([8, 512, 28, 28], f16), T([8, 512, 28, 28], f16)), {})
cnt: 3, ((T([8, 256, 56, 56], f16), T([8, 256, 56, 56], f16)), {})
cnt: 1, ((T([8, 64, 56, 56], f16), T([8, 64, 56, 56], f16)), {})
```

----------------------------------------

TITLE: Using wrap_numpy Decorator for CUDA Tensor Operations
DESCRIPTION: Demonstrates using torch.compiler.wrap_numpy decorator to handle CUDA tensors directly without data movement between CPU and GPU memory.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
@torch.compile(fullgraph=True)
@torch.compiler.wrap_numpy
def numpy_fn(X, Y):
    return np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1))

X = torch.randn(1024, 64, device="cuda")
Y = torch.randn(1024, 64, device="cuda")
Z = numpy_fn(X, Y)
assert isinstance(Z, torch.Tensor)
assert Z.device.type == "cuda"
```

----------------------------------------

TITLE: Matrix Multiplication with aten.mm in Python
DESCRIPTION: The aten.mm.default operator is used for matrix multiplication between two 2D tensors. This is a fundamental operation in linear algebra applications, including machine learning models that require matrix transformations based on input and weight matrices. Compatibility in dimensions and data types (f16) is necessary for successful execution.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/nvidia_deeprecommender_training.txt#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
Operator: aten.mm.default
cnt: 1, ((T([256, 197951], f16), T([197951, 512], f16)), {})
cnt: 1, ((T([197951, 256], f16, stride=(1, 197951)), T([256, 512], f16)), {})
cnt: 2, ((T([256, 512], f16), T([512, 512], f16)), {})
cnt: 2, ((T([512, 256], f16, stride=(1, 512)), T([256, 512], f16)), {})
cnt: 1, ((T([256, 512], f16), T([512, 1024], f16)), {})
cnt: 1, ((T([512, 256], f16, stride=(1, 512)), T([256, 1024], f16)), {})
cnt: 1, ((T([256, 1024], f16), T([1024, 512], f16)), {})
cnt: 1, ((T([1024, 256], f16, stride=(1, 1024)), T([256, 512], f16)), {})
cnt: 1, ((T([512, 256], f16, stride=(1, 512)), T([256, 197951], f16)), {})
```

----------------------------------------

TITLE: ModuleList For Loop Example in TorchScript
DESCRIPTION: Shows how for loops over nn.ModuleList unroll at compile time with each module member.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_31

LANGUAGE: python
CODE:
```
class SubModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(2))

    def forward(self, input):
        return self.weight + input

class MyModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.mods = torch.nn.ModuleList([SubModule() for i in range(10)])

    def forward(self, v):
        for module in self.mods:
            v = module(v)
        return v

model = torch.jit.script(MyModule())
```

----------------------------------------

TITLE: Function with Data-Dependent Control Flow under vmap (Unsupported) - PyTorch - Python
DESCRIPTION: This Python snippet shows a function using data-dependent control flow (if-statement with tensor value) and how it fails under vmap. relu uses a Tensor in an if-condition, which isn't supported by vmap due to ambiguity over batching. Dependencies: PyTorch, torch.func. Limitation: Avoid using Tensors as conditions in control flow when using vmap.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
def relu(x):
  if x > 0:
    return x
  return 0

x = torch.randn(3)
vmap(relu)(x)
```

----------------------------------------

TITLE: Using comptime breakpoint in Python with torch.compile
DESCRIPTION: This snippet demonstrates how to use a comptime breakpoint within a function decorated with torch.compile. It allows for inspecting Dynamo state at a specific location in the user code being traced.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
from torch._dynamo.comptime import comptime

@torch.compile
def f(...):
    ...
    comptime.breakpoint()
    ...
```

----------------------------------------

TITLE: Basic PyTorch Tensor Operation Example
DESCRIPTION: Demonstrates a simple PyTorch script with tensor operations that can be fused, including addition and ReLU activation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/cuda/README.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch

def forward(x):
    o = x + 1.0
    o = o.relu()
    return o

shape = (2, 32, 128, 512)
input = torch.rand(*shape).cuda()
t = torch.jit.script(forward)

with torch.jit.fuser("fuser2"):
    for k in range(4):
        o = t(input)
```

----------------------------------------

TITLE: Type and Device Conversion with aten._to_copy in PyTorch (Python)
DESCRIPTION: These snippets illustrate ATen's _to_copy for data type and device transformations of tensors, such as converting float32 to float16 or int64 to int32 on CUDA devices. Requirements include that input tensor dtype and optionally layout or device be specified. Key parameters are the input tensor and desired target dtype, affecting downstream performance and compatibility.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
Operator: aten._to_copy.default
cnt: 1, ((T([4, 1, 1, 128], f32),), {'dtype': f16})
```

LANGUAGE: python
CODE:
```
Operator: aten._to_copy.default
cnt: 1, ((T([4, 128], b8),), {'dtype': i32})
```

LANGUAGE: python
CODE:
```
Operator: aten._to_copy.default
cnt: 1, ((T([4, 128], i64),), {'dtype': i32, 'layout': torch.strided, 'device': 'cuda'})
```

LANGUAGE: python
CODE:
```
Operator: aten._to_copy.default
cnt: 1, ((T([4, 128], i32),), {'dtype': i64})
```

----------------------------------------

TITLE: Complete EMA Training Loop Implementation
DESCRIPTION: Full example of implementing EMA training including model creation and batch norm updates.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_17

LANGUAGE: Python
CODE:
```
loader, optimizer, model, loss_fn = ...
ema_model = torch.optim.swa_utils.AveragedModel(model, \
            multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(0.999))

for epoch in range(300):
      for input, target in loader:
          optimizer.zero_grad()
          loss_fn(model(input), target).backward()
          optimizer.step()
          ema_model.update_parameters(model)

# Update bn statistics for the ema_model at the end
torch.optim.swa_utils.update_bn(loader, ema_model)
# Use ema_model to make predictions on test data
preds = ema_model(test_input)
```

----------------------------------------

TITLE: Unbatching Data with PyTorch DataPipes
DESCRIPTION: Shows how to use the unbatch() method to reverse batching operations. Demonstrates unbatching at different levels using the unbatch_level argument.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(10).batch(3).shuffle().unbatch()
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(40).batch(2).batch(4).batch(3).unbatch(unbatch_level = 2)
for i in dp:
    print(i)
```

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(40).batch(2).batch(4).batch(3).unbatch(unbatch_level = -1)
for i in dp:
    print(i)
```

----------------------------------------

TITLE: Using Custom Quantization with FX Graph Mode
DESCRIPTION: Example demonstrating custom quantization module usage with PyTorch's FX graph mode quantization including configuration setup and execution.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_12

LANGUAGE: Python
CODE:
```
m = torch.nn.Sequential(CustomModule()).eval()
qconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_qconfig)
prepare_custom_config_dict = {
    "float_to_observed_custom_module_class": {
        "static": {
            CustomModule: ObservedCustomModule,
        }
    }
}
convert_custom_config_dict = {
    "observed_to_quantized_custom_module_class": {
        "static": {
            ObservedCustomModule: StaticQuantCustomModule,
        }
    }
}
mp = torch.ao.quantization.quantize_fx.prepare_fx(
    m, qconfig_mapping, torch.randn(3,3), prepare_custom_config=prepare_custom_config_dict)
# calibration (not shown)
mq = torch.ao.quantization.quantize_fx.convert_fx(
    mp, convert_custom_config=convert_custom_config_dict)
```

----------------------------------------

TITLE: Analyzing Tensor Shapes and Counts in PyTorch
DESCRIPTION: This code snippet represents a series of tensor shape analyses, likely for a PyTorch model. Each line shows the count of occurrences, tensor shapes, data types (f16 for float16), and stride information where applicable. The tensors have varying dimensions (1D to 4D) and sizes, suggesting different layers or operations in a neural network architecture.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_15

LANGUAGE: Python
CODE:
```
cnt: 2, ((T([128, 240, 1, 1], f16), T([128, 240, 1, 1], f16), 0), {})
cnt: 4, ((T([128, 480, 7, 7], f16, stride=(47040, 49, 7, 1)), T([128, 480, 7, 7], f16), 0), {})
cnt: 4, ((T([128, 480, 7, 7], f16), T([128, 480, 7, 7], f16), 0), {})
cnt: 2, ((T([128, 168, 1, 1], f16), T([128, 168, 1, 1], f16), 0), {})
cnt: 2, ((T([128, 336, 14, 14], f16, stride=(131712, 196, 14, 1)), T([128, 336, 14, 14], f16), 0), {})
cnt: 2, ((T([128, 336, 14, 14], f16), T([128, 336, 14, 14], f16), 0), {})
cnt: 1, ((T([128, 120, 1, 1], f16), T([128, 120, 1, 1], f16), 0), {})
cnt: 1, ((T([128, 240, 14, 14], f16, stride=(94080, 196, 14, 1)), T([128, 240, 14, 14], f16), 0), {})
cnt: 1, ((T([128, 240, 14, 14], f16), T([128, 240, 14, 14], f16), 0), {})
cnt: 2, ((T([128, 92, 14, 14], f16, stride=(36064, 196, 14, 1)), T([128, 92, 14, 14], f16), 0), {})
cnt: 2, ((T([128, 92, 14, 14], f16), T([128, 92, 14, 14], f16), 0), {})
cnt: 1, ((T([128, 100, 14, 14], f16, stride=(39200, 196, 14, 1)), T([128, 100, 14, 14], f16), 0), {})
cnt: 1, ((T([128, 100, 14, 14], f16), T([128, 100, 14, 14], f16), 0), {})
cnt: 1, ((T([128, 120, 28, 28], f16, stride=(188160, 784, 28, 1)), T([128, 120, 28, 28], f16), 0), {})
cnt: 1, ((T([128, 120, 28, 28], f16), T([128, 120, 28, 28], f16), 0), {})
cnt: 1, ((T([128, 32, 1, 1], f16), T([128, 32, 1, 1], f16), 0), {})
cnt: 1, ((T([128, 60, 28, 28], f16, stride=(94080, 784, 28, 1)), T([128, 60, 28, 28], f16), 0), {})
cnt: 1, ((T([128, 60, 28, 28], f16), T([128, 60, 28, 28], f16), 0), {})
cnt: 1, ((T([128, 20, 1, 1], f16), T([128, 20, 1, 1], f16), 0), {})
cnt: 2, ((T([128, 36, 56, 56], f16, stride=(225792, 3136, 56, 1)), T([128, 36, 56, 56], f16), 0), {})
cnt: 2, ((T([128, 36, 56, 56], f16), T([128, 36, 56, 56], f16), 0), {})
cnt: 1, ((T([128, 24, 112, 112], f16, stride=(602112, 12544, 112, 1)), T([128, 24, 112, 112], f16), 0), {})
cnt: 1, ((T([128, 24, 112, 112], f16), T([128, 24, 112, 112], f16), 0), {})
cnt: 1, ((T([128, 8, 112, 112], f16, stride=(200704, 12544, 112, 1)), T([128, 8, 112, 112], f16), 0), {})
cnt: 1, ((T([128, 8, 112, 112], f16), T([128, 8, 112, 112], f16), 0), {})
cnt: 1, ((T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16), 0), {})
```

----------------------------------------

TITLE: Analyzing PyTorch Sparse Tensor Usage Patterns
DESCRIPTION: Detailed log showing tensor specifications with varying indices but consistent shape patterns. Each entry shows count (cnt), tensor dimensions, and configuration including float16 dtype, sparse_coo layout, and CUDA device placement.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
cnt: 4, ((1, 1, [965, 192], T([1, 54738], i64), T([54738, 192], f16)), {'dtype': f16, 'layout': torch.sparse_coo, 'device': 'cuda', 'pin_memory': None})
```

----------------------------------------

TITLE: Analyzing PyTorch Batch Normalization Operations
DESCRIPTION: This snippet demonstrates the usage of the aten.native_batch_norm.default operator for performing batch normalization on tensors with various shapes and parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
Operator: aten.native_batch_norm.default
cnt: 5, ((T([3, 64, 512, 512], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})
cnt: 5, ((T([3, 128, 256, 256], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), False, 0.1, 1e-05), {})
# ... (truncated for brevity)
```

----------------------------------------

TITLE: Profiling aten.native_batch_norm_backward.default Calls - PyTorch - Python
DESCRIPTION: Tracks backward batch normalization operations using aten.native_batch_norm_backward.default with various tensor and scalar argument patterns. Inputs include input, grad_output, weight, saved_mean, saved_invstd, running_mean, running_var, as well as options for affine and epsilon. Output consists of gradients for input, weight, and bias, supporting analysis of backpropagation memory patterns.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/selecsls42b_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
Operator: aten.native_batch_norm_backward.default
cnt: 1, ((T([128, 1024, 4, 4], f16), T([128, 1024, 4, 4], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 1280, 4, 4], f16), T([128, 1280, 4, 4], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f32), T([1280], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 1024, 7, 7], f16), T([128, 1024, 7, 7], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 960, 7, 7], f16), T([128, 960, 7, 7], f16), T([960], f16), T([960], f16), T([960], f16), T([960], f32), T([960], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 480, 14, 14], f16), T([128, 480, 14, 14], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f32), T([480], f32), True, 1e-05, [True, True, True]), {})
cnt: 4, ((T([128, 152, 14, 14], f16), T([128, 152, 14, 14], f16), T([152], f16), T([152], f16), T([152], f16), T([152], f32), T([152], f32), True, 1e-05, [True, True, True]), {})
cnt: 7, ((T([128, 304, 14, 14], f16), T([128, 304, 14, 14], f16), T([304], f16), T([304], f16), T([304], f16), T([304], f32), T([304], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 288, 28, 28], f16), T([128, 288, 28, 28], f16), T([288], f16), T([288], f16), T([288], f16), T([288], f32), T([288], f32), True, 1e-05, [True, True, True]), {})
cnt: 4, ((T([128, 72, 28, 28], f16), T([128, 72, 28, 28], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f32), T([72], f32), True, 1e-05, [True, True, True]), {})
cnt: 7, ((T([128, 144, 28, 28], f16), T([128, 144, 28, 28], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f32), T([144], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 128, 56, 56], f16), T([128, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})
cnt: 4, ((T([128, 32, 56, 56], f16), T([128, 32, 56, 56], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), True, 1e-05, [True, True, True]), {})
cnt: 7, ((T([128, 64, 56, 56], f16), T([128, 64, 56, 56], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 32, 112, 112], f16), T([128, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), True, 1e-05, [True, True, True]), {})
```

----------------------------------------

TITLE: Computing Slice Backward Gradients using aten.slice_backward - Python
DESCRIPTION: Records multiple usages of aten.slice_backward.default for slicing gradients along specified axes in tensors, usually a step in gradient propagation after slicing operations. The operator takes start, end, axis, and step for the slice, often with 5D f16 tensors and various complex strides. Prerequisites are PyTorch and correct computation graph shape alignment.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
Operator: aten.slice_backward.default
cnt: 372, ((T([2, 12, 16, 64, 64], f16), [2, 12, 16, 64, 64], 1, 0, 9223372036854775807, 1), {})
cnt: 372, ((T([2, 12, 16, 64, 64], f16), [2, 12, 16, 64, 64], 0, 0, 9223372036854775807, 1), {})
cnt: 72, ((T([2, 12, 14, 192, 64], f16), [2, 12, 14, 192, 64], 1, 0, 9223372036854775807, 1), {})
cnt: 72, ((T([2, 12, 14, 192, 64], f16), [2, 12, 14, 192, 64], 0, 0, 9223372036854775807, 1), {})
cnt: 12, ((T([2, 12, 12, 64, 64], f16), [2, 12, 12, 64, 512], 4, -64, 9223372036854775807, 1), {})
cnt: 48, ((T([2, 12, 12, 64, 512], f16), [2, 12, 12, 64, 512], 3, 0, 9223372036854775807, 1), {})
cnt: 48, ((T([2, 12, 12, 64, 512], f16), [2, 12, 12, 64, 512], 2, 0, 9223372036854775807, 1), {})
cnt: 48, ((T([2, 12, 12, 64, 512], f16), [2, 12, 12, 64, 512], 1, 0, 9223372036854775807, 1), {})
cnt: 48, ((T([2, 12, 12, 64, 512], f16), [2, 12, 12, 64, 512], 0, 0, 9223372036854775807, 1), {})
cnt: 12, ((T([2, 12, 12, 64, 64], f16), [2, 12, 12, 64, 512], 4, 0, 64, 1), {})
cnt: 12, ((T([2, 12, 12, 192, 64], f16), [2, 12, 14, 192, 64], 2, 1, -1, 1), {})
cnt: 12, ((T([2, 12, 12, 64, 192], f16), [2, 12, 12, 64, 512], 4, 256, -64, 1), {})
cnt: 12, ((T([2, 12, 12, 64, 192], f16), [2, 12, 12, 64, 512], 4, 64, 256, 1), {})
cnt: 12, ((T([2, 12, 12, 192, 64], f16, stride=(1769472, 147456, 12288, 1, 192)), [2, 12, 14, 192, 64], 2, 1, -1, 1), {})
cnt: 12, ((T([2, 12, 12, 64, 64], f16), [2, 12, 16, 64, 64], 2, 2, -2, 1), {})
cnt: 12, ((T([2, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 64, 1)), [2, 12, 16, 64, 64], 2, 3, -1, 1), {})
cnt: 12, ((T([2, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 64, 1)), [2, 12, 16, 64, 64], 2, 2, -2, 1), {})
cnt: 12, ((T([2, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 64, 1)), [2, 12, 16, 64, 64], 2, 1, -3, 1), {})
cnt: 12, ((T([2, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 1, 192)), [2, 12, 16, 64, 64], 2, 3, -1, 1), {})
cnt: 12, ((T([2, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 1, 192)), [2, 12, 16, 64, 64], 2, 2, -2, 1), {})
cnt: 12, ((T([2, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 1, 192)), [2, 12, 16, 64, 64], 2, 1, -3, 1), {})
```

----------------------------------------

TITLE: Defining Torch Library in C++
DESCRIPTION: Macro for defining a new Torch library. Used to create custom operators and data types that can be used in PyTorch's eager API and TorchScript.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/library.rst#2025-04-22_snippet_0

LANGUAGE: C++
CODE:
```
TORCH_LIBRARY
```

----------------------------------------

TITLE: Analyzing PyTorch Operator Usage
DESCRIPTION: This code snippet demonstrates the usage of various PyTorch operators in a deep learning model. It includes operations like softmax, matrix multiplication, embedding, and layer normalization. The analysis shows the operator name, count of usage, and input tensor shapes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BlenderbotSmallForCausalLM_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([8192, 50265], f16), 1, False), {})
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([8192, 50265], f16), T([8192, 50265], f16), 1, f16), {})
Operator: aten._softmax.default
cnt: 8, ((T([1024, 128, 128], f16), -1, False), {})
Operator: aten._softmax_backward_data.default
cnt: 8, ((T([1024, 128, 128], f16), T([1024, 128, 128], f16), -1, f16), {})
Operator: aten._to_copy.default
cnt: 1, ((T([128, 128], f32),), {'dtype': f16})
cnt: 1, ((T([64, 1, 128, 128], f16, stride=(0, 16384, 128, 1)),), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})
Operator: aten._unsafe_view.default
cnt: 24, ((T([64, 128, 16, 32], f16), [64, 128, 512]), {})
cnt: 1, ((T([8192, 50265], f16), [64, 128, 50265]), {})
cnt: 8, ((T([64, 16, 128, 32], f16), [1024, 128, 32]), {})
cnt: 8, ((T([64, 128, 512], f16), [8192, 512]), {})
Operator: aten.add.Tensor
cnt: 1, ((T([128], i64), 1), {})
cnt: 1, ((T([64, 128, 512], f16), T([128, 512], f16)), {})
cnt: 8, ((T([64, 16, 128, 128], f16), T([64, 1, 128, 128], f16)), {})
cnt: 48, ((T([64, 128, 512], f16), T([64, 128, 512], f16)), {})
cnt: 1, ((T([50265, 512], f16), T([50265, 512], f16)), {})
Operator: aten.addmm.default
cnt: 32, ((T([512], f16), T([8192, 512], f16), T([512, 512], f16, stride=(1, 512))), {})
cnt: 8, ((T([2048], f16), T([8192, 512], f16), T([512, 2048], f16, stride=(1, 512))), {})
cnt: 8, ((T([512], f16), T([8192, 2048], f16), T([2048, 512], f16, stride=(1, 2048))), {})
Operator: aten.bmm.default
cnt: 16, ((T([1024, 128, 32], f16), T([1024, 32, 128], f16, stride=(4096, 1, 32))), {})
cnt: 16, ((T([1024, 128, 128], f16), T([1024, 128, 32], f16)), {})
cnt: 8, ((T([1024, 128, 128], f16, stride=(16384, 1, 128)), T([1024, 128, 32], f16)), {})
cnt: 8, ((T([1024, 32, 128], f16, stride=(4096, 1, 32)), T([1024, 128, 128], f16)), {})
Operator: aten.clone.default
cnt: 2, ((T([64, 128], i64),), {})
Operator: aten.copy_.default
cnt: 2, ((T([64, 128], i64), T([64, 128], i64)), {})
Operator: aten.embedding.default
cnt: 1, ((T([50265, 512], f16), T([64, 128], i64), 0), {})
cnt: 1, ((T([512, 512], f16), T([128], i64)), {})
Operator: aten.embedding_dense_backward.default
cnt: 1, ((T([128, 512], f16), T([128], i64), 512, -1, False), {})
cnt: 1, ((T([64, 128, 512], f16), T([64, 128], i64), 50265, 0, False), {})
Operator: aten.gelu.default
cnt: 8, ((T([64, 128, 2048], f16),), {})
Operator: aten.gelu_backward.default
cnt: 8, ((T([64, 128, 2048], f16), T([64, 128, 2048], f16)), {})
Operator: aten.lt.Tensor
cnt: 1, ((T([128], i64), T([128, 1], i64)), {})
Operator: aten.masked_fill_.Scalar
cnt: 1, ((T([128, 128], f32), T([128, 128], b8), 0), {})
Operator: aten.mm.default
cnt: 1, ((T([8192, 512], f16), T([512, 50265], f16, stride=(1, 512))), {})
cnt: 1, ((T([50265, 8192], f16, stride=(1, 50265)), T([8192, 512], f16)), {})
cnt: 1, ((T([8192, 50265], f16), T([50265, 512], f16)), {})
cnt: 8, ((T([8192, 512], f16), T([512, 2048], f16)), {})
cnt: 8, ((T([512, 8192], f16, stride=(1, 512)), T([8192, 2048], f16)), {})
cnt: 8, ((T([8192, 2048], f16), T([2048, 512], f16)), {})
cnt: 8, ((T([2048, 8192], f16, stride=(1, 2048)), T([8192, 512], f16)), {})
cnt: 32, ((T([8192, 512], f16), T([512, 512], f16)), {})
cnt: 32, ((T([512, 8192], f16, stride=(1, 512)), T([8192, 512], f16)), {})
Operator: aten.mul.Tensor
cnt: 2, ((T([64, 128, 512], f16), 1.0), {})
cnt: 16, ((T([64, 128, 512], f16), 0.1767766952966369), {})
Operator: aten.native_layer_norm.default
cnt: 17, ((T([64, 128, 512], f16), [512], T([512], f16), T([512], f16), 1e-05), {})
Operator: aten.native_layer_norm_backward.default
cnt: 17, ((T([64, 128, 512], f16), T([64, 128, 512], f16), [512], T([64, 128, 1], f32), T([64, 128, 1], f32), T([512], f16), T([512], f16), [True, True, True]), {})
Operator: aten.nll_loss_backward.default
cnt: 1, ((T([], f16), T([8192, 50265], f16), T([8192], i64), None, 1, -100, T([], f16)), {})
Operator: aten.nll_loss_forward.default
cnt: 1, ((T([8192, 50265], f16), T([8192], i64), None, 1, -100), {})
Operator: aten.sum.SymInt
cnt: 40, ((T([8192, 512], f16), [0], True), {})
cnt: 8, ((T([8192, 2048], f16), [0], True), {})
cnt: 1, ((T([64, 128, 512], f16), [0], True), {})
```

----------------------------------------

TITLE: Using libtorch C++ API in Android Native Code
DESCRIPTION: This C++ code snippet demonstrates how to use the libtorch C++ API in Android native code. It includes necessary headers, sets up a JIT call guard, loads a model, and performs a forward pass.
SOURCE: https://github.com/pytorch/pytorch/blob/main/android/README.md#2025-04-22_snippet_6

LANGUAGE: C++
CODE:
```
#include <string>
#include <ATen/NativeFunctions.h>
#include <torch/script.h>
namespace pytorch_testapp_jni {
namespace {
    struct JITCallGuard {
      c10::InferenceMode guard;
      torch::jit::GraphOptimizerEnabledGuard no_optimizer_guard{false};
    };
}

void loadAndForwardModel(const std::string& modelPath) {
  JITCallGuard guard;
  torch::jit::Module module = torch::jit::load(modelPath);
  module.eval();
  torch::Tensor t = torch::randn({1, 3, 224, 224});
  c10::IValue t_out = module.forward({t});
}
}
```

----------------------------------------

TITLE: Local Autograd Example for Dependency Computation
DESCRIPTION: Example of local autograd computation on a single machine to demonstrate how dependencies are calculated during the backward pass.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/distributed_autograd.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch
a = torch.rand((3, 3), requires_grad=True)
b = torch.rand((3, 3), requires_grad=True)
c = torch.rand((3, 3), requires_grad=True)
d = a + b
e = b * c
d.sum.().backward()
```

----------------------------------------

TITLE: Backward Convolution Operations in PyTorch - Python
DESCRIPTION: Utilizes aten.convolution_backward.default to compute gradients of input tensors used in convolution operations. Essential for training neural networks with backpropagation. Requires matching input and weight dimensions and returns gradient tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v3_large_training.txt#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
Operator: aten.convolution_backward.default
cnt: 3, ((T([32, 960, 7, 7], f16), T([32, 160, 7, 7], f16), T([960, 160, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([32, 160, 7, 7], f16), T([32, 960, 7, 7], f16), T([160, 960, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([32, 960, 1, 1], f16), T([32, 240, 1, 1], f16), T([960, 240, 1, 1], f16), [960], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 2, ((T([32, 240, 1, 1], f16), T([32, 960, 1, 1], f16), T([240, 960, 1, 1], f16), [240], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 2, ((T([32, 960, 7, 7], f16), T([32, 960, 7, 7], f16), T([960, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 960, [True, True, False]), {})
cnt: 1, ((T([32, 160, 7, 7], f16), T([32, 672, 7, 7], f16), T([160, 672, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([32, 672, 1, 1], f16), T([32, 168, 1, 1], f16), T([672, 168, 1, 1], f16), [672], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 2, ((T([32, 168, 1, 1], f16), T([32, 672, 1, 1], f16), T([168, 672, 1, 1], f16), [168], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([32, 672, 7, 7], f16), T([32, 672, 14, 14], f16), T([672, 1, 5, 5], f16), [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 672, [True, True, False]), {})
cnt: 2, ((T([32, 672, 14, 14], f16), T([32, 112, 14, 14], f16), T([672, 112, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([32, 112, 14, 14], f16), T([32, 672, 14, 14], f16), T([112, 672, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([32, 672, 14, 14], f16), T([32, 672, 14, 14], f16), T([672, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 672, [True, True, False]), {})
cnt: 1, ((T([32, 112, 14, 14], f16), T([32, 480, 14, 14], f16), T([112, 480, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([32, 480, 1, 1], f16), T([32, 120, 1, 1], f16), T([480, 120, 1, 1], f16), [480], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([32, 120, 1, 1], f16), T([32, 480, 1, 1], f16), T([120, 480, 1, 1], f16), [120], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([32, 480, 14, 14], f16), T([32, 480, 14, 14], f16), T([480, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 480, [True, True, False]), {})
cnt: 1, ((T([32, 480, 14, 14], f16), T([32, 80, 14, 14], f16), T([480, 80, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})

```

----------------------------------------

TITLE: Pixel Shuffle with Dimensions in PyTorch
DESCRIPTION: Implements pixel shuffle operation for image upscaling using dimension objects.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
def pixel_shuffle(img, upscale_factor=2):
    h2, w2, c, b, h, w = dims(6)
    h2.size = w2.size = upscale_factor
    return img[b, (c, h2, w2), h, w].order(b, c, (h, h2), (w, w2))
```

----------------------------------------

TITLE: PyTorch Convolution Operations
DESCRIPTION: Convolution operations with different kernel sizes, strides and padding configurations operating on 4D tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
aten.convolution.default((T([128, 3, 256, 256], f16), T([24, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1))
aten.convolution.default((T([128, 24, 128, 128], f16), T([32, 24, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))
```

----------------------------------------

TITLE: Combining Ensemble States for functorch vmap - PyTorch - Python
DESCRIPTION: Uses functorch's combine_state_for_ensemble to stack parameters and buffers from a list of models, producing a stateless functional model, a tuple of stacked parameters, and a tuple of stacked buffers. This transformation is required before applying vmap. All models must have the same architecture and device alignment. Parameters are set as require_grad_ for potential use in gradient computations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/ensembling.ipynb#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from functorch import combine_state_for_ensemble

fmodel, params, buffers = combine_state_for_ensemble(models)
[p.requires_grad_() for p in params];

```

----------------------------------------

TITLE: Analyzing Convolution Operations in PyTorch
DESCRIPTION: This snippet shows the configuration of various convolution operations used in the model. It includes input and output tensor shapes, kernel sizes, strides, padding, and other parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
cnt: 2, ((T([128, 36, 56, 56], f16), T([128, 36, 56, 56], f16), T([36, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 36, [True, True, False]), {})
cnt: 2, ((T([128, 36, 56, 56], f16), T([128, 24, 56, 56], f16), T([36, 24, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 12, 56, 56], f16), T([128, 12, 56, 56], f16), T([12, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 12, [True, True, False]), {})
```

----------------------------------------

TITLE: Creating SWA Model in PyTorch
DESCRIPTION: Example showing how to create a Stochastic Weight Averaging (SWA) model using AveragedModel class.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_12

LANGUAGE: Python
CODE:
```
averaged_model = AveragedModel(model)
```

----------------------------------------

TITLE: Calling aten.mul.Tensor (Python)
DESCRIPTION: Performs element-wise multiplication between a tensor and a scalar. Examples show scaling a float16 tensor by a scalar constant.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_23

LANGUAGE: Python
CODE:
```
((T([8, 128, 1024], f16), 1.0), {})
```

LANGUAGE: Python
CODE:
```
((T([8, 128, 1024], f16), 0.125), {})
```

----------------------------------------

TITLE: For Loop Example with Tuples in TorchScript
DESCRIPTION: Demonstrates how for loops unroll when iterating over tuples in TorchScript, generating separate bodies for each tuple member.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_30

LANGUAGE: python
CODE:
```
import torch
from typing import Tuple

@torch.jit.script
def fn():
    tup = (3, torch.ones(4))
    for x in tup:
        print(x)

fn()
```

----------------------------------------

TITLE: Using aten.split_with_sizes.default Operator in PyTorch
DESCRIPTION: Examples of the aten.split_with_sizes.default operator being applied to tensors of various shapes. This operator splits a tensor into chunks of specified sizes along a given dimension (dimension 1 in all cases shown).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/coat_lite_mini_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
Operator: aten.split_with_sizes.default
cnt: 2, ((T([128, 64, 56, 56], f16, stride=(602304, 1, 10752, 192)), [16, 24, 24], 1), {})
cnt: 2, ((T([128, 128, 28, 28], f16, stride=(301440, 1, 10752, 384)), [32, 48, 48], 1), {})
cnt: 2, ((T([128, 320, 14, 14], f16, stride=(189120, 1, 13440, 960)), [80, 120, 120], 1), {})
cnt: 2, ((T([128, 512, 7, 7], f16, stride=(76800, 1, 10752, 1536)), [128, 192, 192], 1), {})
```

----------------------------------------

TITLE: Profiling aten.clone.default Calls in PyTorch Text Trace
DESCRIPTION: Describes clone operations performed on half-precision (f16) tensors of various shapes, logging the count and size of clone invocations. Cloning is common when a tensor needs to be duplicated for further in-place operations or to avoid side effects. The trace provides insight into memory usage trends and intermediate tensor lifetimes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_2

LANGUAGE: text
CODE:
```
Operator: aten.clone.default
cnt: 1, ((T([32, 3, 224, 224], f16),), {})
cnt: 2, ((T([32, 32, 112, 112], f16),), {})
cnt: 1, ((T([32, 8, 1, 1], f16),), {})
cnt: 1, ((T([32, 96, 112, 112], f16),), {})
cnt: 1, ((T([32, 96, 56, 56], f16),), {})
cnt: 1, ((T([32, 4, 1, 1], f16),), {})
cnt: 3, ((T([32, 144, 56, 56], f16),), {})
cnt: 2, ((T([32, 6, 1, 1], f16),), {})
cnt: 1, ((T([32, 144, 28, 28], f16),), {})
cnt: 3, ((T([32, 240, 28, 28], f16),), {})
cnt: 2, ((T([32, 10, 1, 1], f16),), {})
cnt: 1, ((T([32, 240, 14, 14], f16),), {})
cnt: 6, ((T([32, 480, 14, 14], f16),), {})
cnt: 3, ((T([32, 20, 1, 1], f16),), {})
cnt: 5, ((T([32, 672, 14, 14], f16),), {})
cnt: 3, ((T([32, 28, 1, 1], f16),), {})
cnt: 1, ((T([32, 672, 7, 7], f16),), {})
cnt: 8, ((T([32, 1152, 7, 7], f16),), {})
cnt: 4, ((T([32, 48, 1, 1], f16),), {})
cnt: 1, ((T([32, 1280, 7, 7], f16),), {})
```

----------------------------------------

TITLE: Creating and Using TestModule with TorchScript
DESCRIPTION: This example demonstrates creating TestModule instances outside TorchScript scope and using them with different input types. It shows how forward() is automatically compiled while __init__() can contain arbitrary Python code.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
import torch

class TestModule(torch.nn.Module):
    def __init__(self, v):
        super().__init__()
        self.x = v

    def forward(self, inc: int):
        return self.x + inc

m = torch.jit.script(TestModule(1))
print(f"First instance: {m(3)}")

m = torch.jit.script(TestModule(torch.ones([5])))
print(f"Second instance: {m(3)}")
```

----------------------------------------

TITLE: Triggering TorchInductor Lowering Error (Python)
DESCRIPTION: This snippet defines a simple PyTorch model and a function `test_backend_error` that includes a call to a dummy operation `torch.ops.aten._foobar`, which is designed to fail during TorchInductor lowering. It then compiles the function using `torch.compile` with the "inductor" backend and executes it to demonstrate how to trigger a backend-specific error for debugging purposes. Requires `torch` and `torch._dynamo`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting_old.rst#_snippet_1

LANGUAGE: python
CODE:
```
import torch

import torch._dynamo as dynamo

model = torch.nn.Sequential(*[torch.nn.Linear(200, 200) for _ in range(5)])

def test_backend_error():

    y = torch.ones(200, 200)
    x = torch.ones(200, 200)
    z = x + y
    a = torch.ops.aten._foobar(z)  # dummy function which errors
    return model(a)


compiled_test_backend_error = torch.compile(test_backend_error, backend="inductor")
compiled_test_backend_error()
```

----------------------------------------

TITLE: Defining Custom Op with ONNX-script (Part 3: Symbolic Wrapper)
DESCRIPTION: Implements the custom symbolic function custom_selu. This function wraps the ONNX-script defined Selu function, calling it via g.onnxscript_op and ensuring the output type is correctly set, ready for registration with the ONNX exporter.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_27

LANGUAGE: Python
CODE:
```
def custom_selu(g: jit_utils.GraphContext, X):
    return g.onnxscript_op(Selu, X).setType(X.type())
```

----------------------------------------

TITLE: Specifying Module Patterns for PackageExporter
DESCRIPTION: Example of specifying module patterns to intern or extern dependencies in torch.package's PackageExporter. This demonstrates how to include torchvision modules while marking numpy as an external dependency.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
my_exporter.intern("torchvision.**")
my_exporter.extern("numpy")
```

----------------------------------------

TITLE: Installing VC Runtime and MKL Dependencies with Conda (BAT)
DESCRIPTION: This batch command snippet shows how to install the Visual C++ 2017 runtime and essential MKL libraries (`mkl_fft`, `intel_openmp`, `numpy`, `mkl`) using the conda package manager from specific channels (`peterjc123`, default channels). This is a common fix for `ImportError: DLL load failed` issues on Windows when using conda packages.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#_snippet_3

LANGUAGE: bat
CODE:
```
conda install -c peterjc123 vc vs2017_runtime
conda install mkl_fft intel_openmp numpy mkl
```

----------------------------------------

TITLE: Configuring QNNPACK Default Quantization Settings
DESCRIPTION: Shows how to configure PyTorch quantization settings for QNNPACK (mobile/ARM) backends using either post-training quantization (PTQ) or quantization-aware training (QAT) configs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
# set the qconfig for PTQ
qconfig = torch.ao.quantization.get_default_qconfig('qnnpack')
# or, set the qconfig for QAT
qconfig = torch.ao.quantization.get_default_qat_qconfig('qnnpack')
# set the qengine to control weight packing
torch.backends.quantized.engine = 'qnnpack'
```

----------------------------------------

TITLE: Accelerating nn.Linear with Semi-Structured Sparsity in PyTorch
DESCRIPTION: Shows how to accelerate linear layers in a model by converting weights to semi-structured sparse format.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
input = torch.rand(64, 64).half().cuda()
mask = torch.Tensor([0, 0, 1, 1]).tile((64, 16)).cuda().bool()
linear = nn.Linear(64, 64).half().cuda()
linear.weight = nn.Parameter(to_sparse_semi_structured(linear.weight.masked_fill(~mask, 0)))
```

----------------------------------------

TITLE: Referencing checkpoint functions in PyTorch documentation
DESCRIPTION: These lines indicate the documentation for specific functions and classes in the checkpoint module. They include checkpoint, checkpoint_sequential, set_checkpoint_debug_enabled, CheckpointPolicy, SelectiveCheckpointContext, and create_selective_checkpoint_contexts.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/checkpoint.rst#2025-04-22_snippet_1

LANGUAGE: reStructuredText
CODE:
```
.. autofunction:: checkpoint
.. autofunction:: checkpoint_sequential
.. autofunction:: set_checkpoint_debug_enabled
.. autoclass:: CheckpointPolicy
.. autoclass:: SelectiveCheckpointContext
.. autofunction:: create_selective_checkpoint_contexts
```

----------------------------------------

TITLE: Using Deterministic `torch.bmm` Implementation with Sparse Tensors (Python)
DESCRIPTION: Shows that after enabling deterministic mode with `torch.use_deterministic_algorithms(True)`, calling `torch.bmm` with sparse CUDA tensors automatically uses its available deterministic implementation instead of the default nondeterministic one. This ensures reproducibility for this specific operation and input type.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/randomness.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> import torch
>>> torch.use_deterministic_algorithms(True)
>>> torch.bmm(torch.randn(2, 2, 2).to_sparse().cuda(), torch.randn(2, 2, 2).cuda())
tensor([[[ 1.1900, -2.3409],
         [ 0.4796,  0.8003]],
        [[ 0.1509,  1.8027],
         [ 0.0333, -1.1444]]], device='cuda:0')
```

----------------------------------------

TITLE: Python3-style Type Annotation in TorchScript
DESCRIPTION: Example of Python3-style type annotation in TorchScript where parameter 'b' is explicitly typed as int while 'a' uses the default TensorType. The return type is inferred automatically based on the returned value.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
import torch

def f(a, b: int):
    return a+b

m = torch.jit.script(f)
print("TorchScript:", m(torch.ones([6]), 100))
```

----------------------------------------

TITLE: Activation and Normalization Operations in PyTorch Model
DESCRIPTION: This code shows the activation functions (GELU) and batch normalization operations used throughout the model architecture. It includes both forward and backward operations for training, with different tensor shapes at various layers of the network.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/visformer_small_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
Operator: aten.copy_.default
cnt: 1, ((T([128, 3, 224, 224], f16), T([128, 3, 224, 224], f16)), {})
Operator: aten.div.Scalar
cnt: 1, ((T([128, 768, 7, 7], f16, stride=(768, 1, 0, 0)), 49), {})
Operator: aten.gelu.default
cnt: 14, ((T([128, 384, 28, 28], f16),), {})
cnt: 4, ((T([128, 1536, 14, 14], f16),), {})
cnt: 4, ((T([128, 3072, 7, 7], f16),), {})
Operator: aten.gelu_backward.default
cnt: 4, ((T([128, 3072, 7, 7], f16), T([128, 3072, 7, 7], f16)), {})
cnt: 4, ((T([128, 1536, 14, 14], f16), T([128, 1536, 14, 14], f16)), {})
cnt: 14, ((T([128, 384, 28, 28], f16), T([128, 384, 28, 28], f16)), {})
Operator: aten.lift_fresh_copy.default
cnt: 1, ((T([128], i64),), {})
Operator: aten.mean.dim
cnt: 1, ((T([128, 768, 7, 7], f16), [-1, -2], True), {})
Operator: aten.mm.default
cnt: 1, ((T([128, 1000], f16), T([1000, 768], f16)), {})
cnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 768], f16)), {})
Operator: aten.mul.Tensor
cnt: 8, ((T([128, 6, 196, 196], f16), 0.125), {})
cnt: 8, ((T([128, 6, 49, 49], f16), 0.08838834764831845), {})
```

----------------------------------------

TITLE: Creating Hybrid COO Sparse Tensor in PyTorch
DESCRIPTION: Creates a 3D Hybrid COO tensor with 2 sparse and 1 dense dimension, showing how rows with any non-zero elements are stored entirely.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
t = torch.tensor([[[0., 0], [1., 2.]], [[0., 0], [3., 4.]]])
t.to_sparse(sparse_dim=2)
```

----------------------------------------

TITLE: Profiling Matrix Multiplication Operations in PyTorch
DESCRIPTION: Log of matrix multiplication operations showing input and output tensor shapes. These are used in fully connected layers, attention mechanisms, and other linear transformations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
Operator: aten.mm.default
cnt: 2, ((T([262144, 32], f16), T([32, 63], f16, stride=(1, 32))), {})
cnt: 2, ((T([65536, 64], f16), T([64, 31], f16, stride=(1, 64))), {})
cnt: 2, ((T([65536, 128], f16), T([128, 31], f16, stride=(1, 128))), {})
cnt: 2, ((T([16384, 128], f16), T([128, 15], f16, stride=(1, 128))), {})
cnt: 1, ((T([64, 1000], f16), T([1000, 1280], f16)), {})
```

----------------------------------------

TITLE: TorchScript Custom Class Error Example
DESCRIPTION: Shows incorrect usage of instance attributes in TorchScript custom classes when not defined in __init__.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
import torch

@torch.jit.script
class foo:
    def __init__(self):
        self.y = 1

# ERROR: self.x is not defined in __init__
def assign_x(self):
    self.x = torch.rand(2, 3)
```

----------------------------------------

TITLE: Customizing Torchvision Model Normalization with GroupNorm - PyTorch (Python)
DESCRIPTION: Demonstrates how to initialize a torchvision model (e.g., ResNet18) with a custom normalization layer using GroupNorm. The 'norm_layer' parameter is set to a lambda function returning GroupNorm, complying with the requirement that the channel count 'c' is divisible by 'g.' This approach requires torchvision and functools, and is useful for swapping BatchNorm2d for GroupNorm in pretrained models.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.batch_norm.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torchvision
from functools import partial
torchvision.models.resnet18(norm_layer=lambda c: GroupNorm(num_groups=g, c))
```

----------------------------------------

TITLE: Using MemPool with use_on_oom Option in Python
DESCRIPTION: This snippet creates a `torch.cuda.MemPool` with the `use_on_oom` option set to `True`. It then allocates a tensor within this pool. A subsequent allocation outside the pool, which might normally cause an Out-of-Memory error, will attempt to use memory from this pool as a fallback if `use_on_oom` is enabled.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_30

LANGUAGE: python
CODE:
```
pool = torch.cuda.MemPool(allocator, use_on_oom=True)
with torch.cuda.use_mem_pool(pool):
    a = torch.randn(40 * 1024 * 1024, dtype=torch.uint8, device="cuda")
del a

# at the memory limit, this will succeed by using pool's memory in order to avoid the oom
b = torch.randn(40 * 1024 * 1024, dtype=torch.uint8, device="cuda")
```

----------------------------------------

TITLE: Examining Backward Convolution Operations in PyTorch Neural Network
DESCRIPTION: This snippet shows the backward convolution operations used during the backpropagation phase of training. Each operation includes input gradients, output gradients, weight tensors, and configuration parameters, revealing the gradient flow through the network architecture.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tinynet_a_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
Operator: aten.convolution_backward.default
cnt: 1, ((T([128, 1280, 6, 6], f16), T([128, 320, 6, 6], f16), T([1280, 320, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 320, 6, 6], f16), T([128, 1152, 6, 6], f16), T([320, 1152, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 5, ((T([128, 1152, 1, 1], f16), T([128, 48, 1, 1], f16), T([1152, 48, 1, 1], f16), [1152], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 5, ((T([128, 48, 1, 1], f16), T([128, 1152, 1, 1], f16), T([48, 1152, 1, 1], f16), [48], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([128, 1152, 6, 6], f16), T([128, 1152, 6, 6], f16), T([1152, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1152, [True, True, False]), {})
cnt: 5, ((T([128, 1152, 6, 6], f16), T([128, 192, 6, 6], f16), T([1152, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 4, ((T([128, 192, 6, 6], f16), T([128, 1152, 6, 6], f16), T([192, 1152, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 4, ((T([128, 1152, 6, 6], f16), T([128, 1152, 6, 6], f16), T([1152, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 1152, [True, True, False]), {})
cnt: 1, ((T([128, 192, 6, 6], f16), T([128, 672, 6, 6], f16), T([192, 672, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 4, ((T([128, 672, 1, 1], f16), T([128, 28, 1, 1], f16), T([672, 28, 1, 1], f16), [672], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 4, ((T([128, 28, 1, 1], f16), T([128, 672, 1, 1], f16), T([28, 672, 1, 1], f16), [28], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([128, 672, 6, 6], f16), T([128, 672, 12, 12], f16), T([672, 1, 5, 5], f16), [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 672, [True, True, False]), {})
cnt: 4, ((T([128, 672, 12, 12], f16), T([128, 112, 12, 12], f16), T([672, 112, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

----------------------------------------

TITLE: Summing Tensor Elements in PyTorch
DESCRIPTION: This snippet computes the sum of all elements in a tensor. It's often used in loss calculations or for reducing tensor dimensions in neural network architectures.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_18

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([8, 12096, 85], f16),), {})
cnt: 1, ((T([8, 3, 12, 16, 85], f16),), {})
cnt: 1, ((T([8, 3, 24, 32, 85], f16),), {})
cnt: 1, ((T([8, 3, 48, 64, 85], f16),), {})
```

----------------------------------------

TITLE: Implementing Hessian-Vector Product Using Forward-Mode AD in PyTorch
DESCRIPTION: Demonstrates an efficient implementation of Hessian-vector products (HVP) by composing reverse-mode AD with forward-mode AD. This approach is memory efficient as it avoids constructing the full Hessian.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_22

LANGUAGE: python
CODE:
```
from functorch import jvp, grad, vjp

def hvp(f, primals, tangents):
  return jvp(grad(f), primals, tangents)[1]
```

----------------------------------------

TITLE: Compilation Time Profiling Example
DESCRIPTION: Demonstrates how to profile the compilation process itself, including a warm-up compilation step and the main model compilation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_profiling_torch_compile.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch
from torchvision.models import resnet18

device = 'cuda'
model = resnet18().to(device)
inputs = [torch.randn((5, 3, 224, 224), device=device) for _ in range(10)]

model_c = torch.compile(model)

def fwd_bwd(inp):
    out = model_c(inp)
    out.sum().backward()

def warmup_compile():
    def fn(x):
        return x.sin().relu()

    x = torch.rand((2, 2), device=device, requires_grad=True)
    fn_c = torch.compile(fn)
    out = fn_c(x)
    out.sum().backward()

with torch.profiler.profile() as prof:
    with torch.profiler.record_function("warmup compile"):
        warmup_compile()

    with torch.profiler.record_function("resnet18 compile"):
        fwd_bwd(inputs[0])

prof.export_chrome_trace("trace_compile.json")
```

----------------------------------------

TITLE: Invoking aten.silu_.default In-place SiLU Activation in PyTorch ATen
DESCRIPTION: Documents observed calls to the in-place SiLU (Sigmoid Linear Unit) activation function (`aten.silu_.default`) in PyTorch ATen. It lists the various input tensor shapes (all using f16 data type) encountered during profiling and their respective call counts.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_21

LANGUAGE: text
CODE:
```
Operator: aten.silu_.default
cnt: 1, ((T([128, 16, 112, 112], f16),), {})
cnt: 1, ((T([128, 32, 112, 112], f16),), {})
cnt: 1, ((T([128, 64, 112, 112], f16),), {})
cnt: 2, ((T([128, 64, 56, 56], f16),), {})
cnt: 1, ((T([128, 128, 56, 56], f16),), {})
cnt: 3, ((T([128, 128, 28, 28], f16),), {})
cnt: 1, ((T([128, 384, 28, 28], f16),), {})
cnt: 12, ((T([128, 384, 14, 14], f16),), {})
cnt: 5, ((T([128, 384, 7, 7], f16),), {})
cnt: 1, ((T([128, 2304, 7, 7], f16),), {})
```

----------------------------------------

TITLE: Analyzing PyTorch Operator Usage Statistics in a Vision Transformer Model
DESCRIPTION: This code represents operator statistics from a PyTorch model, showing each tensor operation with its count and tensor shapes. The model appears to be a Vision Transformer variant with attention mechanisms and convolutional layers operating on a batch of 128 images with 3 channels at 224x224 resolution.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/visformer_small_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([128, 1000], f16), 1, False), {})
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})
Operator: aten._softmax.default
cnt: 4, ((T([128, 6, 196, 196], f16), -1, False), {})
cnt: 4, ((T([128, 6, 49, 49], f16), -1, False), {})
Operator: aten._softmax_backward_data.default
cnt: 4, ((T([128, 6, 49, 49], f16), T([128, 6, 49, 49], f16), -1, f16), {})
cnt: 4, ((T([128, 6, 196, 196], f16), T([128, 6, 196, 196], f16), -1, f16), {})
Operator: aten._unsafe_view.default
cnt: 8, ((T([128, 6, 196, 64], f16), [768, 196, 64]), {})
cnt: 4, ((T([128, 6, 64, 196], f16), [768, 64, 196]), {})
cnt: 4, ((T([768, 196, 196], f16), [128, 6, 196, 196]), {})
cnt: 4, ((T([768, 196, 64], f16), [128, 6, 196, 64]), {})
cnt: 4, ((T([128, 6, 64, 196], f16), [128, 384, 14, 14]), {})
cnt: 8, ((T([128, 6, 49, 128], f16), [768, 49, 128]), {})
cnt: 4, ((T([128, 6, 128, 49], f16), [768, 128, 49]), {})
cnt: 4, ((T([768, 49, 49], f16), [128, 6, 49, 49]), {})
cnt: 4, ((T([768, 49, 128], f16), [128, 6, 49, 128]), {})
cnt: 4, ((T([128, 6, 128, 49], f16), [128, 768, 7, 7]), {})
cnt: 4, ((T([128, 3, 6, 128, 49], f16), [128, 2304, 7, 7]), {})
cnt: 4, ((T([128, 3, 6, 64, 196], f16), [128, 1152, 14, 14]), {})
```

----------------------------------------

TITLE: Configuring TensorOptions in PyTorch C++
DESCRIPTION: This example shows how to create a TensorOptions object with custom dtype, layout, device, and requires_grad settings. It configures a 64-bit float, strided tensor that requires a gradient and lives on CUDA device 1.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_5

LANGUAGE: cpp
CODE:
```
auto options =
  torch::TensorOptions()
    .dtype(torch::kFloat32)
    .layout(torch::kStrided)
    .device(torch::kCUDA, 1)
    .requires_grad(true);
```

----------------------------------------

TITLE: Generated Graph with Symbolic Shape (SymInt) in PyTorch Dynamo
DESCRIPTION: This shows the FX graph generated by Dynamo after a retrace triggered by a shape change (second call to `fn` with shape `(8, 3)`). The varying dimension is now represented symbolically using `torch.SymInt` (aliased as `s0` in the graph code logs, though shown explicitly here) allowing the graph to be generic for that dimension.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
def forward(self, s0: torch.SymInt, l_a_: torch.Tensor, l_b_: torch.Tensor):
    size = l_a_.size()
    getitem = size[0]
    mul = getitem * l_a_
    mul_1 = mul * l_b_
    return (mul_1,)
```

----------------------------------------

TITLE: Complete Activation Sparsification Example in PyTorch
DESCRIPTION: Full example showing how to set up and use activation sparsification in a PyTorch model, including custom functions and configuration.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/activation_sparsifier/README.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
# Fetch model
model = SomeModel()

# define some aggregate, reduce and mask functions
def aggregate_fn(tensor1, tensor2):
    return tensor1 + tensor2

def reduce_fn(tensor):
    return tensor.mean(dim=0)

def mask_fn(data, threshold):
    mask = torch.ones_like(tensor)
    mask[torch.abs(tensor) < threshold] = 0.0
    return mask)

# sparse config
default_sparse_config = {"threshold": 0.5}

# define activation sparsifier
act_sparsifier = ActivationSparsifier(model=model, aggregate_fn=aggregate_fn, reduce_fn=reduce_fn, mask_fn=mask_fn, **threshold)

# register some layer to sparsify their activations
act_sparsifier.register_layer(model.some_layer, threshold=0.8)  # custom sparse config

for epoch in range(EPOCHS):
    for input, target in dataset:
        ...
        out = model(input)
        ...
    act_sparsifier.step()  # mask is computed

act_sparsifier.squash_mask(attach_sparsify_hook=True)  # activations are multiplied with the computed mask before flowing through the layer
```

----------------------------------------

TITLE: Computing Select Backward Gradients using aten.select_backward - Python
DESCRIPTION: Documents usage of aten.select_backward.default to compute the gradient for select operations, often used in backward propagation. The operator takes input tensor, expanded shape, dimension, and index, often using f16 tensors with various strides. Dependencies: PyTorch, shape/index dimension compatibility.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
Operator: aten.select_backward.default
cnt: 24, ((T([2, 12, 64, 64], f16), [2, 12, 16, 64, 64], 2, -1), {})
cnt: 12, ((T([2, 12, 64, 64], f16), [2, 12, 16, 64, 64], 2, -2), {})
cnt: 12, ((T([2, 12, 192, 64], f16, stride=(344064, 28672, 64, 1)), [2, 12, 14, 192, 64], 2, -1), {})
cnt: 24, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [2, 12, 16, 64, 64], 2, -1), {})
cnt: 12, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [2, 12, 16, 64, 64], 2, -2), {})
cnt: 12, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [2, 12, 16, 64, 64], 2, -3), {})
cnt: 24, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [2, 12, 16, 64, 64], 2, 0), {})
cnt: 12, ((T([2, 12, 192, 64], f16, stride=(344064, 28672, 1, 448)), [2, 12, 14, 192, 64], 2, -1), {})
cnt: 24, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [2, 12, 16, 64, 64], 2, -1), {})
cnt: 12, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [2, 12, 16, 64, 64], 2, -2), {})
cnt: 12, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [2, 12, 16, 64, 64], 2, -3), {})
cnt: 24, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [2, 12, 16, 64, 64], 2, 0), {})
cnt: 24, ((T([2, 12, 64, 64], f16), [2, 12, 16, 64, 64], 2, 0), {})
cnt: 12, ((T([2, 12, 64, 64], f16, stride=(49152, 4096, 1, 64)), [2, 12, 16, 64, 64], 2, -1), {})
cnt: 12, ((T([2, 12, 64, 64], f16, stride=(49152, 4096, 1, 64)), [2, 12, 16, 64, 64], 2, 0), {})
cnt: 12, ((T([2, 12, 64, 64], f16), [2, 12, 16, 64, 64], 2, 1), {})
cnt: 12, ((T([2, 12, 192, 64], f16, stride=(344064, 28672, 64, 1)), [2, 12, 14, 192, 64], 2, 0), {})
cnt: 12, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [2, 12, 16, 64, 64], 2, 2), {})
cnt: 12, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [2, 12, 16, 64, 64], 2, 1), {})
cnt: 12, ((T([2, 12, 192, 64], f16, stride=(344064, 28672, 1, 448)), [2, 12, 14, 192, 64], 2, 0), {})
cnt: 12, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [2, 12, 16, 64, 64], 2, 2), {})
cnt: 12, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [2, 12, 16, 64, 64], 2, 1), {})
```

----------------------------------------

TITLE: Comparing PyTorch DDP Benchmark Results using diff.py
DESCRIPTION: Demonstrates how to use the `diff.py` script to compare two JSON reports generated by the DDP benchmark script (using the `--json` flag). The command takes two file paths (baseline and test) as input. The output shows a side-by-side comparison of configuration parameters (like `bucket_size`, versions) and performance metrics (throughput, percentage difference) for each benchmark scenario, facilitating A/B testing analysis.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/distributed/ddp/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
$ python3 diff.py PATH_TO_BASELINE_FILE PATH_TO_TEST_FILE
```

LANGUAGE: text
CODE:
```
                                 baseline                      test
                     --------------------      --------------------
bucket_size:                           25  vs                     1
cuda_version:                        10.0  vs                  10.0
distributed_backend:                 nccl  vs                  nccl
pytorch_version:          1.4.0a0+05140f0  vs       1.4.0a0+05140f0

Benchmark: resnet50 with batch size 32

                  sec/iter    ex/sec      diff        sec/iter    ex/sec      diff
   1 GPUs:  p75:    0.101s     317/s     -0.3%  p95:    0.101s     317/s     -0.4%
   2 GPUs:  p75:    0.104s     306/s     -1.0%  p95:    0.104s     306/s     -1.0%
   4 GPUs:  p75:    0.105s     305/s     -1.6%  p95:    0.105s     304/s     -1.8%
   8 GPUs:  p75:    0.107s     299/s     -2.6%  p95:    0.107s     298/s     -2.7%
  16 GPUs:  p75:    0.108s     294/s     -3.8%  p95:    0.122s     262/s    -16.4%

Benchmark: resnet101 with batch size 32

                  sec/iter    ex/sec      diff        sec/iter    ex/sec      diff
   1 GPUs:  p75:    0.172s     185/s     -1.2%  p95:    0.172s     185/s     -1.3%
   2 GPUs:  p75:    0.179s     178/s     -2.1%  p95:    0.179s     178/s     -2.0%
   4 GPUs:  p75:    0.180s     177/s     -2.6%  p95:    0.180s     177/s     -2.6%
   8 GPUs:  p75:    0.184s     173/s     -3.5%  p95:    0.184s     173/s     -3.5%
  16 GPUs:  p75:    0.187s     170/s     -0.1%  p95:    0.204s     157/s     -7.9%

Benchmark: resnext50_32x4d with batch size 32

                  sec/iter    ex/sec      diff        sec/iter    ex/sec      diff
   1 GPUs:  p75:    0.149s     214/s     -1.0%  p95:    0.149s     214/s     -0.9%
   2 GPUs:  p75:    0.156s     205/s     -1.5%  p95:    0.156s     205/s     -1.6%
   4 GPUs:  p75:    0.156s     204/s     -1.6%  p95:    0.157s     204/s     -1.8%
   8 GPUs:  p75:    0.159s     200/s     -1.5%  p95:    0.159s     200/s     -1.5%
  16 GPUs:  p75:    0.161s     198/s     -1.9%  p95:    0.162s     197/s     -2.3%

Benchmark: resnext101_32x8d with batch size 32

                  sec/iter    ex/sec      diff        sec/iter    ex/sec      diff
   1 GPUs:  p75:    0.427s      74/s     -0.8%  p95:    0.428s      74/s     -0.7%
   2 GPUs:  p75:    0.444s      72/s     -1.3%  p95:    0.445s      71/s     -0.7%
   4 GPUs:  p75:    0.444s      72/s     -1.1%  p95:    0.445s      71/s     -0.8%
   8 GPUs:  p75:    0.452s      70/s     -1.3%  p95:    0.452s      70/s     -1.3%
  16 GPUs:  p75:    0.455s      70/s     -0.7%  p95:    0.456s      70/s     -0.6%
```

----------------------------------------

TITLE: Generated Graph with Static Shape in PyTorch Dynamo
DESCRIPTION: This shows the FX graph generated by Dynamo for the first call to the compiled function `fn` where the input tensor shape `(4, 3)` is treated as static. The shape dimension `4` is hardcoded into the graph.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
def forward(self, l_a_: torch.Tensor, l_b_: torch.Tensor):
    mul = 4 * l_a_
    mul_1 = mul * l_b_
    return (mul_1,)
```

----------------------------------------

TITLE: Cloning PyTorch Source Bash
DESCRIPTION: Commands to clone the main PyTorch repository from GitHub and update its submodules, which is necessary for a complete build from source. Ensures all required code dependencies are present locally.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_2

LANGUAGE: Bash
CODE:
```
git clone https://github.com/pytorch/pytorch
cd pytorch
# if you are updating an existing checkout
git submodule sync
git submodule update --init --recursive
```

----------------------------------------

TITLE: Backward Convolution in PyTorch
DESCRIPTION: Calculates the gradients for inputs based on the convolution operation, supporting backpropagation in neural networks. Requires tensors representing inputs, weights, and biases with relevant gradient configurations. Outputs include gradients for input and weight tensors, crucial for parameter updates during training phases.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
Operator: aten.convolution_backward.default
cnt: 1, ((T([16, 3, 128, 128], f16), T([16, 64, 128, 128], f16), T([3, 64, 7, 7], f16), [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([16, 64, 128, 128], f16), T([16, 128, 64, 64], f16), T([128, 64, 4, 4], f16), [0], [2, 2], [1, 1], [1, 1], True, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([16, 128, 64, 64], f16), T([16, 256, 32, 32], f16), T([256, 128, 4, 4], f16), [0], [2, 2], [1, 1], [1, 1], True, [0, 0], 1, [True, True, False]), {})
cnt: 12, ((T([16, 256, 32, 32], f16), T([16, 256, 32, 32], f16), T([256, 256, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([16, 256, 32, 32], f16), T([16, 128, 64, 64], f16), T([256, 128, 4, 4], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([16, 128, 64, 64], f16), T([16, 64, 128, 128], f16), T([128, 64, 4, 4], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([16, 64, 128, 128], f16), T([16, 8, 128, 128], f16), T([64, 8, 7, 7], f16), [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 1, [False, True, False]), {})
```

----------------------------------------

TITLE: PyTorch Operator Usage Statistics
DESCRIPTION: This snippet shows the usage statistics of various PyTorch operators in a deep learning model. It includes operator names, usage counts, and input tensor shapes. This information is useful for understanding the model architecture and potential optimization opportunities.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gmlp_s16_224_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([64, 1000], f16), 1, False), {})
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})
Operator: aten._unsafe_view.default
cnt: 30, ((T([64, 768, 196], f16), [64, 768, 196]), {})
Operator: aten.add.Tensor
cnt: 30, ((T([64, 768, 196], f16), T([196], f16)), {})
cnt: 30, ((T([64, 196, 256], f16, stride=(50176, 1, 196)), T([64, 196, 256], f16)), {})
cnt: 30, ((T([64, 196, 256], f16), T([64, 196, 256], f16)), {})
Operator: aten.addmm.default
cnt: 30, ((T([1536], f16), T([12544, 256], f16), T([256, 1536], f16, stride=(1, 256))), {})
cnt: 30, ((T([256], f16), T([12544, 768], f16), T([768, 256], f16, stride=(1, 768))), {})
cnt: 1, ((T([1000], f16), T([64, 256], f16), T([256, 1000], f16, stride=(1, 256))), {})
Operator: aten.bmm.default
cnt: 30, ((T([64, 768, 196], f16, stride=(150528, 1, 768)), T([64, 196, 196], f16, stride=(0, 1, 196))), {})
cnt: 30, ((T([64, 196, 768], f16), T([64, 768, 196], f16, stride=(150528, 1, 768))), {})
cnt: 30, ((T([64, 768, 196], f16, stride=(150528, 1, 768)), T([64, 196, 196], f16, stride=(0, 196, 1))), {})
Operator: aten.cat.default
cnt: 30, (([T([64, 196, 768], f16), T([64, 196, 768], f16, stride=(150528, 1, 196))], 2), {})
Operator: aten.clone.default
cnt: 1, ((T([64, 3, 224, 224], f16),), {})
Operator: aten.convolution.default
cnt: 1, ((T([64, 3, 224, 224], f16), T([256, 3, 16, 16], f16), T([256], f16), [16, 16], [0, 0], [1, 1], False, [0, 0], 1), {})
Operator: aten.convolution_backward.default
cnt: 1, ((T([64, 256, 14, 14], f16, stride=(50176, 1, 3584, 256)), T([64, 3, 224, 224], f16), T([256, 3, 16, 16], f16), [256], [16, 16], [0, 0], [1, 1], False, [0, 0], 1, [False, True, True]), {})
Operator: aten.copy_.default
cnt: 1, ((T([64, 3, 224, 224], f16), T([64, 3, 224, 224], f16)), {})
cnt: 30, ((T([196, 196], f16), T([196, 196], f16, stride=(1, 196))), {})
Operator: aten.div.Scalar
cnt: 1, ((T([64, 196, 256], f16, stride=(256, 0, 1)), 196), {})
Operator: aten.gelu.default
cnt: 30, ((T([64, 196, 1536], f16),), {})
Operator: aten.gelu_backward.default
cnt: 30, ((T([64, 196, 1536], f16), T([64, 196, 1536], f16)), {})
Operator: aten.lift_fresh_copy.default
cnt: 1, ((T([64], i64),), {})
Operator: aten.mean.dim
cnt: 1, ((T([64, 196, 256], f16), [1]), {})
Operator: aten.mm.default
cnt: 1, ((T([64, 1000], f16), T([1000, 256], f16)), {})
cnt: 1, ((T([1000, 64], f16, stride=(1, 1000)), T([64, 256], f16)), {})
cnt: 30, ((T([12544, 256], f16), T([256, 768], f16)), {})
cnt: 30, ((T([256, 12544], f16, stride=(1, 256)), T([12544, 768], f16)), {})
cnt: 30, ((T([12544, 1536], f16), T([1536, 256], f16)), {})
cnt: 30, ((T([1536, 12544], f16, stride=(1, 1536)), T([12544, 256], f16)), {})
Operator: aten.mul.Tensor
cnt: 30, ((T([64, 196, 768], f16, stride=(301056, 1536, 1)), T([64, 196, 768], f16, stride=(150528, 1, 196))), {})
cnt: 30, ((T([64, 196, 768], f16), T([64, 196, 768], f16, stride=(301056, 1536, 1))), {})
cnt: 30, ((T([64, 196, 768], f16), T([64, 196, 768], f16, stride=(150528, 1, 196))), {})
Operator: aten.native_layer_norm.default
cnt: 31, ((T([64, 196, 256], f16, stride=(50176, 1, 196)), [256], T([256], f16), T([256], f16), 1e-06), {})
cnt: 30, ((T([64, 196, 768], f16, stride=(301056, 1536, 1)), [768], T([768], f16), T([768], f16), 1e-05), {})
Operator: aten.native_layer_norm_backward.default
cnt: 31, ((T([64, 196, 256], f16), T([64, 196, 256], f16, stride=(50176, 1, 196)), [256], T([64, 196, 1], f32), T([64, 196, 1], f32), T([256], f16), T([256], f16), [True, True, True]), {})
cnt: 30, ((T([64, 196, 768], f16, stride=(150528, 1, 196)), T([64, 196, 768], f16, stride=(301056, 1536, 1)), [768], T([64, 196, 1], f32), T([64, 196, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})
Operator: aten.new_empty_strided.default
cnt: 30, ((T([196, 196], f16, stride=(1, 196)), [196, 196], [196, 1]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})
Operator: aten.nll_loss_backward.default
cnt: 1, ((T([], f16), T([64, 1000], f16), T([64], i64), None, 1, -100, T([], f16)), {})
Operator: aten.nll_loss_forward.default
cnt: 1, ((T([64, 1000], f16), T([64], i64), None, 1, -100), {})
Operator: aten.split.Tensor
cnt: 30, ((T([64, 196, 1536], f16), 768, -1), {})
Operator: aten.sum.SymInt
cnt: 1, ((T([64, 1000], f16), [0], True), {})
cnt: 30, ((T([12544, 256], f16), [0], True), {})
cnt: 30, ((T([64, 768, 196], f16, stride=(150528, 1, 768)), [0, 1], True), {})
cnt: 30, ((T([64, 196, 196], f16), [0], True), {})
cnt: 30, ((T([12544, 1536], f16), [0], True), {})
```

----------------------------------------

TITLE: Demonstrating Dynamo's Dynamic Shape Handling
DESCRIPTION: Illustrates Dynamo's ability to handle dynamic shapes by tracing integer inputs symbolically, allowing for flexible input sizes without recompilation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch

@torch.compile
def fn(x, n):
    y = x ** 2
    if n >= 0:
        return (n + 1) * y
    else:
        return y / n

x = torch.randn(200)
fn(x, 2)
fn(x, 3)
fn(x, -2)
```

LANGUAGE: python
CODE:
```
def forward(self, l_x_: torch.Tensor, l_n_: torch.SymInt):
    # File: a.py:5, code: y = x ** 2
    y = l_x_ ** 2

    # File: a.py:7, code: return (n + 1) * y
    add = l_n_ + 1
    mul = add * y
    return (mul,)
```

LANGUAGE: python
CODE:
```
def forward(self, l_x_: torch.Tensor, l_n_: torch.SymInt):
    # File: a.py:5, code: y = x ** 2
    y = l_x_ ** 2

    # File: a.py:9, code: return y / n
    truediv = y / l_n_
    return (truediv,)
```

----------------------------------------

TITLE: Conditional Tensor Storage with DataParallel in PyTorch
DESCRIPTION: Implementation of size-based tensor storage optimization using saved tensor hooks within a PyTorch model using DataParallel.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/autograd.rst#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
# Only save on disk tensors that have size >= 1000
SAVE_ON_DISK_THRESHOLD = 1000

def pack_hook(x):
    if x.numel() < SAVE_ON_DISK_THRESHOLD:
        return x
    temp_file = SelfDeletingTempFile()
    torch.save(tensor, temp_file.name)
    return temp_file

def unpack_hook(tensor_or_sctf):
    if isinstance(tensor_or_sctf, torch.Tensor):
        return tensor_or_sctf
    return torch.load(tensor_or_sctf.name)

class Model(nn.Module):
    def forward(self, x):
        with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):
          # ... compute output
          output = x
        return output

model = Model()
net = nn.DataParallel(model)
```

----------------------------------------

TITLE: Implementing Logging Modes Example
DESCRIPTION: Demonstrates implementing custom logging modes using TorchFunctionMode and TorchDispatchMode to intercept and log PyTorch operations at different levels of the execution stack.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
import torch
from torch.overrides import TorchFunctionMode, resolve_name
from torch.utils._python_dispatch import TorchDispatchMode

class FunctionLog(TorchFunctionMode):
    def __torch_function__(self, func, types, args, kwargs=None):
        print(f"Function Log: {resolve_name(func)}(*{args}, **{kwargs})")
        return func(*args, **(kwargs or {}))

class DispatchLog(TorchDispatchMode):
    def __torch_dispatch__(self, func, types, args, kwargs=None):
        print(f"Dispatch Log: {func}(*{args}, **{kwargs})")
        return func(*args, **(kwargs or {}))

def f():
    a = torch.rand(10, requires_grad=True)
    b = a * 2
    b.sum().backward()

print("TorchFunctionMode logging:")
with FunctionLog():
    f()

print("TorchDispatchMode logging:")
with DispatchLog():
    f()
```

----------------------------------------

TITLE: Adjusting the TorchDynamo Recompilation Limit
DESCRIPTION: Configures the maximum number of times TorchDynamo will recompile a function or graph fragment due to guard failures. Increasing this limit can accommodate bounded dynamism, but setting it too high might incur excessive compilation overhead.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting_old.rst#_snippet_8

LANGUAGE: python
CODE:
```
torch._dynamo.config.recompile_limit = <your desired cache limit>
```

----------------------------------------

TITLE: Negative Log Likelihood Loss Backward - PyTorch Aten
DESCRIPTION: Computes the gradient for the Negative Log Likelihood Loss. This internal operator is part of the backward pass for `nll_loss_forward`. It requires the gradient of the output (which is a scalar or empty for reduction='none'), the input log-probabilities, the target indices, optional weight, reduction method, ignore index, and total weight.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BartForConditionalGeneration_training.txt#_snippet_21

LANGUAGE: Python
CODE:
```
import torch

grad_output = torch.tensor([], dtype=torch.float16) # Empty for mean reduction
input_log_probs = torch.randn(2048, 50265, dtype=torch.float16)
target = torch.randint(0, 50265, (2048,), dtype=torch.int64)
total_weight = torch.tensor([], dtype=torch.float16) # Depends on reduction
input_grad = torch.nll_loss_backward(grad_output, input_log_probs, target, weight=None, reduction=1, ignore_index=-100, total_weight=total_weight)
```

----------------------------------------

TITLE: Comparing DataFrame and Regular DataPipe Iteration
DESCRIPTION: Demonstration of iteration over both DataFrame Pipe and Regular DataPipe to show their similar behavior.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/dataframes_pipes.ipynb#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
print('DataFrames Pipe')
dp = get_dataframes_pipe()
for i in dp:
    print(i)

print('Regular DataPipe')
dp = get_regular_pipe()
for i in dp:
    print(i)
```

----------------------------------------

TITLE: Creating Gradient Computation Function with functorch.grad
DESCRIPTION: Uses `functorch.grad` to create a new function `ft_compute_grad` from `compute_loss_stateless_model`. `grad` transforms the function so that when `ft_compute_grad` is called with the same arguments as `compute_loss_stateless_model`, it returns the gradient of the original function's output (the loss) with respect to its first argument (the `params`). This function computes the gradient for a single sample.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
ft_compute_grad = grad(compute_loss_stateless_model)
```

----------------------------------------

TITLE: Matrix Multiplication with Named Tensors
DESCRIPTION: Shows how matrix multiplication operations handle tensor names during computation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/name_inference.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> x = torch.randn(3, 3, names=('N', 'D'))
>>> y = torch.randn(3, 3, names=('in', 'out'))
>>> x.mm(y).names
('N', 'out')

>>> x = torch.randn(3, 3, 3, 3, names=('A', 'B', 'C', 'D'))
>>> y = torch.randn(3, 3, 3, names=('B', 'E', 'F'))
>>> torch.matmul(x, y).names
('A', 'B', 'C', 'F')
```

----------------------------------------

TITLE: Working with Zero-Dimensional Tensors in ATen
DESCRIPTION: Example showing how to work with zero-dimensional tensors in ATen. These tensors hold a single value and can be created by indexing operations on larger tensors or by operators that reduce dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_basics.rst#2025-04-22_snippet_5

LANGUAGE: cpp
CODE:
```
torch::Tensor two = torch::rand({10, 20});
two[1][2] = 4;
// ^^^^^^ <- zero-dimensional Tensor
```

----------------------------------------

TITLE: Defining Custom C++ Operator Symbolic (Part 4: Exporting Model)
DESCRIPTION: Exports the FooModel instance to ONNX. The exporter uses the registered symbolic_foo_forward to convert the torch.ops.custom_ops.foo_forward call into the custom_domain::Foo ONNX node. A custom opset version for the custom_domain is specified.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_33

LANGUAGE: Python
CODE:
```
model = FooModel(attr1, attr2)
torch.onnx.export(
    model,
    (example_input1, example_input1),
    "model.onnx",
    # only needed if you want to specify an opset version > 1.
    custom_opsets={"custom_domain": 2}
)
```

----------------------------------------

TITLE: Calling a Registered Quantized Operator from Python
DESCRIPTION: Provides a Python function `quantized_xand` that wraps the call to the registered C++ quantized operator. It imports `ops` from `torch._ops` and calls the kernel using the schema name (`ops.quantized.xand`), passing the input tensors `qa` and `qb`. The recommended location for such functions is `torch/ao/nn/quantized/functional.py`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/README.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from torch._ops import ops

def quantized_xand(qa, qb):
#Notice the schema changed from `quantized::xand` to `quantized.xand`
  return ops.quantized.xand(qa, qb)
```

----------------------------------------

TITLE: Custom EMA Implementation using avg_fn
DESCRIPTION: Example demonstrating how to implement a custom exponential moving average using the avg_fn parameter.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_14

LANGUAGE: Python
CODE:
```
ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged:\
        0.9 * averaged_model_parameter + 0.1 * model_parameter
ema_model = torch.optim.swa_utils.AveragedModel(model, avg_fn=ema_avg)
```

----------------------------------------

TITLE: Testing PyTorch Model Function
DESCRIPTION: Creates test inputs and performs basic forward and backward pass to verify the model function works
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/aot_autograd_optimizations.ipynb#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
a, b, c, d = [torch.randn(2, 4, requires_grad=True) for _ in range(4)]
ref = fn(a, b, c, d)
loss = ref.sum()
loss.backward()
```

----------------------------------------

TITLE: Matrix Multiplication Operations in PyTorch
DESCRIPTION: Records of matrix multiplication operations between tensors using the aten.mm.default operator. Two operations are shown with different tensor shapes, demonstrating forward and potentially backward passes in a neural network.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/spnasnet_100_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
Operator: aten.mm.default
cnt: 1, ((T([128, 1000], f16), T([1000, 1280], f16)), {})
cnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 1280], f16)), {})
```

----------------------------------------

TITLE: Creating Tensors on Meta Device with Context Manager in Python
DESCRIPTION: Illustrates using the `torch.device('meta')` context manager. Any tensor created inside this block without an explicit device specification will be instantiated on the 'meta' device, providing a convenient way to create metadata-only tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/meta.rst#_snippet_1

LANGUAGE: Python
CODE:
```
>>> with torch.device('meta'):
...     print(torch.randn(30, 30))
...
tensor(..., device='meta', size=(30, 30))
```

----------------------------------------

TITLE: Creating and Binding First-class Dimensions
DESCRIPTION: Examples showing how to create dimension objects and bind them to tensors, demonstrating dimension properties and indexing behavior.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch
from torchdim import dims

batch, channel, width, height = dims(4)

input = torch.rand(2, 3, 224, 224)
print(input.ndim)

input_fc = input[batch, channel, width, height]
print(input_fc.dims)
print(input_fc.ndim)
```

----------------------------------------

TITLE: Zipping DataPipes in PyTorch
DESCRIPTION: Shows how to use the zip() method to combine multiple DataPipes into a single DataPipe of tuples.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
_dp = ExampleIterPipe(5).shuffle()
dp = ExampleIterPipe(5).zip(_dp)
for i in dp:
    print(i)
```

----------------------------------------

TITLE: Running Forward Graph Profile
DESCRIPTION: Command to profile the forward graph execution with the -p flag.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_inductor_profiling.rst#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
python fwd.py -p
```

----------------------------------------

TITLE: Executing Log Softmax in PyTorch
DESCRIPTION: Performs the log of the softmax function on a given tensor with specified dimensions. This function requires the PyTorch library and operates on tensor inputs of FP16 precision. The main parameters include the tensor to operate on and the dimension over which the softmax is computed. The output is a log-softmaxed tensor, suitable for applications such as neural network output transformation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/Speech2Text2ForCausalLM_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
aten._log_softmax.default(T([8192, 10000], f16), 1, False)
```

----------------------------------------

TITLE: Computing Sums Along an Axis with aten.sum in Python
DESCRIPTION: The aten.sum.SymInt operator calculates the sum of elements along a specified axis, while preserving dimensions and supporting integer output size. This operation allows for aggregation across axes, maintaining broadcastable shapes due to the 'keepdims=True' setting, typically summing elements along the first axis (axis 0).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/nvidia_deeprecommender_training.txt#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([256, 197951], f16), [0], True), {})
cnt: 4, ((T([256, 512], f16), [0], True), {})
cnt: 1, ((T([256, 1024], f16), [0], True), {})
```

----------------------------------------

TITLE: Forking DataPipes in PyTorch
DESCRIPTION: Demonstrates the use of the fork() method to create multiple copies of a DataPipe that can be consumed independently.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(2)
dp1, dp2, dp3 = dp.fork(3)
for i in dp1 + dp2 + dp3:
    print(i)
```

----------------------------------------

TITLE: Using Multiple Streams within CUDA Graph Capture
DESCRIPTION: Shows the correct and incorrect patterns for using multiple CUDA streams within a `torch.cuda.graph` capture context. It emphasizes that streams used for work within the graph must branch out from the initial capturing stream and rejoin it before capture ends by using `wait_stream`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_45

LANGUAGE: python
CODE:
```
with torch.cuda.graph(g):
        # at context manager entrance, torch.cuda.current_stream()
        # is the initial capturing stream

        # INCORRECT (does not branch out from or rejoin initial stream)
        with torch.cuda.stream(s):
            cuda_work()

        # CORRECT:
        # branches out from initial stream
        s.wait_stream(torch.cuda.current_stream())
        with torch.cuda.stream(s):
            cuda_work()
        # rejoins initial stream before capture ends
        torch.cuda.current_stream().wait_stream(s)
```

----------------------------------------

TITLE: Loading Tensor onto Meta Device from File in Python
DESCRIPTION: Demonstrates how to save a tensor and then load it back onto the 'meta' device using the `map_location='meta'` option in `torch.load`. This allows inspecting the tensor's metadata (size, device) without loading its data into memory.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/meta.rst#_snippet_0

LANGUAGE: Python
CODE:
```
>>> torch.save(torch.randn(2), 'foo.pt')
>>> torch.load('foo.pt', map_location='meta')
tensor(..., device='meta', size=(2,))
```

----------------------------------------

TITLE: Implementing and Training a Neural Network with PyTorch C++
DESCRIPTION: This code demonstrates how to define a neural network, create a data loader for the MNIST dataset, and train the network using SGD optimization. It showcases key components of the PyTorch C++ frontend including tensor operations, modules, optimizers, and data loading.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/frontend.rst#2025-04-22_snippet_0

LANGUAGE: cpp
CODE:
```
#include <torch/torch.h>

// Define a new Module.
struct Net : torch::nn::Module {
  Net() {
    // Construct and register two Linear submodules.
    fc1 = register_module("fc1", torch::nn::Linear(784, 64));
    fc2 = register_module("fc2", torch::nn::Linear(64, 32));
    fc3 = register_module("fc3", torch::nn::Linear(32, 10));
  }

  // Implement the Net's algorithm.
  torch::Tensor forward(torch::Tensor x) {
    // Use one of many tensor manipulation functions.
    x = torch::relu(fc1->forward(x.reshape({x.size(0), 784})));
    x = torch::dropout(x, /*p=*/0.5, /*train=*/is_training());
    x = torch::relu(fc2->forward(x));
    x = torch::log_softmax(fc3->forward(x), /*dim=*/1);
    return x;
  }

  // Use one of many "standard library" modules.
  torch::nn::Linear fc1{nullptr}, fc2{nullptr}, fc3{nullptr};
};

int main() {
  // Create a new Net.
  auto net = std::make_shared<Net>();

  // Create a multi-threaded data loader for the MNIST dataset.
  auto data_loader = torch::data::make_data_loader(
      torch::data::datasets::MNIST("./data").map(
          torch::data::transforms::Stack<>()),
      /*batch_size=*/64);

  // Instantiate an SGD optimization algorithm to update our Net's parameters.
  torch::optim::SGD optimizer(net->parameters(), /*lr=*/0.01);

  for (size_t epoch = 1; epoch <= 10; ++epoch) {
    size_t batch_index = 0;
    // Iterate the data loader to yield batches from the dataset.
    for (auto& batch : *data_loader) {
      // Reset gradients.
      optimizer.zero_grad();
      // Execute the model on the input data.
      torch::Tensor prediction = net->forward(batch.data);
      // Compute a loss value to judge the prediction of our model.
      torch::Tensor loss = torch::nll_loss(prediction, batch.target);
      // Compute gradients of the loss w.r.t. the parameters of our model.
      loss.backward();
      // Update the parameters based on the calculated gradients.
      optimizer.step();
      // Output the loss and checkpoint every 100 batches.
      if (++batch_index % 100 == 0) {
        std::cout << "Epoch: " << epoch << " | Batch: " << batch_index
                  << " | Loss: " << loss.item<float>() << std::endl;
        // Serialize your model periodically as a checkpoint.
        torch::save(net, "net.pt");
      }
    }
  }
}
```

----------------------------------------

TITLE: Analyzing PyTorch Operator Usage in ResNet-like Model
DESCRIPTION: This code snippet provides a comprehensive analysis of PyTorch operator usage in what appears to be a ResNet or similar convolutional neural network architecture. It includes operator names, input tensor shapes, and frequency counts for various operations like convolutions, additions, and backward passes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_senet154_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([32, 1000], f16), 1, False), {})
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([32, 1000], f16), T([32, 1000], f16), 1, f16), {})
Operator: aten.add.Tensor
cnt: 5, ((T([32, 2048, 7, 7], f16), T([32, 2048, 7, 7], f16)), {})
cnt: 72, ((T([32, 1024, 14, 14], f16), T([32, 1024, 14, 14], f16)), {})
cnt: 16, ((T([32, 512, 28, 28], f16), T([32, 512, 28, 28], f16)), {})
cnt: 6, ((T([32, 256, 56, 56], f16), T([32, 256, 56, 56], f16)), {})
cnt: 1, ((T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16)), {})
Operator: aten.add_.Tensor
cnt: 157, ((T([], i64), 1), {})
cnt: 3, ((T([32, 256, 56, 56], f16), T([32, 256, 56, 56], f16)), {})
cnt: 8, ((T([32, 512, 28, 28], f16), T([32, 512, 28, 28], f16)), {})
cnt: 36, ((T([32, 1024, 14, 14], f16), T([32, 1024, 14, 14], f16)), {})
cnt: 3, ((T([32, 2048, 7, 7], f16), T([32, 2048, 7, 7], f16)), {})
Operator: aten.addmm.default
cnt: 1, ((T([1000], f16), T([32, 2048], f16), T([2048, 1000], f16, stride=(1, 2048))), {})
Operator: aten.clone.default
cnt: 1, ((T([32, 3, 224, 224], f16),), {})
Operator: aten.convolution.default
cnt: 1, ((T([32, 3, 224, 224], f16), T([64, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
# ... [additional convolution operations omitted for brevity]
Operator: aten.convolution_backward.default
cnt: 3, ((T([32, 2048, 1, 1], f16), T([32, 128, 1, 1], f16), T([2048, 128, 1, 1], f16), [2048], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
# ... [additional backward convolution operations omitted for brevity]
Operator: aten.copy_.default
cnt: 1, ((T([32, 3, 224, 224], f16), T([32, 3, 224, 224], f16)), {})
Operator: aten.div.Scalar
```

----------------------------------------

TITLE: Extended ScalarTensor with Torch Function Support
DESCRIPTION: Enhanced implementation of ScalarTensor that includes __torch_function__ support for integrating with torch operations. Includes a global dispatch table for handling torch functions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
HANDLED_FUNCTIONS = {}
class ScalarTensor(object):
    def __init__(self, N, value):
        self._N = N
        self._value = value

    def __repr__(self):
        return "ScalarTensor(N={}, value={})".format(self._N, self._value)

    def tensor(self):
        return self._value * torch.eye(self._N)

    @classmethod
    def __torch_function__(cls, func, types, args=(), kwargs=None):
        if kwargs is None:
            kwargs = {}
        if func not in HANDLED_FUNCTIONS or not all(
            issubclass(t, (torch.Tensor, ScalarTensor))
            for t in types
        ):
            return NotImplemented
        return HANDLED_FUNCTIONS[func](*args, **kwargs)
```

----------------------------------------

TITLE: Running HuggingFace Performance Benchmarks (Python)
DESCRIPTION: Commands to run HuggingFace performance benchmarks for both training and inference using TorchInductor backend. The commands specify device, precision, and output format.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/README.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
./benchmarks/dynamo/huggingface.py --performance --training --amp --backend=inductor --output=huggingface_training.csv
./benchmarks/dynamo/huggingface.py --performance --inference --bfloat16 --backend=inductor --output=huggingface_inference.csv
```

----------------------------------------

TITLE: Running PyTorch Inference Benchmark with Command-line Arguments
DESCRIPTION: Example command for running the inference benchmark with specific parameters. The command ignores warnings, sets 1000 iterations and a batch size of a 32.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/inference/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python -W ignore server.py --num_iters 1000 --batch_size 32
```

----------------------------------------

TITLE: Calculating Wirtinger and Conjugate Wirtinger Gradients for Complex Gradcheck (PyTorch, Python)
DESCRIPTION: This code snippet numerically estimates the Wirtinger (w_d) and Conjugate Wirtinger (conj_w_d) derivatives for a scalar complex-to-real function, in the context of PyTorch's gradcheck. It applies finite differences in both real and imaginary directions, combines them to get the required Wirtinger forms, and computes the corresponding gradient contribution using grad_out values. Dependencies include a compute_gradient callable, and it assumes grad_out=1 as commonly used in PyTorch gradcheck logic. The snippet outputs the conjugate Wirtinger gradient scaled appropriately for the gradcheck mechanism.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/gradcheck.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
# Code from https://github.com/pytorch/pytorch/blob/58eb23378f2a376565a66ac32c93a316c45b6131/torch/autograd/gradcheck.py#L99-L105\n# Notation changes in this code block:\n# s here is y above\n# x, y here are a, b above\n\nds_dx = compute_gradient(eps)\nds_dy = compute_gradient(eps * 1j)\n# conjugate wirtinger derivative\nconj_w_d = 0.5 * (ds_dx + ds_dy * 1j)\n# wirtinger derivative\nw_d = 0.5 * (ds_dx - ds_dy * 1j)\nd[d_idx] = grad_out.conjugate() * conj_w_d + grad_out * w_d.conj()\n\n# Since grad_out is always 1, and W and CW are complex conjugate of each other, the last line ends up computing exactly `conj_w_d + w_d.conj() = conj_w_d + conj_w_d = 2 * conj_w_d`.
```

----------------------------------------

TITLE: Enabling Detailed DDP Debug Logging in PyTorch Distributed Python
DESCRIPTION: This code shows how to enable detailed debugging for `DistributedDataParallel` (DDP) by setting the `TORCH_DISTRIBUTED_DEBUG` environment variable to "DETAIL" and `TORCH_CPP_LOG_LEVEL` to "INFO". It initializes a simple DDP model and runs a training loop. Running this code with these environment variables will produce detailed logs about DDP initialization and runtime performance statistics during training, useful for performance analysis.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.rst#_snippet_5

LANGUAGE: python
CODE:
```
import os

import torch
import torch.distributed as dist
import torch.multiprocessing as mp


class TwoLinLayerNet(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.a = torch.nn.Linear(10, 10, bias=False)
        self.b = torch.nn.Linear(10, 1, bias=False)

    def forward(self, x):
        a = self.a(x)
        b = self.b(x)
        return (a, b)


def worker(rank):
    dist.init_process_group("nccl", rank=rank, world_size=2)
    torch.cuda.set_device(rank)
    print("init model")
    model = TwoLinLayerNet().cuda()
    print("init ddp")
    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])

    inp = torch.randn(10, 10).cuda()
    print("train")

    for _ in range(20):
        output = ddp_model(inp)
        loss = output[0] + output[1]
        loss.sum().backward()


if __name__ == "__main__":
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "29501"
    os.environ["TORCH_CPP_LOG_LEVEL"]="INFO"
    os.environ[
        "TORCH_DISTRIBUTED_DEBUG"
    ] = "DETAIL"  # set to DETAIL for runtime logging.
    mp.spawn(worker, nprocs=2, args=())
```

----------------------------------------

TITLE: Running Performance Benchmark
DESCRIPTION: Executes performance comparison between eager mode and AOT compiled functions
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/aot_autograd_optimizations.ipynb#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
large_inputs = [torch.randn(1024, 2048, requires_grad=True) for _ in range(4)]

bench(fn, large_inputs, "Eager")
bench(aot_nnc_fn, large_inputs, "AOT")
```

----------------------------------------

TITLE: Disabling Reduced-Precision Reduction for FP16/BF16 GEMMs in PyTorch (Python)
DESCRIPTION: This snippet provides settings to disable reduced-precision accumulation for GEMM operations with FP16 and BF16 data types on PyTorch’s CUDA backend. Disabling these features can avoid numerical overflows by ensuring intermediate accumulation uses single-precision, at the cost of performance. This code is relevant when numerical reproducibility is preferred over speed and is only meaningful on GPU backends supporting these configuration flags. The main configuration properties are torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction and torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction, which are set to False to disable reduced precision.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/numerical_accuracy.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False
torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False
```

----------------------------------------

TITLE: Computing Vector-Jacobian Product (VJP) with torch.func.vjp in Python
DESCRIPTION: This snippet shows how to use `torch.func.vjp` to compute vector-Jacobian products. `vjp` is applied to a function (`torch.sin`) and input tensors. It returns the original function's output and a new function (`vjp_fn`). This `vjp_fn` takes cotangent vectors (matching the structure of the function's output) and computes the VJP.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.whirlwind_tour.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from torch.func import vjp

inputs = torch.randn(3)
func = torch.sin
cotangents = (torch.randn(3),)

outputs, vjp_fn = vjp(func, inputs); vjps = vjp_fn(*cotangents)
```

----------------------------------------

TITLE: View Operations for Attention Mechanism Reshaping
DESCRIPTION: Lists the usage of unsafe view operations to reshape tensors for attention heads and sequences. These operations change tensor dimensions without copying data, enabling efficient reshaping for multi-head attention calculations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
Operator: aten._unsafe_view.default
cnt: 108, ((T([2, 16, 576, 48], f16), [32, 576, 48]), {})
cnt: 36, ((T([2, 16, 48, 576], f16), [32, 48, 576]), {})
cnt: 36, ((T([32, 576, 576], f16), [2, 16, 576, 576]), {})
cnt: 144, ((T([2, 576, 576, 16], f16), [663552, 16]), {})
cnt: 72, ((T([663552, 16], f16), [2, 576, 576, 16]), {})
cnt: 72, ((T([2, 16, 576, 576], f16), [32, 576, 576]), {})
cnt: 36, ((T([32, 576, 48], f16), [2, 16, 576, 48]), {})
cnt: 36, ((T([2, 576, 16, 48], f16), [2, 576, 768]), {})
cnt: 2, ((T([2, 16, 48, 577], f16), [32, 48, 577]), {})
cnt: 2, ((T([32, 1, 577], f16), [2, 16, 1, 577]), {})
cnt: 2, ((T([2, 16, 577, 48], f16), [32, 577, 48]), {})
cnt: 2, ((T([32, 1, 48], f16), [2, 16, 1, 48]), {})
cnt: 2, ((T([2, 577, 16, 48], f16), [2, 577, 768]), {})
cnt: 2, ((T([2, 577, 768], f16), [1154, 768]), {})
cnt: 36, ((T([2, 576, 3, 16, 48], f16), [2, 576, 2304]), {})
```

----------------------------------------

TITLE: Enumerating Tensor Signatures for relu_, sigmoid, sigmoid_backward, and silu_ Operators - PyTorch - Python
DESCRIPTION: This block sequences invocation examples for multiple PyTorch in-place and functional operators (relu_, sigmoid, sigmoid_backward, silu_). Each entry encodes the shapes and dtypes of input (and where applicable, output or gradient) tensors for specific operator variants. Arguments are compactly described as tuples containing tensor specifications, enabling easy tracking of tensor patterns for operator call statistics, testing, or kernel dispatch analysis. There are no external runtime dependencies, but implied use is by tensor-level operator instrumentation. Inputs and outputs are schematic and use symbolic tensor signatures. Limitations: These are data lines or log entries, not executable code.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 19, 1, 1], f16),), {})
Operator: aten.relu_.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 25, 1, 1], f16),), {})
Operator: aten.relu_.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 30, 1, 1], f16),), {})
Operator: aten.relu_.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 36, 1, 1], f16),), {})
Operator: aten.relu_.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 42, 1, 1], f16),), {})
Operator: aten.relu_.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 47, 1, 1], f16),), {})
Operator: aten.relu_.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 53, 1, 1], f16),), {})
Operator: aten.relu_.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 58, 1, 1], f16),), {})
Operator: aten.relu_.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 64, 1, 1], f16),), {})
Operator: aten.relu_.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 70, 1, 1], f16),), {})
Operator: aten.relu_.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 75, 1, 1], f16),), {})
Operator: aten.relu_.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 81, 1, 1], f16),), {})
Operator: aten.relu_.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 87, 1, 1], f16),), {})
Operator: aten.relu_.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 228, 1, 1], f16),), {})
Operator: aten.sigmoid.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 300, 1, 1], f16),), {})
Operator: aten.sigmoid.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 366, 1, 1], f16),), {})
Operator: aten.sigmoid.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 432, 1, 1], f16),), {})
Operator: aten.sigmoid.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 504, 1, 1], f16),), {})
Operator: aten.sigmoid.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 570, 1, 1], f16),), {})
Operator: aten.sigmoid.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 636, 1, 1], f16),), {})
Operator: aten.sigmoid.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 702, 1, 1], f16),), {})
Operator: aten.sigmoid.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 768, 1, 1], f16),), {})
Operator: aten.sigmoid.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 840, 1, 1], f16),), {})
Operator: aten.sigmoid.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 906, 1, 1], f16),), {})
Operator: aten.sigmoid.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 972, 1, 1], f16),), {})
Operator: aten.sigmoid.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 1044, 1, 1], f16),), {})
Operator: aten.sigmoid.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 1044, 1, 1], f16), T([128, 1044, 1, 1], f16)), {})
Operator: aten.sigmoid_backward.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 972, 1, 1], f16), T([128, 972, 1, 1], f16)), {})
Operator: aten.sigmoid_backward.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 906, 1, 1], f16), T([128, 906, 1, 1], f16)), {})
Operator: aten.sigmoid_backward.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 840, 1, 1], f16), T([128, 840, 1, 1], f16)), {})
Operator: aten.sigmoid_backward.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 768, 1, 1], f16), T([128, 768, 1, 1], f16)), {})
Operator: aten.sigmoid_backward.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 702, 1, 1], f16), T([128, 702, 1, 1], f16)), {})
Operator: aten.sigmoid_backward.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 636, 1, 1], f16), T([128, 636, 1, 1], f16)), {})
Operator: aten.sigmoid_backward.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 570, 1, 1], f16), T([128, 570, 1, 1], f16)), {})
Operator: aten.sigmoid_backward.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 504, 1, 1], f16), T([128, 504, 1, 1], f16)), {})
Operator: aten.sigmoid_backward.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 432, 1, 1], f16), T([128, 432, 1, 1], f16)), {})
Operator: aten.sigmoid_backward.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 366, 1, 1], f16), T([128, 366, 1, 1], f16)), {})
Operator: aten.sigmoid_backward.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 300, 1, 1], f16), T([128, 300, 1, 1], f16)), {})
Operator: aten.sigmoid_backward.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 228, 1, 1], f16), T([128, 228, 1, 1], f16)), {})
Operator: aten.sigmoid_backward.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 32, 112, 112], f16),), {})
Operator: aten.silu_.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 96, 112, 112], f16),), {})
Operator: aten.silu_.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 162, 56, 56], f16),), {})
Operator: aten.silu_.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 228, 56, 56], f16),), {})
Operator: aten.silu_.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 300, 28, 28], f16),), {})
Operator: aten.silu_.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 366, 28, 28], f16),), {})
Operator: aten.silu_.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 432, 14, 14], f16),), {})
Operator: aten.silu_.default
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 504, 14, 14], f16),), {})
Operator: aten.silu_.default
```

----------------------------------------

TITLE: Decompiling Dynamo Generated Code in Python
DESCRIPTION: This snippet demonstrates how to decompile the compiled code object generated by Dynamo using the depyf library. This allows inspection of the structure of the compiled function, including how it handles graph breaks and calls to compiled subgraphs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_overview.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from depyf import decompile
print(decompile(code))
```

----------------------------------------

TITLE: Performing Inference with Torch Compile on Intel GPU
DESCRIPTION: Illustrates inference with and without torch.compile optimization for ResNet50 model on Intel GPU, measuring performance before and after applying torch.compile.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/get_start_xpu.rst#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
import torch
import torchvision.models as models
import time

model = models.resnet50(weights="ResNet50_Weights.DEFAULT")
model.eval()
data = torch.rand(1, 3, 224, 224)
ITERS = 10

model = model.to("xpu")
data = data.to("xpu")

for i in range(ITERS):
    start = time.time()
    with torch.no_grad():
        model(data)
        torch.xpu.synchronize()
    end = time.time()
    print(f"Inference time before torch.compile for iteration {i}: {(end-start)*1000} ms")

model = torch.compile(model)
for i in range(ITERS):
    start = time.time()
    with torch.no_grad():
        model(data)
        torch.xpu.synchronize()
    end = time.time()
    print(f"Inference time after torch.compile for iteration {i}: {(end-start)*1000} ms")

print("Execution finished")
```

----------------------------------------

TITLE: Enabling oneDNN Graph Fusion for Float - Python
DESCRIPTION: This Python snippet shows how to enable global oneDNN Graph fusion, trace a PyTorch model with example float input, and then run the traced model. Fusion occurs automatically during runtime execution.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/onednn/README.md#_snippet_6

LANGUAGE: python
CODE:
```
# enable oneDNN graph fusion globally\ntorch.jit.enable_onednn_fusion(True)\n\n# define the model\ndef MyModel(torch.nn.Module):\n    ...\n\n# construct the model\nmodel = MyModel(…)\nwith torch.no_grad():\n    model.eval()\n    model = torch.jit.trace(model, torch.rand(args.batch_size, 3, 224, 224))\n\n# run the model\nwith torch.no_grad():\n    # oneDNN graph fusion will be triggered during runtime\n    output = model(images)
```

----------------------------------------

TITLE: Defining TorchScript Expression Grammar
DESCRIPTION: Specifies the grammar for TorchScript expressions, including atoms, primaries, arithmetic operations, and function calls. It defines the syntax for various expression constructs like list comprehensions, dictionary displays, and slicing operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_18

LANGUAGE: TorchScript
CODE:
```
atom      ::=  identifier | literal | enclosure
enclosure ::=  parenth_form | list_display | dict_display

parenth_form ::=  '(' [expression_list] ')'

list_comprehension ::=  expression comp_for
comp_for           ::=  'for' target_list 'in' or_expr
list_display       ::=  '[' [expression_list | list_comprehension] ']'
dict_display       ::=  '{' [key_datum_list | dict_comprehension] '}'
key_datum_list     ::=  key_datum (',' key_datum)*
key_datum          ::=  expression ':' expression
dict_comprehension ::=  key_datum comp_for

primary ::=  atom | attributeref | subscription | slicing | call

attributeref ::=  primary '.' identifier

subscription ::=  primary '[' expression_list ']'

slicing      ::=  primary '[' slice_list ']'
slice_list   ::=  slice_item (',' slice_item)* [',']
slice_item   ::=  expression | proper_slice
proper_slice ::=  [expression] ':' [expression] [':' [expression] ]

call          ::=  primary '(' argument_list ')'
argument_list ::=  args [',' kwargs] | kwargs
args          ::=  [arg (',' arg)*]
kwargs        ::=  [kwarg (',' kwarg)*]
kwarg         ::=  arg '=' expression
arg           ::=  identifier

power ::=  primary ['**' u_expr]

u_expr ::=  power | '-' power | '~' power

m_expr ::=  u_expr | m_expr '*' u_expr | m_expr '@' m_expr | m_expr '//' u_expr | m_expr '/' u_expr | m_expr '%' u_expr
a_expr ::=  m_expr | a_expr '+' m_expr | a_expr '-' m_expr
```

----------------------------------------

TITLE: PyTorch Log Softmax Operations
DESCRIPTION: Log softmax forward and backward operations on 128x1000 tensors using float16 precision
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
aten._log_softmax.default(T([128, 1000], f16), 1, False)
aten._log_softmax_backward_data.default(T([128, 1000], f16), T([128, 1000], f16), 1, f16)
```

----------------------------------------

TITLE: Invoking aten.sum.SymInt Operator (Log)
DESCRIPTION: Log entries detailing calls to the PyTorch `aten.sum.SymInt` operator. Each line shows the invocation count (`cnt`) and arguments: the input tensor (shape, dtype f16, sometimes stride), the dimension(s) to sum over, and a boolean indicating whether to keep the dimension.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/crossvit_9_240_training.txt#2025-04-22_snippet_8

LANGUAGE: text
CODE:
```
Operator: aten.sum.SymInt
cnt: 2, ((T([64, 1000], f16), [0], True), {})
cnt: 6, ((T([64, 256], f16, stride=(50432, 1)), [0], True), {})
cnt: 3, ((T([64, 128], f16), [0], True), {})
cnt: 12, ((T([25664, 128], f16), [0], True), {})
cnt: 3, ((T([64, 1, 128], f16), [0, 1], True), {})
cnt: 6, ((T([64, 128], f16, stride=(51328, 1)), [0], True), {})
cnt: 3, ((T([64, 256], f16), [0], True), {})
cnt: 24, ((T([12608, 256], f16), [0], True), {})
cnt: 3, ((T([64, 1, 256], f16), [0, 1], True), {})
cnt: 18, ((T([12608, 768], f16), [0], True), {})
cnt: 6, ((T([25664, 384], f16), [0], True), {})
cnt: 1, ((T([64, 197, 256], f16), [0], True), {})
cnt: 1, ((T([64, 1, 256], f16, stride=(50432, 256, 1)), [0], True), {})
cnt: 1, ((T([64, 401, 128], f16), [0], True), {})
cnt: 1, ((T([64, 1, 128], f16, stride=(51328, 128, 1)), [0], True), {})
```

----------------------------------------

TITLE: Custom Module with State Dict Serialization - PyTorch - Python
DESCRIPTION: Presents the process of defining a custom torch.nn.Module with submodules, extracting its state_dict, saving this to disk, and restoring it into an identically structured module. Depends on torch and torch.nn.functional. Inputs are the module definition and instantiation; outputs are saved and restored state_dicts. Key parameters include layer dimensions. This pattern is extendable to complex, nested modules.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
# A module with two linear layers
>>> class MyModule(torch.nn.Module):
      def __init__(self):
        super().__init__()
        self.l0 = torch.nn.Linear(4, 2)
        self.l1 = torch.nn.Linear(2, 1)

      def forward(self, input):
        out0 = self.l0(input)
        out0_relu = torch.nn.functional.relu(out0)
        return self.l1(out0_relu)

>>> m = MyModule()
>>> m.state_dict()
OrderedDict([('l0.weight', tensor([[ 0.1400, 0.4563, -0.0271, -0.4406],
                                   [-0.3289, 0.2827, 0.4588, 0.2031]])),
             ('l0.bias', tensor([ 0.0300, -0.1316])),
             ('l1.weight', tensor([[0.6533, 0.3413]])),
             ('l1.bias', tensor([-0.1112]))])

>>> torch.save(m.state_dict(), 'mymodule.pt')
>>> m_state_dict = torch.load('mymodule.pt')
>>> new_m = MyModule()
>>> new_m.load_state_dict(m_state_dict)
<All keys matched successfully>
```

----------------------------------------

TITLE: PyTorch Dtype Conversion Flow
DESCRIPTION: Example showing performance impact of dtype conversions in quantized models, comparing inefficient and efficient conversion patterns.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization-accuracy-debugging.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
fp32_op -> quant -> int8_op -> dequant -> fp32_op -> quant -> int8_op -> dequant
```

LANGUAGE: python
CODE:
```
fp32_op -> fp32_op -> quant -> int8_op -> int8_op -> dequant
```

----------------------------------------

TITLE: Calling aten.embedding.default (Python)
DESCRIPTION: Looks up embeddings for given indices in an embedding table. Used to map integer indices (like token IDs) to dense vectors. Examples show lookup in float16 embedding tables using int64 index tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_12

LANGUAGE: Python
CODE:
```
((T([50265, 1024], f16), T([8, 128], i64), 1), {})
```

LANGUAGE: Python
CODE:
```
((T([1026, 1024], f16), T([8, 128], i64)), {})
```

----------------------------------------

TITLE: TorchScript Enum Example
DESCRIPTION: Demonstrates defining and using enum types in TorchScript with comparison operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
import torch
from enum import Enum

class Color(Enum):
    RED = 1
    GREEN = 2

def enum_fn(x: Color, y: Color) -> bool:
    if x == Color.RED:
        return True
    return x == y

m = torch.jit.script(enum_fn)

print("Eager: ", enum_fn(Color.RED, Color.GREEN))
print("TorchScript: ", m(Color.RED, Color.GREEN))
```

----------------------------------------

TITLE: Analyzing PyTorch Operator Statistics for Deep Neural Network
DESCRIPTION: This data represents usage statistics of PyTorch operators in what appears to be an Inception-style neural network implementation. It captures tensor dimensions, data types (mostly float16/f16), and call frequency for various operators such as convolution, pooling, tensor addition, and softmax operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_inception_v3_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([128, 1000], f16), 1, False), {})
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})
Operator: aten.add.Tensor
cnt: 4, ((T([128, 384, 8, 8], f16), T([128, 384, 8, 8], f16)), {})
cnt: 3, ((T([128, 2048, 8, 8], f16), T([128, 2048, 8, 8], f16)), {})
cnt: 3, ((T([128, 1280, 8, 8], f16), T([128, 1280, 8, 8], f16)), {})
cnt: 14, ((T([128, 768, 17, 17], f16), T([128, 768, 17, 17], f16)), {})
cnt: 5, ((T([128, 288, 35, 35], f16), T([128, 288, 35, 35], f16)), {})
cnt: 3, ((T([128, 256, 35, 35], f16), T([128, 256, 35, 35], f16)), {})
cnt: 3, ((T([128, 192, 35, 35], f16), T([128, 192, 35, 35], f16)), {})
Operator: aten.add_.Tensor
cnt: 94, ((T([], i64), 1), {})
Operator: aten.addmm.default
cnt: 1, ((T([1000], f16), T([128, 2048], f16), T([2048, 1000], f16, stride=(1, 2048))), {})
Operator: aten.avg_pool2d.default
cnt: 1, ((T([128, 192, 35, 35], f16), [3, 3], [1, 1], [1, 1]), {})
cnt: 1, ((T([128, 256, 35, 35], f16), [3, 3], [1, 1], [1, 1]), {})
cnt: 1, ((T([128, 288, 35, 35], f16), [3, 3], [1, 1], [1, 1]), {})
cnt: 4, ((T([128, 768, 17, 17], f16), [3, 3], [1, 1], [1, 1]), {})
cnt: 1, ((T([128, 1280, 8, 8], f16), [3, 3], [1, 1], [1, 1]), {})
cnt: 1, ((T([128, 2048, 8, 8], f16), [3, 3], [1, 1], [1, 1]), {})
Operator: aten.avg_pool2d_backward.default
cnt: 1, ((T([128, 2048, 8, 8], f16), T([128, 2048, 8, 8], f16), [3, 3], [1, 1], [1, 1], False, True, None), {})
cnt: 1, ((T([128, 1280, 8, 8], f16), T([128, 1280, 8, 8], f16), [3, 3], [1, 1], [1, 1], False, True, None), {})
cnt: 4, ((T([128, 768, 17, 17], f16), T([128, 768, 17, 17], f16), [3, 3], [1, 1], [1, 1], False, True, None), {})
cnt: 1, ((T([128, 288, 35, 35], f16), T([128, 288, 35, 35], f16), [3, 3], [1, 1], [1, 1], False, True, None), {})
cnt: 1, ((T([128, 256, 35, 35], f16), T([128, 256, 35, 35], f16), [3, 3], [1, 1], [1, 1], False, True, None), {})
cnt: 1, ((T([128, 192, 35, 35], f16), T([128, 192, 35, 35], f16), [3, 3], [1, 1], [1, 1], False, True, None), {})
Operator: aten.cat.default
cnt: 1, (([T([128, 64, 35, 35], f16), T([128, 64, 35, 35], f16), T([128, 96, 35, 35], f16), T([128, 32, 35, 35], f16)], 1), {})
cnt: 2, (([T([128, 64, 35, 35], f16), T([128, 64, 35, 35], f16), T([128, 96, 35, 35], f16), T([128, 64, 35, 35], f16)], 1), {})
cnt: 1, (([T([128, 384, 17, 17], f16), T([128, 96, 17, 17], f16), T([128, 288, 17, 17], f16)], 1), {})
cnt: 4, (([T([128, 192, 17, 17], f16), T([128, 192, 17, 17], f16), T([128, 192, 17, 17], f16), T([128, 192, 17, 17], f16)], 1), {})
cnt: 1, (([T([128, 320, 8, 8], f16), T([128, 192, 8, 8], f16), T([128, 768, 8, 8], f16)], 1), {})
cnt: 4, (([T([128, 384, 8, 8], f16), T([128, 384, 8, 8], f16)], 1), {})
cnt: 2, (([T([128, 320, 8, 8], f16), T([128, 768, 8, 8], f16), T([128, 768, 8, 8], f16), T([128, 192, 8, 8], f16)], 1), {})
Operator: aten.clone.default
cnt: 1, ((T([128, 3, 299, 299], f16),), {})
Operator: aten.convolution.default
cnt: 1, ((T([128, 3, 299, 299], f16), T([32, 3, 3, 3], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 32, 149, 149], f16), T([32, 32, 3, 3], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 32, 147, 147], f16), T([64, 32, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 64, 73, 73], f16), T([80, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 80, 73, 73], f16), T([192, 80, 3, 3], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 192, 35, 35], f16), T([64, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 192, 35, 35], f16), T([48, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([128, 48, 35, 35], f16), T([64, 48, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 64, 35, 35], f16), T([96, 64, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([128, 96, 35, 35], f16), T([96, 96, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 192, 35, 35], f16), T([32, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([128, 256, 35, 35], f16), T([64, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 256, 35, 35], f16), T([48, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 288, 35, 35], f16), T([64, 288, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 288, 35, 35], f16), T([48, 288, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 288, 35, 35], f16), T([384, 288, 3, 3], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 96, 35, 35], f16), T([96, 96, 3, 3], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 12, ((T([128, 768, 17, 17], f16), T([192, 768, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 768, 17, 17], f16), T([128, 768, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 128, 17, 17], f16), T([128, 128, 1, 7], f16), None, [1, 1], [0, 3], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 128, 17, 17], f16), T([192, 128, 7, 1], f16), None, [1, 1], [3, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 128, 17, 17], f16), T([128, 128, 7, 1], f16), None, [1, 1], [3, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 128, 17, 17], f16), T([192, 128, 1, 7], f16), None, [1, 1], [0, 3], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 768, 17, 17], f16), T([160, 768, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 160, 17, 17], f16), T([160, 160, 1, 7], f16), None, [1, 1], [0, 3], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 160, 17, 17], f16), T([192, 160, 7, 1], f16), None, [1, 1], [3, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 160, 17, 17], f16), T([160, 160, 7, 1], f16), None, [1, 1], [3, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 160, 17, 17], f16), T([192, 160, 1, 7], f16), None, [1, 1], [0, 3], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 192, 17, 17], f16), T([192, 192, 1, 7], f16), None, [1, 1], [0, 3], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 192, 17, 17], f16), T([192, 192, 7, 1], f16), None, [1, 1], [3, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 192, 17, 17], f16), T([320, 192, 3, 3], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 192, 17, 17], f16), T([192, 192, 3, 3], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 1280, 8, 8], f16), T([320, 1280, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 1280, 8, 8], f16), T([384, 1280, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 384, 8, 8], f16), T([384, 384, 1, 3], f16), None, [1, 1], [0, 1], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 384, 8, 8], f16), T([384, 384, 3, 1], f16), None, [1, 1], [1, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 1280, 8, 8], f16), T([448, 1280, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 448, 8, 8], f16), T([384, 448, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 1280, 8, 8], f16), T([192, 1280, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 2048, 8, 8], f16), T([320, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 2048, 8, 8], f16), T([384, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 2048, 8, 8], f16), T([448, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 2048, 8, 8], f16), T([192, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
Operator: aten.convolution_backward.default
cnt: 1, ((T([128, 192, 8, 8], f16), T([128, 2048, 8, 8], f16), T([192, 2048, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 4, ((T([128, 384, 8, 8], f16), T([128, 384, 8, 8], f16), T([384, 384, 3, 1], f16), [0], [1, 1], [1, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 4, ((T([128, 384, 8, 8], f16), T([128, 384, 8, 8], f16), T([384, 384, 1, 3], f16), [0], [1, 1], [0, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 384, 8, 8], f16), T([128, 448, 8, 8], f16), T([384, 448, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 448, 8, 8], f16), T([128, 2048, 8, 8], f16), T([448, 2048, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 384, 8, 8], f16), T([128, 2048, 8, 8], f16), T([384, 2048, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 320, 8, 8], f16), T([128, 2048, 8, 8], f16), T([320, 2048, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 192, 8, 8], f16), T([128, 1280, 8, 8], f16), T([192, 1280, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 448, 8, 8], f16), T([128, 1280, 8, 8], f16), T([448, 1280, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 384, 8, 8], f16), T([128, 1280, 8, 8], f16), T([384, 1280, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 320, 8, 8], f16), T([128, 1280, 8, 8], f16), T([320, 1280, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 192, 8, 8], f16), T([128, 192, 17, 17], f16), T([192, 192, 3, 3], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

----------------------------------------

TITLE: Creating and Running a GPipe Schedule in Python
DESCRIPTION: This snippet demonstrates how to create a GPipe schedule using a PipelineStage and execute it with input data. It shows the process of creating the schedule, preparing input data, and running the pipeline step for different ranks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.pipelining.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from torch.distributed.pipelining import ScheduleGPipe

# Create a schedule
schedule = ScheduleGPipe(stage, n_microbatches)

# Input data (whole batch)
x = torch.randn(batch_size, in_dim, device=device)

# Run the pipeline with input `x`
# `x` will be divided into microbatches automatically
if rank == 0:
    schedule.step(x)
else:
    output = schedule.step()
```

----------------------------------------

TITLE: Demonstrating Tensor Aliasing in PyTorch Python
DESCRIPTION: Shows how assigning one Tensor to another or taking a slice creates new Tensor objects that share the same underlying storage, illustrating the reference nature of Tensors in PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_32

LANGUAGE: Python
CODE:
```
a = torch.rand(2, 3)
b = a
# At this point, `a` and `b` share their storage.
c = b[0]
# `c` shares storage with `a` and `b`, but only sees a slice of the allocated memory.
```

----------------------------------------

TITLE: Implementing Torch Function Decorator
DESCRIPTION: Decorator for registering torch function implementations in the global dispatch table for ScalarTensor operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
import functools
def implements(torch_function):
    """Register a torch function override for ScalarTensor"""
    def decorator(func):
        functools.update_wrapper(func, torch_function)
        HANDLED_FUNCTIONS[torch_function] = func
        return func
    return decorator
```

----------------------------------------

TITLE: Backward Pass Stream Semantics - Safe Initial Grad in Same Stream - PyTorch Python
DESCRIPTION: This snippet shows a safe pattern for providing an initial gradient to `loss.backward()`. The initial gradient tensor (`torch.ones_like(loss)`) is created and the `backward()` call using it are both placed within the same stream context (`with torch.cuda.stream(s)`), ensuring proper ordering.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_18

LANGUAGE: Python
CODE:
```
with torch.cuda.stream(s):
    loss.backward(gradient=torch.ones_like(loss))
```

----------------------------------------

TITLE: Using CUDAGuard to Change Device Index in PyTorch (C++)
DESCRIPTION: This example demonstrates the use of at::cuda::CUDAGuard to temporarily change the current CUDA device index within a scope. It changes the device index to 1 while maintaining the current CUDA stream.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_8

LANGUAGE: C++
CODE:
```
{
  at::cuda::CUDAGuard device_guard(1);

  // current device index is changed to 1 within scope
  // current CUDA stream is still `streams1[0]` on device 1, no change
}
// current device index is reset to 0 after `device_guard` is destroyed
```

----------------------------------------

TITLE: Incompatible Operators for PyTorch CUDAGraph
DESCRIPTION: List of PyTorch operators that are incompatible with CUDAGraph Trees, causing the compiler to skip graph recording for functions containing these operators.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_cudagraph_trees.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
aten._fused_moving_avg_obs_fq_helper.default
aten._fused_moving_avg_obs_fq_helper_functional.default
aten.multinomial.default
fbgemm.dense_to_jagged.default
fbgemm.jagged_to_padded_dense.default
run_and_save_rng_state
run_with_rng_state
aten._local_scalar_dense
aten._assert_scalar
```

----------------------------------------

TITLE: Uninstalling PyTorch Packages (Bash)
DESCRIPTION: These bash commands uninstall existing PyTorch installations using both conda and pip. The first command uses `conda uninstall` and the second uses `pip uninstall` with confirmation. This is a prerequisite before reinstalling, especially in develop mode.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_2

LANGUAGE: bash
CODE:
```
conda uninstall pytorch -y
yes | pip uninstall torch
```

----------------------------------------

TITLE: Debugging Hangs with MonitoredBarrier in PyTorch Distributed Python
DESCRIPTION: This snippet demonstrates using `torch.distributed.monitored_barrier` to identify ranks that fail to reach a synchronization point, simulating a hang on one rank. It requires a Gloo process group for the barrier and sets a timeout. The expected output is a `RuntimeError` on rank 0 indicating the hanging rank after the timeout expires, which helps diagnose distributed application hangs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.rst#_snippet_4

LANGUAGE: python
CODE:
```
import os
from datetime import timedelta

import torch
import torch.distributed as dist
import torch.multiprocessing as mp


def worker(rank):
    dist.init_process_group("nccl", rank=rank, world_size=2)
    # monitored barrier requires gloo process group to perform host-side sync.
    group_gloo = dist.new_group(backend="gloo")
    if rank not in [1]:
        dist.monitored_barrier(group=group_gloo, timeout=timedelta(seconds=2))


if __name__ == "__main__":
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "29501"
    mp.spawn(worker, nprocs=2, args=())
```

----------------------------------------

TITLE: Custom NumPy Take Function with vmap Support
DESCRIPTION: Implementation of a custom take operation using NumPy with vmap support. Includes forward, backward, setup_context, and partial vmap implementation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.func.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
class NumpyTake(torch.autograd.Function):
    @staticmethod
    def forward(x, ind, ind_inv, dim):
        device = x.device
        x = to_numpy(x)
        ind = to_numpy(ind)
        return torch.tensor(np.take_along_axis(x, ind, dim), device=device)

    @staticmethod
    def setup_context(ctx, inputs, output):
        x, ind, ind_inv, dim = inputs
        ctx.save_for_backward(ind, ind_inv)
        ctx.dim = dim

    @staticmethod
    def backward(ctx, grad_output):
        ind, ind_inv = ctx.saved_tensors
        result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim)
        return result, None, None, None

    @staticmethod
    def vmap(info, in_dims, x, ind, ind_inv, dim):
        x_bdim, ind_bdim, ind_inv_bdim, _ = in_dims
        logical_dim = x.dim() if x_bdim is None else x_bdim - 1
        dim = dim if dim >= 0 else dim + logical_dim

        def maybe_expand_bdim_at_front(x, x_bdim):
```

----------------------------------------

TITLE: Enabling or Disabling TF32 Precision for CUDA Backend in PyTorch (Python)
DESCRIPTION: This snippet shows how to configure the use of TensorFloat32 (TF32) tensor cores for matrix multiplications and convolutions in PyTorch by setting the allow_tf32 property. The operation requires PyTorch with a CUDA-enabled Nvidia Ampere (or later) GPU. Enabling TF32 can improve performance at the cost of reduced precision. The key configuration parameters are torch.backends.cuda.matmul.allow_tf32 and torch.backends.cudnn.allow_tf32, accepting boolean values to enable or disable TF32 usage in matmul and convolution routines.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/numerical_accuracy.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = False
```

----------------------------------------

TITLE: Optimization with Closure Function in PyTorch
DESCRIPTION: Example for advanced optimizers like Conjugate Gradient and LBFGS that need to re-evaluate the function multiple times. A closure function encapsulates the gradient computation process.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
for input, target in dataset:
    def closure():
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        return loss
    optimizer.step(closure)
```

----------------------------------------

TITLE: Unsupported Autocast Decorator on JIT Scripted Functions
DESCRIPTION: Example showing that applying the autocast decorator directly to a JIT scripted function is not supported in PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/JIT-AUTOCAST.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch
from torch.cpu.amp import autocast

@torch.jit.script
@autocast() # not supported
def foo(a, b, c, d):
    ...
```

----------------------------------------

TITLE: Computing Jacobians with jacrev Transform in PyTorch
DESCRIPTION: Shows how to compute Jacobians using reverse-mode AD and batch them using vmap.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/whirlwind_tour.ipynb#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from functorch import jacrev
x = torch.randn(5)
jacobian = jacrev(torch.sin)(x)
expected = torch.diag(torch.cos(x))
assert torch.allclose(jacobian, expected)

x = torch.randn(64, 5)
jacobian = vmap(jacrev(torch.sin))(x)
assert jacobian.shape == (64, 5, 5)
```

----------------------------------------

TITLE: Calling aten.any.default (Python)
DESCRIPTION: Checks if any element in a boolean tensor is True, returning a single boolean value. Example shows application on a boolean tensor derived from a float16 tensor shape.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_8

LANGUAGE: Python
CODE:
```
((T([8, 128, 1024], b8),), {})
```

----------------------------------------

TITLE: Implementing Custom Torch Operations
DESCRIPTION: Implementation of custom mean and add operations for ScalarTensor, including type conversion utilities and support for mixed tensor operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
def ensure_tensor(data):
    if isinstance(data, ScalarTensor):
        return data.tensor()
    return torch.as_tensor(data)

@implements(torch.add)
def add(input, other):
   try:
       if input._N == other._N:
           return ScalarTensor(input._N, input._value + other._value)
       else:
           raise ValueError("Shape mismatch!")
   except AttributeError:
       return torch.add(ensure_tensor(input), ensure_tensor(other))

@implements(torch.mean)
def mean(input):
    return float(input._value) / input._N
```

----------------------------------------

TITLE: Creating Tensors with Default Options in PyTorch C++
DESCRIPTION: This example shows how to create tensors using factory functions without specifying TensorOptions, which results in default properties (32-bit float, strided, CPU tensor that does not require a gradient).
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_7

LANGUAGE: cpp
CODE:
```
// A 32-bit float, strided, CPU tensor that does not require a gradient.
torch::Tensor tensor = torch::randn({3, 4});
torch::Tensor range = torch::arange(5, 10);
```

----------------------------------------

TITLE: Logging PyTorch Tensor Dimensions and Counts
DESCRIPTION: Log entries showing the frequency of tensor shapes in a PyTorch model, with each line containing a count and tensor dimensions. Each tensor pair has identical input/output shapes using float16 precision.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nasnetalarge_training.txt#2025-04-22_snippet_4

LANGUAGE: plaintext
CODE:
```
cnt: 2, ((T([16, 2688, 11, 11], f16), T([16, 2688, 11, 11], f16), 0), {})
cnt: 12, ((T([16, 2016, 21, 21], f16), T([16, 2016, 21, 21], f16), 0), {})
cnt: 4, ((T([16, 672, 21, 21], f16), T([16, 672, 21, 21], f16), 0), {})
cnt: 66, ((T([16, 336, 21, 21], f16), T([16, 336, 21, 21], f16), 0), {})
cnt: 2, ((T([16, 1344, 21, 21], f16), T([16, 1344, 21, 21], f16), 0), {})
cnt: 12, ((T([16, 1008, 42, 42], f16), T([16, 1008, 42, 42], f16), 0), {})
cnt: 6, ((T([16, 336, 42, 42], f16), T([16, 336, 42, 42], f16), 0), {})
cnt: 60, ((T([16, 168, 42, 42], f16), T([16, 168, 42, 42], f16), 0), {})
cnt: 2, ((T([16, 168, 83, 83], f16), T([16, 168, 83, 83], f16), 0), {})
cnt: 6, ((T([16, 84, 42, 42], f16), T([16, 84, 42, 42], f16), 0), {})
cnt: 4, ((T([16, 84, 83, 83], f16), T([16, 84, 83, 83], f16), 0), {})
cnt: 5, ((T([16, 96, 165, 165], f16), T([16, 96, 165, 165], f16), 0), {})
cnt: 6, ((T([16, 42, 83, 83], f16), T([16, 42, 83, 83], f16), 0), {})
cnt: 1, ((T([16, 42, 165, 165], f16), T([16, 42, 165, 165], f16), 0), {})
```

----------------------------------------

TITLE: Vectorized Ensemble Prediction with vmap (Distinct Minibatch) - functorch/PyTorch - Python
DESCRIPTION: Applies functorch's vmap to compute predictions for all models in one operation, using their respective stacked parameters, buffers, and a distinct minibatch per model. An assertion checks consistency with the traditional looping approach. Requires functorch, correctly prepared fmodel, params, buffers, and matching data dimensions for minibatches.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/ensembling.ipynb#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from functorch import vmap

predictions1_vmap = vmap(fmodel)(params, buffers, minibatches)

# verify the vmap predictions match the 
assert torch.allclose(predictions1_vmap, torch.stack(predictions_diff_minibatch_loop), atol=1e-3, rtol=1e-5)
```

----------------------------------------

TITLE: Serializing and Loading Exported PyTorch Programs
DESCRIPTION: Example of saving an ExportedProgram to disk using torch.export.save and loading it back with torch.export.load. The convention is to use the .pt2 file extension for exported programs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
import torch
import io

class MyModule(torch.nn.Module):
    def forward(self, x):
        return x + 10

exported_program = torch.export.export(MyModule(), torch.randn(5))

torch.export.save(exported_program, 'exported_program.pt2')
saved_exported_program = torch.export.load('exported_program.pt2')
```

----------------------------------------

TITLE: General Schema for Tensor Creation in PyTorch C++
DESCRIPTION: This snippet shows the general schema for creating tensors using factory functions in PyTorch C++. It includes placeholders for the function name, function-specific options, sizes, and tensor options.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_0

LANGUAGE: cpp
CODE:
```
torch::<function-name>(<function-specific-options>, <sizes>, <tensor-options>)
```

----------------------------------------

TITLE: Enabling Full FP16 Accumulation (Python)
DESCRIPTION: Shows the Python code to enable full FP16 accumulation for FP16 matrix multiplications. This setting can increase performance on certain GPUs (compute capability 7.0 or newer) but comes with a higher risk of numerical instability and overflow.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_9

LANGUAGE: python
CODE:
```
torch.backends.cuda.matmul.allow_fp16_accumulation = True
```

----------------------------------------

TITLE: Basic Usage Examples of First-class Dimensions in PyTorch
DESCRIPTION: Demonstrates key use cases including matrix multiplication with einsum, pixel shuffling with rearrange, batched matrix multiplication, and embedding bag operations using first-class dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from torchdim import dims

# einsum
def mm(A: torch.Tensor, B: torch.Tensor):
    i, j, k = dims(3)
    r = (A[i, k] * B[k, j]).sum(k)
    return r.order(i, j)

# rearrange
def pixel_shuffle(img: torch.Tensor, upscale_factor=2):
    h2, w2, c, b, h, w = dims(6)
    h2.size = w2.size = upscale_factor
    return img[b, (c, h2, w2), h, w].order(b, c, (h, h2), (w, w2))

# batching
def bmm(A: torch.Tensor, B: torch.Tensor):
    i = dims(1)
    return mm(A[i], B[i]).order(i)

# indexing
def embedding_bag(input: torch.Tensor, embedding_weights: torch.Tensor):
    batch, sequence, features = dims(3)
    r = embedding_weights[input[batch, sequence], features].sum(sequence)
    return r.order(batch, features)
```

----------------------------------------

TITLE: Installing Ninja and Setting CMake Generator (BAT)
DESCRIPTION: This snippet shows how to install the Ninja build system using pip and then configure CMake to use Ninja as the generator via the `CMAKE_GENERATOR` environment variable. This is recommended to parallelize CUDA build tasks on Windows, which Visual Studio currently doesn't support efficiently.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#_snippet_1

LANGUAGE: bat
CODE:
```
REM Let's install ninja first.
pip install ninja

REM Set it as the cmake generator
set CMAKE_GENERATOR=Ninja
```

----------------------------------------

TITLE: Using torch.cond for Data-Dependent Operations
DESCRIPTION: Demonstrates how to use torch.cond as a replacement for data-dependent if statements to maintain graph compilation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
# old
@torch.compile
def fn(x):
    if x.sum() > 0:
        return x + 1
    return x - 1

# new
@torch.compile
def fn(x):
    return torch.cond(
        x.sum() > 0,
        lambda x: x + 1,
        lambda x: x - 1,
        (x,),
    )
```

----------------------------------------

TITLE: Limitation with Disabling Autocast in Scripted Functions
DESCRIPTION: Example demonstrating that disabling autocast within a scripted function doesn't override an active autocast context from eager mode - the operation will still be performed in mixed precision.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/JIT-AUTOCAST.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
import torch
from torch.cuda.amp import autocast

@torch.jit.script
def fn(a, b):
    with autocast(enabled=False):
        return torch.mm(a, b)

x = torch.rand((2, 2), device='cuda', dtype=torch.float)
y = torch.rand((2, 2), device='cuda', dtype=torch.float)

# this will print half-precision dtype
with autocast(enabled=True):
    print(fn(x, y).dtype)
```

----------------------------------------

TITLE: Applying Tanh Activation and its Backward using aten.tanh and aten.tanh_backward - Python
DESCRIPTION: Showcases the use of the aten.tanh.default and aten.tanh_backward.default operators, which perform element-wise tanh activation and propagate gradients through tanh in backward passes. Operates on both high-rank and lower-rank f16 tensors, returning outputs of the same shape. Dependencies: PyTorch and input tensors with compatible dtype and shape.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
Operator: aten.tanh.default
cnt: 12, ((T([2, 1024, 3072], f16),), {})
cnt: 1, ((T([2, 768], f16),), {})
cnt: 1, ((T([2, 1024, 768], f16),), {})
Operator: aten.tanh_backward.default
cnt: 1, ((T([2, 1024, 768], f16), T([2, 1024, 768], f16)), {})
cnt: 12, ((T([2, 1024, 3072], f16), T([2, 1024, 3072], f16)), {})
```

----------------------------------------

TITLE: Using Global Interpreter Lock (GIL) in C++
DESCRIPTION: This snippet demonstrates how to use the pybind11::gil_scoped_acquire RAII struct to handle taking and releasing the Python Global Interpreter Lock (GIL) when making calls to the Python API.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/README.md#2025-04-22_snippet_2

LANGUAGE: C++
CODE:
```
void iWantToUsePython() {
  pybind11::gil_scoped_acquire gil;
  ...
}
```

----------------------------------------

TITLE: Initializing Modules for Partial Network Capture PyTorch
DESCRIPTION: This snippet sets up the modules (`Linear`) and loss function (`MSELoss`) that would be used in an example demonstrating partial network capture with `torch.cuda.make_graphed_callables`. It defines the tensor dimensions and creates two linear layers and an MSE loss instance, moving them to the CUDA device. This setup precedes the actual usage of `make_graphed_callables` for capturing only parts of a larger, potentially non-capturable, workflow.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_42

LANGUAGE: python
CODE:
```
N, D_in, H, D_out = 640, 4096, 2048, 1024

module1 = torch.nn.Linear(D_in, H).cuda()
module2 = torch.nn.Linear(H, D_out).cuda()
module3 = torch.nn.Linear(H, D_out).cuda()

loss_fn = torch.nn.MSELoss()
```

----------------------------------------

TITLE: Vector-Jacobian Product (VJP) Transform in PyTorch
DESCRIPTION: Shows how to use the vjp transform to compute vector-Jacobian products with cotangents.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/whirlwind_tour.ipynb#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from functorch import vjp

inputs = torch.randn(3)
func = torch.sin
cotangents = (torch.randn(3),)

outputs, vjp_fn = vjp(func, inputs); vjps = vjp_fn(*cotangents)
```

----------------------------------------

TITLE: Demonstrating CUDA Stream Synchronization Error
DESCRIPTION: Example showing a data race condition where a tensor is modified on different CUDA streams without proper synchronization, potentially causing race conditions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cuda._sanitizer.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch

a = torch.rand(4, 2, device="cuda")

with torch.cuda.stream(torch.cuda.Stream()):
    torch.mul(a, 5, out=a)
```

----------------------------------------

TITLE: Configuring PyTorch ATen Project with CMake
DESCRIPTION: This CMake configuration sets up a project that uses PyTorch's ATen library. It requires a minimum CMake version of 3.0, finds and includes the ATen package, enables C++17 support (except on MSVC), and configures the main executable to link against ATen libraries.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/test/test_install/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
cmake_minimum_required(VERSION 3.0)
find_package(ATen REQUIRED)
include_directories(${ATEN_INCLUDE_DIR})

# C++17
if(not MSVC)
    set(CMAKE_CXX_FLAGS "--std=c++17 ${CMAKE_CXX_FLAGS}")
endif()
add_executable(main main.cpp)
target_link_libraries(main ${ATEN_LIBRARIES})
```

----------------------------------------

TITLE: Matrix Multiplication for Linear Projections
DESCRIPTION: Shows the matrix multiplication operations used throughout the network for linear projections. These operations handle various tensor shapes including attention projections, feed-forward networks, and the final classification layer.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
Operator: aten.mm.default
cnt: 72, ((T([663552, 16], f16), T([16, 16], f16, stride=(1, 16))), {})
cnt: 1, ((T([2, 1000], f16), T([1000, 768], f16)), {})
cnt: 1, ((T([1000, 2], f16, stride=(1, 1000)), T([2, 768], f16, stride=(443136, 1))), {})
cnt: 2, ((T([2, 768], f16), T([768, 3072], f16)), {})
cnt: 2, ((T([768, 2], f16, stride=(1, 768)), T([2, 3072], f16)), {})
cnt: 2, ((T([2, 3072], f16), T([3072, 768], f16)), {})
cnt: 2, ((T([3072, 2], f16, stride=(1, 3072)), T([2, 768], f16)), {})
cnt: 4, ((T([2, 768], f16), T([768, 768], f16)), {})
cnt: 2, ((T([768, 2], f16, stride=(1, 768)), T([2, 768], f16)), {})
cnt: 4, ((T([1154, 768], f16), T([768, 768], f16)), {})
cnt: 4, ((T([768, 1154], f16, stride=(1, 768)), T([1154, 768], f16)), {})
cnt: 2, ((T([768, 2], f16, stride=(1, 768)), T([2, 768], f16, stride=(443136, 1))), {})
cnt: 36, ((T([1152, 768], f16), T([768, 3072], f16)), {})
cnt: 36, ((T([768, 1152], f16, stride=(1, 768)), T([1152, 3072], f16)), {})
cnt: 36, ((T([1152, 3072], f16), T([3072, 768], f16)), {})
cnt: 36, ((T([3072, 1152], f16, stride=(1, 3072)), T([1152, 768], f16)), {})
cnt: 36, ((T([1152, 768], f16), T([768, 768], f16)), {})
cnt: 36, ((T([768, 1152], f16, stride=(1, 768)), T([1152, 768], f16)), {})
cnt: 72, ((T([16, 663552], f16, stride=(1, 16)), T([663552, 16], f16)), {})
cnt: 72, ((T([663552, 16], f16), T([16, 16], f16)), {})
cnt: 36, ((T([1152, 2304], f16), T([2304, 768], f16)), {})
cnt: 36, ((T([2304, 1152], f16, stride=(1, 2304)), T([1152, 768], f16)), {})
```

----------------------------------------

TITLE: Defining a Simple Model for Automatic Splitting in Python
DESCRIPTION: This code defines a simple model class with an embedding layer, multiple layers in a ModuleList, and a language model head. This model structure is used to demonstrate automatic splitting for pipeline parallelism.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.pipelining.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
class Model(torch.nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.emb = torch.nn.Embedding(10, 3)
        self.layers = torch.nn.ModuleList(
            Layer() for _ in range(2)
        )
        self.lm = LMHead()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.emb(x)
        for layer in self.layers:
            x = layer(x)
        x = self.lm(x)
        return x
```

----------------------------------------

TITLE: Setting Current CUDA Stream in PyTorch (C++)
DESCRIPTION: This snippet sets the current CUDA stream to the first stream in the streams1 array. It's a prerequisite for the subsequent operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_7

LANGUAGE: C++
CODE:
```
at::cuda::setCurrentCUDAStream(streams1[0]);
```

----------------------------------------

TITLE: PyTorch Matrix Operations - Addition and Multiplication
DESCRIPTION: Matrix addition and multiplication operations with various tensor shapes and stride configurations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_Bert_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
aten.add.Tensor((T([4, 512, 768], f16), T([4, 512, 768], f16)), {})
aten.addmm.default((T([768], f16), T([2048, 768], f16), T([768, 768], f16, stride=(1, 768))), {})
aten.bmm.default((T([48, 512, 64], f16), T([48, 64, 512], f16)), {})
```

----------------------------------------

TITLE: Matrix Multiplication in PyTorch
DESCRIPTION: This snippet shows various matrix multiplication operations using aten.mm.default. It demonstrates different tensor shapes and data types, primarily using half-precision (f16) tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_11

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([1024, 1], f16), T([1, 4000], f16)), {})
cnt: 1, ((T([1, 1024], f16), T([1024, 4000], f16)), {})
cnt: 8, ((T([1024, 4000], f16), T([4000, 4000], f16)), {})
cnt: 8, ((T([4000, 1024], f16, stride=(1, 4000)), T([1024, 4000], f16)), {})
cnt: 1, ((T([1024, 4000], f16), T([4000, 31068], f16)), {})
cnt: 1, ((T([4000, 1024], f16, stride=(1, 4000)), T([1024, 31068], f16)), {})
cnt: 1, ((T([1024, 192], f16), T([192, 1500], f16)), {})
cnt: 1, ((T([192, 1024], f16, stride=(1, 192)), T([1024, 1500], f16)), {})
cnt: 2, ((T([1024, 1500], f16), T([1500, 1500], f16)), {})
cnt: 2, ((T([1500, 1024], f16, stride=(1, 1500)), T([1024, 1500], f16)), {})
cnt: 1, ((T([1500, 1024], f16, stride=(1, 1500)), T([1024, 2000], f16)), {})
```

----------------------------------------

TITLE: Padding Tensors with ATen ConstantPadND Operator
DESCRIPTION: The `aten.constant_pad_nd` demonstrates tensor padding in PyTorch. Utilizes padding values, such as [0, 1] for tensors like [8192, 32, 63]. Allows controlled expansion of tensor dimensionality.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_12

LANGUAGE: plaintext
CODE:
```
Operator: aten.constant_pad_nd.default
cnt: 2, ((T([8192, 32, 63], f16), [0, 1], 0.0), {})
```

----------------------------------------

TITLE: Defining a PyTorch fuser method (Python)
DESCRIPTION: Defines a Python function (`fuse_linear_relu`) used by `BackendPatternConfig` to specify how to combine a `Linear` module and a `ReLU` module into a fused `nn.intrinsic.LinearReLU`. This function is invoked during the fusion step of `prepare_fx`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
def fuse_linear_relu(is_qat, linear, relu):
    return nni.LinearReLU(linear, relu)
```

----------------------------------------

TITLE: Defining a Simple TorchScript Module (Python)
DESCRIPTION: Defines a basic torch.jit.ScriptModule class 'M' with a 'forward' method demonstrating conditional logic. This module serves as an example to illustrate how Python code is compiled into the TorchScript graph representation and subsequently translated back into Python-like code by the Python printer.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_41

LANGUAGE: Python
CODE:
```
class M(torch.jit.ScriptModule):
    @torch.jit.script_method
    def forward(self, x, y, z):
        # type: (Tensor, int, float) -> Tensor
        if y > 2:
            x = x + z
        else:
            x = x + y
        return x

m = M()
```

----------------------------------------

TITLE: Creating a CSC Tensor in PyTorch from Explicit Components
DESCRIPTION: Shows how to create a sparse CSC (Compressed Sparse Column) tensor by providing ccol_indices, row_indices, and values tensors. The example creates a 2x2 sparse matrix with all elements defined.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
ccol_indices = torch.tensor([0, 2, 4])
row_indices = torch.tensor([0, 1, 0, 1])
values = torch.tensor([1, 2, 3, 4])
csc = torch.sparse_csc_tensor(ccol_indices, row_indices, values, dtype=torch.float64)
csc
```

----------------------------------------

TITLE: Demonstrating comptime breakpoint usage in Python with torch.compile
DESCRIPTION: This example shows a complete function using torch.compile with a comptime breakpoint. It illustrates how to trigger the breakpoint and use various debugging commands to inspect the Dynamo state.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
import torch
from torch._dynamo.comptime import comptime

@torch.compile(backend="eager")
def f(x):
    y = x + 1
    comptime.breakpoint()
    y = y + 1
    return y

f(torch.ones(3, 3))
```

----------------------------------------

TITLE: Loading Resources from a Package
DESCRIPTION: Shows how to load resources from a torch package using load_pickle, load_text, and load_binary methods. This demonstrates accessing saved artifacts after the package is created.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
importer = torch.PackageImporter("package.pt")
my_tensor = importer.load_pickle("my_resources", "tensor.pkl")
text = importer.load_text("config_stuff", "words.txt")
binary = importer.load_binary("raw_data", "binary")
```

----------------------------------------

TITLE: Demonstrating Input Mutation Support in CUDAGraph Trees
DESCRIPTION: This code example shows how CUDAGraph Trees handle input mutations for different types of inputs, including tensors from eager execution, parameters and buffers, and outputs from previous CUDAGraph Tree operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_cudagraph_trees.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch

@torch.compile(mode="reduce-overhead")
def foo(x):
    return x + 1

@torch.compile(mode="reduce-overhead")
def mut(x):
    return x.add_(2)

# Enable input mutation support
torch._inductor.config.triton.cudagraph_support_input_mutation = True

for i in range(3):
    torch.compiler.cudagraph_mark_step_begin()
    inp = torch.rand([4], device="cuda")

    # CUDAGraph is applied since `foo` does not mutate `inp`
    tmp = foo(inp)
    # Although `mut` mutates `tmp`, which is an output of a CUDAGraph
    # managed function. So CUDAGraph is still applied.
    mut(tmp)


torch.compiler.cudagraph_mark_step_begin()
inp = torch.rand([4], device="cuda")

tmp = foo(inp)
# While `tmp` is a CUDAGraph Tree managed function's output, `tmp.clone()`
# is not. So CUDAGraph is not applied to `mut` and there is a log
# `skipping cudagraphs due to mutated inputs`
mut(tmp.clone())
```

----------------------------------------

TITLE: Auto-Convert Example with FakeTensorMode
DESCRIPTION: Illustrates a potential issue with automatic conversion of real tensors to fake tensors within FakeTensorMode context.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_fake_tensor.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
with FakeTensorMode():
    real_tensor.t_()
```

----------------------------------------

TITLE: ReLU Backward Operations in PyTorch
DESCRIPTION: This snippet shows backward pass operations for ReLU activations, used during gradient computation. These operations calculate gradients through the ReLU function, zeroing out gradients where input was negative and passing through gradients where input was positive.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
Operator: aten.threshold_backward.default
cnt: 4, ((T([96, 512, 4, 4], f16), T([96, 512, 4, 4], f16), 0), {})
cnt: 4, ((T([96, 256, 8, 8], f16), T([96, 256, 8, 8], f16), 0), {})
cnt: 4, ((T([96, 128, 16, 16], f16), T([96, 128, 16, 16], f16), 0), {})
cnt: 4, ((T([96, 64, 32, 32], f16), T([96, 64, 32, 32], f16), 0), {})
cnt: 1, ((T([96, 64, 64, 64], f16), T([96, 64, 64, 64], f16), 0), {})
```

----------------------------------------

TITLE: Performing Inference with AMP on Intel GPU
DESCRIPTION: Executes an AMP-based inference using ResNet50 on Intel GPU, utilizing PyTorch's autocast to handle data types efficiently for accelerated processing on available XPU.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/get_start_xpu.rst#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
import torch
import torchvision.models as models

model = models.resnet50(weights="ResNet50_Weights.DEFAULT")
model.eval()
data = torch.rand(1, 3, 224, 224)

model = model.to("xpu")
data = data.to("xpu")

with torch.no_grad():
    d = torch.rand(1, 3, 224, 224)
    d = d.to("xpu")
    # set dtype=torch.bfloat16 for BF16
    with torch.autocast(device_type="xpu", dtype=torch.float16, enabled=True):
        model(data)

print("Execution finished")
```

----------------------------------------

TITLE: Allocating PyTorch Tensors on HIP Devices (via 'cuda') - Python
DESCRIPTION: Shows how to create and move tensors to specific HIP (ROCm) devices in PyTorch. It uses the 'cuda' string as the device type, which maps internally to HIP when PyTorch is built with ROCm support. Demonstrates device specification via `torch.device`, `.cuda()`, `.to()`, and within a `torch.cuda.device` context manager.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/hip.rst#_snippet_0

LANGUAGE: Python
CODE:
```
cuda = torch.device('cuda')     # Default HIP device
cuda0 = torch.device('cuda:0')  # 'rocm' or 'hip' are not valid, use 'cuda'
cuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)

x = torch.tensor([1., 2.], device=cuda0)
# x.device is device(type='cuda', index=0)
y = torch.tensor([1., 2.]).cuda()
# y.device is device(type='cuda', index=0)

with torch.cuda.device(1):
    # allocates a tensor on GPU 1
    a = torch.tensor([1., 2.], device=cuda)

    # transfers a tensor from CPU to GPU 1
    b = torch.tensor([1., 2.]).cuda()
    # a.device and b.device are device(type='cuda', index=1)

    # You can also use ``Tensor.to`` to transfer a tensor:
    b2 = torch.tensor([1., 2.]).to(device=cuda)
    # b.device and b2.device are device(type='cuda', index=1)

    c = a + b
    # c.device is device(type='cuda', index=1)

    z = x + y
    # z.device is device(type='cuda', index=0)

    # even within a context, you can specify the device
    # (or give a GPU index to the .cuda call)
    d = torch.randn(2, device=cuda2)
    e = torch.randn(2).to(cuda2)
    f = torch.randn(2).cuda(cuda2)
    # d.device, e.device, and f.device are all device(type='cuda', index=2)
```

----------------------------------------

TITLE: Incorrectly Passing Received Shared Tensors Between Queues (Python)
DESCRIPTION: Illustrates an incorrect pattern for inter-process communication using shared tensors. Directly putting a tensor `x` received from one queue (`queue`) into another queue (`queue_2`) will not work as intended due to the underlying shared memory mechanisms and reference counting. The shared tensor cannot simply be re-shared this way.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/multiprocessing.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
# not going to work
x = queue.get()
queue_2.put(x)
```

----------------------------------------

TITLE: Implementing Multi-threaded Static Runtime with Thread Pool
DESCRIPTION: Demonstrates how to implement data parallel execution using Static Runtime with a thread pool. Uses boost::lockfree::stack to cache runtime instances for efficient multi-threaded inference.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/runtime/static/README.md#2025-04-22_snippet_1

LANGUAGE: cpp
CODE:
```
  // initialization
  auto mod = PrepareForStaticRuntime(m);
  // 128 is good for most cases. Pick a number that works for you
  boost::lockfree::stack<std::shared_ptr<StaticRuntime>,
    boost::lockfree::fixed_sized<true>> pool(128);

  // inference
  std::shared_ptr<StaticRuntime> runtime = nullptr;
  pool.pop(runtime);
  if (!runtime) {
    runtime = std::make_shared<StaticRuntime>(mod, opts);
  }
  auto output = runtime->run(args, kwargs);
  pool.push(runtime);
```

----------------------------------------

TITLE: Defining Stateless Loss Computation Function for functorch
DESCRIPTION: Defines a function `compute_loss_stateless_model` compatible with `functorch` transforms. It accepts the functional model's `params`, `buffers`, a single input `sample`, and a single `target`. Inside, it unsqueezes the sample and target to create a mini-batch of size 1 (as required by the original model structure), performs the forward pass using the stateless `fmodel`, and calculates the loss using the predefined `loss_fn`. The function returns the scalar loss value for the single sample.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
def compute_loss_stateless_model (params, buffers, sample, target):
    batch = sample.unsqueeze(0)
    targets = target.unsqueeze(0)

    predictions = fmodel(params, buffers, batch) 
    loss = loss_fn(predictions, targets)
    return loss
```

----------------------------------------

TITLE: Defining Simple Statements in TorchScript
DESCRIPTION: Specifies the syntax for various simple statements in TorchScript, including expression statements, assignments, augmented assignments, and annotated assignments.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_25

LANGUAGE: python
CODE:
```
expression_stmt    ::=  starred_expression
starred_expression ::=  expression | (starred_item ",")* [starred_item]
starred_item       ::=  assignment_expression | "*" or_expr

assignment_stmt ::=  (target_list "=")+ (starred_expression)
target_list     ::=  target ("," target)* [","]
target          ::=  identifier
                     | "(" [target_list] ")"
                     | "[" [target_list] "]"
                     | attributeref
                     | subscription
                     | slicing
                     | "*" target

augmented_assignment_stmt ::= augtarget augop (expression_list)
augtarget                 ::= identifier | attributeref | subscription
augop                     ::= "+=" | "-=" | "*=" | "/=" | "//=" | "%=" |
                              "**="| ">>=" | "<<=" | "&=" | "^=" | "|="

annotated_assignment_stmt ::= augtarget ":" expression
                              ["=" (starred_expression)]
```

----------------------------------------

TITLE: Enabling CPU Fallback for Unsupported MPS Operations (Environment Variable)
DESCRIPTION: Set `PYTORCH_ENABLE_MPS_FALLBACK` to `1` to allow PyTorch operations to automatically fall back to the CPU execution if they are not currently supported by the MPS backend on Apple Silicon.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mps_environment_variables.rst#2025-04-22_snippet_7

LANGUAGE: plaintext
CODE:
```
PYTORCH_ENABLE_MPS_FALLBACK
```

----------------------------------------

TITLE: Automatic Transfer of Scalar Tensors Between CPU and GPU in PyTorch
DESCRIPTION: This code snippet demonstrates how scalar tensors are automatically transferred between CPU and GPU when needed in operations, which is an exception to the general rule that tensors are never moved automatically between devices.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> # two scalars
>>> torch.ones(()) + torch.ones(()).cuda()  # OK, scalar auto-transferred from CPU to GPU
>>> torch.ones(()).cuda() + torch.ones(())  # OK, scalar auto-transferred from CPU to GPU

>>> # one scalar (CPU), one vector (GPU)
>>> torch.ones(()) + torch.ones(1).cuda()  # OK, scalar auto-transferred from CPU to GPU
>>> torch.ones(1).cuda() + torch.ones(())  # OK, scalar auto-transferred from CPU to GPU

>>> # one scalar (GPU), one vector (CPU)
```

----------------------------------------

TITLE: Creating Remote Reference and Retrieving Data PyTorch RPC Python
DESCRIPTION: This snippet shows a user worker (A) creating a Remote Reference (RRef) to an object on an owner worker (B) using `rpc.remote`, illustrating the scenario where the RRef is returned to the user. It then calls `to_here()` on the RRef to retrieve the actual data locally, blocking until the result is available. This demonstrates the interaction when the RRef is created remotely and needed locally. Requires PyTorch and the distributed RPC framework.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/rref.rst#_snippet_0

LANGUAGE: Python
CODE:
```
import torch
import torch.distributed.rpc as rpc

# on worker A
rref = rpc.remote('B', torch.add, args=(torch.ones(2), 1))
# say the rref has RRefId 100 and ForkId 1
rref.to_here()
```

----------------------------------------

TITLE: Reorderable Logging in PyTorch Compilation
DESCRIPTION: Example showing how to configure logging functions to be reordered during compilation to avoid graph breaks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
import torch

torch._dynamo.config.reorderable_logging_functions.add(print)

@torch.compile
def fn(x):
    x += 1
    print("log!")
    return torch.sin(x)
```

----------------------------------------

TITLE: Matrix Multiplication Operations in PyTorch
DESCRIPTION: This snippet shows matrix multiplication operations used in fully connected layers. These operations take input tensors and multiply them with weight matrices, transforming feature representations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
Operator: aten.mm.default
cnt: 1, ((T([96, 65], f16), T([65, 512], f16)), {})
cnt: 1, ((T([65, 96], f16, stride=(1, 65)), T([96, 512], f16)), {})
```

----------------------------------------

TITLE: Displaying Shape of Calculated Per-Sample Gradients
DESCRIPTION: Prints the shape of the first element in the `per_sample_grads` list, which corresponds to the per-sample gradients computed for the first parameter of the model (likely `model.conv1.weight`). The output shape (e.g., `[64, 32, 1, 3, 3]`) demonstrates that a separate gradient tensor has been computed for each of the 64 samples in the batch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
print(per_sample_grads[0].shape)
```

----------------------------------------

TITLE: Aligning Tensor Dimensions by Names
DESCRIPTION: Describes how to align tensor dimensions by their names using Tensor.align_as or Tensor.align_to methods, ensuring compatibility across tensor operations regardless of initial dimension ordering.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/named_tensor.rst#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
def scale_channels(input, scale):
    scale = scale.refine_names('C')
    return input * scale.align_as(input)

num_channels = 3
scale = torch.randn(num_channels, names=('C',))
imgs = torch.rand(3, 3, 3, num_channels, names=('N', 'H', 'W', 'C'))
more_imgs = torch.rand(3, num_channels, 3, 3, names=('N', 'C', 'H', 'W'))
videos = torch.randn(3, num_channels, 3, 3, 3, names=('N', 'C', 'H', 'W', 'D'))

scale_channels(imgs, scale)
scale_channels(more_imgs, scale)
scale_channels(videos, scale)
```

----------------------------------------

TITLE: Computing Jacobians with Respect to Multiple Parameters Using jacrev - Python
DESCRIPTION: Demonstrates functorch.jacrev's 'argnums' argument to compute Jacobians of both model parameters (weight and bias) instead of input. Returns two Jacobians (for weight and bias). Prerequisite: functorch.jacrev and all variables in scope. No output, just computation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
# note the change in input via argnums params of 0,1 to map to weight and bias
ft_jac_weight, ft_jac_bias = jacrev(predict, argnums=(0, 1))(weight, bias, x)
```

----------------------------------------

TITLE: Applying In-Place ReLU Activation (aten.relu_.default) in PyTorch (Python)
DESCRIPTION: These are in-place ReLU operator calls for float16 tensors of various shapes. They modify the input tensor directly, with no returned value, for efficiency during forward inference in deep neural networks. PyTorch must be used in-place context, input tensor is overwritten, dtype and shape must match.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mnasnet1_0_training.txt#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
Operator: aten.relu_.default
cnt: 2, ((T([32, 32, 112, 112], f16),), {})
cnt: 1, ((T([32, 48, 112, 112], f16),), {})
cnt: 1, ((T([32, 48, 56, 56], f16),), {})
cnt: 5, ((T([32, 72, 56, 56], f16),), {})
cnt: 1, ((T([32, 72, 28, 28], f16),), {})
cnt: 4, ((T([32, 120, 28, 28], f16),), {})
cnt: 1, ((T([32, 240, 28, 28], f16),), {})
cnt: 1, ((T([32, 240, 14, 14], f16),), {})
cnt: 6, ((T([32, 480, 14, 14], f16),), {})
cnt: 3, ((T([32, 576, 14, 14], f16),), {})
cnt: 1, ((T([32, 576, 7, 7], f16),), {})
cnt: 8, ((T([32, 1152, 7, 7], f16),), {})
cnt: 1, ((T([32, 1280, 7, 7], f16),), {})
```

----------------------------------------

TITLE: Benchmarking Jacobian Computation With and Without vmap - Python
DESCRIPTION: Uses torch.utils.benchmark.Timer to time both manual (row-by-row) and functorch.jacrev (vectorized) Jacobian computations. Runs each Timer for 500 iterations and prints the timing results. Requires previous 'compute_jac', 'jacrev', and associated variables. Outputs: raw timing statistics for both approaches.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from torch.utils.benchmark import Timer

without_vmap = Timer(stmt="compute_jac(xp)", globals=globals())
with_vmap = Timer(stmt="jacrev(predict, argnums=2)(weight, bias, x)", globals=globals())

no_vmap_timer = without_vmap.timeit(500)
with_vmap_timer = with_vmap.timeit(500)

print(no_vmap_timer)
print(with_vmap_timer)
```

----------------------------------------

TITLE: Converting Sparse to Dense for Operations in PyTorch
DESCRIPTION: Shows how to handle unsupported operations by converting to dense format first.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
b_s.to_dense().cos()
```

----------------------------------------

TITLE: Installing PyTorch CPU-only on Windows CMD
DESCRIPTION: Executes the standard Python setup script with the 'develop' flag in a Windows Command Prompt environment to build and install a version of PyTorch that is optimized for CPU computation only, without GPU acceleration.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_14

LANGUAGE: CMD
CODE:
```
python setup.py develop
```

----------------------------------------

TITLE: AOT Function with Backpropagation Example
DESCRIPTION: Shows how to use AOT function with backpropagation, demonstrating that regular PyTorch operations can be mixed with compiled functions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/COMPILE_README.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
inp = torch.randn(3, requires_grad=True)
inp = inp.cos()
out = nf(inp)
out = out.sin().sum().backward()
```

----------------------------------------

TITLE: Example Program with Graph Breaks
DESCRIPTION: Basic code structure demonstrating where TorchDynamo attempts to compile tensor operations into a single FX graph
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
def some_fun(x):
    ...

torch.compile(some_fun)(x)
...
```

----------------------------------------

TITLE: DataFrame Operations and Shuffling
DESCRIPTION: Examples of performing operations on DataFrames including column creation, shuffling, and comparing with regular DataPipe shuffling.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/dataframes_pipes.ipynb#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
dp = get_dataframes_pipe(dataframe_size = 3)
dp['y'] = dp.i * 100 + dp.j - 2.7
dp = dp.shuffle()
dp = dp - 17
dp['y'] = dp.y * 10000
for i in dp.raw_iterator():
    print(i)
```

----------------------------------------

TITLE: Forward-mode Jacobian Computation with jacfwd in PyTorch
DESCRIPTION: Demonstrates computing Jacobians using forward-mode AD as an alternative to reverse-mode.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/whirlwind_tour.ipynb#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from functorch import jacfwd
x = torch.randn(5)
jacobian = jacfwd(torch.sin)(x)
expected = torch.diag(torch.cos(x))
assert torch.allclose(jacobian, expected)
```

----------------------------------------

TITLE: Forward Convolution Operations in PyTorch MobileNetV3
DESCRIPTION: Statistics for all forward convolution operations in the model. These form the backbone of the network and include standard convolutions, depthwise separable convolutions (indicated by groups parameter), and pointwise (1x1) convolutions for channel projection and expansion.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
Operator: aten.convolution.default
cnt: 1, ((T([128, 3, 224, 224], f16), T([16, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 16, 112, 112], f16), T([16, 16, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 16, 112, 112], f16), T([16, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 16), {})
cnt: 1, ((T([128, 16, 112, 112], f16), T([96, 16, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 96, 112, 112], f16), T([96, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 96), {})
cnt: 1, ((T([128, 96, 56, 56], f16), T([24, 96, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 24, 56, 56], f16), T([24, 24, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 24, 56, 56], f16), T([24, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 24), {})
cnt: 1, ((T([128, 24, 56, 56], f16), T([144, 24, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 144, 56, 56], f16), T([144, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 144), {})
cnt: 1, ((T([128, 144, 28, 28], f16), T([32, 144, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 32, 28, 28], f16), T([96, 32, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 96, 28, 28], f16), T([96, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 96), {})
cnt: 1, ((T([128, 96, 28, 28], f16), T([32, 96, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([128, 32, 28, 28], f16), T([192, 32, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 192, 28, 28], f16), T([192, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 192), {})
cnt: 2, ((T([128, 192, 28, 28], f16), T([32, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 192, 28, 28], f16), T([192, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 192), {})
cnt: 1, ((T([128, 192, 28, 28], f16), T([192, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 192), {})
cnt: 2, ((T([128, 192, 14, 14], f16), T([64, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 64, 14, 14], f16), T([192, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 192, 14, 14], f16), T([192, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 192), {})
cnt: 3, ((T([128, 64, 14, 14], f16), T([384, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([128, 384, 14, 14], f16), T([384, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 384), {})
cnt: 2, ((T([128, 384, 14, 14], f16), T([64, 384, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 384, 14, 14], f16), T([112, 384, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([128, 112, 14, 14], f16), T([672, 112, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 672, 14, 14], f16), T([672, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 672), {})
cnt: 2, ((T([128, 672, 14, 14], f16), T([112, 672, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 112, 14, 14], f16), T([336, 112, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 336, 14, 14], f16), T([336, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 336), {})
cnt: 1, ((T([128, 336, 14, 14], f16), T([112, 336, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 672, 14, 14], f16), T([672, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 672), {})
cnt: 1, ((T([128, 672, 7, 7], f16), T([184, 672, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 184, 7, 7], f16), T([1104, 184, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([128, 1104, 7, 7], f16), T([1104, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 1104), {})
cnt: 3, ((T([128, 1104, 7, 7], f16), T([184, 1104, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 1104, 7, 7], f16), T([1104, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1104), {})
cnt: 1, ((T([128, 1104, 7, 7], f16), T([352, 1104, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 352, 7, 7], f16), T([1984, 352, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

----------------------------------------

TITLE: Tuple Type Import Example in TorchScript
DESCRIPTION: Demonstrates the correct way to use tuple types in TorchScript by importing from typing module. Shows error case and solution.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
import torch

# ERROR: Tuple not recognized because not imported from typing
@torch.jit.export
def inc(x: Tuple[int, int]):
    return (x[0]+1, x[1]+1)

m = torch.jit.script(inc)
print(m((1,2)))
```

----------------------------------------

TITLE: Loading Data from Iterable-style Dataset without Batching
DESCRIPTION: Shows the equivalent operation of loading individual samples from an iterable-style dataset when automatic batching is disabled.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/data.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
for data in iter(dataset):
    yield collate_fn(data)
```

----------------------------------------

TITLE: Tracking Batch Normalization Forward Operator Calls in PyTorch
DESCRIPTION: Records frequency counts of batch normalization forward operations with different tensor shapes and parameters. Each entry shows a half-precision (f16) input tensor followed by parameters for the operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
cnt: 4, ((T([128, 184, 14, 14], f16), T([184], f16), T([184], f16), T([184], f16), T([184], f16), True, 0.1, 1e-05), {})
cnt: 2, ((T([128, 480, 14, 14], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f16), True, 0.1, 1e-05), {})
cnt: 2, ((T([128, 112, 14, 14], f16), T([112], f16), T([112], f16), T([112], f16), T([112], f16), True, 0.1, 1e-05), {})
cnt: 3, ((T([128, 672, 14, 14], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 672, 7, 7], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), True, 0.1, 1e-05), {})
cnt: 3, ((T([128, 160, 7, 7], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f16), True, 0.1, 1e-05), {})
cnt: 5, ((T([128, 960, 7, 7], f16), T([960], f16), T([960], f16), T([960], f16), T([960], f16), True, 0.1, 1e-05), {})
```

----------------------------------------

TITLE: Unsupported In-place Operations with vmap
DESCRIPTION: This example demonstrates how vmap raises an error when encountering an unsupported in-place operation where a tensor with fewer elements would be overwritten by a tensor with more elements due to broadcasting.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/ux_limitations.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
def f(x, y):
  x.add_(y)
  return x

x = torch.randn(1)
y = torch.randn(3)

# Raises an error because `y` has fewer elements than `x`.
vmap(f, in_dims=(None, 0))(x, y)
```

----------------------------------------

TITLE: Sparsifying Model Embeddings in PyTorch
DESCRIPTION: Example showing how to use the data sparsifier to sparsify embeddings in a neural network model. The sparsifier is applied to the embedding layer and integrated into the model training process.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/README.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
class Model(nn.Module):
    def __init__(self, feature_dim, emb_dim, num_classes):
        self.emb = nn.EmbeddingBag(feature_dim, emb_dim)
        self.linear1 = nn.Linear(emb_dim, 32)
        self.linear2 = nn.Linear(32, num_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        out = self.emb(x)
        out = self.relu(self.linear1(out))
        out = self.linear2(out)
        return out

model = Model(100, 32, 10)
my_sparsifier = ImplementedDataSparsifier(threshold=0.5)
my_sparsifier.add_data(name='emb', data=model.emb)

...
# Train model
...

my_sparsifier.step()  # creates mask for embeddings

my_sparsifier.squash_mask()  # applies and removes mask
```

----------------------------------------

TITLE: Checkout PyTorch Nightly with CUDA & venv (bash)
DESCRIPTION: Uses the `./tools/nightly.py` script to checkout a new branch (`-b`) and install PyTorch nightly binaries built specifically with CUDA support (`--cuda`) into a `venv` environment. This requires a CUDA-capable system; a subsequent `source` command activates the environment (use `& .\venv\Scripts\Activate.ps1` on Windows).
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_9

LANGUAGE: bash
CODE:
```
./tools/nightly.py checkout -b my-nightly-branch --cuda
source venv/bin/activate
```

----------------------------------------

TITLE: Symbolic Tracing of PyTorch Module
DESCRIPTION: Demonstrates how to use torch.fx.symbolic_trace to create a GraphModule from a custom PyTorch Module. The example shows the creation of a simple Module with various operations and its symbolic tracing.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/fx/README.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch

class MyModule(torch.nn.Module):
  def __init__(self) -> None:
    super().__init__()
    self.param = torch.nn.Parameter(
        torch.rand(3, 4))
    self.linear = torch.nn.Linear(4, 5)

  def forward(self, x):
    return self.linear(x + self.param).clamp(min=0.0, max=1.0)

from torch.fx import symbolic_trace
module = MyModule()
symbolic_traced : torch.fx.GraphModule = symbolic_trace(module)

input = torch.rand(3, 4)
torch.testing.assert_close(symbolic_traced(input), module(input))
```

----------------------------------------

TITLE: Serialization of Complex Tensors
DESCRIPTION: Demonstrates how to save and load complex tensors using torch.save and torch.load.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/complex_numbers.rst#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
torch.save(y, 'complex_tensor.pt')
torch.load('complex_tensor.pt')
```

----------------------------------------

TITLE: Using torch.distributed.rpc.rpc_async() in TorchScript
DESCRIPTION: Makes a non-blocking RPC call to run a function on a remote worker. RPC messages are sent and received in parallel to execution of Python code.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_34

LANGUAGE: python
CODE:
```
torch.distributed.rpc.rpc_async()
```

----------------------------------------

TITLE: Convolution Forward Operations in PyTorch
DESCRIPTION: Statistics for the aten.convolution.default operator showing forward convolution operations with various configurations. These operations implement different convolutional layers in a neural network, including different kernel sizes, strides, and channel dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
Operator: aten.convolution.default
cnt: 1, ((T([128, 3, 193, 193], f16), T([16, 3, 3, 3], f16), T([16], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 16, 96, 96], f16), T([32, 16, 3, 3], f16), T([32], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 32, 96, 96], f16), T([64, 32, 3, 3], f16), T([64], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 64, 97, 97], f16), T([128, 64, 3, 3], f16), T([128], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 128, 48, 48], f16), T([256, 128, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 128, 48, 48], f16), T([128, 128, 1, 1], f16), T([128], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 128, 48, 48], f16), T([128, 128, 3, 3], f16), T([128], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 256, 1, 1], f16), T([128, 256, 1, 1], f16), T([128], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 128, 1, 1], f16), T([256, 128, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([128, 256, 24, 24], f16), T([512, 256, 1, 1], f16), T([512], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 256, 48, 48], f16), T([256, 256, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 256, 49, 49], f16), T([256, 128, 3, 3], f16), T([256], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 2), {})
cnt: 3, ((T([128, 256, 24, 24], f16), T([256, 128, 3, 3], f16), T([256], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 2), {})
cnt: 2, ((T([128, 512, 1, 1], f16), T([256, 512, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 256, 1, 1], f16), T([512, 256, 1, 1], f16), T([512], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 512, 24, 24], f16), T([256, 512, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 512, 12, 12], f16), T([1536, 512, 1, 1], f16), T([1536], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 512, 24, 24], f16), T([768, 512, 1, 1], f16), T([768], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 768, 25, 25], f16), T([768, 128, 3, 3], f16), T([768], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 6), {})
cnt: 11, ((T([128, 768, 12, 12], f16), T([768, 128, 3, 3], f16), T([768], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 6), {})
cnt: 6, ((T([128, 768, 12, 12], f16), T([1536, 768, 1, 1], f16), T([1536], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 9, ((T([128, 1536, 1, 1], f16), T([768, 1536, 1, 1], f16), T([768], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 9, ((T([128, 768, 1, 1], f16), T([1536, 768, 1, 1], f16), T([1536], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 6, ((T([128, 1536, 12, 12], f16), T([768, 1536, 1, 1], f16), T([768], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 1536, 6, 6], f16), T([1536, 1536, 1, 1], f16), T([1536], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 768, 13, 13], f16), T([768, 128, 3, 3], f16), T([768], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 6), {})
cnt: 5, ((T([128, 768, 6, 6], f16), T([768, 128, 3, 3], f16), T([768], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 6), {})
cnt: 3, ((T([128, 768, 6, 6], f16), T([1536, 768, 1, 1], f16), T([1536], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 1536, 6, 6], f16), T([768, 1536, 1, 1], f16), T([768], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 1536, 6, 6], f16), T([3072, 1536, 1, 1], f16), T([3072], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

----------------------------------------

TITLE: Listing Available Backends for torch.compile in Python
DESCRIPTION: Shows how to view the list of supported backends for torch.compile using the torch.compiler.list_backends() function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
torch.compiler.list_backends()
```

----------------------------------------

TITLE: Adding Multi-Worker Support for Model Predictions in PyTorch
DESCRIPTION: Introduces a --num_workers option in server.py to allow multiple workers in the ThreadPoolWorker for model predictions. Each worker uses its own CUDA stream created during thread initialization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/inference/CHANGELOG.md#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
parser.add_argument('--num_workers', type=int, default=1)

# In worker initialization
worker_stream = torch.cuda.Stream()
with torch.cuda.stream(worker_stream):
    # Perform model prediction
```

----------------------------------------

TITLE: Optimizing PyTorch Mobile Modules
DESCRIPTION: The torch.utils.mobile_optimizer.optimize_for_mobile function optimizes PyTorch models for mobile execution. It requires a torch.jit.ScriptModule object, a blocklist of optimization passes, a list of preserved methods, and a backend type. The method applies various optimizations depending on the backend, such as operation fusion and prepacking for CPU, and GPU data transfer for Vulkan. Ensure you pass the correct method list to preserve any additional methods beyond 'forward'.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mobile_optimizer.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
torch.utils.mobile_optimizer.optimize_for_mobile
```

----------------------------------------

TITLE: Operator Decomposition for Trace Function in C++
DESCRIPTION: Shows how to implement a batching rule by decomposing an operator into simpler operations. This example demonstrates decomposing the trace operation into diagonal and sum operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/writing_batching_rules.md#2025-04-22_snippet_1

LANGUAGE: cpp
CODE:
```
Tensor trace_decomp(const Tensor& self) {
  return at::sum(at::diagonal(self));
}
...
m.impl("trace", trace_decomp);
```

----------------------------------------

TITLE: Computing Jacobian of a Function using PyTorch
DESCRIPTION: This snippet demonstrates how to compute the Jacobian of a simple neural network function using the jacrev function from the torch.func module. It requires PyTorch to be installed and utilizes the torch.nn.Module and torch.randn for defining the model and input, respectively.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.api.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
model = torch.nn.Linear(3, 3)

def f(x):
    return model(x)

x = torch.randn(3)
jacobian = jacrev(f)(x)
assert jacobian.shape == (3, 3)
```

----------------------------------------

TITLE: Importing and Using torch.xpu Module in Python
DESCRIPTION: This snippet demonstrates how to import and use the torch.xpu module in Python. It includes various functions for device management, stream handling, and other XPU-specific operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/xpu.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
import torch.xpu

# Device management
torch.xpu.current_device()
torch.xpu.device_count()
torch.xpu.set_device(0)

# Stream handling
with torch.xpu.StreamContext():
    # XPU operations here
    pass

# Random number generation
torch.xpu.manual_seed(42)

# Memory management
torch.xpu.empty_cache()
torch.xpu.memory_allocated()
```

----------------------------------------

TITLE: Feature Concatenation Operations in DenseNet Dense Blocks
DESCRIPTION: This snippet shows the tensor concatenation operations used in DenseNet's dense blocks. DenseNet concatenates feature maps from previous layers along the channel dimension (dimension 1) to create dense connections.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
Operator: aten.cat.default
cnt: 1, (([T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16)], 1), {})
cnt: 1, (([T([32, 256, 28, 28], f16), T([32, 160, 28, 28], f16), T([32, 160, 28, 28], f16), T([32, 160, 28, 28], f16), T([32, 160, 28, 28], f16), T([32, 160, 28, 28], f16)], 1), {})
cnt: 1, (([T([32, 512, 14, 14], f16), T([32, 192, 14, 14], f16), T([32, 192, 14, 14], f16), T([32, 192, 14, 14], f16), T([32, 192, 14, 14], f16), T([32, 192, 14, 14], f16)], 1), {})
cnt: 1, (([T([32, 768, 14, 14], f16), T([32, 192, 14, 14], f16), T([32, 192, 14, 14], f16), T([32, 192, 14, 14], f16), T([32, 192, 14, 14], f16), T([32, 192, 14, 14], f16)], 1), {})
cnt: 1, (([T([32, 768, 7, 7], f16), T([32, 224, 7, 7], f16), T([32, 224, 7, 7], f16), T([32, 224, 7, 7], f16), T([32, 224, 7, 7], f16), T([32, 224, 7, 7], f16)], 1), {})
cnt: 1, (([T([32, 1024, 7, 7], f16), T([32, 224, 7, 7], f16), T([32, 224, 7, 7], f16), T([32, 224, 7, 7], f16), T([32, 224, 7, 7], f16), T([32, 224, 7, 7], f16)], 1), {})
```

----------------------------------------

TITLE: Calling NumPy in torch.autograd.Function for Composable torch.func Transforms (Python)
DESCRIPTION: Implements a torch.autograd.Function where forward and backward operations use NumPy for computation and are compatible with PyTorch's torch.func transforms due to proper implementation of forward, setup_context, and transformable backward methods. NumpySort saves intermediate indices as outputs for later use in backward, which relies on a helper NumpyTake function, itself implemented as a torch.autograd.Function. Dependencies: torch, numpy. Inputs: tensors and dimension integer. Outputs: sorted tensor and index intermediates. Limitations: assumes CPU compatibility and that inputs/outputs are convertible between torch and numpy as needed.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.func.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
import numpy as np

def to_numpy(tensor):
    return tensor.cpu().numpy()

class NumpySort(torch.autograd.Function):
    # Note that forward does not take ctx
    @staticmethod
    def forward(x, dim):
        device = x.device
        x = to_numpy(x)
        ind = np.argsort(x, axis=dim)
        ind_inv = np.argsort(ind, axis=dim)
        result = np.take_along_axis(x, ind, axis=dim)
        # Any intermediates to be saved in backward must be returned as
        # outputs.
        return (
            # The desired output
            torch.tensor(result, device=device),
            # intermediate to save for backward
            torch.tensor(ind, device=device),
            # intermediate to save for backward
            torch.tensor(ind_inv, device=device),
        )

    # setup_context is responsible for calling methods and/or assigning to
    # the ctx object. Please do not do additional compute (e.g. add
    # Tensors together) in setup_context.
    @staticmethod
    def setup_context(ctx, inputs, output):
        x, dim = inputs
        # Note that output is whatever you returned from forward.
        # If you returned multiple values, then output is a Tuple of multiple values.
        # If you returned a single Tensor, then output is a Tensor.
        # If you returned a Tuple with a single Tensor, then output is a
        # Tuple with a single Tensor.
        _, ind, ind_inv = output
        ctx.mark_non_differentiable(ind, ind_inv)
        # Tensors must be saved via ctx.save_for_backward. Please do not
        # assign them directly onto the ctx object.
        ctx.save_for_backward(ind, ind_inv)
        # Non-tensors may be saved by assigning them as attributes on the ctx object.
        ctx.dim = dim

    @staticmethod
    def backward(ctx, grad_output, _0, _1):
        # For the autograd.Function to be arbitrarily composable with function
        # transforms, all staticmethod other than forward and setup_context
        # must be implemented in a "transformable" way; that is, they must
        # only consist of PyTorch operations or autograd.Function.
        #
        # For example, this allows us to do double backwards and/or compute
        # second order gradients.
        #
        # We've written the backward pass of NumpySort in terms of another
        # autograd.Function, NumpyTake.
        ind, ind_inv = ctx.saved_tensors
        return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None

class NumpyTake(torch.autograd.Function):
    @staticmethod
    def forward(x, ind, ind_inv, dim):
        device = x.device
        x = to_numpy(x)
        ind = to_numpy(ind)
        return torch.tensor(np.take_along_axis(x, ind, dim), device=device)

    @staticmethod
    def setup_context(ctx, inputs, output):
        x, ind, ind_inv, dim = inputs
        ctx.save_for_backward(ind, ind_inv)
        ctx.dim = dim

    @staticmethod
    def backward(ctx, grad_output):
        ind, ind_inv = ctx.saved_tensors
        result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim)
        return result, None, None, None

```

LANGUAGE: python
CODE:
```
def numpy_sort(x, dim=-1):
    result, _, _ = NumpySort.apply(x, dim)
    return result
```

LANGUAGE: python
CODE:
```
x = torch.randn(2, 3)
grad_x = torch.func.grad(lambda x: numpy_sort(x).sum())(x)
assert torch.allclose(grad_x, torch.ones_like(x))
```

----------------------------------------

TITLE: Batch Normalization Backward Operations in PyTorch
DESCRIPTION: Records of backward passes for batch normalization operations with various tensor shapes. Each operation includes gradients, input, scale, bias, running statistics, and flags for computing different gradients.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/spnasnet_100_training.txt#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
Operator: aten.native_batch_norm_backward.default
cnt: 1, ((T([128, 1280, 7, 7], f16), T([128, 1280, 7, 7], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f32), T([1280], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 320, 7, 7], f16), T([128, 320, 7, 7], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f32), T([320], f32), True, 1e-05, [True, True, True]), {})
cnt: 8, ((T([128, 1152, 7, 7], f16), T([128, 1152, 7, 7], f16), T([1152], f16), T([1152], f16), T([1152], f16), T([1152], f32), T([1152], f32), True, 1e-05, [True, True, True]), {})
cnt: 4, ((T([128, 192, 7, 7], f16), T([128, 192, 7, 7], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 576, 7, 7], f16), T([128, 576, 7, 7], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f32), T([576], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 576, 14, 14], f16), T([128, 576, 14, 14], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f32), T([576], f32), True, 1e-05, [True, True, True]), {})
cnt: 4, ((T([128, 96, 14, 14], f16), T([128, 96, 14, 14], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), True, 1e-05, [True, True, True]), {})
cnt: 6, ((T([128, 288, 14, 14], f16), T([128, 288, 14, 14], f16), T([288], f16), T([288], f16), T([288], f16), T([288], f32), T([288], f32), True, 1e-05, [True, True, True]), {})
cnt: 2, ((T([128, 480, 14, 14], f16), T([128, 480, 14, 14], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f32), T([480], f32), True, 1e-05, [True, True, True]), {})
cnt: 4, ((T([128, 80, 14, 14], f16), T([128, 80, 14, 14], f16), T([80], f16), T([80], f16), T([80], f16), T([80], f32), T([80], f32), True, 1e-05, [True, True, True]), {})
cnt: 7, ((T([128, 240, 14, 14], f16), T([128, 240, 14, 14], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f32), T([240], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 240, 28, 28], f16), T([128, 240, 28, 28], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f32), T([240], f32), True, 1e-05, [True, True, True]), {})
cnt: 4, ((T([128, 40, 28, 28], f16), T([128, 40, 28, 28], f16), T([40], f16), T([40], f16), T([40], f16), T([40], f32), T([40], f32), True, 1e-05, [True, True, True]), {})
cnt: 6, ((T([128, 120, 28, 28], f16), T([128, 120, 28, 28], f16), T([120], f16), T([120], f16), T([120], f16), T([120], f32), T([120], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 144, 28, 28], f16), T([128, 144, 28, 28], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f32), T([144], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 144, 56, 56], f16), T([128, 144, 56, 56], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f32), T([144], f32), True, 1e-05, [True, True, True]), {})
cnt: 3, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f32), T([24], f32), True, 1e-05, [True, True, True]), {})
cnt: 4, ((T([128, 72, 56, 56], f16), T([128, 72, 56, 56], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f32), T([72], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 48, 56, 56], f16), T([128, 48, 56, 56], f16), T([48], f16), T([48], f16), T([48], f16), T([48], f32), T([48], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 48, 112, 112], f16), T([128, 48, 112, 112], f16), T([48], f16), T([48], f16), T([48], f16), T([48], f32), T([48], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f32), T([16], f32), True, 1e-05, [True, True, True]), {})
cnt: 2, ((T([128, 32, 112, 112], f16), T([128, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), True, 1e-05, [True, True, True]), {})
```

----------------------------------------

TITLE: Analyzing Tensor Operations in PyTorch
DESCRIPTION: This code snippet lists various tensor operations, their shapes, data types, and occurrence counts. It includes both regular tensor operations and the aten.threshold_backward.default operator with different tensor dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/botnet26t_256_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([128, 1000], f16), [0], True), {})
cnt: 1, ((T([512, 8, 8, 8, 8], f16, stride=(4096, 64, 1, 512, 8)), [2], True), {})
cnt: 1, ((T([512, 8, 8, 8, 8], f16, stride=(4096, 512, 8, 64, 1)), [2], True), {})
cnt: 2, ((T([512, 16, 16, 16, 16], f16, stride=(65536, 256, 1, 4096, 16)), [2], True), {})
cnt: 2, ((T([512, 16, 16, 16, 16], f16, stride=(65536, 4096, 16, 256, 1)), [2], True), {})
Operator: aten.threshold_backward.default
cnt: 2, ((T([128, 2048, 8, 8], f16), T([128, 2048, 8, 8], f16), 0), {})
cnt: 3, ((T([128, 512, 8, 8], f16), T([128, 512, 8, 8], f16), 0), {})
cnt: 1, ((T([128, 512, 16, 16], f16), T([128, 512, 16, 16], f16), 0), {})
cnt: 2, ((T([128, 1024, 16, 16], f16), T([128, 1024, 16, 16], f16), 0), {})
cnt: 3, ((T([128, 256, 16, 16], f16), T([128, 256, 16, 16], f16), 0), {})
cnt: 1, ((T([128, 256, 32, 32], f16), T([128, 256, 32, 32], f16), 0), {})
cnt: 2, ((T([128, 512, 32, 32], f16), T([128, 512, 32, 32], f16), 0), {})
cnt: 3, ((T([128, 128, 32, 32], f16), T([128, 128, 32, 32], f16), 0), {})
cnt: 1, ((T([128, 128, 64, 64], f16), T([128, 128, 64, 64], f16), 0), {})
cnt: 2, ((T([128, 256, 64, 64], f16), T([128, 256, 64, 64], f16), 0), {})
cnt: 4, ((T([128, 64, 64, 64], f16), T([128, 64, 64, 64], f16), 0), {})
cnt: 1, ((T([128, 64, 128, 128], f16), T([128, 64, 128, 128], f16), 0), {})
cnt: 1, ((T([128, 32, 128, 128], f16), T([128, 32, 128, 128], f16), 0), {})
cnt: 1, ((T([128, 24, 128, 128], f16), T([128, 24, 128, 128], f16), 0), {})
```

----------------------------------------

TITLE: Adding Sparse and Dense Tensors in PyTorch
DESCRIPTION: Demonstrates addition between sparse and dense tensors, showing how the result defaults to dense format.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
a + b.to_sparse()
```

----------------------------------------

TITLE: Applying Log Softmax - PyTorch Aten
DESCRIPTION: Applies the log-softmax function to a tensor along a specified dimension. This internal operator is commonly used in models requiring log-probabilities, such as cross-entropy loss calculations. It takes an input tensor, a dimension, and an optional boolean flag.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BartForConditionalGeneration_training.txt#_snippet_0

LANGUAGE: Python
CODE:
```
import torch

input_tensor = torch.randn(2048, 50265, dtype=torch.float16)
output = torch._log_softmax(input_tensor, dim=1, half_to_float=False)
```

----------------------------------------

TITLE: Analyzing Convolution Backward Operations in PyTorch
DESCRIPTION: Records of aten.convolution_backward.default operator calls for gradient computation. These operations compute gradients for convolution operations with various tensor shapes and convolution parameters, with flags indicating which gradients to compute.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_resnest_training.txt#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
Operator: aten.convolution_backward.default
cnt: 1, ((T([32, 2048, 7, 7], f16), T([32, 1024, 7, 7], f16), T([2048, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([32, 2048, 7, 7], f16), T([32, 512, 7, 7], f16), T([2048, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([32, 1024, 1, 1], f16), T([32, 256, 1, 1], f16), T([1024, 256, 1, 1], f16), [1024], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([32, 256, 1, 1], f16), T([32, 512, 1, 1], f16), T([256, 512, 1, 1], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([32, 1024, 14, 14], f16), T([32, 512, 14, 14], f16), T([1024, 256, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 2, [True, True, False]), {})
cnt: 1, ((T([32, 512, 14, 14], f16), T([32, 1024, 14, 14], f16), T([512, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([32, 1024, 14, 14], f16), T([32, 512, 14, 14], f16), T([1024, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([32, 1024, 14, 14], f16), T([32, 256, 14, 14], f16), T([1024, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([32, 512, 1, 1], f16), T([32, 128, 1, 1], f16), T([512, 128, 1, 1], f16), [512], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([32, 128, 1, 1], f16), T([32, 256, 1, 1], f16), T([128, 256, 1, 1], f16), [128], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([32, 512, 28, 28], f16), T([32, 256, 28, 28], f16), T([512, 128, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 2, [True, True, False]), {})
cnt: 1, ((T([32, 256, 28, 28], f16), T([32, 512, 28, 28], f16), T([256, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([32, 512, 28, 28], f16), T([32, 256, 28, 28], f16), T([512, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([32, 512, 28, 28], f16), T([32, 128, 28, 28], f16), T([512, 128, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([32, 256, 1, 1], f16), T([32, 64, 1, 1], f16), T([256, 64, 1, 1], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([32, 64, 1, 1], f16), T([32, 128, 1, 1], f16), T([64, 128, 1, 1], f16), [64], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([32, 256, 56, 56], f16), T([32, 128, 56, 56], f16), T([256, 64, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 2, [True, True, False]), {})
cnt: 1, ((T([32, 128, 56, 56], f16), T([32, 256, 56, 56], f16), T([128, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([32, 256, 56, 56], f16), T([32, 64, 56, 56], f16), T([256, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([32, 128, 1, 1], f16), T([32, 32, 1, 1], f16), T([128, 32, 1, 1], f16), [128], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([32, 32, 1, 1], f16), T([32, 64, 1, 1], f16), T([32, 64, 1, 1], f16), [32], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([32, 128, 56, 56], f16), T([32, 64, 56, 56], f16), T([128, 32, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 2, [True, True, False]), {})
cnt: 1, ((T([32, 64, 56, 56], f16), T([32, 64, 56, 56], f16), T([64, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([32, 64, 112, 112], f16), T([32, 32, 112, 112], f16), T([64, 32, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

----------------------------------------

TITLE: Tracking PyTorch Operator Usage Statistics
DESCRIPTION: A log of different PyTorch operators with their call counts, input tensor shapes, and data types. This section specifically tracks sigmoid, sigmoid_backward, slice_backward, sum, and threshold_backward operations with details about their tensor dimensions and parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
Operator: aten.sigmoid.default
cnt: 1, ((T([1024, 1], f16),), {})
Operator: aten.sigmoid_backward.default
cnt: 1, ((T([1024, 1], f16, stride=(0, 0)), T([1024, 1], f16)), {})
Operator: aten.slice_backward.default
cnt: 1, ((T([1024, 249, 249], f16), [1024, 249, 249], 0, 0, 9223372036854775807, 1), {})
Operator: aten.sum.SymInt
cnt: 1, ((T([1024, 1], f16), [0], True), {})
cnt: 9, ((T([1024, 4000], f16), [0], True), {})
cnt: 1, ((T([1024, 192], f16), [0], True), {})
cnt: 3, ((T([1024, 1500], f16), [0], True), {})
Operator: aten.sum.default
cnt: 1, ((T([1024, 1], f16),), {})
Operator: aten.threshold_backward.default
cnt: 9, ((T([1024, 4000], f16), T([1024, 4000], f16), 0), {})
cnt: 1, ((T([1024, 192], f16), T([1024, 192], f16), 0), {})
cnt: 3, ((T([1024, 1500], f16), T([1024, 1500], f16), 0), {})
```

----------------------------------------

TITLE: PyTorch Operator Usage Count for aten.clone.default Operations
DESCRIPTION: Lists tensor cloning operations across the network for various tensor shapes, used to create copies of tensors for operations that might modify the original data.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
Operator: aten.clone.default
cnt: 1, ((T([128, 3, 224, 224], f16),), {})
cnt: 3, ((T([128, 16, 112, 112], f16),), {})
cnt: 1, ((T([128, 64, 112, 112], f16),), {})
cnt: 1, ((T([128, 64, 56, 56], f16),), {})
cnt: 6, ((T([128, 48, 56, 56], f16),), {})
cnt: 1, ((T([128, 120, 56, 56], f16),), {})
cnt: 9, ((T([128, 120, 28, 28], f16),), {})
cnt: 1, ((T([128, 8, 1, 1], f16),), {})
cnt: 4, ((T([128, 16, 1, 1], f16),), {})
cnt: 1, ((T([128, 200, 28, 28], f16),), {})
cnt: 1, ((T([128, 200, 14, 14], f16),), {})
cnt: 8, ((T([128, 216, 14, 14], f16),), {})
cnt: 12, ((T([128, 360, 14, 14], f16),), {})
cnt: 1, ((T([128, 24, 1, 1], f16),), {})
cnt: 6, ((T([128, 32, 1, 1], f16),), {})
cnt: 1, ((T([128, 720, 14, 14], f16),), {})
cnt: 1, ((T([128, 720, 7, 7], f16),), {})
cnt: 10, ((T([128, 736, 7, 7], f16),), {})
cnt: 6, ((T([128, 48, 1, 1], f16),), {})
cnt: 2, ((T([128, 1104, 7, 7], f16),), {})
cnt: 1, ((T([128, 1344, 7, 7], f16),), {})
cnt: 1, ((T([128, 1984, 1, 1], f16),), {})
```

----------------------------------------

TITLE: Converting Between Real and Complex Representations
DESCRIPTION: Shows how to convert between real tensors and complex tensors using view_as_complex and view_as_real functions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/complex_numbers.rst#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
x = torch.randn(3, 2)
x
y = torch.view_as_complex(x)
y
torch.view_as_real(y)
```

----------------------------------------

TITLE: Creating Compressed Sparse Tensors in PyTorch
DESCRIPTION: Demonstrates creating CSR and CSC tensors using the same input data by specifying different layouts with sparse_compressed_tensor function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_22

LANGUAGE: Python
CODE:
```
compressed_indices = torch.tensor([0, 2, 4])
plain_indices = torch.tensor([0, 1, 0, 1])
values = torch.tensor([1, 2, 3, 4])
csr = torch.sparse_compressed_tensor(compressed_indices, plain_indices, values, layout=torch.sparse_csr)
csc = torch.sparse_compressed_tensor(compressed_indices, plain_indices, values, layout=torch.sparse_csc)
```

----------------------------------------

TITLE: Converting Dense to COO Sparse Tensor in PyTorch
DESCRIPTION: Demonstrates converting a 2D dense tensor to a COO (Coordinate Format) sparse tensor, storing only non-zero elements and their indices.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
a = torch.tensor([[0, 2.], [3, 0]])
a.to_sparse()
```

----------------------------------------

TITLE: Manipulating Tensor Storage in PyTorch
DESCRIPTION: Example demonstrating how to access and modify a tensor's underlying storage, including cloning storage, filling with zeros, and reassigning storage to a tensor. Shows the low-level relationship between tensors and their storage.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/storage.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
t = torch.ones(3)
s0 = t.untyped_storage()
s0
 0
 0
 128
 63
 0
 0
 128
 63
 0
 0
 128
 63
[torch.storage.UntypedStorage(device=cpu) of size 12]
s1 = s0.clone()
s1.fill_(0)
 0
 0
 0
 0
 0
 0
 0
 0
 0
 0
 0
 0
[torch.storage.UntypedStorage(device=cpu) of size 12]
# Fill the tensor with a zeroed storage
t.set_(s1, storage_offset=t.storage_offset(), stride=t.stride(), size=t.size())
tensor([0., 0., 0.])
```

----------------------------------------

TITLE: Creating a Stage Module
DESCRIPTION: This snippet demonstrates how to retrieve a model partition as a nn.Module using the Pipe object. The stage_mod can be utilized for creating an optimizer or loading checkpoints. No special prerequisites other than an initialized pipe object are needed.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.pipelining.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
stage_mod : nn.Module = pipe.get_stage_module(stage_idx)
```

----------------------------------------

TITLE: Dynamic Shape Recompilation Example
DESCRIPTION: Demonstrates how recompilations occur with dynamic shapes and fixed cache size limits.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
import torch

@torch.compile(dynamic=False)
def fn(x):
    return x + 1

for i in range(1, 10):
    fn(torch.ones(i))
```

----------------------------------------

TITLE: Shape Manipulations with Nested Jagged Tensors in PyTorch
DESCRIPTION: Demonstrates various shape manipulation operations on nested jagged tensors, including unsqueeze, unflatten, cat, stack, and transpose.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> a = torch.randn(2, 6)
>>> b = torch.randn(4, 6)
>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)
>>> nt.shape
torch.Size([2, j1, 6])
>>> nt.unsqueeze(-1).shape
torch.Size([2, j1, 6, 1])
>>> nt.unflatten(-1, [2, 3]).shape
torch.Size([2, j1, 2, 3])
>>> torch.cat([nt, nt], dim=2).shape
torch.Size([2, j1, 12])
>>> torch.stack([nt, nt], dim=2).shape
torch.Size([2, j1, 2, 6])
>>> nt.transpose(-1, -2).shape
torch.Size([2, 6, j1])
```

----------------------------------------

TITLE: Implementing Self-Deleting Temporary File with Pack/Unpack Hooks in PyTorch
DESCRIPTION: Example implementation of pack_hook and unpack_hook for tensor serialization with automatic temporary file cleanup. The hooks allow saving tensors to disk and loading them back during backward pass computation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/autograd.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
class SelfDeletingTempFile():
    def __init__(self):
        self.name = os.path.join(tmp_dir, str(uuid.uuid4()))

    def __del__(self):
        os.remove(self.name)

def pack_hook(tensor):
    temp_file = SelfDeletingTempFile()
    torch.save(tensor, temp_file.name)
    return temp_file

def unpack_hook(temp_file):
    return torch.load(temp_file.name)
```

----------------------------------------

TITLE: Creating a CSR Tensor in PyTorch from Explicit Components
DESCRIPTION: Demonstrates how to create a sparse CSR tensor by directly providing crow_indices, col_indices, and values tensors. The example creates a 2x2 sparse matrix with all elements defined, showing how to specify the dtype.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
crow_indices = torch.tensor([0, 2, 4])
col_indices = torch.tensor([0, 1, 0, 1])
values = torch.tensor([1, 2, 3, 4])
csr = torch.sparse_csr_tensor(crow_indices, col_indices, values, dtype=torch.float64)
csr
```

----------------------------------------

TITLE: Working with External Data in ATen
DESCRIPTION: Example of creating a Tensor from pre-allocated memory using torch::from_blob. This allows using existing data without copying, though the resulting tensor cannot be resized since ATen doesn't own the memory.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_basics.rst#2025-04-22_snippet_3

LANGUAGE: cpp
CODE:
```
float data[] = { 1, 2, 3,
                 4, 5, 6 };
torch::Tensor f = torch::from_blob(data, {2, 3});
```

----------------------------------------

TITLE: Graph Module Inspection Example
DESCRIPTION: Shows various methods to inspect and debug a traced GraphModule including printing code, graph structure, and tabular representation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
class M(torch.nn.Module):
    def forward(self, x, y):
        return x + y

m = M()
traced = symbolic_trace(m)

print(traced)
print(traced.graph)
traced.graph.print_tabular()
```

----------------------------------------

TITLE: Managing CUDA Streams on Same Device in PyTorch C++
DESCRIPTION: This example demonstrates how to acquire and set CUDA streams on the same device, using setCurrentCUDAStream to switch between a custom stream and the default stream.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_2

LANGUAGE: cpp
CODE:
```
// create a tensor on device 0
torch::Tensor tensor0 = torch::ones({2, 2}, torch::device(torch::kCUDA));
// get a new CUDA stream from CUDA stream pool on device 0
at::cuda::CUDAStream myStream = at::cuda::getStreamFromPool();
// set current CUDA stream from default stream to `myStream` on device 0
at::cuda::setCurrentCUDAStream(myStream);
// sum() on tensor0 uses `myStream` as current CUDA stream
tensor0.sum();

// get the default CUDA stream on device 0
at::cuda::CUDAStream defaultStream = at::cuda::getDefaultCUDAStream();
// set current CUDA stream back to default CUDA stream on device 0
at::cuda::setCurrentCUDAStream(defaultStream);
// sum() on tensor0 uses `defaultStream` as current CUDA stream
tensor0.sum();
```

----------------------------------------

TITLE: PyTorch Convolution Operations in Neural Network
DESCRIPTION: Shows various convolution operations in a neural network, including the initial layer that processes the input image and subsequent convolutional layers with different kernel sizes, strides, and channel dimensions across network stages.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convnext_base_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
Operator: aten.convolution.default
cnt: 1, ((T([32, 3, 224, 224], f16), T([128, 3, 4, 4], f16), T([128], f16), [4, 4], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([32, 128, 56, 56], f16, stride=(401408, 1, 7168, 128)), T([128, 1, 7, 7], f16), T([128], f16), [1, 1], [3, 3], [1, 1], False, [0, 0], 128), {})
cnt: 1, ((T([32, 128, 56, 56], f16, stride=(401408, 1, 7168, 128)), T([256, 128, 2, 2], f16), T([256], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([32, 256, 28, 28], f16, stride=(200704, 1, 7168, 256)), T([256, 1, 7, 7], f16), T([256], f16), [1, 1], [3, 3], [1, 1], False, [0, 0], 256), {})
cnt: 1, ((T([32, 256, 28, 28], f16, stride=(200704, 1, 7168, 256)), T([512, 256, 2, 2], f16), T([512], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 27, ((T([32, 512, 14, 14], f16, stride=(100352, 1, 7168, 512)), T([512, 1, 7, 7], f16), T([512], f16), [1, 1], [3, 3], [1, 1], False, [0, 0], 512), {})
```

----------------------------------------

TITLE: Specialized Shape-Based Control Flow in Exported PyTorch Program
DESCRIPTION: The exported program resulting from shape specialization, showing how conditional branches based on tensor shapes are removed during export. Only the branch matching the example input's shape is preserved in the final graph.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
ExportedProgram:
class GraphModule(torch.nn.Module):
    def forward(self, x: "f32[10, 2]"):
        # code: return x + 1
        add: "f32[10, 2]" = torch.ops.aten.add.Tensor(x, 1)
        return (add,)
```

----------------------------------------

TITLE: Disabling TF32 CuBLAS and CuDNN (C++)
DESCRIPTION: Provides the C++ code snippet necessary to programmatically disable TensorFloat32 usage in PyTorch's underlying CUDA backends (CuBLAS for matrix multiplication and CuDNN for convolutions) by interacting with the global context.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_4

LANGUAGE: cpp
CODE:
```
at::globalContext().setAllowTF32CuBLAS(false);
at::globalContext().setAllowTF32CuDNN(false);
```

----------------------------------------

TITLE: Using Constant ModuleList in TorchScript
DESCRIPTION: Example showing how to use nn.ModuleList in TorchScript by marking it as a constant with __constants__. This allows the module list to be used in for loops which will unroll at compile time.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_12

LANGUAGE: Python
CODE:
```
class SubModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(2))

    def forward(self, input):
        return self.weight + input

class MyModule(torch.nn.Module):
    __constants__ = ['mods']

    def __init__(self):
        super().__init__()
        self.mods = torch.nn.ModuleList([SubModule() for i in range(10)])

    def forward(self, v):
        for module in self.mods:
            v = module(v)
        return v


m = torch.jit.script(MyModule())
```

----------------------------------------

TITLE: Accessing Real and Imaginary Components
DESCRIPTION: Demonstrates accessing and modifying real and imaginary parts of complex tensors using real and imag attributes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/complex_numbers.rst#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
y.real
y.imag
y.real.mul_(2)
y
y.real.stride()
```

----------------------------------------

TITLE: Basic FX Graph Node Check Example
DESCRIPTION: Demonstrates checking if nodes in a graph module are div operations
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
for node in graph_module.graph.nodes:
    if node.op == "call_function" and node.target != torch.div:
        raise ValueError("Target should be div!")

pm.add_checks(check_div_target)

pm(graph_module)    # raises ValueError after replace_div_with_mul pass
```

----------------------------------------

TITLE: Integrating GoogleTest via FetchContent in CMake (CMake)
DESCRIPTION: Uses FetchContent to automatically download and configure the GoogleTest framework at a specified release version. Disables GMock, enables building GTest, and sets shared CRT usage to ensure consistent runtime linking in MSVC environments. This snippet makes C++ unit-testing setup reproducible and avoids system dependency issues. Requires an internet connection and CMake 3.11+ for FetchContent support.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/inductor/cpp/CMakeLists.txt#2025-04-22_snippet_2

LANGUAGE: CMake
CODE:
```
################################
# GTest
################################
project(googletest-git NONE)

include(FetchContent)
FetchContent_Declare(
  googletest
  GIT_REPOSITORY https://github.com/google/googletest.git
  GIT_TAG        release-1.12.1
)

set(gtest_force_shared_crt ON CACHE BOOL "" FORCE)
set(BUILD_GMOCK OFF CACHE BOOL "" FORCE)
set(BUILD_GTEST ON CACHE BOOL "" FORCE)

FetchContent_MakeAvailable(googletest)
```

----------------------------------------

TITLE: Performing Matrix Multiplication with ATen AddMM Operator
DESCRIPTION: The `aten.addmm` operation combines matrix multiplication with addition, crucial for neural network forward passes, especially in dense layers. Takes matrices of compatible dimensions like [64, 1280] and [1280, 1000].
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_6

LANGUAGE: plaintext
CODE:
```
Operator: aten.addmm.default
cnt: 1, ((T([1000], f16), T([64, 1280], f16), T([1280, 1000], f16, stride=(1, 1280))), {})
```

----------------------------------------

TITLE: Invoking aten.sigmoid.default Sigmoid Activation in PyTorch ATen
DESCRIPTION: Documents observed calls to the Sigmoid activation function (`aten.sigmoid.default`) in PyTorch ATen. It lists the different input tensor shapes (all using f16 data type) and their respective call counts during profiling.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_18

LANGUAGE: text
CODE:
```
Operator: aten.sigmoid.default
cnt: 1, ((T([128, 256, 1, 1], f16),), {})
cnt: 2, ((T([128, 512, 1, 1], f16),), {})
cnt: 9, ((T([128, 1536, 1, 1], f16),), {})
```

----------------------------------------

TITLE: Setting MPS Allocator Low Watermark Ratio (Environment Variable)
DESCRIPTION: Configures the low watermark ratio for the MPS allocator using `PYTORCH_MPS_LOW_WATERMARK_RATIO`. This is a soft limit used to trigger garbage collection or more frequent command buffer commits (adaptive commit). Default is 1.4 (unified memory) or 1.0 (discrete memory). Set to 0.0 to disable adaptive commit and garbage collection.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mps_environment_variables.rst#2025-04-22_snippet_4

LANGUAGE: plaintext
CODE:
```
PYTORCH_MPS_LOW_WATERMARK_RATIO
```

----------------------------------------

TITLE: Static Control Flow Example in PyTorch FX
DESCRIPTION: Shows successful symbolic tracing of static control flow where conditions depend on module parameters rather than inputs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
import torch
import torch.fx

class MyModule(torch.nn.Module):
    def __init__(self, do_activation : bool = False):
        super().__init__()
        self.do_activation = do_activation
        self.linear = torch.nn.Linear(512, 512)

    def forward(self, x):
        x = self.linear(x)
        if self.do_activation:
            x = torch.relu(x)
        return x
```

----------------------------------------

TITLE: Creating Sparse COO Tensor in PyTorch
DESCRIPTION: This code snippet demonstrates the creation of a sparse COO tensor using PyTorch's _sparse_coo_tensor_with_dims_and_tensors function. It specifies the tensor dimensions, indices, and values, along with additional parameters like data type and device.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
((1, 1, [965, 192], T([1, 54765], i64), T([54765, 192], f16)), {'dtype': f16, 'layout': torch.sparse_coo, 'device': 'cuda', 'pin_memory': None})
```

----------------------------------------

TITLE: L2 Gradient Penalty without AMP or GradScaler (Python)
DESCRIPTION: This snippet presents a reference implementation of L2 gradient penalty calculation during training without the use of automatic mixed precision or gradient scaling. It uses torch.autograd.grad to compute the gradients, sums their norms to add as a penalty to the loss, and performs a standard backward pass. This is useful for comparison or as a baseline when AMP is not required.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/amp_examples.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
for epoch in epochs:
    for input, target in data:
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)

        # Creates gradients
        grad_params = torch.autograd.grad(outputs=loss,
                                          inputs=model.parameters(),
                                          create_graph=True)

        # Computes the penalty term and adds it to the loss
        grad_norm = 0
        for grad in grad_params:
            grad_norm += grad.pow(2).sum()
        grad_norm = grad_norm.sqrt()
        loss = loss + grad_norm

        loss.backward()

        # clip gradients here, if desired

        optimizer.step()

```

----------------------------------------

TITLE: Subgraph Rewriting in PyTorch FX
DESCRIPTION: This example demonstrates how to use the subgraph rewriter in PyTorch FX to replace a pattern of operations with a single operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from torch.fx import subgraph_rewriter

def replace_patterns(graph_module):
    def pattern(x, y):
        x = torch.ops.aten.add.Tensor(x, y)
        x = torch.ops.aten.mul.Tensor(x, y)
        return x

    def replacement(x, y):
        return torch.ops.aten.sub.Tensor(x, y)

replaced_patterns = subgraph_rewriter.replace_pattern_with_filters(
    traced_module, pattern, replacement
)
```

----------------------------------------

TITLE: Matrix Multiplication with Dimensions in PyTorch
DESCRIPTION: Implements matrix multiplication using dimension objects and tensor operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
def mm(A, B):
    i, j, k = dims(3)
    r = (A[i, k] * B[k, j]).sum(k)
    return r.order(i, j)
mm(torch.rand(3, 4), torch.rand(4, 5)).shape
```

----------------------------------------

TITLE: Installing Compatible Numpy Version (pip)
DESCRIPTION: Installs a version of the numpy package that is less than 2.0. This command is provided as a solution for potential numpy incompatibility errors that might occur during the documentation build process. Requires pip.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_24

LANGUAGE: pip
CODE:
```
pip install 'numpy<2'
```

----------------------------------------

TITLE: Debugging Backend with Graph Printing
DESCRIPTION: Implements a debugging backend that prints the FX graph structure before execution.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_custom_backends.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from typing import List
import torch
def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):
    print("my_compiler() called with FX graph:")
    gm.graph.print_tabular()
    return gm.forward  # return a python callable
@torch.compile(backend=my_compiler)
def fn(x, y):
    a = torch.cos(x)
    b = torch.sin(y)
    return a + b
fn(torch.randn(10), torch.randn(10))
```

----------------------------------------

TITLE: Textual FX Graph with Placeholder, Call Function, and Output Nodes in PyTorch
DESCRIPTION: This snippet demonstrates the textual representation of an Export IR graph as output from torch.export, showing nodes of type placeholder (inputs), call_function (ATen operator, here add), and output (return value node). The format reveals node names, typing/usage information, operation types, ATen targets, argument lists, and graph output structure. The code is illustrative and is meant for visualization/debugging, not for direct execution; it expects the context of torch.export and torch.fx.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
graph():
  %x : [num_users=1] = placeholder[target=x]
  %y : [num_users=1] = placeholder[target=y]
  %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%x, %y), kwargs = {})
  return (add,)
```

----------------------------------------

TITLE: Generating Dummy Data and Creating Model Ensemble - PyTorch - Python
DESCRIPTION: Creates dummy image and label tensors mimicking an MNIST-like batch for an ensemble of models, specifying device as CUDA and constructing multiple instances of the SimpleMLP model. Requires a CUDA-enabled GPU and the previously defined SimpleMLP class. Outputs a data tensor of shape [100, 64, 1, 28, 28], a targets tensor, and a list of identical initialized models.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/ensembling.ipynb#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
device = 'cuda'
num_models = 10

data = torch.randn(100, 64, 1, 28, 28, device=device)
targets = torch.randint(10, (6400,), device=device)

models = [SimpleMLP().to(device) for _ in range(num_models)]
```

----------------------------------------

TITLE: Dynamic Shape Handling in FakeTensor
DESCRIPTION: Explains how FakeTensor handles dynamic shapes through ShapeEnv integration. The implementation manages data-dependent meta functions and memoization of unbacked SymInts for consistent symbolic size handling.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_fake_tensor.rst#2025-04-22_snippet_8



----------------------------------------

TITLE: Setting up MNIST Training Loop with PyTorch Lazy Tensor
DESCRIPTION: Configures the MNIST dataset, data loader, model, optimizer, and training loop using PyTorch's Lazy Tensor. The model is moved to the 'lazy' device for Lazy Tensor computation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import os
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import StepLR
import torch._lazy
import torch._lazy.ts_backend
import torch._lazy.metrics
torch._lazy.ts_backend.init()

if __name__  == '__main__':
    bsz = 64
    device = 'lazy'
    epochs = 14
    log_interval = 10
    lr = 1
    gamma = 0.7
    train_kwargs = {'batch_size': bsz}
    # if we want to use CUDA
    if "LTC_TS_CUDA" in os.environ:
        cuda_kwargs = {'num_workers': 1,
                       'pin_memory': True,
                       'shuffle': True,
                       'batch_size': bsz}
        train_kwargs.update(cuda_kwargs)

    transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
        ])
    dataset1 = datasets.MNIST('./data', train=True, download=True,
                        transform=transform)
    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)
    model = Net().to(device)
    optimizer = optim.Adadelta(model.parameters(), lr=lr)
    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)
    for epoch in range(1, epochs + 1):
        train(log_interval, model, device, train_loader, optimizer, epoch)
        scheduler.step()
```

----------------------------------------

TITLE: Print Compile Example
DESCRIPTION: Demonstration of using print_compile to obtain and display the FX graph for debugging purposes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/minifier.ipynb#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from functorch.compile import aot_function

from functorch.compile import print_compile
# Or...
def print_compile(fx_g, _):
    print(fx_g.code)
    return fx_g

def foo(x):
    return x.cos().cos()
inp = torch.randn(3, requires_grad=True)
aot_function(foo, print_compile)(inp)
```

----------------------------------------

TITLE: Tensor Copy Operation in PyTorch
DESCRIPTION: Copies the contents of one tensor to another with the same shape and data type.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
aten.copy_.default((T([64, 3, 224, 224], f16), T([64, 3, 224, 224], f16)), {})
```

----------------------------------------

TITLE: Custom SaliencyPruner Implementation
DESCRIPTION: Implementation of a custom pruner that extends BaseStructuredSparsifier. This pruner uses row-wise L1 norm saliency to determine which rows to prune, removing those with the smallest total magnitude.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/pruner/README.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
class SaliencyPruner(BaseStructuredSparsifier):
     """
     Prune filters based on the saliency
     The saliency for a filter is given by the sum of the L1 norms of all of its weights
     """

     def update_mask(self, module, tensor_name, **kwargs):
        # tensor_name will give you the FQN, all other keys in pruning config are present in kwargs
         weights = getattr(module, tensor_name)
         mask = getattr(module.parametrizations, tensor_name)[0].mask

         # use negative weights so we can use topk (we prune out the smallest)
         saliency = -weights.norm(dim=tuple(range(1, weights.dim())), p=1)
         num_to_pick = int(len(mask) * kwargs["sparsity_level"])
         prune = saliency.topk(num_to_pick).indices

         # Set the mask to be false for the rows we want to prune
         mask.data[prune] = False
```

----------------------------------------

TITLE: Enumerating PyTorch sum Operator Arguments for SymInt and IntList - Python
DESCRIPTION: These snippets detail structured argument lists for the 'sum' operator with both symbolic integer dimension and explicit integer list variants. They show float16 tensors with various shapes, axes or dimensions to sum over, and boolean keepdim flags. The SymInt version involves symbolic indices suitable for shape-inferred operations, while dim_IntList enumerates specific dimensions. Used for verifying reduction operations in testing or kernel dispatch. Relies on compatible input tensors and PyTorch sum operator.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/resnest101e_training.txt#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([32, 1000], f16), [0], True), {})
cnt: 2, ((T([32, 2, 512, 8, 8], f16), [3, 4], True), {})
cnt: 1, ((T([32, 2, 512, 16, 16], f16), [3, 4], True), {})
cnt: 22, ((T([32, 2, 256, 16, 16], f16), [3, 4], True), {})
cnt: 1, ((T([32, 2, 256, 32, 32], f16), [3, 4], True), {})
cnt: 3, ((T([32, 2, 128, 32, 32], f16), [3, 4], True), {})
cnt: 1, ((T([32, 2, 128, 64, 64], f16), [3, 4], True), {})
cnt: 3, ((T([32, 2, 64, 64, 64], f16), [3, 4], True), {})
Operator: aten.sum.dim_IntList
cnt: 6, ((T([32, 2, 64, 64, 64], f16), [1]), {})
cnt: 2, ((T([32, 2, 128, 64, 64], f16), [1]), {})
cnt: 6, ((T([32, 2, 128, 32, 32], f16), [1]), {})
cnt: 2, ((T([32, 2, 256, 32, 32], f16), [1]), {})
cnt: 44, ((T([32, 2, 256, 16, 16], f16), [1]), {})
cnt: 2, ((T([32, 2, 512, 16, 16], f16), [1]), {})
cnt: 4, ((T([32, 2, 512, 8, 8], f16), [1]), {})
```

----------------------------------------

TITLE: Adding Sparse COO Tensors in PyTorch
DESCRIPTION: This example shows how addition of sparse COO tensors is implemented by concatenating the indices and values tensors. It demonstrates the efficiency of operations on uncoalesced tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_12

LANGUAGE: Python
CODE:
```
>>> a = torch.sparse_coo_tensor([[1, 1]], [5, 6], (2,))
>>> b = torch.sparse_coo_tensor([[0, 0]], [7, 8], (2,))
>>> a + b
tensor(indices=tensor([[0, 0, 1, 1]]),
       values=tensor([7, 8, 5, 6]),
       size=(2,), nnz=4, layout=torch.sparse_coo)
```

----------------------------------------

TITLE: Implementing Custom Serialization for TorchScript Modules
DESCRIPTION: Example of implementing __getstate__ and __setstate__ methods for custom serialization behavior in a TorchScript module. These methods allow users to control how their class or module is pickled and unpickled.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/docs/serialization.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
class M(torch.nn.Module):
    def __init__(self) -> None:
        self.a = torch.rand(2, 3)
        self.b = torch.nn.Linear(10, 10)

    def __getstate__(self):
        # Compiler infers that this is a tuple of (Tensor, Linear)
        return (self.a, self.b)

    def __setstate__(self, state):
        # Don't need to annotate this, we know what type `state` is!
        self.a = state[0]
        self.b = state[1]
```

----------------------------------------

TITLE: Removing Detach Operations in PyTorch FX Graph
DESCRIPTION: This transformer removes torch.ops.aten.detach.default and torch.ops.aten.detach_copy.default operations from a PyTorch FX graph.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
class RemoveDetachPass(torch.fx.Transformer):
    def call_function(self, target, args, kwargs):
        if target not in (
            torch.ops.aten.detach.default,
            torch.ops.aten.detach_copy.default,
        ):
            return super().call_function(target, args, kwargs, meta)

        assert len(args) == 1
        return args[0]

transformed_graph_module = RemoveDetachPass(graph_module).transform()
```

----------------------------------------

TITLE: Named Tensors with Unnamed Dimensions
DESCRIPTION: Demonstrates the coexistence of named and unnamed dimensions within tensors, highlighting that unnamed dimensions are given 'None' as a default name.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/named_tensor.rst#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
imgs = torch.randn(1, 2, 2, 3 , names=(None, 'C', 'H', 'W'))
imgs.names
```

----------------------------------------

TITLE: Using isinstance with Any Type in TorchScript Function (Python)
DESCRIPTION: This snippet scripts a function taking an argument of type Any, prints it, and returns whether it is a torch.Tensor. It demonstrates use of runtime type checking within a TorchScripted function. Requires torch and typing.Any; expects any input, prints and outputs a boolean. The scripted function can be called with any type and will handle type checks via isinstance.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch
from typing import Any

def f(a:Any):
    print(a)
    return (isinstance(a, torch.Tensor))

ones = torch.ones([2])
m = torch.jit.script(f)
print(m(ones))
```

----------------------------------------

TITLE: Inspecting TorchScript-Compiled Code Using .code Attribute (Python)
DESCRIPTION: Demonstrates inspection of the Python-like compiled TorchScript code using the 'code' attribute of a scripted function. Requires PyTorch. The scripted function 'foo' applies a loop and conditional logic on a tensor. Printing 'foo.code' outputs the synthetic, pretty-printed representation of TorchScript's internal code for debugging and verification.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
@torch.jit.script\ndef foo(len):\n    # type: (int) -> torch.Tensor\n    rv = torch.zeros(3, 4)\n    for i in range(len):\n        if i < 10:\n            rv = rv - 1.0\n        else:\n            rv = rv + 1.0\n    return rv\n\nprint(foo.code)\n
```

----------------------------------------

TITLE: Loading a PyTorch Module in C++
DESCRIPTION: Shows how to load a serialized PyTorch module in C++.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_14

LANGUAGE: cpp
CODE:
```
>>> torch::jit::script::Module module;
>>> module = torch::jit::load('controlflowmodule_scripted.pt');
```

----------------------------------------

TITLE: Using Type-Annotated Empty Lists and Dicts in TorchScript Modules - PyTorch - Python
DESCRIPTION: This example shows how to create empty list and dict objects with explicit type annotations in a TorchScript-compatible nn.Module. It uses Python 3 type annotations and the typing module to declare a module that generates a list of (int, float) tuples and a dict mapping string to int in its forward method. The code highlights the need for type hints to disambiguate empty data structures. Requires torch, torch.nn, and typing modules.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import torch
import torch.nn as nn
from typing import Dict, List, Tuple

class EmptyDataStructures(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x: torch.Tensor) -> Tuple[List[Tuple[int, float]], Dict[str, int]]:
        # This annotates the list to be a `List[Tuple[int, float]]`
        my_list: List[Tuple[int, float]] = []
        for i in range(10):
            my_list.append((i, x.item()))

        my_dict: Dict[str, int] = {}
        return my_list, my_dict

x = torch.jit.script(EmptyDataStructures())
```

----------------------------------------

TITLE: Enabling Reduced-Precision Reduction for FP16/BF16 in Scaled Dot Product Attention in PyTorch (Python)
DESCRIPTION: This snippet enables reduced-precision reductions for Scaled Dot Product Attention (SDPA) when using FP16/BF16 tensors in PyTorch, by toggling a global backend setting. This is useful for improving speed when accuracy trade-offs are acceptable, or when performance is a higher priority. The key method is torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp, which accepts a boolean argument. Requires PyTorch with compatible CUDA hardware.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/numerical_accuracy.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp(True)
```

----------------------------------------

TITLE: Defining a Function with Dynamic Control Flow in Python
DESCRIPTION: This snippet defines a function 'add_two_maybe' that conditionally adds 2 to a tensor based on a boolean input. It demonstrates a case where jit.trace can fail due to dynamic control flow.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch

def add_two_maybe(t: torch.Tensor, maybe: torch.Tensor):
    if maybe:
        return t + 2
    return t
```

----------------------------------------

TITLE: PyTorch NLL Loss Forward Operation
DESCRIPTION: NLL Loss forward pass computation with f16 tensors, processing batches of 128 samples across 1000 classes with ignore index of -100.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/levit_128_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
((T([128, 1000], f16), T([128], i64), None, 1, -100), {})
```

----------------------------------------

TITLE: Cloning Tensors in PyTorch using ATen
DESCRIPTION: Clones the input tensor, creating a new tensor that has the same content but does not share memory with the input tensor. This operation requires a single tensor input of various shapes, such as [16, 3, 128, 128] or [16, 5], all of data type f16. Key characteristics are its ability to duplicate tensor data without shared memory, useful in computational graphs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
Operator: aten.clone.default
cnt: 1, ((T([16, 3, 128, 128], f16),), {})
cnt: 1, ((T([16, 5], f16),), {})
```

----------------------------------------

TITLE: Manually Configuring CMake Build Options on MacOS Bash
DESCRIPTION: Configures environment variables including CMAKE_PREFIX_PATH and compiler settings for macOS, runs the PyTorch setup script with '--cmake-only' to prepare CMake build files, and then suggests using ccmake or cmake-gui to interactively modify build options before compiling.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_18

LANGUAGE: Bash
CODE:
```
export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only
ccmake build  # or cmake-gui build
```

----------------------------------------

TITLE: TorchScript-based ONNX Export Memory Analysis
DESCRIPTION: Script demonstrating memory usage tracking for TorchScript-based ONNX export of a HighResNet model. Records memory allocation history and generates a snapshot for visualization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_dynamo_memory_usage.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch

from monai.networks.nets import (
    HighResNet,
)

torch.cuda.memory._record_memory_history()

model = HighResNet(
    spatial_dims=3, in_channels=1, out_channels=3, norm_type="batch"
).eval()

model = model.to("cuda")
data = torch.randn(30, 1, 48, 48, 48, dtype=torch.float32).to("cuda")

with torch.no_grad():
    onnx_program = torch.onnx.export(
        model,
        data,
        "torchscript_exporter_highresnet.onnx",
        dynamo=False,
    )

snapshot_name = "torchscript_exporter_example.pickle"
print(f"generate {snapshot_name}")

torch.cuda.memory._dump_snapshot(snapshot_name)
print("Export is done.")
```

----------------------------------------

TITLE: TorchScript LSTM Graph (After Fusion Optimization)
DESCRIPTION: Shows the TorchScript graph after post-derivative optimization, specifically kernel fusion (FuseGraph pass). Multiple operations (broadcast, additions, chunking, activations, element-wise ops) related to the gate calculations and final output are combined into a single `prim::FusionGroup` node, which can be compiled into an optimized kernel, typically for inference scenarios where gradients are not required.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_27

LANGUAGE: TorchScript
CODE:
```
graph(%x : Float(*, *),
      %hx : Float(*, *),
      %cx : Float(*, *),
      %w_ih : Float(*, *),
      %w_hh : Float(*, *),
      %b_ih : Float(*),
      %b_hh : Float(*)):
  %9 : Float(*, *) = aten::t(%w_ih)
  %10 : Float(*, *) = aten::mm(%x, %9)
  %11 : Float(*, *) = aten::t(%w_hh)
  %12 : Float(*, *) = aten::mm(%hx, %11)
  %77 : Tensor[] = prim::ListConstruct(%b_hh, %b_ih, %10, %12)
  %78 : Tensor[] = aten::broadcast_tensors(%77)
  %79 : Tensor, %80 : Tensor, %81 : Tensor, %82 : Tensor = prim::ListUnpack(%78)
  %hy : Float(*, *), %cy : Float(*, *) = prim::FusionGroup_0(%cx, %82, %81, %80, %79)
  %30 : (Float(*, *), Float(*, *)) = prim::TupleConstruct(%hy, %cy)
  return (%30);

with prim::FusionGroup_0 = graph(%13 : Float(*, *),
      %71 : Tensor,
      %76 : Tensor,
      %81 : Tensor,
      %86 : Tensor):
  %87 : Float(*, *), %88 : Float(*, *), %89 : Float(*, *), %90 : Float(*, *) = prim::ConstantChunk[chunks=4, dim=1](%86)
  %82 : Float(*, *), %83 : Float(*, *), %84 : Float(*, *), %85 : Float(*, *) = prim::ConstantChunk[chunks=4, dim=1](%81)
  %77 : Float(*, *), %78 : Float(*, *), %79 : Float(*, *), %80 : Float(*, *) = prim::ConstantChunk[chunks=4, dim=1](%76)
  %72 : Float(*, *), %73 : Float(*, *), %74 : Float(*, *), %75 : Float(*, *) = prim::ConstantChunk[chunks=4, dim=1](%71)
  %69 : int = prim::Constant[value=1]()
  %70 : Float(*, *) = aten::add(%77, %72, %69)
  %66 : Float(*, *) = aten::add(%78, %73, %69)
  %62 : Float(*, *) = aten::add(%79, %74, %69)
  %58 : Float(*, *) = aten::add(%80, %75, %69)
  %54 : Float(*, *) = aten::add(%70, %82, %69)
  %50 : Float(*, *) = aten::add(%66, %83, %69)
  %46 : Float(*, *) = aten::add(%62, %84, %69)
  %42 : Float(*, *) = aten::add(%58, %85, %69)
  %38 : Float(*, *) = aten::add(%54, %87, %69)
  %34 : Float(*, *) = aten::add(%50, %88, %69)
  %30 : Float(*, *) = aten::add(%46, %89, %69)
  %26 : Float(*, *) = aten::add(%42, %90, %69)
  %ingate : Float(*, *) = aten::sigmoid(%38)
  %forgetgate : Float(*, *) = aten::sigmoid(%34)
  %cellgate : Float(*, *) = aten::tanh(%30)
  %outgate : Float(*, *) = aten::sigmoid(%26)
  %14 : Float(*, *) = aten::mul(%forgetgate, %13)
  %11 : Float(*, *) = aten::mul(%ingate, %cellgate)
  %cy : Float(*, *) = aten::add(%14, %11, %69)
  %4 : Float(*, *) = aten::tanh(%cy)
  %hy : Float(*, *) = aten::mul(%outgate, %4)
  return (%hy, %cy)
```

----------------------------------------

TITLE: Automatic Batching with Dimensions in PyTorch
DESCRIPTION: Demonstrates automatic batching of unbatched code using dimension objects.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
batch_size, feature_size = 3, 5
weights = torch.randn(feature_size)

def model(feature_vec):
    # Very simple linear model with activation
    assert feature_vec.dim() == 1
    return feature_vec.dot(weights).relu()

examples = torch.randn(batch_size, feature_size)
batch = dims(1)
r = model(examples[batch])
print(r)
```

----------------------------------------

TITLE: Preserving Autograd History with torch.as_nested_tensor in Python
DESCRIPTION: This code showcases the use of `torch.nested.as_nested_tensor` to maintain autograd history while constructing nested tensors. It demonstrates backpropagation through the nested tensor and highlights memory copying involved in the process.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> a = torch.randn(12, 512, requires_grad=True)
>>> b = torch.randn(23, 512, requires_grad=True)
>>> nt = torch.nested.as_nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)
>>> nt.sum().backward()
>>> a.grad
tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]])
>>> b.grad
tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        ...,
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.],
        [1., 1., 1.,  ..., 1., 1., 1.]])
```

----------------------------------------

TITLE: Creating Batched CSR Sparse Tensor in PyTorch
DESCRIPTION: Shows creation of a 3D (batched) CSR (Compressed Sparse Row) tensor from a 3D dense tensor, demonstrating batch dimension support.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
t = torch.tensor([[[1., 0], [2., 3.]], [[4., 0], [5., 6.]]])
t.dim()
t.to_sparse_csr()
```

----------------------------------------

TITLE: Creating Empty Strided Tensors in PyTorch
DESCRIPTION: This snippet creates empty strided tensors with specified shapes and strides. It's used for initializing tensors with custom memory layouts, which can be beneficial for certain operations or hardware optimizations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_11

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([8, 3, 48, 64, 85], f16, stride=(0, 0, 0, 0, 0)), [8, 3, 48, 64, 85], [783360, 261120, 5440, 85, 1]), {})
cnt: 4, ((T([8, 3, 48, 64, 85], f16), [8, 3, 48, 64, 85], [783360, 261120, 5440, 85, 1]), {})
cnt: 1, ((T([8, 3, 24, 32, 85], f16, stride=(0, 0, 0, 0, 0)), [8, 3, 24, 32, 85], [195840, 65280, 2720, 85, 1]), {})
cnt: 4, ((T([8, 3, 24, 32, 85], f16), [8, 3, 24, 32, 85], [195840, 65280, 2720, 85, 1]), {})
cnt: 1, ((T([8, 3, 12, 16, 85], f16, stride=(0, 0, 0, 0, 0)), [8, 3, 12, 16, 85], [48960, 16320, 1360, 85, 1]), {})
cnt: 4, ((T([8, 3, 12, 16, 85], f16), [8, 3, 12, 16, 85], [48960, 16320, 1360, 85, 1]), {})
```

----------------------------------------

TITLE: Analyzing Threshold Backward Operation in PyTorch
DESCRIPTION: This code snippet shows the usage of the threshold_backward operation in PyTorch. It includes the input tensor shapes and the threshold value used for each operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v3_large_training.txt#2025-04-22_snippet_11

LANGUAGE: Python
CODE:
```
Operator: aten.threshold_backward.default
cnt: 2, ((T([32, 240, 1, 1], f16), T([32, 240, 1, 1], f16), 0), {})
cnt: 2, ((T([32, 168, 1, 1], f16), T([32, 168, 1, 1], f16), 0), {})
cnt: 1, ((T([32, 120, 1, 1], f16), T([32, 120, 1, 1], f16), 0), {})
cnt: 2, ((T([32, 32, 1, 1], f16), T([32, 32, 1, 1], f16), 0), {})
cnt: 4, ((T([32, 120, 28, 28], f16), T([32, 120, 28, 28], f16), 0), {})
cnt: 1, ((T([32, 24, 1, 1], f16), T([32, 24, 1, 1], f16), 0), {})
cnt: 1, ((T([32, 72, 28, 28], f16), T([32, 72, 28, 28], f16), 0), {})
cnt: 3, ((T([32, 72, 56, 56], f16), T([32, 72, 56, 56], f16), 0), {})
cnt: 1, ((T([32, 64, 56, 56], f16), T([32, 64, 56, 56], f16), 0), {})
cnt: 1, ((T([32, 64, 112, 112], f16), T([32, 64, 112, 112], f16), 0), {})
cnt: 1, ((T([32, 16, 112, 112], f16), T([32, 16, 112, 112], f16), 0), {})
```

----------------------------------------

TITLE: PyTorch Operator Usage Statistics
DESCRIPTION: Comprehensive listing of PyTorch operators showing call counts and tensor configurations. Includes operations like softmax, matrix multiplication, embedding, and layer normalization commonly used in transformer architectures. Each entry shows operator name, call count, input tensor shapes, dtypes and additional parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_Albert_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
Operator: aten._softmax.default
cnt: 12, ((T([8, 12, 512, 512], f16), -1, False), {})

Operator: aten._softmax_backward_data.default
cnt: 12, ((T([8, 12, 512, 512], f16), T([8, 12, 512, 512], f16), -1, f16), {})

Operator: aten._to_copy.default
cnt: 1, ((T([8, 1, 1, 512], f32),), {'dtype': f16})

Operator: aten._unsafe_view.default
cnt: 36, ((T([8, 12, 512, 64], f16), [96, 512, 64]), {})
cnt: 12, ((T([8, 12, 64, 512], f16), [96, 64, 512]), {})
cnt: 12, ((T([96, 512, 512], f16), [8, 12, 512, 512]), {})
cnt: 12, ((T([96, 512, 64], f16), [8, 12, 512, 64]), {})
cnt: 36, ((T([8, 512, 12, 64], f16), [8, 512, 768]), {})
cnt: 12, ((T([8, 512, 768], f16), [4096, 768]), {})
```

----------------------------------------

TITLE: Applying Select Operation to MaskedTensor in PyTorch
DESCRIPTION: This snippet demonstrates how the `select` operation behaves when applied to a MaskedTensor. It shows the creation of a tensor and a mask, combining them into a MaskedTensor, and then applying `select` to the original data, the mask, and the resulting MaskedTensor to illustrate that the operation is applied to both components.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/masked.rst#_snippet_0

LANGUAGE: Python
CODE:
```
data = torch.arange(12, dtype=torch.float).reshape(3, 4)
mask = torch.tensor([[True, False, False, True], [False, True, False, False], [True, True, True, True]])
mt = masked_tensor(data, mask)
data.select(0, 1)
mask.select(0, 1)
mt.select(0, 1)
```

----------------------------------------

TITLE: Solving Tensor Puzzlers with First-Class Dimensions in Python
DESCRIPTION: This section includes multiple snippets demonstrating how to solve various tensor puzzles using first-class dimensions in PyTorch. It covers operations like outer product, diagonal vector, identity matrix, upper triangular matrix, running difference, vector stacking, circular shift, and sequence masking.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_21

LANGUAGE: python
CODE:
```
def outer(a, b):
    i, j = dims(2)
    return (a[i] * b[j]).order(i, j)

def diag(a):
    i = dims(1)
    return a[i, i].order(i)

from torch import where
def eye(j: int):
    i,j = dims(sizes=[j, j])
    return where(i == j, 1, 0).order(i, j)

def triu(j: int):
    i,j = dims(sizes=[j, j])
    return where(i <= j, 1, 0).order(i, j)

def diff(a, i: int):
    i = dims(1)
    d = a[i] - a[i - 1]
    return where(i - 1 >= 0, d, a[i]).order(i)

def vstack(a, b):
    v, i = dims(sizes=[2, None])
    return where(v == 0,  a[i], b[i]).order(v, i)

def roll(a, i: int):
    i = dims(sizes=[a.size(0)])
    return a[where(i + 1 < i.size, i + 1, 0)].order(i)

def flip(a, i: int):
    i = dims(sizes=[a.size(0)])
    return a[i.size - i - 1].order(i)

def sequence_mask(values, length):
    j, i = dims()
    v = values[i, j]
    return where(j < length[i], v, 0).order(i, j)
```

----------------------------------------

TITLE: Using CPU Tensor Accessors in C++
DESCRIPTION: Example of creating and using CPU tensor accessors for efficient element-wise access. The accessor provides a type-safe view of tensor data with simplified indexing, ideal for operations where performance is critical.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_basics.rst#2025-04-22_snippet_1

LANGUAGE: cpp
CODE:
```
torch::Tensor foo = torch::rand({12, 12});

// assert foo is 2-dimensional and holds floats.
auto foo_a = foo.accessor<float,2>();
float trace = 0;

for(int i = 0; i < foo_a.size(0); i++) {
  // use the accessor foo_a to get tensor data.
  trace += foo_a[i][i];
}
```

----------------------------------------

TITLE: Cleaning PyTorch Build After Ninja Install (Bash)
DESCRIPTION: Runs `python setup.py clean` to clear the existing build system configuration. This step is necessary after installing Ninja to ensure that subsequent builds correctly use Ninja instead of the previous make configuration.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_43

LANGUAGE: bash
CODE:
```
python setup.py clean
```

----------------------------------------

TITLE: Performing Single-GPU Offline GEMM Tuning with PyTorch (Python)
DESCRIPTION: Python script for the second step of offline tuning on a single GPU. It programmatically sets environment variables to enable TunableOp (`PYTORCH_TUNABLEOP_ENABLED=1`) and tuning (`PYTORCH_TUNABLEOP_TUNING=1`), disables further recording of untuned ops (`PYTORCH_TUNABLEOP_RECORD_UNTUNED=0`), and then calls `torch.cuda.tunable.tune_gemm_in_file` to read untuned GEMM configurations from the specified input CSV (`tunableop_untuned0.csv`), perform the tuning benchmarks, and write the results to an output CSV (`tunableop_results0.csv`).
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/cuda/tunable/README.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch.cuda.tunable as tunable
import os

os.putenv('PYTORCH_TUNABLEOP_ENABLED', '1')
os.putenv('PYTORCH_TUNABLEOP_TUNING', '1')
os.putenv('PYTORCH_TUNABLEOP_RECORD_UNTUNED', '0')
tunable.tune_gemm_in_file("tunableop_untuned0.csv")
```

----------------------------------------

TITLE: Implementing PyTorch Convolution Operations in Python
DESCRIPTION: This section describes the use of the 'aten.convolution.default' operator to apply convolution operations on tensors with varying strides, paddings, and kernel sizes. It supports neural network layers for feature extraction by altering the spatial dimensions of input tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnext50_32x4d_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([8, 3, 224, 224], f16), T([64, 3, 7, 7], f16), None, [2, 2], [3, 3], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([8, 64, 56, 56], f16), T([128, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([8, 128, 56, 56], f16), T([128, 4, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 32), {})
cnt: 3, ((T([8, 128, 56, 56], f16), T([256, 128, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

----------------------------------------

TITLE: NCCL Integration with CUDAGraph Trees in PyTorch
DESCRIPTION: Example of a function using NCCL operators with torch.compile in "reduce-overhead" mode for cross-device communication within CUDAGraph Trees.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_cudagraph_trees.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
@torch.compile(mode="reduce-overhead")
def func(x):
    y = x * x
    y = torch.distributed.all_reduce(y, op=torch.distributed.ReduceOp.SUM)
    x = torch.nn.functional.silu(x)
    return x * y
```

----------------------------------------

TITLE: Accessing Tensor Elements using index() in C++
DESCRIPTION: This snippet demonstrates how to use the torch::Tensor::index method to access tensor elements in C++, equivalent to using square brackets in Python.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_indexing.rst#2025-04-22_snippet_0

LANGUAGE: C++
CODE:
```
tensor.index({None})
```

----------------------------------------

TITLE: Autograd Function Example for Inlining
DESCRIPTION: Defines a MyLogExp operation using torch.autograd.Function. This function applies exponential and then two logarithm operations. It is provided as an example where no custom symbolic is defined, forcing the exporter to inline its operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_23

LANGUAGE: Python
CODE:
```
class MyLogExp(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input: torch.Tensor) -> torch.Tensor:
        ctx.save_for_backward(input)
        h = input.exp()
        return h.log().log()
```

----------------------------------------

TITLE: Wrapping Constants with Tensors Example
DESCRIPTION: Shows how to avoid recompilations by wrapping float constants with tensors in optimizer configurations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
import torch

mod = torch.nn.Linear(3, 3)
opt = torch.optim.Adam(mod.parameters(), lr=torch.tensor(0.01))
sched = torch.optim.lr_scheduler.ExponentialLR(opt, torch.tensor(0.9))
```

----------------------------------------

TITLE: Checking Unfused Operations in Python
DESCRIPTION: This command enables detailed logging to show which operations are not fused and why, using the partition.cpp GRAPH_UPDATE feature.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/cuda/README.md#2025-04-22_snippet_5

LANGUAGE: Bash
CODE:
```
PYTORCH_JIT_LOG_LEVEL=">partition:graph_fuser" python your_script.py &> log
```

----------------------------------------

TITLE: Initializing PostTrainingDataSparsity Callback in Python
DESCRIPTION: Example of creating a PostTrainingDataSparsity callback that sparsifies model parameters after training. It configures a DataNormSparsifier with specific sparsity levels and block parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/lightning/callbacks/README.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from data_sparsity import PostTrainingDataSparsity
sparsifier_args = {
    'sparsity_level': 0.5,
    'sparse_block_shape': (1, 4),
    'zeros_per_block': 4
}
pt_callback = PostTrainingDataSparsity(data_sparsifier_class=DataNormSparsifier, data_sparsifier_args=sparsifier_args)
```

----------------------------------------

TITLE: Recording Native Batch Norm Backward Calls (aten.native_batch_norm_backward) - PyTorch - Python
DESCRIPTION: This snippet encodes input configurations for aten.native_batch_norm_backward, used in gradient computations of batch normalization, including input/output tensors, running mean/var, scale/bias, training flags, epsilon, and mask flags. All shapes and types (including mixed precision) are described, facilitating the study of backward op scheduling and resource usage.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_14

LANGUAGE: Python
CODE:
```
Operator: aten.native_batch_norm_backward.default
cnt: 1, ((T([1, 2304, 1536], f16), T([1, 2304, 1536], f16), T([2304], f16), None, None, T([2304], f32), T([2304], f32), True, 1e-05, [True, True, False]), {})
cnt: 9, ((T([1, 1536, 384], f16), T([1, 1536, 384], f16), T([1536], f16), None, None, T([1536], f32), T([1536], f32), True, 1e-05, [True, True, False]), {})
cnt: 18, ((T([1, 384, 576], f16), T([1, 384, 576], f16), T([384], f16), None, None, T([384], f32), T([384], f32), True, 1e-05, [True, True, False]), {})
cnt: 8, ((T([1, 384, 1536], f16), T([1, 384, 1536], f16), T([384], f16), None, None, T([384], f32), T([384], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 1536, 1536], f16), T([1, 1536, 1536], f16), T([1536], f16), None, None, T([1536], f32), T([1536], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 384, 512], f16), T([1, 384, 512], f16), T([384], f16), None, None, T([384], f32), T([384], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 1536, 512], f16), T([1, 1536, 512], f16), T([1536], f16), None, None, T([1536], f32), T([1536], f32), True, 1e-05, [True, True, False]), {})
cnt: 2, ((T([1, 512, 128], f16), T([1, 512, 128], f16), T([512], f16), None, None, T([512], f32), T([512], f32), True, 1e-05, [True, True, False]), {})
cnt: 5, ((T([1, 128, 576], f16), T([1, 128, 576], f16), T([128], f16), None, None, T([128], f32), T([128], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 128, 512], f16), T([1, 128, 512], f16), T([128], f16), None, None, T([128], f32), T([128], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 128, 256], f16), T([1, 128, 256], f16), T([128], f16), None, None, T([128], f32), T([128], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 512, 256], f16), T([1, 512, 256], f16), T([512], f16), None, None, T([512], f32), T([512], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 256, 64], f16), T([1, 256, 64], f16), T([256], f16), None, None, T([256], f32), T([256], f32), True, 1e-05, [True, True, False]), {})
cnt: 2, ((T([1, 64, 576], f16), T([1, 64, 576], f16), T([64], f16), None, None, T([64], f32), T([64], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 64, 128], f16), T([1, 64, 128], f16), T([64], f16), None, None, T([64], f32), T([64], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 256, 128], f16), T([1, 256, 128], f16), T([256], f16), None, None, T([256], f32), T([256], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 64, 288], f16), T([1, 64, 288], f16), T([64], f16), None, None, T([64], f32), T([64], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 32, 144], f16), T([1, 32, 144], f16), T([32], f16), None, None, T([32], f32), T([32], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 16, 27], f16), T([1, 16, 27], f16), T([16], f16), None, None, T([16], f32), T([16], f32), True, 1e-05, [True, True, False]), {})
```

----------------------------------------

TITLE: Installing Linux Dependencies (MKL) Bash
DESCRIPTION: Installs Intel MKL static and include files required for optimizing certain numerical operations on Linux systems when building PyTorch from source.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_4

LANGUAGE: Bash
CODE:
```
pip install mkl-static mkl-include
```

----------------------------------------

TITLE: Collating Data with PyTorch DataPipes
DESCRIPTION: Demonstrates the use of the collate() method to process batched data in DataPipes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(10).batch(3).collate()
for i in dp:
    print(i)
```

----------------------------------------

TITLE: Exporting a TorchScript Function Handling Any in Tuple (Python)
DESCRIPTION: This snippet demonstrates creating and scripting a function with @torch.jit.export that takes a tuple where the second element has the Any type, returning both the incremented first element and the second element unchanged. It illustrates how the Any type allows passing arbitrary types in the parameter tuple. Requires torch and typing.Any, expects tuple inputs; output is a tuple of incremented int and unmodified Any. This function can be scripted and invoked with different types bound to Any.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch

from typing import Tuple
from typing import Any

@torch.jit.export
def inc_first_element(x: Tuple[int, Any]):
    return (x[0]+1, x[1])

m = torch.jit.script(inc_first_element)
print(m((1,2.0)))
print(m((1,(100,200))))
```

----------------------------------------

TITLE: Inserting ReLU After Add in PyTorch FX Graph
DESCRIPTION: This code inserts a torch.ops.aten.relu.default call after each torch.ops.aten.add.Tensor call in a PyTorch FX graph.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch

def insert_relu_after_add(gm: torch.fx.GraphModule) -> torch.fx.GraphModule:
    for node in gm.graph.nodes:
        if node.op == "call_function" and node.target == torch.ops.aten.add.Tensor:

            # Specifies the insertion point. Any nodes added to the graph within
            # this scope will be inserted after `node`
            with gm.graph.inserting_after(node):
                # Insert a new `call_function` node with op `torch.ops.aten.relu.default`
                new_relu_node = gm.graph.call_function(torch.ops.aten.relu.default, args=(node,))
                # Replace all the places that use `node` to now use the `new_relu_node`
                node.replace_all_uses_with(new_relu_node)
```

----------------------------------------

TITLE: Specifying TorchScript Types With Python 3 Type Hints - PyTorch - Python
DESCRIPTION: Here, standard Python 3 type hints from the typing module are used to annotate the types in a TorchScript function. The function foo is decorated with @torch.jit.script, specifying types for its int and tuple-of-tensor arguments and a tensor return type. This approach is compatible with newer versions of Python and is preferred for clearer type annotations. Dependencies include torch and typing, with the function expecting an int and a tuple of two tensors and returning a tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import torch
from typing import Tuple

@torch.jit.script
def foo(x: int, tup: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:
    t0, t1 = tup
    return t0 + t1 + x

print(foo(3, (torch.rand(3), torch.rand(3))))
```

----------------------------------------

TITLE: In-place Sigmoid Activation in PyTorch
DESCRIPTION: This snippet applies the sigmoid activation function in-place, modifying the input tensor directly. This can be more memory-efficient as it doesn't create a new tensor for the result.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_14

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([8, 3, 12, 16, 81], f16, stride=(48960, 16320, 1360, 85, 1)),), {})
cnt: 1, ((T([8, 3, 24, 32, 81], f16, stride=(195840, 65280, 2720, 85, 1)),), {})
cnt: 1, ((T([8, 3, 48, 64, 81], f16, stride=(783360, 261120, 5440, 85, 1)),), {})
```

----------------------------------------

TITLE: Creating Asynchronous Tasks with torch.jit.fork() in TorchScript
DESCRIPTION: Creates an asynchronous task executing func and a reference to the value of the result of this execution. Fork will return immediately.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_36

LANGUAGE: python
CODE:
```
torch.jit.fork()
```

----------------------------------------

TITLE: Incorrect Use of torch.zeros in batched function (vmap-incompatible) - PyTorch - Python
DESCRIPTION: This Python example shows how using global tensor factory functions (like torch.zeros) inside a function intended for vmap can produce shape mismatches and errors. In diag_embed, result is initialized with torch.zeros, which fails under vmap because the underlying shape may not match batched inputs. Dependencies: PyTorch, torch.func. Limitation: Factory functions should generally be replaced with batched equivalents when using vmap.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
def diag_embed(vec):
  assert vec.dim() == 1
  result = torch.zeros(vec.shape[0], vec.shape[0])
  result.diagonal().copy_(vec)
  return result

vecs = torch.tensor([[0., 1, 2], [3., 4, 5]])

# RuntimeError: vmap: inplace arithmetic(self, *extra_args) is not possible ...
vmap(diag_embed)(vecs)
```

----------------------------------------

TITLE: Implementing Parallel Execution with fork/wait (Python)
DESCRIPTION: Shows the TorchScript primitives `torch.jit.fork` and `torch.jit.wait` for achieving user-specified parallel execution. `fork` runs a function asynchronously and returns a Future object, while `wait` blocks the calling thread until the Future's computed value is available.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_18

LANGUAGE: python
CODE:
```
def fn(arg0, arg1, ...):
  ...
  return v

fut = torch.jit.fork(fn, arg0, arg1, ...)
...
v = torch.jit.wait(fut)
```

----------------------------------------

TITLE: Logging aten.scatter_add.default Operation Arguments
DESCRIPTION: These log entries record calls to the `aten.scatter_add.default` PyTorch operator. Each line shows the count (`cnt`) for a specific combination of arguments. The arguments tuple `((self, dim, index, src), {})` includes the target tensor (`T([965], f16)`), the dimension (`0`), the index tensor (`T([...], i64)`), and the source tensor (`T([...], f16)`), followed by empty keyword arguments `{}`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_16

LANGUAGE: plaintext
CODE:
```
Operator: aten.scatter_add.default
cnt: 1, ((T([965], f16), 0, T([54765], i64), T([54765], f16)), {})
cnt: 2, ((T([965], f16), 0, T([54704], i64), T([54704], f16)), {})
cnt: 4, ((T([965], f16), 0, T([54786], i64), T([54786], f16)), {})
cnt: 2, ((T([965], f16), 0, T([54804], i64), T([54804], f16)), {})
cnt: 3, ((T([965], f16), 0, T([54757], i64), T([54757], f16)), {})
# ... (many similar lines omitted for brevity)
```

----------------------------------------

TITLE: Creating Sparse Hybrid COO Tensor in PyTorch
DESCRIPTION: Demonstrates the creation of a sparse hybrid COO tensor with tensor values instead of scalar values.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_10

LANGUAGE: Python
CODE:
```
i = [[0, 1, 1],
     [2, 0, 2]]
v =  [[3, 4], [5, 6], [7, 8]]
s = torch.sparse_coo_tensor(i, v, (2, 3, 2))
s.to_dense()
```

----------------------------------------

TITLE: Type Annotation with torch.jit.annotate() in TorchScript
DESCRIPTION: Provides a type hint to TorchScript where Python 3 style type hints do not work well. Used for annotating types of expressions like empty lists.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_38

LANGUAGE: python
CODE:
```
torch.jit.annotate(List[int], [])
```

----------------------------------------

TITLE: Setting Tensor Elements using index_put_() in C++
DESCRIPTION: This snippet shows how to use the torch::Tensor::index_put_ method to set tensor elements in C++, equivalent to assignment with indexing in Python.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_indexing.rst#2025-04-22_snippet_1

LANGUAGE: C++
CODE:
```
tensor.index_put_({None}, 1)
```

----------------------------------------

TITLE: Chaining Multiple PyTorch Operator Registrations (C++)
DESCRIPTION: Illustrates how to register multiple operators or kernels within a single `torch::RegisterOperators` instance by chaining calls to the `.op()` method. This example registers two different operators, `my_namespace::my_op_1` and `my_namespace::my_op_2`, each with a CPU kernel.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_5

LANGUAGE: cpp
CODE:
```
static auto registry = torch::RegisterOperators()
    .op("my_namespace::my_op_1", torch::RegisterOperators::options()
        .kernel<MyKernel1>(CPU()))
    .op("my_namespace::my_op_2", torch::RegisterOperators::options()
        .kernel<MyKernel2>(CPU()));
```

----------------------------------------

TITLE: Relative Benchmark for jacfwd vs jacrev (Taller Matrix) - Python
DESCRIPTION: Runs get_perf to compare jacfwd versus jacrev timings in the tall matrix scenario. Requires previous benchmarks and get_perf. Prints percentage improvement between methods.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
get_perf(jacfwd_timing, "jacfwd", jacrev_timing, "jacrev", );
```

----------------------------------------

TITLE: Dimension Unbinding and Transposition in PyTorch
DESCRIPTION: Examples showing how to unbind dimensions and perform transpositions using the order method.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
i, j = dims(2)
A = torch.rand(3, 4)
A_T = A[i, j].order(j, i)
assert torch.allclose(A.T, A_T)
```

----------------------------------------

TITLE: Conditional Compilation for CUDA/ROCm in CMake
DESCRIPTION: Configures the `test_lazy` target based on the available GPU backend. If `USE_CUDA` is true, it adds the `USE_CUDA` compile definition. If `USE_ROCM` is true, it links ROCm-specific libraries (`hiprtc`, `amdhip64`, `${TORCH_CUDA_LIBRARIES}`) and adds the `USE_ROCM` compile definition.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/lazy/CMakeLists.txt#2025-04-22_snippet_7

LANGUAGE: cmake
CODE:
```
if(USE_CUDA)
  target_compile_definitions(test_lazy PRIVATE USE_CUDA)
elseif(USE_ROCM)
  target_link_libraries(test_lazy PRIVATE
    hiprtc::hiprtc
    hip::amdhip64
    ${TORCH_CUDA_LIBRARIES})

  target_compile_definitions(test_lazy PRIVATE USE_ROCM)
endif()
```

----------------------------------------

TITLE: Debugging Module Transformation Functions
DESCRIPTION: A skeleton code example showing how to create a transformation function for PyTorch modules using FX symbolic tracing. This is a template for implementing module transformations with placeholders for actual transformation logic.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
import torch
import torch.fx
import torchvision.models as models

def transform(m : torch.nn.Module) -> torch.nn.Module:
    gm = torch.fx.symbolic_trace(m)

    # Imagine we're doing some transforms here
    # <...>

    gm.recompile()

    return gm
```

----------------------------------------

TITLE: Ensembling Predictions with Shared Minibatch via Loop - PyTorch - Python
DESCRIPTION: Feeds the same data minibatch to all models, collecting their predictions in a list. This is a control scenario for ensemble evaluation and can be compared to vectorized predictions later. Assumes models and data are properly initialized.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/ensembling.ipynb#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
minibatch = data[0]
predictions2 = [model(minibatch) for model in models]
```

----------------------------------------

TITLE: Analyzing ATen Cloning Operations in PyTorch
DESCRIPTION: Records occurrences of the \"aten.clone.default\" operator which duplicates tensors while keeping their contents. It focuses on tensor shapes and integer data types.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_9

LANGUAGE: pseudocode
CODE:
```
Operator: aten.clone.default
cnt: 2, ((T([8, 128], i64),), {})
```

----------------------------------------

TITLE: Analyzing ATen Softmax Backward Operations in PyTorch
DESCRIPTION: This snippet analyzes usage of the \"aten._log_softmax_backward_data.default\" operator, detailing how gradients are computed in backpropagation through a log-softmax layer.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_1

LANGUAGE: pseudocode
CODE:
```
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([1024, 50265], f16), T([1024, 50265], f16), 1, f16), {})
```

----------------------------------------

TITLE: Installing MacOS Dependencies (Distributed) Bash
DESCRIPTION: Installs pkg-config and libuv using conda, which are required packages for enabling distributed training support (torch.distributed) when building PyTorch on macOS.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_8

LANGUAGE: Bash
CODE:
```
# Add these packages if torch.distributed is needed
conda install pkg-config libuv
```

----------------------------------------

TITLE: Computing Jacobians using functorch.jacrev and make_functional in Python
DESCRIPTION: Illustrates computing Jacobians of model outputs with respect to parameters using the legacy `functorch` library. `functorch.make_functional` converts the `model` into a functional form (`fmodel`) and extracts its parameters (`params`). `functorch.jacrev` is then applied to the functional model `fmodel` to compute the Jacobians. Requires `torch` and `functorch`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.migrating.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
# ---------------
# using functorch
# ---------------
import torch
import functorch
inputs = torch.randn(64, 3)
model = torch.nn.Linear(3, 3)

fmodel, params = functorch.make_functional(model)
jacobians = functorch.jacrev(fmodel)(params, inputs)
```

----------------------------------------

TITLE: Convolution Operations in PyTorch
DESCRIPTION: Performs 2D convolution operations with various input, kernel, and output sizes. Includes parameters for stride, padding, and dilation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
aten.convolution.default((T([64, 3, 224, 224], f16), T([64, 3, 3, 3], f16), T([64], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
```

LANGUAGE: Python
CODE:
```
aten.convolution.default((T([64, 64, 224, 224], f16), T([64, 64, 3, 3], f16), T([64], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
```

----------------------------------------

TITLE: Type Annotations for Variables in TorchScript
DESCRIPTION: Demonstrates how to use Python 3 style type hints for variable annotations in TorchScript, including containers and optional types.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
import torch
from typing import Dict, Optional

@torch.jit.script
def make_dict(flag: bool):
    x: Dict[str, int] = {}
    x['hi'] = 2
    b: Optional[int] = None
    if flag:
        b = 2
    return x, b
```

----------------------------------------

TITLE: ReLU Backward Pass in PyTorch
DESCRIPTION: Computes the gradients for the ReLU activation function during the backward pass of the neural network.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_12

LANGUAGE: Python
CODE:
```
aten.threshold_backward.default((T([64, 4096], f16), T([64, 4096], f16), 0), {})
```

LANGUAGE: Python
CODE:
```
aten.threshold_backward.default((T([64, 512, 14, 14], f16), T([64, 512, 14, 14], f16), 0), {})
```

----------------------------------------

TITLE: Jacobians Using functorch.jacrev with Argument Selection - Python
DESCRIPTION: Uses functorch's convenient 'jacrev' transform to compute the Jacobian of 'predict' with respect to the specified function argument (argnums=2 refers to 'x'). Confirms that this method matches previous Jacobian computations. Requires functorch.jacrev and all variables in scope.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from functorch import jacrev

ft_jacobian = jacrev(predict, argnums=2)(weight, bias, x)

# confirm 
assert torch.allclose(ft_jacobian, jacobian)
```

----------------------------------------

TITLE: Invoking aten.sigmoid.default with Tensor Arguments (Text)
DESCRIPTION: This section logs calls to the `aten.sigmoid.default` operator, which applies the sigmoid activation function element-wise. The examples show the operator being called on tensors with shapes like [128, 256, 1, 1] and [128, 1536, 1, 1], all using the float16 (f16) data type. The input is a tuple containing a single tensor description.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_19

LANGUAGE: text
CODE:
```
Operator: aten.sigmoid.default
```

LANGUAGE: text
CODE:
```
cnt: 1, ((T([128, 256, 1, 1], f16),), {})
```

LANGUAGE: text
CODE:
```
cnt: 2, ((T([128, 512, 1, 1], f16),), {})
```

LANGUAGE: text
CODE:
```
cnt: 9, ((T([128, 1536, 1, 1], f16),), {})
```

----------------------------------------

TITLE: Setting MPS Allocator High Watermark Ratio (Environment Variable)
DESCRIPTION: Configures the high watermark ratio for the MPS allocator using `PYTORCH_MPS_HIGH_WATERMARK_RATIO`. This acts as a hard limit for total allowed allocations relative to the device's recommended maximum working set size. Default is 1.7. A value of 0.0 disables the limit, 1.0 corresponds to the recommended max, and >1.0 allows exceeding it.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mps_environment_variables.rst#2025-04-22_snippet_3

LANGUAGE: plaintext
CODE:
```
PYTORCH_MPS_HIGH_WATERMARK_RATIO
```

----------------------------------------

TITLE: Defining Comparison Operations in TorchScript
DESCRIPTION: Specifies the syntax for comparison operations in TorchScript, including less than, greater than, equality, and others. These yield boolean values or Tensors and can be chained.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_21

LANGUAGE: python
CODE:
```
comparison    ::=  or_expr (comp_operator or_expr)*
comp_operator ::=  '<' | '>' | '==' | '>=' | '<=' | '!=' | 'is' ['not'] | ['not'] 'in'
```

----------------------------------------

TITLE: Analyzing Scalar Division Patterns (aten.div) - PyTorch - Python
DESCRIPTION: This code catalogues how the aten.div operator (elementwise scalar division) is applied across tensors of different shapes, datatypes, and memory layouts (strides); each entry lists arguments and scalar divisors. Dependencies are standard PyTorch tensor broadcasting and dtype rules. Inputs are tensors and scalars as listed, focus is on invocation variety.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
Operator: aten.div.Scalar
cnt: 1, ((T([128, 2304, 7, 7], f16, stride=(2304, 1, 0, 0)), 49), {})
cnt: 3, ((T([128, 1536, 7, 7], f16, stride=(1536, 1, 0, 0)), 49), {})
cnt: 6, ((T([128, 1536, 14, 14], f16, stride=(1536, 1, 0, 0)), 196), {})
cnt: 2, ((T([128, 512, 28, 28], f16, stride=(512, 1, 0, 0)), 784), {})
cnt: 1, ((T([128, 256, 56, 56], f16, stride=(256, 1, 0, 0)), 3136), {})
```

----------------------------------------

TITLE: Profiling PyTorch Sigmoid and Sigmoid Backward Operators - Python
DESCRIPTION: This snippet records typical input shapes and argument combinations for testing the ATen sigmoid.default and sigmoid_backward.default operators in PyTorch. It exercises both elementwise activation and its backward pass with diverse tensor sizes and dtypes (float16), covering forward and gradient propagation scenarios. Inputs are tuples of tensors with matching shapes. No external dependencies needed besides PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
cnt: 2, ((T([128, 1, 64], f16),), {})
cnt: 2, ((T([128, 1, 128], f16),), {})
cnt: 1, ((T([128, 1, 256], f16),), {})
cnt: 1, ((T([128, 1, 256], f16), T([128, 1, 256], f16)), {})
cnt: 2, ((T([128, 1, 128], f16), T([128, 1, 128], f16)), {})
cnt: 2, ((T([128, 1, 64], f16), T([128, 1, 64], f16)), {})
```

----------------------------------------

TITLE: Log Softmax Backward Computation in PyTorch (Python)
DESCRIPTION: Calculates the backward pass of the log softmax operation often used during backpropagation in neural networks. This function aids in gradient computations required for updating the model parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
aten._log_softmax_backward_data.default
cnt: 1, ((T([16, 2], f16), T([16, 2], f16), 1, f16), {})
```

----------------------------------------

TITLE: Tensor View and Reshape Operations in PyTorch
DESCRIPTION: This snippet demonstrates the usage of aten._unsafe_view.default operator for reshaping tensors with various dimensions and sizes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/attention_is_all_you_need_pytorch_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
Operator: aten._unsafe_view.default
cnt: 36, ((T([8448, 512], f16), [256, 33, 512]), {})
cnt: 24, ((T([256, 8, 33, 64], f16), [2048, 33, 64]), {})
cnt: 12, ((T([256, 8, 64, 33], f16), [2048, 64, 33]), {})
cnt: 6, ((T([2048, 33, 33], f16), [256, 8, 33, 33]), {})
cnt: 6, ((T([2048, 33, 64], f16), [256, 8, 33, 64]), {})
cnt: 36, ((T([7936, 512], f16), [256, 31, 512]), {})
cnt: 30, ((T([256, 8, 31, 64], f16), [2048, 31, 64]), {})
cnt: 6, ((T([256, 8, 64, 31], f16), [2048, 64, 31]), {})
cnt: 6, ((T([2048, 31, 31], f16), [256, 8, 31, 31]), {})
cnt: 12, ((T([2048, 31, 64], f16), [256, 8, 31, 64]), {})
cnt: 6, ((T([2048, 31, 33], f16), [256, 8, 31, 33]), {})
cnt: 1, ((T([7936, 9521], f16), [256, 31, 9521]), {})
cnt: 18, ((T([256, 33, 8, 64], f16), [256, 33, 512]), {})
cnt: 12, ((T([256, 33, 512], f16), [8448, 512]), {})
cnt: 18, ((T([256, 31, 8, 64], f16), [256, 31, 512]), {})
cnt: 6, ((T([256, 31, 512], f16), [7936, 512]), {})
```

----------------------------------------

TITLE: TorchScript Graph Output for Traced ELU Module (Text)
DESCRIPTION: Displays the structure of the TorchScript graph generated by tracing the `torch.nn.ELU` module. It shows the inputs, constant values, and the `aten::elu` node, indicating the operator that needs a symbolic function for ONNX export.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_17

LANGUAGE: text
CODE:
```
graph(%self : __torch__.torch.nn.modules.activation.___torch_mangle_0.ELU,
      %input : Float(1, strides=[1], requires_grad=0, device=cpu)):
  %4 : float = prim::Constant[value=1.]()
  %5 : int = prim::Constant[value=1]()
  %6 : int = prim::Constant[value=1]()
  %7 : Float(1, strides=[1], requires_grad=0, device=cpu) = aten::elu(%input, %4, %5, %6)
  return (%7)
```

----------------------------------------

TITLE: Non-torch Function Handling in FX
DESCRIPTION: Demonstrates how to handle non-torch functions like built-in Python functions using the wrap API.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
import torch
import torch.fx
from math import sqrt

def normalize(x):
    return x / sqrt(len(x))

torch.fx.wrap('len')
torch.fx.wrap('sqrt')
```

----------------------------------------

TITLE: Computing NLL Loss Backward in PyTorch
DESCRIPTION: The nll_loss_backward operator calculates the negative log likelihood loss gradient, essential for propagating errors back through a network during training. It is applied to loss terms derived from cross-entropy errors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/legacy_senet154_training.txt#2025-04-22_snippet_7

LANGUAGE: text
CODE:
```
cnt: 1, ((T([], f16), T([32, 1000], f16), T([32], i64), None, 1, -100, T([], f16)), {})
```

----------------------------------------

TITLE: Custom EMA Implementation using multi_avg_fn
DESCRIPTION: Example showing how to implement EMA using the more efficient multi_avg_fn parameter.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_15

LANGUAGE: Python
CODE:
```
ema_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(0.9))
```

----------------------------------------

TITLE: Implementing Custom Metric Handler for TorchElastic in Python
DESCRIPTION: This snippet demonstrates how to implement a custom metric handler by extending the MetricHandler class and configuring it in a custom launcher to process and emit metrics.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/customization.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
# my_launcher.py

import torch.distributed.elastic.metrics as metrics

class MyMetricHandler(metrics.MetricHandler):
    def emit(self, metric_data: metrics.MetricData):
        # push metric_data to your metric sink

def main():
  metrics.configure(MyMetricHandler())

  spec = WorkerSpec(...)
  agent = LocalElasticAgent(spec)
  agent.run()
```

----------------------------------------

TITLE: Running an Exported ONNX Model with ONNX Runtime in Python
DESCRIPTION: This snippet demonstrates loading an ONNX model using ONNX Runtime and running inference with dummy input data. It requires the `onnxruntime` and `numpy` libraries installed. The input is the path to the `.onnx` file and sample data; the output is the result of the model inference.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_2

LANGUAGE: python
CODE:
```
import onnxruntime as ort
import numpy as np

ort_session = ort.InferenceSession("alexnet.onnx")

outputs = ort_session.run(
    None,
    {"actual_input_1": np.random.randn(10, 3, 224, 224).astype(np.float32)},
)
print(outputs[0])
```

----------------------------------------

TITLE: Defining Custom C++ Operator Symbolic (Part 2: Registering Symbolic)
DESCRIPTION: Registers the custom symbolic function symbolic_foo_forward for the PyTorch C++ operator custom_ops::foo_forward. This allows the ONNX exporter to correctly translate calls to this specific C++ custom operator into the defined ONNX graph node.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_31

LANGUAGE: Python
CODE:
```
torch.onnx.register_custom_op_symbolic("custom_ops::foo_forward", symbolic_foo_forward, 9)
```

----------------------------------------

TITLE: Configuring TunableOp Environment Variables for Untuned GEMM Collection (Shell)
DESCRIPTION: Demonstrates setting environment variables in a shell to configure TunableOp for the first step of offline tuning. `PYTORCH_TUNABLEOP_ENABLED=1` enables the feature, `PYTORCH_TUNABLEOP_TUNING=0` disables runtime tuning, and `PYTORCH_TUNABLEOP_RECORD_UNTUNED=1` instructs TunableOp to record encountered operations (like GEMMs) without tuning them, typically into a file like `tunableop_untuned0.csv`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/cuda/tunable/README.md#2025-04-22_snippet_1

LANGUAGE: shell
CODE:
```
PYTORCH_TUNABLEOP_ENABLED=1
PYTORCH_TUNABLEOP_TUNING=0
PYTORCH_TUNABLEOP_RECORD_UNTUNED=1
...
```

----------------------------------------

TITLE: Handling Scalar Operations in ONNX Export for PyTorch
DESCRIPTION: Example of how scalar operations are dispatched in PyTorch's ONNX export. This snippet shows the difference in dispatch between tensor-tensor addition and tensor-scalar addition.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/onnx/README.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
add(*[self, other], **{"alpha": alpha})

add(*[self], **{"other": other, "alpha": alpha})
```

----------------------------------------

TITLE: Using Complex Indexing in C++ with torch::Tensor::index
DESCRIPTION: This example demonstrates a more complex indexing operation in C++, using multiple index types including ellipsis, integer, boolean, slice, and tensor indices.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_indexing.rst#2025-04-22_snippet_2

LANGUAGE: C++
CODE:
```
tensor.index({"...", 0, true, Slice(1, None, 2), torch::tensor({1, 2})})
```

----------------------------------------

TITLE: Sharing Memory Pool Across Multiple CUDA Graph Captures
DESCRIPTION: Demonstrates how to capture multiple `torch.cuda.CUDAGraph` instances while sharing the same memory pool between them. This is achieved by passing the `pool` attribute of the first captured graph (`g1`) to the `torch.cuda.graph` context manager when capturing the second graph (`g2`), optimizing memory usage when graphs are replayed sequentially.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_47

LANGUAGE: python
CODE:
```
g1 = torch.cuda.CUDAGraph()
    g2 = torch.cuda.CUDAGraph()

    # (create static inputs for g1 and g2, run warmups of their workloads...)

    # Captures g1
    with torch.cuda.graph(g1):
        static_out_1 = g1_workload(static_in_1)

    # Captures g2, hinting that g2 may share a memory pool with g1
    with torch.cuda.graph(g2, pool=g1.pool()):
        static_out_2 = g2_workload(static_in_2)

    static_in_1.copy_(real_data_1)
    static_in_2.copy_(real_data_2)
    g1.replay()
    g2.replay()
```

----------------------------------------

TITLE: Sigmoid Backward Pass in PyTorch
DESCRIPTION: This snippet computes the gradient of the sigmoid function during the backward pass of backpropagation. It's crucial for training neural networks that use sigmoid activation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_15

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([8, 3, 48, 64, 81], f16), T([8, 3, 48, 64, 81], f16, stride=(783360, 261120, 5440, 85, 1))), {})
cnt: 1, ((T([8, 3, 48, 64, 2], f16), T([8, 3, 48, 64, 2], f16)), {})
cnt: 1, ((T([8, 3, 24, 32, 81], f16), T([8, 3, 24, 32, 81], f16, stride=(195840, 65280, 2720, 85, 1))), {})
cnt: 1, ((T([8, 3, 24, 32, 2], f16), T([8, 3, 24, 32, 2], f16)), {})
cnt: 1, ((T([8, 3, 12, 16, 81], f16), T([8, 3, 12, 16, 81], f16, stride=(48960, 16320, 1360, 85, 1))), {})
cnt: 1, ((T([8, 3, 12, 16, 2], f16), T([8, 3, 12, 16, 2], f16)), {})
```

----------------------------------------

TITLE: Visualizing DTensor sharding
DESCRIPTION: Uses the visualize_sharding function to display the sharding of a DTensor with up to 3 dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.tensor.rst#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
visualize_sharding(dtensor)
```

----------------------------------------

TITLE: Registering a Symbolic Function for ONNX Export in PyTorch
DESCRIPTION: Example of how to register a new symbolic function for ONNX export using the internal registration.onnx_symbolic decorator. This snippet demonstrates the registration of the 'reshape' function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/onnx/README.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
@registration.onnx_symbolic
def reshape(g, self, shape):
```

----------------------------------------

TITLE: Computing Jacobian with PyTorch Linear Module
DESCRIPTION: Demonstrates how to compute the Jacobian of a PyTorch Linear layer using jacrev transform. The example shows creating a 3x3 linear layer, defining a wrapper function, and computing its Jacobian with respect to the input.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/functorch.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
model = torch.nn.Linear(3, 3)

def f(x):
    return model(x)

x = torch.randn(3)
jacobian = jacrev(f)(x)
assert jacobian.shape == (3, 3)
```

----------------------------------------

TITLE: Applying Native Batch Normalization with ATen
DESCRIPTION: Applies batch normalization to input tensors, a critical step in stabilizing learning by normalizing activations. Requires input tensors, running mean, and variance tensors all shaped accordingly, with specific momentum and epsilon values. Outputs normalized tensors facilitating enhanced convergence and learning rates in training phases.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
Operator: aten.native_batch_norm.default
cnt: 2, ((T([1, 1024, 128, 128], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f16), False, 0.1, 1e-05), {})
cnt: 2, ((T([1, 2048, 64, 64], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f16), False, 0.1, 1e-05), {})
cnt: 13, ((T([1, 4096, 32, 32], f16), T([4096], f16), T([4096], f16), T([4096], f16), T([4096], f16), False, 0.1, 1e-05), {})
```

----------------------------------------

TITLE: Computing Batch Hessian with vmap in PyTorch
DESCRIPTION: Demonstrates computing batched Hessians using vmap combined with the hessian function. This applies the hessian computation to each input in the batch and stacks the results.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_21

LANGUAGE: python
CODE:
```
compute_batch_hessian = vmap(hessian(predict, argnums=2), in_dims=(None, None, 0))

batch_hess = compute_batch_hessian(weight, bias, x)
batch_hess.shape
```

----------------------------------------

TITLE: Using Data Scheduler with Data Sparsifier in PyTorch Training Loop
DESCRIPTION: Example code demonstrating how to integrate a data scheduler with a data sparsifier in a PyTorch training loop. The scheduler adjusts sparsification parameters after each epoch, while the sparsifier processes the input data during training.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_scheduler/README.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
model = SomeModel()
optimizer = SomeOptimizer(model.parameters(), lr=...)
data_sparsifier = SomeDataSparsifier(...)


data_scheduler = SomeDataScheduler(data_sparsifier, ...)


data_name = 'train_data'

for epoch in range(EPOCHS):
    for input, target in dataset:
        input = data_sparsifier.add_data(name=data_name, data=input)

        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
        data_sparsifier.step()

    data_scheduler.step()
```

----------------------------------------

TITLE: Building libtorch Using setup.py
DESCRIPTION: This snippet shows an alternative method to build libtorch using setup.py. It builds the C++ libraries and indicates where to find the output, including libtorch.so.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/libtorch.rst#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
cd <pytorch_root>
python setup.py build

ls torch/lib/tmp_install # output is produced here
ls torch/lib/tmp_install/lib/libtorch.so # of particular interest
```

----------------------------------------

TITLE: Attempting Invalid Conversion from Meta Tensor in Python
DESCRIPTION: Demonstrates the limitation of meta tensors: they cannot be directly converted to data-carrying devices like CPU or CUDA using methods like `.to()`. This is because meta tensors lack the actual data required for such a conversion, resulting in a `NotImplementedError`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/meta.rst#_snippet_3

LANGUAGE: Python
CODE:
```
>>> torch.ones(5, device='meta').to("cpu")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NotImplementedError: Cannot copy out of meta tensor; no data!
```

----------------------------------------

TITLE: CMake Build Configuration
DESCRIPTION: CMake configuration file for building the C++ inference example, including automatic model compilation and proper LibTorch linking.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_aot_inductor.rst#2025-04-22_snippet_3

LANGUAGE: cmake
CODE:
```
cmake_minimum_required(VERSION 3.18 FATAL_ERROR)
project(aoti_example)

find_package(Torch REQUIRED)

add_executable(aoti_example inference.cpp model.pt2)

add_custom_command(
    OUTPUT model.pt2
    COMMAND python ${CMAKE_CURRENT_SOURCE_DIR}/model.py
    DEPENDS model.py
)

target_link_libraries(aoti_example "${TORCH_LIBRARIES}")
set_property(TARGET aoti_example PROPERTY CXX_STANDARD 17)
```

----------------------------------------

TITLE: Convolution Operations in PyTorch for Feature Extraction
DESCRIPTION: This code shows forward convolution operations used in the model for feature extraction at different scales. It includes initial input processing, downsampling operations, and various convolution operations with different kernel sizes, strides, and channel dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/visformer_small_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
Operator: aten.clone.default
cnt: 1, ((T([128, 3, 224, 224], f16),), {})
Operator: aten.convolution.default
cnt: 1, ((T([128, 3, 224, 224], f16), T([32, 3, 7, 7], f16), None, [2, 2], [3, 3], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 32, 112, 112], f16), T([192, 32, 4, 4], f16), T([192], f16), [4, 4], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 7, ((T([128, 192, 28, 28], f16), T([384, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 7, ((T([128, 384, 28, 28], f16), T([384, 48, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 8), {})
cnt: 7, ((T([128, 384, 28, 28], f16), T([192, 384, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 192, 28, 28], f16), T([384, 192, 2, 2], f16), T([384], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 384, 14, 14], f16), T([1152, 384, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 384, 14, 14], f16), T([384, 384, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 384, 14, 14], f16), T([1536, 384, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 1536, 14, 14], f16), T([384, 1536, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 384, 14, 14], f16), T([768, 384, 2, 2], f16), T([768], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 768, 7, 7], f16), T([2304, 768, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 768, 7, 7], f16), T([768, 768, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 768, 7, 7], f16), T([3072, 768, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 3072, 7, 7], f16), T([768, 3072, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

----------------------------------------

TITLE: Unsupported Data-dependent Control Flow with vmap
DESCRIPTION: This example shows how vmap does not support functions with data-dependent control flow, where conditional statements depend on tensor values being vmapped over.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/ux_limitations.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
def relu(x):
  if x > 0:
    return x
  return 0

x = torch.randn(3)
vmap(relu)(x)
```

----------------------------------------

TITLE: Configuring X86 Default Quantization Settings
DESCRIPTION: Shows how to configure PyTorch quantization settings for x86 backends using either post-training quantization (PTQ) or quantization-aware training (QAT) configs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
# set the qconfig for PTQ
# Note: the old 'fbgemm' is still available but 'x86' is the recommended default on x86 CPUs
qconfig = torch.ao.quantization.get_default_qconfig('x86')
# or, set the qconfig for QAT
qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')
# set the qengine to control weight packing
torch.backends.quantized.engine = 'x86'
```

----------------------------------------

TITLE: Export IR Node Structure for torch.fx.Node Representation in Python
DESCRIPTION: This snippet establishes the Python schema for a Node in Export IR, specifying the required properties such as name, operation type (op_name), target operation/callable, argument list, keyword argument dictionary, and metadata dictionary. It serves as a basis for representing computational operations within the torch.fx framework, as used by PyTorch Export IR. Expected input values align with torch.fx conventions; limitations include that the specific meaning of target/args/kwargs/meta depends on op_name.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
class Node:
  name: str # name of node
  op_name: str  # type of operation

  # interpretation of the fields below depends on op_name
  target: [str|Callable]
  args: List[object]
  kwargs: Dict[str, object]
  meta: Dict[str, object]
```

----------------------------------------

TITLE: Implementing Custom Pruning Pattern for Conv2D with Pooling
DESCRIPTION: Example showing how to create a custom pruning function for a Conv2D-Pool-Activation-Conv2D pattern and integrate it with the default pruning patterns. The code demonstrates pattern definition and registration with SaliencyPruner.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/pruner/README.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from torch.ao.pruning._experimental.pruner.prune_functions import prune_conv2d_activation_conv2d

def prune_conv2d_pool_activation_conv2d(
    c1: nn.Conv2d,
    pool: nn.Module,
    activation: Optional[Callable[[Tensor], Tensor]],
    c2: nn.Conv2d,
) -> None:
    prune_conv2d_activation_conv2d(c1, activation, c2)

# note how the pattern defined in the key will be passed to the pruning function as args
my_patterns = {(nn.Conv2d, nn.MaxPool2d, nn.ReLU, nn.Conv2d): prune_conv2d_activation_conv2d}

pruning_patterns = _get_default_structured_pruning_patterns()
pruning_patterns.update(my_patterns)

pruner = SaliencyPruner({}, patterns=pruning_patterns)
```

----------------------------------------

TITLE: Extended TensorOptions Configuration
DESCRIPTION: Shows how to specify multiple tensor properties using chained method calls.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_10

LANGUAGE: cpp
CODE:
```
torch::ones(10, torch::TensorOptions().dtype(torch::kFloat32).layout(torch::kStrided))
```

LANGUAGE: cpp
CODE:
```
torch::ones(10, torch::dtype(torch::kFloat32).layout(torch::kStrided))
```

----------------------------------------

TITLE: Using CUDAMultiStreamGuard for Multiple Streams in PyTorch C++
DESCRIPTION: This example shows how to use CUDAMultiStreamGuard to set and manage multiple CUDA streams on different devices simultaneously.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_5

LANGUAGE: cpp
CODE:
```
// create two tensor, one on device 0, one on device 1
torch::Tensor tensor0 = torch::ones({2, 2}, torch::device({torch::kCUDA, 0}));
torch::Tensor tensor1 = torch::ones({2, 2}, torch::device({torch::kCUDA, 1}));

// acquire new CUDA streams from CUDA stream pool on device 0 and device 1
at::cuda::CUDAStream myStream0 = at::cuda::getStreamFromPool(false, 0);
at::cuda::CUDAStream myStream1 = at::cuda::getStreamFromPool(false, 1);

// set current CUDA stream on device 0 to `myStream0` and
// set current CUDA stream on device 1 to `myStream1` CUDA using multistream guard
{
  at::cuda::CUDAMultiStreamGuard multi_guard({myStream0, myStream1});

  // sum() on tensor0 uses `myStream0` as current CUDA stream on device 0
  tensor0.sum();
  // sum() on tensor1 uses `myStream1` as current CUDA stream on device 1
  tensor1.sum();
}

// current CUDA stream on device 0 is reset to default CUDA stream on device 0
// current CUDA stream on device 1 is reset to default CUDA stream on device 1

// sum() on tensor0 uses default CUDA stream as current CUDA stream on device 0
tensor0.sum();
// sum() on tensor1 uses default CUDA stream as current CUDA stream on device 1
tensor1.sum();
```

----------------------------------------

TITLE: Defining Custom C++ Operator Symbolic (Part 3: PyTorch Module)
DESCRIPTION: Defines a simple PyTorch nn.Module, FooModel, which utilizes the custom C++ operator torch.ops.custom_ops.foo_forward in its forward method. This demonstrates how a model would use the custom C++ op before export.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_32

LANGUAGE: Python
CODE:
```
class FooModel(torch.nn.Module):
    def __init__(self, attr1, attr2):
        super().__init__()
        self.attr1 = attr1
        self.attr2 = attr2

    def forward(self, input1, input2):
        # Calling custom op
        return torch.ops.custom_ops.foo_forward(input1, input2, self.attr1, self.attr2)
```

----------------------------------------

TITLE: Benchmarking Ensemble Prediction Performance with Timer - PyTorch - Python
DESCRIPTION: Benchmarks the execution time of prediction generation with and without vmap for 100 iterations, leveraging torch.utils.benchmark.Timer. Outputs comparative timing for the loop-based and vectorized approaches. Useful for demonstrating practical performance gains from adopting vmap for ensembling.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/ensembling.ipynb#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from torch.utils.benchmark import Timer
without_vmap = Timer(
    stmt="[model(minibatch) for model, minibatch in zip(models, minibatches)]",
    globals=globals())
with_vmap = Timer(
    stmt="vmap(fmodel)(params, buffers, minibatches)",
    globals=globals())
print(f'Predictions without vmap {without_vmap.timeit(100)}')
print(f'Predictions with vmap {with_vmap.timeit(100)}')
```

----------------------------------------

TITLE: Building CMake Project Bash
DESCRIPTION: Presents the standard sequence of shell commands used to configure and build a project configured with CMake. It involves creating a build directory, running `cmake ..` to configure the project files, and `cmake --build .` to compile the source code within the build directory.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_58

LANGUAGE: Bash
CODE:
```
mkdir build
cd build
cmake ..
cmake --build .
```

----------------------------------------

TITLE: Matrix Multiplication Operations in PyTorch
DESCRIPTION: This snippet shows the usage of aten.bmm.default operator for batch matrix multiplication with various tensor shapes and strides.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/attention_is_all_you_need_pytorch_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
Operator: aten.bmm.default
cnt: 6, ((T([2048, 33, 64], f16), T([2048, 64, 33], f16)), {})
cnt: 6, ((T([2048, 33, 33], f16), T([2048, 33, 64], f16)), {})
cnt: 6, ((T([2048, 31, 64], f16), T([2048, 64, 31], f16)), {})
cnt: 6, ((T([2048, 31, 31], f16), T([2048, 31, 64], f16)), {})
cnt: 6, ((T([2048, 31, 64], f16), T([2048, 64, 33], f16)), {})
cnt: 6, ((T([2048, 31, 33], f16), T([2048, 33, 64], f16)), {})
cnt: 6, ((T([2048, 33, 31], f16, stride=(1023, 1, 33)), T([2048, 31, 64], f16)), {})
cnt: 6, ((T([2048, 31, 64], f16), T([2048, 64, 33], f16, stride=(2112, 1, 64))), {})
cnt: 6, ((T([2048, 64, 31], f16, stride=(1984, 1, 64)), T([2048, 31, 33], f16)), {})
cnt: 6, ((T([2048, 31, 33], f16), T([2048, 33, 64], f16, stride=(2112, 1, 33))), {})
cnt: 6, ((T([2048, 31, 31], f16, stride=(961, 1, 31)), T([2048, 31, 64], f16)), {})
cnt: 6, ((T([2048, 31, 64], f16), T([2048, 64, 31], f16, stride=(1984, 1, 64))), {})
cnt: 6, ((T([2048, 64, 31], f16, stride=(1984, 1, 64)), T([2048, 31, 31], f16)), {})
cnt: 6, ((T([2048, 31, 31], f16), T([2048, 31, 64], f16, stride=(1984, 1, 31))), {})
cnt: 6, ((T([2048, 33, 33], f16, stride=(1089, 1, 33)), T([2048, 33, 64], f16)), {})
cnt: 6, ((T([2048, 33, 64], f16), T([2048, 64, 33], f16, stride=(2112, 1, 64))), {})
cnt: 6, ((T([2048, 64, 33], f16, stride=(2112, 1, 64)), T([2048, 33, 33], f16)), {})
cnt: 6, ((T([2048, 33, 33], f16), T([2048, 33, 64], f16, stride=(2112, 1, 33))), {})
```

----------------------------------------

TITLE: Jacobian-vector Product (JVP) Transform in PyTorch
DESCRIPTION: Demonstrates computing Jacobian-vector products using forward-mode automatic differentiation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/whirlwind_tour.ipynb#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from functorch import jvp
x = torch.randn(5)
y = torch.randn(5)
f = lambda x, y: (x * y)
_, output = jvp(f, (x, y), (torch.ones(5), torch.ones(5)))
assert torch.allclose(output, x + y)
```

----------------------------------------

TITLE: Implementing Batched Matrix Multiply with First-Class Dimensions in Python
DESCRIPTION: This snippet demonstrates how to implement batched matrix multiplication (bmm) using first-class dimensions in PyTorch. It adds a batch dimension to the matrix multiply operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
def bmm(A, B):
    i = dims(1) # note: i here is a different value from i inside mm so it works
    return mm(A[i], B[i]).order(i)
```

----------------------------------------

TITLE: Using Lazy Tensor for Dynamic Control Flow in PyTorch
DESCRIPTION: This code demonstrates how to use Lazy Tensor to handle the same function that caused issues with jit.trace. It shows moving tensors to the 'lazy' device and executing the function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
dev = "lazy"
t_lazy = torch.ones(1).to(dev)
maybe_false_lazy = torch.BoolTensor([0]).to(dev)
lazy_result = add_two_maybe(t_lazy, maybe_false_lazy)
```

----------------------------------------

TITLE: Manipulating Tensor Dimensions
DESCRIPTION: Demonstrates how to permute and flatten tensor dimensions more expressively than standard approaches, using Tensor.align_to, Tensor.flatten, and Tensor.unflatten for dimension management.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/named_tensor.rst#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
tensor = torch.randn(2, 2, 2, 2, 2, 2)
named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')

# Move the F (dim 5) and E dimension (dim 4) to the front while keeping
tensor.permute(5, 4, 0, 1, 2, 3)
named_tensor.align_to('F', 'E', ...)

imgs = torch.randn(32, 3, 128, 128)
named_imgs = imgs.refine_names('N', 'C', 'H', 'W')

flat_imgs = imgs.view(32, -1)
named_flat_imgs = named_imgs.flatten(['C', 'H', 'W'], 'features')
named_flat_imgs.names

unflattened_named_imgs = named_flat_imgs.unflatten('features', [('C', 3), ('H', 128), ('W', 128)])
unflattened_named_imgs.names
```

----------------------------------------

TITLE: Implementing Construct-time Type Checking in Python DataPipes
DESCRIPTION: Demonstrates how to use the @argument_validation decorator for construct-time type checking in DataPipe classes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/typing.ipynb#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
from torch.utils.data import argument_validation

class DP(IterDataPipe):
    @argument_validation
    def __init__(self, dp: IterDataPipe[Union[int, tuple]]):
        self.dp = dp

    def __iter__(self):
        for d in self.dp:
            yield d
```

LANGUAGE: python
CODE:
```
dp = DP(range(10))
```

LANGUAGE: python
CODE:
```
class Temp(IterDataPipe[str]):
    def __iter__(self):
        pass
dp = DP(Temp())
```

LANGUAGE: python
CODE:
```
class Temp(IterDataPipe[Tuple[int, T_co]]):
    def __iter__(self):
        pass
dp = DP(Temp())
```

----------------------------------------

TITLE: Computing Jacobian Row-by-Row Using Autograd - Python
DESCRIPTION: Implements 'compute_jac' to calculate the Jacobian of the 'predict' function with respect to input using PyTorch's autograd.grad in a row-wise fashion. Expects xp (input vector) and uses a global 'unit_vectors' variable. Returns the Jacobian matrix by stacking gradients for each unit vector. Requires 'predict', 'weight', 'bias' in scope, and 'unit_vectors' to be initialized as identity matrix.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
def compute_jac(xp):
    jacobian_rows = [torch.autograd.grad(predict(weight, bias, xp), xp, vec)[0]
                     for vec in unit_vectors]
    return torch.stack(jacobian_rows)
```

----------------------------------------

TITLE: Unsupported Read Indexing with Negative Values (Python)
DESCRIPTION: Shows an unsupported tensor indexing pattern for reading data in PyTorch when exporting to ONNX. The pattern includes negative index values within tensor indices. Workaround is to use positive index values.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_9

LANGUAGE: python
CODE:
```
data[torch.tensor([[1, 2], [2, -3]]), torch.tensor([-2, 3])]
# Workarounds: use positive index values.
```

----------------------------------------

TITLE: Using CommDebugMode for debugging DTensor operations
DESCRIPTION: Enables communication debugging mode to track collective operations performed by DTensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.tensor.rst#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
with CommDebugMode():
    # Your DTensor operations here
```

----------------------------------------

TITLE: Creating Tensor Copies with aten.clone.default
DESCRIPTION: Shows usage of the clone.default operator which creates deep copies of tensors. This is typically used before modifying tensors to prevent in-place changes to the originals, with operations focusing on feature maps and input images.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
Operator: aten.clone.default
cnt: 1, ((T([8, 3, 384, 512], f16),), {})
cnt: 1, ((T([8, 3, 12, 16, 85], f16),), {})
cnt: 1, ((T([8, 3, 24, 32, 85], f16),), {})
cnt: 1, ((T([8, 3, 48, 64, 85], f16),), {})
```

----------------------------------------

TITLE: Advanced Tensor Reshaping with aten._unsafe_view in PyTorch (Python)
DESCRIPTION: Demonstrates use of the _unsafe_view method for creating tensor views with new shapes, affecting memory structure without copy. Users must ensure reshaped tensors are compatible; improper usage can result in undefined behavior. Examples highlight reshaping for multi-head attention models, with key parameters being the input tensor and target shape.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
Operator: aten._unsafe_view.default
cnt: 36, ((T([4, 12, 128, 64], f16), [48, 128, 64]), {})
```

LANGUAGE: python
CODE:
```
Operator: aten._unsafe_view.default
cnt: 12, ((T([4, 12, 64, 128], f16), [48, 64, 128]), {})
```

LANGUAGE: python
CODE:
```
Operator: aten._unsafe_view.default
cnt: 12, ((T([48, 128, 128], f16), [4, 12, 128, 128]), {})
```

LANGUAGE: python
CODE:
```
Operator: aten._unsafe_view.default
cnt: 12, ((T([48, 128, 64], f16), [4, 12, 128, 64]), {})
```

LANGUAGE: python
CODE:
```
Operator: aten._unsafe_view.default
cnt: 24, ((T([4, 128, 12, 64], f16), [4, 128, 768]), {})
```

LANGUAGE: python
CODE:
```
Operator: aten._unsafe_view.default
cnt: 12, ((T([4, 128, 768], f16), [512, 768]), {})
```

----------------------------------------

TITLE: Example ONNX Export Failure Error Message (Text)
DESCRIPTION: Displays a typical error message encountered when the PyTorch ONNX exporter fails because it encounters an unsupported operator (e.g., `foo`). This error indicates that a symbolic function is missing for the operator.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_14

LANGUAGE: text
CODE:
```
RuntimeError: ONNX export failed: Couldn't export operator foo
```

----------------------------------------

TITLE: Tracking Batch Normalization Operations in PyTorch
DESCRIPTION: Records of batch normalization forward operations with various tensor shapes and parameters. Each line shows the count (cnt) of operations with specific tensor dimensions (batch_size, channels, height, width), data types (f16), and normalization parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tinynet_a_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
cnt: 4, ((T([128, 112, 12, 12], f16), T([112], f16), T([112], f16), T([112], f16), T([112], f16), True, 0.1, 1e-05), {})
cnt: 7, ((T([128, 672, 12, 12], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 672, 6, 6], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), True, 0.1, 1e-05), {})
cnt: 5, ((T([128, 192, 6, 6], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), True, 0.1, 1e-05), {})
cnt: 10, ((T([128, 1152, 6, 6], f16), T([1152], f16), T([1152], f16), T([1152], f16), T([1152], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 320, 6, 6], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 1280, 6, 6], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f16), True, 0.1, 1e-05), {})
```

----------------------------------------

TITLE: Complex Indexing with Assignment in C++ using torch::Tensor::index_put_
DESCRIPTION: This snippet shows a complex indexing operation with assignment in C++, using multiple index types including ellipsis, integer, boolean, slice, and tensor indices.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_indexing.rst#2025-04-22_snippet_3

LANGUAGE: C++
CODE:
```
tensor.index_put_({"...", 0, true, Slice(1, None, 2), torch::tensor({1, 2})}, 1)
```

----------------------------------------

TITLE: Usage Log: aten.native_batch_norm_backward.default Operator (Text)
DESCRIPTION: Logs calls to the `aten.native_batch_norm_backward.default` operator. Each entry displays the count (`cnt`) and the argument tuple, detailing tensor shapes (f16, f32), boolean flags, epsilon value (1e-05), and gradient enablement flags. This indicates usage in computing gradients for batch normalization layers.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dm_nfnet_f0_training.txt#2025-04-22_snippet_2

LANGUAGE: text
CODE:
```
Operator: aten.native_batch_norm_backward.default
cnt: 1, ((T([1, 3072, 1536], f16), T([1, 3072, 1536], f16), T([3072], f16), None, None, T([3072], f32), T([3072], f32), True, 1e-05, [True, True, False]), {})
cnt: 9, ((T([1, 1536, 768], f16), T([1, 1536, 768], f16), T([1536], f16), None, None, T([1536], f32), T([1536], f32), True, 1e-05, [True, True, False]), {})
cnt: 18, ((T([1, 768, 1152], f16), T([1, 768, 1152], f16), T([768], f16), None, None, T([768], f32), T([768], f32), True, 1e-05, [True, True, False]), {})
cnt: 8, ((T([1, 768, 1536], f16), T([1, 768, 1536], f16), T([768], f16), None, None, T([768], f32), T([768], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 1536, 1536], f16), T([1, 1536, 1536], f16), T([1536], f16), None, None, T([1536], f32), T([1536], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 768, 512], f16), T([1, 768, 512], f16), T([768], f16), None, None, T([768], f32), T([768], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 1536, 512], f16), T([1, 1536, 512], f16), T([1536], f16), None, None, T([1536], f32), T([1536], f32), True, 1e-05, [True, True, False]), {})
cnt: 3, ((T([1, 512, 256], f16), T([1, 512, 256], f16), T([512], f16), None, None, T([512], f32), T([512], f32), True, 1e-05, [True, True, False]), {})
cnt: 4, ((T([1, 256, 1152], f16), T([1, 256, 1152], f16), T([256], f16), None, None, T([256], f32), T([256], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 256, 512], f16), T([1, 256, 512], f16), T([256], f16), None, None, T([256], f32), T([256], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 256, 256], f16), T([1, 256, 256], f16), T([256], f16), None, None, T([256], f32), T([256], f32), True, 1e-05, [True, True, False]), {})
cnt: 2, ((T([1, 256, 128], f16), T([1, 256, 128], f16), T([256], f16), None, None, T([256], f32), T([256], f32), True, 1e-05, [True, True, False]), {})
cnt: 2, ((T([1, 128, 1152], f16), T([1, 128, 1152], f16), T([128], f16), None, None, T([128], f32), T([128], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 128, 128], f16), T([1, 128, 128], f16), T([128], f16), None, None, T([128], f32), T([128], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 128, 576], f16), T([1, 128, 576], f16), T([128], f16), None, None, T([128], f32), T([128], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 64, 288], f16), T([1, 64, 288], f16), T([64], f16), None, None, T([64], f32), T([64], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 32, 144], f16), T([1, 32, 144], f16), T([32], f16), None, None, T([32], f32), T([32], f32), True, 1e-05, [True, True, False]), {})
cnt: 1, ((T([1, 16, 27], f16), T([1, 16, 27], f16), T([16], f16), None, None, T([16], f32), T([16], f32), True, 1e-05, [True, True, False]), {})
```

----------------------------------------

TITLE: Profiling Convolution Operations in PyTorch
DESCRIPTION: This snippet shows the tensor shapes and parameters for various convolution operations in the model. It includes different stride and padding configurations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2net50_14w_8s_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([128, 2048, 7, 7], f16), T([128, 1024, 14, 14], f16), T([2048, 1024, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 7, ((T([128, 112, 7, 7], f16), T([128, 112, 14, 14], f16, stride=(175616, 196, 14, 1)), T([112, 112, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 896, 14, 14], f16), T([128, 1024, 14, 14], f16), T([896, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

----------------------------------------

TITLE: Translating Python For Loop to PyTorch JIT IR Graph
DESCRIPTION: Shows the PyTorch JIT IR graph representation for the example Python function `f(x)` containing a `for` loop. It demonstrates how the loop is translated into a `prim::Loop` node, using `aten::size` to determine the `max_trip_count` and `prim::Constant[value=1]` for the `initial_condition`, with the loop body (`aten::mul`) contained within the loop's block.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_6

LANGUAGE: JIT IR
CODE:
```
graph(%z.1 : Dynamic):
  %3 : bool = prim::Constant[value=1]()
  %1 : int = prim::Constant[value=0]()
  %2 : int = aten::size(%z.1, %1)
  %z : Dynamic = prim::Loop(%2, %3, %z.1)
    block0(%i : int, %5 : Dynamic):
      %z.2 : Dynamic = aten::mul(%5, %5)
      -> (%3, %z.2)
  return (%z)
```

----------------------------------------

TITLE: Enumerating ATen Operator Invocation Patterns - PyTorch Text
DESCRIPTION: This snippet lists invocation schema patterns for various ATen operators in PyTorch, describing operand shapes, types, strides, and argument sets. It provides counts for how often each pattern is used, facilitating coverage analysis and systematization of computation paths in neural network pipelines. Required dependency is PyTorch (ATen), and this format expects symbolic tensor specifications (e.g., T([128, 1000], f16)) instead of actual data; no computation is performed.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/selecsls42b_training.txt#2025-04-22_snippet_0

LANGUAGE: Text
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([128, 1000], f16), 1, False), {})
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})
Operator: aten.add.Tensor
cnt: 1, ((T([128, 152, 14, 14], f16, stride=(178752, 196, 14, 1)), T([128, 152, 14, 14], f16)), {})
cnt: 2, ((T([128, 304, 14, 14], f16, stride=(178752, 196, 14, 1)), T([128, 304, 14, 14], f16)), {})
cnt: 1, ((T([128, 152, 14, 14], f16, stride=(119168, 196, 14, 1)), T([128, 152, 14, 14], f16)), {})
cnt: 1, ((T([128, 304, 14, 14], f16, stride=(119168, 196, 14, 1)), T([128, 304, 14, 14], f16)), {})
cnt: 1, ((T([128, 72, 28, 28], f16, stride=(338688, 784, 28, 1)), T([128, 72, 28, 28], f16)), {})
cnt: 2, ((T([128, 144, 28, 28], f16, stride=(338688, 784, 28, 1)), T([128, 144, 28, 28], f16)), {})
cnt: 1, ((T([128, 72, 28, 28], f16, stride=(225792, 784, 28, 1)), T([128, 72, 28, 28], f16)), {})
cnt: 1, ((T([128, 144, 28, 28], f16, stride=(225792, 784, 28, 1)), T([128, 144, 28, 28], f16)), {})
cnt: 1, ((T([128, 32, 56, 56], f16, stride=(602112, 3136, 56, 1)), T([128, 32, 56, 56], f16)), {})
cnt: 2, ((T([128, 64, 56, 56], f16, stride=(602112, 3136, 56, 1)), T([128, 64, 56, 56], f16)), {})
cnt: 1, ((T([128, 32, 56, 56], f16, stride=(401408, 3136, 56, 1)), T([128, 32, 56, 56], f16)), {})
cnt: 1, ((T([128, 64, 56, 56], f16, stride=(401408, 3136, 56, 1)), T([128, 64, 56, 56], f16)), {})
Operator: aten.add_.Tensor
cnt: 41, ((T([], i64), 1), {})
Operator: aten.addmm.default
cnt: 1, ((T([1000], f16), T([128, 1024], f16), T([1024, 1000], f16, stride=(1, 1024))), {})
Operator: aten.cat.default
cnt: 1, (([T([128, 64, 56, 56], f16), T([128, 32, 56, 56], f16), T([128, 32, 56, 56], f16)], 1), {})
cnt: 1, (([T([128, 64, 56, 56], f16), T([128, 32, 56, 56], f16), T([128, 32, 56, 56], f16), T([128, 64, 56, 56], f16)], 1), {})
cnt: 1, (([T([128, 144, 28, 28], f16), T([128, 72, 28, 28], f16), T([128, 72, 28, 28], f16)], 1), {})
cnt: 1, (([T([128, 144, 28, 28], f16), T([128, 72, 28, 28], f16), T([128, 72, 28, 28], f16), T([128, 144, 28, 28], f16)], 1), {})
cnt: 1, (([T([128, 304, 14, 14], f16), T([128, 152, 14, 14], f16), T([128, 152, 14, 14], f16)], 1), {})
cnt: 1, (([T([128, 304, 14, 14], f16), T([128, 152, 14, 14], f16), T([128, 152, 14, 14], f16), T([128, 304, 14, 14], f16)], 1), {})
Operator: aten.clone.default
cnt: 1, ((T([128, 3, 224, 224], f16),), {})
Operator: aten.convolution.default
cnt: 1, ((T([128, 3, 224, 224], f16), T([32, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 32, 112, 112], f16), T([64, 32, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 64, 56, 56], f16), T([64, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 64, 56, 56], f16), T([32, 64, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 32, 56, 56], f16), T([64, 32, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 128, 56, 56], f16), T([64, 128, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 64, 56, 56], f16), T([64, 64, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 192, 56, 56], f16), T([128, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 128, 56, 56], f16), T([144, 128, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 144, 28, 28], f16), T([144, 144, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 144, 28, 28], f16), T([72, 144, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 72, 28, 28], f16), T([144, 72, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 288, 28, 28], f16), T([144, 288, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 144, 28, 28], f16), T([144, 144, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 432, 28, 28], f16), T([288, 432, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 288, 28, 28], f16), T([304, 288, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 304, 14, 14], f16), T([304, 304, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 304, 14, 14], f16), T([152, 304, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 152, 14, 14], f16), T([304, 152, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 608, 14, 14], f16), T([304, 608, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 304, 14, 14], f16), T([304, 304, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 912, 14, 14], f16), T([480, 912, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 480, 14, 14], f16), T([960, 480, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 960, 7, 7], f16), T([1024, 960, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 1024, 7, 7], f16), T([1280, 1024, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 1280, 4, 4], f16), T([1024, 1280, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
Operator: aten.convolution_backward.default
cnt: 1, ((T([128, 1024, 4, 4], f16), T([128, 1280, 4, 4], f16), T([1024, 1280, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 1280, 4, 4], f16), T([128, 1024, 7, 7], f16), T([1280, 1024, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 1024, 7, 7], f16), T([128, 960, 7, 7], f16), T([1024, 960, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 960, 7, 7], f16), T([128, 480, 14, 14], f16), T([960, 480, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 480, 14, 14], f16), T([128, 912, 14, 14], f16), T([480, 912, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 4, ((T([128, 152, 14, 14], f16), T([128, 304, 14, 14], f16), T([152, 304, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 304, 14, 14], f16), T([128, 152, 14, 14], f16), T([304, 152, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 304, 14, 14], f16), T([128, 304, 14, 14], f16), T([304, 304, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 304, 14, 14], f16), T([128, 304, 14, 14], f16), T([304, 304, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 304, 14, 14], f16), T([128, 608, 14, 14], f16), T([304, 608, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 304, 14, 14], f16), T([128, 288, 28, 28], f16), T([304, 288, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 288, 28, 28], f16), T([128, 432, 28, 28], f16), T([288, 432, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 4, ((T([128, 72, 28, 28], f16), T([128, 144, 28, 28], f16), T([72, 144, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 144, 28, 28], f16), T([128, 72, 28, 28], f16), T([144, 72, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 144, 28, 28], f16), T([128, 144, 28, 28], f16), T([144, 144, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 144, 28, 28], f16), T([128, 144, 28, 28], f16), T([144, 144, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 144, 28, 28], f16), T([128, 288, 28, 28], f16), T([144, 288, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 144, 28, 28], f16), T([128, 128, 56, 56], f16), T([144, 128, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 128, 56, 56], f16), T([128, 192, 56, 56], f16), T([128, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 4, ((T([128, 32, 56, 56], f16), T([128, 64, 56, 56], f16), T([32, 64, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 64, 56, 56], f16), T([128, 32, 56, 56], f16), T([64, 32, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 64, 56, 56], f16), T([128, 64, 56, 56], f16), T([64, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 64, 56, 56], f16), T([128, 64, 56, 56], f16), T([64, 64, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 64, 56, 56], f16), T([128, 128, 56, 56], f16), T([64, 128, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 64, 56, 56], f16), T([128, 32, 112, 112], f16), T([64, 32, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 32, 112, 112], f16), T([128, 3, 224, 224], f16), T([32, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})
Operator: aten.copy_.default
cnt: 1, ((T([128, 3, 224, 224], f16), T([128, 3, 224, 224], f16)), {})
Operator: aten.div.Scalar
cnt: 1, ((T([128, 1024, 4, 4], f16, stride=(1024, 1, 0, 0)), 16), {})
Operator: aten.lift_fresh_copy.default

```

----------------------------------------

TITLE: Discovering All Unconvertible ATen Ops (Part 2: Printing Results)
DESCRIPTION: Prints the set of discovered unconvertible ATen operators returned by the unconvertible_ops utility. This provides a list of operators that need custom handling or cannot be exported directly.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_35

LANGUAGE: Python
CODE:
```
print(set(unconvertible_ops))
```

----------------------------------------

TITLE: Defining an Output Node in FX Graph (Python)
DESCRIPTION: This snippet outlines how to represent an output (return) node in an FX graph using FX's pseudo-Python format. The output node designates which node result is returned by the graph function, and is always placed last. This construct semantically mirrors a Python function's return statement, encapsulating the FX graph's termination point.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
output[](args = (%something, …))
```

----------------------------------------

TITLE: Executing Unsafe View Operation in PyTorch
DESCRIPTION: Reshapes tensors in an unsafe manner without copying data premises, intending to alter the view of tensors for efficient computations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/XGLMForCausalLM_training.txt#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
"""Operator: aten._unsafe_view.default\ncnt: 72, ((T([2, 128, 16, 64], f16), [2, 128, 1024]), {})\ncnt: 1, ((T([256, 256008], f16), [2, 128, 256008]), {})\ncnt: 24, ((T([2, 16, 128, 64], f16), [32, 128, 64]), {})\ncnt: 24, ((T([2, 128, 1024], f16), [256, 1024]), {})"""
```

----------------------------------------

TITLE: Defining a Custom IterDataPipe in Python
DESCRIPTION: Creates an example IterDataPipe class called ExampleIterPipe that yields a range of integers. This class is used in subsequent examples to demonstrate various DataPipe operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
# Example IterDataPipe
class ExampleIterPipe(IterDataPipe):
    def __init__(self, range = 20):
        self.range = range
    def __iter__(self):
        for i in range(self.range):
            yield i
```

----------------------------------------

TITLE: Example Function for Delta Debugging
DESCRIPTION: Sample function showing how delta debugging works by promoting intermediate nodes to inputs to minimize the graph.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/minifier.ipynb#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
def f(a):
    b = x * 2
    c = b + 3
    d = c / 4
    return d
```

LANGUAGE: python
CODE:
```
def f(a, c):
    b = x * 2
    d = c / 4
    return d
```

----------------------------------------

TITLE: Migrating Code from CUDA to Intel XPU
DESCRIPTION: Shows how to migrate CUDA code to use Intel XPU by changing device references from 'cuda' to 'xpu'. It maintains the logic intact while switching the device target.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/get_start_xpu.rst#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
# CUDA CODE
tensor = torch.tensor([1.0, 2.0]).to("cuda")

# CODE for Intel GPU
tensor = torch.tensor([1.0, 2.0]).to("xpu")
```

----------------------------------------

TITLE: Tensor Multiplication Operations in PyTorch
DESCRIPTION: This snippet shows tensor multiplication operations with scalar values. It includes operations on tensors of various shapes and dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
Operator: aten.mul.Tensor
cnt: 4, ((T([64, 6, 196, 9, 9], f16, stride=(95256, 81, 486, 9, 1)), 0.1767766952966369), {})
cnt: 28, ((T([64, 12, 196, 196], f16), 0.1767766952966369), {})
cnt: 4, ((T([64, 12, 1, 32], f16), 0.1767766952966369), {})
cnt: 2, ((T([64, 1000], f16), 0.5), {})
cnt: 4, ((T([64, 6, 196, 9, 9], f16), 0.1767766952966369), {})
```

----------------------------------------

TITLE: Raw DataFrame Iterator Usage
DESCRIPTION: Example of using raw_iterator() to iterate over DataFrame contents directly.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/dataframes_pipes.ipynb#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
dp = get_dataframes_pipe()
for i in dp.raw_iterator():
    print(i)
```

----------------------------------------

TITLE: Analyzing PyTorch SiLU Backward Operation Calls
DESCRIPTION: Logs of aten.silu_backward.default operations with tensor shapes and data types. Each line shows the count, tensor shapes ([batch_size, channels, height, width]), and data type (f16 for float16).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
Operator: aten.silu_backward.default
cnt: 1, ((T([128, 1280, 7, 7], f16), T([128, 1280, 7, 7], f16)), {})
cnt: 1, ((T([128, 1044, 7, 7], f16), T([128, 1044, 7, 7], f16)), {})
cnt: 1, ((T([128, 972, 7, 7], f16), T([128, 972, 7, 7], f16)), {})
cnt: 1, ((T([128, 906, 7, 7], f16), T([128, 906, 7, 7], f16)), {})
cnt: 1, ((T([128, 840, 7, 7], f16), T([128, 840, 7, 7], f16)), {})
cnt: 1, ((T([128, 768, 14, 14], f16), T([128, 768, 14, 14], f16)), {})
cnt: 1, ((T([128, 702, 14, 14], f16), T([128, 702, 14, 14], f16)), {})
cnt: 1, ((T([128, 636, 14, 14], f16), T([128, 636, 14, 14], f16)), {})
cnt: 1, ((T([128, 570, 14, 14], f16), T([128, 570, 14, 14], f16)), {})
cnt: 1, ((T([128, 504, 14, 14], f16), T([128, 504, 14, 14], f16)), {})
cnt: 1, ((T([128, 432, 14, 14], f16), T([128, 432, 14, 14], f16)), {})
cnt: 1, ((T([128, 366, 28, 28], f16), T([128, 366, 28, 28], f16)), {})
cnt: 1, ((T([128, 300, 28, 28], f16), T([128, 300, 28, 28], f16)), {})
cnt: 1, ((T([128, 228, 56, 56], f16), T([128, 228, 56, 56], f16)), {})
cnt: 1, ((T([128, 162, 56, 56], f16), T([128, 162, 56, 56], f16)), {})
cnt: 1, ((T([128, 96, 112, 112], f16), T([128, 96, 112, 112], f16)), {})
cnt: 1, ((T([128, 32, 112, 112], f16), T([128, 32, 112, 112], f16)), {})
```

----------------------------------------

TITLE: Defining Constants in TorchScript Using Final Annotation
DESCRIPTION: This example shows how to mark an attribute of a ScriptModule as constant using the Final[T] annotation. The constant value is computed in the constructor and can be used in the forward method.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
import torch
import torch.nn as nn

class Foo(nn.Module):
    # `Final` from the `typing_extensions` module can also be used
    a : torch.jit.Final[int]

    def __init__(self):
        super().__init__()
        self.a = 1 + 4

    def forward(self, input):
        return self.a + input

f = torch.jit.script(Foo())
```

----------------------------------------

TITLE: Customizing TorchVision Models with Modified BatchNorm in PyTorch
DESCRIPTION: Configures a torchvision ResNet18 model with a custom norm_layer that doesn't track running statistics, making it compatible with functorch's vmap operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/batch_norm.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torchvision
from functools import partial
torchvision.models.resnet18(norm_layer=partial(BatchNorm2d, track_running_stats=False))
```

----------------------------------------

TITLE: Using torch._dynamo.disallow_in_graph for Operators
DESCRIPTION: Example demonstrating how to disallow specific operators from being included in the TorchDynamo graph, causing a graph break and fallback to eager mode execution.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_fine_grain_apis.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
torch._dynamo.disallow_in_graph(torch.ops.custom_op)
```

----------------------------------------

TITLE: Matrix Multiplication and Adding in PyTorch
DESCRIPTION: The 'aten.addmm.default' operation in PyTorch combines matrix multiplication with addition, a common neural network operation. Inputs comprise tensors for the bias, input, and weight matrices, where the result is calculated as bias plus the product of the input and weight matrices. It assumes half-precision tensor operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v2_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
Operator: aten.addmm.default
cnt: 1, ((T([1000], f16), T([96, 1280], f16), T([1280, 1000], f16, stride=(1, 1280))), {})
```

----------------------------------------

TITLE: Importing and Using torch.__config__ Module in Python
DESCRIPTION: This snippet demonstrates how to import and use the torch.__config__ module. It includes two main functions: 'show' for displaying PyTorch configuration and 'parallel_info' for obtaining parallel processing information.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/config_mod.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
import torch

# Show PyTorch configuration
torch.__config__.show()

# Get parallel processing information
parallel_info = torch.__config__.parallel_info()
```

----------------------------------------

TITLE: Ensembling Models using functorch.combine_state_for_ensemble and vmap in Python
DESCRIPTION: Shows how to perform model ensembling using the legacy `functorch` library. `functorch.combine_state_for_ensemble` takes a list of models and stacks their parameters and buffers, returning a functional model `fmodel` and the stacked state (`params`, `buffers`). `functorch.vmap` is then used to apply `fmodel` in parallel across the ensemble dimension (first dimension of `params` and `buffers`), processing the same input `data`. Requires `torch` and `functorch`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.migrating.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import torch
num_models = 5
batch_size = 64
in_features, out_features = 3, 3
models = [torch.nn.Linear(in_features, out_features) for i in range(num_models)]
data = torch.randn(batch_size, 3)

# ---------------
# using functorch
# ---------------
import functorch
fmodel, params, buffers = functorch.combine_state_for_ensemble(models)
output = functorch.vmap(fmodel, (0, 0, None))(params, buffers, data)
assert output.shape == (num_models, batch_size, out_features)
```

----------------------------------------

TITLE: Analyzing Convolution Operations in PyTorch
DESCRIPTION: This snippet shows various convolution operations with different input and output tensor shapes, kernel sizes, strides, and padding. The operations are represented in a compact format, likely for performance analysis or optimization purposes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hrnet_w18_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([128, 128, 56, 56], f16), T([128, 18, 56, 56], f16), T([128, 18, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

----------------------------------------

TITLE: Constructing Sparse COO Tensor in PyTorch
DESCRIPTION: Creates a sparse COO tensor by providing indices and values, demonstrating the difference between COO and dense representations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
i = [[0, 1, 1],
     [2, 0, 2]]
v =  [3, 4, 5]
s = torch.sparse_coo_tensor(i, v, (2, 3))
s.to_dense()
```

----------------------------------------

TITLE: Disabling JIT Optimizations via Python API
DESCRIPTION: Provides the Python function call to globally disable most runtime optimizations performed by the TorchScript JIT graph executor in the current process. This is useful for debugging compilation issues by running with minimal optimizations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_29

LANGUAGE: Python
CODE:
```
torch._C._get_graph_executor_optimize(False)
```

----------------------------------------

TITLE: Computing and Verifying Per-Sample Gradients using functorch
DESCRIPTION: Computes the per-sample gradients efficiently by calling the `vmap`-transformed function `ft_compute_sample_grad` with the model's `params`, `buffers`, and the entire batches of `data` and `targets`. It then verifies the correctness of this `functorch` approach by comparing its output (`ft_per_sample_grads`) element-wise with the results from the naive loop-based method (`per_sample_grads`) using `torch.allclose` with specified tolerances (`atol`, `rtol`).
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
ft_per_sample_grads = ft_compute_sample_grad(params, buffers, data, targets)

# we can double check that the results using functorch grad and vmap match the results of hand processing each one individually:
for per_sample_grad, ft_per_sample_grad in zip(per_sample_grads, ft_per_sample_grads):
    assert torch.allclose(per_sample_grad, ft_per_sample_grad, atol=3e-3, rtol=1e-5)
```

----------------------------------------

TITLE: Basic Name Inference for Tensor Addition
DESCRIPTION: Explains name inference rules through adding one-dimensional tensors, showcasing scenarios of matching and unifying names, where tensors x, y, and z are combined with match checks and potential errors if names do not align.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/named_tensor.rst#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
x = torch.randn(3, names=('X',))
y = torch.randn(3)
z = torch.randn(3, names=('Z',))
# x + y  # match('X', None) is True
# x + z  # match('X', 'Z') is False
# x + x  # match('X', 'X') is True

x + z

(x + y).names
(x + x).names
```

----------------------------------------

TITLE: PyTorch Tensor Operations - Core Functions
DESCRIPTION: Collection of fundamental PyTorch tensor operations including softmax, log_softmax, view operations, and basic arithmetic operations. Operations are performed on tensors with various shapes and primarily use float16 (f16) precision.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/AllenaiLongformerBase_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
# Sample tensor operations
aten._log_softmax.default(T([1024, 50265], f16), 1, False)
aten._softmax.default(T([1, 1024, 12, 513], f16), -1, True)
aten._unsafe_view.default(T([12, 3, 512, 64, 1], f16), [36, 512, 64])
aten.add.Tensor(T([1, 1024, 768], f16), T([1, 1024, 768], f16))
```

----------------------------------------

TITLE: Computing Jacobian with Forward-Mode AD using torch.func.jacfwd in Python
DESCRIPTION: This snippet introduces `torch.func.jacfwd` as an alternative to `jacrev` for computing Jacobians, but using forward-mode automatic differentiation. It computes the Jacobian of `torch.sin` for an input vector `x` and verifies the result against the expected diagonal matrix containing `torch.cos(x)`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.whirlwind_tour.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from torch.func import jacfwd
x = torch.randn(5)
jacobian = jacfwd(torch.sin)(x)
expected = torch.diag(torch.cos(x))
assert torch.allclose(jacobian, expected)
```

----------------------------------------

TITLE: Embedding Lookups with embedding in PyTorch (Python)
DESCRIPTION: Performs aten.embedding, useful for embedding lookup tables used in natural language processing, where it maps indices to vector representations, facilitating the input of categorical data into a neural network.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_12

LANGUAGE: Python
CODE:
```
aten.embedding.default
cnt: 1, ((T([30522, 768], f16), T([16, 512], i64), 0), {})
cnt: 1, ((T([512, 768], f16), T([1, 512], i64)), {})
cnt: 4, ((T([1024, 768], f16), T([16, 512], i64, stride=(2048, 4))), {})
cnt: 2, ((T([1024, 768], f16), T([16, 512], i64)), {})
cnt: 1, ((T([2, 768], f16), T([16, 512], i64)), {})
```

----------------------------------------

TITLE: Using torch.distributed.rpc.remote() in TorchScript
DESCRIPTION: Executes a remote call on a worker and gets a Remote Reference RRef as the return value.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_35

LANGUAGE: python
CODE:
```
torch.distributed.rpc.remote()
```

----------------------------------------

TITLE: Defining Valid DataPipe Classes in Python
DESCRIPTION: Shows valid DataPipe class definitions with correct type hints and subtyping for the __iter__ method.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/typing.ipynb#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
class DP(IterDataPipe[Tuple]):
    def __iter__(self) -> Iterator[Tuple[int, str]]:
        pass
```

LANGUAGE: python
CODE:
```
class DP(IterDataPipe):
    def __iter__(self) -> Iterator[int]:
        pass
```

LANGUAGE: python
CODE:
```
class DP(IterDataPipe):
    def __iter__(self):
        pass
print(DP.type)
class DP(IterDataPipe):
    def __iter__(self) -> Iterator:
        pass
print(DP.type)
class DP(IterDataPipe):
    def __iter__(self) -> Iterator[T_co]:
        pass
print(DP.type)
```

LANGUAGE: python
CODE:
```
class DP(IterDataPipe[Tuple[T_co, str]]):
    def __iter__(self) -> Iterator[Tuple[T_co, str]]:
        pass
print(DP.type)

T = TypeVar('T', int, str)  # equals to Union[int, str]
class DP(IterDataPipe[Tuple[T, str]]):
    def __iter__(self) -> Iterator[Tuple[Union[int, str], str]]:
        pass
print(DP.type)
```

----------------------------------------

TITLE: Modeling Control Flow and Attribute Access in an FX Graph (Python)
DESCRIPTION: This extended Python snippet represents a full FX graph for a control flow model using get_attr and call_function nodes. It illustrates creating placeholders for inputs, retrieving subgraph attributes, invoking a higher-order conditional using those graph pieces, and returning the result. The graph provides a realistic sketch of FX node interconnections when modeling functional control flow as in functorch's cond operator.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
graph():
    %x_1 : [num_users=1] = placeholder[target=x_1]
    %y_1 : [num_users=1] = placeholder[target=y_1]
    %true_graph_0 : [num_users=1] = get_attr[target=true_graph_0]
    %false_graph_0 : [num_users=1] = get_attr[target=false_graph_0]
    %conditional : [num_users=1] = call_function[target=torch.ops.higher_order.cond](args = (%y_1, %true_graph_0, %false_graph_0, [%x_1]), kwargs = {})
    return conditional
```

----------------------------------------

TITLE: MetadataTensor Wrapper Implementation
DESCRIPTION: Example of a tensor wrapper class that attaches metadata to tensors and propagates it through torch operations. Implements __torch_function__ to handle the full torch API.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
class MetadataTensor(object):
    def __init__(self, data, metadata=None, **kwargs):
        self._t = torch.as_tensor(data, **kwargs)
        self._metadata = metadata

    def __repr__(self):
        return "Metadata:\n{}\n\ndata:\n{}".format(self._metadata, self._t)

    @classmethod
    def __torch_function__(cls, func, types, args=(), kwargs=None):
        if kwargs is None:
            kwargs = {}
        metadatas = tuple(a._metadata for a in args if hasattr(a, '_metadata'))
        args = [getattr(a, '_t', a) for a in args]
        assert len(metadatas) > 0
        ret = func(*args, **kwargs)
        return MetadataTensor(ret, metadata=metadatas[0])
```

----------------------------------------

TITLE: Building PyTorch Docker Image from Source Bash
DESCRIPTION: Uses the provided docker.Makefile within the PyTorch source directory to build a custom Docker image. This command compiles PyTorch within the Docker environment based on the configuration specified in the Makefile and Dockerfile.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_20

LANGUAGE: Bash
CODE:
```
make -f docker.Makefile
```

----------------------------------------

TITLE: Importing IterDataPipe from PyTorch
DESCRIPTION: Imports the IterDataPipe class from torch.utils.data module, which is used as a base class for creating custom DataPipes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from torch.utils.data import IterDataPipe
```

----------------------------------------

TITLE: Optional Type Annotation for Local Variables
DESCRIPTION: Demonstrates annotating local variables with Optional type to handle variables that may be None or a specific type. This is necessary when a variable might have different concrete types.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
import torch

def f(a, setVal: bool):
    value: Optional[torch.Tensor] = None
    if setVal:
        value = a
    return value

ones = torch.ones([6])
m = torch.jit.script(f)
print("TorchScript:", m(ones, True), m(ones, False))
```

----------------------------------------

TITLE: Comparing Dropout Behavior with Symbolic Trace in PyTorch FX
DESCRIPTION: This code demonstrates a difference in behavior between functional and module-based dropout when using torch.fx.symbolic_trace. The functional version (torch.nn.functional.dropout) maintains its training flag during tracing, while the nn.Dropout module version properly respects the module's training state.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
dropout = torch.nn.functional.dropout(x, p = 0.5, training = True, inplace = False);  x = None
return dropout
```

LANGUAGE: python
CODE:
```
traced.eval()

x = torch.randn(5, 3)
torch.testing.assert_close(traced(x), x)
```

LANGUAGE: python
CODE:
```
class DropoutRepro2(torch.nn.Module):
  def __init__(self):
    super().__init__()
    self.drop = torch.nn.Dropout()

  def forward(self, x):
    return self.drop(x)

traced = torch.fx.symbolic_trace(DropoutRepro2())
print(traced.code)
"""
def forward(self, x):
  drop = self.drop(x);  x = None
  return drop
"""

traced.eval()

x = torch.randn(5, 3)
torch.testing.assert_close(traced(x), x)
```

----------------------------------------

TITLE: PyTorch Tensor Addition Operations in Neural Network
DESCRIPTION: Shows various tensor addition operations (aten.add.Tensor) in a neural network. The operations include adding bias terms to feature maps, element-wise addition of feature maps, and adding small epsilon values for normalization operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convnext_base_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
Operator: aten.add.Tensor
cnt: 3, ((T([32, 56, 56, 512], f16), T([512], f16)), {})
cnt: 3, ((T([32, 56, 56, 128], f16), T([128], f16)), {})
cnt: 7, ((T([32, 128, 56, 56], f16, stride=(401408, 1, 7168, 128)), T([32, 128, 56, 56], f16, stride=(401408, 1, 7168, 128))), {})
cnt: 1, ((T([32, 1, 56, 56], f16), 1e-06), {})
cnt: 1, ((T([32, 128, 56, 56], f16, stride=(401408, 1, 7168, 128)), T([128, 1, 1], f16)), {})
cnt: 3, ((T([32, 28, 28, 1024], f16), T([1024], f16)), {})
cnt: 3, ((T([32, 28, 28, 256], f16), T([256], f16)), {})
cnt: 7, ((T([32, 256, 28, 28], f16, stride=(200704, 1, 7168, 256)), T([32, 256, 28, 28], f16, stride=(200704, 1, 7168, 256))), {})
```

----------------------------------------

TITLE: Logging Aten Operator: aten.stack.default (Text)
DESCRIPTION: Log entries showing example invocations of the 'aten.stack.default' operator. Each line ('cnt') represents a call signature, detailing a list of input tensors (T) being stacked, including their shapes, data types (f16), and potentially strides.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/pit_b_224_training.txt#2025-04-22_snippet_5

LANGUAGE: text
CODE:
```
Operator: aten.stack.default
cnt: 4, (([T([64, 16, 65, 64], f16), T([64, 16, 65, 64], f16, stride=(66560, 4160, 1, 65)), T([64, 16, 65, 64], f16)],), {})
cnt: 6, (([T([64, 8, 257, 64], f16), T([64, 8, 257, 64], f16, stride=(131584, 16448, 1, 257)), T([64, 8, 257, 64], f16)],), {})
cnt: 3, (([T([64, 4, 962, 64], f16), T([64, 4, 962, 64], f16, stride=(246272, 61568, 1, 962)), T([64, 4, 962, 64], f16)],), {})
```

----------------------------------------

TITLE: Analyzing aten.native_batch_norm_backward in PyTorch
DESCRIPTION: This snippet involves the use of the PyTorch aten.native_batch_norm_backward operator, which is essential for computing gradients during the backward pass of batch normalization. The inputs include tensors of specified dimensions and data types, with True as a boolean flag and a float for epsilon. Dependencies include PyTorch's native functions for tensor operations. The outputs will be gradients essential for backpropagation. The function expects matching dimensions and appropriate data types for its execution. This operation is highly sensitive to input shapes and requires floating-point computations for precision.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 1536, 7, 7], f16), T([64, 1536, 7, 7], f16), T([1536], f16), T([1536], f16), T([1536], f16), T([1536], f32), T([1536], f32), True, 1e-05, [True, True, True]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 4, ((T([64, 264, 7, 7], f16), T([64, 264, 7, 7], f16), T([264], f16), T([264], f16), T([264], f16), T([264], f32), T([264], f32), True, 1e-05, [True, True, True]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 6, ((T([64, 1584, 7, 7], f16), T([64, 1584, 7, 7], f16), T([1584], f16), T([1584], f16), T([1584], f16), T([1584], f32), T([1584], f32), True, 1e-05, [True, True, True]), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 960, 7, 7], f16), T([64, 960, 7, 7], f16), T([960], f16), T([960], f16), T([960], f16), T([960], f32), T([960], f32), True, 1e-05, [True, True, True]), {})
```

----------------------------------------

TITLE: Tensor Division Operations
DESCRIPTION: Division operations between tensors and scalar values, including both element-wise divisions and scalar divisions with different data types.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
((T([5000], f16), 2), {})
((T([5000], f32), 2.0), {})
```

----------------------------------------

TITLE: Invoking Softmax and Log-Softmax Operations in PyTorch (Python)
DESCRIPTION: These examples present the use of PyTorch's low-level softmax, log_softmax, and their backward counterparts, with clear tensor shapes and dtypes. Dependencies include PyTorch with ATen backend enabled; inputs and outputs are FP16 tensors, typically representing model activations or gradients over batches and feature dimensions. Key parameters are the tensor, the dimension for softmax, and whether the operation is stable—output shapes match input shapes, with constraints based on tensor ranks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([508, 30522], f16), 1, False), {})
```

LANGUAGE: python
CODE:
```
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([508, 30522], f16), T([508, 30522], f16), 1, f16), {})
```

LANGUAGE: python
CODE:
```
Operator: aten._softmax.default
cnt: 12, ((T([4, 12, 128, 128], f16), -1, False), {})
```

LANGUAGE: python
CODE:
```
Operator: aten._softmax_backward_data.default
cnt: 12, ((T([4, 12, 128, 128], f16), T([4, 12, 128, 128], f16), -1, f16), {})
```

----------------------------------------

TITLE: Creating ThreadPoolExecutor for Asynchronous Model Forward Pass in PyTorch
DESCRIPTION: Implements a ThreadPoolExecutor with a single worker to asynchronously run the model's forward pass and response step. This modification aims to prevent blocking while polling the request queue.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/inference/CHANGELOG.md#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
ThreadPoolExecutor(max_workers=1)
```

----------------------------------------

TITLE: Invoking aten._softmax and aten._softmax_backward_data Operators - PyTorch - Python
DESCRIPTION: These snippets record calls to softmax and its backward pass for a tensor of shape [4, 12, 1024, 1024] with half-precision. The forward softmax is performed over axis -1 with a contiguous memory flag, and the backward variant backpropagates gradients through the softmax. Expected input/output: high-dimensional activation tensors. Dependencies: torch, GPU hardware for large data and f16. Constraints: input rank and memory requirements.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GPT2ForSequenceClassification_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
Operator: aten._softmax.default
cnt: 12, ((T([4, 12, 1024, 1024], f16), -1, False), {})
```

LANGUAGE: Python
CODE:
```
Operator: aten._softmax_backward_data.default
cnt: 12, ((T([4, 12, 1024, 1024], f16), T([4, 12, 1024, 1024], f16), -1, f16), {})
```

----------------------------------------

TITLE: Basic Custom Backend Implementation in Python
DESCRIPTION: Shows how to implement a basic custom backend function and use it with torch.compile. The backend function receives a GraphModule and example inputs, returning the forward function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_custom_backends.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch

def my_custom_backend(gm, example_inputs):
    return gm.forward

def f(...):
    ...

f_opt = torch.compile(f, backend=my_custom_backend)

@torch.compile(backend=my_custom_backend)
def g(...):
    ...
```

----------------------------------------

TITLE: Supported Non-data-dependent Control Flow with vmap
DESCRIPTION: This example demonstrates that vmap works correctly with control flow that doesn't depend on the tensor values being vmapped over, such as checking tensor dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/ux_limitations.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
def custom_dot(x):
  if x.dim() == 1:
    return torch.dot(x, x)
  return (x * x).sum()

x = torch.randn(3)
vmap(custom_dot)(x)
```

----------------------------------------

TITLE: Tensor Division Operations
DESCRIPTION: Division operations between tensors and scalar values, handling various tensor shapes and broadcast patterns.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
((T([3, 427, 640], f16, stride=(1, 1920, 3)), T([3, 1, 1], f16)), {})
```

----------------------------------------

TITLE: Debugging FX Generated Code with PDB
DESCRIPTION: Shows how to use Python debugger (pdb) to debug FX-generated forward functions in transformed modules.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
import torch
import torch.fx
import torchvision.models as models

def my_pass(inp: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:
    graph = tracer_class().trace(inp)
    # Transformation logic here
    # <...>

    # Return new Module
    return fx.GraphModule(inp, graph)

my_module = models.resnet18()
my_module_transformed = my_pass(my_module)

input_value = torch.randn(5, 3, 224, 224)

import pdb; pdb.set_trace()

my_module_transformed(input_value)
```

----------------------------------------

TITLE: Debugging and Configuring DDPOptimizer in TorchDynamo
DESCRIPTION: Instructions for debugging DDPOptimizer by setting environment variables to control log verbosity, and how to disable the optimization if needed. TORCH_LOGS can be set to 'ddp_graphs' for full graph dumps or other values for basic information.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/ddp.rst#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
TORCH_LOGS='ddp_graphs'
```

LANGUAGE: python
CODE:
```
torch._dynamo.config.optimize_ddp=False
```

----------------------------------------

TITLE: Multiplying Tensor by Scalar - PyTorch Aten
DESCRIPTION: Performs element-wise multiplication of a tensor by a scalar value. This internal operator handles scalar scaling of tensors. It takes the input tensor and a scalar.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BartForConditionalGeneration_training.txt#_snippet_17

LANGUAGE: Python
CODE:
```
import torch

input_tensor = torch.randn(2, 1024, 1024, dtype=torch.float16)
result = torch.mul(input_tensor, 1.0)
```

----------------------------------------

TITLE: Defining a Simple MLP Model with PyTorch - Python
DESCRIPTION: Defines a simple multilayer perceptron (MLP) as a subclass of nn.Module in PyTorch with three fully connected layers and ReLU activations. The network expects input of shape [*, 784] (flattened 28x28), and outputs logits for 10 classes. No external dependencies are needed beyond PyTorch, and the architecture is designed for tasks like MNIST digit classification.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/ensembling.ipynb#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
# Here's a simple MLP
class SimpleMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, 10)

    def forward(self, x):
        x = x.flatten(1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        x = F.relu(x)
        x = self.fc3(x)
        return x

```

----------------------------------------

TITLE: Declaring ATen Functions with Argument and Return Type Details
DESCRIPTION: The snippet illustrates the declaration format of ATen functions, highlighting the permissible argument and return types. It outlines the syntax for specifying function signatures, including overloaded functions and the use of namespaces. Dependencies include familiarity with C++ and PyTorch's code generation conventions. Key parameters include function names, argument types (like Tensor, int, float), and return types (like Tensor, Tensor[]). The functions are primarily designed for C++ integration in PyTorch, enabling complex type mappings and function overloading.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_1

LANGUAGE: markdown
CODE:
```
\n- func: func_name[.overload_name](ArgType arg0[=default], ArgType arg1[=default], ...) -> Return\n
```

----------------------------------------

TITLE: Tracking NLL Loss Operations in PyTorch
DESCRIPTION: Records of negative log likelihood loss forward and backward operations. Shows tensor shapes for input predictions, target labels, and loss values with parameters for reduction and ignore index.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tinynet_a_training.txt#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
Operator: aten.nll_loss_backward.default
cnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})
Operator: aten.nll_loss_forward.default
cnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})
```

----------------------------------------

TITLE: Applying Native Batch Normalization (aten.native_batch_norm.default) in PyTorch (Python)
DESCRIPTION: Represents multiple invocations of the native batch normalization operator, each on float16 tensors of NCHW layout and multiple channel sizes. Requires input, weight (gamma), bias (beta), running mean/var, a boolean for training mode, epsilon, and momentum. Used for layer normalization in deep neural networks in float16, dependent on PyTorch batch norm APIs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mnasnet1_0_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
Operator: aten.native_batch_norm.default
cnt: 2, ((T([32, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), False, 0.00029999999999996696, 1e-05), {})
cnt: 1, ((T([32, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), False, 0.00029999999999996696, 1e-05), {})
cnt: 1, ((T([32, 48, 112, 112], f16), T([48], f16), T([48], f16), T([48], f16), T([48], f16), False, 0.00029999999999996696, 1e-05), {})
cnt: 1, ((T([32, 48, 56, 56], f16), T([48], f16), T([48], f16), T([48], f16), T([48], f16), False, 0.00029999999999996696, 1e-05), {})
cnt: 3, ((T([32, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), False, 0.00029999999999996696, 1e-05), {})
cnt: 5, ((T([32, 72, 56, 56], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f16), False, 0.00029999999999996696, 1e-05), {})
cnt: 1, ((T([32, 72, 28, 28], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f16), False, 0.00029999999999996696, 1e-05), {})
cnt: 3, ((T([32, 40, 28, 28], f16), T([40], f16), T([40], f16), T([40], f16), T([40], f16), False, 0.00029999999999996696, 1e-05), {})
cnt: 4, ((T([32, 120, 28, 28], f16), T([120], f16), T([120], f16), T([120], f16), T([120], f16), False, 0.00029999999999996696, 1e-05), {})
cnt: 1, ((T([32, 240, 28, 28], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f16), False, 0.00029999999999996696, 1e-05), {})
cnt: 1, ((T([32, 240, 14, 14], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f16), False, 0.00029999999999996696, 1e-05), {})
cnt: 3, ((T([32, 80, 14, 14], f16), T([80], f16), T([80], f16), T([80], f16), T([80], f16), False, 0.00029999999999996696, 1e-05), {})
cnt: 6, ((T([32, 480, 14, 14], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f16), False, 0.00029999999999996696, 1e-05), {})
cnt: 2, ((T([32, 96, 14, 14], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), False, 0.00029999999999996696, 1e-05), {})
cnt: 3, ((T([32, 576, 14, 14], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f16), False, 0.00029999999999996696, 1e-05), {})
cnt: 1, ((T([32, 576, 7, 7], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f16), False, 0.00029999999999996696, 1e-05), {})
cnt: 4, ((T([32, 192, 7, 7], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), False, 0.00029999999999996696, 1e-05), {})
cnt: 8, ((T([32, 1152, 7, 7], f16), T([1152], f16), T([1152], f16), T([1152], f16), T([1152], f16), False, 0.00029999999999996696, 1e-05), {})
cnt: 1, ((T([32, 320, 7, 7], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f16), False, 0.00029999999999996696, 1e-05), {})
cnt: 1, ((T([32, 1280, 7, 7], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f16), False, 0.00029999999999996696, 1e-05), {})
```

----------------------------------------

TITLE: Generating Dummy Data for Model Evaluation - Python
DESCRIPTION: Initializes test data with random tensors using the previously set random seed for reproducibility. Defines weight matrix, bias vector, and input feature vector x with dimension D=16. No external dependencies besides PyTorch. Output is a set of initialized tensors for subsequent computations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
D = 16
weight = torch.randn(D, D)
bias = torch.randn(D)
x = torch.randn(D) # feature vector
```

----------------------------------------

TITLE: Custom NumPy Sort Function with Manual vmap Implementation
DESCRIPTION: Implementation of a custom sorting function using NumPy with manual vmap support. Includes forward, backward, setup_context, and vmap methods with proper tensor-numpy conversion handling.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.func.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
def to_numpy(tensor):
    return tensor.cpu().numpy()

class NumpySort(torch.autograd.Function):
    @staticmethod
    def forward(x, dim):
        device = x.device
        x = to_numpy(x)
        ind = np.argsort(x, axis=dim)
        ind_inv = np.argsort(ind, axis=dim)
        result = np.take_along_axis(x, ind, axis=dim)
        return (
            torch.tensor(result, device=device),
            torch.tensor(ind, device=device),
            torch.tensor(ind_inv, device=device),
        )

    @staticmethod
    def setup_context(ctx, inputs, output):
        x, dim = inputs
        _, ind, ind_inv = output
        ctx.mark_non_differentiable(ind, ind_inv)
        ctx.save_for_backward(ind, ind_inv)
        ctx.dim = dim

    @staticmethod
    def backward(ctx, grad_output, _0, _1):
        ind, ind_inv = ctx.saved_tensors
        return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None

    @staticmethod
    def vmap(info, in_dims, x, dim):
        x_bdim, _ = in_dims
        x = x.movedim(x_bdim, 0)
        dim = dim if dim >= 0 else dim + x.dim() - 1
        result = NumpySort.apply(x, dim + 1)
        return NumpySort.apply(x, dim + 1), (0, 0, 0)
```

----------------------------------------

TITLE: Building PyTorch Libraries (Python)
DESCRIPTION: The build_pytorch_libs.py script is a cross-platform tool that builds all constituent libraries of PyTorch, excluding the PyTorch Python extension.
SOURCE: https://github.com/pytorch/pytorch/blob/main/tools/README.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
python build_pytorch_libs.py
```

----------------------------------------

TITLE: Passing Remote Reference as Argument PyTorch RPC Python
DESCRIPTION: This code block demonstrates how a user worker (A) creates an RRef on an owner worker (B) using `rpc.remote` and subsequently passes this RRef as an argument in another asynchronous RPC call (`rpc.rpc_async`) back to the *same* owner worker (B). It includes a placeholder function `func` to illustrate the remote execution context. This highlights the scenario where an RRef is shared via an RPC argument. Requires PyTorch and the distributed RPC framework.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/rref.rst#_snippet_1

LANGUAGE: Python
CODE:
```
import torch
import torch.distributed.rpc as rpc

# on worker A and worker B
def func(rref):
  pass

# on worker A
rref = rpc.remote('B', torch.add, args=(torch.ones(2), 1))
# say the rref has RRefId 100 and ForkId 1
rpc.rpc_async('B', func, args=(rref, ))
```

----------------------------------------

TITLE: Embedding operation in PyTorch
DESCRIPTION: The aten.embedding.default operator maps an input index tensor to embeddings from a weight matrix [128100, 1536], used widely in NLP tasks for converting indices to dense vectors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_10

LANGUAGE: Python
CODE:
```
Operator: aten.embedding.default
cnt: 1, ((T([128100, 1536], f16), T([1, 512], i64), 0), {})
```

----------------------------------------

TITLE: Using the Hessian Convenience Function in PyTorch
DESCRIPTION: Demonstrates using the hessian convenience function that combines jacfwd and jacrev transforms.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/whirlwind_tour.ipynb#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from functorch import hessian

def f(x):
  return x.sin().sum()

x = torch.randn(5)
hess = hessian(f)(x)
```

----------------------------------------

TITLE: vmap with torch.nonzero Causing Dynamic Shape Error - PyTorch - Python
DESCRIPTION: This snippet illustrates the failure mode that occurs when vmap is used with PyTorch operations returning dynamic shapes (e.g., torch.nonzero). Since each 'example' may yield a tensor of a different shape, vmap cannot stack the results, leading to an error. Dependencies: PyTorch, torch.func. Limitation: Avoid using ops that may emit ragged or variably-shaped outputs inside vmap.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
xs = torch.tensor([[0, 1, 2], [0, 0, 3]])
vmap(torch.nonzero)(xs)
```

----------------------------------------

TITLE: Initializing Lazy Tensor Backend in PyTorch
DESCRIPTION: This snippet shows how to initialize the TorchScript-based backend for Lazy Tensor, which is necessary for running traced graphs with Lazy Tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import torch._lazy
import torch._lazy.ts_backend
torch._lazy.ts_backend.init()
```

----------------------------------------

TITLE: Type Checking in TorchScript with torch.jit.isinstance()
DESCRIPTION: Returns a boolean indicating whether a variable is of the specified type. Used for type refinement in TorchScript.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_43

LANGUAGE: python
CODE:
```
torch.jit.isinstance()
```

----------------------------------------

TITLE: Generating Python Code from FX IR in PyTorch
DESCRIPTION: This code snippet demonstrates how FX generates valid Python source code based on the IR it is instantiated with. It shows the forward method of a traced module, including operations like addition, linear transformation, and clamping.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/fx/README.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
def forward(self, x):
    param = self.param
    add_1 = x + param;  x = param = None
    linear_1 = self.linear(add_1);  add_1 = None
    clamp_1 = linear_1.clamp(min = 0.0, max = 1.0);  linear_1 = None
    return clamp_1
```

----------------------------------------

TITLE: Calling aten.masked_fill_.Scalar (Python)
DESCRIPTION: Fills elements of the tensor in-place with a scalar value where the corresponding element in a mask tensor is True. Examples show filling an int64 tensor with 1 and a float32 tensor with 0 based on boolean masks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_21

LANGUAGE: Python
CODE:
```
((T([8, 128], i64), T([8, 128], b8), 1), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 128], f32), T([128, 128], b8), 0), {})
```

----------------------------------------

TITLE: Scalar Division Operations in PyTorch
DESCRIPTION: Records of division operations where a tensor is divided by a scalar value. This specific operation divides a tensor with dimensions [128, 1280, 7, 7] by the scalar value 49.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/spnasnet_100_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
Operator: aten.div.Scalar
cnt: 1, ((T([128, 1280, 7, 7], f16, stride=(1280, 1, 0, 0)), 49), {})
```

----------------------------------------

TITLE: Defining BackendConfig for Quantization Patterns in PyTorch (Python)
DESCRIPTION: Illustrates the programmatic definition of a `BackendConfig` object for a pattern such as `nniqat.LinearReLU` in PyTorch quantization. It shows how to configure the observation type (e.g., separate input/output observers) and specify supported data type combinations for inputs, outputs, weights, and biases.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#_snippet_6

LANGUAGE: Python
CODE:
```
BackendConfig(nniqat.LinearReLU)
    .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT)
    .set_dtype_configs([
        DTypeConfig(input_dtype=torch.quint8, output_dtype = torch.quint8, weight_dtype = torch.qint8, bias_dtype = torch.float32)]
    )
```

----------------------------------------

TITLE: Trace PyTorch ELU Module to Inspect Graph (Python)
DESCRIPTION: Shows Python code using `torch.jit.trace` to trace an instance of the `torch.nn.ELU` module with a dummy input. The `.graph` attribute is then printed to inspect the TorchScript graph, revealing the underlying ATen operator (`aten::elu`).
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_16

LANGUAGE: python
CODE:
```
print(
    torch.jit.trace(
        torch.nn.ELU(), # module
        torch.ones(1)   # example input
    ).graph
)
```

----------------------------------------

TITLE: Defining PyTorch Model Entrypoint in Python
DESCRIPTION: This snippet defines an entrypoint for the ResNet18 model in PyTorch, detailing how to load pretrained weights and dependencies using a Python function. It explains how the entrypoint integrates with torchvision and emphasizes the use of documentation within entrypoints.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/hub.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
dependencies = ['torch']
from torchvision.models.resnet import resnet18 as _resnet18

# resnet18 is the name of entrypoint
def resnet18(pretrained=False, **kwargs):
    """ # This docstring shows up in hub.help()
    Resnet18 model
    pretrained (bool): kwargs, load pretrained weights into the model
    """
    # Call the model, load pretrained weights
    model = _resnet18(pretrained=pretrained, **kwargs)
    return model
```

----------------------------------------

TITLE: Computing Batch Jacobian Using Output Summing in PyTorch
DESCRIPTION: Shows an alternative approach to compute batch Jacobians by summing outputs and using jacrev. This method works when each input produces an independent output, avoiding the need for vmap.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
def predict_with_output_summed(weight, bias, x):
    return predict(weight, bias, x).sum(0)

batch_jacobian1 = jacrev(predict_with_output_summed, argnums=2)(weight, bias, x).movedim(1, 0)
assert torch.allclose(batch_jacobian0, batch_jacobian1)
```

----------------------------------------

TITLE: Shape Propagation Interpreter for FX GraphModules
DESCRIPTION: An implementation of the interpreter pattern for FX GraphModules that records tensor shapes and dtypes during execution. This class executes each node in the graph with given arguments and stores shape and dtype information on each node for analysis purposes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
import torch
import torch.fx
from torch.fx.node import Node

from typing import Dict

class ShapeProp:
    """
    Shape propagation. This class takes a `GraphModule`.
    Then, its `propagate` method executes the `GraphModule`
    node-by-node with the given arguments. As each operation
    executes, the ShapeProp class stores away the shape and
    element type for the output values of each operation on
    the `shape` and `dtype` attributes of the operation's
    `Node`.
    """
    def __init__(self, mod):
        self.mod = mod
        self.graph = mod.graph
        self.modules = dict(self.mod.named_modules())

    def propagate(self, *args):
        args_iter = iter(args)
        env : Dict[str, Node] = {}

        def load_arg(a):
            return torch.fx.graph.map_arg(a, lambda n: env[n.name])

        def fetch_attr(target : str):
            target_atoms = target.split('.')
            attr_itr = self.mod
            for i, atom in enumerate(target_atoms):
                if not hasattr(attr_itr, atom):
                    raise RuntimeError(f"Node referenced nonexistent target {'.'.join(target_atoms[:i])}")
                attr_itr = getattr(attr_itr, atom)
            return attr_itr

        for node in self.graph.nodes:
            if node.op == 'placeholder':
                result = next(args_iter)
            elif node.op == 'get_attr':
                result = fetch_attr(node.target)
            elif node.op == 'call_function':
                result = node.target(*load_arg(node.args), **load_arg(node.kwargs))
            elif node.op == 'call_method':
                self_obj, *args = load_arg(node.args)
                kwargs = load_arg(node.kwargs)
                result = getattr(self_obj, node.target)(*args, **kwargs)
            elif node.op == 'call_module':
                result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))

            # This is the only code specific to shape propagation.
            # you can delete this `if` branch and this becomes
            # a generic GraphModule interpreter.
            if isinstance(result, torch.Tensor):
                node.shape = result.shape
                node.dtype = result.dtype

            env[node.name] = result

        return load_arg(self.graph.result)
```

----------------------------------------

TITLE: Disabling Running Stats in BatchNorm2d - PyTorch (Python)
DESCRIPTION: Configures a BatchNorm2d layer with track_running_stats set to False, preventing updates to running_mean and running_var. This is useful when using vmapped modules in Functorch, as it avoids in-place tensor modification errors. "64" is the channel count for this example, and no additional dependencies are needed.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.batch_norm.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
BatchNorm2d(64, track_running_stats=False)
```

----------------------------------------

TITLE: Matrix Multiplication with mm in PyTorch (Python)
DESCRIPTION: Utilizes aten.mm for matrix-matrix product of two tensors. It is one of the core operations that occur within neural networks to process forward and backward propagation of data.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_16

LANGUAGE: Python
CODE:
```
aten.mm.default
cnt: 1, ((T([16, 2], f16), T([2, 768], f16)), {})
cnt: 1, ((T([2, 16], f16, stride=(1, 2)), T([16, 768], f16)), {})
cnt: 1, ((T([16, 768], f16), T([768, 768], f16)), {})
cnt: 1, ((T([768, 16], f16, stride=(1, 768)), T([16, 768], f16, stride=(393216, 1))), {})
cnt: 12, ((T([8192, 768], f16), T([768, 3072], f16)), {})
cnt: 12, ((T([768, 8192], f16, stride=(1, 768)), T([8192, 3072], f16)), {})
cnt: 12, ((T([8192, 3072], f16), T([3072, 768], f16)), {})
cnt: 12, ((T([3072, 8192], f16, stride=(1, 3072)), T([8192, 768], f16)), {})
cnt: 48, ((T([8192, 768], f16), T([768, 768], f16)), {})
cnt: 48, ((T([768, 8192], f16, stride=(1, 768)), T([8192, 768], f16)), {})
```

----------------------------------------

TITLE: Initializing Tensors for Batch Jacobian Computation in PyTorch
DESCRIPTION: Creates the necessary tensors for batch Jacobian computation: a weight matrix, bias vector, and batch of input features. The weight dimensions connect Din input features to Dout output features.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
batch_size = 64
Din = 31
Dout = 33

weight = torch.randn(Dout, Din)
print(f"weight shape = {weight.shape}")

bias = torch.randn(Dout)

x = torch.randn(batch_size, Din)
```

----------------------------------------

TITLE: Implementing Custom Quantized Module in PyTorch
DESCRIPTION: Example demonstrating how to create custom quantized modules using PyTorch's quantization API. Shows implementation of original module, observed module, and quantized module along with necessary conversion functions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_10

LANGUAGE: Python
CODE:
```
import torch
import torch.ao.nn.quantized as nnq
from torch.ao.quantization import QConfigMapping
import torch.ao.quantization.quantize_fx

# original fp32 module to replace
class CustomModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(3, 3)

    def forward(self, x):
        return self.linear(x)

# custom observed module, provided by user
class ObservedCustomModule(torch.nn.Module):
    def __init__(self, linear):
        super().__init__()
        self.linear = linear

    def forward(self, x):
        return self.linear(x)

    @classmethod
    def from_float(cls, float_module):
        assert hasattr(float_module, 'qconfig')
        observed = cls(float_module.linear)
        observed.qconfig = float_module.qconfig
        return observed

# custom quantized module, provided by user
class StaticQuantCustomModule(torch.nn.Module):
    def __init__(self, linear):
        super().__init__()
        self.linear = linear

    def forward(self, x):
        return self.linear(x)

    @classmethod
    def from_observed(cls, observed_module):
        assert hasattr(observed_module, 'qconfig')
        assert hasattr(observed_module, 'activation_post_process')
        observed_module.linear.activation_post_process = \
            observed_module.activation_post_process
        quantized = cls(nnq.Linear.from_float(observed_module.linear))
        return quantized
```

----------------------------------------

TITLE: Validating Milestone Fixes in Release Branch (Python)
DESCRIPTION: This Python command executes a script from the test-infra repository to verify that all fixes associated with a specific GitHub milestone have been successfully included in a target release branch. It requires specifying the local path to the PyTorch repository, the Git remote name, the release branch name (e.g., `release/1.11`), and the numerical ID of the milestone.
The `--missing-in-branch` flag is used to identify which fixes from the milestone are *not* present in the specified branch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/RELEASE.md#_snippet_0

LANGUAGE: python
CODE:
```
python github_analyze.py --repo-path ~/local/pytorch --remote upstream --branch release/1.11 --milestone-id 26 --missing-in-branch
```

----------------------------------------

TITLE: Configuring and Using AOTInductor Minifier in PyTorch
DESCRIPTION: Example model implementation demonstrating how to set up AOTInductor minifier and intentionally trigger a compilation error using ReLU operation for testing purposes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_aot_inductor_minifier.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from torch._inductor import config as inductor_config

class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = torch.nn.Linear(10, 16)
        self.relu = torch.nn.ReLU()
        self.sigmoid = torch.nn.Sigmoid()

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.sigmoid(x)
        return x


inductor_config.aot_inductor.dump_aoti_minifier = True
torch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = "compile_error"

with torch.no_grad():
    model = Model().to("cuda")
    example_inputs = (torch.randn(8, 10).to("cuda"),)
    ep = torch.export.export(model, example_inputs)
    package_path = torch._inductor.aoti_compile_and_package(ep)
    compiled_model = torch._inductor.aoti_load_package(package_path)
    result = compiled_model(*example_inputs)
```

----------------------------------------

TITLE: Querying Package Directory Structure
DESCRIPTION: Shows how to use the has_file() method to check for the existence of a specific file within a torch package. This is useful for programmatically verifying package contents.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
importer_file_structure = importer.file_structure()
found: bool = importer_file_structure.has_file("package_a/subpackage.py")
```

----------------------------------------

TITLE: Accessing and Renaming Tensor Dimensions
DESCRIPTION: Illustrates how to access and rename named dimensions of a tensor using the Tensor.names and Tensor.rename methods. Supports partial renaming of dimensions and is essential for rearranging dimensions when necessary.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/named_tensor.rst#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
imgs = torch.randn(1, 2, 2, 3 , names=('N', 'C', 'H', 'W'))
imgs.names
renamed_imgs = imgs.rename(H='height', W='width')
renamed_imgs.names
```

----------------------------------------

TITLE: Displaying PyTorch Performance Benchmark Results in Markdown Table
DESCRIPTION: A markdown table displaying performance metrics for different PyTorch configurations with batch size 1 and compilation disabled. Metrics include warmup latency, average latency, throughput, and GPU utilization across various worker configurations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/inference/results/output_1_false.md#2025-04-22_snippet_0

LANGUAGE: markdown
CODE:
```
## Batch Size 1 Compile false

| Experiment | Warmup_latency (s) | Average_latency (s) | Throughput (samples/sec) | GPU Utilization (%) |
| ---------- | ------------------ | ------------------- | ------------------------ | ------------------- |
| original | 5.497 +/- 0.681 | 0.383 +/- 0.007 | 168.818 +/- 2.169 | 12.574 +/- 2.205 |
| h2d_d2h_threads | 4.039 +/- 1.175 | 0.593 +/- 0.246 | 127.449 +/- 34.358 | 11.445 +/- 1.925 |
| 2_predict_workers | 3.766 +/- 0.610 | 0.369 +/- 0.010 | 186.485 +/- 2.657 | 11.857 +/- 2.591 |
| 3_predict_workers | 3.460 +/- 0.063 | 0.407 +/- 0.033 | 168.863 +/- 9.227 | 12.427 +/- 1.430 |
| 4_predict_workers | 4.184 +/- 0.636 | 0.692 +/- 0.182 | 110.231 +/- 31.420 | 8.490 +/- 1.826 |
```

----------------------------------------

TITLE: Computing Gradients with grad Transform in PyTorch
DESCRIPTION: Demonstrates using the grad transform to compute first and second-order gradients of trigonometric functions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/whirlwind_tour.ipynb#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from functorch import grad
x = torch.randn([])
cos_x = grad(lambda x: torch.sin(x))(x)
assert torch.allclose(cos_x, x.cos())

# Second-order gradients
neg_sin_x = grad(grad(lambda x: torch.sin(x)))(x)
assert torch.allclose(neg_sin_x, -x.sin())
```

----------------------------------------

TITLE: Good Practice: Non-inplace Operations with Traced Tensor Shapes
DESCRIPTION: This snippet provides the workaround for the in-place shape issue: rewrite the operation to be non-inplace, like `variable = variable + value`. This ensures that variables derived from traced tensor shapes do not share memory unexpectedly.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_8

LANGUAGE: python
CODE:
```
real_seq_length = real_seq_length + 2
```

----------------------------------------

TITLE: Implementing Torch Library in C++
DESCRIPTION: Macro for implementing a Torch library. Used in conjunction with TORCH_LIBRARY to define the actual implementation of custom operators and data types.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/library.rst#2025-04-22_snippet_1

LANGUAGE: C++
CODE:
```
TORCH_LIBRARY_IMPL
```

----------------------------------------

TITLE: Using Hessian-Vector Product with Example Function in PyTorch
DESCRIPTION: Shows how to use the HVP function with a simple sine function. This example demonstrates applying the HVP to compute the product of the Hessian of the sum of sine with a tangent vector.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_23

LANGUAGE: python
CODE:
```
def f(x):
  return x.sin().sum()

x = torch.randn(2048)
tangent = torch.randn(2048)

result = hvp(f, (x,), (tangent,))
```

----------------------------------------

TITLE: Dimension Flattening and Splitting in PyTorch
DESCRIPTION: Shows how to use tuples of dimensions for splitting and flattening tensor dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
i, j, k = dims(3)
j.size = 2
A = torch.rand(6, 4)
a = A[(i, j), k] # split dim 0 into i,j
print(i.size, j.size, k.size)

r = a.order(i, (j, k)) # flatten j and k
print(r.shape)
```

----------------------------------------

TITLE: Registering Custom PyTorch Operator Lambda Kernel (C++)
DESCRIPTION: Shows how to register a stateless C++ lambda function directly as a kernel for a custom operator (`my_namespace::my_op`) associated with the CPU backend. The registration uses `torch::RegisterOperators` and passes the lambda to the `.kernel()` method along with the dispatch key (`CPU()`). The lambda must not have a closure.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_1

LANGUAGE: cpp
CODE:
```
static auto registry = torch::RegisterOperators()
    .op("my_namespace::my_op", torch::RegisterOperators::options()
        .kernel(CPU(), [] (const Tensor& a) -> Tensor{...}));
```

----------------------------------------

TITLE: Computing Hessians using Composed Jacobian Transforms in PyTorch
DESCRIPTION: Shows how to compute Hessians by composing jacrev with itself or jacfwd.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/whirlwind_tour.ipynb#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
def f(x):
  return x.sin().sum()

x = torch.randn(5)
hessian0 = jacrev(jacrev(f))(x)
hessian1 = jacfwd(jacrev(f))(x)
```

----------------------------------------

TITLE: Applying Native Layer Normalization - PyTorch Aten
DESCRIPTION: Applies Layer Normalization to a tensor. This internal operator normalizes the inputs across the last dimensions, typically within a Transformer block. It takes the input tensor, the shape to normalize over, optional weight and bias tensors, and an epsilon for numerical stability.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BartForConditionalGeneration_training.txt#_snippet_18

LANGUAGE: Python
CODE:
```
import torch

input_tensor = torch.randn(2, 1024, 1024, dtype=torch.float16)
normalized_shape = [1024]
weight = torch.randn(1024, dtype=torch.float16)
bias = torch.randn(1024, dtype=torch.float16)
output, mean, var = torch.native_layer_norm(input_tensor, normalized_shape, weight, bias, 1e-5)
```

----------------------------------------

TITLE: Configuring Linear-ReLU fusion (Python)
DESCRIPTION: Demonstrates how to configure the fusion process for the `(Linear, ReLU)` pattern using PyTorch's `BackendPatternConfig`. It specifies the custom fuser method (`fuse_linear_relu`) and placeholder functions for root node and extra inputs getters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
BackendPatternConfig((torch.nn.Linear, torch.nn.ReLU))
    .set_fuser_method(fuse_linear_relu)
    ._set_root_node_getter(my_root_node_getter)
    ._set_extra_inputs_getter(my_extra_inputs_getter)
```

----------------------------------------

TITLE: Custom Tracer Implementation in FX
DESCRIPTION: Shows how to customize tracing behavior by subclassing the Tracer class.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
class MyCustomTracer(torch.fx.Tracer):
    pass

class MyModule(torch.nn.Module):
    def forward(self, x):
        return torch.relu(x) + torch.ones(3, 4)

mod = MyModule()
traced_graph = MyCustomTracer().trace(mod)
```

----------------------------------------

TITLE: Benchmarking Forward vs Reverse Mode Jacobian - Taller Matrix Case - Python
DESCRIPTION: Tests jacfwd (forward mode) and jacrev (reverse mode) on a case with many more outputs than inputs (tall matrix). Initializes random weights, bias, and input, runs Timer benchmarks for both Jacobian transform calls, and prints timings. Requires functorch, Timer and appropriate shapes for tall matrix. Outputs: timing for each method.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
Din = 32
Dout = 2048
weight = torch.randn(Dout, Din)

bias = torch.randn(Dout)
x = torch.randn(Din)

# remember the general rule about taller vs wider...here we have a taller matrix:
print(weight.shape)

using_fwd = Timer(stmt="jacfwd(predict, argnums=2)(weight, bias, x)", globals=globals())
using_bwd = Timer(stmt="jacrev(predict, argnums=2)(weight, bias, x)", globals=globals())

jacfwd_timing = using_fwd.timeit(500)
jacrev_timing = using_bwd.timeit(500)

print(f'jacfwd time: {jacfwd_timing}')
print(f'jacrev time: {jacrev_timing}')

```

----------------------------------------

TITLE: Importing Error Propagation Module in PyTorch
DESCRIPTION: This snippet shows how to import the errors module from PyTorch's distributed elastic multiprocessing package. It provides access to error handling and propagation functionality for distributed processing.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/errors.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from torch.distributed.elastic.multiprocessing import errors
```

----------------------------------------

TITLE: Implementing Hessian-Vector Product Using Double Reverse-Mode AD in PyTorch
DESCRIPTION: Provides an alternative HVP implementation using reverse-mode AD twice. This approach is useful when PyTorch forward-AD doesn't have coverage for specific operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_24

LANGUAGE: python
CODE:
```
def hvp_revrev(f, primals, tangents):
  _, vjp_fn = vjp(grad(f), *primals)
  return vjp_fn(*tangents)
```

----------------------------------------

TITLE: FakeTensorProp Usage with GraphModule
DESCRIPTION: Demonstrates various ways to use FakeTensorProp for propagating fake tensors through a GraphModule, including handling both real and fake inputs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_fake_tensor.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import FakeTensorProp from torch.fx.passes.fake_tensor_prop
gm: GraphModule
real_inputs: List[Tensor]
FakeTensorProp(gm).propagate(*real_inputs)
# This will populate meta['val'] on all the FX nodes with a fake tensor
# or if you have a preexisting fake mode, you should use it
FakeTensorProp(gm, mode=fake_mode).propagate(*real_inputs)
# There is also propagate_dont_convert_inputs if your inputs are already fake
fake_inputs: List[FakeTensor]
FakeTensorProp(gm, mode=fake_mode).propagate_dont_convert_inputs(*fake_inputs)
```

----------------------------------------

TITLE: Implementing Example DataPipe Classes
DESCRIPTION: Definition of an example IterDataPipe class and helper functions to create DataFrame and regular pipes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/dataframes_pipes.ipynb#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
# Example IterDataPipe
class ExampleIterPipe(IterDataPipe):
    def __init__(self, range = 20):
        self.range = range
    def __iter__(self):
        for i in range(self.range):
            yield i

def get_dataframes_pipe(range = 10, dataframe_size = 7):
    return ExampleIterPipe(range = range).map(lambda i: (i, i % 3))._to_dataframes_pipe(columns = ['i','j'], dataframe_size = dataframe_size)

def get_regular_pipe(range = 10):
    return ExampleIterPipe(range = range).map(lambda i: (i, i % 3))
```

----------------------------------------

TITLE: Sharing User RRef using RPC (Python)
DESCRIPTION: This snippet illustrates the more complex user-to-user RRef sharing pattern. A worker (A) that holds an RRef (owned by B) passes this user-side RRef as an argument to a function called on another worker (C) via `rpc.rpc_async`, involving coordination between A, B, and C.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/rref.rst#_snippet_3

LANGUAGE: Python
CODE:
```
import torch
import torch.distributed.rpc as rpc

# on worker A and worker C
def func(rref):
  pass

# on worker A
rref = rpc.remote('B', torch.add, args=(torch.ones(2), 1))
# say the rref has RRefId 100 and ForkId 1
rpc.rpc_async('C', func, args=(rref, ))
```

----------------------------------------

TITLE: Implementing a Custom Gamma Scheduler for Data Sparsification in PyTorch
DESCRIPTION: A custom data scheduler that gradually increases the sparsity level by multiplying it with gamma every epoch until a threshold is reached. This scheduler inherits from the BaseDataScheduler class and implements the required get_schedule_param() method.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_scheduler/README.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
class GammaScheduler(BaseDataScheduler):
    def __init__(self, data_sparsifier, gamma, threshold_sl):
        super().__init__(data_sparsifier, "sparsity_level")
        self.gamma = gamma
        self.threshold_sl = threshold_sl

    def get_schedule_param(self):
        if self.last_epoch > 0:
            return {name: min(self.threshold_sl, config["sparsity_level"] * self.gamma) for name, config in self.data_sparsifier.data_groups.items()}
        else:
            return {name: 0.0 for name, config in self.data_sparsifier.data_groups.items()}
```

----------------------------------------

TITLE: PyTorch Matrix Multiplication
DESCRIPTION: Matrix multiplication operations between tensors, including cases with transposed tensors and specific strides.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_efficientnet_b0_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
mm.default((T([128, 1000], f16), T([1000, 1280], f16)))
```

----------------------------------------

TITLE: Batch Normalization Operations in PyTorch
DESCRIPTION: This snippet shows batch normalization operations and their backward passes. It includes operations on 4D tensors with different channel sizes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
Operator: aten.native_batch_norm.default
cnt: 3, ((T([64, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})

Operator: aten.native_batch_norm_backward.default
cnt: 3, ((T([64, 64, 112, 112], f16), T([64, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})
```

----------------------------------------

TITLE: Debug Compile Usage
DESCRIPTION: Example showing how to use debug_compile to dump graph information and constants to disk for debugging.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/minifier.ipynb#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from functorch.compile import memory_efficient_fusion, debug_compile

memory_efficient_fusion(foo, bw_compiler=debug_compile)(inp)
```

----------------------------------------

TITLE: Bitwise Operations on Boolean Tensors in PyTorch
DESCRIPTION: This snippet shows bitwise AND operations on boolean tensors (b8 data type) in PyTorch. These operations are likely used for mask combination or logical operations on detection results.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
Operator: aten.bitwise_and.Tensor
cnt: 4, ((T([5000], b8), T([5000], b8)), {})
cnt: 4, ((T([0], b8), T([0], b8)), {})
```

----------------------------------------

TITLE: Checking Compilation Status with torch.jit.is_scripting() in TorchScript
DESCRIPTION: Returns a boolean value indicating whether the current program is compiled by torch.jit.script or not. Used in conditionals to control compilation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_39

LANGUAGE: python
CODE:
```
torch.jit.is_scripting()
```

----------------------------------------

TITLE: Profiling Batch Normalization Operations in PyTorch
DESCRIPTION: Log of batch normalization operations with running mean and variance tracking. Each operation includes the input tensor, scale, bias, running mean, running variance, and various configuration parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_22

LANGUAGE: python
CODE:
```
Operator: aten.native_batch_norm.default
cnt: 1, ((T([64, 24, 128, 128], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([64, 32, 128, 128], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})
cnt: 5, ((T([64, 64, 64, 64], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})
```

----------------------------------------

TITLE: Using Named Tuples in TorchScript Functions
DESCRIPTION: Example of using collections.namedtuple types in TorchScript. Named tuples work seamlessly with type annotations and can be passed to scripted functions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_10

LANGUAGE: Python
CODE:
```
import torch
import collections

Point = collections.namedtuple('Point', ['x', 'y'])

@torch.jit.script
def total(point):
    # type: (Point) -> Tensor
    return point.x + point.y

p = Point(x=torch.rand(3), y=torch.rand(3))
print(total(p))
```

----------------------------------------

TITLE: Creating Sparse BSC Tensor in PyTorch
DESCRIPTION: Shows how to create a Block Sparse Column (BSC) tensor using ccol_indices, row_indices, and values tensors with specified data type.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_21

LANGUAGE: Python
CODE:
```
ccol_indices = torch.tensor([0, 2, 4])
row_indices = torch.tensor([0, 1, 0, 1])
values = torch.tensor([[[0, 1, 2], [6, 7, 8]],
                      [[3, 4, 5], [9, 10, 11]],
                      [[12, 13, 14], [18, 19, 20]],
                      [[15, 16, 17], [21, 22, 23]]])
bsc = torch.sparse_bsc_tensor(ccol_indices, row_indices, values, dtype=torch.float64)
```

----------------------------------------

TITLE: Working with Data-Dependent Operations in torch.compile
DESCRIPTION: Demonstrates how to configure torch.compile to handle data-dependent operations on nested jagged tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
>>> a = torch.randn(50, 128)
>>> b = torch.randn(32, 128)
>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)
>>> def f(nt): return nt.chunk(2, dim=0)[0]
...
>>> compiled_f = torch.compile(f, fullgraph=True)
>>> output = compiled_f(nt)
```

LANGUAGE: python
CODE:
```
>>> torch._dynamo.config.capture_dynamic_output_shape_ops = True
>>> torch._dynamo.config.capture_scalar_outputs = True
```

----------------------------------------

TITLE: Illustrating Target Dtype Mapping in PyTorch Quantization (Python)
DESCRIPTION: Shows a conceptual dictionary mapping FX graph node names to their target data types for inputs, outputs, and weights during the quantization process. This map is generated initially and used to determine QDQStub placement based on dtype mismatches. It uses PyTorch data types like `torch.float32` and `torch.quint8`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
# node_name_to_target_dtype_info =
# {
#     # this is placeholder node in FX Graph
#     "input" : {"input_activation": torch.float32, "output_activation": torch.float32},
#     "qat_linear_relu": {"input_activation": torch.quint8, "output_activation": torch.quint8, "weight": ...}
#     # this is the return node in FX Graph
#     "output": {"input_activation": torch.float32, "output_activation": torch.float32}
# }
```

----------------------------------------

TITLE: Setting PyTorch Logging Options Programmatically
DESCRIPTION: Example showing how to enable specific logging options in torch.compile using torch._logging.set_logs
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import logging
torch._logging.set_logs(graph_breaks=True)
...
```

----------------------------------------

TITLE: Enforcing Full Graph Compilation with fullgraph=True
DESCRIPTION: Shows how to use the `fullgraph=True` option with `torch.compile`. This mode disables TorchDynamo's Python fallback, causing an error if the entire function cannot be compiled into a single graph. It's useful for ensuring complete compilation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting_old.rst#_snippet_7

LANGUAGE: python
CODE:
```
def toy_example(a, b):
   ...

compiled_toy = torch.compile(toy_example, fullgraph=True, backend=<compiler>)(a, b)
```

----------------------------------------

TITLE: Analyzing ATen AddMM Operations in PyTorch
DESCRIPTION: This piece enumerates the use of the ATen addmm operation, which combines matrix multiplication with matrix addition in PyTorch. It involves performing operations between batched matrices, reflecting common practices in neural network layers, particularly in manipulating weights and activations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistilBertForMaskedLM_training.txt#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
Operator: aten.addmm.default
cnt: 25, ((T([768], f16), T([2048, 768], f16), T([768, 768], f16, stride=(1, 768))), {})
cnt: 6, ((T([3072], f16), T([2048, 768], f16), T([768, 3072], f16, stride=(1, 768))), {})
cnt: 6, ((T([768], f16), T([2048, 3072], f16), T([3072, 768], f16, stride=(1, 3072))), {})
cnt: 1, ((T([30522], f16), T([2048, 768], f16), T([768, 30522], f16, stride=(1, 768))), {})
```

----------------------------------------

TITLE: Element-wise Multiplication for Scaling and Residual Connections
DESCRIPTION: Documents tensor multiplication operations used for scaling attention scores and element-wise operations in residual connections. The scaling factor 0.14433756729740643 is approximately 1/√48, suggesting scaled dot-product attention.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
Operator: aten.mul.Tensor
cnt: 36, ((T([2, 16, 576, 48], f16, stride=(1327104, 48, 2304, 1)), 0.14433756729740643), {})
cnt: 72, ((T([768], f16), T([2, 576, 768], f16)), {})
cnt: 4, ((T([2, 16, 1, 48], f16), 0.14433756729740643), {})
cnt: 4, ((T([768], f16), T([2, 1, 768], f16)), {})
cnt: 1, ((T([2, 1, 768], f16, stride=(443136, 768, 1)), T([768], f16)), {})
cnt: 1, ((T([2, 1, 768], f16, stride=(443136, 768, 1)), T([2, 1, 768], f16)), {})
cnt: 3, ((T([2, 1, 768], f16), T([768], f16)), {})
cnt: 3, ((T([2, 1, 768], f16), T([2, 1, 768], f16)), {})
cnt: 72, ((T([2, 576, 768], f16), T([768], f16)), {})
cnt: 72, ((T([2, 576, 768], f16), T([2, 576, 768], f16)), {})
cnt: 36, ((T([2, 16, 576, 48], f16), 0.14433756729740643), {})
```

----------------------------------------

TITLE: Invoking aten.native_layer_norm and Backward Operator - PyTorch - Python
DESCRIPTION: This documents forward and backward calls to layer normalization using aten.native_layer_norm and aten.native_layer_norm_backward. Forward receives a 3D f16 tensor and norm dim specs, weight, bias, and epsilon. Backward takes gradients plus input/weight/bias/other tensors and computes parameter updates. Outputs: normalized activations/gradients. Prereqs: correct tensor shapes, support for half precision.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GPT2ForSequenceClassification_training.txt#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
Operator: aten.native_layer_norm.default
cnt: 25, ((T([4, 1024, 768], f16), [768], T([768], f16), T([768], f16), 1e-05), {})
```

LANGUAGE: Python
CODE:
```
Operator: aten.native_layer_norm_backward.default
cnt: 25, ((T([4, 1024, 768], f16), T([4, 1024, 768], f16), [768], T([4, 1024, 1], f32), T([4, 1024, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})
```

----------------------------------------

TITLE: Defining and Registering a GTest-based C++ Test Executable (CMake)
DESCRIPTION: Configures a test C++ executable using an explicit source and header include, links it against GTest and the main GTest runner, then registers it as a discoverable CTest unit test. The snippet uses include_directories for custom headers and add_executable/target_link_libraries for target specification. It assumes source files and header location correctness. Requires preceding GTest integration.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/inductor/cpp/CMakeLists.txt#2025-04-22_snippet_3

LANGUAGE: CMake
CODE:
```
################################
# Tests
################################

# TODO(voz): This is a little assumptive of just this one test, rewrite with real dir includes
include_directories(${ATEN_INCLUDE})
add_executable(test_cpp_prefix test_cpp_prefix.cpp ../../torchinductor/codegen/cpp_prefix.h)
target_link_libraries(test_cpp_prefix gtest gtest_main)
add_test(NAME test_cpp_prefix COMMAND test_cpp_prefix)
```

----------------------------------------

TITLE: Exporting a PyTorch Model with Conv2D and ReLU
DESCRIPTION: Illustrates exporting a PyTorch model with Conv2D and ReLU using torch.export. Dependencies include PyTorch and torch.export. Inputs are sample tensors mimicking a 256x256 image with 3 channels. This provides a functional graph devoid of inplace operations, utilizing the aten operators.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
from torch.export import export

# Simple module for demonstration
class M(torch.nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.conv = torch.nn.Conv2d(
            in_channels=3, out_channels=16, kernel_size=3, padding=1
        )
        self.relu = torch.nn.ReLU()
        self.maxpool = torch.nn.MaxPool2d(kernel_size=3)

    def forward(self, x: torch.Tensor, *, constant=None) -> torch.Tensor:
        a = self.conv(x)
        a.add_(constant)
        return self.maxpool(self.relu(a))

example_args = (torch.randn(1, 3, 256, 256),)
example_kwargs = {"constant": torch.ones(1, 16, 256, 256)}

exported_program: torch.export.ExportedProgram = export(
    M(), args=example_args, kwargs=example_kwargs
)
print(exported_program)
```

----------------------------------------

TITLE: Aggregating Tensor Mean Computation Calls (aten.mean) - PyTorch - Python
DESCRIPTION: This code records parameterizations of mean reduction operations (aten.mean) in PyTorch, documenting dimensions reduced, tensor shapes, datatypes, and keepdim flags. Patterns suggest reduction over spatial dimensions in higher-rank tensors. Inputs are tensors and dimensions to reduce; output is pattern occurrence only.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_10

LANGUAGE: Python
CODE:
```
Operator: aten.mean.dim
cnt: 1, ((T([128, 256, 56, 56], f16), [2, 3], True), {})
cnt: 2, ((T([128, 512, 28, 28], f16), [2, 3], True), {})
cnt: 6, ((T([128, 1536, 14, 14], f16), [2, 3], True), {})
cnt: 3, ((T([128, 1536, 7, 7], f16), [2, 3], True), {})
cnt: 1, ((T([128, 2304, 7, 7], f16), [-1, -2], True), {})
```

----------------------------------------

TITLE: Defining Custom Symbolic for prim::PythonOp (Part 2: Symbolic Function)
DESCRIPTION: Implements the symbolic function symbolic_python_op that maps a prim::PythonOp in the PyTorch graph to equivalent ONNX operators (Clip or Relu) based on the name attribute. It inspects the input/output nodes and arguments.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_21

LANGUAGE: Python
CODE:
```
def symbolic_python_op(g: "GraphContext", *args, **kwargs):
    n = ctx.cur_node
    print("original node: ", n)
    for i, out in enumerate(n.outputs()):
        print("original output {}: {}, requires grad: {}".format(i, out, out.requiresGrad()))
    import torch.onnx.symbolic_helper as sym_helper
    for i, arg in enumerate(args):
        requires_grad = arg.requiresGrad() if sym_helper._is_value(arg) else False
        print("arg {}: {}, requires grad: {}".format(i, arg, requires_grad))

    name = kwargs["name"]
    ret = None
    if name == "MyClip":
        ret = g.op("Clip", args[0], args[1])
    elif name == "MyRelu":
        ret = g.op("Relu", args[0])
    else:
        # Logs a warning and returns None
        return _unimplemented("prim::PythonOp", "unknown node kind: " + name)
    # Copy type and shape from original node.
    ret.setType(n.type())
    return ret
```

----------------------------------------

TITLE: Applying ELU Activation with aten.elu in PyTorch
DESCRIPTION: The aten.elu.default operator implements the Exponential Linear Unit (ELU) activation function. This non-linear transformation is widely used in neural networks, requiring specific alpha and scale parameters for precise control over the shape of the activation curve. The tensors involved must be of type f16.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/nvidia_deeprecommender_training.txt#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
Operator: aten.elu.default
cnt: 4, ((T([256, 512], f16), 1.6732632423543772, 1.0507009873554805), {})
cnt: 1, ((T([256, 1024], f16), 1.6732632423543772, 1.0507009873554805), {})
cnt: 1, ((T([256, 197951], f16), 1.6732632423543772, 1.0507009873554805), {})
```

----------------------------------------

TITLE: Dividing Tensors by Scalar - PyTorch - Python
DESCRIPTION: Shows usages of the element-wise tensor division by a scalar, especially normalizing outputs after pooling via scalar constants (49, 196). Ensures output values are appropriately scaled, as per e.g. average pooling normalization. Inputs are high-dimensional and possibly non-contiguous; output shape is the same as input.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ecaresnet101d_training.txt#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
Operator: aten.div.Scalar
cnt: 4, ((T([64, 2048, 7, 7], f16, stride=(2048, 1, 0, 0)), 49), {})
cnt: 23, ((T([64, 1024, 14, 14], f16, stride=(1024, 1, 0, 0)), 196), {})
```

----------------------------------------

TITLE: Cleaning and Reinstalling PyTorch Development Setup (Bash)
DESCRIPTION: This sequence of bash commands cleans the Git repository and build artifacts before attempting a fresh setup in develop mode. It deinitializes submodules, removes untracked files, cleans build output, updates submodules, and finally runs `python setup.py develop`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_3

LANGUAGE: bash
CODE:
```
git submodule deinit -f .
git clean -xdf
python setup.py clean
git submodule update --init --recursive
python setup.py develop
```

----------------------------------------

TITLE: Tracing and Saving a PyTorch Module with Control Flow
DESCRIPTION: Shows how to trace a module with control flow, save it, and load it back. Demonstrates the limitations of tracing compared to scripting.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
# A module with control flow
>>> class ControlFlowModule(torch.nn.Module):
      def __init__(self):
        super().__init__()
        self.l0 = torch.nn.Linear(4, 2)
        self.l1 = torch.nn.Linear(2, 1)

      def forward(self, input):
        if input.dim() > 1:
            return torch.tensor(0)

        out0 = self.l0(input)
        out0_relu = torch.nn.functional.relu(out0)
        return self.l1(out0_relu)

>>> traced_module = torch.jit.trace(ControlFlowModule(), torch.randn(4))
>>> torch.jit.save(traced_module, 'controlflowmodule_traced.pt')
>>> loaded = torch.jit.load('controlflowmodule_traced.pt')
>>> loaded(torch.randn(2, 4)))
tensor([[-0.1571], [-0.3793]], grad_fn=<AddBackward0>)

>>> scripted_module = torch.jit.script(ControlFlowModule(), torch.randn(4))
>>> torch.jit.save(scripted_module, 'controlflowmodule_scripted.pt')
>>> loaded = torch.jit.load('controlflowmodule_scripted.pt')
>> loaded(torch.randn(2, 4))
tensor(0)
```

----------------------------------------

TITLE: Gradient Transform Examples
DESCRIPTION: Demonstrates using grad transform for computing first and second-order gradients.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/README.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from functorch import grad
x = torch.randn([])
cos_x = grad(lambda x: torch.sin(x))(x)
assert torch.allclose(cos_x, x.cos())

# Second-order gradients
neg_sin_x = grad(grad(lambda x: torch.sin(x)))(x)
assert torch.allclose(neg_sin_x, -x.sin())
```

----------------------------------------

TITLE: Matrix Multiplication Operations
DESCRIPTION: Series of matrix multiplication operations between tensors of various shapes using float16 precision, showing both forward and transposed operations
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/swin_base_patch4_window7_224_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
cnt: 1, ((T([50176, 512], f16), T([512, 256], f16, stride=(1, 512))), {})
cnt: 1, ((T([12544, 1024], f16), T([1024, 512], f16, stride=(1, 1024))), {})
```

----------------------------------------

TITLE: Profiling Batch Normalization and Backward Operations with ATen in PyTorch (Python)
DESCRIPTION: Summarizes all calls to ATen-based native_batch_norm (forward) and native_batch_norm_backward operators, including shapes and dtypes for the main data, running stats, weight, and bias tensors, as well as eps and momentum settings. Dependencies: PyTorch model with batch normalization layers. Inputs include N-d tensors and channel vectors; outputs are normalized tensors and gradients. Applies to both inference and training steps.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet18_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
Operator: aten.native_batch_norm.default
cnt: 1, ((T([16, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})
cnt: 4, ((T([16, 64, 56, 56], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})
cnt: 5, ((T([16, 128, 28, 28], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), False, 0.1, 1e-05), {})
cnt: 5, ((T([16, 256, 14, 14], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), False, 0.1, 1e-05), {})
cnt: 5, ((T([16, 512, 7, 7], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), False, 0.1, 1e-05), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.native_batch_norm_backward.default
cnt: 5, ((T([16, 512, 7, 7], f16), T([16, 512, 7, 7], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), False, 1e-05, [True, True, True]), {})
cnt: 5, ((T([16, 256, 14, 14], f16), T([16, 256, 14, 14], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), False, 1e-05, [True, True, True]), {})
cnt: 5, ((T([16, 128, 28, 28], f16), T([16, 128, 28, 28], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), False, 1e-05, [True, True, True]), {})
cnt: 4, ((T([16, 64, 56, 56], f16), T([16, 64, 56, 56], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([16, 64, 112, 112], f16), T([16, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})
```

----------------------------------------

TITLE: Using ModelReport in Pytorch Quantization Workflow
DESCRIPTION: Example code demonstrating how to use the ModelReport class within a PyTorch quantization workflow. This includes preparing a model, creating detector sets, inserting observers, calibrating the model, generating reports, and visualizing the results.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/_model_report/README.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
# prep model
qconfig_mapping = torch.ao.quantization.get_default_qconfig_mapping()
model = Model() # TODO define model
example_input = torch.randn((*args)) # TODO get example data for callibration
prepared_model = quantize_fx.prepare_fx(model, qconfig_mapping, example_input)

# create ModelReport instance and insert observers
detector_set = set([DynamicStaticDetector()]) # TODO add all desired detectors
model_report = ModelReport(model, detector_set)
ready_for_callibrate = model_report.prepare_detailed_callibration()

# callibrate model and generate report
ready_for_callibrate(example_input) # TODO run callibration of model with relevant data
reports = model_report.generate_model_report(remove_inserted_observers=True)
for report_name in report.keys():
    text_report, report_dict = reports[report_name]
    print(text_report, report_dict)

# Optional: we get a ModelReportVisualizer instance to do any visualizations desired
mod_rep_visualizer = tracer_reporter.generate_visualizer()
mod_rep_visualizer.generate_table_visualization() # shows collected data as a table

# TODO updated qconfig based on suggestions
```

----------------------------------------

TITLE: Implementing Custom Launcher for TorchElastic in Python
DESCRIPTION: This snippet demonstrates how to create a custom launcher for TorchElastic by programmatically creating an agent and passing worker specifications. It includes error handling and result processing.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/customization.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
# my_launcher.py

if __name__ == "__main__":
  args = parse_args(sys.argv[1:])
  rdzv_handler = RendezvousHandler(...)
  spec = WorkerSpec(
      local_world_size=args.nproc_per_node,
      fn=trainer_entrypoint_fn,
      args=(trainer_entrypoint_fn args.fn_args,...),
      rdzv_handler=rdzv_handler,
      max_restarts=args.max_restarts,
      monitor_interval=args.monitor_interval,
  )

  agent = LocalElasticAgent(spec, start_method="spawn")
  try:
      run_result = agent.run()
      if run_result.is_failed():
          print(f"worker 0 failed with: run_result.failures[0]")
      else:
          print(f"worker 0 return value is: run_result.return_values[0]")
  except Exception ex:
      # handle exception
```

----------------------------------------

TITLE: Relationship-Based Dynamic Shape Representation in Exported Program
DESCRIPTION: Output showing how relationships between dynamic dimensions are preserved in the exported program. The example demonstrates one dimension being defined as one larger than another dimension, with corresponding range constraints.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
ExportedProgram:
class GraphModule(torch.nn.Module):
    def forward(self, x: "f32[s0]", y: "f32[s0 + 1]"):
        # code: return x + y[1:]
        slice_1: "f32[s0]" = torch.ops.aten.slice.Tensor(y, 0, 1, 9223372036854775807)
        add: "f32[s0]" = torch.ops.aten.add.Tensor(x, slice_1)
        return (add,)

Range constraints: {s0: VR[3, 6], s0 + 1: VR[4, 7]}
```

----------------------------------------

TITLE: Disabling Inductor Caches for Cold Start Timing/Cache Debugging
DESCRIPTION: Sets a configuration flag to force disable all compile-time caching in TorchInductor. This is useful for measuring the cold start compilation time or debugging potential cache corruption issues by ensuring recompilation every time.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting_old.rst#_snippet_10

LANGUAGE: python
CODE:
```
torch._inductor.config.force_disable_caches = True
```

----------------------------------------

TITLE: Matrix Multiplication in PyTorch
DESCRIPTION: This snippet shows matrix multiplication operations, likely used in fully connected layers of the neural network. The operations involve 2D tensors with specific shapes and data types.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hrnet_w18_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([128, 1000], f16), T([1000, 2048], f16)), {})
```

----------------------------------------

TITLE: Usage Log: aten.threshold_backward.default Operator (Text)
DESCRIPTION: Logs calls to the backward pass of a threshold function (`aten.threshold_backward.default`), likely related to ReLU's backward pass. Arguments include the gradient w.r.t. the output, the original input tensor (both f16 with same shape, e.g., [128, 768, 1, 1]), and the threshold value (0).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dm_nfnet_f0_training.txt#2025-04-22_snippet_10

LANGUAGE: text
CODE:
```
Operator: aten.threshold_backward.default
cnt: 9, ((T([128, 768, 1, 1], f16), T([128, 768, 1, 1], f16), 0), {})
cnt: 2, ((T([128, 256, 1, 1], f16), T([128, 256, 1, 1], f16), 0), {})
cnt: 1, ((T([128, 128, 1, 1], f16), T([128, 128, 1, 1], f16), 0), {})
```

----------------------------------------

TITLE: Displaying Benchmark Results for Batch Size 32 in Markdown Table
DESCRIPTION: A markdown table showing performance metrics for different worker configurations with batch size 32 and compilation disabled. Metrics include warmup latency, average latency, throughput, and GPU utilization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/inference/results/output_32_false.md#2025-04-22_snippet_0

LANGUAGE: markdown
CODE:
```
| Experiment | Warmup_latency (s) | Average_latency (s) | Throughput (samples/sec) | GPU Utilization (%) |
| ---------- | ------------------ | ------------------- | ------------------------ | ------------------- |
| original | 5.680 +/- 0.919 | 4.785 +/- 0.864 | 394.178 +/- 81.705 | 38.515 +/- 11.152 |
| h2d_d2h_threads | 4.856 +/- 0.142 | 6.694 +/- 0.497 | 287.201 +/- 41.480 | 27.028 +/- 4.773 |
| 2_predict_workers | 3.465 +/- 0.082 | 5.369 +/- 0.900 | 334.981 +/- 50.292 | 31.635 +/- 4.492 |
| 3_predict_workers | 3.819 +/- 0.617 | 4.409 +/- 0.149 | 402.236 +/- 22.151 | 35.893 +/- 0.877 |
| 4_predict_workers | 3.994 +/- 0.509 | 6.007 +/- 0.408 | 296.260 +/- 16.524 | 25.751 +/- 1.810 |
```

----------------------------------------

TITLE: Displaying Performance Metrics Table in Markdown
DESCRIPTION: A markdown table showing performance metrics for different experimental setups in a PyTorch project. It includes measurements for warmup latency, average latency, throughput, and GPU utilization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/inference/results/output_64_false.md#2025-04-22_snippet_0

LANGUAGE: markdown
CODE:
```
| Experiment | Warmup_latency (s) | Average_latency (s) | Throughput (samples/sec) | GPU Utilization (%) |
| ---------- | ------------------ | ------------------- | ------------------------ | ------------------- |
| original | 5.823 +/- 0.541 | 8.024 +/- 0.508 | 460.474 +/- 29.896 | 47.112 +/- 6.576 |
| h2d_d2h_threads | 3.971 +/- 0.709 | 9.583 +/- 1.030 | 393.394 +/- 26.110 | 37.550 +/- 1.381 |
| 2_predict_workers | 3.535 +/- 0.124 | 8.258 +/- 1.188 | 441.640 +/- 85.705 | 38.630 +/- 3.499 |
| 3_predict_workers | 3.724 +/- 0.312 | 7.614 +/- 0.951 | 467.236 +/- 36.619 | 37.834 +/- 2.418 |
| 4_predict_workers | 4.346 +/- 0.744 | 8.417 +/- 0.541 | 430.672 +/- 17.979 | 34.461 +/- 3.257 |
```

----------------------------------------

TITLE: Defining LSTM Cell with torch.jit.script (Python)
DESCRIPTION: This Python snippet defines a simple LSTM cell function using standard PyTorch operations decorated with `@torch.jit.script`. This decorator instructs the PyTorch JIT frontend to compile the Python code into a graph representation, which the graph executor can then optimize and prepare for execution.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_22

LANGUAGE: Python
CODE:
```
@torch.jit.script
def LSTMCellS(x, hx, cx, w_ih, w_hh, b_ih, b_hh):
    gates = x.mm(w_ih.t()) + hx.mm(w_hh.t()) + b_ih + b_hh
    ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)
    ingate = torch.sigmoid(ingate)
    forgetgate = torch.sigmoid(forgetgate)
    cellgate = torch.tanh(cellgate)
    outgate = torch.sigmoid(outgate)
    cy = (forgetgate * cx) + (ingate * cellgate)
    hy = outgate * torch.tanh(cy)
    return hy, cy
```

----------------------------------------

TITLE: Creating Complex Tensors in PyTorch
DESCRIPTION: Demonstrates how to create complex tensors using torch.randn with cfloat dtype.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/complex_numbers.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
x = torch.randn(2,2, dtype=torch.cfloat)
x
```

----------------------------------------

TITLE: Invoking aten.sum.SymInt in PyTorch ATen
DESCRIPTION: Logs calls to the `aten.sum.SymInt` operator, which computes the sum of elements across specified dimensions of a tensor. The arguments include the input tensor (f16), a list of dimensions to reduce (`[0]` or `[2, 3]`), and a boolean flag indicating whether to keep the reduced dimensions (`True`). The `cnt` indicates call frequency.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/lcnet_050_training.txt#2025-04-22_snippet_8

LANGUAGE: plaintext
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([128, 1000], f16), [0], True), {})
cnt: 1, ((T([128, 256, 7, 7], f16), [2, 3], True), {})
cnt: 1, ((T([128, 128, 7, 7], f16), [2, 3], True), {})
```

----------------------------------------

TITLE: Defining PyTorch Operator Schema with Annotations and Defaults (C++)
DESCRIPTION: Shows how to explicitly define an operator schema with type annotations, argument names, default values, and return value annotations using TorchScript syntax. This provides more detailed schema information than automatic inference. The C++ kernel signature must match the specified schema.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_8

LANGUAGE: cpp
CODE:
```
namespace {
    Tensor my_kernel_cpu(const Tensor& a, int64_t b, std::optional<int64_t> c) {...}
}

static auto registry = torch::RegisterOperators()
   .op("my_namespace::my_op(Tensor(a) x, int y = 3, int? z = None) -> Tensor(a|b)",
       torch::RegisterOperators::options()
         .kernel<decltype(my_kernel_cpu), &my_kernel_cpu>(CPU()));
```

----------------------------------------

TITLE: Usage Log: aten.sum.SymInt Operator (Text)
DESCRIPTION: Logs calls to the `aten.sum.SymInt` operator, which performs summation over specified dimensions. Arguments show the input tensor shape (f16), the dimensions to sum over (e.g., [2, 3]), and a boolean flag (True, likely keepdim).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dm_nfnet_f0_training.txt#2025-04-22_snippet_8

LANGUAGE: text
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([128, 1000], f16), [0], True), {})
cnt: 3, ((T([128, 1536, 6, 6], f16), [2, 3], True), {})
cnt: 6, ((T([128, 1536, 12, 12], f16), [2, 3], True), {})
cnt: 2, ((T([128, 512, 24, 24], f16), [2, 3], True), {})
cnt: 1, ((T([128, 256, 48, 48], f16), [2, 3], True), {})
```

----------------------------------------

TITLE: Using MemPool Context and Registering for NVLS in Python
DESCRIPTION: This snippet uses the `torch.cuda.use_mem_pool` context manager to allocate a tensor within the custom memory pool, ensuring the tensor memory comes from the specified allocator. It then registers the memory pool with the backend (`ProcessGroupNCCL`) using `backend.register_mem_pool` to enable NVLS reductions, followed by an all-reduce operation on a slice of the tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_28

LANGUAGE: python
CODE:
```
with torch.cuda.use_mem_pool(pool):
    # tensor gets allocated with ncclMemAlloc passed in the pool
    tensor = torch.arange(1024 * 1024 * 2, device=device)
    print(f"tensor ptr on rank {rank} is {hex(tensor.data_ptr())}")

# register user buffers using ncclCommRegister (called under the hood)
backend.register_mem_pool(pool)

# Collective uses Zero Copy NVLS
dist.all_reduce(tensor[0:4])
torch.cuda.synchronize()
print(tensor[0:4])
```

----------------------------------------

TITLE: Demonstrating BC-safe function signature change in Python
DESCRIPTION: Example of a backwards-compatible change to a function signature where a new parameter with a default value is added to an existing function. This type of change preserves compatibility with existing code as it doesn't require callers to change their invocation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/operator_upgraders/README.md#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
# before
def foo(x, y):
    return x, y
```

LANGUAGE: python
CODE:
```
# after
def foo(x, y, z=100):
    return x, y, z
```

----------------------------------------

TITLE: Building libtorch Using Python Script
DESCRIPTION: This snippet demonstrates how to build libtorch using a Python script from the tools package. It creates a separate build directory to avoid polluting source directories.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/libtorch.rst#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
cd <pytorch_root>

# Make a new folder to build in to avoid polluting the source directories
mkdir build_libtorch && cd build_libtorch

# You might need to export some required environment variables here.
# Normally setup.py sets good default env variables, but you'll have to do
# that manually.
python ../tools/build_libtorch.py
```

----------------------------------------

TITLE: Configuring PyTorch Load Options in Python
DESCRIPTION: Configuration options for torch.load including mmap settings, endianness control, mmap flags, and storage offset calculations. These settings affect how PyTorch deserializes data when loading saved models and tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
torch.utils.serialization.config.load = {
    "mmap": False,  # Controls memory mapping behavior
    "endianness": torch.serialization.LoadEndianness.NATIVE,  # Controls byte order
    "mmap_flags": "MAP_PRIVATE",  # Memory mapping access flags
    "calculate_storage_offsets": False  # Controls storage offset calculation method
}
```

----------------------------------------

TITLE: Building Libtorch C++ Library (Python)
DESCRIPTION: The build_libtorch.py script builds libtorch, a standalone C++ library without Python support. This build script is tested in CI.
SOURCE: https://github.com/pytorch/pytorch/blob/main/tools/README.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
python build_libtorch.py
```

----------------------------------------

TITLE: Autograd Handling of Named Tensors
DESCRIPTION: Details the limited support for named tensors in PyTorch autograd, noting that while gradient computations remain valid, names are currently ignored in gradient outputs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/named_tensor.rst#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
x = torch.randn(3, names=('D',))
weight = torch.randn(3, names=('D',), requires_grad=True)
loss = (x - weight).abs()
grad_loss = torch.randn(3)
loss.backward(grad_loss)
weight.grad  # Unnamed for now. Will be named in the future

weight.grad.zero_()
grad_loss = grad_loss.refine_names('C')
loss = (x - weight).abs()
# Ideally we'd check that the names of loss and grad_loss match but we don't yet.
loss.backward(grad_loss)
weight.grad
```

----------------------------------------

TITLE: Name Propagation in Operations
DESCRIPTION: Shows how tensor operations can handle named dimensions automatically through name inference, checking matches and propagating names, as demonstrated with tensor absolute value computation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/named_tensor.rst#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
x = torch.randn(3, 3, names=('N', 'C'))
x.abs().names
```

----------------------------------------

TITLE: Neural Network Module Transformation Example
DESCRIPTION: Shows how to compute per-sample gradients using a functional version of nn.Linear.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/README.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
import torch
from functorch import make_functional, vmap, grad

model = torch.nn.Linear(3, 3)
data = torch.randn(64, 3)
targets = torch.randn(64, 3)

func_model, params = make_functional(model)

def compute_loss(params, data, targets):
    preds = func_model(params, data)
    return torch.mean((preds - targets) ** 2)

per_sample_grads = vmap(grad(compute_loss), (None, 0, 0))(params, data, targets)
```

----------------------------------------

TITLE: Building and Linking Custom Shared Library for PyTorch (CMake)
DESCRIPTION: This CMake snippet builds a shared library “aoti_custom_ops” from “custom_ops.cpp”, linking it against the “torch” library as a dependency. It is intended for use with torch.ops.load_library() in Python test contexts, enabling custom operator integration. The optional install command places the built library in the correct directory for use in tests, conditional on the INSTALL_TEST flag being set. Necessary dependencies include PyTorch C++ libraries and a properly configured build environment; inputs are source files and build flags, and outputs are one or more shared library files.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/inductor/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
# Build separate libraries the define custom classes/operators used from our Python tests.
# These are intended to be used with torch.ops.load_library() in our Python test suite.
add_library(aoti_custom_ops SHARED custom_ops.cpp)
target_link_libraries(aoti_custom_ops torch)

if(INSTALL_TEST)
  install(TARGETS aoti_custom_ops DESTINATION lib)
endif()
```

----------------------------------------

TITLE: Correct Use of Tensor.new_zeros for batched initialization - PyTorch - Python
DESCRIPTION: This snippet demonstrates the preferred approach for batched tensor creation inside vmap-compatible functions. diag_embed uses vec.new_zeros to create a tensor aligned with batching, enabling correct in-place operations. Dependencies: PyTorch, torch.func. Inputs and outputs are batched tensors; method can be generalized for any similar batched-tensor factory needs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
def diag_embed(vec):
  assert vec.dim() == 1
  result = vec.new_zeros(vec.shape[0], vec.shape[0])
  result.diagonal().copy_(vec)
  return result

vecs = torch.tensor([[0., 1, 2], [3., 4, 5]])
vmap(diag_embed)(vecs)
```

----------------------------------------

TITLE: Function with Non-Data-Dependent Control Flow under vmap (Supported) - PyTorch - Python
DESCRIPTION: This example shows a function using control flow that depends on meta-data (tensor shape) rather than tensor values, which works correctly under vmap. custom_dot checks the dimension of its input and applies different operations accordingly. Dependencies: PyTorch, torch.func. This pattern is safe under vmap since the control flow does not depend on batched data values.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
def custom_dot(x):
  if x.dim() == 1:
    return torch.dot(x, x)
  return (x * x).sum()

x = torch.randn(3)
vmap(custom_dot)(x)
```

----------------------------------------

TITLE: Mixing Tracing and Scripting with torch.jit in PyTorch (Python)
DESCRIPTION: Demonstrates how to trace a simple function and subsequently utilize it within a scripted TorchScript function. Requires PyTorch. The example uses a traced function 'traced_foo' and a scripted function 'bar', showing how scripted functions can call traced ones. Input and output are both torch.Tensors, illustrating function composition in TorchScript.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch\n\ndef foo(x, y):\n    return 2 * x + y\n\ntraced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\n\n@torch.jit.script\ndef bar(x):\n    return traced_foo(x, x)\n
```

----------------------------------------

TITLE: Traversing Tree Views C++
DESCRIPTION: Demonstrates a common pattern for traversing the AST represented by `Tree` objects in C++. It uses a `switch` statement on the `tree.kind()` to determine the node type and then constructs the appropriate tree view (e.g., `Var`, `Select`, `Apply`) for easier access to its components before processing the node. This pattern relies on the defined Tree View structures and the concept of different node kinds (`TK_VAR`, `.`, `TK_APPLY`).
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_12

LANGUAGE: C++
CODE:
```
switch (tree.kind()) {
  case TK_VAR:
          auto var = Var(tree); // construct tree view
        return environment_stack->getSugaredVar(var.name());
  case '.': {
        auto select = Select(tree); // construct tree view
        auto sv = emitSugaredExpr(select.value(), 1);
        return sv->attr(select.range(), method, select.selector().name());
  }
  case TK_APPLY: {
        auto apply = Apply(tree); // construct tree view
        return emitApplyExpr(apply, n_binders);
  } break;

}
```

----------------------------------------

TITLE: PyTorch vmap with Different Randomness
DESCRIPTION: Demonstrates how to use vmap with 'different' randomness mode where each element in the batch receives different random values. The example shows adding random noise to a tensor where each element gets unique random values.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
def add_noise(x):
    y = torch.randn(())  # y will be different across the batch
    return x + y

x = torch.ones(3)
result = vmap(add_noise, randomness="different")(x)  # we get 3 different values
```

----------------------------------------

TITLE: Implementing a Composable C++ Operator for PyTorch
DESCRIPTION: Defines a simple C++ function `my_op` that takes two `at::Tensor` objects and returns a new tensor computed as `self + 2 * other`. This example illustrates a function composed of existing PyTorch operators (`+`, `*`) that can potentially be registered with `CompositeImplicitAutograd` for automatic differentiation support.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_7

LANGUAGE: cpp
CODE:
```
at::Tensor my_op(const Tensor& self, const Tensor& other) {
  return self + 2 * other;
}
```

----------------------------------------

TITLE: Managing CUDA Streams on Multiple Devices in PyTorch C++
DESCRIPTION: This example demonstrates how to acquire and set CUDA streams on multiple devices, using CUDAGuard and CUDAStreamGuard to manage device and stream contexts.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_4

LANGUAGE: cpp
CODE:
```
// acquire new CUDA streams from CUDA stream pool on device 0 and device 1
at::cuda::CUDAStream myStream0 = at::cuda::getStreamFromPool(false, 0);
at::cuda::CUDAStream myStream1 = at::cuda::getStreamFromPool(false, 1);

// set current CUDA stream to `myStream0` on device 0
at::cuda::setCurrentCUDAStream(myStream0);
// set current CUDA stream to `myStream1` on device 1
at::cuda::setCurrentCUDAStream(myStream1);

// create a tensor on device 0, no need to specify device index since
// current device index is 0
torch::Tensor tensor0 = torch::ones({2, 2}, torch::device(at::kCUDA));
// sum() on tensor0 use `myStream0` as current CUDA stream on device 0
tensor0.sum();

// change the current device index to 1 by using CUDA device guard within a bracket scope
{
  at::cuda::CUDAGuard device_guard{1};
  // create a tensor on device 1
  torch::Tensor tensor1 = torch::ones({2, 2}, torch::device(at::kCUDA));
  // sum() on tensor 1 uses `myStream1` as current CUDA stream on device 1
  tensor1.sum();
}

// current device is reset to device 0 after device_guard is destroyed

// acquire a new CUDA stream on device 1
at::cuda::CUDAStream myStream1_1 = at::cuda::getStreamFromPool(false, 1);
// create a new tensor on device 1
torch::Tensor tensor1 = torch::ones({2, 2}, torch::device({torch::kCUDA, 1}));

// change the current device index to 1 and current CUDA stream on device 1
// to `myStream1_1` using CUDA stream guard within a scope
{
  at::cuda::CUDAStreamGuard stream_guard(myStream1_1);
  // sum() on tensor1 use `myStream1_1` as current CUDA stream on device 1
  tensor1.sum();
}

// current device is reset to device 0 and current CUDA stream on device 1 is
// reset to `myStream1`

// sum() on tensor1 uses `myStream1` as current CUDA stream on device 1
tensor1.sum();
```

----------------------------------------

TITLE: Using CUDAStreamGuard to Change Device and Stream in PyTorch (C++)
DESCRIPTION: This snippet shows how to use at::cuda::CUDAStreamGuard to change both the current device index and CUDA stream within a scope. It sets the device index to 1 and the stream to streams1[1].
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_9

LANGUAGE: C++
CODE:
```
{
  at::cuda::CUDAStreamGuard stream_guard(streams1[1]);

  // current device index and current CUDA stream are set to 1 and `streams1[1]` within scope
}
// current device index and current CUDA stream are reset to 0 and `streams0[0]` after
// stream_guard is destroyed
```

----------------------------------------

TITLE: Reclaiming MemPool Memory in Python
DESCRIPTION: This snippet shows how to explicitly delete the tensor and the memory pool object created earlier. Deleting the tensor releases its reference to the pool, and deleting the pool itself, once its use count drops to 1 (only the object holding the reference), triggers an internal `empty_cache` call to return the managed memory to the system.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_29

LANGUAGE: python
CODE:
```
del tensor, del pool
```

----------------------------------------

TITLE: Minimal Repro for Custom Backend Error (Python)
DESCRIPTION: This code is an example of a minified reproduction script for a custom backend error, generated when `TORCHDYNAMO_REPRO_AFTER="dynamo"` is used. It defines a simple `Repro` module containing the problematic operation (`torch.relu` in this case) and uses `torch.compile` with a "None" backend and `run_fwd_maybe_bwd` to execute it, demonstrating the minimal code needed to trigger the custom backend's error condition. Requires `torch`, `torch._dynamo`, `torch.fx`, `torch._dynamo.testing`, and `torch._dynamo.debug_utils`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting_old.rst#_snippet_4

LANGUAGE: python
CODE:
```
import torch
import torch._dynamo as dynamo
from torch import tensor, device
import torch.fx as fx
from torch._dynamo.testing import rand_strided
from math import inf
from torch._dynamo.debug_utils import run_fwd_maybe_bwd

from torch.nn import *

class Repro(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, add):
        relu = torch.relu(add);  add = None
        return (relu,)


mod = Repro().cuda()
opt_mod = torch.compile(mod, backend="None")


args = [((200, 200), (200, 1), torch.float32, 'cpu', False)]
args = [rand_strided(sh, st, dt, dev).requires_grad_(rg) for (sh, st, dt, dev, rg) in args]


with torch.cuda.amp.autocast(enabled=False):
    ref = run_fwd_maybe_bwd(mod, args)
    res = run_fwd_maybe_bwd(opt_mod, args)
```

----------------------------------------

TITLE: Using PassManager for Multiple Passes in PyTorch FX
DESCRIPTION: This example shows how to use the PassManager to run multiple passes on a PyTorch FX graph module and set up checks after each pass.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
from torch.fx.passes.infra.pass_manager import PassManager

pm = PassManager(
    passes=[replace_add_with_div, replace_div_with_mul],
    run_checks_after_each_pass=True,
    suppress_check_failures=False,
)
graph_module_out = pm(graph_module)

pm = PassManager(passes=[replace_add_with_div, replace_div_with_mul])

def check_div_target(graph_module):
```

----------------------------------------

TITLE: Convolution Backward Operations in PyTorch
DESCRIPTION: Statistics for the aten.convolution_backward.default operator showing backward pass for convolution operations. These operations compute gradients for various convolutional layers with different configurations, used during the backpropagation phase of training.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
Operator: aten.convolution_backward.default
cnt: 1, ((T([128, 3072, 6, 6], f16), T([128, 1536, 6, 6], f16), T([3072, 1536, 1, 1], f16), [3072], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 9, ((T([128, 1536, 1, 1], f16), T([128, 768, 1, 1], f16), T([1536, 768, 1, 1], f16), [1536], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 9, ((T([128, 768, 1, 1], f16), T([128, 1536, 1, 1], f16), T([768, 1536, 1, 1], f16), [768], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 3, ((T([128, 1536, 6, 6], f16), T([128, 768, 6, 6], f16), T([1536, 768, 1, 1], f16), [1536], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 5, ((T([128, 768, 6, 6], f16), T([128, 768, 6, 6], f16), T([768, 128, 3, 3], f16), [768], [1, 1], [1, 1], [1, 1], False, [0, 0], 6, [True, True, True]), {})
cnt: 2, ((T([128, 768, 6, 6], f16), T([128, 1536, 6, 6], f16), T([768, 1536, 1, 1], f16), [768], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([128, 768, 6, 6], f16), T([128, 768, 13, 13], f16), T([768, 128, 3, 3], f16), [768], [2, 2], [0, 0], [1, 1], False, [0, 0], 6, [True, True, True]), {})
cnt: 6, ((T([128, 768, 12, 12], f16), T([128, 1536, 12, 12], f16), T([768, 1536, 1, 1], f16), [768], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([128, 1536, 6, 6], f16), T([128, 1536, 6, 6], f16), T([1536, 1536, 1, 1], f16), [1536], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 6, ((T([128, 1536, 12, 12], f16), T([128, 768, 12, 12], f16), T([1536, 768, 1, 1], f16), [1536], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 11, ((T([128, 768, 12, 12], f16), T([128, 768, 12, 12], f16), T([768, 128, 3, 3], f16), [768], [1, 1], [1, 1], [1, 1], False, [0, 0], 6, [True, True, True]), {})
cnt: 1, ((T([128, 768, 12, 12], f16), T([128, 768, 25, 25], f16), T([768, 128, 3, 3], f16), [768], [2, 2], [0, 0], [1, 1], False, [0, 0], 6, [True, True, True]), {})
cnt: 1, ((T([128, 768, 24, 24], f16), T([128, 512, 24, 24], f16), T([768, 512, 1, 1], f16), [768], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([128, 1536, 12, 12], f16), T([128, 512, 12, 12], f16), T([1536, 512, 1, 1], f16), [1536], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 2, ((T([128, 512, 1, 1], f16), T([128, 256, 1, 1], f16), T([512, 256, 1, 1], f16), [512], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 2, ((T([128, 256, 1, 1], f16), T([128, 512, 1, 1], f16), T([256, 512, 1, 1], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 3, ((T([128, 512, 24, 24], f16), T([128, 256, 24, 24], f16), T([512, 256, 1, 1], f16), [512], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 3, ((T([128, 256, 24, 24], f16), T([128, 256, 24, 24], f16), T([256, 128, 3, 3], f16), [256], [1, 1], [1, 1], [1, 1], False, [0, 0], 2, [True, True, True]), {})
cnt: 1, ((T([128, 256, 24, 24], f16), T([128, 512, 24, 24], f16), T([256, 512, 1, 1], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([128, 256, 24, 24], f16), T([128, 256, 49, 49], f16), T([256, 128, 3, 3], f16), [256], [2, 2], [0, 0], [1, 1], False, [0, 0], 2, [True, True, True]), {})
cnt: 1, ((T([128, 256, 48, 48], f16), T([128, 256, 48, 48], f16), T([256, 256, 1, 1], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([128, 256, 1, 1], f16), T([128, 128, 1, 1], f16), T([256, 128, 1, 1], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([128, 128, 1, 1], f16), T([128, 256, 1, 1], f16), T([128, 256, 1, 1], f16), [128], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 2, ((T([128, 256, 48, 48], f16), T([128, 128, 48, 48], f16), T([256, 128, 1, 1], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 2, ((T([128, 128, 48, 48], f16), T([128, 128, 48, 48], f16), T([128, 128, 3, 3], f16), [128], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, True]), {})
```

----------------------------------------

TITLE: Profiling ATen Operator Invocation Arguments - PyTorch - Python
DESCRIPTION: This data structure captures the invocation count and detailed parameterization of each ATen operator call, including tensor shapes, data types, additional arguments, and options or attributes. Each record maps a profiling sample (e.g., 'cnt: 4, ((T([1, 112, 40, 40], f16), ...), {})') for a particular operator configuration, suitable for further analysis or generating test benches. Dependencies include PyTorch's ATen operator set and tensor representations; expected inputs are numerical arguments and tensor metadata, and no outputs are computed in this static format. No computation is performed; this serves data logging and is constrained to static schema and PyTorch types.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
cnt: 4, ((T([1, 112, 40, 40], f16), T([1, 112, 40, 40], f16), T([112], f16), T([112], f16), T([112], f16), T([112], f32), T([112], f32), False, 0.001, [True, True, True]), {})\ncnt: 8, ((T([1, 480, 40, 40], f16), T([1, 480, 40, 40], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f32), T([480], f32), False, 0.001, [True, True, True]), {})\ncnt: 4, ((T([1, 80, 40, 40], f16), T([1, 80, 40, 40], f16), T([80], f16), T([80], f16), T([80], f16), T([80], f32), T([80], f32), False, 0.001, [True, True, True]), {})\ncnt: 1, ((T([1, 240, 40, 40], f16), T([1, 240, 40, 40], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f32), T([240], f32), False, 0.001, [True, True, True]), {})\ncnt: 5, ((T([1, 240, 80, 80], f16), T([1, 240, 80, 80], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f32), T([240], f32), False, 0.001, [True, True, True]), {})\ncnt: 3, ((T([1, 40, 80, 80], f16), T([1, 40, 80, 80], f16), T([40], f16), T([40], f16), T([40], f16), T([40], f32), T([40], f32), False, 0.001, [True, True, True]), {})\ncnt: 1, ((T([1, 144, 80, 80], f16), T([1, 144, 80, 80], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f32), T([144], f32), False, 0.001, [True, True, True]), {})\ncnt: 5, ((T([1, 144, 160, 160], f16), T([1, 144, 160, 160], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f32), T([144], f32), False, 0.001, [True, True, True]), {})\ncnt: 3, ((T([1, 24, 160, 160], f16), T([1, 24, 160, 160], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f32), T([24], f32), False, 0.001, [True, True, True]), {})\ncnt: 1, ((T([1, 96, 160, 160], f16), T([1, 96, 160, 160], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), False, 0.001, [True, True, True]), {})\ncnt: 1, ((T([1, 96, 320, 320], f16), T([1, 96, 320, 320], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), False, 0.001, [True, True, True]), {})\ncnt: 3, ((T([1, 16, 320, 320], f16), T([1, 16, 320, 320], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f32), T([16], f32), False, 0.001, [True, True, True]), {})\ncnt: 2, ((T([1, 32, 320, 320], f16), T([1, 32, 320, 320], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), False, 0.001, [True, True, True]), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.neg.default\ncnt: 2, ((T([5000], f32, stride=(4,)),), {})\ncnt: 8, ((T([1, 88, 5, 5], f16),), {})\ncnt: 20, ((T([1, 88, 10, 10], f16),), {})\ncnt: 20, ((T([1, 88, 20, 20], f16),), {})\ncnt: 20, ((T([1, 88, 40, 40], f16),), {})\ncnt: 8, ((T([1, 88, 80, 80], f16),), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.new_zeros.default\ncnt: 1, ((T([100, 1], f32, stride=(0, 0)), [5000, 1]), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda'})\ncnt: 1, ((T([100, 4], f32), [5000, 4]), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda'})\ncnt: 1, ((T([1, 5000, 1], f16), [1, 5000, 90]), {})\ncnt: 1, ((T([1, 5000, 90], f16), [1, 76725, 90]), {})\ncnt: 1, ((T([1, 5000, 4], f16), [1, 76725, 4]), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.relu.default\ncnt: 20, ((T([2], f16),), {})\ncnt: 12, ((T([3], f16),), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.remainder.Scalar\ncnt: 1, ((T([1, 5000], i64), 90), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.scatter_add_.default\ncnt: 1, ((T([1, 5000, 90], f16), 2, T([1, 5000, 1], i64), T([1, 5000, 1], f16)), {})\ncnt: 1, ((T([1, 76725, 90], f16), 1, T([1, 5000, 90], i64, stride=(5000, 1, 0)), T([1, 5000, 90], f16)), {})\ncnt: 1, ((T([1, 76725, 4], f16), 1, T([1, 5000, 4], i64, stride=(5000, 1, 0)), T([1, 5000, 4], f16)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.select_backward.default\ncnt: 1, ((T([5000, 4], f16), [1, 5000, 4], 0, 0), {})\ncnt: 1, ((T([5000, 1], f16), [1, 5000, 1], 0, 0), {})\ncnt: 20, ((T([], f16), [2], 0, 1), {})\ncnt: 20, ((T([], f16), [2], 0, 0), {})\ncnt: 12, ((T([], f16), [3], 0, 2), {})\ncnt: 12, ((T([], f16), [3], 0, 1), {})\ncnt: 12, ((T([], f16), [3], 0, 0), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.sigmoid.default\ncnt: 1, ((T([1, 32, 1, 1], f16),), {})\ncnt: 1, ((T([1, 16, 1, 1], f16),), {})\ncnt: 1, ((T([1, 96, 1, 1], f16),), {})\ncnt: 3, ((T([1, 144, 1, 1], f16),), {})\ncnt: 3, ((T([1, 240, 1, 1], f16),), {})\ncnt: 4, ((T([1, 480, 1, 1], f16),), {})\ncnt: 4, ((T([1, 672, 1, 1], f16),), {})\ncnt: 5, ((T([1, 1152, 1, 1], f16),), {})\ncnt: 1, ((T([1, 1920, 1, 1], f16),), {})\ncnt: 1, ((T([5000, 1], f16),), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.sigmoid_backward.default\ncnt: 1, ((T([5000, 1], f16), T([5000, 1], f16)), {})\ncnt: 1, ((T([1, 1920, 1, 1], f16), T([1, 1920, 1, 1], f16)), {})\ncnt: 5, ((T([1, 1152, 1, 1], f16), T([1, 1152, 1, 1], f16)), {})\ncnt: 4, ((T([1, 672, 1, 1], f16), T([1, 672, 1, 1], f16)), {})\ncnt: 4, ((T([1, 480, 1, 1], f16), T([1, 480, 1, 1], f16)), {})\ncnt: 3, ((T([1, 240, 1, 1], f16), T([1, 240, 1, 1], f16)), {})\ncnt: 3, ((T([1, 144, 1, 1], f16), T([1, 144, 1, 1], f16)), {})\ncnt: 1, ((T([1, 96, 1, 1], f16), T([1, 96, 1, 1], f16)), {})\ncnt: 1, ((T([1, 16, 1, 1], f16), T([1, 16, 1, 1], f16)), {})\ncnt: 1, ((T([1, 32, 1, 1], f16), T([1, 32, 1, 1], f16)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.silu_.default\ncnt: 2, ((T([1, 32, 320, 320], f16),), {})\ncnt: 1, ((T([1, 8, 1, 1], f16),), {})\ncnt: 1, ((T([1, 16, 320, 320], f16),), {})\ncnt: 2, ((T([1, 4, 1, 1], f16),), {})\ncnt: 1, ((T([1, 96, 320, 320], f16),), {})\ncnt: 1, ((T([1, 96, 160, 160], f16),), {})\ncnt: 5, ((T([1, 144, 160, 160], f16),), {})\ncnt: 3, ((T([1, 6, 1, 1], f16),), {})\ncnt: 1, ((T([1, 144, 80, 80], f16),), {})\ncnt: 5, ((T([1, 240, 80, 80], f16),), {})\ncnt: 3, ((T([1, 10, 1, 1], f16),), {})\ncnt: 1, ((T([1, 240, 40, 40], f16),), {})\ncnt: 8, ((T([1, 480, 40, 40], f16),), {})\ncnt: 4, ((T([1, 20, 1, 1], f16),), {})\ncnt: 7, ((T([1, 672, 40, 40], f16),), {})\ncnt: 4, ((T([1, 28, 1, 1], f16),), {})\ncnt: 1, ((T([1, 672, 20, 20], f16),), {})\ncnt: 10, ((T([1, 1152, 20, 20], f16),), {})\ncnt: 5, ((T([1, 48, 1, 1], f16),), {})\ncnt: 2, ((T([1, 1920, 20, 20], f16),), {})\ncnt: 1, ((T([1, 80, 1, 1], f16),), {})\ncnt: 14, ((T([1, 88, 10, 10], f16),), {})\ncnt: 14, ((T([1, 88, 20, 20], f16),), {})\ncnt: 14, ((T([1, 88, 40, 40], f16),), {})\ncnt: 10, ((T([1, 88, 80, 80], f16),), {})\ncnt: 10, ((T([1, 88, 5, 5], f16),), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.silu_backward.default\ncnt: 10, ((T([1, 88, 5, 5], f16), T([1, 88, 5, 5], f16)), {})\ncnt: 14, ((T([1, 88, 10, 10], f16), T([1, 88, 10, 10], f16)), {})\ncnt: 14, ((T([1, 88, 20, 20], f16), T([1, 88, 20, 20], f16)), {})\ncnt: 14, ((T([1, 88, 40, 40], f16), T([1, 88, 40, 40], f16)), {})\ncnt: 10, ((T([1, 88, 80, 80], f16), T([1, 88, 80, 80], f16)), {})\ncnt: 1, ((T([1, 80, 1, 1], f16), T([1, 80, 1, 1], f16)), {})\ncnt: 2, ((T([1, 1920, 20, 20], f16), T([1, 1920, 20, 20], f16)), {})\ncnt: 5, ((T([1, 48, 1, 1], f16), T([1, 48, 1, 1], f16)), {})\ncnt: 10, ((T([1, 1152, 20, 20], f16), T([1, 1152, 20, 20], f16)), {})\ncnt: 4, ((T([1, 28, 1, 1], f16), T([1, 28, 1, 1], f16)), {})\ncnt: 1, ((T([1, 672, 20, 20], f16), T([1, 672, 20, 20], f16)), {})\ncnt: 7, ((T([1, 672, 40, 40], f16), T([1, 672, 40, 40], f16)), {})\ncnt: 4, ((T([1, 20, 1, 1], f16), T([1, 20, 1, 1], f16)), {})\ncnt: 8, ((T([1, 480, 40, 40], f16), T([1, 480, 40, 40], f16)), {})\ncnt: 3, ((T([1, 10, 1, 1], f16), T([1, 10, 1, 1], f16)), {})\ncnt: 1, ((T([1, 240, 40, 40], f16), T([1, 240, 40, 40], f16)), {})\ncnt: 5, ((T([1, 240, 80, 80], f16), T([1, 240, 80, 80], f16)), {})\ncnt: 3, ((T([1, 6, 1, 1], f16), T([1, 6, 1, 1], f16)), {})\ncnt: 1, ((T([1, 144, 80, 80], f16), T([1, 144, 80, 80], f16)), {})\ncnt: 5, ((T([1, 144, 160, 160], f16), T([1, 144, 160, 160], f16)), {})\ncnt: 2, ((T([1, 4, 1, 1], f16), T([1, 4, 1, 1], f16)), {})\ncnt: 1, ((T([1, 96, 160, 160], f16), T([1, 96, 160, 160], f16)), {})\ncnt: 1, ((T([1, 96, 320, 320], f16), T([1, 96, 320, 320], f16)), {})\ncnt: 1, ((T([1, 16, 320, 320], f16), T([1, 16, 320, 320], f16)), {})\ncnt: 1, ((T([1, 8, 1, 1], f16), T([1, 8, 1, 1], f16)), {})\ncnt: 2, ((T([1, 32, 320, 320], f16), T([1, 32, 320, 320], f16)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.stack.default\ncnt: 4, (([T([1, 88, 10, 10], f16), T([1, 88, 10, 10], f16)], -1), {})\ncnt: 4, (([T([1, 88, 20, 20], f16), T([1, 88, 20, 20], f16)], -1), {})\ncnt: 4, (([T([1, 88, 40, 40], f16), T([1, 88, 40, 40], f16)], -1), {})\ncnt: 4, (([T([1, 88, 80, 80], f16), T([1, 88, 80, 80], f16)], -1), {})\ncnt: 4, (([T([1, 88, 40, 40], f16), T([1, 88, 40, 40], f16), T([1, 88, 40, 40], f16)], -1), {})\ncnt: 4, (([T([1, 88, 20, 20], f16), T([1, 88, 20, 20], f16), T([1, 88, 20, 20], f16)], -1), {})\ncnt: 4, (([T([1, 88, 10, 10], f16), T([1, 88, 10, 10], f16), T([1, 88, 10, 10], f16)], -1), {})\ncnt: 4, (([T([1, 88, 5, 5], f16), T([1, 88, 5, 5], f16)], -1), {})\ncnt: 2, (([T([5000], f32), T([5000], f32), T([5000], f32), T([5000], f32)], 1), {})\ncnt: 1, (([T([100, 6], f32)],), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.sub.Tensor\ncnt: 2, ((T([5000], f16, stride=(4,)), T([5000], f16, stride=(4,))), {})\ncnt: 2, ((T([5000], f32), T([5000], f32)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.sum.SymInt\ncnt: 1, ((T([1, 1920, 20, 20], f16), [2, 3], True), {})\ncnt: 5, ((T([1, 1152, 20, 20], f16), [2, 3], True), {})\ncnt: 1, ((T([1, 672, 20, 20], f16), [2, 3], True), {})\ncnt: 3, ((T([1, 672, 40, 40], f16), [2, 3], True), {})\ncnt: 4, ((T([1, 480, 40, 40], f16), [2, 3], True), {})\ncnt: 1, ((T([1, 240, 40, 40], f16), [2, 3], True), {})\ncnt: 2, ((T([1, 240, 80, 80], f16), [2, 3], True), {})\ncnt: 1, ((T([1, 144, 80, 80], f16), [2, 3], True), {})\ncnt: 2, ((T([1, 144, 160, 160], f16), [2, 3], True), {})\ncnt: 1, ((T([1, 96, 160, 160], f16), [2, 3], True), {})\ncnt: 1, ((T([1, 16, 320, 320], f16), [2, 3], True), {})\ncnt: 1, ((T([1, 32, 320, 320], f16), [2, 3], True), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.sum.default\ncnt: 20, ((T([2], f16),), {})\ncnt: 12, ((T([3], f16),), {})\ncnt: 1, ((T([1, 100, 6], f32),), {})\ncnt: 16, ((T([1, 88, 5, 5], f16),), {})\ncnt: 40, ((T([1, 88, 10, 10], f16),), {})\ncnt: 40, ((T([1, 88, 20, 20], f16),), {})\ncnt: 40, ((T([1, 88, 40, 40], f16),), {})\ncnt: 16, ((T([1, 88, 80, 80], f16),), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.sum.dim_IntList\ncnt: 4, ((T([1, 88, 10, 10, 2], f16), [-1]), {})\ncnt: 4, ((T([1, 88, 20, 20, 2], f16), [-1]), {})\ncnt: 4, ((T([1, 88, 40, 40, 2], f16), [-1]), {})\ncnt: 4, ((T([1, 88, 80, 80, 2], f16), [-1]), {})\ncnt: 4, ((T([1, 88, 40, 40, 3], f16), [-1]), {})\ncnt: 4, ((T([1, 88, 20, 20, 3], f16), [-1]), {})
```

----------------------------------------

TITLE: Example DType Configuration for FBGEMM Static Quantization in PyTorch (Python)
DESCRIPTION: Presents an example dictionary representing a combination of target data types for input activations, weights, and output activations. This specific combination is used within `BackendConfig` to define support for a quantization mode like FBGEMM static quantization for a pattern.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#_snippet_7

LANGUAGE: Python
CODE:
```
{
  "input_activation": torch.quint8,
  "weight": torch.qint8,
  "output_activation": torch.quint8
}
```

----------------------------------------

TITLE: Model Transformation Comparison with ResNet18
DESCRIPTION: Demonstrates incorrect usage of equality comparison between transformed models, showing why torch.allclose() should be used instead of == operator.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
resnet18 = models.resnet18()
transformed_resnet18 = transform(resnet18)

input_image = torch.randn(5, 3, 224, 224)

assert resnet18(input_image) == transformed_resnet18(input_image)
"""
RuntimeError: Boolean value of Tensor with more than one value is ambiguous
"""

assert torch.allclose(resnet18(input_image), transformed_resnet18(input_image))
```

----------------------------------------

TITLE: Registering Tensor Hooks Example in PyTorch
DESCRIPTION: Demonstrates how to register pack and unpack hooks on a saved tensor using grad_fn attributes in PyTorch's autograd system.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/autograd.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
x = torch.randn(5, requires_grad=True)
y = x.pow(2)
y.grad_fn._raw_saved_self.register_hooks(pack_hook, unpack_hook)
```

----------------------------------------

TITLE: Calling Scripted Functions from Traced Functions in torch.jit (Python)
DESCRIPTION: Shows how a scripted TorchScript function can be called from within a traced function. Requires PyTorch. The scripted function 'foo' uses control-flow to select between two tensors. The outer function 'bar' is then traced; when invoked, it will preserve the control-flow logic from 'foo'. Both inputs and outputs are torch.Tensors, useful for models with mixed computation paradigms.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch\n\n@torch.jit.script\ndef foo(x, y):\n    if x.max() > y.max():\n        r = x\n    else:\n        r = y\n    return r\n\ndef bar(x, y, z):\n    return foo(x, y) + z\n\ntraced_bar = torch.jit.trace(bar, (torch.rand(3), torch.rand(3), torch.rand(3)))\n
```

----------------------------------------

TITLE: Comparing Relative Performance Using get_perf - Python
DESCRIPTION: Applies the previously defined 'get_perf' function to compare performance of 'no_vmap_timer' vs 'with_vmap_timer' for Jacobian computation. Prints improvement percentage. Depends on prior Timer and get_perf definitions. No direct output values, just print side-effect.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
get_perf(no_vmap_timer, "without vmap",  with_vmap_timer, "vmap");
```

----------------------------------------

TITLE: Constraining Integer Tensor Ranges in PyTorch using constrain_range API
DESCRIPTION: The constrain_range API allows users to specify known upper and lower bounds for integer tensor data. This is useful when the system cannot automatically determine that values are non-negative.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamic_shapes.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
constrain_range
```

----------------------------------------

TITLE: Avoiding Segfaults with Single-Process Queue Put/Get (Python)
DESCRIPTION: Highlights a potential pitfall when using multiprocessing queues within the *same* process. Putting a tensor into a queue and then immediately getting it back from the same queue in the same process can lead to segmentation faults. This pattern should be avoided; queues are intended for inter-process communication.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/multiprocessing.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
# putting and getting from the same queue in the same process will likely end up with segfault
queue.put(tensor)
x = queue.get()
```

----------------------------------------

TITLE: Defining Helper Function for Performance Comparison
DESCRIPTION: Defines a utility function `get_perf` to compare the performance of two methods benchmarked using `torch.utils.benchmark.Timer`. It takes two `Measurement` objects (returned by `Timer.timeit`) and descriptive strings as input. It calculates the percentage improvement of the first method over the second based on their median execution times (`.times[0]`) and prints the result.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
def get_perf(first, first_descriptor, second, second_descriptor):
  """  takes torch.benchmark objects and compares delta of second vs first. """
  second_res = second.times[0]
  first_res = first.times[0]

  gain = (first_res-second_res)/first_res
  if gain < 0: gain *=-1 
  final_gain = gain*100

  print(f" Performance delta: {final_gain:.4f} percent improvement with {first_descriptor} ")
```

----------------------------------------

TITLE: PyTorch vmap with Same Randomness
DESCRIPTION: Shows how to use vmap with 'same' randomness mode where all elements in the batch receive the same random values. The example demonstrates adding random noise to a tensor where all elements get the same random value.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
def add_noise(x):
    y = torch.randn(())  # y will be the same across the batch
    return x + y

x = torch.ones(3)
result = vmap(add_noise, randomness="same")(x)  # we get the same value, repeated 3 times
```

----------------------------------------

TITLE: Defining TorchScript-Compatible Classes - PyTorch JIT - Python
DESCRIPTION: This snippet shows how to define a simple class that is compatible with TorchScript using the @torch.jit.script decorator. The class Foo includes a constructor and a method that mutates state, demonstrating valid use cases within the TorchScript subset. All member functions must themselves be TorchScript-compatible. There are current limitations, including the need for all code paths and types to be statically resolvable. Requires the torch library and is only for illustrative or experimental use due to the experimental status of scripted classes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
@torch.jit.script
class Foo:
  def __init__(self, x, y):
    self.x = x

  def aug_add_x(self, inc):
    self.x += inc
```

----------------------------------------

TITLE: Analyzing PyTorch Tensor Addition Operations
DESCRIPTION: This snippet shows the usage patterns of the aten.add.Tensor operator, including tensor shapes, data types, and call frequencies. It demonstrates various tensor addition scenarios with different shapes and strides.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten.add.Tensor
cnt: 27, ((T([3, 256, 128, 128], f16), T([3, 256, 128, 128], f16)), {})
cnt: 1, ((T([], f16), 0), {})
cnt: 1, ((T([], f16), T([], f16)), {})
cnt: 1, ((T([3, 256, 128, 128], f16, stride=(7340032, 16384, 128, 1)), T([3, 256, 128, 128], f16, stride=(8388608, 16384, 128, 1))), {})
cnt: 2, ((T([3, 256, 128, 128], f16), T([3, 256, 128, 128], f16, stride=(8388608, 16384, 128, 1))), {})
cnt: 1, ((T([3, 256, 128, 128], f16, stride=(8388608, 16384, 128, 1)), T([3, 256, 128, 128], f16, stride=(8388608, 16384, 128, 1))), {})
cnt: 1, ((T([3, 128, 256, 256], f16, stride=(16777216, 65536, 256, 1)), T([3, 128, 256, 256], f16)), {})
```

----------------------------------------

TITLE: Analyzing PyTorch Tensor Concatenation Operations
DESCRIPTION: This snippet demonstrates the usage of the aten.cat.default operator for concatenating tensors along specified dimensions. It shows various tensor shapes and concatenation scenarios.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
Operator: aten.cat.default
cnt: 2, (([T([3, 256, 128, 128], f16), T([3, 256, 128, 128], f16)], 1), {})
cnt: 1, (([T([3, 256, 128, 128], f16), T([3, 256, 128, 128], f16, stride=(4194304, 1, 32768, 256))], 1), {})
cnt: 1, (([T([3, 64, 128, 128], f16), T([3, 64, 128, 128], f16), T([3, 64, 128, 128], f16)], 1), {})
cnt: 1, (([T([3, 256, 128, 128], f16), T([3, 192, 128, 128], f16)], 1), {})
cnt: 1, (([T([3, 128, 256, 256], f16), T([3, 128, 256, 256], f16)], 1), {})
```

----------------------------------------

TITLE: Calling aten.gelu_backward.default (Python)
DESCRIPTION: Computes the gradient of the GELU activation function with respect to the input. Requires the output gradient and the input tensor to the forward pass. Example shows backward pass on float16 tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_17

LANGUAGE: Python
CODE:
```
((T([8, 128, 4096], f16), T([8, 128, 4096], f16)), {})
```

----------------------------------------

TITLE: vmap with Same Randomness Behavior
DESCRIPTION: This example demonstrates using vmap with 'same' randomness flag, where random values are generated identically for each element in the batch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/ux_limitations.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
def add_noise(x):
  y = torch.randn(())  # y will be the same across the batch
  return x + y

x = torch.ones(3)
result = vmap(add_noise, randomness="same")(x)  # we get the same value, repeated 3 times
```

----------------------------------------

TITLE: Unsupported Write Indexing with Multiple Non-Consecutive Tensors (Python)
DESCRIPTION: Shows an unsupported tensor indexing pattern for writing data where multiple tensor indices are used but are not consecutive. A workaround is to transpose the data tensor so that the tensor indices become consecutive.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_11

LANGUAGE: python
CODE:
```
data[torch.tensor([2, 3]), :, torch.tensor([1, 2])] = new_data
# Workarounds: transpose `data` such that tensor indices are consecutive.
```

----------------------------------------

TITLE: Examining Guard Conditions in Dynamo Compiled Functions
DESCRIPTION: This snippet shows how to inspect the guarding conditions of a compiled function directly without examining bytecode. It iterates through the code parts of the guard function to display the conditions that must be satisfied for the compiled code to be executed.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_overview.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
for code_part in guard.code_parts:
    print(code_part)
```

----------------------------------------

TITLE: Name Unification in Binary Operations
DESCRIPTION: Shows how binary arithmetic operations unify names from input tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/name_inference.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> tensor = torch.randn(3, 3, names=('N', None))
>>> other = torch.randn(3, 3, names=(None, 'C'))
>>> (tensor + other).names
('N', 'C')
```

----------------------------------------

TITLE: Extracting a TorchScript Serialized Model Archive
DESCRIPTION: This snippet demonstrates how to use the unzip command to extract a TorchScript model file (model.pt) and view its internal file structure using the tree command. It shows the standard organization of code, data, and constants in the archive.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/docs/serialization.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
$ unzip model.pt
Archive:  model.pt
  extracting ...

$ tree model/
├── code/
│   ├── __torch__.py
│   ├── __torch__.py.debug_pkl
│   ├── foo/
│   │   ├── bar.py
│   │   ├── bar.py.debug_pkl
├── data.pkl
├── constants.pkl
└── data/
    ├── 0
    └── 1
```

----------------------------------------

TITLE: Accessing a Submodule Attribute via get_attr Node in FX Graph (Python)
DESCRIPTION: This FX graph snippet shows how to add a get_attr node, which reads a submodule or attribute from an enclosing GraphModule. In PyTorch FX, get_attr is used to reference submodules, parameters, or buffers by their names, and to fetch components needed as inputs for subsequent graph nodes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
%name = get_attr[target = name](args = ())
```

----------------------------------------

TITLE: Example Symbolic Function Signature for ONNX Export (Python)
DESCRIPTION: Provides a template for the signature of a symbolic function used to define how a PyTorch operator should be represented using ONNX operators. It shows the expected arguments, including the graph object and input values, and the return type.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_15

LANGUAGE: python
CODE:
```
def foo(
g,
input_0: torch._C.Value,
input_1: torch._C.Value) -> Union[None, torch._C.Value, List[torch._C.Value]]:
"""
Adds the ONNX operations representing this PyTorch function by updating the
graph g with `g.op()` calls.

Args:
  g (Graph): graph to write the ONNX representation into.
  input_0 (Value): value representing the variables which contain
      the first input for this operator.
  input_1 (Value): value representing the variables which contain
      the second input for this operator.

Returns:
  A Value or List of Values specifying the ONNX nodes that compute something
  equivalent to the original PyTorch operator with the given inputs.

  None if it cannot be converted to ONNX.
"""
...
```

----------------------------------------

TITLE: Functional Stage Runtime Construction
DESCRIPTION: This code demonstrates using the functional version of the build_stage API to create a distributed stage runtime after modifications. It requires importing necessary modules and a distributed ProcessGroup, with outputs including a stage runtime.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.pipelining.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
from torch.distributed.pipelining import build_stage
from torch.nn.parallel import DistributedDataParallel

dp_mod = DistributedDataParallel(stage_mod)
info = pipe.info()
stage = build_stage(dp_mod, stage_idx, info, device, group)
```

----------------------------------------

TITLE: Performing Tensor Addition with ATen in Python
DESCRIPTION: This operator performs element-wise addition on tensors with identical dimensions. Dependencies include two input tensors of shape [16, 256, 32, 32] and data type f16. The outputs are tensors of the same shape and type. Key functionality lies in its ability to efficiently compute the sum of two tensors used in neural network layers.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten.add.Tensor
cnt: 12, ((T([16, 256, 32, 32], f16), T([16, 256, 32, 32], f16)), {})
```

----------------------------------------

TITLE: Preventing In-Source CMake Builds (CMake)
DESCRIPTION: Checks if the source directory (CMAKE_SOURCE_DIR) is identical to the binary output directory (CMAKE_BINARY_DIR). If they are the same, it triggers a fatal error, prohibiting in-source builds which are generally discouraged in CMake projects for maintaining a clean source tree.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#_snippet_1

LANGUAGE: CMake
CODE:
```
# Prohibit in-source builds
if(${CMAKE_SOURCE_DIR} STREQUAL ${CMAKE_BINARY_DIR})
  message(FATAL_ERROR "In-source build are not supported")
endif()
```

----------------------------------------

TITLE: SubgraphMatcher Usage Example
DESCRIPTION: Shows how to use SubgraphMatcher to find pattern matches in a larger model graph
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
from torch.fx.passes.utils.matcher_utils import SubgraphMatcher

class LargeModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self._weight = torch.nn.Parameter(torch.ones(3, 3))
        self._bias = torch.nn.Parameter(torch.ones(3, 3))

    def forward(self, x):
        return torch.ops.aten.addmm.default(self._bias, x, self._weight)

large_model_graph = torch.export(LargeModel(), inputs).graph

class PatternModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self._weight_1 = torch.nn.Parameter(torch.ones(5, 5))
        self._bias_1 = torch.nn.Parameter(torch.ones(5, 5))

    def forward(self, x):
        return torch.ops.aten.addmm.default(self._bias_1, x, self._weight_1)

pattern_graph = torch.export(PatternModel(), inputs).graph

subgraph_matcher = SubgraphMatcher(pattern_graph)
match_result = subgraph_matcher.match(large_model_graph)
```

----------------------------------------

TITLE: Implementing aten.sigmoid Operator in PyTorch
DESCRIPTION: This snippet captures the execution of the aten.sigmoid.default operator in PyTorch, applying the sigmoid activation function on input tensors. Dependencies are essential from PyTorch's library. Inputs involve tensors that need a sigmoid transformation, and outputs are the tensors converted to the probability space between 0 and 1. It is well-suited for binary classification models and expects float inputs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_12

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 240, 1, 1], f16),), {})
```

----------------------------------------

TITLE: Element-wise Addition Operations with aten.add.Tensor
DESCRIPTION: Documents usage of the add.Tensor operator which performs element-wise addition between tensors. Various tensor shapes and patterns are shown, including both scalar and tensor additions, with most operations using half-precision (float16) data.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
Operator: aten.add.Tensor
cnt: 2, ((T([8, 64, 192, 256], f16), T([8, 64, 192, 256], f16)), {})
cnt: 4, ((T([8, 128, 96, 128], f16), T([8, 128, 96, 128], f16)), {})
cnt: 16, ((T([8, 256, 48, 64], f16), T([8, 256, 48, 64], f16)), {})
cnt: 16, ((T([8, 512, 24, 32], f16), T([8, 512, 24, 32], f16)), {})
cnt: 8, ((T([8, 1024, 12, 16], f16), T([8, 1024, 12, 16], f16)), {})
cnt: 1, ((T([8, 3, 12, 16, 2], f16), T([1, 1, 12, 16, 2], f32)), {})
cnt: 1, ((T([8, 3, 24, 32, 2], f16), T([1, 1, 24, 32, 2], f32)), {})
cnt: 1, ((T([8, 3, 48, 64, 2], f16), T([1, 1, 48, 64, 2], f32)), {})
cnt: 2, ((T([], f16), 0), {})
cnt: 3, ((T([], f16), T([], f16)), {})
cnt: 3, ((T([8, 3, 48, 64, 85], f16), T([8, 3, 48, 64, 85], f16)), {})
cnt: 1, ((T([8, 3, 48, 64, 85], f16, stride=(0, 0, 0, 0, 0)), T([8, 3, 48, 64, 85], f16)), {})
cnt: 3, ((T([8, 3, 24, 32, 85], f16), T([8, 3, 24, 32, 85], f16)), {})
cnt: 1, ((T([8, 3, 24, 32, 85], f16, stride=(0, 0, 0, 0, 0)), T([8, 3, 24, 32, 85], f16)), {})
cnt: 1, ((T([8, 256, 24, 32], f16), T([8, 256, 24, 32], f16)), {})
cnt: 3, ((T([8, 3, 12, 16, 85], f16), T([8, 3, 12, 16, 85], f16)), {})
cnt: 1, ((T([8, 3, 12, 16, 85], f16, stride=(0, 0, 0, 0, 0)), T([8, 3, 12, 16, 85], f16)), {})
cnt: 3, ((T([8, 512, 12, 16], f16), T([8, 512, 12, 16], f16)), {})
cnt: 1, ((T([8, 512, 12, 16], f16, stride=(393216, 192, 16, 1)), T([8, 512, 12, 16], f16)), {})
cnt: 1, ((T([8, 512, 24, 32], f16, stride=(589824, 768, 32, 1)), T([8, 512, 24, 32], f16)), {})
cnt: 1, ((T([8, 256, 48, 64], f16, stride=(1179648, 3072, 64, 1)), T([8, 256, 48, 64], f16)), {})
```

----------------------------------------

TITLE: Unbinding and Unsqueezing Tensors using aten.unbind and aten.unsqueeze_ - Python
DESCRIPTION: Presents examples of aten.unbind.int for splitting tensors along a dimension and aten.unsqueeze_.default for inserting singleton dimensions (in-place). Operates over tensors of various ranks and dtypes, highlighting shape manipulations central to neural network data flows. Requires PyTorch and appropriate tensor shapes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
Operator: aten.unbind.int
cnt: 12, ((T([2, 16, 64], f32),), {})
cnt: 12, ((T([2, 12, 14, 3], i64),), {})
Operator: aten.unsqueeze_.default
cnt: 1, ((T([2, 12, 64, 192], f32), 1), {})
cnt: 12, ((T([12, 14, 3], i64), 0), {})
cnt: 48, ((T([2, 12, 64, 64], f16), 2), {})
```

----------------------------------------

TITLE: Cloning Tensors in PyTorch - Python
DESCRIPTION: Demonstrates the usage of aten.clone.default to create copies of existing tensors while preserving the original tensor's state. Useful to prevent unintentional modifications. Requires original tensor to be defined, returns a new tensor with identical content.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v3_large_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
Operator: aten.clone.default
cnt: 1, ((T([32, 3, 224, 224], f16),), {})
cnt: 1, ((T([32, 16, 112, 112], f16),), {})
cnt: 1, ((T([32, 240, 28, 28], f16),), {})
cnt: 1, ((T([32, 240, 14, 14], f16),), {})
cnt: 2, ((T([32, 200, 14, 14], f16),), {})
cnt: 4, ((T([32, 184, 14, 14], f16),), {})
cnt: 2, ((T([32, 480, 14, 14], f16),), {})
cnt: 3, ((T([32, 672, 14, 14], f16),), {})
cnt: 1, ((T([32, 672, 7, 7], f16),), {})
cnt: 5, ((T([32, 960, 7, 7], f16),), {})
cnt: 1, ((T([32, 1280], f16),), {})

```

----------------------------------------

TITLE: Configuring Expiration Timers in PyTorch Distributed Elastic
DESCRIPTION: Function to configure the expiration timer settings. It is part of the client methods for interacting with the timer system.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/timer.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
torch.distributed.elastic.timer.configure
```

----------------------------------------

TITLE: Incorrect TorchScript Module Type Usage
DESCRIPTION: This example shows incorrect usage by attempting to instantiate a TestModule within TorchScript scope, which will result in a RuntimeError since module constructors cannot be invoked within TorchScript.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
import torch

class TestModule(torch.nn.Module):
    def __init__(self, v):
        super().__init__()
        self.x = v

    def forward(self, x: int):
        return self.x + x

class MyModel:
    def __init__(self, v: int):
        self.val = v

    @torch.jit.export
    def doSomething(self, val: int) -> int:
        # error: should not invoke the constructor of module type
        myModel = TestModule(self.val)
        return myModel(val)

# m = torch.jit.script(MyModel(2)) # Results in below RuntimeError
# RuntimeError: Could not get name of python class object
```

----------------------------------------

TITLE: Building PyTorch Android from Source
DESCRIPTION: This bash script outlines the process of building PyTorch Android from source. It includes cloning the repository, updating submodules, and running the build script.
SOURCE: https://github.com/pytorch/pytorch/blob/main/android/README.md#2025-04-22_snippet_2

LANGUAGE: Bash
CODE:
```
git clone https://github.com/pytorch/pytorch.git
cd pytorch
git submodule update --init --recursive
bash ./scripts/build_pytorch_android.sh
```

----------------------------------------

TITLE: Using ATen _softmax Operator in PyTorch
DESCRIPTION: Demonstrates the `aten._softmax` operator which normalizes input data across specified dimensions. Various tensor shapes such as [256, 1024, 1024], [256, 256, 256] with data type f16 and dimension -1 are used, showcasing adaptability in tensor operations. Dependencies include the input tensor and the dimension along which to apply the softmax.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_2

LANGUAGE: plaintext
CODE:
```
Operator: aten._softmax.default
cnt: 1, ((T([256, 1024, 1024], f16), -1, False), {})
cnt: 2, ((T([256, 256, 256], f16), -1, False), {})
cnt: 1, ((T([256, 64, 64], f16), -1, False), {})
```

----------------------------------------

TITLE: Calling aten.nll_loss_forward.default (Python)
DESCRIPTION: Computes the Negative Log Likelihood (NLL) loss between log-probabilities and a target tensor. Used as a loss function for classification. Requires log-probabilities, target indices, optional weights, reduction mode, and an ignore index.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_28

LANGUAGE: Python
CODE:
```
((T([1024, 50265], f16), T([1024], i64), None, 1, -100), {})
```

----------------------------------------

TITLE: Disabling FP16 Reduced Precision Reduction (Python)
DESCRIPTION: Shows the Python code to disable reduced precision reductions within FP16 matrix multiplications performed by the CUDA backend. Setting this flag to `False` ensures that full precision reductions are used, potentially impacting performance.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_5

LANGUAGE: python
CODE:
```
torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False
```

----------------------------------------

TITLE: Embedding Layer Lookups and Backward Passes in PyTorch (Python)
DESCRIPTION: These snippets detail usage of torch.embedding and embedding_dense_backward, for forward and backward passes in embedding/table lookup layers. Arguments include weight matrices, index tensors, and scale/offset values. The backward variant computes gradients; input tensors must match expected shapes for correct mapping between ids and embeddings.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
Operator: aten.embedding.default
cnt: 1, ((T([30522, 768], f16), T([4, 128], i64), 0), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.embedding.default
cnt: 1, ((T([2, 768], f16), T([4, 128], i64, stride=(0, 1))), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.embedding.default
cnt: 1, ((T([512, 768], f16), T([4, 128], i64), 0), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.embedding_dense_backward.default
cnt: 1, ((T([4, 128, 768], f16), T([4, 128], i64), 512, 0, False), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.embedding_dense_backward.default
cnt: 1, ((T([4, 128, 768], f16), T([4, 128], i64, stride=(0, 1)), 2, -1, False), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.embedding_dense_backward.default
cnt: 1, ((T([4, 128, 768], f16), T([4, 128], i64), 30522, 0, False), {})
```

----------------------------------------

TITLE: Profiling Convolution Backward Pass in PyTorch
DESCRIPTION: Log of backward pass operations for convolutional layers showing gradients flowing through the network. Each entry includes input gradients, output gradients, weights, and configuration parameters for the operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
Operator: aten.convolution_backward.default
cnt: 1, ((T([64, 1280, 8, 8], f16), T([64, 1536, 8, 8], f16), T([1280, 1536, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 3, ((T([64, 1536, 8, 8], f16), T([64, 512, 8, 8], f16), T([1536, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([64, 512, 8, 8], f16), T([64, 1536, 8, 8], f16), T([512, 1536, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

----------------------------------------

TITLE: Conceptual Implementation of Dynamo's Compilation Process
DESCRIPTION: This snippet provides a conceptual implementation of how Dynamo transforms a user function. It shows the high-level logic of how guard functions are checked and compiled code is executed, falling back to recompilation when necessary.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_overview.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
def compiled_example(a, b):
    L = {'a': a, 'b': b}
    for guard, code in get_cache_entries():
        if guard(L):
            return code(a, b)
    recompile_and_add_another_cache_entry()
```

----------------------------------------

TITLE: Analyzing Tensor Operations in PyTorch
DESCRIPTION: This code snippet represents a series of tensor operations in PyTorch, including their shapes, data types, and occurrence counts. It showcases various tensor configurations used in a neural network model, primarily focusing on f16 (float16) data type.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_12

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 240, 56, 56], f16), [60, 60, 60, 60], 1), {})
cnt: 3, ((T([64, 56, 28, 28], f16), [28, 28], 1), {})
cnt: 6, ((T([64, 336, 28, 28], f16), [168, 168], 1), {})
cnt: 1, ((T([64, 336, 28, 28], f16), [112, 112, 112], 1), {})
cnt: 3, ((T([64, 104, 14, 14], f16), [52, 52], 1), {})
cnt: 3, ((T([64, 624, 14, 14], f16), [156, 156, 156, 156], 1), {})
cnt: 3, ((T([64, 624, 14, 14], f16), [312, 312], 1), {})
cnt: 3, ((T([64, 160, 14, 14], f16), [80, 80], 1), {})
cnt: 3, ((T([64, 480, 14, 14], f16), [120, 120, 120, 120], 1), {})
cnt: 3, ((T([64, 480, 14, 14], f16), [240, 240], 1), {})
cnt: 1, ((T([64, 960, 14, 14], f16), [240, 240, 240, 240], 1), {})
cnt: 3, ((T([64, 1584, 7, 7], f16), [396, 396, 396, 396], 1), {})
cnt: 3, ((T([64, 1584, 7, 7], f16), [792, 792], 1), {})
```

----------------------------------------

TITLE: Unsupported Write Indexing with Multiple High-Rank Tensors (Python)
DESCRIPTION: Illustrates an unsupported tensor indexing pattern for writing data. It involves multiple tensor indices where at least one index tensor has a rank of 2 or higher. Workarounds include using a single high-rank index or multiple consecutive rank-1 indices.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_10

LANGUAGE: python
CODE:
```
data[torch.tensor([[1, 2], [2, 3]]), torch.tensor([2, 3])] = new_data
# Workarounds: use single tensor index with rank >= 2,
#              or multiple consecutive tensor indices with rank == 1.
```

----------------------------------------

TITLE: PyTorch Tensor Operations - Common Patterns
DESCRIPTION: Collection of frequently used tensor operations including softmax, matrix multiplications (mm, bmm), embeddings, and layer normalization. Operations are shown with their input tensor shapes, data types (primarily float16/f16), and execution counts.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BigBird_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
# Example tensor operation patterns:

# Softmax operations
aten._softmax.default((T([1, 12, 64, 1024], f16), -1, False))

# Matrix multiplications
aten.bmm.default((T([12, 64, 64], f16), T([12, 64, 1024], f16)))

# Embedding operations
aten.embedding.default((T([50358, 768], f16), T([1, 1024], i64), 0))

# Layer normalization
aten.native_layer_norm.default((T([1, 1024, 768], f16), [768], T([768], f16), T([768], f16), 1e-12))
```

----------------------------------------

TITLE: Demonstrating TorchScript Variable Scope Restrictions
DESCRIPTION: Example showcasing TorchScript's variable resolution rules. Variables must have the same type on all paths and must be defined on all branches before use.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_13

LANGUAGE: Python
CODE:
```
@torch.jit.script
def foo(x):
    if x < 0:
        y = 4
    print(y)
```

----------------------------------------

TITLE: Debugging TorchScript Execution Using Scripting and Tracing (Python)
DESCRIPTION: Provides an example of integrating scripted, traced, and native Python functions for debugging. Requires PyTorch. Demonstrates how to use the PYTORCH_JIT environment variable to disable JIT ahead-of-time compilation, allowing use of Python debugging tools (pdb) in scripted code. Shows tracing over a function that internally calls a scripted function; illustrates input/output type constraints.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
@torch.jit.script\ndef scripted_fn(x : torch.Tensor):\n    for i in range(12):\n        x = x + x\n    return x\n\ndef fn(x):\n    x = torch.neg(x)\n    import pdb; pdb.set_trace()\n    return scripted_fn(x)\n\ntraced_fn = torch.jit.trace(fn, (torch.rand(4, 5),))\ntraced_fn(torch.rand(3, 4))\n
```

----------------------------------------

TITLE: Initializing Single-threaded Static Runtime in PyTorch
DESCRIPTION: Shows two ways to initialize Static Runtime in single-threaded mode with intra-op parallelism. The runtime instance executes the TorchScript module with optimized memory management.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/runtime/static/README.md#2025-04-22_snippet_0

LANGUAGE: cpp
CODE:
```
  // m is the TorchScript module
  auto runtime = StaticRuntime(m, opts);
  auto output = runtime.run(args, kwargs);
```

LANGUAGE: cpp
CODE:
```
  auto mod = PrepareForStaticRuntime(m);
  auto runtime = StaticRuntime(mod, opts);
  auto output = runtime.run(args, kwargs);
```

----------------------------------------

TITLE: Checking Tracing Status with torch.jit.is_tracing() in TorchScript
DESCRIPTION: Returns a boolean value indicating whether the current program is traced by torch.jit.trace / torch.jit.trace_module or not.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_40

LANGUAGE: python
CODE:
```
torch.jit.is_tracing()
```

----------------------------------------

TITLE: Registering Custom PyTorch Operator Kernel Function (C++)
DESCRIPTION: Demonstrates registering a C++ function (`my_kernel_cpu`) as a kernel implementation for a custom operator named `my_namespace::my_op`. The registration uses `torch::RegisterOperators` and specifies the kernel function via `decltype` and function pointer, associating it with the CPU dispatch key. It's recommended to place the kernel function within an anonymous namespace.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_0

LANGUAGE: cpp
CODE:
```
namespace { Tensor my_kernel_cpu(const Tensor& a, const Tensor& b) {...} }

static auto registry = torch::RegisterOperators()
   .op("my_namespace::my_op",  torch::RegisterOperators::options()
       .kernel<decltype(my_kernel_cpu), &my_kernel_cpu>(CPU()));
```

----------------------------------------

TITLE: Non-batchwise Operation Fallback for PyTorch NJT
DESCRIPTION: This snippet suggests using an 'unbind()'-based fallback for non-batchwise operations on Nested Jagged Tensors in PyTorch. This approach can help quickly implement operations that don't operate over the batch dimension.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_12

LANGUAGE: Python
CODE:
```
# For non-batchwise operations
# Use unbind()-based fallback
# Example:
# result = torch.stack([op(tensor) for tensor in njt.unbind()])
```

----------------------------------------

TITLE: Benchmarking jacfwd vs jacrev for Wide Matrix Case - Python
DESCRIPTION: Runs the same jacfwd/jacrev timing experiment on the reverse case (much more inputs than outputs; wide matrix). Initializes appropriate random tensors and prints both times. Requires functorch and Timer. Compares performance implications of matrix shape for autodiff.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
Din = 2048
Dout = 32
weight = torch.randn(Dout, Din)
bias = torch.randn(Dout)
x = torch.randn(Din)

using_fwd = Timer(stmt="jacfwd(predict, argnums=2)(weight, bias, x)", globals=globals())
using_bwd = Timer(stmt="jacrev(predict, argnums=2)(weight, bias, x)", globals=globals())

jacfwd_timing = using_fwd.timeit(500)
jacrev_timing = using_bwd.timeit(500)

print(f'jacfwd time: {jacfwd_timing}')
print(f'jacrev time: {jacrev_timing}')
```

----------------------------------------

TITLE: Distributed Computation Example for Dependency Analysis
DESCRIPTION: Example demonstrating distributed computation with RPC calls, illustrating the challenges in computing dependencies for the distributed autograd graph.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/distributed_autograd.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import torch
import torch.distributed.rpc as rpc

a = torch.rand((3, 3), requires_grad=True)
b = torch.rand((3, 3), requires_grad=True)
c = torch.rand((3, 3), requires_grad=True)

d = rpc.rpc_sync("worker1", torch.add, args=(a, b))
e = rpc.rpc_sync("worker1", torch.mul, args=(b, c))
loss = d.sum()
```

----------------------------------------

TITLE: Implementing Runtime Type Checking in Python DataPipes
DESCRIPTION: Shows how to use the @runtime_validation decorator and runtime_validation_disabled context manager for runtime type checking in DataPipe classes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/typing.ipynb#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
from torch.utils.data import runtime_validation, runtime_validation_disabled

class DP(IterDataPipe[Tuple[int, T_co]]):
    def __init__(self, datasource):
        self.ds = datasource

    @runtime_validation
    def __iter__(self):
        for d in self.ds:
            yield d
```

LANGUAGE: python
CODE:
```
dp = DP([(1, 1), (2, 2), ('3', 3)])
for d in dp:
    print(d)
```

LANGUAGE: python
CODE:
```
with runtime_validation_disabled():
    print(list(dp))
```

LANGUAGE: python
CODE:
```
dp = DP([(1, 1), (2, 2), [3, 3]])
for d in dp:
    print(d)
```

LANGUAGE: python
CODE:
```
with runtime_validation_disabled():
    print(list(dp))
```

LANGUAGE: python
CODE:
```
dp = DP([(1, 1), (2, '2'), (3, 3.)])
print(list(dp))
```

----------------------------------------

TITLE: Creating and Inspecting Uncoalesced Sparse COO Tensor in PyTorch
DESCRIPTION: This snippet demonstrates how to create an uncoalesced sparse COO tensor and inspect its properties. It shows the tensor representation with duplicate indices and how coalescing combines duplicate values.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_11

LANGUAGE: Python
CODE:
```
>>> i = [[1, 1]]
>>> v =  [3, 4]
>>> s=torch.sparse_coo_tensor(i, v, (3,))
>>> s
tensor(indices=tensor([[1, 1]]),
       values=tensor(  [3, 4]),
       size=(3,), nnz=2, layout=torch.sparse_coo)

>>> s.coalesce()
tensor(indices=tensor([[1]]),
       values=tensor([7]),
       size=(3,), nnz=1, layout=torch.sparse_coo)
```

----------------------------------------

TITLE: Log Softmax Backward Data Operation in PyTorch
DESCRIPTION: The aten._log_softmax_backward_data.default operator is used for gradient computation of the logarithmic softmax function. It processes inputs and gradients, both shaped [1, 512] in float16 precision, and is necessary for backward passes in training.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
Operator: aten._log_softmax_backward_data.default
cnt: 2, ((T([1, 512], f16), T([1, 512], f16), 1, f16), {})
```

----------------------------------------

TITLE: Logging Tensor Subclass Implementation
DESCRIPTION: Implementation of a LoggingTensor subclass that logs all function and method calls except __repr__ to avoid infinite recursion.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
class LoggingTensor(torch.Tensor):
    @classmethod
    def __torch_function__(cls, func, types, args=(), kwargs=None):
        # NOTE: Logging calls Tensor.__repr__, so we can't log __repr__ without infinite recursion
        if func is not torch.Tensor.__repr__:
            logging.info(f"func: {func.__name__}, args: {args!r}, kwargs: {kwargs!r}")
        if kwargs is None:
            kwargs = {}
        return super().__torch_function__(func, types, args, kwargs)
```

----------------------------------------

TITLE: Performing Matrix Multiplication in PyTorch
DESCRIPTION: Executes matrix multiplication (mm) operations on half-precision tensors. Used for linear transformations in the neural network, typically between reshaped input/output tensors and weight matrices.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/tts_angular_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
aten.mm.default(T([3200, 768], f16), T([768, 256], f16, stride=(1, 768)))
```

----------------------------------------

TITLE: Profiling Batch Normalization Operations in PyTorch
DESCRIPTION: This snippet shows the tensor shapes and parameters for batch normalization operations at different layers of the model. It includes running mean and variance calculations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2net50_14w_8s_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
Operator: aten.native_batch_norm.default
cnt: 1, ((T([128, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})
cnt: 3, ((T([128, 112, 56, 56], f16), T([112], f16), T([112], f16), T([112], f16), T([112], f16), True, 0.1, 1e-05), {})
cnt: 21, ((T([128, 14, 56, 56], f16), T([14], f16), T([14], f16), T([14], f16), T([14], f16), True, 0.1, 1e-05), {})
```

----------------------------------------

TITLE: Multiplexing DataPipes in PyTorch
DESCRIPTION: Shows how to use the mux() method to combine multiple DataPipes into a single DataPipe, interleaving their elements.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
dp1 = ExampleIterPipe(3)
dp2 = ExampleIterPipe(3).map(lambda x: x * 10)
dp3 = ExampleIterPipe(3).map(lambda x: x * 100)

dp = dp1.mux(dp2, dp3)
for i in dp:
    print(i)
```

----------------------------------------

TITLE: Executing and Verifying Lazy Tensor Results in PyTorch
DESCRIPTION: This snippet shows how to trigger the execution of Lazy Tensor computations and verify the results against the original function for both false and true conditions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
lazy_result = add_two_maybe(t_lazy, maybe_false_lazy)
print(lazy_result)
assert lazy_result.cpu() == add_two_maybe(t, maybe_false)

maybe_true_lazy = torch.BoolTensor([1]).to(dev)
lazy_result = add_two_maybe(t_lazy, maybe_true_lazy)
assert lazy_result.cpu() == add_two_maybe(t, maybe_true)
```

----------------------------------------

TITLE: Defining Invalid DataPipe Classes in Python
DESCRIPTION: Demonstrates invalid DataPipe class definitions with mismatched return type hints for the __iter__ method.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/typing.ipynb#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
class InvalidDP1(IterDataPipe[int]):
    def __iter__(self) -> str:
        pass
```

LANGUAGE: python
CODE:
```
class InvalidDP2(IterDataPipe[int]):
    def __iter__(self) -> Iterator[str]:
        pass
```

----------------------------------------

TITLE: Custom Operator Decomposition for conv2d - PyTorch - Python
DESCRIPTION: This code customizes the decomposition of the conv2d operator during IR lowering by assigning a custom function to torch.ops.aten.conv2d.default in the decomposition table. It then runs decompositions on the exported program to obtain an IR where conv2d is replaced with twice the result of a convolution op. Demonstrates extension of PyTorch's export and decomposition system with user-defined behavior.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
# Lower to core aten inference IR, but customize conv2d
decomp_table = torch.export.default_decompositions()

def my_awesome_custom_conv2d_function(x, weight, bias, stride=[1, 1], padding=[0, 0], dilation=[1, 1], groups=1):
    return 2 * torch.ops.aten.convolution(x, weight, bias, stride, padding, dilation, False, [0, 0], groups)

decomp_table[torch.ops.aten.conv2d.default] = my_awesome_conv2d_function
ep_for_inference = ep_for_training.run_decompositions(decomp_table)

print(ep_for_inference)
```

----------------------------------------

TITLE: Function Decoration in TorchScript
DESCRIPTION: Shows different ways to decorate functions in TorchScript, including making functions ignored, unused, or explicitly exported.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
# Same behavior as pre-PyTorch 1.2
@torch.jit.script
def some_fn():
    return 2

# Marks a function as ignored, if nothing
# ever calls it then this has no effect
@torch.jit.ignore
def some_fn2():
    return 2

# As with ignore, if nothing calls it then it has no effect.
# If it is called in script it is replaced with an exception.
@torch.jit.unused
def some_fn3():
  import pdb; pdb.set_trace()
  return 4

# Doesn't do anything, this function is already
# the main entry point
@torch.jit.export
def some_fn4():
    return 2
```

----------------------------------------

TITLE: Annotating Instance Data Attributes in TorchScript
DESCRIPTION: Example showing how to annotate instance data attributes in a Module class. Instance attributes can be optionally marked as Final to indicate they cannot be reassigned outside __init__.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
import torch

class MyModule(torch.nn.Module):
    offset_: int

def __init__(self, offset):
    self.offset_ = offset

...
```

----------------------------------------

TITLE: Reinforcing Types for DataPipe Instances in Python
DESCRIPTION: Demonstrates how to use the reinforce_type method to specify more precise types for DataPipe instances at runtime.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/typing.ipynb#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
T = TypeVar('T', int, str)
ds = list(range(10))
```

LANGUAGE: python
CODE:
```
class DP(IterDataPipe[T]):
    def __init__(self, ds):
        self.ds = ds

    def __iter__(self):
        for d in self.ds:
            yield d
dp = DP(ds).reinforce_type(int)
```

LANGUAGE: python
CODE:
```
class DP(IterDataPipe[T]):
    def __init__(self, ds):
        self.ds = ds

    @runtime_validation
    def __iter__(self):
        for d in self.ds:
            yield d
```

LANGUAGE: python
CODE:
```
dp = DP(ds).reinforce_type(float)
```

LANGUAGE: python
CODE:
```
dp = DP(ds).reinforce_type(str)
list(dp)
```

LANGUAGE: python
CODE:
```
with runtime_validation_disabled():
    print(list(dp))
```

LANGUAGE: python
CODE:
```
dp = DP(ds).reinforce_type(int)
print(list(dp))
```

LANGUAGE: python
CODE:
```
class DP(IterDataPipe[Union[int, str]]):
    def __init__(self, label):
        if label == 'int':
            self.reinforce_type(int)
        elif label == 'str':
            self.reinforce_type(str)
```

LANGUAGE: python
CODE:
```
dp = DP('int')
print(dp.type)
```

LANGUAGE: python
CODE:
```
dp = DP('str')
print(dp.type)
```

LANGUAGE: python
CODE:
```
dp = DP('')
print(dp.type)
```

----------------------------------------

TITLE: Defining Control Flow Statements in TorchScript
DESCRIPTION: Specifies the syntax for control flow statements in TorchScript, including raise, assert, return, del, pass, print, break, and continue.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_26

LANGUAGE: python
CODE:
```
raise_stmt ::=  "raise" [expression ["from" expression]]

assert_stmt ::=  "assert" expression ["," expression]

return_stmt ::=  "return" [expression_list]

del_stmt ::=  "del" target_list

pass_stmt ::= "pass"

print_stmt ::= "print" "(" expression  [, expression] [.format{expression_list}] ")"

break_stmt ::= "break"

continue_stmt ::= "continue"
```

----------------------------------------

TITLE: Checking Supported Ops in Lazy Tensor
DESCRIPTION: Demonstrates how to check which operations are supported by Lazy Tensor for a given model, helping identify potential performance bottlenecks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
torch._lazy.metrics.reset()
train(...)
print(torch._lazy.metrics.counter_names())
```

----------------------------------------

TITLE: Implementing Length Method for DataPipe
DESCRIPTION: Implementation of the __len__ method that returns the length of the source DataPipe. Shows proper length computation pattern for chained DataPipes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/datapipes/README.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
class MapperIterDataPipe(IterDataPipe):
    ...

    def __len__(self):
        return len(self.dp)
```

----------------------------------------

TITLE: Unsupported Autocast Usage Pattern
DESCRIPTION: Example showing an uncommon autocast usage pattern that is not supported in JIT scripting, specifically reusing an autocast instance in nested contexts.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/JIT-AUTOCAST.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import torch
from torch.cpu.amp import autocast

@torch.jit.script
def fn(a, b, c, d):
    with autocast(enabled=True) as autocast_instance: # not supported
        ...
        with autocast_instance:
            ...
```

----------------------------------------

TITLE: Exported Program Graph Representation with Dynamic Shapes
DESCRIPTION: The textual representation of an ExportedProgram showing forward method with symbolic shape 's0'. Includes input parameters, operations, and range constraints for dynamic dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
ExportedProgram:
class GraphModule(torch.nn.Module):
    def forward(self, p_branch1_0_weight: "f32[32, 64]", p_branch1_0_bias: "f32[32]", p_branch2_0_weight: "f32[64, 128]", p_branch2_0_bias: "f32[64]", c_buffer: "f32[32]", x1: "f32[s0, 64]", x2: "f32[s0, 128]"):

         # code: out1 = self.branch1(x1)
        linear: "f32[s0, 32]" = torch.ops.aten.linear.default(x1, p_branch1_0_weight, p_branch1_0_bias)
        relu: "f32[s0, 32]" = torch.ops.aten.relu.default(linear)

         # code: out2 = self.branch2(x2)
        linear_1: "f32[s0, 64]" = torch.ops.aten.linear.default(x2, p_branch2_0_weight, p_branch2_0_bias)
        relu_1: "f32[s0, 64]" = torch.ops.aten.relu.default(linear_1)

         # code: return (out1 + self.buffer, out2)
        add: "f32[s0, 32]" = torch.ops.aten.add.Tensor(relu, c_buffer)
        return (add, relu_1)

Range constraints: {s0: VR[0, int_oo]}
```

----------------------------------------

TITLE: Calling Mean Reduction (aten.mean.dim) in PyTorch (Python)
DESCRIPTION: This snippet represents invocation of the ATen 'mean' operator along specified dimensions on a 4D float16 tensor. It expects a tensor of shape [32, 1280, 7, 7] and reduces along the last two axes, returning a mean-reduced tensor. No explicit dependencies apart from PyTorch are required; shapes and dtype are critical for correct usage.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mnasnet1_0_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten.mean.dim
cnt: 1, ((T([32, 1280, 7, 7], f16), [2, 3]), {})
```

----------------------------------------

TITLE: Invoking aten.threshold_backward.default Threshold Backward Pass in PyTorch ATen
DESCRIPTION: Documents observed calls to the backward pass for a threshold operation (`aten.threshold_backward.default`), likely related to ReLU backward pass. It lists the different gradient output and original input tensor shapes (all f16), the threshold value (0), and their respective call counts.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_24

LANGUAGE: text
CODE:
```
Operator: aten.threshold_backward.default
cnt: 9, ((T([128, 384, 1, 1], f16), T([128, 384, 1, 1], f16), 0), {})
cnt: 2, ((T([128, 128, 1, 1], f16), T([128, 128, 1, 1], f16), 0), {})
cnt: 1, ((T([128, 64, 1, 1], f16), T([128, 64, 1, 1], f16), 0), {})
```

----------------------------------------

TITLE: Applying HardTanh Function in PyTorch
DESCRIPTION: The 'aten.hardtanh_.default' operator applies a Hard Tanh activation function in-place on tensors, ensuring outputs remain within specified limits [0.0, 6.0]. This operator is crucial for keeping outputs standardized during processing in neural networks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v2_training.txt#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
Operator: aten.hardtanh_.default
cnt: 2, ((T([96, 32, 112, 112], f16), 0.0, 6.0), {})
cnt: 1, ((T([96, 96, 112, 112], f16), 0.0, 6.0), {})
cnt: 1, ((T([96, 96, 56, 56], f16), 0.0, 6.0), {})
cnt: 3, ((T([96, 144, 56, 56], f16), 0.0, 6.0), {})
cnt: 1, ((T([96, 144, 28, 28], f16), 0.0, 6.0), {})
```

----------------------------------------

TITLE: Attaching Extra Files to TorchScript Exports in PyTorch C++
DESCRIPTION: This snippet demonstrates how to attach additional metadata files to TorchScript model archives using C++. The SetExportModuleExtraFilesHook function is used to define a global hook that bundles extra files like metadata with every TorchScript model exported in the current process. It requires access to the PyTorch C++ API and environment variables for customizing metadata.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/large_scale_deployments.rst#2025-04-22_snippet_2

LANGUAGE: cpp
CODE:
```
SetExportModuleExtraFilesHook([](const Module&) {
    ExtraFilesMap files;
    files["producer_info.json"] = "{\"user\": \"" + getenv("USER") + "\"}";
    return files;
});
```

----------------------------------------

TITLE: Clone Operation
DESCRIPTION: Basic tensor clone operation for input data with shape [32, 3, 224, 224] in half precision format.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dpn107_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
Operator: aten.clone.default
cnt: 1, ((T([32, 3, 224, 224], f16),), {})
```

----------------------------------------

TITLE: Tracking Convolution Backward Operations in Neural Network
DESCRIPTION: Lists all convolution backward operations used for gradient calculation during backpropagation. Each entry shows the gradient flow through the network layers from the output back to input, with tensor shapes and parameters matching the forward operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hardcorenas_a_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
Operator: aten.convolution_backward.default
cnt: 1, ((T([128, 1280, 1, 1], f16), T([128, 960, 1, 1], f16), T([1280, 960, 1, 1], f16), [1280], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([128, 960, 7, 7], f16), T([128, 192, 7, 7], f16), T([960, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 192, 7, 7], f16), T([128, 1152, 7, 7], f16), T([192, 1152, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 1152, 1, 1], f16), T([128, 288, 1, 1], f16), T([1152, 288, 1, 1], f16), [1152], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([128, 288, 1, 1], f16), T([128, 1152, 1, 1], f16), T([288, 1152, 1, 1], f16), [288], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([128, 1152, 7, 7], f16), T([128, 1152, 7, 7], f16), T([1152, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 1152, [True, True, False]), {})
cnt: 1, ((T([128, 1152, 7, 7], f16), T([128, 192, 7, 7], f16), T([1152, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 192, 7, 7], f16), T([128, 672, 7, 7], f16), T([192, 672, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 672, 1, 1], f16), T([128, 168, 1, 1], f16), T([672, 168, 1, 1], f16), [672], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 2, ((T([128, 168, 1, 1], f16), T([128, 672, 1, 1], f16), T([168, 672, 1, 1], f16), [168], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([128, 672, 7, 7], f16), T([128, 672, 14, 14], f16), T([672, 1, 5, 5], f16), [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 672, [True, True, False]), {})
cnt: 2, ((T([128, 672, 14, 14], f16), T([128, 112, 14, 14], f16), T([672, 112, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 112, 14, 14], f16), T([128, 672, 14, 14], f16), T([112, 672, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 672, 14, 14], f16), T([128, 672, 14, 14], f16), T([672, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 672, [True, True, False]), {})
cnt: 1, ((T([128, 112, 14, 14], f16), T([128, 480, 14, 14], f16), T([112, 480, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 480, 1, 1], f16), T([128, 120, 1, 1], f16), T([480, 120, 1, 1], f16), [480], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 2, ((T([128, 120, 1, 1], f16), T([128, 480, 1, 1], f16), T([120, 480, 1, 1], f16), [120], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 2, ((T([128, 480, 14, 14], f16), T([128, 480, 14, 14], f16), T([480, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 480, [True, True, False]), {})
cnt: 2, ((T([128, 480, 14, 14], f16), T([128, 80, 14, 14], f16), T([480, 80, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 80, 14, 14], f16), T([128, 480, 14, 14], f16), T([80, 480, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 80, 14, 14], f16), T([128, 240, 14, 14], f16), T([80, 240, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

----------------------------------------

TITLE: Profiling aten.mm.default Calls - PyTorch - Python
DESCRIPTION: Captures operations using the matrix multiplication (aten.mm.default) operator with inputs as float16 tensors of specified shapes and strides. Dependencies are two-dimensional or broadcast-compatible tensors of compatible dimensions. Inputs are the input tensors; outputs are result tensors of the appropriate shape, with operator count and stride variations noted.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/selecsls42b_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
Operator: aten.mm.default
cnt: 1, ((T([128, 1000], f16), T([1000, 1024], f16)), {})
cnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 1024], f16)), {})
```

----------------------------------------

TITLE: Implementing Post Training Dynamic Quantization in PyTorch
DESCRIPTION: Demonstrates how to apply dynamic quantization to a simple linear layer model. The weights are quantized to int8 while activations remain in fp32, suitable for LSTM and Transformer models with small batch sizes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch

# define a floating point model
class M(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = torch.nn.Linear(4, 4)

    def forward(self, x):
        x = self.fc(x)
        return x

# create a model instance
model_fp32 = M()
# create a quantized model instance
model_int8 = torch.ao.quantization.quantize_dynamic(
    model_fp32,  # the original model
    {torch.nn.Linear},  # a set of layers to dynamically quantize
    dtype=torch.qint8)  # the target dtype for quantized weights

# run the model
input_fp32 = torch.randn(4, 4, 4, 4)
res = model_int8(input_fp32)
```

----------------------------------------

TITLE: Executing Release Branch Cut Script (Bash)
DESCRIPTION: This Bash command executes the `cut-release-branch.sh` script, which is responsible for creating the new release branches for PyTorch. The `DRY_RUN=disabled` prefix is crucial as it instructs the script to perform the actual branch creation rather than just simulating the process.
The script is expected to create two branches: `release/{MAJOR}.{MINOR}` for ongoing development and `orig/release/{MAJOR}.{MINOR}` as a backup or reference point.
SOURCE: https://github.com/pytorch/pytorch/blob/main/RELEASE.md#_snippet_2

LANGUAGE: bash
CODE:
```
DRY_RUN=disabled scripts/release/cut-release-branch.sh
```

----------------------------------------

TITLE: Tracking Tensor Summation Operations in PyTorch
DESCRIPTION: Records sum operations across specific dimensions of tensors. These operations typically calculate spatial averages over feature maps or sum across the batch dimension for loss calculation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([128, 1000], f16), [0], True), {})
cnt: 2, ((T([128, 960, 7, 7], f16), [2, 3], True), {})
cnt: 1, ((T([128, 672, 7, 7], f16), [2, 3], True), {})
cnt: 1, ((T([128, 672, 14, 14], f16), [2, 3], True), {})
cnt: 1, ((T([128, 480, 14, 14], f16), [2, 3], True), {})
cnt: 2, ((T([128, 120, 28, 28], f16), [2, 3], True), {})
cnt: 1, ((T([128, 72, 28, 28], f16), [2, 3], True), {})
```

----------------------------------------

TITLE: Setup PyTorch Local Linting Environment (bash)
DESCRIPTION: Executes the `make setup-lint` command, which runs a target defined in the project's Makefile to install the necessary prerequisites for running local linting checks. This includes tools like `lintrunner` and `mypy`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_17

LANGUAGE: bash
CODE:
```
make setup-lint
```

----------------------------------------

TITLE: Installing torchvision Dependency for MNIST Dataset
DESCRIPTION: Installs the torchvision package, which provides the MNIST dataset with images of handwritten digits for training.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_7

LANGUAGE: shell
CODE:
```
pip install torchvision
```

----------------------------------------

TITLE: Invoking aten.unbind.int Operator (Log)
DESCRIPTION: Log entries detailing calls to the PyTorch `aten.unbind.int` operator. Each line shows the invocation count (`cnt`) and the input tensor being unbound, including its shape, data type (f16), and stride information. The default dimension (0) is implied.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/crossvit_9_240_training.txt#2025-04-22_snippet_9

LANGUAGE: text
CODE:
```
Operator: aten.unbind.int
cnt: 3, ((T([3, 64, 4, 401, 32], f16, stride=(128, 153984, 32, 384, 1)),), {})
cnt: 9, ((T([3, 64, 4, 197, 64], f16, stride=(256, 151296, 64, 768, 1)),), {})
cnt: 1, ((T([2, 64, 1000], f16),), {})
```

----------------------------------------

TITLE: Using const Tensor& for Mutable Tensors in PyTorch Kernel Signatures (native_functions.yaml)
DESCRIPTION: Demonstrates the `use_const_ref_for_mutable_tensors: True` setting in `native_functions.yaml`. When enabled, this flag instructs the code generator to use `const Tensor&` for tensor arguments in C++ function signatures, even if the underlying tensor data might be modified by the kernel. This replaces the older `Tensor&` style, providing better C++ const correctness.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_11

LANGUAGE: yaml
CODE:
```
use_const_ref_for_mutable_tensors: True
```

----------------------------------------

TITLE: Profiling Division Operations in PyTorch
DESCRIPTION: Log of scalar division operations on tensors with various shapes. These are likely normalizing operations, dividing by batch size or feature count.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
Operator: aten.div.Scalar
cnt: 1, ((T([64, 1280, 8, 8], f16, stride=(1280, 1, 0, 0)), 64), {})
cnt: 2, ((T([64, 256, 16, 16], f16, stride=(256, 1, 0, 0)), 256), {})
cnt: 2, ((T([64, 128, 32, 32], f16, stride=(128, 1, 0, 0)), 1024), {})
cnt: 2, ((T([64, 64, 64, 64], f16, stride=(64, 1, 0, 0)), 4096), {})
```

----------------------------------------

TITLE: Unbind Operation in PyTorch
DESCRIPTION: This snippet shows unbind operations on 5D tensors. It splits the tensor into a tuple of tensors along the specified dimension.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_12

LANGUAGE: Python
CODE:
```
Operator: aten.unbind.int
cnt: 14, ((T([3, 64, 12, 196, 32], f16, stride=(384, 225792, 32, 1152, 1)),), {})
cnt: 2, ((T([2, 64, 12, 197, 32], f16, stride=(384, 151296, 32, 768, 1)),), {})
```

----------------------------------------

TITLE: ReLU In-Place Operations
DESCRIPTION: Collection of in-place ReLU operations (aten.relu_.default) on tensors of various shapes. These operations modify tensors directly without creating new memory allocations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/densenet121_training.txt#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
((T([4, 64, 112, 112], f16),), {})
```

----------------------------------------

TITLE: Calling aten.native_layer_norm_backward.default (Python)
DESCRIPTION: Computes the gradient of the layer normalization operation with respect to the input, weights, and bias. Requires the output gradient, input tensor, normalized shape, mean, variance, weights, bias, and a mask indicating which gradients to compute.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_25

LANGUAGE: Python
CODE:
```
((T([8, 128, 1024], f16), T([8, 128, 1024], f16), [1024], T([8, 128, 1], f32), T([8, 128, 1], f32), T([1024], f16), T([1024], f16), [True, True, True]), {})
```

----------------------------------------

TITLE: Usage Log: aten.relu_.default Operator (Text)
DESCRIPTION: Logs calls to the in-place ReLU activation function (`aten.relu_.default`). The arguments show the shapes of the input tensors (e.g., [128, 128, 1, 1] f16) being modified. Different counts are observed for various tensor shapes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dm_nfnet_f0_training.txt#2025-04-22_snippet_5

LANGUAGE: text
CODE:
```
Operator: aten.relu_.default
cnt: 1, ((T([128, 128, 1, 1], f16),), {})
cnt: 2, ((T([128, 256, 1, 1], f16),), {})
cnt: 9, ((T([128, 768, 1, 1], f16),), {})
```

----------------------------------------

TITLE: Disabling NVFuser Features in Python
DESCRIPTION: This code snippet shows how to disable specific NVFuser features using environment variables to address unexpected outputs or performance issues.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/cuda/README.md#2025-04-22_snippet_8

LANGUAGE: Bash
CODE:
```
export PYTORCH_NVFUSER_DISABLE=fma,index_hoist
```

----------------------------------------

TITLE: Manipulating Package Contents with Python's zipfile Module
DESCRIPTION: Demonstrates how to read and modify the contents of a torch package using Python's zipfile module. This allows programmatic access to read files and write modified content back into the package.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from zipfile import ZipFile
with ZipFile("my_package.pt") as myzip:
    file_bytes = myzip.read("torchvision/models/resnet.py")
    # edit file_bytes in some way
    myzip.writestr("torchvision/models/resnet.py", new_file_bytes)
```

----------------------------------------

TITLE: Defining Fusion Pattern Structure in PyTorch
DESCRIPTION: This snippet defines the structure of fusion patterns used for matching operations in PyTorch. It explains that patterns consist of operators (which can be module types, functional operators, or PyTorch operators) followed by nested patterns for the operator's arguments.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/pattern.md#2025-04-22_snippet_0

LANGUAGE: text
CODE:
```
operator = module_type | functional | torch op | native op | MatchAllNode
Pattern = (operator, Pattern, Pattern, ...) | operator
```

----------------------------------------

TITLE: Batch Normalization Backward Operations in PyTorch
DESCRIPTION: This snippet shows backward pass operations for batch normalization, used during gradient computation. These operations compute gradients with respect to input, scale and shift parameters through the normalization layers.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
Operator: aten.native_batch_norm_backward.default
cnt: 5, ((T([96, 512, 4, 4], f16), T([96, 512, 4, 4], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), False, 1e-05, [True, True, True]), {})
cnt: 5, ((T([96, 256, 8, 8], f16), T([96, 256, 8, 8], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), False, 1e-05, [True, True, True]), {})
cnt: 5, ((T([96, 128, 16, 16], f16), T([96, 128, 16, 16], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), False, 1e-05, [True, True, True]), {})
cnt: 5, ((T([96, 64, 32, 32], f16), T([96, 64, 32, 32], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([96, 64, 64, 64], f16), T([96, 64, 64, 64], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})
```

----------------------------------------

TITLE: Analyzing Tensor Addition Operations in PyTorch Neural Network
DESCRIPTION: Summary of tensor addition operations (aten.add.Tensor) in the model, showing the count, tensor shapes and data types. The operations primarily work with half-precision (f16) tensors of various dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_regnet_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
Operator: aten.add.Tensor
cnt: 6, ((T([32, 224, 56, 56], f16), T([32, 224, 56, 56], f16)), {})
cnt: 15, ((T([32, 448, 28, 28], f16), T([32, 448, 28, 28], f16)), {})
cnt: 33, ((T([32, 896, 14, 14], f16), T([32, 896, 14, 14], f16)), {})
cnt: 2, ((T([32, 2240, 7, 7], f16), T([32, 2240, 7, 7], f16)), {})
cnt: 1, ((T([32, 32, 112, 112], f16), T([32, 32, 112, 112], f16)), {})
```

----------------------------------------

TITLE: Unsafe View Reshaping in PyTorch Python
DESCRIPTION: In PyTorch, the 'aten._unsafe_view' operator is demonstrated for tensor reshaping without memory layout verification, useful for manipulating tensor dimensions efficiently. This operator requires exact input dimensions; incorrect size specifications lead to runtime errors. Primarily leveraged in scenarios prioritizing speed over safety checks, it outputs reshaped tensors based on new size parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
Operator: aten._unsafe_view.default
cnt: 1, ((T([1, 80, 80, 810], f16), [1, 57600, 90]), {})
cnt: 1, ((T([1, 40, 40, 810], f16), [1, 14400, 90]), {})
```

----------------------------------------

TITLE: Registering Hook After In-place Tensor Modification in PyTorch
DESCRIPTION: Demonstrates the proper way to register a hook after performing an in-place modification on a tensor. The hook will receive gradients with respect to the modified tensor value rather than the original value.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/autograd.rst#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
t = torch.tensor(1., requires_grad=True).sin()
t.cos_()
t.register_hook(fn)
t.backward()
```

----------------------------------------

TITLE: Initializing New Zero Tensors with ATen
DESCRIPTION: Creates new tensors initialized to zero, essential for setting up parameters and biases in neural networks. Requires template tensors and size lists, such as [16777216]. Outputs zero-filled tensors ready for computational tasks. It's constrained by the need for specified sizes for each dimension.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_11

LANGUAGE: Python
CODE:
```
Operator: aten.new_zeros.default
cnt: 2, ((T([16, 64, 128, 128], f16), [16777216]), {})
cnt: 2, ((T([16, 128, 64, 64], f16), [8388608]), {})
cnt: 7, ((T([16, 256, 32, 32], f16), [4194304]), {})
```

----------------------------------------

TITLE: Installing Numpy and MKL Dependencies with Pip (BAT)
DESCRIPTION: This batch command provides an alternative fix for `ImportError: DLL load failed` when using PyTorch wheel packages on Windows. It installs `numpy`, `mkl`, `intel-openmp`, and `mkl_fft` using pip to ensure NumPy uses MKL instead of OpenBLAS and includes necessary MKL dependencies. Manual installation of the VS 2017 redistributable is also required for wheels.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#_snippet_4

LANGUAGE: bat
CODE:
```
pip install numpy mkl intel-openmp mkl_fft
```

----------------------------------------

TITLE: Defining a Placeholder Node in FX Graph (Python)
DESCRIPTION: This Python code snippet demonstrates the creation of a placeholder node within an FX graph, following the syntax conventions used in PyTorch's FX framework. The placeholder target designates the input variable's name, and the node can optionally hold a default argument. This node structure is essential for marking graph inputs and should appear at the top of the nodes list in a graph block.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
%name = placeholder[target = name](args = ())
```

----------------------------------------

TITLE: Invoking aten.sum.default with Tensor Arguments (Text)
DESCRIPTION: This section logs calls to the `aten.sum.default` operator, likely performing a reduction sum over all dimensions of the input tensor. The examples show this operation applied to float16 (f16) tensors of various shapes (e.g., [128, 1000], [128, 1536, 6, 6]). The input is a tuple containing only the tensor description, implying default reduction behavior.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_22

LANGUAGE: text
CODE:
```
Operator: aten.sum.default
```

LANGUAGE: text
CODE:
```
cnt: 1, ((T([128, 1000], f16),), {})
```

LANGUAGE: text
CODE:
```
cnt: 3, ((T([128, 1536, 6, 6], f16),), {})
```

LANGUAGE: text
CODE:
```
cnt: 6, ((T([128, 1536, 12, 12], f16),), {})
```

LANGUAGE: text
CODE:
```
cnt: 2, ((T([128, 512, 24, 24], f16),), {})
```

LANGUAGE: text
CODE:
```
cnt: 1, ((T([128, 256, 48, 48], f16),), {})
```

----------------------------------------

TITLE: Analyzing Batch Normalization Operations in PyTorch
DESCRIPTION: This snippet shows the configuration of batch normalization operations used in the model. It includes input tensor shapes, normalization parameters, and the frequency of these operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
Operator: aten.native_batch_norm.default
cnt: 1, ((T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), True, 0.1, 1e-05), {})
cnt: 4, ((T([128, 8, 112, 112], f16), T([8], f16), T([8], f16), T([8], f16), T([8], f16), True, 0.1, 1e-05), {})
cnt: 2, ((T([128, 24, 112, 112], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), True, 0.1, 1e-05), {})
```

----------------------------------------

TITLE: Testing Tensor In-Place Activation for aten.relu_.default - Python
DESCRIPTION: These test cases represent in-place application of the ReLU activation on tensors of varying shapes for the 'aten.relu_.default' operator. Each tuple encodes the tensor details only; the operation mutates the tensor in place. ReLU is a common neural network nonlinearity, requiring input of numeric (typically f16/f32) tensors, without additional parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
Operator: aten.relu_.default
cnt: 1, ((T([128, 16, 112, 112], f16),), {})
```

LANGUAGE: python
CODE:
```
cnt: 2, ((T([128, 8, 112, 112], f16),), {})
```

----------------------------------------

TITLE: Configuring CMake for LibTorch Project
DESCRIPTION: Basic CMakeLists.txt configuration for a project using LibTorch. It sets up the project, finds the Torch package, and configures the build settings including C++ standard and linking.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/installing.rst#2025-04-22_snippet_1

LANGUAGE: cmake
CODE:
```
cmake_minimum_required(VERSION 3.18 FATAL_ERROR)
project(example-app)

find_package(Torch REQUIRED)
set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}")

add_executable(example-app example-app.cpp)
target_link_libraries(example-app "${TORCH_LIBRARIES}")
set_property(TARGET example-app PROPERTY CXX_STANDARD 17)

# The following code block is suggested to be used on Windows.
# According to https://github.com/pytorch/pytorch/issues/25457,
# the DLLs need to be copied to avoid memory errors.
if (MSVC)
  file(GLOB TORCH_DLLS "${TORCH_INSTALL_PREFIX}/lib/*.dll")
  add_custom_command(TARGET example-app
                     POST_BUILD
                     COMMAND ${CMAKE_COMMAND} -E copy_if_different
                     ${TORCH_DLLS}
                     $<TARGET_FILE_DIR:example-app>)
endif (MSVC)
```

----------------------------------------

TITLE: Initializing TrainingAwareDataSparsity Callback in Python
DESCRIPTION: Example of creating a TrainingAwareDataSparsity callback that sparsifies model parameters during training. It configures both a DataNormSparsifier and a StepSLScheduler with their respective parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/lightning/callbacks/README.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from data_sparsity import TrainingAwareDataSparsity
sparsifier_args = {
    'sparsity_level': 0.5,
    'sparse_block_shape': (1, 4),
    'zeros_per_block': 4
}
scheduler_args = {
    'gamma': 2,
    'step_size': 1
}

ta_callback = TrainingAwareDataSparsity(
    data_sparsifier_class=DataNormSparsifier,
    data_sparsifier_args=sparsifier_args,
    data_scheduler_class=StepSLScheduler,
    data_scheduler_args=scheduler_args
)
```

----------------------------------------

TITLE: Implementing Custom Events Handler for TorchElastic in Python
DESCRIPTION: This snippet shows how to implement a custom events handler by extending the EventHandler class and configuring it in a custom launcher to process and record events during TorchElastic execution.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/customization.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
# my_launcher.py

import torch.distributed.elastic.events as events

class MyEventHandler(events.EventHandler):
    def record(self, event: events.Event):
        # process event

def main():
  events.configure(MyEventHandler())

  spec = WorkerSpec(...)
  agent = LocalElasticAgent(spec)
  agent.run()
```

----------------------------------------

TITLE: Attention Mechanism with Dimensions in PyTorch
DESCRIPTION: Implements an attention mechanism using dimension objects for batch, channel, key, and query operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
from torchdim import softmax
def attention(K, Q, V):
    batch, channel, key, query = dims(4)
    k = K[batch, channel, key]
    q = Q[batch, channel, query]
    v = V[batch, channel, key]

    a = (k * q).sum(channel) # matrix multiply
    a = softmax(a * (channel.size ** -0.5), dim=key)
    r = (v * a).sum(key) # matrix multiply
    return torch.cat((r.order(batch, channel, query), Q), dim=1)

inputs = (torch.rand(2, 3, 4) for _ in range(3))
attention(*inputs)
```

----------------------------------------

TITLE: Initializing Fuzzer for Tensor Generation in Python
DESCRIPTION: This code snippet demonstrates how to create a Fuzzer object to generate random tensors with specific parameters. It creates two tensors, 'x' and 'y', where 'x' is a 2D tensor and 'y' is broadcastable to the shape of 'x'.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/benchmark/README.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
fuzzer = Fuzzer(
  parameters=[
    FuzzedParameter("k0", 16, 16 * 1024, "loguniform"),
    FuzzedParameter("k1", 16, 16 * 1024, "loguniform"),
  ],
  tensors=[
    FuzzedTensor(
      name="x", size=("k0", "k1"), probability_contiguous=0.75
    ),
    FuzzedTensor(
      name="y", size=("k0", 1), probability_contiguous=0.75
    ),
  ],
  seed=0,
)
```

----------------------------------------

TITLE: Importing Process Context Classes for PyTorch Distributed Processing
DESCRIPTION: This snippet imports various process context classes from the PyTorch distributed elastic multiprocessing API. These classes are used to manage and interact with distributed processes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/multiprocessing.rst#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
from torch.distributed.elastic.multiprocessing.api import PContext, MultiprocessContext, SubprocessContext, RunProcsResult, DefaultLogsSpecs, LogsDest, LogsSpecs
```

----------------------------------------

TITLE: Verifying Hessian Results Match Between API and Manual - Python
DESCRIPTION: Confirms that both functorch.hessian and manual jacfwd(jacfwd()) approaches produce the same Hessian output, using torch.allclose. Requires hess_api and hess_fwdfwd populated. Returns Boolean result.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
torch.allclose(hess_api, hess_fwdfwd)
```

----------------------------------------

TITLE: Proper Use of Function Returns with functorch
DESCRIPTION: This example shows the correct approach to refactor a function to be compatible with functorch transforms by returning intermediate values as part of the function's output instead of using global variables.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/ux_limitations.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
def f(x):
  intermediate = x.sin()
  z = intermediate.sin()
  return z, intermediate

grad_x, intermediate = grad(f, has_aux=True)(x)
```

----------------------------------------

TITLE: Analyzing Matrix Multiplication with Bias Addition in PyTorch
DESCRIPTION: Record of aten.addmm.default operator call for matrix multiplication with bias addition. This operation uses half-precision (f16) tensors and likely represents the final fully connected layer in a neural network with 1000 output classes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_resnest_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
Operator: aten.addmm.default
cnt: 1, ((T([1000], f16), T([32, 2048], f16), T([2048, 1000], f16, stride=(1, 2048))), {})
```

----------------------------------------

TITLE: Implementing Element-wise Operations using map_nt C++
DESCRIPTION: This C++ template function `map_nt` provides a mechanism to apply a function `f` to the underlying dense buffer of a NestedTensor `nt` for implementing efficient element-wise operations. It retrieves the NestedTensor's implementation details, applies the function `f` to the buffer, and constructs a new NestedTensor from the result and the original sizes. It requires a NestedTensor input and a callable function `f` that operates on a dense Tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/nested/README.md#_snippet_0

LANGUAGE: C++
CODE:
```
template <typename Func>
Tensor map_nt(const Tensor& nt, Func f) {
  auto* nt_impl = get_nested_tensor_impl(nt);
  const auto& sizes = nt_impl->get_nested_sizes();
  return at::detail::make_tensor<NestedTensorImpl>(f(nt_impl->get_buffer()), sizes);
}
```

----------------------------------------

TITLE: Retrieving SubprocessHandler in PyTorch Distributed Elastic
DESCRIPTION: Function to retrieve the SubprocessHandler for managing subprocesses in PyTorch's distributed elastic multiprocessing module.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/subprocess_handler.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
torch.distributed.elastic.multiprocessing.subprocess_handler.handlers.get_subprocess_handler
```

----------------------------------------

TITLE: Computing Angle and Absolute Values
DESCRIPTION: Shows how to compute the angle and absolute values of complex tensors using torch.angle and torch.abs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/complex_numbers.rst#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
x1=torch.tensor([3j, 4+4j])
x1.abs()
x1.angle()
```

----------------------------------------

TITLE: Setting CUDA Streams in PyTorch C++
DESCRIPTION: This snippet shows how to set the current CUDA stream using the setCurrentCUDAStream function. It also mentions the CUDAStreamGuard as a recommended alternative for managing stream and device context.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_1

LANGUAGE: cpp
CODE:
```
void setCurrentCUDAStream(CUDAStream stream);
```

----------------------------------------

TITLE: Practical Example Using Minifier
DESCRIPTION: Complete example demonstrating how to use the minifier with a failing function that contains a multiplication operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/minifier.ipynb#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import torch
import torch.fx as fx
from functorch.compile import minifier

def failing_f(x, y):
    y = torch.ops.aten.div(x, y)
    x = torch.ops.aten.add(x, 3)
    x = torch.ops.aten.mul(x, y)
    return torch.ops.aten.sub(x, y)

inps = [torch.randn(3), torch.randn(3)]

def pass_checker(fx_g, inps):
    return (torch.ops.aten.mul in {i.target for i in fx_g.graph.nodes})

min_f, inps = minifier(fx.symbolic_trace(failing_f), inps, pass_checker)
```

----------------------------------------

TITLE: Reshaping Tensors Unsafely in PyTorch
DESCRIPTION: Describes the use of aten._unsafe_view.default to reshape tensors without safety checks. This can lead to undefined behavior if tensors are misaligned, underscoring the importance of cautious usage.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaForQuestionAnswering_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
Operator: aten._unsafe_view.default
cnt: 12, ((T([2048, 2304], f16), [4, 512, 2304]), {})
cnt: 36, ((T([4, 12, 512, 64], f16), [48, 512, 64]), {})
cnt: 12, ((T([4, 12, 64, 512], f16), [48, 64, 512]), {})
cnt: 12, ((T([48, 512, 512], f16), [4, 12, 512, 512]), {})
cnt: 12, ((T([48, 512, 64], f16), [4, 12, 512, 64]), {})
cnt: 12, ((T([4, 512, 12, 192], f16), [4, 512, 2304]), {})
```

----------------------------------------

TITLE: Defining Function to Import Static Libraries on Android in CMake
DESCRIPTION: Defines a CMake function `import_static_lib` specifically for Android builds (`if(ANDROID_ABI)` block). This function takes a library name as an argument, creates an `IMPORTED STATIC` library target for it, and sets its `IMPORTED_LOCATION` property to the path where the pre-built static library (`.a` file) is expected within the `jniLibs` directory, based on the current `ANDROID_ABI`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_13

LANGUAGE: cmake
CODE:
```
if(ANDROID_ABI)
  function(import_static_lib name)
    add_library(${name} STATIC IMPORTED)
    set_property(
        TARGET ${name}
        PROPERTY IMPORTED_LOCATION
        ${CMAKE_CURRENT_LIST_DIR}/src/main/jniLibs/${ANDROID_ABI}/${name}.a)
  endfunction(import_static_lib)
```

----------------------------------------

TITLE: Running Specific Benchmark Test with Filtering
DESCRIPTION: Runs a specific benchmark test by name with single-threaded configuration.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
python -m pt.add_test --test-name add_K32_M8_N1
--omp-num-threads 1 --mkl-num-threads 1
```

----------------------------------------

TITLE: Using NVFuser Checkers
DESCRIPTION: Example showing how to use built-in NVFuser checkers with the minifier for checking errors and correctness.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/minifier.ipynb#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from functorch.compile import minifier, check_nvfuser_subprocess, check_nvfuser_correctness_subprocess
minifier(failing_graph, inps, check_nvfuser_subprocess)
```

----------------------------------------

TITLE: Mathematical Formulation of Scale and Zero Point Calculation
DESCRIPTION: This snippet provides the mathematical equations for how scale and zero point parameters are computed in PyTorch's quantization, with different formulations for symmetric and asymmetric quantization schemes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization-support.rst#2025-04-22_snippet_1

LANGUAGE: LaTeX
CODE:
```
\begin{aligned}
    \text{if Symmetric:}&\\
    &s = 2 \max(|x_\text{min}|, x_\text{max}) /
        \left( Q_\text{max} - Q_\text{min} \right) \\
    &z = \begin{cases}
        0 & \text{if dtype is qint8} \\
        128 & \text{otherwise}
    \end{cases}\\
    \text{Otherwise:}&\\
        &s = \left( x_\text{max} - x_\text{min}  \right ) /
            \left( Q_\text{max} - Q_\text{min} \right ) \\
        &z = Q_\text{min} - \text{round}(x_\text{min} / s)
\end{aligned}
```

----------------------------------------

TITLE: Unsupported Write Indexing with Negative Values (Python)
DESCRIPTION: Illustrates an unsupported tensor indexing pattern for writing data that includes negative index values within the tensor indices. The workaround suggests using positive index values instead.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_12

LANGUAGE: python
CODE:
```
data[torch.tensor([1, -2]), torch.tensor([-2, 3])] = new_data
# Workarounds: use positive index values.
```

----------------------------------------

TITLE: Unsupported Use of .item() with vmap
DESCRIPTION: This example shows that vmap doesn't support functions that call .item() on tensors, as this breaks the vectorized computation model.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/ux_limitations.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
def f(x):
  return x.item()

x = torch.randn(3)
vmap(f)(x)
```

----------------------------------------

TITLE: Pixel Shuffle with Einops in PyTorch
DESCRIPTION: Alternative implementation of pixel shuffle using Einops for dimension rearrangement.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
def pixel_shuffle_einops(img, upscale_factor=2):
    from einops import rearrange
    return rearrange(img, 'b (c h2 w2) h w -> b c (h h2) (w w2)', h2=upscale_factor, w2=upscale_factor)
```

----------------------------------------

TITLE: Implementing DataPipe Constructor in Python
DESCRIPTION: Constructor implementation for a MapperIterDataPipe class that takes a source DataPipe and mapping function as arguments. Demonstrates proper initialization pattern for DataPipes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/datapipes/README.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
class MapperIterDataPipe(IterDataPipe):
    def __init__(self, dp, fn):
        super().__init__()
        self.dp = dp
        self.fn = fn
```

----------------------------------------

TITLE: Accessing Package Contents from Packaged Code
DESCRIPTION: Shows how to use the importlib.resources API and the torch_package_importer to access resources and pickled objects from within packaged code.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
# foo.py:
import importlib.resources
import my_resource

# returns "hello world!"
def get_my_resource():
    return importlib.resources.read_text(my_resource, "a.txt")

# bar.py:
import torch_package_importer # this is the PackageImporter that imported this module.

# Prints "hello world!", equivalent to importlib.resources.read_text
def get_my_resource():
    return torch_package_importer.load_text("my_resource", "a.txt")

# You also do things that the importlib.resources API does not support, like loading
# a pickled object from the package.
def get_my_pickle():
    return torch_package_importer.load_pickle("my_pickle", "obj.pkl")
```

----------------------------------------

TITLE: Customizing Torchvision Model Normalization with BatchNorm2d (No Running Stats) - PyTorch (Python)
DESCRIPTION: Initializes a torchvision model with a BatchNorm2d layer that does not track running statistics via the 'norm_layer' parameter, using functools.partial to preset track_running_stats to False. This avoids vmap-related errors when running Functorch. Requires torchvision and functools as dependencies.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.batch_norm.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import torchvision
from functools import partial
torchvision.models.resnet18(norm_layer=partial(BatchNorm2d, track_running_stats=False))
```

----------------------------------------

TITLE: PyTorch Tensor Addition Operations
DESCRIPTION: Various tensor addition operations with different shapes and dimensions using float16 precision
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
aten.add.Tensor(T([], i64), 1)
aten.add.Tensor(T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16))
```

----------------------------------------

TITLE: Installation Commands for torchdim
DESCRIPTION: Shell commands for setting up conda environment and installing PyTorch nightly build along with torchdim package.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_1

LANGUAGE: shell
CODE:
```
conda create --name dim
conda activate dim

# For CUDA 10.2
conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch-nightly
# For CUDA 11.3
conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch-nightly
# For CPU-only build
conda install pytorch torchvision torchaudio cpuonly -c pytorch-nightly

pip install ninja  # Makes the build go faster
pip install --user "git+https://github.com/facebookresearch/torchdim"
```

----------------------------------------

TITLE: Invoking aten.layer_norm with Tensor Arguments (Text)
DESCRIPTION: This section logs calls to an implicit operator, likely `aten::layer_norm`, based on the argument structure. It shows various invocations with different input tensor shapes (e.g., [1, 3072, 1536]), weight/bias shapes, and data types (f16 for inputs/weights, f32 for running stats). Key arguments include the input tensor, normalized shape (implicitly derived from weight/bias), weight, bias, running mean, running variance, elementwise_affine flag (True), epsilon (1e-05), and potentially cudnn_enable flag.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_17

LANGUAGE: text
CODE:
```
cnt: 1, ((T([1, 3072, 1536], f16), T([1, 3072, 1536], f16), T([3072], f16), None, None, T([3072], f32), T([3072], f32), True, 1e-05, [True, True, False]), {})
```

LANGUAGE: text
CODE:
```
cnt: 9, ((T([1, 1536, 768], f16), T([1, 1536, 768], f16), T([1536], f16), None, None, T([1536], f32), T([1536], f32), True, 1e-05, [True, True, False]), {})
```

LANGUAGE: text
CODE:
```
cnt: 18, ((T([1, 768, 1152], f16), T([1, 768, 1152], f16), T([768], f16), None, None, T([768], f32), T([768], f32), True, 1e-05, [True, True, False]), {})
```

LANGUAGE: text
CODE:
```
cnt: 8, ((T([1, 768, 1536], f16), T([1, 768, 1536], f16), T([768], f16), None, None, T([768], f32), T([768], f32), True, 1e-05, [True, True, False]), {})
```

LANGUAGE: text
CODE:
```
cnt: 1, ((T([1, 1536, 1536], f16), T([1, 1536, 1536], f16), T([1536], f16), None, None, T([1536], f32), T([1536], f32), True, 1e-05, [True, True, False]), {})
```

LANGUAGE: text
CODE:
```
cnt: 1, ((T([1, 768, 512], f16), T([1, 768, 512], f16), T([768], f16), None, None, T([768], f32), T([768], f32), True, 1e-05, [True, True, False]), {})
```

LANGUAGE: text
CODE:
```
cnt: 1, ((T([1, 1536, 512], f16), T([1, 1536, 512], f16), T([1536], f16), None, None, T([1536], f32), T([1536], f32), True, 1e-05, [True, True, False]), {})
```

LANGUAGE: text
CODE:
```
cnt: 3, ((T([1, 512, 256], f16), T([1, 512, 256], f16), T([512], f16), None, None, T([512], f32), T([512], f32), True, 1e-05, [True, True, False]), {})
```

LANGUAGE: text
CODE:
```
cnt: 4, ((T([1, 256, 1152], f16), T([1, 256, 1152], f16), T([256], f16), None, None, T([256], f32), T([256], f32), True, 1e-05, [True, True, False]), {})
```

LANGUAGE: text
CODE:
```
cnt: 1, ((T([1, 256, 512], f16), T([1, 256, 512], f16), T([256], f16), None, None, T([256], f32), T([256], f32), True, 1e-05, [True, True, False]), {})
```

LANGUAGE: text
CODE:
```
cnt: 1, ((T([1, 256, 256], f16), T([1, 256, 256], f16), T([256], f16), None, None, T([256], f32), T([256], f32), True, 1e-05, [True, True, False]), {})
```

LANGUAGE: text
CODE:
```
cnt: 2, ((T([1, 256, 128], f16), T([1, 256, 128], f16), T([256], f16), None, None, T([256], f32), T([256], f32), True, 1e-05, [True, True, False]), {})
```

LANGUAGE: text
CODE:
```
cnt: 2, ((T([1, 128, 1152], f16), T([1, 128, 1152], f16), T([128], f16), None, None, T([128], f32), T([128], f32), True, 1e-05, [True, True, False]), {})
```

LANGUAGE: text
CODE:
```
cnt: 1, ((T([1, 128, 128], f16), T([1, 128, 128], f16), T([128], f16), None, None, T([128], f32), T([128], f32), True, 1e-05, [True, True, False]), {})
```

LANGUAGE: text
CODE:
```
cnt: 1, ((T([1, 128, 576], f16), T([1, 128, 576], f16), T([128], f16), None, None, T([128], f32), T([128], f32), True, 1e-05, [True, True, False]), {})
```

LANGUAGE: text
CODE:
```
cnt: 1, ((T([1, 64, 288], f16), T([1, 64, 288], f16), T([64], f16), None, None, T([64], f32), T([64], f32), True, 1e-05, [True, True, False]), {})
```

LANGUAGE: text
CODE:
```
cnt: 1, ((T([1, 32, 144], f16), T([1, 32, 144], f16), T([32], f16), None, None, T([32], f32), T([32], f32), True, 1e-05, [True, True, False]), {})
```

LANGUAGE: text
CODE:
```
cnt: 1, ((T([1, 16, 27], f16), T([1, 16, 27], f16), T([16], f16), None, None, T([16], f32), T([16], f32), True, 1e-05, [True, True, False]), {})
```

----------------------------------------

TITLE: Leaf Modules Example in PyTorch FX
DESCRIPTION: Demonstrates how leaf modules are handled during symbolic tracing with custom submodules.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
class MySpecialSubmodule(torch.nn.Module):
    def forward(self, x):
        return torch.neg(x)

class MyModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(3, 4)
        self.submod = MySpecialSubmodule()

    def forward(self, x):
        return self.submod(self.linear(x))
```

----------------------------------------

TITLE: ReLU Activation Operations in PyTorch
DESCRIPTION: Records of ReLU activation operations applied in-place to tensors of various shapes. These operations apply element-wise rectified linear unit function, replacing negative values with zeros.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/spnasnet_100_training.txt#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
Operator: aten.relu_.default
cnt: 2, ((T([128, 32, 112, 112], f16),), {})
cnt: 1, ((T([128, 48, 112, 112], f16),), {})
cnt: 1, ((T([128, 48, 56, 56], f16),), {})
cnt: 4, ((T([128, 72, 56, 56], f16),), {})
cnt: 1, ((T([128, 144, 56, 56], f16),), {})
cnt: 1, ((T([128, 144, 28, 28], f16),), {})
cnt: 6, ((T([128, 120, 28, 28], f16),), {})
cnt: 1, ((T([128, 240, 28, 28], f16),), {})
cnt: 7, ((T([128, 240, 14, 14], f16),), {})
cnt: 2, ((T([128, 480, 14, 14], f16),), {})
cnt: 6, ((T([128, 288, 14, 14], f16),), {})
cnt: 1, ((T([128, 576, 14, 14], f16),), {})
cnt: 1, ((T([128, 576, 7, 7], f16),), {})
cnt: 8, ((T([128, 1152, 7, 7], f16),), {})
cnt: 1, ((T([128, 1280, 7, 7], f16),), {})
```

----------------------------------------

TITLE: Configuring MPS Profiler Log Options (Environment Variable)
DESCRIPTION: Sets the log options bitmask for the `MPSProfiler` using `PYTORCH_MPS_LOG_PROFILE_INFO`. The available options are defined by the `LogOptions` enum located in the specified header file `aten/src/ATen/mps/MPSProfiler.h`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mps_environment_variables.rst#2025-04-22_snippet_1

LANGUAGE: plaintext
CODE:
```
PYTORCH_MPS_LOG_PROFILE_INFO
```

----------------------------------------

TITLE: Batch Normalization in PyTorch with Half-Precision Tensors
DESCRIPTION: Multiple batch normalization operations on tensors of various shapes. Operations use half-precision (f16) tensors and include running mean and variance calculations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/inception_v3_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
((T([128, 32, 149, 149], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 0.001), {})
```

----------------------------------------

TITLE: Registering Custom Symbolic for prim::PythonOp
DESCRIPTION: Registers the custom symbolic function symbolic_python_op with the PyTorch ONNX exporter for the prim::PythonOp. This ensures the symbolic function is called when a prim::PythonOp is encountered during export.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_22

LANGUAGE: Python
CODE:
```
from torch.onnx import register_custom_op_symbolic
register_custom_op_symbolic("prim::PythonOp", symbolic_python_op, 1)
```

----------------------------------------

TITLE: Listing Example Inputs for aten.upsample_bilinear2d.vec Operator - PyTorch - Python
DESCRIPTION: This code shows representative input arguments for the aten.upsample_bilinear2d.vec operator, which upsamples spatial tensor data using bilinear interpolation. Each tuple specifies an input tensor (shape and data type), a placeholder (None) for deprecated or optional argument, the boolean align_corners (True), and the upsampling scale factor ([2.0, 2.0]). This setup helps users or test systems validate upsampling operator behavior for typical deep learning feature map sizes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_unet_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
Operator: aten.upsample_bilinear2d.vec
cnt: 1, ((T([1, 512, 40, 59], f16), None, True, [2.0, 2.0]), {})
cnt: 1, ((T([1, 256, 80, 119], f16), None, True, [2.0, 2.0]), {})
cnt: 1, ((T([1, 128, 160, 239], f16), None, True, [2.0, 2.0]), {})
cnt: 1, ((T([1, 64, 320, 479], f16), None, True, [2.0, 2.0]), {})
```

----------------------------------------

TITLE: Initializing Custom Observer for PyTorch ModelReport API
DESCRIPTION: Example of initializing a custom observer with a unique PRE_OBSERVER_NAME for use in a detector. This code demonstrates how to create a fully qualified name (fqn) for each observer that acts as a key in the returned dictionary.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/_model_report/README.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
PRE_OBSERVER_NAME = "custom_observer_pre"
```

----------------------------------------

TITLE: Tracking Tensor Shapes and Operations in PyTorch Model
DESCRIPTION: This log captures tensor shapes, counts, and operations during a PyTorch model execution. Each line records the count of occurrences, tensor shapes with data type (f16 for float16), and operation parameters. The log appears to track operations for a convolutional neural network with decreasing spatial dimensions (from 112x112 to 7x7).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
cnt: 1, ((T([64, 608, 14, 14], f16),), {})
cnt: 1, ((T([64, 640, 14, 14], f16),), {})
cnt: 1, ((T([64, 672, 14, 14], f16),), {})
cnt: 1, ((T([64, 704, 14, 14], f16),), {})
cnt: 1, ((T([64, 736, 14, 14], f16),), {})
cnt: 1, ((T([64, 768, 14, 14], f16),), {})
cnt: 1, ((T([64, 800, 14, 14], f16),), {})
cnt: 1, ((T([64, 832, 14, 14], f16),), {})
cnt: 1, ((T([64, 864, 14, 14], f16),), {})
cnt: 1, ((T([64, 896, 14, 14], f16),), {})
cnt: 1, ((T([64, 928, 14, 14], f16),), {})
cnt: 1, ((T([64, 960, 14, 14], f16),), {})
cnt: 1, ((T([64, 992, 14, 14], f16),), {})
cnt: 1, ((T([64, 1024, 14, 14], f16),), {})
cnt: 1, ((T([64, 512, 7, 7], f16),), {})
cnt: 16, ((T([64, 128, 7, 7], f16),), {})
cnt: 1, ((T([64, 544, 7, 7], f16),), {})
cnt: 1, ((T([64, 576, 7, 7], f16),), {})
cnt: 1, ((T([64, 608, 7, 7], f16),), {})
cnt: 1, ((T([64, 640, 7, 7], f16),), {})
cnt: 1, ((T([64, 672, 7, 7], f16),), {})
cnt: 1, ((T([64, 704, 7, 7], f16),), {})
cnt: 1, ((T([64, 736, 7, 7], f16),), {})
cnt: 1, ((T([64, 768, 7, 7], f16),), {})
cnt: 1, ((T([64, 800, 7, 7], f16),), {})
cnt: 1, ((T([64, 832, 7, 7], f16),), {})
cnt: 1, ((T([64, 864, 7, 7], f16),), {})
cnt: 1, ((T([64, 896, 7, 7], f16),), {})
cnt: 1, ((T([64, 928, 7, 7], f16),), {})
cnt: 1, ((T([64, 960, 7, 7], f16),), {})
cnt: 1, ((T([64, 992, 7, 7], f16),), {})
cnt: 1, ((T([64, 1024, 7, 7], f16),), {})
```

----------------------------------------

TITLE: Calling aten.nll_loss_backward.default (Python)
DESCRIPTION: Computes the gradient of the Negative Log Likelihood (NLL) loss. Commonly used for classification problems after a log-softmax layer. Requires the output gradient, log-probabilities (input to forward), target indices, weights, reduction mode, ignore index, and total weight.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_27

LANGUAGE: Python
CODE:
```
((T([], f16), T([1024, 50265], f16), T([1024], i64), None, 1, -100, T([], f16)), {})
```

----------------------------------------

TITLE: Unwrapping and Redispatching NJT Operations in PyTorch
DESCRIPTION: This snippet describes the process of implementing an operation on a Nested Jagged Tensor by unwrapping it, redispatching on the underlying 'values' buffer, and propagating metadata to create a new output NJT. It highlights the need to compute new metadata when the output shape differs from the input.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_11

LANGUAGE: Python
CODE:
```
# Pseudocode for NJT operation implementation
# 1. Unwrap the NJT
# 2. Redispatch the op on the 'values' buffer
# 3. Propagate relevant metadata (including offsets)
# 4. Compute new metadata if output shape differs
# 5. Create new output NJT
```

----------------------------------------

TITLE: Unbinding Nested Jagged Tensors in PyTorch
DESCRIPTION: Demonstrates creating a nested jagged tensor and using the unbind() method to retrieve the constituent tensors. Shows that unbind() returns slices of the underlying memory rather than copies.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)
>>> nt.unbind()
(tensor([[-0.9916, -0.3363, -0.2799],
        [-2.3520, -0.5896, -0.4374]]), tensor([[-2.0969, -1.0104,  1.4841],
        [ 2.0952,  0.2973,  0.2516],
        [ 0.9035,  1.3623,  0.2026]]))
>>> nt.unbind()[0] is not a
True
>>> nt.unbind()[0].mul_(3)
tensor([[ 3.6858, -3.7030, -4.4525],
        [-2.3481,  2.0236,  0.1975]])
>>> nt.unbind()
(tensor([[-2.9747, -1.0089, -0.8396],
        [-7.0561, -1.7688, -1.3122]]), tensor([[-2.0969, -1.0104,  1.4841],
        [ 2.0952,  0.2973,  0.2516],
        [ 0.9035,  1.3623,  0.2026]]))
```

----------------------------------------

TITLE: Using Final Variables in TorchScript If Statements
DESCRIPTION: Demonstrates how using a final-annotated boolean variable in an if statement results in only the true branch being evaluated.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_29

LANGUAGE: python
CODE:
```
import torch

a : torch.jit.final[Bool] = True

if a:
    return torch.empty(2,3)
else:
    return []
```

----------------------------------------

TITLE: Defining Custom Op with ONNX-script (Part 1: Opset Setup & Model)
DESCRIPTION: Sets up the ONNX opset version for ONNX-script and initializes a sample input tensor and a standard PyTorch SELU model, which will be replaced by the custom ONNX-script implementation during export.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_25

LANGUAGE: Python
CODE:
```
import onnxscript
# There are three opset version needed to be aligned
# This is (1) the opset version in ONNX function
from onnxscript.onnx_opset import opset15 as op
opset_version = 15

x = torch.randn(1, 2, 3, 4, requires_grad=True)
model = torch.nn.SELU()
```

----------------------------------------

TITLE: Verifying Model and Minibatch Dimensions Before vmap - PyTorch - Python
DESCRIPTION: Displays the leading dimension (number of models) of each parameter tensor after stacking and asserts the minibatch tensor has correct leading shape for vmap. Ensures compatibility between model parameters and input data dimensions before vectorized mapping.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/ensembling.ipynb#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
print([p.size(0) for p in params]) # show the leading 'num_models' dimension

assert minibatches.shape == (num_models, 64, 1, 28, 28) # verify minibatch has leading dimension of size 'num_models'
```

----------------------------------------

TITLE: FX Graph Node Format for Operator Calls in Export IR (Textual Representation)
DESCRIPTION: This snippet presents an example of the formatted string output for a call_function node in FX/Export IR, showing node naming, operation typing, ATen operator target, and argument referencing other nodes. It is used for graph text export or debugging in PyTorch-based workflows, not for direct execution. Dependencies include torch.export and torch.fx graph creation, and the node labeling and attributes must be understood in the context of torch.fx node conventions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
%add1 = call_function[target = torch.op.aten.add.Tensor](args = (%x, %y), kwargs = {})
```

----------------------------------------

TITLE: Embedding Lookup with Dimensions in PyTorch
DESCRIPTION: Demonstrates looking up features in an embedding table using dimension objects for sequence and feature indices.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
sequence, features = dims(2)
embeddings = torch.rand(8, 128)
words = torch.tensor([5, 4, 0,])

state = embeddings[words[sequence], features]
print(state.dims)
```

----------------------------------------

TITLE: Applying Threshold Backward Operation in PyTorch
DESCRIPTION: This snippet shows the tensor shapes and parameters for the backward pass of a threshold operation in PyTorch. It includes input gradients, output, and the threshold value, likely used in ReLU or similar activations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_inception_v3_training.txt#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
cnt: 2, ((T([128, 192, 8, 8], f16, stride=(131072, 64, 8, 1)), T([128, 192, 8, 8], f16), 0), {})
```

----------------------------------------

TITLE: NTK Computation using Jacobian Contraction
DESCRIPTION: Implementation of empirical NTK computation using Jacobian contraction method, supporting full matrix, trace, and diagonal computations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/neural_tangent_kernels.ipynb#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
def empirical_ntk_jacobian_contraction(fnet_single, params, x1, x2, compute='full'):
    # Compute J(x1)
    jac1 = vmap(jacrev(fnet_single), (None, 0))(params, x1)
    jac1 = [j.flatten(2) for j in jac1]
    
    # Compute J(x2)
    jac2 = vmap(jacrev(fnet_single), (None, 0))(params, x2)
    jac2 = [j.flatten(2) for j in jac2]
    
    # Compute J(x1) @ J(x2).T
    einsum_expr = None
    if compute == 'full':
        einsum_expr = 'Naf,Mbf->NMab'
    elif compute == 'trace':
        einsum_expr = 'Naf,Maf->NM'
    elif compute == 'diagonal':
        einsum_expr = 'Naf,Maf->NMa'
    else:
        assert False
        
    result = torch.stack([torch.einsum(einsum_expr, j1, j2) for j1, j2 in zip(jac1, jac2)])
    result = result.sum(0)
    return result
```

----------------------------------------

TITLE: Reshaping Tensors with Unsafe View in PyTorch
DESCRIPTION: Reshapes tensors using the _unsafe_view operation, which can be faster but may share storage with the original tensor. Used for reshaping between 3D and 2D representations of the data.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/tts_angular_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
aten._unsafe_view.default(T([64, 50, 768], f16), [3200, 768])
```

----------------------------------------

TITLE: Converting a Dense Tensor to CSC Format in PyTorch
DESCRIPTION: Demonstrates how to convert a dense tensor to a sparse CSC tensor using the to_sparse_csc() method. The example converts a 3x4 dense tensor into CSC format.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
a = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype=torch.float64)
sp = a.to_sparse_csc()
sp
```

----------------------------------------

TITLE: Enabling Verbose MPS Allocator Logging (Environment Variable)
DESCRIPTION: Set `PYTORCH_DEBUG_MPS_ALLOCATOR` to `1` to enable verbose logging for the MPS memory allocator in PyTorch. This is useful for debugging memory allocation issues on Apple Silicon.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mps_environment_variables.rst#2025-04-22_snippet_0

LANGUAGE: plaintext
CODE:
```
PYTORCH_DEBUG_MPS_ALLOCATOR
```

----------------------------------------

TITLE: Handling Dispatch Operations in PyTorch
DESCRIPTION: This snippet logs a sequence of dispatch operations performed during the backward pass of neural networks in PyTorch. It includes creating initial gradients, expanding them, multiplying and detaching tensors. These operations rely on the PyTorch library and require tensor objects. The input includes tensors and certain flags such as 'pin_memory', outputs are intermediary results stored in tensors. Constraints include maintaining correct tensor shapes and memory formats.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_17

LANGUAGE: Python
CODE:
```
# Initial gradient creation using ones_like\nDispatch Log: aten.ones_like.default(*(tensor(11.4637, grad_fn=<SumBackward0>),), **{'pin_memory': False, 'memory_format': torch.preserve_format})
```

LANGUAGE: Python
CODE:
```
# Expanding tensor during backward\nDispatch Log: aten.expand.default(*(tensor(1.), [10]), **{})
```

LANGUAGE: Python
CODE:
```
# Multiplying tensor elements\nDispatch Log: aten.mul.Tensor(*(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 2), **{})
```

LANGUAGE: Python
CODE:
```
# Detaching tensor to stop gradient tracking\nDispatch Log: aten.detach.default(*(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),), **{})
```

LANGUAGE: Python
CODE:
```
# Detaching tensor to stop gradient tracking\nDispatch Log: aten.detach.default(*(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),), **{})
```

----------------------------------------

TITLE: Demonstrating Module Name Collision Example in PyTorch Package
DESCRIPTION: Example showing how module name collisions can occur between local and package-loaded models without mangling, potentially causing issues with module lookups in sys.modules.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/package/mangling.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from torchvision.models import resnet18
local_resnet18 = resnet18()

# a loaded resnet18, potentially with a different implementation than the local one!
i = torch.PackageImporter('my_resnet_18.pt')
loaded_resnet18 = i.load_pickle('model', 'model.pkl')

print(type(local_resnet18).__module__)  # 'torchvision.models.resnet18'
print(type(loaded_resnet18).__module__)  # ALSO 'torchvision.models.resnet18'
```

----------------------------------------

TITLE: Executing Tensor Copy Operations
DESCRIPTION: The _to_copy operator handles copying tensors to a specified device, using a particular data type. Dependencies include PyTorch with outputs being a tensor on the intended device and format. Special attention is given to tensor layout and device specifics such as CUDA.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForConditionalGeneration_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
Operator: aten._to_copy.default
cnt: 1, ((T([128, 128], f32),), {'dtype': f16})
cnt: 1, ((T([8, 1, 128, 128], f16, stride=(0, 16384, 128, 1)),), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})
```

----------------------------------------

TITLE: Configuring CMake with Mold Linker (Bash)
DESCRIPTION: Sets the `CMAKE_LINKER_TYPE` environment variable to 'MOLD' (or 'LLD') before running `setup.py develop`. This tells CMake to use a faster linker like mold or lld instead of the system default (e.g., GNU ld), significantly reducing linking time. Requires CMake 3.29+ and the specified linker installed.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_48

LANGUAGE: bash
CODE:
```
CMAKE_LINKER_TYPE=MOLD python setup.py develop
```

----------------------------------------

TITLE: Average Pooling Operations in PyTorch
DESCRIPTION: Statistics for the aten.avg_pool2d.default operator showing forward pass of average pooling. These operations use 2x2 kernels with stride 2 and no padding, typically used for downsampling feature maps in neural networks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
Operator: aten.avg_pool2d.default
cnt: 1, ((T([128, 256, 48, 48], f16), [2, 2], [2, 2], [0, 0], True, False), {})
cnt: 1, ((T([128, 512, 24, 24], f16), [2, 2], [2, 2], [0, 0], True, False), {})
cnt: 1, ((T([128, 1536, 12, 12], f16), [2, 2], [2, 2], [0, 0], True, False), {})
```

----------------------------------------

TITLE: Setting up Environment and Installing PyTorch for Intel GPU on Windows CMD
DESCRIPTION: Configures the CMAKE_PREFIX_PATH environment variable to correctly locate dependencies, potentially within a conda environment, which is necessary for finding packages required for Intel GPU support, before running the standard Python setup script to build and install PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_16

LANGUAGE: CMD
CODE:
```
:: CMD Commands:
:: Set the CMAKE_PREFIX_PATH to help find corresponding packages
:: %CONDA_PREFIX% only works after `conda activate custom_env`

if defined CMAKE_PREFIX_PATH (
    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library;%CMAKE_PREFIX_PATH%"
) else (
    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library"
)

python setup.py develop

```

----------------------------------------

TITLE: Instantiating and Calling a Custom PyTorch Module
DESCRIPTION: Demonstrates how to instantiate the previously defined `MyLinear` module with specific input and output feature sizes (4 and 3, respectively). It then shows how to call the module instance with a sample input tensor, invoking its `forward` method to perform the computation. The output tensor includes gradient tracking information (`grad_fn`).
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
m = MyLinear(4, 3)
sample_input = torch.randn(4)
m(sample_input)
: tensor([-0.3037, -1.0413, -4.2057], grad_fn=<AddBackward0>)
```

----------------------------------------

TITLE: Average Pooling Backward Operations in PyTorch
DESCRIPTION: Backward pass operations for average pooling in PyTorch. These compute gradients for the average pooling layers during backpropagation with 2x2 kernels and stride 2.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
Operator: aten.avg_pool2d_backward.default
cnt: 1, ((T([64, 512, 7, 7], f16), T([64, 512, 14, 14], f16), [2, 2], [2, 2], [0, 0], False, True, None), {})
cnt: 1, ((T([64, 256, 14, 14], f16), T([64, 256, 28, 28], f16), [2, 2], [2, 2], [0, 0], False, True, None), {})
cnt: 1, ((T([64, 128, 28, 28], f16), T([64, 128, 56, 56], f16), [2, 2], [2, 2], [0, 0], False, True, None), {})
```

----------------------------------------

TITLE: Specialized Python Primitives in Exported PyTorch Program
DESCRIPTION: The exported program showing how Python primitives are specialized. The loop has been unrolled with the constant value of 1 hard-coded into each operation, and the original inputs 'const' and 'times' are no longer used.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "f32[2, 2]", const, times):
            # code: x = x + const
            add: "f32[2, 2]" = torch.ops.aten.add.Tensor(x, 1)
            add_1: "f32[2, 2]" = torch.ops.aten.add.Tensor(add, 1)
            add_2: "f32[2, 2]" = torch.ops.aten.add.Tensor(add_1, 1)
            return (add_2,)
```

----------------------------------------

TITLE: Performing Convolution with ATen Convolution Operator
DESCRIPTION: Details `aten.convolution`, crucial for feature extraction in neural networks using filters of various sizes. Fueled by input-output combinations like [64, 3, 256, 256] and [24, 3, 3, 3], supporting multiple kernel sizes and strides.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_13

LANGUAGE: plaintext
CODE:
```
Operator: aten.convolution.default
cnt: 1, ((T([64, 3, 256, 256], f16), T([24, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
```

----------------------------------------

TITLE: Distributing nn.Module Parameters Using DTensor in PyTorch
DESCRIPTION: This example demonstrates how to use the high-level API to distribute parameters of a PyTorch nn.Module using DTensor, specifically sharding Linear layers.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch.nn as nn
from torch.distributed.tensor import Shard, distribute_tensor, distribute_module, init_device_mesh

class MyModule(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.fc1 = nn.Linear(8, 8)
        self.fc2 = nn.Linear(8, 8)
        self.relu = nn.ReLU()

    def forward(self, input):
        return self.relu(self.fc1(input) + self.fc2(input))

mesh = init_device_mesh("cuda", (4,))

def shard_params(mod_name, mod, mesh):
    col_linear_placement = [Shard(0)]
    if isinstance(mod, nn.Linear):
        for name, param in mod.named_parameters():
            dist_param = nn.Parameter(
                distribute_tensor(param, mesh, col_linear_placement)
            )
            mod.register_parameter(name, dist_param)

sharded_module = distribute_module(MyModule(), mesh, partition_fn=shard_params)
```

----------------------------------------

TITLE: Allocating New Empty Tensors with Strides using aten.new_empty_strided - Python
DESCRIPTION: Uses the aten.new_empty_strided.default operator to create an empty tensor of a specified shape and stride, mainly for efficient memory allocations, especially on CUDA devices. The code specifies dimensions and stride arrays for the tensor. Prerequisites: PyTorch with CUDA support and tensor memory layout knowledge.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
Operator: aten.new_empty_strided.default
cnt: 36, ((T([288, 64, 64], f16), [288, 64, 64], [4096, 64, 1]), {})
```

----------------------------------------

TITLE: Enabling oneDNN Graph Fusion for BFloat16 - Python
DESCRIPTION: This snippet illustrates how to enable oneDNN Graph fusion and use BFloat16 via torch.cpu.amp.autocast(). It explicitly disables JIT AMP, traces the model, freezes it, and shows the warm-up runs needed before speedup is observed. Requires native BFloat16 hardware support.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/onednn/README.md#_snippet_7

LANGUAGE: python
CODE:
```
# Assuming we have a model of the name 'model'\n\nexample_input = torch.rand(1, 3, 224, 224)\n\n# enable oneDNN Graph\ntorch.jit.enable_onednn_fusion(True)\n# Disable AMP for JIT\ntorch._C._jit_set_autocast_mode(False)\nwith torch.no_grad(), torch.cpu.amp.autocast():\n    model = torch.jit.trace(model, (example_input))\n    model = torch.jit.freeze(model)\n     # 2 warm-ups (2 for tracing/scripting with an example, 3 without an example)\n    model(example_input)\n    model(example_input)\n\n    # speedup would be observed in subsequent runs.\n    model(example_input)
```

----------------------------------------

TITLE: Collecting Raw CUDA Memory Usage in PyTorch
DESCRIPTION: This code snippet shows how to use the device_memory_used function to collect raw CUDA memory usage information for a specific device in PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch_cuda_memory.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
device_idx = ...
print(torch.cuda.device_memory_used(device_idx))
```

----------------------------------------

TITLE: Batching Operations on DataFrames
DESCRIPTION: Implementation of batching operations on DataFrames and comparison with regular DataPipe batching.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/dataframes_pipes.ipynb#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
dp = get_dataframes_pipe(dataframe_size = 3)
dp = dp.shuffle()
dp = dp.batch(2)
print("Iterate over DataFrame batches")
for i,v in enumerate(dp):
    print(v)
```

----------------------------------------

TITLE: Modified Bytecode for Function with Graph Break
DESCRIPTION: The bytecode generated by Dynamo after compilation, showing how it splits the function at the graph break point. It creates two separate functions - the original and a continuation function - and executes them in sequence.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
MODIFIED BYTECODE fn script.py line 3
 0 LOAD_GLOBAL              1 (__compiled_fn_0)
 2 LOAD_FAST                0 (a)
 4 CALL_FUNCTION            1
 6 STORE_FAST               3 (graph_out_0)
 8 LOAD_GLOBAL              0 (print)
10 LOAD_CONST               2 ('Hi')
12 LOAD_FAST                3 (graph_out_0)
14 LOAD_CONST               3 (0)
16 BINARY_SUBSCR
18 STORE_FAST               1 (b)

20 CALL_FUNCTION            1
22 LOAD_GLOBAL              2 (__resume_at_14_1)
24 ROT_TWO
26 LOAD_FAST                0 (a)
28 LOAD_FAST                1 (b)
30 CALL_FUNCTION            3
32 RETURN_VALUE

MODIFIED BYTECODE resume_in_fn script.py line 6
 0 LOAD_GLOBAL              1 (__compiled_fn_2)
 2 LOAD_FAST                2 (b)
 4 LOAD_FAST                1 (a)
 6 CALL_FUNCTION            2
 8 UNPACK_SEQUENCE          1
10 RETURN_VALUE
```

----------------------------------------

TITLE: Creating Sparse BSR Tensor in PyTorch
DESCRIPTION: Demonstrates the construction of a Block Sparse Row (BSR) tensor using crow_indices, col_indices, and values tensors. Shows both direct construction and conversion from dense tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_20

LANGUAGE: Python
CODE:
```
crow_indices = torch.tensor([0, 2, 4])
col_indices = torch.tensor([0, 1, 0, 1])
values = torch.tensor([[[0, 1, 2], [6, 7, 8]],
                      [[3, 4, 5], [9, 10, 11]],
                      [[12, 13, 14], [18, 19, 20]],
                      [[15, 16, 17], [21, 22, 23]]])
bsr = torch.sparse_bsr_tensor(crow_indices, col_indices, values, dtype=torch.float64)
```

----------------------------------------

TITLE: Forcing Metal Kernels over MPS Graph API (Environment Variable)
DESCRIPTION: Set `PYTORCH_MPS_PREFER_METAL` to `1` to force the use of Metal kernels instead of the MPS Graph APIs. Currently, this primarily affects the matrix multiplication (matmul) operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mps_environment_variables.rst#2025-04-22_snippet_6

LANGUAGE: plaintext
CODE:
```
PYTORCH_MPS_PREFER_METAL
```

----------------------------------------

TITLE: PyTorch Grid Sampling Operations
DESCRIPTION: Grid sampling operations for 2D tensors with specified interpolation modes and padding configurations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Super_SloMo_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
((T([6, 3, 352, 352], f16), T([6, 352, 352, 2], f16), 0, 0, False), {})
```

----------------------------------------

TITLE: ReLU Activation in PyTorch
DESCRIPTION: This snippet shows the ReLU (Rectified Linear Unit) activation operation. It is applied in-place on a 4D tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
Operator: aten.relu_.default
cnt: 3, ((T([64, 64, 112, 112], f16),), {})
```

----------------------------------------

TITLE: Profiling Multiple aten Operator Calls - PyTorch - Python
DESCRIPTION: These snippets list calls to various aten operators in PyTorch with tensor descriptor tuples, parameter details, and invocation counts. Dependencies include having PyTorch installed and a working knowledge of its tensor API. Each call records the tensor shape, data type (e.g., f16, i64), potential stride, and operator-specific parameters; the structure is useful for profiling or reproducing workloads for benchmarking, fuzzing, or test generation. The inputs are tuples containing tensor properties and operator arguments, and the outputs are typically results of those operations or information for test harnesses.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/pnasnet5large_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([16, 54, 83, 83], f16), T([16, 54, 167, 167], f16), T([54, 1, 3, 3], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 54, [True, True, False]), {})
cnt: 2, ((T([16, 54, 83, 83], f16), T([16, 54, 83, 83], f16), T([54, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 54, [True, True, False]), {})
cnt: 1, ((T([16, 54, 83, 83], f16), T([16, 54, 169, 169], f16), T([54, 1, 5, 5], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 54, [True, True, False]), {})
cnt: 1, ((T([16, 54, 83, 83], f16), T([16, 54, 83, 83], f16), T([54, 1, 7, 7], f16), [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 54, [True, True, False]), {})
cnt: 1, ((T([16, 54, 83, 83], f16), T([16, 54, 171, 171], f16), T([54, 1, 7, 7], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 54, [True, True, False]), {})
cnt: 1, ((T([16, 96, 83, 83], f16), T([16, 96, 169, 169], f16), T([96, 1, 5, 5], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 96, [True, True, False]), {})
cnt: 1, ((T([16, 54, 165, 165], f16), T([16, 96, 165, 165], f16), T([54, 96, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([16, 96, 165, 165], f16), T([16, 3, 331, 331], f16), T([96, 3, 3, 3], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [False, True, False]), {})
Operator: aten.copy_.default
cnt: 1, ((T([16, 3, 331, 331], f16), T([16, 3, 331, 331], f16)), {})
Operator: aten.div.Scalar
cnt: 1, ((T([16, 4320, 11, 11], f16, stride=(4320, 1, 0, 0)), 121), {})
Operator: aten.lift_fresh_copy.default
cnt: 1, ((T([16], i64),), {})
Operator: aten.max_pool2d_with_indices.default
cnt: 1, ((T([16, 96, 167, 167], f16), [3, 3], [2, 2]), {})
cnt: 2, ((T([16, 54, 167, 167], f16), [3, 3], [2, 2]), {})
cnt: 3, ((T([16, 108, 85, 85], f16), [3, 3], [2, 2]), {})
cnt: 12, ((T([16, 216, 42, 42], f16), [3, 3], [1, 1], [1, 1]), {})
cnt: 3, ((T([16, 432, 43, 43], f16), [3, 3], [2, 2]), {})
cnt: 9, ((T([16, 432, 21, 21], f16), [3, 3], [1, 1], [1, 1]), {})
cnt: 3, ((T([16, 864, 23, 23], f16), [3, 3], [2, 2]), {})
cnt: 9, ((T([16, 864, 11, 11], f16), [3, 3], [1, 1], [1, 1]), {})
Operator: aten.max_pool2d_with_indices_backward.default
cnt: 9, ((T([16, 864, 11, 11], f16, stride=(522720, 121, 11, 1)), T([16, 864, 11, 11], f16), [3, 3], [1, 1], [1, 1], [1, 1], False, T([16, 864, 11, 11], i64)), {})
cnt: 3, ((T([16, 864, 11, 11], f16, stride=(522720, 121, 11, 1)), T([16, 864, 23, 23], f16), [3, 3], [2, 2], [0, 0], [1, 1], False, T([16, 864, 11, 11], i64)), {})
cnt: 9, ((T([16, 432, 21, 21], f16, stride=(952560, 441, 21, 1)), T([16, 432, 21, 21], f16), [3, 3], [1, 1], [1, 1], [1, 1], False, T([16, 432, 21, 21], i64)), {})
cnt: 3, ((T([16, 432, 21, 21], f16, stride=(952560, 441, 21, 1)), T([16, 432, 43, 43], f16), [3, 3], [2, 2], [0, 0], [1, 1], False, T([16, 432, 21, 21], i64)), {})
cnt: 12, ((T([16, 216, 42, 42], f16, stride=(1905120, 1764, 42, 1)), T([16, 216, 42, 42], f16), [3, 3], [1, 1], [1, 1], [1, 1], False, T([16, 216, 42, 42], i64)), {})
cnt: 3, ((T([16, 108, 42, 42], f16, stride=(952560, 1764, 42, 1)), T([16, 108, 85, 85], f16), [3, 3], [2, 2], [0, 0], [1, 1], False, T([16, 108, 42, 42], i64)), {})
cnt: 2, ((T([16, 54, 83, 83], f16, stride=(1860030, 6889, 83, 1)), T([16, 54, 167, 167], f16), [3, 3], [2, 2], [0, 0], [1, 1], False, T([16, 54, 83, 83], i64)), {})
cnt: 1, ((T([16, 96, 83, 83], f16), T([16, 96, 167, 167], f16), [3, 3], [2, 2], [0, 0], [1, 1], False, T([16, 96, 83, 83], i64)), {})
Operator: aten.mean.dim
cnt: 1, ((T([16, 4320, 11, 11], f16), [-1, -2], True), {})
Operator: aten.mm.default
cnt: 1, ((T([16, 1000], f16), T([1000, 4320], f16)), {})
cnt: 1, ((T([1000, 16], f16, stride=(1, 1000)), T([16, 4320], f16)), {})
Operator: aten.native_batch_norm.default
cnt: 1, ((T([16, 96, 165, 165], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), True, 0.1, 0.001), {})
cnt: 1, ((T([16, 54, 165, 165], f16), T([54], f16), T([54], f16), T([54], f16), T([54], f16), True, 0.1, 0.001), {})
cnt: 14, ((T([16, 54, 83, 83], f16), T([54], f16), T([54], f16), T([54], f16), T([54], f16), True, 0.1, 0.001), {})
cnt: 2, ((T([16, 108, 83, 83], f16), T([108], f16), T([108], f16), T([108], f16), T([108], f16), True, 0.1, 0.001), {})
cnt: 13, ((T([16, 108, 42, 42], f16), T([108], f16), T([108], f16), T([108], f16), T([108], f16), True, 0.1, 0.001), {})
cnt: 56, ((T([16, 216, 42, 42], f16), T([216], f16), T([216], f16), T([216], f16), T([216], f16), True, 0.1, 0.001), {})
cnt: 2, ((T([16, 432, 42, 42], f16), T([432], f16), T([432], f16), T([432], f16), T([432], f16), True, 0.1, 0.001), {})
cnt: 55, ((T([16, 432, 21, 21], f16), T([432], f16), T([432], f16), T([432], f16), T([432], f16), True, 0.1, 0.001), {})
cnt: 2, ((T([16, 864, 21, 21], f16), T([864], f16), T([864], f16), T([864], f16), T([864], f16), True, 0.1, 0.001), {})
cnt: 55, ((T([16, 864, 11, 11], f16), T([864], f16), T([864], f16), T([864], f16), T([864], f16), True, 0.1, 0.001), {})
Operator: aten.native_batch_norm_backward.default
cnt: 17, ((T([16, 864, 11, 11], f16, stride=(522720, 121, 11, 1)), T([16, 864, 11, 11], f16), T([864], f16), T([864], f16), T([864], f16), T([864], f32), T([864], f32), True, 0.001, [True, True, True]), {})
cnt: 38, ((T([16, 864, 11, 11], f16), T([16, 864, 11, 11], f16), T([864], f16), T([864], f16), T([864], f16), T([864], f32), T([864], f32), True, 0.001, [True, True, True]), {})
cnt: 2, ((T([16, 864, 21, 21], f16), T([16, 864, 21, 21], f16), T([864], f16), T([864], f16), T([864], f16), T([864], f32), T([864], f32), True, 0.001, [True, True, True]), {})
cnt: 17, ((T([16, 432, 21, 21], f16, stride=(952560, 441, 21, 1)), T([16, 432, 21, 21], f16), T([432], f16), T([432], f16), T([432], f16), T([432], f32), T([432], f32), True, 0.001, [True, True, True]), {})
cnt: 38, ((T([16, 432, 21, 21], f16), T([16, 432, 21, 21], f16), T([432], f16), T([432], f16), T([432], f16), T([432], f32), T([432], f32), True, 0.001, [True, True, True]), {})
cnt: 2, ((T([16, 432, 42, 42], f16), T([16, 432, 42, 42], f16), T([432], f16), T([432], f16), T([432], f16), T([432], f32), T([432], f32), True, 0.001, [True, True, True]), {})
cnt: 16, ((T([16, 216, 42, 42], f16, stride=(1905120, 1764, 42, 1)), T([16, 216, 42, 42], f16), T([216], f16), T([216], f16), T([216], f16), T([216], f32), T([216], f32), True, 0.001, [True, True, True]), {})
cnt: 40, ((T([16, 216, 42, 42], f16), T([16, 216, 42, 42], f16), T([216], f16), T([216], f16), T([216], f16), T([216], f32), T([216], f32), True, 0.001, [True, True, True]), {})
cnt: 5, ((T([16, 108, 42, 42], f16, stride=(952560, 1764, 42, 1)), T([16, 108, 42, 42], f16), T([108], f16), T([108], f16), T([108], f16), T([108], f32), T([108], f32), True, 0.001, [True, True, True]), {})
cnt: 8, ((T([16, 108, 42, 42], f16), T([16, 108, 42, 42], f16), T([108], f16), T([108], f16), T([108], f16), T([108], f32), T([108], f32), True, 0.001, [True, True, True]), {})
cnt: 2, ((T([16, 108, 83, 83], f16), T([16, 108, 83, 83], f16), T([108], f16), T([108], f16), T([108], f16), T([108], f32), T([108], f32), True, 0.001, [True, True, True]), {})
cnt: 6, ((T([16, 54, 83, 83], f16, stride=(1860030, 6889, 83, 1)), T([16, 54, 83, 83], f16), T([54], f16), T([54], f16), T([54], f16), T([54], f32), T([54], f32), True, 0.001, [True, True, True]), {})
cnt: 8, ((T([16, 54, 83, 83], f16), T([16, 54, 83, 83], f16), T([54], f16), T([54], f16), T([54], f16), T([54], f32), T([54], f32), True, 0.001, [True, True, True]), {})
cnt: 1, ((T([16, 54, 165, 165], f16), T([16, 54, 165, 165], f16), T([54], f16), T([54], f16), T([54], f16), T([54], f32), T([54], f32), True, 0.001, [True, True, True]), {})
cnt: 1, ((T([16, 96, 165, 165], f16), T([16, 96, 165, 165], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), True, 0.001, [True, True, True]), {})
Operator: aten.nll_loss_backward.default
cnt: 1, ((T([], f16), T([16, 1000], f16), T([16], i64), None, 1, -100, T([], f16)), {})
Operator: aten.nll_loss_forward.default
cnt: 1, ((T([16, 1000], f16), T([16], i64), None, 1, -100), {})
Operator: aten.relu.default
cnt: 4, ((T([16, 96, 165, 165], f16),), {})
cnt: 7, ((T([16, 54, 83, 83], f16),), {})
cnt: 4, ((T([16, 54, 165, 165], f16),), {})
cnt: 2, ((T([16, 270, 83, 83], f16),), {})
cnt: 6, ((T([16, 108, 83, 83], f16),), {})
cnt: 7, ((T([16, 108, 42, 42], f16),), {})
cnt: 2, ((T([16, 540, 42, 42], f16),), {})
cnt: 48, ((T([16, 216, 42, 42], f16),), {})
cnt: 8, ((T([16, 1080, 42, 42], f16),), {})
cnt: 6, ((T([16, 432, 42, 42], f16),), {})
cnt: 43, ((T([16, 432, 21, 21], f16),), {})
cnt: 8, ((T([16, 2160, 21, 21], f16),), {})
cnt: 6, ((T([16, 864, 21, 21], f16),), {})
cnt: 43, ((T([16, 864, 11, 11], f16),), {})
cnt: 6, ((T([16, 4320, 11, 11], f16),), {})
Operator: aten.sum.SymInt
cnt: 1, ((T([16, 1000], f16), [0], True), {})
Operator: aten.threshold_backward.default
cnt: 6, ((T([16, 4320, 11, 11], f16), T([16, 4320, 11, 11], f16), 0), {})
cnt: 43, ((T([16, 864, 11, 11], f16), T([16, 864, 11, 11], f16), 0), {})
cnt: 8, ((T([16, 2160, 21, 21], f16), T([16, 2160, 21, 21], f16), 0), {})
cnt: 6, ((T([16, 864, 21, 21], f16), T([16, 864, 21, 21], f16), 0), {})
cnt: 43, ((T([16, 432, 21, 21], f16), T([16, 432, 21, 21], f16), 0), {})
cnt: 8, ((T([16, 1080, 42, 42], f16), T([16, 1080, 42, 42], f16), 0), {})
cnt: 6, ((T([16, 432, 42, 42], f16), T([16, 432, 42, 42], f16), 0), {})
cnt: 48, ((T([16, 216, 42, 42], f16), T([16, 216, 42, 42], f16), 0), {})
cnt: 2, ((T([16, 540, 42, 42], f16), T([16, 540, 42, 42], f16), 0), {})
cnt: 2, ((T([16, 270, 83, 83], f16), T([16, 270, 83, 83], f16), 0), {})
cnt: 6, ((T([16, 108, 83, 83], f16), T([16, 108, 83, 83], f16), 0), {})
cnt: 7, ((T([16, 108, 42, 42], f16), T([16, 108, 42, 42], f16), 0), {})
cnt: 4, ((T([16, 96, 165, 165], f16), T([16, 96, 165, 165], f16), 0), {})
cnt: 4, ((T([16, 54, 165, 165], f16), T([16, 54, 165, 165], f16), 0), {})
cnt: 7, ((T([16, 54, 83, 83], f16), T([16, 54, 83, 83], f16), 0), {})
```

----------------------------------------

TITLE: Using experimental local_map feature
DESCRIPTION: Applies a local function to each shard of a DTensor using the experimental local_map function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.tensor.rst#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
local_map(dtensor, local_func)
```

----------------------------------------

TITLE: Built-in TorchScript Classes Example
DESCRIPTION: Example showing usage of built-in TorchScript classes including torch.device and tensor operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
import torch

@torch.jit.script
class A:
    def __init__(self):
        self.x = torch.rand(3)

    def f(self, y: torch.device):
        return self.x.to(device=y)

def g():
    a = A()
    return a.f(torch.device("cpu"))

script_g = torch.jit.script(g)
print(script_g.graph)
```

----------------------------------------

TITLE: Backpropagating ELU with aten.elu_backward in Python
DESCRIPTION: This operator, aten.elu_backward.default, computes gradients of the ELU activation function for backpropagation in training. The function requires an input tensor, parameters to determine activation shape, and a gradient tensor for the backward pass, ensuring efficient gradient updates under different tensor strides.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/nvidia_deeprecommender_training.txt#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
Operator: aten.elu_backward.default
cnt: 1, ((T([256, 197951], f16, stride=(0, 0)), 1.6732632423543772, 1.0507009873554805, 1, False, T([256, 197951], f16)), {})
cnt: 4, ((T([256, 512], f16), 1.6732632423543772, 1.0507009873554805, 1, False, T([256, 512], f16)), {})
cnt: 1, ((T([256, 1024], f16), 1.6732632423543772, 1.0507009873554805, 1, False, T([256, 1024], f16)), {})
```

----------------------------------------

TITLE: Creating a Non-Contiguous NJT View in Python
DESCRIPTION: This snippet demonstrates creating a non-contiguous NJT using `torch.nested.narrow` over an existing dense tensor. It shows how to specify the sequence lengths and use the method to avoid memory allocation and copying.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
>>> padded = torch.randn(3, 5, 4)
>>> seq_lens = torch.tensor([3, 2, 5], dtype=torch.int64)
>>> nt = torch.nested.narrow(padded, dim=1, start=0, length=seq_lens, layout=torch.jagged)
>>> nt.shape
torch.Size([3, j1, 4])
>>> nt.is_contiguous()
False
```

----------------------------------------

TITLE: Logging Lift Fresh Copy Operations (aten.lift_fresh_copy) - PyTorch - Python
DESCRIPTION: This snippet registers call patterns for the aten.lift_fresh_copy operator, which creates a fresh copy of PyTorch tensors, typically with altered memory layout or device. Only simple tensor shape and dtype specifications are logged. Useful for tracking data movement and aliasing behavior.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
Operator: aten.lift_fresh_copy.default
cnt: 1, ((T([128], i64),), {})
```

----------------------------------------

TITLE: Average Pooling Backward Pass in PyTorch
DESCRIPTION: Statistics for the aten.avg_pool2d_backward.default operator used in backpropagation for average pooling operations. These operations compute gradients for the corresponding forward average pooling operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
Operator: aten.avg_pool2d_backward.default
cnt: 1, ((T([128, 1536, 6, 6], f16), T([128, 1536, 12, 12], f16), [2, 2], [2, 2], [0, 0], True, False, None), {})
cnt: 1, ((T([128, 512, 12, 12], f16), T([128, 512, 24, 24], f16), [2, 2], [2, 2], [0, 0], True, False, None), {})
cnt: 1, ((T([128, 256, 24, 24], f16), T([128, 256, 48, 48], f16), [2, 2], [2, 2], [0, 0], True, False, None), {})
```

----------------------------------------

TITLE: Convolutional Operations Profiling in PyTorch
DESCRIPTION: This data shows profiling of convolutional neural network operations with details about input, output, and weight tensor shapes, stride, padding, dilation, groups, and other parameters. Each line shows the count and configuration of a specific convolutional operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 112, 14, 14], f16), T([32, 480, 14, 14], f16), T([112, 480, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

----------------------------------------

TITLE: Invoking aten.upsample_bicubic2d.vec Operator (Log)
DESCRIPTION: Log entry detailing a call to the PyTorch `aten.upsample_bicubic2d.vec` operator. It shows the invocation count (`cnt`) and arguments: the input tensor (shape, dtype f16), the target output size, an align_corners flag (False), and scale factors (None, inferred from output size).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/crossvit_9_240_training.txt#2025-04-22_snippet_10

LANGUAGE: text
CODE:
```
Operator: aten.upsample_bicubic2d.vec
cnt: 1, ((T([64, 3, 240, 240], f16), [224, 224], False, None), {})
```

----------------------------------------

TITLE: Performing Batch Matrix Multiplication with ATen BMM Operator
DESCRIPTION: Covers `aten.bmm`, an essential operator for batch matrix multiplication in sequence models. Supports tensors of dimensions like [256, 1024, 32]. The operation is highly parallelizable and supports efficient computation on GPUs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_9

LANGUAGE: plaintext
CODE:
```
Operator: aten.bmm.default
cnt: 2, ((T([256, 1024, 32], f16, stride=(32768, 1, 1024)), T([256, 32, 1024], f16)), {})
cnt: 2, ((T([256, 1024, 1024], f16), T([256, 1024, 32], f16, stride=(32768, 1, 1024))), {})
```

----------------------------------------

TITLE: Executing aten._log_softmax in PyTorch
DESCRIPTION: This snippet captures the application of the aten._log_softmax.default operator on a tensor with dimensions [2048, 30522] and data type f16, focusing on softmax along dimension 1 without keeping dimensions. It may require PyTorch library availability and suitable CUDA environment for efficient execution.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MobileBertForMaskedLM_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
aten._log_softmax.default, ((T([2048, 30522], f16), 1, False), {})
```

----------------------------------------

TITLE: Negative Log Likelihood Loss Forward - PyTorch Aten
DESCRIPTION: Computes the Negative Log Likelihood Loss between input log-probabilities and target indices. This internal operator is commonly used as a loss function for multi-class classification. It takes the input log-probabilities, target indices, optional weight, reduction method, and an ignore index.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BartForConditionalGeneration_training.txt#_snippet_22

LANGUAGE: Python
CODE:
```
import torch

input_log_probs = torch.randn(2048, 50265, dtype=torch.float16)
target = torch.randint(0, 50265, (2048,), dtype=torch.int64)
loss = torch.nll_loss_forward(input_log_probs, target, weight=None, reduction=1, ignore_index=-100)
```

----------------------------------------

TITLE: Configuring Sparse Fully Connected Test for QNNPACK in CMake
DESCRIPTION: Creates and configures the sparse fully connected network test executable with C++14 standard requirements, includes the necessary directories, and links against required libraries like pytorch_qnnpack, clog, cpuinfo, fp16, and gtest.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_12

LANGUAGE: CMake
CODE:
```
add_executable(fully-connected-sparse-test test/fully-connected-sparse.cc)
set_target_properties(fully-connected-sparse-test PROPERTIES
  CXX_STANDARD 14
  CXX_STANDARD_REQUIRED YES
  CXX_EXTENSIONS NO)
target_include_directories(fully-connected-sparse-test PRIVATE src test)
target_link_libraries(fully-connected-sparse-test PRIVATE pytorch_qnnpack clog cpuinfo fp16 gtest gtest_main)
add_test(fully-connected-sparse-test fully-connected-sparse-test)
```

----------------------------------------

TITLE: Applying Threshold Backward Pass in PyTorch
DESCRIPTION: This snippet shows the frequency and tensor shapes for the backward pass of a thresholding operation, typically used in conjunction with ReLU or similar activation functions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/adv_inception_v3_training.txt#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
Operator: aten.threshold_backward.default
cnt: 2, ((T([128, 192, 8, 8], f16, stride=(131072, 64, 8, 1)), T([128, 192, 8, 8], f16), 0), {})
cnt: 8, ((T([128, 384, 8, 8], f16, stride=(131072, 64, 8, 1)), T([128, 384, 8, 8], f16), 0), {})
```

----------------------------------------

TITLE: Using Scalars with Tensors in ATen
DESCRIPTION: Examples of ATen Scalar usage with tensor operations. Scalars are dynamically typed single numbers that can be implicitly constructed from C++ number types and are used for functions that take both tensors and scalar values.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_basics.rst#2025-04-22_snippet_4

LANGUAGE: cpp
CODE:
```
namespace torch {
Tensor addmm(Scalar beta, const Tensor & self,
             Scalar alpha, const Tensor & mat1,
             const Tensor & mat2);
Scalar sum(const Tensor & self);
} // namespace torch

// Usage.
torch::Tensor a = ...
torch::Tensor b = ...
torch::Tensor c = ...
torch::Tensor r = torch::addmm(1.0, a, .5, b, c);
```

----------------------------------------

TITLE: Saving and Loading Pickled Objects with PackageExporter and PackageImporter
DESCRIPTION: Demonstrates how to save pickled objects using PackageExporter and load them using PackageImporter. It also shows how to print the file structure of a package and access attributes of imported objects.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
pe.save_pickle('foo_collection', 'foo1.pkl', foo_1)
pe.save_pickle('foo_collection', 'foo2.pkl', foo_2)

pi = PackageImporter('foo_package.pt')
print(pi.file_structure())
imported_foo = pi.load_pickle('foo_collection', 'foo1.pkl')
print(f"foo_1 string: '{imported_foo.my_string}'")
print(f"foo_1 export time: {imported_foo.time_exported}")
print(f"foo_1 import time: {imported_foo.time_imported}")
```

----------------------------------------

TITLE: Implementing Observer Insertion Points in PyTorch ModelReport Detector
DESCRIPTION: Example of implementing the determine_observer_insert_points() method in a custom detector. This method initializes the custom Observer and adds it to the returned dictionary.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/_model_report/README.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
def determine_observer_insert_points():
    # Initialize and add custom Observer to the returned dictionary
    pass
```

----------------------------------------

TITLE: Usage Log: aten.nll_loss_backward.default Operator (Text)
DESCRIPTION: Logs a call to the `aten.nll_loss_backward.default` operator, used for computing gradients of the negative log likelihood loss. The log shows the arguments including the gradient output (scalar f16), input tensor shape ([128, 1000] f16), target tensor shape ([128] i64), reduction type (1, likely 'mean'), and ignore index (-100).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dm_nfnet_f0_training.txt#2025-04-22_snippet_3

LANGUAGE: text
CODE:
```
Operator: aten.nll_loss_backward.default
cnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})
```

----------------------------------------

TITLE: Invoking aten.nll_loss_backward.default Operator (Log)
DESCRIPTION: Log entry detailing a call to the PyTorch `aten.nll_loss_backward.default` operator. It shows the invocation count (`cnt`) and arguments including gradient output tensor, model output tensor, target tensor, weight tensor (None), reduction type, ignore index, and total weight tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/crossvit_9_240_training.txt#2025-04-22_snippet_3

LANGUAGE: text
CODE:
```
Operator: aten.nll_loss_backward.default
cnt: 1, ((T([], f16), T([64, 1000], f16), T([64], i64), None, 1, -100, T([], f16)), {})
```

----------------------------------------

TITLE: Batch Normalization Operations with Half Precision
DESCRIPTION: PyTorch batch normalization operations with tensor shapes and half precision (f16) data type. Shows both forward and backward passes with various input dimensions and batch sizes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_23

LANGUAGE: python
CODE:
```
((T([64, 128, 32, 32], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})
```

----------------------------------------

TITLE: Transforming Tensors using the Tanh Function
DESCRIPTION: Applies the hyperbolic tangent function to a tensor, transforming input values to a range between -1 and 1. Requires an input tensor of shape [16, 3, 128, 128]. The operation facilitates non-linear transformations in activation functions, crucial for deep model inference.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_16

LANGUAGE: Python
CODE:
```
Operator: aten.tanh.default
cnt: 1, ((T([16, 3, 128, 128], f16),), {})
```

----------------------------------------

TITLE: Creating ThreadPoolExecutors for Asynchronous Data Transfer in PyTorch
DESCRIPTION: Implements two ThreadPoolExecutors with one worker each for device-to-host (D2H) and host-to-device (H2D) data transfers. Uses CUDA streams and semaphores with CUDA events for synchronization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/inference/CHANGELOG.md#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
d2h_executor = ThreadPoolExecutor(max_workers=1)
h2d_executor = ThreadPoolExecutor(max_workers=1)

# In each executor
cuda_stream = torch.cuda.Stream()
with torch.cuda.stream(cuda_stream):
    # Perform data transfer
```

----------------------------------------

TITLE: Implementing Matrix Multiplication and Batched Matrix Multiplication with Dimension Objects in Python
DESCRIPTION: This snippet demonstrates how to implement matrix multiplication (mm) and batched matrix multiplication (bmm) using dimension objects in PyTorch. It shows how dimension objects avoid naming conflicts even when using the same dimension name in nested functions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_22

LANGUAGE: python
CODE:
```
def mm(A, B):
    i, j, k = dims()
    r = (A[i, k] * B[k, j]).sum(k)
    return r.order(i, j)

def bmm(A, B):
    i = dims() # note: doesn't matter than mm internally also uses i
    return mm(A[i], B[i])
```

----------------------------------------

TITLE: Usage Log: aten.sigmoid.default Operator (Text)
DESCRIPTION: Logs calls to the sigmoid activation function (`aten.sigmoid.default`). The arguments show the shapes of the input tensors (e.g., [128, 256, 1, 1] f16). Different invocation counts are recorded for various input tensor dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dm_nfnet_f0_training.txt#2025-04-22_snippet_6

LANGUAGE: text
CODE:
```
Operator: aten.sigmoid.default
cnt: 1, ((T([128, 256, 1, 1], f16),), {})
cnt: 2, ((T([128, 512, 1, 1], f16),), {})
cnt: 9, ((T([128, 1536, 1, 1], f16),), {})
```

----------------------------------------

TITLE: Stacking Tensors in PyTorch
DESCRIPTION: This snippet stacks tensors along a new dimension. It's commonly used to combine multiple tensors into a single tensor, often for creating coordinate grids or combining features.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_17

LANGUAGE: Python
CODE:
```
cnt: 1, (([T([12, 16], i64, stride=(0, 1)), T([12, 16], i64, stride=(1, 0))], 2), {})
cnt: 1, (([T([24, 32], i64, stride=(0, 1)), T([24, 32], i64, stride=(1, 0))], 2), {})
cnt: 1, (([T([48, 64], i64, stride=(0, 1)), T([48, 64], i64, stride=(1, 0))], 2), {})
```

----------------------------------------

TITLE: Enabling Manual C++ Kernel Registration in PyTorch (native_functions.yaml)
DESCRIPTION: Illustrates the `manual_kernel_registration: True` flag in `native_functions.yaml`. Setting this flag prevents the PyTorch code generation system from automatically registering the C++ operator implementation with the dispatcher via the `TypeDefault` (catch-all) key. Manual registration is required elsewhere, typically in `torch/csrc/autograd/VariableTypeManual.cpp`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_10

LANGUAGE: yaml
CODE:
```
manual_kernel_registration: True
```

----------------------------------------

TITLE: PyTorch Matrix Multiplication Operations
DESCRIPTION: This snippet shows matrix multiplication operations between tensors in f16 precision. The first multiplies a [128, 1000] tensor with a [1000, 1984] tensor, and the second multiplies a [1000, 128] tensor with a [128, 1984] tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
Operator: aten.mm.default
cnt: 1, ((T([128, 1000], f16), T([1000, 1984], f16)), {})
cnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 1984], f16)), {})
```

----------------------------------------

TITLE: Conditionally Adding ATen Test Subdirectory in CMake
DESCRIPTION: Checks build configuration flags to decide whether to include the ATen tests. If `ATEN_NO_TEST` is set or if `BUILD_LITE_INTERPRETER` is enabled, it prints a message indicating tests are disabled. Otherwise, it uses `add_subdirectory(test)` to include the `test` subdirectory in the build.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_30

LANGUAGE: cmake
CODE:
```
if(ATEN_NO_TEST)
  message("disable test because ATEN_NO_TEST is set")
elif(BUILD_LITE_INTERPRETER)
  message("disable aten test when BUILD_LITE_INTERPRETER is enabled")
else()
  add_subdirectory(test)
endif()
```

----------------------------------------

TITLE: Disabling cuDNN Convolution Benchmarking in Python
DESCRIPTION: Disables the cuDNN benchmarking feature by setting `torch.backends.cudnn.benchmark = False`. This forces cuDNN to deterministically select a convolution algorithm instead of benchmarking multiple options, thus avoiding variability between runs caused by benchmarking noise or hardware differences. This may potentially reduce performance compared to allowing benchmarking.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/randomness.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
torch.backends.cudnn.benchmark = False
```

----------------------------------------

TITLE: Downloading MNIST Dataset for Integration Tests (Shell)
DESCRIPTION: Shell command to download the MNIST dataset, which is a prerequisite for running C++ Frontend integration tests. This command executes a Python script (`tools/download_mnist.py`) specifying the target directory (`test/cpp/api/mnist`) relative to the PyTorch root. This command must be run from the PyTorch root folder before executing the integration tests.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/api/README.md#2025-04-22_snippet_2

LANGUAGE: sh
CODE:
```
```sh
$ python tools/download_mnist.py -d test/cpp/api/mnist
```
```

----------------------------------------

TITLE: Expressing Relationships Between Dynamic Dimensions in PyTorch Export
DESCRIPTION: Example of using torch.export.Dim to define relationships between input shapes. Shows how to specify that one dimension is one larger than another dimension, with constraints on possible values.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
class M(torch.nn.Module):
    def forward(self, x, y):
        return x + y[1:]

x, y = torch.randn(5), torch.randn(6)
dimx = torch.export.Dim("dimx", min=3, max=6)
dimy = dimx + 1

exported_program = torch.export.export(
    M(), (x, y), dynamic_shapes=({0: dimx}, {0: dimy}),
)
print(exported_program)
```

----------------------------------------

TITLE: Analyzing Threshold Backward Operation in PyTorch
DESCRIPTION: This snippet shows the usage of threshold_backward.default operator with various tensor shapes and counts of each unique configuration.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/swsl_resnext101_32x16d_training.txt#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([32, 2048, 7, 7], f16), T([32, 2048, 7, 7], f16), 0), {})
cnt: 5, ((T([32, 4096, 7, 7], f16), T([32, 4096, 7, 7], f16), 0), {})
cnt: 1, ((T([32, 4096, 14, 14], f16), T([32, 4096, 14, 14], f16), 0), {})
cnt: 23, ((T([32, 1024, 14, 14], f16), T([32, 1024, 14, 14], f16), 0), {})
```

----------------------------------------

TITLE: Applying avg_pool2d and avg_pool2d_backward Operators - PyTorch - Python
DESCRIPTION: Shows configuration and arguments to forward and backward average pooling layers. Parameters such as kernel size, stride, padding, ceil_mode, and count_include_pad are specified for spatial reduction in neural nets. Actual input and output shapes match the feature map dimensions before and after pooling operations, both in forward and backward passes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ecaresnet101d_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
Operator: aten.avg_pool2d.default
cnt: 1, ((T([64, 256, 56, 56], f16), [2, 2], [2, 2], [0, 0], True, False), {})
cnt: 1, ((T([64, 512, 28, 28], f16), [2, 2], [2, 2], [0, 0], True, False), {})
cnt: 1, ((T([64, 1024, 14, 14], f16), [2, 2], [2, 2], [0, 0], True, False), {})
Operator: aten.avg_pool2d_backward.default
cnt: 1, ((T([64, 1024, 7, 7], f16), T([64, 1024, 14, 14], f16), [2, 2], [2, 2], [0, 0], True, False, None), {})
cnt: 1, ((T([64, 512, 14, 14], f16), T([64, 512, 28, 28], f16), [2, 2], [2, 2], [0, 0], True, False, None), {})
cnt: 1, ((T([64, 256, 28, 28], f16), T([64, 256, 56, 56], f16), [2, 2], [2, 2], [0, 0], True, False, None), {})
```

----------------------------------------

TITLE: Configuring Operator Dispatch in PyTorch native_functions.yaml
DESCRIPTION: Shows the syntax in `native_functions.yaml` for defining the dispatch target for a specific function overload (`func.out_overload(...)`) using the `CompositeImplicitAutograd` backend, mapping it to the `func_out` implementation. This backend allows automatic differentiation if the kernel's constituent operations support it.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_6

LANGUAGE: yaml
CODE:
```
# did, you could call them here and autograd would be inferred)
func: func.out_overload(...) -> ...
dispatch:
    CompositeImplicitAutograd: func_out
```

----------------------------------------

TITLE: Configuring Metal Support in ATen CMake
DESCRIPTION: Sets up Metal-related source files and compilation flags based on build options. It handles different scenarios for Metal support, including export, regular usage on Apple platforms, and fallback.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_7

LANGUAGE: CMake
CODE:
```
if(USE_PYTORCH_METAL_EXPORT)
  # Add files needed from exporting metal models(optimized_for_mobile)
  set(all_cpu_cpp ${all_cpu_cpp} ${metal_cpp} ${metal_prepack_cpp})
elseif(APPLE AND USE_PYTORCH_METAL)
  # Compile Metal kernels
  set(all_cpu_cpp ${all_cpu_cpp} ${metal_cpp} ${native_metal_srcs})
else()
  set(all_cpu_cpp ${all_cpu_cpp} ${metal_cpp})
endif()
```

----------------------------------------

TITLE: Usage Log: aten.sum.default Operator (Text)
DESCRIPTION: Logs calls to the `aten.sum.default` operator, likely summing all elements of the input tensor. The arguments only show the input tensor shape (f16), indicating a reduction across all dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dm_nfnet_f0_training.txt#2025-04-22_snippet_9

LANGUAGE: text
CODE:
```
Operator: aten.sum.default
cnt: 3, ((T([128, 1536, 6, 6], f16),), {})
cnt: 6, ((T([128, 1536, 12, 12], f16),), {})
cnt: 2, ((T([128, 512, 24, 24], f16),), {})
cnt: 1, ((T([128, 256, 48, 48], f16),), {})
```

----------------------------------------

TITLE: Executing aten.addmm operation in PyTorch
DESCRIPTION: Utilizes the Aten backend to perform matrix multiplication of two matrices with an added bias matrix in PyTorch, leveraging half-precision floating point tensors. It requires predefined tensor shapes. Outputs a resultant tensor from the matrix-multiplication operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
Operator: aten.addmm.default
cnt: 1, ((T([1000], f16), T([128, 2304], f16), T([2304, 1000], f16, stride=(1, 2304))), {})
```

----------------------------------------

TITLE: Registering Side-Effectful Custom Op in PyTorch C++
DESCRIPTION: Demonstrates how to register a custom PyTorch operator in C++ using `TORCH_LIBRARY` and `m.def`. The `c10::AliasAnalysisKind::CONSERVATIVE` annotation is used to mark the operation as side-effectful or conservatively analyzed, preventing dead-code elimination by TorchScript for ops like logging.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_39

LANGUAGE: C++
CODE:
```
TORCH_LIBRARY(my_library, m) {
  m.def(torch::schema(
    "my_logging_op(Tensor data) -> ()",
    c10::AliasAnalysisKind::CONSERVATIVE));
}
```

----------------------------------------

TITLE: Guards Generated for Dynamic Shape Input in PyTorch Dynamo
DESCRIPTION: These are the guards generated by Dynamo after retracing due to a shape change. The guards for size now use `None` for the first dimension, indicating it's dynamic. Additional guards ensure that the dynamic dimension (`L['a'].size()[0]`) is equal for both tensors ('duck shaping') and that it meets certain constraints (e.g., `2 <= L['a'].size()[0]`, avoiding specialization for 0 and 1).
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
# Guards second call
check_tensor(L['a'], torch.float32, device=None, requires_grad=False, size=[None, 3], stride=[3, 1])
check_tensor(L['b'], torch.float32, device=None, requires_grad=False, size=[None, 3], stride=[3, 1])

L['b'].size()[0] == L['a'].size()[0]
2 <= L['a'].size()[0]
```

----------------------------------------

TITLE: Using CUDAMultiStreamGuard for Multiple Streams on Multiple Devices in PyTorch (C++)
DESCRIPTION: This example illustrates the use of at::cuda::CUDAMultiStreamGuard to manage multiple CUDA streams across multiple devices simultaneously. It sets streams on device 0 and 1 without changing the current device index.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_10

LANGUAGE: C++
CODE:
```
{
  // This is the same as calling `torch::cuda::setCurrentCUDAStream` on both streams
  at::cuda::CUDAMultiStreamGuard multi_guard({streams0[1], streams1[1]});

  // current device index is not change, still 0
  // current CUDA stream on device 0 and device 1 are set to `streams0[1]` and `streams1[1]`
}
// current CUDA stream on device 0 and device 1 are reset to `streams0[0]` and `streams1[0]`
// after `multi_guard` is destroyed.
```

----------------------------------------

TITLE: Mean Dimension Operations in PyTorch
DESCRIPTION: Collection of mean operations across specific dimensions with various tensor shapes, primarily using half-precision (f16) tensors. Operations focus on reducing spatial dimensions (2,3) or (-1,-2) with keepdim=True.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
aten.mean.dim
cnt: 5, ((T([128, 120, 28, 28], f16), [2, 3], True), {})
cnt: 6, ((T([128, 360, 14, 14], f16), [2, 3], True), {})
cnt: 1, ((T([128, 720, 7, 7], f16), [2, 3], True), {})
```

----------------------------------------

TITLE: Ensembling Predictions with Distinct Minibatches via Loop - PyTorch - Python
DESCRIPTION: Assigns a separate data minibatch to each model and computes predictions using a Python list comprehension. Minibatches must be generated by slicing the data to match the number of models. This for-loop method is the baseline for evaluating performance and correctness against vectorized alternatives.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/ensembling.ipynb#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
minibatches = data[:num_models]
predictions_diff_minibatch_loop = [model(minibatch) for model, minibatch in zip(models, minibatches)]
```

----------------------------------------

TITLE: Adding Tensors in PyTorch
DESCRIPTION: This snippet represents the 'aten.add.Tensor' operation, which handles the addition of tensors with identical dimensions using the PyTorch framework. Dependencies include PyTorch with support for half-precision floating point computations. It processes tensor pairings, where inputs and outputs are tensors of the same dimensions having performed element-wise addition.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v2_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten.add.Tensor
cnt: 2, ((T([96, 24, 56, 56], f16), T([96, 24, 56, 56], f16)), {})
cnt: 4, ((T([96, 32, 28, 28], f16), T([96, 32, 28, 28], f16)), {})
cnt: 6, ((T([96, 64, 14, 14], f16), T([96, 64, 14, 14], f16)), {})
cnt: 4, ((T([96, 96, 14, 14], f16), T([96, 96, 14, 14], f16)), {})
cnt: 4, ((T([96, 160, 7, 7], f16), T([96, 160, 7, 7], f16)), {})
```

----------------------------------------

TITLE: Adding Global Callback for Operator Profiling in PyTorch C++
DESCRIPTION: This C++ snippet demonstrates how to initialize and add global callbacks to profile PyTorch operator invocations. It uses the RecordFunctionCallback interface to track function entries and exits, capturing input and execution details for profiling. The snippet requires the PyTorch C++ API, particularly the torch::addGlobalCallback function. The initialization ensures callbacks are triggered during operator execution, and random sampling can be configured to reduce overhead.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/large_scale_deployments.rst#2025-04-22_snippet_0

LANGUAGE: cpp
CODE:
```
// Called somewhere in the program beginning
void init() {
    // Sample one in a hundred operator runs randomly
    addGlobalCallback(
      RecordFunctionCallback(
        &onFunctionEnter,
        &onFunctionExit)
      .needsInputs(true)
      .samplingProb(0.01)
    );
    // Note, to enable observers in the model calling thread,
    // call enableRecordFunction() in the thread before running a model
}

void onFunctionEnter(const RecordFunction& fn) {
    std::cerr << "Before function " << fn.name()
              << " with " << fn.inputs().size() << " inputs" << std::endl;
}

void onFunctionExit(const RecordFunction& fn) {
    std::cerr << "After function " << fn.name();
}
```

----------------------------------------

TITLE: C++ Inference Implementation
DESCRIPTION: Example C++ code demonstrating how to load and use the compiled model for inference, including support for dynamic batch sizes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_aot_inductor.rst#2025-04-22_snippet_2

LANGUAGE: cpp
CODE:
```
#include <iostream>
#include <vector>

#include <torch/torch.h>
#include <torch/csrc/inductor/aoti_package/model_package_loader.h>

int main() {
    c10::InferenceMode mode;

    torch::inductor::AOTIModelPackageLoader loader("model.pt2");
    std::vector<torch::Tensor> inputs = {torch::randn({8, 10}, at::kCUDA)};
    std::vector<torch::Tensor> outputs = loader.run(inputs);
    std::cout << "Result from the first inference:"<< std::endl;
    std::cout << outputs[0] << std::endl;

    std::cout << "Result from the second inference:"<< std::endl;
    std::cout << loader.run({torch::randn({1, 10}, at::kCUDA)})[0] << std::endl;

    return 0;
}
```

----------------------------------------

TITLE: Displaying Functional Model Representation
DESCRIPTION: Prints the representation of the `fmodel` object obtained from `make_functional_with_buffers`. This output confirms that `fmodel` is an instance of `FunctionalModuleWithBuffers`, indicating the successful transformation of the original model into a stateless functional form suitable for `functorch` operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
fmodel
```

----------------------------------------

TITLE: Calling aten._to_copy.default (Python)
DESCRIPTION: Creates a copy of a tensor with potential changes to its data type, layout, or device. Examples show casting float32 to float16 and copying a float16 tensor to a different device ('cuda') while potentially preserving layout.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_4

LANGUAGE: Python
CODE:
```
((T([128, 128], f32),), {'dtype': f16})
```

LANGUAGE: Python
CODE:
```
((T([8, 1, 128, 128], f16, stride=(0, 16384, 128, 1)),), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})
```

----------------------------------------

TITLE: Graph Transformation Debugging
DESCRIPTION: Example of how to implement and debug graph transformations using FX.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
def transform_graph(module: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:
    g = tracer_class().trace(module)
    """
    Transformations on `g` go here
    """
    return fx.GraphModule(module, g)

transformed = transform_graph(traced)
print(transformed)
```

----------------------------------------

TITLE: Performing Matrix Multiplication with Sparse Tensor in PyTorch
DESCRIPTION: Demonstrates how to use a semi-structured sparse tensor in matrix multiplication operation, comparing results with dense tensor multiplication.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
a = torch.Tensor([0, 0, 1, 1]).tile((64, 16)).half().cuda()
b = torch.rand(64, 64).half().cuda()
c = torch.mm(a, b)
a_sparse = to_sparse_semi_structured(a)
torch.allclose(c, torch.mm(a_sparse, b))
```

----------------------------------------

TITLE: Run PyTorch Local Linting Checks (bash)
DESCRIPTION: Executes the `make lint` command, which runs the local linting checks using tools configured in the project's Makefile and `.lintrunnerrc`. This checks for code style, quality, and type errors before submitting changes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_18

LANGUAGE: bash
CODE:
```
make lint
```

----------------------------------------

TITLE: Implementing Conditional Compilation for CUDA/HIP in C++
DESCRIPTION: Shows correct C++ preprocessor logic (`#if`, `defined`, `&&`, `||`) to conditionally include code based on whether PyTorch is built with CUDA (`CUDA_VERSION`) or ROCm/HIP (`USE_ROCM`, `ROCM_VERSION`). Provides examples for excluding ROCm/HIP, including ROCm/HIP, and including for specific ROCm/HIP versions alongside CUDA checks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/hip.rst#_snippet_2

LANGUAGE: C++
CODE:
```
#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000 && !defined(USE_ROCM)
```

LANGUAGE: C++
CODE:
```
#if (defined(CUDA_VERSION) && CUDA_VERSION >= 11000) || defined(USE_ROCM)
```

LANGUAGE: C++
CODE:
```
#if (defined(CUDA_VERSION) && CUDA_VERSION >= 11000) || (defined(USE_ROCM) && ROCM_VERSION >= 40300)
```

----------------------------------------

TITLE: Defining a Nonlinear Linear Model Function - Python
DESCRIPTION: Defines the 'predict' function that computes a linear transformation (with weight and bias) followed by a tanh activation. Used as a test function for Jacobian and Hessian computations. Input parameters: weight (matrix), bias (vector), x (feature vector). Returns: activated linear output. Requires torch.nn.functional and input tensors of matching shapes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
def predict(weight, bias, x):
    return F.linear(x, weight, bias).tanh()
```

----------------------------------------

TITLE: Executing CUDNN RNN Backward Pass in PyTorch
DESCRIPTION: Performs a backward pass of a CUDNN RNN layer, computing gradients with respect to inputs and weights. Uses half-precision (f16) tensors and includes workspace and reserve space parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/tts_angular_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
aten._cudnn_rnn_backward.default(T([64, 50, 256], f16), [T([3072, 256], f16), T([3072, 768], f16), T([3072], f16), T([3072], f16)], 4, T([3151872], f16), T([1, 64, 768], f16), T([1, 64, 768], f16), T([64, 50, 768], f16, stride=(768, 49152, 1)), T([64, 50, 768], f16), None, None, 2, 768, 0, 1, True, 0.0, True, False, [], None, T([24576016], u8), [True, False, False, True])
```

----------------------------------------

TITLE: Demonstrating CUDAGraph Tensor Access Limitation in PyTorch
DESCRIPTION: Example showing how accessing tensor outputs from a previous CUDAGraph invocation can lead to runtime errors due to memory being overwritten by subsequent runs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_cudagraph_trees.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
import torch

@torch.compile(mode="reduce-overhead")
def my_model(x):
    y = torch.matmul(x, x)
    return y

x = torch.randn(10, 10, device="cuda")
y1 = my_model(x)
y2 = my_model(x)
print(y1)
# RuntimeError: Error: accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run.
```

----------------------------------------

TITLE: ONNX Graph Output for Inlined Function
DESCRIPTION: Shows the resulting ONNX graph representation after exporting the MyLogExp function. The exporter has inlined the function's operations (Exp, Log, Log) into the final ONNX graph, demonstrating the inlining behavior when no symbolic function is provided.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_24

LANGUAGE: ONNX Graph IR
CODE:
```
graph(%input : Float(1, strides=[1], requires_grad=0, device=cpu)):
    %1 : float = onnx::Exp[](%input)
    %2 : float = onnx::Log[](%1)
    %3 : float = onnx::Log[](%2)
    return (%3)
```

----------------------------------------

TITLE: Operators Incompatible with Deterministic Algorithms in PyTorch CUDAGraph
DESCRIPTION: PyTorch operators that become incompatible with CUDAGraph when deterministic algorithms are enabled.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_cudagraph_trees.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
aten._fused_moving_avg_obs_fq_helper.default
aten._fused_moving_avg_obs_fq_helper_functional.default
aten.multinomial.default
fbgemm.dense_to_jagged.default
fbgemm.jagged_to_padded_dense.default
run_and_save_rng_state
run_with_rng_state
aten._local_scalar_dense
aten._assert_scalar
```

----------------------------------------

TITLE: Enabling Fast Math for MPS Metal Kernels (Environment Variable)
DESCRIPTION: Set `PYTORCH_MPS_FAST_MATH` to `1` to enable fast math optimizations for MPS metal kernels. This may affect numerical precision; consult section 1.6.3 of the Metal Shading Language Specification for details.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mps_environment_variables.rst#2025-04-22_snippet_5

LANGUAGE: plaintext
CODE:
```
PYTORCH_MPS_FAST_MATH
```

----------------------------------------

TITLE: Incorrect Use of Global Variables with functorch
DESCRIPTION: This example demonstrates the incorrect way of using global variables with functorch transformations, which will not work properly because functorch requires that all outputs be returned from the function rather than assigned to global variables.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/ux_limitations.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch
from functorch import grad

# Don't do this
intermediate = None

def f(x):
  global intermediate
  intermediate = x.sin()
  z = intermediate.sin()
  return z

x = torch.randn([])
grad_x = grad(f)(x)
```

----------------------------------------

TITLE: Testing FX Symbolic Traceability for Structured Pruning
DESCRIPTION: Code snippet for verifying if a model is compatible with structured pruning by testing if it can be symbolically traced with torch.fx.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/pruner/README.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from torch.fx import symbolic_trace
model = MyModel()
symbolic_trace(model)
```

----------------------------------------

TITLE: Tensor Manipulation Operations
DESCRIPTION: Various tensor operations including slicing, splitting, and summation across different dimensions with specified strides and shapes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_26

LANGUAGE: python
CODE:
```
((T([2048, 8, 8], f16), [2048, 8, 15], 2, 7, 9223372036854775807, 1), {})
```

----------------------------------------

TITLE: Implementing Upper Triangular Matrix with First-Class Dimensions in Python
DESCRIPTION: This snippet demonstrates how to extract the upper triangular part of a matrix using first-class dimensions in PyTorch. It uses dimensions as loop indices to compute masks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
from torch import where
def triu(A):
    i,j = dims()
    a = A[i, j]
    return where(i <= j, a, 0).order(i, j)
triu(torch.rand(3, 4))
```

----------------------------------------

TITLE: Configuring CMake with PyTorch from conda or pip
DESCRIPTION: Command to configure CMake using the PyTorch installation path from conda or pip. This allows seamless integration with existing PyTorch installations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/installing.rst#2025-04-22_snippet_4

LANGUAGE: sh
CODE:
```
cmake -DCMAKE_PREFIX_PATH=`python3 -c 'import torch;print(torch.utils.cmake_prefix_path)'` ..
```

----------------------------------------

TITLE: Implementing PyTorch Add Operator Benchmark
DESCRIPTION: Example code showing how to support torch.add with different input configurations in the benchmark suite. It defines short configurations through cross-product of parameters and creates a benchmark class.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
add_short_configs = op_bench.cross_product_configs(
    M=[8, 64, 128],
    N=range(2, 10, 3),
    K=[2 ** x for x in range(0, 3)],
    tags=["short"]
)

class AddBenchmark(op_bench.TorchBenchmarkBase):
    def init(self, M, N, K, device):
        self.inputs = {
            "input_one": torch.rand(M, N, K, device=device, requires_grad=self.auto_set()),
            "input_two": torch.rand(M, N, K, device=device, requires_grad=self.auto_set())
        }
        self.set_module_name("add")

    def forward(self, input_one, input_two):
        return torch.add(input_one, input_two)

op_bench.generate_pt_test(add_short_configs, AddBenchmark)
```

----------------------------------------

TITLE: Defining Binary Bitwise Operations in TorchScript
DESCRIPTION: Specifies the syntax for bitwise AND (&), XOR (^), and OR (|) operations in TorchScript. These operate on int or Tensor arguments with specific rules for shape matching and broadcasting.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
and_expr ::=  shift_expr | and_expr '&' shift_expr
xor_expr ::=  and_expr | xor_expr '^' and_expr
or_expr  ::=  xor_expr | or_expr '|' xor_expr
```

----------------------------------------

TITLE: Handling Non-Valid Python Identifiers in Module Attributes
DESCRIPTION: This code demonstrates the technique used to serialize module attributes with names that aren't valid Python identifiers (like numeric indices). It uses direct assignments to the __annotations__ dictionary rather than standard attribute syntax.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/docs/serialization.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
class MyModule(Module):
    __annotations__ = []
    __annotations__["0"] = ASubmodule
    __annotations__["1"] = ASubmodule
```

----------------------------------------

TITLE: Sparse Tensor Operations in PyTorch
DESCRIPTION: Demonstrates operation support for sparse tensors, showing both supported (sin) and unsupported (cos) operations on CSR format.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
b = torch.tensor([[0, 0, 1, 2, 3, 0], [4, 5, 0, 6, 0, 0]])
b_s = b.to_sparse_csr()
b_s.cos()
b_s.sin()
```

----------------------------------------

TITLE: CUDA Device Count Query in C++
DESCRIPTION: This function returns the number of available CUDA devices. It's crucial for managing multi-GPU setups in PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_4

LANGUAGE: C++
CODE:
```
_ZN3c104cuda12device_countEv
```

----------------------------------------

TITLE: Demonstrating Lowered PyTorch GraphModule with QuantizedLinearReLU
DESCRIPTION: This snippet presents a PyTorch `GraphModule` representing a quantized model after the lowering pass. It shows a `QuantizedLinearReLU` submodule and the `forward` method, which explicitly performs `quantize_per_tensor`, calls the `QuantizedLinearReLU` module, and then `dequantize`. This illustrates the output of the lowering process using native quantized operators.
Dependencies: Requires PyTorch with quantization support.
Inputs: `x` (an input tensor).
Outputs: The resulting tensor after quantized linear and dequantization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#_snippet_10

LANGUAGE: Python
CODE:
```
GraphModule(
  (linear): QuantizedLinearReLU(in_features=5, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)
)

def forward(self, x):
    linear_input_scale_0 = self.linear_input_scale_0
    linear_input_zero_point_0 = self.linear_input_zero_point_0
    quantize_per_tensor = torch.quantize_per_tensor(x, linear_input_scale_0, linear_input_zero_point_0, torch.quint8);  x = linear_input_scale_0 = linear_input_zero_point_0 = None
    linear = self.linear(quantize_per_tensor);  quantize_per_tensor = None
    dequantize_1 = linear.dequantize();  linear = None
    return dequantize_1
```

----------------------------------------

TITLE: PyTorch Batch Normalization Operations
DESCRIPTION: Batch normalization operations on tensors of different sizes with training mode enabled, momentum 0.1 and epsilon 1e-05.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_xception65_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
((T([32, 32, 150, 150], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})
```

----------------------------------------

TITLE: Performing Log Softmax in PyTorch (Python)
DESCRIPTION: Executes the aten._log_softmax operation, which applies the logarithm of the softmax function on a tensor. This is often used in the final layer of a neural network for classification purposes. The operation includes specifying the input tensor shape and dimension to apply the function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
aten._log_softmax.default
cnt: 1, ((T([16, 2], f16), 1, False), {})
```

----------------------------------------

TITLE: Managing CUDA Streams on Multiple Devices (Skeleton Example)
DESCRIPTION: This skeleton example demonstrates various approaches to handle CUDA streams on multiple devices, including stream acquisition, setting current streams, and using device guards.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_6

LANGUAGE: cpp
CODE:
```
// Usage 0: acquire CUDA stream and set current CUDA stream with `setCurrentCUDAStream`
// Create a CUDA stream vector `streams0` on device 0
std::vector<at::cuda::CUDAStream> streams0 =
  {at::cuda::getDefaultCUDAStream(), at::cuda::getStreamFromPool()};
// set current stream as `streams0[0]` on device 0
at::cuda::setCurrentCUDAStream(streams0[0]);

// create a CUDA stream vector `streams1` on device using CUDA device guard
std::vector<at::cuda::CUDAStream> streams1;
{
  // device index is set to 1 within this scope
  at::cuda::CUDAGuard device_guard(1);
  streams1.push_back(at::cuda::getDefaultCUDAStream());
  streams1.push_back(at::cuda::getStreamFromPool());
}
// device index is reset to 0 after device_guard is destroyed

// set current stream as `streams1[0]` on device 1
```

----------------------------------------

TITLE: BLAS and LAPACK Operations List in RestructuredText
DESCRIPTION: A list of linear algebra operations in PyTorch implemented using BLAS and LAPACK libraries, including matrix multiplication, decomposition, and solving operations, formatted as a RestructuredText autosummary.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.rst#2025-04-22_snippet_4

LANGUAGE: restructuredtext
CODE:
```
.. autosummary::
    :toctree: generated
    :nosignatures:

    addbmm
    addmm
    addmv
    addr
    baddbmm
    bmm
    chain_matmul
    cholesky
    cholesky_inverse
    cholesky_solve
    dot
    geqrf
    ger
    inner
    inverse
    det
    logdet
    slogdet
    lu
    lu_solve
    lu_unpack
    matmul
    matrix_power
    matrix_exp
    mm
    mv
    orgqr
    ormqr
    outer
    pinverse
    qr
    svd
    svd_lowrank
    pca_lowrank
    lobpcg
    trapz
    trapezoid
    cumulative_trapezoid
    triangular_solve
    vdot
```

----------------------------------------

TITLE: Convolution Operations in PyTorch with Half-Precision Tensors
DESCRIPTION: Multiple convolution operations with different input/output shapes, kernel sizes, and strides. Operations use half-precision (f16) tensors and include various padding and dilation settings.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/inception_v3_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
((T([128, 192, 17, 17], f16), T([128, 192, 17, 17], f16), T([192, 192, 7, 1], f16), [0], [1, 1], [3, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

----------------------------------------

TITLE: PyTorch Softmax Operations
DESCRIPTION: Records usage of softmax and log_softmax operators with various tensor shapes and dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/twins_pcpvt_base_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
aten._log_softmax.default((T([32, 1000], f16), 1, False))
aten._log_softmax_backward_data.default((T([32, 1000], f16), T([32, 1000], f16), 1, f16))
aten._softmax.default((T([32, 1, 3136, 49], f16), -1, False))
aten._softmax.default((T([32, 2, 784, 49], f16), -1, False))
aten._softmax.default((T([32, 5, 196, 49], f16), -1, False))
```

----------------------------------------

TITLE: Tanh Activation Function in PyTorch (Python)
DESCRIPTION: Parameters for the Forward computation of the hyperbolic tangent function using aten.tanh, a traditional activation function in neural networks, especially recurrent networks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_27

LANGUAGE: Python
CODE:
```
aten.tanh.default
cnt: 1, ((T([16, 768], f16),), {})
```

----------------------------------------

TITLE: Defining Headers for Installation Based on Build Configuration in CMake
DESCRIPTION: Initializes the `INSTALL_HEADERS` list with core ATen headers. It then conditionally appends backend-specific headers (native, CPU, sparse, quantized, CUDA, HIP, XPU, MPS, KleidiAI, MiOPEN, MKL-DNN/oneDNN) based on the `INTERN_BUILD_MOBILE` flag. Further conditional logic within the mobile/non-mobile branches handles specific Metal header installation based on flags like `USE_PYTORCH_METAL_EXPORT`, `USE_PYTORCH_METAL`, `APPLE`, and `IOS`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_25

LANGUAGE: cmake
CODE:
```
set(INSTALL_HEADERS ${base_h} ${ATen_CORE_HEADERS} ${native_nested_h} ${ATen_TRANSFORMER_HEADERS})
if(NOT INTERN_BUILD_MOBILE)
  list(APPEND INSTALL_HEADERS ${native_h} ${native_cpu_h} ${native_ao_sparse_h} ${native_quantized_h} ${cuda_h} ${native_cuda_h} ${native_hip_h} ${cudnn_h} ${hip_h} ${xpu_h} ${mps_h} ${native_kleidiai_h} ${native_mps_h} ${native_utils_h} ${miopen_h} ${mkldnn_xpu_h})
  # Metal
  if(USE_PYTORCH_METAL_EXPORT)
    # Add files needed from exporting metal models(optimized_for_mobile)
    list(APPEND INSTALL_HEADERS ${metal_h} ${metal_prepack_h})
  elseif(APPLE AND USE_PYTORCH_METAL)
    # Needed by Metal kernels
    list(APPEND INSTALL_HEADERS ${metal_h} ${native_metal_h})
  else()
    list(APPEND INSTALL_HEADERS ${metal_h})
  endif()
else()
  if(IOS AND USE_PYTORCH_METAL)
      list(APPEND INSTALL_HEADERS ${metal_h} ${native_metal_h})
  else()
      list(APPEND INSTALL_HEADERS ${metal_h} ${metal_prepack_h})
  endif()
endif()
```

----------------------------------------

TITLE: Tensor Addition and Matrix Multiplication Operations in PyTorch Model
DESCRIPTION: Code showing tensor addition, matrix multiplication and other basic operations used throughout the model. These operations handle residual connections, multi-headed attention mechanisms, and linear projections across different feature resolutions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/visformer_small_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
Operator: aten.add.Tensor
cnt: 1, ((T([128, 192, 28, 28], f16), T([1, 192, 28, 28], f16)), {})
cnt: 14, ((T([128, 192, 28, 28], f16), T([128, 192, 28, 28], f16)), {})
cnt: 1, ((T([128, 384, 14, 14], f16), T([1, 384, 14, 14], f16)), {})
cnt: 16, ((T([128, 384, 14, 14], f16), T([128, 384, 14, 14], f16)), {})
cnt: 1, ((T([128, 768, 7, 7], f16), T([1, 768, 7, 7], f16)), {})
cnt: 16, ((T([128, 768, 7, 7], f16), T([128, 768, 7, 7], f16)), {})
Operator: aten.add_.Tensor
cnt: 28, ((T([], i64), 1), {})
Operator: aten.addmm.default
cnt: 1, ((T([1000], f16), T([128, 768], f16), T([768, 1000], f16, stride=(1, 768))), {})
Operator: aten.bmm.default
cnt: 4, ((T([768, 196, 64], f16), T([768, 64, 196], f16)), {})
cnt: 4, ((T([768, 196, 196], f16), T([768, 196, 64], f16)), {})
cnt: 4, ((T([768, 49, 128], f16), T([768, 128, 49], f16)), {})
cnt: 4, ((T([768, 49, 49], f16), T([768, 49, 128], f16)), {})
cnt: 4, ((T([768, 49, 49], f16, stride=(2401, 1, 49)), T([768, 49, 128], f16, stride=(6272, 1, 49))), {})
cnt: 4, ((T([768, 49, 128], f16, stride=(6272, 1, 49)), T([768, 128, 49], f16, stride=(6272, 1, 128))), {})
cnt: 4, ((T([768, 128, 49], f16, stride=(6272, 1, 128)), T([768, 49, 49], f16)), {})
cnt: 4, ((T([768, 49, 49], f16), T([768, 49, 128], f16, stride=(6272, 1, 49))), {})
cnt: 4, ((T([768, 196, 196], f16, stride=(38416, 1, 196)), T([768, 196, 64], f16, stride=(12544, 1, 196))), {})
cnt: 4, ((T([768, 196, 64], f16, stride=(12544, 1, 196)), T([768, 64, 196], f16, stride=(12544, 1, 64))), {})
cnt: 4, ((T([768, 64, 196], f16, stride=(12544, 1, 64)), T([768, 196, 196], f16)), {})
cnt: 4, ((T([768, 196, 196], f16), T([768, 196, 64], f16, stride=(12544, 1, 196))), {})
```

----------------------------------------

TITLE: Layer Normalization with native_layer_norm in PyTorch (Python)
DESCRIPTION: Performs aten.native_layer_norm for normalizing activations across feature dimensions, a common normalization method in neural networks to stabilize learning and improve convergence.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_18

LANGUAGE: Python
CODE:
```
aten.native_layer_norm.default
cnt: 25, ((T([16, 512, 768], f16), [768], T([768], f16), T([768], f16), 1e-12), {})
```

----------------------------------------

TITLE: Implementing Embedding Bag with First-Class Dimensions in Python
DESCRIPTION: This snippet shows how to implement an embedding bag operation using first-class dimensions in PyTorch. It performs an embedding table lookup followed by a sum.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
def embedding_bag(input, embedding_weights):
    batch, sequence, features = dims(3)
    r = embedding_weights[input[batch, sequence], features].sum(sequence)
    return r.order(batch, features)

input = torch.tensor([[1, 0, 4, 3]])
W = torch.rand(5,2)
embedding_bag(input, W)
```

----------------------------------------

TITLE: Running Pre-built PyTorch Docker Image Bash
DESCRIPTION: Executes a Docker command to pull and run the latest pre-built PyTorch image from Docker Hub. Includes flags to enable GPU access (--gpus all) and ensure sufficient shared memory (--ipc=host) for multiprocessing.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_19

LANGUAGE: Bash
CODE:
```
docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest
```

----------------------------------------

TITLE: Applying In-Place ReLU Activation with PyTorch
DESCRIPTION: Applies the ReLU activation function in-place on the input tensor, converting all negative values to zero, crucial in deep learning. Requires input tensors of shapes like [16, 64, 128, 128]. Outputs the same modified tensor, highlighting efficiency in memory usage by avoiding tensor duplication.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_12

LANGUAGE: Python
CODE:
```
Operator: aten.relu_.default
cnt: 2, ((T([16, 64, 128, 128], f16),), {})
cnt: 2, ((T([16, 128, 64, 64], f16),), {})
cnt: 7, ((T([16, 256, 32, 32], f16),), {})
```

----------------------------------------

TITLE: Importing and Type Checking Classes in PyTorch Package
DESCRIPTION: Demonstrates how importing a class through the package importer creates a distinct type from the original class, causing isinstance() checks to fail between the two versions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_21

LANGUAGE: python
CODE:
```
imported_MyClass = importer.import_module("foo").MyClass

assert isinstance(my_class_instance, MyClass)  # works
assert isinstance(my_class_instance, imported_MyClass)  # ERROR!
```

----------------------------------------

TITLE: Tensor Reduction Input Tuples for aten.sum.SymInt - Python
DESCRIPTION: These tuples configure inputs for the PyTorch 'aten.sum.SymInt' operator, defining tensors, reduction dimensions (axes), and a keepdims option (boolean). Used to test summing across specified axes for f16 tensors, constraining input shapes and reduction dimensions as per the PyTorch API. Overly large reductions may challenge device memory capacity.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([128, 1000], f16), [0], True), {})
```

LANGUAGE: python
CODE:
```
cnt: 2, ((T([128, 960, 7, 7], f16), [2, 3], True), {})
```

----------------------------------------

TITLE: Building PyTorch for iOS
DESCRIPTION: Basic command to build PyTorch libraries for iOS platform from the PyTorch root directory.
SOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/README.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
#in your PyTorch root directory
bash scripts/build_ios.sh
```

----------------------------------------

TITLE: Configuring PyTorch Android Dependencies in Gradle (Release)
DESCRIPTION: This snippet shows how to configure Gradle dependencies for PyTorch Android, including both lite interpreter and full JIT builds. It specifies the repository and implementation details for release versions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/android/README.md#2025-04-22_snippet_0

LANGUAGE: Groovy
CODE:
```
repositories {
    jcenter()
}

# lite interpreter build
dependencies {
    implementation 'org.pytorch:pytorch_android_lite:1.10.0'
    implementation 'org.pytorch:pytorch_android_torchvision_lite:1.10.0'
}

# full jit build
dependencies {
    implementation 'org.pytorch:pytorch_android:1.10.0'
    implementation 'org.pytorch:pytorch_android_torchvision:1.10.0'
}
```

----------------------------------------

TITLE: Batch Normalization Operations in DenseNet Forward Pass
DESCRIPTION: This snippet shows the batch normalization operations used throughout DenseNet. It normalizes activations using running statistics with a momentum of 0.1 and epsilon of 1e-05, applied at different network depths with corresponding channel dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
Operator: aten.native_batch_norm.default
cnt: 2, ((T([32, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})
cnt: 6, ((T([32, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([32, 256, 56, 56], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), False, 0.1, 1e-05), {})
cnt: 5, ((T([32, 160, 28, 28], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([32, 512, 28, 28], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), False, 0.1, 1e-05), {})
cnt: 10, ((T([32, 192, 14, 14], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), False, 0.1, 1e-05), {})
cnt: 2, ((T([32, 768, 14, 14], f16), T([768], f16), T([768], f16), T([768], f16), T([768], f16), False, 0.1, 1e-05), {})
cnt: 10, ((T([32, 224, 7, 7], f16), T([224], f16), T([224], f16), T([224], f16), T([224], f16), False, 0.1, 1e-05), {})
cnt: 2, ((T([32, 1024, 7, 7], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f16), False, 0.1, 1e-05), {})
```

----------------------------------------

TITLE: Analyzing PyTorch Convolution Forward Operations
DESCRIPTION: A collection of convolution operations with varied input tensor shapes, kernel sizes, and stride parameters. The operations use half-precision (f16) format tensors with batch size 4 and different spatial dimensions (28x28, 14x14, 7x7).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/densenet121_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
cnt: 1, ((T([4, 480, 28, 28], f16), T([128, 480, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

----------------------------------------

TITLE: Demultiplexing DataPipes in PyTorch
DESCRIPTION: Illustrates the use of the demux() method to split a DataPipe into multiple DataPipes based on a routing function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
dp = ExampleIterPipe(10)
dp1, dp2, dp3 = dp.demux(3, lambda x: x % 3)
for i in dp2:
    print(i)
```

----------------------------------------

TITLE: Applying _log_softmax Function in PyTorch
DESCRIPTION: The _log_softmax operator is called to compute the logarithm of softmax values along a specified dimension. Dependencies include PyTorch, and inputs are tensors with specified shapes and types. Output is a tensor with the same shape transformed by the log softmax operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForConditionalGeneration_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([1024, 50005], f16), 1, False), {})
```

----------------------------------------

TITLE: Analyzing Batch Normalization Operations in PyTorch
DESCRIPTION: This snippet shows the usage patterns of batch normalization operations with different tensor shapes and configurations. It includes counts for each unique configuration and details on input tensors, running statistics, and hyperparameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_14

LANGUAGE: Python
CODE:
```
cnt: 2, ((T([32, 1024, 7, 7], f16), T([32, 1024, 7, 7], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), False, 1e-05, [True, True, True]), {})
cnt: 10, ((T([32, 224, 7, 7], f16), T([32, 224, 7, 7], f16), T([224], f16), T([224], f16), T([224], f16), T([224], f32), T([224], f32), False, 1e-05, [True, True, True]), {})
cnt: 2, ((T([32, 768, 14, 14], f16), T([32, 768, 14, 14], f16), T([768], f16), T([768], f16), T([768], f16), T([768], f32), T([768], f32), False, 1e-05, [True, True, True]), {})
cnt: 10, ((T([32, 192, 14, 14], f16), T([32, 192, 14, 14], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([32, 512, 28, 28], f16), T([32, 512, 28, 28], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), False, 1e-05, [True, True, True]), {})
cnt: 5, ((T([32, 160, 28, 28], f16), T([32, 160, 28, 28], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f32), T([160], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([32, 256, 56, 56], f16), T([32, 256, 56, 56], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), False, 1e-05, [True, True, True]), {})
cnt: 6, ((T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), False, 1e-05, [True, True, True]), {})
cnt: 2, ((T([32, 64, 112, 112], f16), T([32, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})
```

----------------------------------------

TITLE: Implement Symbolic Function for ATen ELU (Python)
DESCRIPTION: Provides the Python implementation of a symbolic function for the `aten::elu` operator. It takes the graph `g` and input values, and adds an ONNX `Elu` operator node to the graph using `g.op()`, returning the output value of the ONNX node.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_18

LANGUAGE: python
CODE:
```
def elu(g, input: torch.Value, alpha: torch.Value, inplace: bool = False):
    return g.op("Elu", input, alpha_f=alpha)
```

----------------------------------------

TITLE: Zip Examples in TorchScript
DESCRIPTION: Demonstrates valid and invalid uses of zip() function in TorchScript with different container types.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_32

LANGUAGE: python
CODE:
```
a = [1, 2] # List
b = [2, 3, 4] # List
zip(a, b) # works
```

LANGUAGE: python
CODE:
```
a = (1, 2) # Tuple
b = [2, 3, 4] # List
zip(a, b) # Runtime error
```

LANGUAGE: python
CODE:
```
a = [1.3, 2.4]
b = [2, 3, 4]
zip(a, b) # Works
```

----------------------------------------

TITLE: Enabling cuDNN Convolution Benchmarking in Python
DESCRIPTION: Enables the cuDNN benchmarking feature by setting `torch.backends.cudnn.benchmark = True`. This allows cuDNN to test multiple convolution algorithms and select the fastest one for the given input sizes, potentially improving performance for a single run but introducing nondeterminism across different runs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/randomness.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
torch.backends.cudnn.benchmark = True
```

----------------------------------------

TITLE: PyTorch Tensor Operations Count Analysis
DESCRIPTION: Logs of tensor operations showing count (cnt), tensor shapes, and data types for various PyTorch operators including hardswish_backward, index operations, batch normalization, and matrix multiplications. Each entry shows the operator name, count of calls, input tensor specifications and any additional parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/levit_128_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
cnt: 4, ((T([128, 196, 128], f16),), {})
cnt: 4, ((T([128, 196, 256], f16),), {})
cnt: 6, ((T([128, 49, 512], f16),), {})
cnt: 4, ((T([128, 49, 256], f16),), {})
cnt: 1, ((T([128, 16, 1024], f16),), {})
cnt: 5, ((T([128, 16, 768], f16),), {})
cnt: 4, ((T([128, 16, 384], f16),), {})
```

----------------------------------------

TITLE: Running Max Autotune
DESCRIPTION: Command to run kernel benchmarking with maximum autotuning enabled.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_inductor_profiling.rst#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
TORCHINDUCTOR_MAX_AUTOTUNE=1 python /tmp/k.py
```

----------------------------------------

TITLE: Type Mismatch Error Illustration in TorchScript - PyTorch - Python
DESCRIPTION: This code illustrates a runtime type mismatch error in TorchScript when a variable is assigned different types in different branches of an if-statement. The snippet, using the @torch.jit.script decorator, defines a function that returns a torch.Tensor or an int depending on a boolean. Since TorchScript enforces static typing, it triggers a compilation error. PyTorch is required as a dependency and the code is used to show TorchScript limitations on variable assignment types.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch

@torch.jit.script
def an_error(x):
    if x:
        r = torch.rand(1)
    else:
        r = 4
    return r
```

----------------------------------------

TITLE: Manual Synchronization for Shared CUDA Memory in Python
DESCRIPTION: Demonstrates a manual approach to synchronize shared CUDA memory between producer and consumer processes using a queue and an event. This method requires blocking the producer process and can become complicated with multiple consumers.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/multiprocessing/cuda_multiprocessing.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
# Producer
queue.put(tensor)
event.wait()

# Consumer
tensor = queue.get()
safe_to_use_tensor = tensor.clone()
event.set()
```

----------------------------------------

TITLE: Module Parameter Declaration in Serialized Python Code
DESCRIPTION: This example shows how TorchScript represents module parameters in the serialized Python code. It uses a special __parameters__ list to track which attributes are parameters, along with type annotations for all attributes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/docs/serialization.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
class MyModule(Module):
    __parameters__ = ["foo", "bar", ]
    foo : Tensor
    bar : Tensor
    attribute_but_not_param : Tensor
```

----------------------------------------

TITLE: Implementing TorchScript Enums with the Enum Class
DESCRIPTION: Example showing how to define and use Python enums in TorchScript. Enums can be used without additional annotations, but all values must be of the same type (int, float, or str).
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
from enum import Enum


class Color(Enum):
    RED = 1
    GREEN = 2

@torch.jit.script
def enum_fn(x: Color, y: Color) -> bool:
    if x == Color.RED:
        return True

    return x == y
```

----------------------------------------

TITLE: Enabling Input Mutation Support with CUDAGraph in PyTorch
DESCRIPTION: Configuration setting to enable input mutation support in CUDAGraph when using the "reduce-overhead" mode.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_cudagraph_trees.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
torch._inductor.config.cudagraph_support_input_mutation = True
```

----------------------------------------

TITLE: Defining Conditional Expressions in TorchScript
DESCRIPTION: Specifies the syntax for conditional (ternary) expressions in TorchScript. These allow for inline if-else logic within expressions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_23

LANGUAGE: python
CODE:
```
conditional_expression ::=  or_expr ['if' or_test 'else' conditional_expression]
expression            ::=  conditional_expression
```

----------------------------------------

TITLE: Implementing Linear Function Aliases in PyTorch
DESCRIPTION: Shows two ways to make custom operations easier to use - either through direct aliasing or wrapping in a function with default arguments.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
# Option 1: alias
linear = LinearFunction.apply

# Option 2: wrap in a function, to support default args and keyword args.
def linear(input, weight, bias=None):
    return LinearFunction.apply(input, weight, bias)
```

----------------------------------------

TITLE: Invoking Loss Functions with aten.nll_loss_forward and aten.nll_loss_backward - PyTorch - Python
DESCRIPTION: These snippets describe logging of forward and backward passes for negative log-likelihood loss computation, using inputs such as prediction tensor ([4, 2], f16) and target indexes ([4], i64), ignoring class -100. Outputs: loss value and gradients. The backward variant operates similarly with additional input tensors for autograd. Requires: torch, correct input shape matching.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GPT2ForSequenceClassification_training.txt#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
Operator: aten.nll_loss_backward.default
cnt: 1, ((T([], f16), T([4, 2], f16), T([4], i64), None, 1, -100, T([], f16)), {})
```

LANGUAGE: Python
CODE:
```
Operator: aten.nll_loss_forward.default
cnt: 1, ((T([4, 2], f16), T([4], i64), None, 1, -100), {})
```

----------------------------------------

TITLE: Dynamic Control Flow Example in PyTorch FX
DESCRIPTION: Demonstrates a failed symbolic tracing attempt due to dynamic control flow where the condition depends on input values.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
def func_to_trace(x):
    if x.sum() > 0:
        return torch.relu(x)
    else:
        return torch.neg(x)

traced = torch.fx.symbolic_trace(func_to_trace)
```

----------------------------------------

TITLE: Computing Pooling Gradients with ATen AvgPool2DBackward Operator
DESCRIPTION: The `aten.avg_pool2d_backward` calculates gradients for the average pooling operation, allowing backward pass through the pooling layer in a neural network. Involves input activations and gradient with respect to output.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_8

LANGUAGE: plaintext
CODE:
```
Operator: aten.avg_pool2d_backward.default
cnt: 1, ((T([64, 512, 8, 8], f16), T([64, 512, 16, 16], f16), [2, 2], [2, 2], [0, 0], False, True, None), {})
```

----------------------------------------

TITLE: Accessing Named Tensor Properties in PyTorch
DESCRIPTION: This snippet demonstrates how to access the names and shape properties of a named tensor in PyTorch. The example uses a flattened images tensor that has named dimensions 'N' for batch and 'features' for the flattened feature vector, with a shape of 32 samples and 49152 features.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/named_tensor.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
>>> flat_imgs.names, flat_imgs.shape
(('N', 'features'), torch.Size([32, 49152]))
```

----------------------------------------

TITLE: Matrix Multiplication Operations in DenseNet Classification Layer
DESCRIPTION: This snippet shows matrix multiplication operations used in DenseNet's final classification layer. It performs multiplication between the pooled features and the weight matrix, both in forward and backward passes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
Operator: aten.mm.default
cnt: 1, ((T([32, 1000], f16, stride=(0, 0)), T([1000, 1024], f16)), {})
cnt: 1, ((T([1000, 32], f16, stride=(0, 0)), T([32, 1024], f16)), {})
```

----------------------------------------

TITLE: Filling Tensor Elements by Mask with Scalar (In-place) - PyTorch Aten
DESCRIPTION: Fills elements of a tensor with a scalar value at locations specified by a boolean mask, in-place. This internal operator is useful for setting specific values conditionally. It takes the tensor to modify, a boolean mask, and the scalar value.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BartForConditionalGeneration_training.txt#_snippet_15

LANGUAGE: Python
CODE:
```
import torch

input_tensor = torch.zeros(2, 1024, dtype=torch.int64)
mask = torch.randint(0, 2, (2, 1024), dtype=torch.bool)
input_tensor.masked_fill_(mask, 1)
```

----------------------------------------

TITLE: Using TorchScript Classes with Function Definitions
DESCRIPTION: Demonstrates how to define and use a TorchScript class with the torch.jit.script decorator. The example creates a Pair class with two tensor members and a function to sum them.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
# Declare a TorchScript class
@torch.jit.script
class Pair:
  def __init__(self, first, second):
    self.first = first
    self.second = second

@torch.jit.script
def sum_pair(p):
  # type: (Pair) -> Tensor
  return p.first + p.second

p = Pair(torch.rand(2, 3), torch.rand(2, 3))
print(sum_pair(p))
```

----------------------------------------

TITLE: Layer Normalization Operations
DESCRIPTION: Native layer normalization operations on tensors of various shapes, including both forward and backward passes with epsilon value of 1e-05
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/swin_base_patch4_window7_224_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
cnt: 1, ((T([64, 3136, 128], f16, stride=(401408, 1, 3136)), [128], T([128], f16), T([128], f16), 1e-05), {})
cnt: 4, ((T([64, 3136, 128], f16), [128], T([128], f16), T([128], f16), 1e-05), {})
```

----------------------------------------

TITLE: Copying Tensors in PyTorch
DESCRIPTION: This snippet covers 'aten.copy_.default', which is used to perform in-place copying from a source tensor to a destination tensor in PyTorch. Useful for ensuring data consistency or setting initial conditions, it requires tensors of matching dimensions and half-precision compatibility.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v2_training.txt#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
Operator: aten.copy_.default
cnt: 1, ((T([96, 3, 224, 224], f16), T([96, 3, 224, 224], f16)), {})
```

----------------------------------------

TITLE: Analyzing ATen Unsafe View Operations in PyTorch
DESCRIPTION: Logs occurrences of \"aten._unsafe_view.default\" which allows reshaping tensors without copying data, detailing the specifics of tensor shapes before and after reshaping.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_5

LANGUAGE: pseudocode
CODE:
```
Operator: aten._unsafe_view.default
cnt: 36, ((T([8, 128, 16, 64], f16), [8, 128, 1024]), {})
cnt: 1, ((T([1024, 50265], f16), [8, 128, 50265]), {})
cnt: 12, ((T([8, 16, 128, 64], f16), [128, 128, 64]), {})
cnt: 12, ((T([8, 128, 1024], f16), [1024, 1024]), {})
```

----------------------------------------

TITLE: Log Entry for Tensor Pair (Shape [128, 64, 112, 112], f16)
DESCRIPTION: Logs the occurrence (count 1) of a tensor pair, both with shape [128, 64, 112, 112] and dtype f16, using default strides. Likely generated during PyTorch execution.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2next50_training.txt#2025-04-22_snippet_13

LANGUAGE: text
CODE:
```
cnt: 1, ((T([128, 64, 112, 112], f16), T([128, 64, 112, 112], f16), 0), {})
```

----------------------------------------

TITLE: Softmax Operation in PyTorch
DESCRIPTION: The data on aten._softmax.default operator shows its usage of handling a large tensor with shape [1, 24, 512, 512] in float16, computing softmax along the last dimension. Useful for obtaining normalized probability distributions across a dataset.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
Operator: aten._softmax.default
cnt: 24, ((T([1, 24, 512, 512], f16), -1, False), {})
```

----------------------------------------

TITLE: Tensor Memory Operations
DESCRIPTION: Memory allocation operations for tensors including empty and zero initialization with specific strides and shapes. Uses CUDA device with half-precision (f16) format.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convnext_base_training.txt#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
cnt: 1, ((T([1024, 512, 2, 2], f16, stride=(2048, 1, 1024, 512)), [1024, 512, 2, 2], [2048, 4, 2, 1]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})
cnt: 1, ((T([32, 1024], f16), [32768]), {})
```

----------------------------------------

TITLE: TorchDynamo-based ONNX Export Memory Analysis
DESCRIPTION: Script demonstrating memory usage tracking for TorchDynamo-based ONNX export of a HighResNet model, utilizing FakeTensorMode for reduced memory consumption.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_dynamo_memory_usage.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
import torch

from monai.networks.nets import (
    HighResNet,
)

torch.cuda.memory._record_memory_history()

model = HighResNet(
    spatial_dims=3, in_channels=1, out_channels=3, norm_type="batch"
).eval()

model = model.to("cuda")
data = torch.randn(30, 1, 48, 48, 48, dtype=torch.float32).to("cuda")

with torch.no_grad():
    onnx_program = torch.onnx.export(
                        model,
                        data,
                        "test_faketensor.onnx",
                        dynamo=True,
                    )

snapshot_name = f"torchdynamo_exporter_example.pickle"
print(f"generate {snapshot_name}")

torch.cuda.memory._dump_snapshot(snapshot_name)
print(f"Export is done.")
```

----------------------------------------

TITLE: Convolution Operations with Half Precision
DESCRIPTION: Multiple convolution operations with half-precision (fp16) tensors of various dimensions, including 1x1, 3x3, 5x5, and 7x1 convolutions with different stride and padding configurations
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_inception_v3_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
((T([128, 192, 17, 17], f16), T([128, 192, 17, 17], f16), T([192, 192, 7, 1], f16), [0], [1, 1], [3, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

----------------------------------------

TITLE: Composing Modules using nn.Sequential
DESCRIPTION: Illustrates composing modules using `torch.nn.Sequential`. It creates a simple network by chaining two instances of the custom `MyLinear` module with a `torch.nn.ReLU` activation function in between. Input data flows sequentially through the defined layers.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
net = nn.Sequential(
  MyLinear(4, 3),
  nn.ReLU(),
  MyLinear(3, 1)
)

sample_input = torch.randn(4)
net(sample_input)
: tensor([-0.6749], grad_fn=<AddBackward0>)
```

----------------------------------------

TITLE: Running setup.py with Environment Variables (Bash)
DESCRIPTION: This bash snippet shows how to pass environment variables to the `python setup.py develop` command. Multiple variables can be set using the `ENV_KEY=ENV_VAL` syntax before the command. This is useful for debugging or customizing the build process.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_4

LANGUAGE: bash
CODE:
```
ENV_KEY1=ENV_VAL1[, ENV_KEY2=ENV_VAL2]* python setup.py develop
```

----------------------------------------

TITLE: Sparse COO Tensor Operations in PyTorch
DESCRIPTION: Series of sparse COO tensor operations with shape [965, 192] using half-precision (f16) format on CUDA device. Each operation contains counter (cnt) indicating number of occurrences.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
cnt: 2, ((1, 1, [965, 192], T([1, 54827], i64), T([54827, 192], f16)), {'dtype': f16, 'layout': torch.sparse_coo, 'device': 'cuda', 'pin_memory': None})
```

----------------------------------------

TITLE: Tensor Addition Operations in PyTorch ResNet Model
DESCRIPTION: This snippet shows various tensor addition operations used in a ResNet model, including skip connections. Operations mainly work with half-precision (f16) tensors of different sizes corresponding to different stages of the network.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
Operator: aten.add.Tensor
cnt: 7, ((T([1, 64, 1, 1], f16), 0.0), {})
cnt: 1, ((T([4, 64, 592, 608], f16), T([1, 64, 1, 1], f16)), {})
cnt: 6, ((T([4, 64, 296, 304], f16), T([1, 64, 1, 1], f16)), {})
cnt: 16, ((T([1, 256, 1, 1], f16), 0.0), {})
cnt: 4, ((T([4, 256, 296, 304], f16), T([1, 256, 1, 1], f16)), {})
cnt: 8, ((T([1, 128, 1, 1], f16), 0.0), {})
cnt: 1, ((T([4, 128, 296, 304], f16), T([1, 128, 1, 1], f16)), {})
cnt: 7, ((T([4, 128, 148, 152], f16), T([1, 128, 1, 1], f16)), {})
cnt: 11, ((T([1, 512, 1, 1], f16), 0.0), {})
cnt: 5, ((T([4, 512, 148, 152], f16), T([1, 512, 1, 1], f16)), {})
cnt: 1, ((T([4, 256, 148, 152], f16), T([1, 256, 1, 1], f16)), {})
cnt: 11, ((T([4, 256, 74, 76], f16), T([1, 256, 1, 1], f16)), {})
cnt: 7, ((T([1, 1024, 1, 1], f16), 0.0), {})
cnt: 7, ((T([4, 1024, 74, 76], f16), T([1, 1024, 1, 1], f16)), {})
cnt: 1, ((T([4, 512, 74, 76], f16), T([1, 512, 1, 1], f16)), {})
cnt: 5, ((T([4, 512, 37, 38], f16), T([1, 512, 1, 1], f16)), {})
cnt: 4, ((T([1, 2048, 1, 1], f16), 0.0), {})
cnt: 4, ((T([4, 2048, 37, 38], f16), T([1, 2048, 1, 1], f16)), {})
cnt: 2, ((T([4, 256, 74, 76], f16), T([4, 256, 74, 76], f16)), {})
cnt: 2, ((T([4, 256, 148, 152], f16), T([4, 256, 148, 152], f16)), {})
cnt: 1, ((T([4, 256, 296, 304], f16), T([4, 256, 296, 304], f16)), {})
cnt: 1, ((T([89984, 1, 4], i32), T([1, 3, 4], f16)), {})
cnt: 1, ((T([22496, 1, 4], i32), T([1, 3, 4], f16)), {})
cnt: 1, ((T([5624, 1, 4], i32), T([1, 3, 4], f16)), {})
cnt: 1, ((T([1406, 1, 4], i32), T([1, 3, 4], f16)), {})
cnt: 1, ((T([361, 1, 4], i32), T([1, 3, 4], f16)), {})
cnt: 2, ((T([1438452], f16, stride=(4,)), T([1438452], f16)), {})
cnt: 4, ((T([1438452, 1], f16), T([1438452, 1], f16)), {})
cnt: 1, ((T([4, 1000], i64), 0), {})
cnt: 1, ((T([4, 1000], i64), 269952), {})
cnt: 1, ((T([4, 1000], i64), 337440), {})
cnt: 1, ((T([4, 1000], i64), 354312), {})
cnt: 1, ((T([4, 1000], i64), 358530), {})
cnt: 2, ((T([0], f32), 4), {})
cnt: 2, ((T([0], f32), T([], f32)), {})
cnt: 18, ((T([0], f16), T([0], f16)), {})
cnt: 2, ((T([0, 91], f16), T([0, 1], f16)), {})
cnt: 6, ((T([0, 91], f16), T([0, 91], f16)), {})
cnt: 4, ((T([], f16), 0), {})
cnt: 4, ((T([], f16), T([], f32)), {})
cnt: 8, ((T([], f32), T([], f16)), {})
cnt: 1, ((T([], f32), 0), {})
cnt: 3, ((T([], f32), T([], f32)), {})
cnt: 7, ((T([0, 364], f16), T([0, 364], f16)), {})
cnt: 1, ((T([0, 1024], f16), T([0, 1024], f16)), {})
cnt: 1, ((T([4, 256, 37, 38], f16), T([4, 256, 37, 38], f16)), {})
cnt: 2, ((T([4, 2048, 37, 38], f16), T([4, 2048, 37, 38], f16)), {})
cnt: 7, ((T([4, 1024, 74, 76], f16), T([4, 1024, 74, 76], f16)), {})
cnt: 5, ((T([4, 512, 148, 152], f16), T([4, 512, 148, 152], f16)), {})
```

----------------------------------------

TITLE: Adding LibTorch Include Directories in CMake
DESCRIPTION: Adds the LibTorch include directory, specified by the `libtorch_include_DIR` variable, to the JNI target's public include directories. The `BEFORE` keyword ensures it takes precedence, and `$<BUILD_INTERFACE:...>` makes it available to consuming targets.
SOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_10

LANGUAGE: cmake
CODE:
```
target_include_directories(${PYTORCH_JNI_TARGET} BEFORE
PUBLIC $<BUILD_INTERFACE:${libtorch_include_DIR}>)
```

----------------------------------------

TITLE: PyTorch Convolution Operations
DESCRIPTION: Documents convolution and convolution backward operations with various kernel sizes and channel dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/twins_pcpvt_base_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
aten.convolution.default((T([32, 3, 224, 224], f16), T([64, 3, 4, 4], f16), T([64], f16), [4, 4], [0, 0], [1, 1], False, [0, 0], 1))
aten.convolution_backward.default((T([32, 512, 7, 7], f16, stride=(25088, 1, 3584, 512)), T([32, 512, 7, 7], f16, stride=(25088, 1, 3584, 512)), T([512, 1, 3, 3], f16), [512], [1, 1], [1, 1], [1, 1], False, [0, 0], 512, [True, True, True]))
```

----------------------------------------

TITLE: Documenting Alias Relationship in PyTorch JIT View Schema
DESCRIPTION: Defines a JIT FunctionSchema for a `view` operation. The shared `(a)` annotation on the input `self` and the output indicates that the output Tensor will be an alias (share storage) with the input, belonging to the same alias set `a`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_35

LANGUAGE: JIT FunctionSchema
CODE:
```
view(Tensor(a) self, int[] size) -> Tensor(a)
```

----------------------------------------

TITLE: Calculating Mean Over Dimensions in PyTorch
DESCRIPTION: The mean operator calculates the arithmetic mean of tensor elements across specified dimensions, here indicated by lists [2, 3]. This operation is similar to global averaging and is often used in feature map reduction.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/legacy_senet154_training.txt#2025-04-22_snippet_3

LANGUAGE: text
CODE:
```
cnt: 3, ((T([32, 256, 56, 56], f16), [2, 3], True), {})
```

LANGUAGE: text
CODE:
```
cnt: 8, ((T([32, 512, 28, 28], f16), [2, 3], True), {})
```

LANGUAGE: text
CODE:
```
cnt: 36, ((T([32, 1024, 14, 14], f16), [2, 3], True), {})
```

LANGUAGE: text
CODE:
```
cnt: 3, ((T([32, 2048, 7, 7], f16), [2, 3], True), {})
```

LANGUAGE: text
CODE:
```
cnt: 1, ((T([32, 2048, 7, 7], f16), [-1, -2], True), {})
```

----------------------------------------

TITLE: Forward Convolution Operations in PyTorch
DESCRIPTION: Multiple forward convolution operations with different tensor shapes, kernel sizes, and parameters. These operations are likely part of a neural network model, possibly a variant of MobileNet or EfficientNet.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
((T([128, 672, 1, 1], f16), T([168, 672, 1, 1], f16), T([168], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

----------------------------------------

TITLE: Tensor Division Using aten.div in Python
DESCRIPTION: The aten.div.Tensor operator performs element-wise division between a tensor and a scalar, supporting computation involving broadcasting. It is essential for normalizing tensor values or adjusting scales, requiring f16 tensors and a scalar divisor as input.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/nvidia_deeprecommender_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
Operator: aten.div.Tensor
cnt: 2, ((T([], f16), 50675456), {})
```

----------------------------------------

TITLE: Matrix Multiplication Operations - PyTorch
DESCRIPTION: Various matrix multiplication operations (bmm, mm) used in transformer layers for computing attention scores and value projections. Includes operations with different stride patterns and tensor shapes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_GPT2_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
aten.bmm.default((T([48, 512, 64], f16), T([48, 64, 512], f16)), {})
aten.mm.default((T([2048, 768], f16), T([768, 50257], f16, stride=(1, 768))), {})
```

----------------------------------------

TITLE: PyTorch Tensor Split Operations
DESCRIPTION: Documents the tensor splitting operations (aten.split.Tensor) with various input shapes and split sizes, showing how tensors are divided along specified dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2net50_14w_8s_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
Operator: aten.split.Tensor
cnt: 3, ((T([128, 112, 56, 56], f16), 14, 1), {})
cnt: 1, ((T([128, 224, 56, 56], f16), 28, 1), {})
cnt: 3, ((T([128, 224, 28, 28], f16), 28, 1), {})
```

----------------------------------------

TITLE: Re-exporting an Imported Object
DESCRIPTION: Shows how to re-export an object that was previously imported by a PackageImporter, making the new PackageExporter aware of the original PackageImporter to find source code for dependencies.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
importer = PackageImporter(f)
obj = importer.load_pickle("model", "model.pkl")

# re-export obj in a new package
with PackageExporter(f2, importer=(importer, sys_importer)) as exporter:
    exporter.save_pickle("model", "model.pkl", obj)
```

----------------------------------------

TITLE: Matrix Multiplication Operations
DESCRIPTION: Batch matrix multiplication operations for attention mechanism calculations, operating on 16-bit floating point tensors with various shapes and strides.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_DistilBert_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
aten.bmm.default(T([96, 512, 64], f16), T([96, 64, 512], f16))
aten.mm.default(T([4096, 768], f16), T([768, 768], f16))
aten.addmm.default(T([768], f16), T([4096, 768], f16), T([768, 768], f16, stride=(1, 768)))
```

----------------------------------------

TITLE: NLL Loss Forward Operations in PyTorch
DESCRIPTION: Records of negative log likelihood loss forward operations for classification tasks. The operation includes input logits, target indices, weight tensor, reduction mode, and ignore index parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/spnasnet_100_training.txt#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
Operator: aten.nll_loss_forward.default
cnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})
```

----------------------------------------

TITLE: Handling Ragged Structure Incompatibility in PyTorch
DESCRIPTION: Shows how to work around errors when operating on NJTs with incompatible ragged structures by constructing them with the same offsets.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
>>> a = torch.randn(50, 128)
>>> b = torch.randn(32, 128)
>>> nt1 = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)
>>> nt2 = torch.nested.nested_tensor_from_jagged(values=torch.randn(82, 128), offsets=nt1.offsets())
>>> nt3 = nt1 + nt2
>>> nt3.shape
torch.Size([2, j1, 128])
```

----------------------------------------

TITLE: Invoking aten.relu_.default with Tensor Arguments (Text)
DESCRIPTION: This section logs calls to the `aten.relu_.default` operator, which performs an in-place Rectified Linear Unit (ReLU) activation. The examples show the operator being called on tensors with various shapes (e.g., [128, 128, 1, 1], [128, 768, 1, 1]) and float16 (f16) data type. The input is a tuple containing a single tensor description.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_18

LANGUAGE: text
CODE:
```
Operator: aten.relu_.default
```

LANGUAGE: text
CODE:
```
cnt: 1, ((T([128, 128, 1, 1], f16),), {})
```

LANGUAGE: text
CODE:
```
cnt: 2, ((T([128, 256, 1, 1], f16),), {})
```

LANGUAGE: text
CODE:
```
cnt: 9, ((T([128, 768, 1, 1], f16),), {})
```

----------------------------------------

TITLE: Constructing Semi-Structured Sparse Tensor in PyTorch
DESCRIPTION: Creates a dense tensor adhering to 2:4 sparse format, then compresses it to a semi-structured sparse tensor using to_sparse_semi_structured function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
from torch.sparse import to_sparse_semi_structured
A = torch.Tensor([0, 0, 1, 1]).tile((128, 32)).half().cuda()
A_sparse = to_sparse_semi_structured(A)
```

----------------------------------------

TITLE: Tensor Softmax and Backward in PyTorch
DESCRIPTION: This snippet shows the use of `aten._softmax.default` and `aten._softmax_backward_data.default`, for softmax computation and its gradient computation, applied across various tensor dimensions. Inputs include tensors, dimensions, and a boolean flag for optional functionality.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForCausalLM_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
Operator: aten._softmax.default
cnt: 6, ((T([192, 128, 128], f16), -1, False), {})
```

LANGUAGE: Python
CODE:
```
Operator: aten._softmax_backward_data.default
cnt: 6, ((T([192, 128, 128], f16), T([192, 128, 128], f16), -1, f16), {})
```

----------------------------------------

TITLE: Convolution Operation Parameters
DESCRIPTION: Multiple convolution operations with varying tensor shapes, strides and parameters for different layers of a neural network. Uses half-precision (f16) tensors with specific stride configurations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
cnt: 6, ((T([64, 240, 14, 14], f16, stride=(94080, 196, 14, 1)), T([64, 80, 14, 14], f16, stride=(31360, 196, 14, 1)), T([240, 80, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

----------------------------------------

TITLE: Loop-Level Implementation of Embedding Lookup
DESCRIPTION: Shows the loop-based equivalent of the dimension-based embedding lookup operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
for sequence in range(words.size(0)):
    for features in range(embeddings.size(1)):
        state = embeddings[words[sequence], features]
```

----------------------------------------

TITLE: Unsupported Autocast Decorator in JIT Scripting
DESCRIPTION: Example showing unsupported usage of autocast as a function decorator when calling a decorated function from within a JIT scripted function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/JIT-AUTOCAST.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch
from torch.cpu.amp import autocast

@autocast(enabled=True)
def helper(x):
    ...

@torch.jit.script
def foo(x):
    return helper(x) # not supported
```

----------------------------------------

TITLE: Rebuilding PyTorch with CMake After Adding Tests
DESCRIPTION: Shows the command to rebuild the PyTorch project using its setup script after adding a new test file. This step is necessary because test files are globbed by CMake, requiring a CMake re-run to detect new files.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/tensorexpr/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
python setup.py build --cmake
```

----------------------------------------

TITLE: Importing Tensor Parallelism Module in Python
DESCRIPTION: This snippet shows how to import the Tensor Parallelism module in PyTorch. It's the entry point for parallelizing nn.Module using Tensor Parallelism.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.tensor.parallel.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from torch.distributed.tensor.parallel import parallelize_module
```

----------------------------------------

TITLE: Python Loop Tracing Example
DESCRIPTION: Demonstrates a problematic case of tracing a loop that depends on input tensor shape, showing how the trace differs across different input shapes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
def loop_in_traced_fn(x):
    result = x[0]
    for i in range(x.size(0)):
        result = result * x[i]
    return result

inputs = (torch.rand(3, 4, 5),)
check_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]

traced = torch.jit.trace(loop_in_traced_fn, inputs, check_inputs=check_inputs)
```

----------------------------------------

TITLE: Defining Custom Op with ONNX-script (Part 4: Registering Symbolic)
DESCRIPTION: Registers the custom_selu symbolic function for the aten::selu operator. This step links the PyTorch operator to the custom ONNX-script implementation defined previously, allowing the exporter to use it.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_28

LANGUAGE: Python
CODE:
```
torch.onnx.register_custom_op_symbolic(
    symbolic_name="aten::selu",
    symbolic_fn=custom_selu,
    opset_version=opset_version,
)
```

----------------------------------------

TITLE: Profiling Copy Operations in PyTorch
DESCRIPTION: Log of memory copy operations between tensors with the same shape. This shows a copy between two tensors of shape [64, 3, 256, 256] with half-precision (f16) data type.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
Operator: aten.copy_.default
cnt: 1, ((T([64, 3, 256, 256], f16), T([64, 3, 256, 256], f16)), {})
```

----------------------------------------

TITLE: Copying Tensor Data (In-place) - PyTorch Aten
DESCRIPTION: Copies the data from a source tensor into a destination tensor in-place. The destination tensor is modified. This internal operator is used for efficient data transfer between tensors. It takes the destination tensor and the source tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BartForConditionalGeneration_training.txt#_snippet_11

LANGUAGE: Python
CODE:
```
import torch

dest_tensor = torch.zeros(2, 1024, dtype=torch.int64)
src_tensor = torch.ones(2, 1024, dtype=torch.int64)
dest_tensor.copy_(src_tensor)
```

----------------------------------------

TITLE: Adding Static CUDA Runtime Dependencies in CMake
DESCRIPTION: Checks if the environment variable `ATEN_STATIC_CUDA` is set. If true, it appends the static CUDA OS abstraction library (`CUDA::culibos`) and the static CUDA runtime library (`CUDA::cudart_static`) to the `ATen_CUDA_DEPENDENCY_LIBS` list.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_15

LANGUAGE: cmake
CODE:
```
  if($ENV{ATEN_STATIC_CUDA})
    list(APPEND ATen_CUDA_DEPENDENCY_LIBS
      CUDA::culibos
      CUDA::cudart_static
    )
  endif($ENV{ATEN_STATIC_CUDA})
endif()
```

----------------------------------------

TITLE: Convolution Backward Pass Parameters - PyTorch
DESCRIPTION: Backward pass specifications for convolution operations, including gradient tensors, input tensors, weight tensors and configuration parameters for gradient computation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
((T([1, 36, 5, 5], f16, stride=(900, 1, 180, 36)), T([1, 88, 5, 5], f16), T([36, 88, 1, 1], f16), [36], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
```

----------------------------------------

TITLE: Saving Custom Class Instances in a Package
DESCRIPTION: Shows how to save instances of a custom class in a torch package using PackageExporter. This demonstrates that the custom serialization happens automatically when instances are encountered.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
# example of saving instances of class Foo

import torch
from torch.package import PackageImporter, PackageExporter
import foo

foo_1 = foo.Foo("foo_1 initial string")
foo_2 = foo.Foo("foo_2 initial string")
with PackageExporter('foo_package.pt') as pe:
    # save as normal, no extra work necessary
```

----------------------------------------

TITLE: PyTorch Convolution Operations
DESCRIPTION: Sequence of convolution operations with different input/output channels and spatial dimensions. Operations use float16 precision and include parameters for stride, padding, and dilation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/densenet121_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
((T([4, 128, 14, 14], f16), T([4, 704, 14, 14], f16), T([128, 704, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

----------------------------------------

TITLE: Activation Function Operations (ReLU, SiLU, Sigmoid)
DESCRIPTION: Implementation of various activation functions including ReLU, SiLU (Swish), and Sigmoid, with their corresponding backward passes. Operations use half precision tensors with different shapes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_24

LANGUAGE: python
CODE:
```
((T([64, 8, 1, 1], f16),), {})
```

----------------------------------------

TITLE: Tensor View and Reshape Operations
DESCRIPTION: Series of tensor view operations for reshaping attention mechanisms, including transformations between different shapes for multi-head attention calculations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_DistilBert_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
aten._unsafe_view.default(T([8, 12, 512, 64], f16), [96, 512, 64])
aten._unsafe_view.default(T([8, 12, 64, 512], f16), [96, 64, 512])
aten._unsafe_view.default(T([96, 512, 512], f16), [8, 12, 512, 512])
```

----------------------------------------

TITLE: Summing Tensor Along Dimension in PyTorch
DESCRIPTION: This snippet shows a sum operation on a tensor in PyTorch, likely used for reducing the output of the final layer. It sums along the first dimension (batch dimension) and keeps the result's dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_inception_v3_training.txt#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([128, 1000], f16), [0], True), {})
```

----------------------------------------

TITLE: Handling Graph Breaks in CUDA Execution
DESCRIPTION: Example showing how to handle graph breaks when executing NumPy code on CUDA using device context manager.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
@torch.compile
@torch.compiler.wrap_numpy
def numpy_fn(X, Y):
    prod = X[:, :, None] * Y[:, None, :]
    print("oops, a graph break!")
    return np.sum(prod, axis=(-2, -1))

X = torch.randn(1024, 64, device="cuda")
Y = torch.randn(1024, 64, device="cuda")

with torch.device("cuda"):
    Z = numpy_fn(X, Y)
assert isinstance(Z, torch.Tensor)
assert Z.device.type == "cuda"
```

----------------------------------------

TITLE: Analyzing ATen Sum Operations in PyTorch
DESCRIPTION: Tracks \"aten.sum.SymInt\" operations, detailing summation over unspecified dimensions of tensors in  neural network layers.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_23

LANGUAGE: pseudocode
CODE:
```
Operator: aten.sum.SymInt
cnt: 60, ((T([1024, 1024], f16), [0], True), {})
cnt: 12, ((T([1024, 4096], f16), [0], True), {})
```

----------------------------------------

TITLE: Negative Log Likelihood Loss Backward Pass in PyTorch (Python)
DESCRIPTION: Carries out the aten.nll_loss_backward function to compute gradients during backpropagation by recovering derivatives of the negative log likelihood loss, vital in optimizing classification models.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_20

LANGUAGE: Python
CODE:
```
aten.nll_loss_backward.default
cnt: 1, ((T([], f16), T([16, 2], f16), T([16], i64), None, 1, -100, T([], f16)), {})
```

----------------------------------------

TITLE: Sigmoid Backward Operations in PyTorch
DESCRIPTION: This snippet shows the backward pass for sigmoid activation, used during gradient computation. This operation computes gradients through the sigmoid function based on the derivative of sigmoid with respect to its input.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
Operator: aten.sigmoid_backward.default
cnt: 1, ((T([96, 65], f16, stride=(0, 0)), T([96, 65], f16)), {})
```

----------------------------------------

TITLE: Dimension Removal Operations in PyTorch
DESCRIPTION: Demonstrates how reduction operations and dimension removal functions handle named tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/name_inference.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
>>> x = torch.randn(1, 3, 3, 3, names=('N', 'C', 'H', 'W'))
>>> x.squeeze('N').names
('C', 'H', 'W')

>>> x = torch.randn(3, 3, 3, 3, names=('N', 'C', 'H', 'W'))
>>> x.sum(['N', 'C']).names
('H', 'W')

# Reduction ops with keepdim=True don't actually remove dimensions.
>>> x = torch.randn(3, 3, 3, 3, names=('N', 'C', 'H', 'W'))
>>> x.sum(['N', 'C'], keepdim=True).names
('N', 'C', 'H', 'W')
```

----------------------------------------

TITLE: Implementing sqrt Kernel for CUDA in PyTorch
DESCRIPTION: This function implements the sqrt operation for CUDA tensors in PyTorch. It uses a 2D loop strategy derived from a 1D implementation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_41

LANGUAGE: C++
CODE:
```
_ZN3c1012function_refIFvPPcPKlllEE11callback_fnIZN2at18TensorIteratorBase15loop_2d_from_1dIZZZNS8_6native7DEFAULT11sqrt_kernelERS9_ENKUlvE_clEvENKUlvE0_clEvEUlS2_S4_lE_EEDaRKT_EUlS2_S4_llE_EEvlS2_S4_ll
```

----------------------------------------

TITLE: Computing NLL Loss Backward Pass in PyTorch
DESCRIPTION: This snippet shows the tensor shapes and parameters for the backward pass of the negative log-likelihood loss in a PyTorch model. It includes input gradients, predictions, and target labels.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_inception_v3_training.txt#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})
```

----------------------------------------

TITLE: Nearest Neighbor Upsampling in PyTorch
DESCRIPTION: This snippet performs nearest neighbor upsampling on 2D tensors. It's commonly used in computer vision tasks to increase the spatial dimensions of feature maps or images.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_19

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([8, 256, 12, 16], f16), None, [2.0, 2.0]), {})
cnt: 1, ((T([8, 128, 24, 32], f16), None, [2.0, 2.0]), {})
```

----------------------------------------

TITLE: Basic ScalarTensor Implementation
DESCRIPTION: Initial implementation of a ScalarTensor class that represents a 2D scalar tensor with diagonal values. This version doesn't include torch function integration.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
class ScalarTensor(object):
    def __init__(self, N, value):
        self._N = N
        self._value = value

    def __repr__(self):
        return "ScalarTensor(N={}, value={})".format(self._N, self._value)

    def tensor(self):
        return self._value * torch.eye(self._N)
```

----------------------------------------

TITLE: PyTorch Tensor Split Operations
DESCRIPTION: Split operations on tensors with specified size divisions, operating on 4D tensors with varying channel dimensions and batch sizes of 128.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/levit_128_training.txt#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
((T([128, 196, 4, 64], f16), [16, 16, 32], 3), {})
((T([128, 196, 8, 80], f16), [16, 64], 3), {})
```

----------------------------------------

TITLE: Running TIMM Models Performance Benchmarks (Python)
DESCRIPTION: Commands to run TIMM models performance benchmarks for both training and inference using TorchInductor backend. The commands specify device, precision, and output format.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/README.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
./benchmarks/dynamo/timm_models.py --performance --training --amp --backend=inductor --output=timm_models_training.csv
./benchmarks/dynamo/timm_models.py --performance --inference --bfloat16 --backend=inductor --output=timm_models_inference.csv
```

----------------------------------------

TITLE: Interning Multiple Module Patterns
DESCRIPTION: Example of interning multiple module patterns at once in torch.package, which allows selecting specific submodules from a package to include in the export.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
exporter.intern(["torchvision.models.**", "torchvision.utils.**"])
```

----------------------------------------

TITLE: Checking Supported ISA Instructions using collect_env Script
DESCRIPTION: Command to check supported Instruction Set Architecture (ISA) features on the machine by using PyTorch's collect_env script and filtering for AVX/AMX instructions using grep.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_best_practices_for_backends.rst#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python collect_env.py | grep "a[(v|m)]x"
```

----------------------------------------

TITLE: Max Pooling Operations in DenseNet Forward Pass
DESCRIPTION: This snippet shows the max pooling operations used in DenseNet's forward pass. These operations perform downsampling with 3×3 kernels, stride 2, and track indices for use in the backward pass.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
Operator: aten.max_pool2d_with_indices.default
cnt: 1, ((T([32, 256, 56, 56], f16), [3, 3], [2, 2], [0, 0], [1, 1], True), {})
cnt: 1, ((T([32, 512, 28, 28], f16), [3, 3], [2, 2], [0, 0], [1, 1], True), {})
cnt: 1, ((T([32, 768, 14, 14], f16), [3, 3], [2, 2], [0, 0], [1, 1], True), {})
```

----------------------------------------

TITLE: Using file_structure() API to Explore Package Contents
DESCRIPTION: Demonstrates how to use the file_structure() method of PackageImporter to explore and print the contents of a torch package with filtering options. Shows both filtered and unfiltered output.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
with PackageExporter('my_package.pt') as pe:
    pe.save_pickle('models', 'model_1.pkl', mod)

importer = PackageImporter('my_package.pt')
# can limit printed items with include/exclude args
print(importer.file_structure(include=["**/utils.py", "**/*.pkl"], exclude="**/*.storage"))
print(importer.file_structure()) # will print out all files
```

----------------------------------------

TITLE: Accessing Nested Tensor Components in Python
DESCRIPTION: This example illustrates how to directly access the values and offsets of a nested tensor using `values()` and `offsets()` methods. It also shows constructing an NJT from jagged `values` and `offsets` directly.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
>>> a = torch.randn(50, 128) # text 1
>>> b = torch.randn(32, 128) # text 2
>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)
>>> nt.values().shape  # note the "packing" of the ragged dimension; no padding needed
torch.Size([82, 128])
>>> nt.offsets()
tensor([ 0, 50, 82])
```

LANGUAGE: python
CODE:
```
>>> values = torch.randn(82, 128)
>>> offsets = torch.tensor([0, 50, 82], dtype=torch.int64)
>>> nt = torch.nested.nested_tensor_from_jagged(values=values, offsets=offsets)
```

----------------------------------------

TITLE: Sum Operations with SymInt in PyTorch
DESCRIPTION: A single record of a symbolic integer summation operation (aten.sum.SymInt) applied to a tensor with shape [8, 1000] and float16 data type. The operation reduces along dimension 0 with keep_dim=True.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnext50_32x4d_training.txt#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([8, 1000], f16, stride=(0, 0)), [0], True), {})
```

----------------------------------------

TITLE: Constant Padding Operations in PyTorch
DESCRIPTION: Statistics for the aten.constant_pad_nd.default operator used for padding tensors with constant values. The operation adds padding to various tensors, both for adding boundaries (with zeros) and for trimming tensor edges (with negative values).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
Operator: aten.constant_pad_nd.default
cnt: 1, ((T([128, 3, 192, 192], f16), [0, 1, 0, 1], 0.0), {})
cnt: 1, ((T([128, 64, 96, 96], f16), [0, 1, 0, 1], 0.0), {})
cnt: 1, ((T([128, 256, 48, 48], f16), [0, 1, 0, 1], 0.0), {})
cnt: 1, ((T([128, 768, 24, 24], f16), [0, 1, 0, 1], 0.0), {})
cnt: 1, ((T([128, 768, 12, 12], f16), [0, 1, 0, 1], 0.0), {})
cnt: 1, ((T([128, 768, 13, 13], f16), [0, -1, 0, -1]), {})
cnt: 1, ((T([128, 768, 25, 25], f16), [0, -1, 0, -1]), {})
cnt: 1, ((T([128, 256, 49, 49], f16), [0, -1, 0, -1]), {})
cnt: 1, ((T([128, 64, 97, 97], f16), [0, -1, 0, -1]), {})
```

----------------------------------------

TITLE: Disabling FP16 Reduction CuBLAS (C++)
DESCRIPTION: Provides the C++ code snippet to disable reduced precision reductions specifically for FP16 matrix multiplications within the CuBLAS backend by interacting with the global context.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_6

LANGUAGE: cpp
CODE:
```
at::globalContext().setAllowFP16ReductionCuBLAS(false);
```

----------------------------------------

TITLE: Building PyTorch for Android
DESCRIPTION: Basic command to build PyTorch libraries for Android platform from the PyTorch root directory.
SOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
#in your PyTorch root directory
bash scripts/build_android.sh
```

----------------------------------------

TITLE: Explicitly Defining PyTorch Operator Schema (C++)
DESCRIPTION: Demonstrates how to explicitly define the operator schema as a string literal passed to the `.op()` method, instead of relying on automatic inference from the kernel signature. This allows for documentation and ensures the schema matches expectations. The schema string follows TorchScript syntax.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_7

LANGUAGE: cpp
CODE:
```
namespace { Tensor my_kernel_cpu(const Tensor& a, const Tensor& b) {...} }

static auto registry = torch::RegisterOperators()
   .op("my_namespace::my_op(Tensor a, Tensor b) -> Tensor",
       torch::RegisterOperators::options()
         .kernel<decltype(my_kernel_cpu), &my_kernel_cpu>(CPU()));
```

----------------------------------------

TITLE: Configuring cuDNN Support for PyTorch Python
DESCRIPTION: Adds cuDNN support to PyTorch Python bindings when either CUDA or ROCm is enabled with cuDNN.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_10

LANGUAGE: CMake
CODE:
```
if(USE_CUDNN OR USE_ROCM)
    list(APPEND TORCH_PYTHON_SRCS
      ${TORCH_SRC_DIR}/csrc/cuda/shared/cudnn.cpp
      )
    if(USE_STATIC_CUDNN)
        set_source_files_properties(
          ${TORCH_SRC_DIR}/csrc/cuda/shared/cudnn.cpp
          PROPERTIES COMPILE_FLAGS "-DUSE_STATIC_CUDNN"
        )
    endif()
endif()
```

----------------------------------------

TITLE: Profiling aten.sum.SymInt Calls - PyTorch - Python
DESCRIPTION: Presents structure for tensor sum reductions using symbolically-typed axes (aten.sum.SymInt) in PyTorch. Inputs are float16 tensors and sum dimensions (usually a list), with keepdim option. Outputs are reduced tensors or scalars, used to identify reduction hotspots. Requires PyTorch with symbolic shape support.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/selecsls42b_training.txt#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([128, 1000], f16), [0], True), {})
```

----------------------------------------

TITLE: Matrix Multiplication Operation Statistics
DESCRIPTION: Logs of matrix multiplication operations (aten.mm.default) showing tensor shapes and data types, primarily using float16 tensors with varying dimensions
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 1000], f16), T([1000, 1280], f16)), {})
cnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 1280], f16)), {})
```

----------------------------------------

TITLE: Scripting a Function Operating on a NamedTuple in TorchScript (Python)
DESCRIPTION: This snippet defines a NamedTuple MyTuple with two integer fields, a function inc that increments both fields, then scripts and invokes inc via torch.jit.script. It highlights use of typing.NamedTuple and tuple destructuring within TorchScript, returning a new tuple of incremented integers. Prerequisites: torch and typing.NamedTuple; expects a MyTuple input; outputs a 2-tuple of incremented ints. Use this pattern for structured, statically typed containers.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import torch
from typing import NamedTuple
from typing import Tuple

class MyTuple(NamedTuple):
    first: int
    second: int

def inc(x: MyTuple) -> Tuple[int, int]:
    return (x.first+1, x.second+1)

t = MyTuple(first=1, second=2)
scripted_inc = torch.jit.script(inc)
print("TorchScript:", scripted_inc(t))
```

----------------------------------------

TITLE: PyTorch CUDA BLAS Operations
DESCRIPTION: CUDA BLAS implementations for matrix operations like GEMM for different data types including Half and Float.
SOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_28

LANGUAGE: C++
CODE:
```
_ZN2at4cuda4blas5bgemmIN3c104HalfEEEvcclllNS_10OpMathTypeIT_E4typeEPKS6_llSA_llS8_PS6_lll
_ZN2at4cuda4blas4gemmIN3c104HalfEEEvcclllNS_10OpMathTypeIT_E4typeEPKS6_lSA_lS8_PS6_l
```

----------------------------------------

TITLE: Loss Function Calculations
DESCRIPTION: NLL (Negative Log Likelihood) loss calculations for model training, including both forward and backward passes with half precision tensors and integer labels.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_25

LANGUAGE: python
CODE:
```
((T([], f16), T([64, 1000], f16), T([64], i64), None, 1, -100, T([], f16)), {})
```

----------------------------------------

TITLE: Importing DLPack Conversion Functions in PyTorch
DESCRIPTION: This snippet demonstrates how to import the from_dlpack and to_dlpack functions from the torch.utils.dlpack module. These functions are used for converting between PyTorch tensors and DLPack tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/dlpack.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
from torch.utils.dlpack import from_dlpack, to_dlpack
```

----------------------------------------

TITLE: Example TorchScript Function for Alias Analysis Demonstration
DESCRIPTION: A sample Python function decorated with `@torch.jit.script` to be compiled by TorchScript. It demonstrates tensor operations including a pure multiply, an in-place add, conditional logic accessing elements, and returning multiple tensors, serving as an example graph for alias analysis visualization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_40

LANGUAGE: Python
CODE:
```
@torch.jit.script
def foo(a : Tensor, b : Tensor):
  c = 2 * b
  a += 1
  if a.max() > 4:
    r = a[0]
  else:
    r = b[0]
  return c, r
```

----------------------------------------

TITLE: Configuring PyTorch Android Dependencies in Gradle (Nightly)
DESCRIPTION: This snippet demonstrates how to set up Gradle dependencies for nightly (snapshot) builds of PyTorch Android. It includes repository configuration and implementation details for both lite interpreter and full JIT builds.
SOURCE: https://github.com/pytorch/pytorch/blob/main/android/README.md#2025-04-22_snippet_1

LANGUAGE: Groovy
CODE:
```
repositories {
    maven {
        url "https://oss.sonatype.org/content/repositories/snapshots"
    }
}

# lite interpreter build
dependencies {
    ...
    implementation 'org.pytorch:pytorch_android_lite:1.12.0-SNAPSHOT'
    implementation 'org.pytorch:pytorch_android_torchvision_lite:1.12.0-SNAPSHOT'
    ...
}

# full jit build
dependencies {
    ...
    implementation 'org.pytorch:pytorch_android:1.12.0-SNAPSHOT'
    implementation 'org.pytorch:pytorch_android_torchvision:1.12.0-SNAPSHOT'
    ...
}
```

----------------------------------------

TITLE: Creating Tensors Filled with Zeros using aten.new_zeros - Python
DESCRIPTION: Illustrates the usage of aten.new_zeros.default to allocate tensors initialized with zeros, including specifying device, dtype, and memory layout. It handles both strided and contiguous input tensors, on CUDA devices, and is dependent on PyTorch. Shape and stride combinations should be valid.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
Operator: aten.new_zeros.default
cnt: 12, ((T([2, 12, 12, 64, 64], f16, stride=(786432, 64, 49152, 768, 1)), [1179648]), {})
cnt: 24, ((T([1008, 64, 64], f16), [384, 64, 64]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})
```

----------------------------------------

TITLE: Matrix Multiplication with Bias for Linear Layers
DESCRIPTION: Documents the addmm operator calls for linear layer transformations in the neural network. These operations perform matrix multiplication with an added bias term, used primarily in feed-forward layers and projection operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
Operator: aten.addmm.default
cnt: 36, ((T([2304], f16), T([1152, 768], f16), T([768, 2304], f16, stride=(1, 768))), {})
cnt: 36, ((T([768], f16), T([1152, 768], f16), T([768, 768], f16, stride=(1, 768))), {})
cnt: 36, ((T([3072], f16), T([1152, 768], f16), T([768, 3072], f16, stride=(1, 768))), {})
cnt: 36, ((T([768], f16), T([1152, 3072], f16), T([3072, 768], f16, stride=(1, 3072))), {})
cnt: 2, ((T([768], f16), T([2, 768], f16, stride=(443136, 1)), T([768, 768], f16, stride=(1, 768))), {})
cnt: 4, ((T([768], f16), T([1154, 768], f16), T([768, 768], f16, stride=(1, 768))), {})
cnt: 2, ((T([768], f16), T([2, 768], f16), T([768, 768], f16, stride=(1, 768))), {})
cnt: 2, ((T([3072], f16), T([2, 768], f16), T([768, 3072], f16, stride=(1, 768))), {})
cnt: 2, ((T([768], f16), T([2, 3072], f16), T([3072, 768], f16, stride=(1, 3072))), {})
cnt: 1, ((T([1000], f16), T([2, 768], f16, stride=(443136, 1)), T([768, 1000], f16, stride=(1, 768))), {})
```

----------------------------------------

TITLE: Creating Operator and Mobile Optimization Utilities
DESCRIPTION: Defines binary targets for utilities that dump operator names and optimize models for mobile deployment.
SOURCE: https://github.com/pytorch/pytorch/blob/main/binaries/CMakeLists.txt#2025-04-22_snippet_5

LANGUAGE: cmake
CODE:
```
caffe2_binary_target("dump_operator_names.cc")
caffe2_binary_target("optimize_for_mobile.cc")
```

----------------------------------------

TITLE: Subclassing for Generated Code Debugging
DESCRIPTION: Demonstrates how to debug generated code by creating a subclass with the generated forward function for comparison.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
class SubclassM(M):
    def __init__(self):
        super().__init__()

    def forward(self, y):
        x = self.x
        add_1 = x + y;  x = y = None
        return add_1

pre_trace = M()
post_trace = SubclassM()
```

----------------------------------------

TITLE: vmap with Side-Effectful Mutation (Discouraged) - PyTorch - Python
DESCRIPTION: This Python snippet demonstrates a function with side effects (mutation of a list and printing) wrapped with vmap. The function 'f' pops an element from a list and prints a message, then sums a tensor. vmap is applied with mismatched batching (in_dims), and the example shows that side effects (like mutation and print) only happen once, not per batch. The code requires PyTorch and torch.func. This highlights a pattern to avoid—using mutation or print statements within vmap-mapped functions leads to unexpected or incorrect behavior.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
def f(x, list):
  list.pop()
  print("hello!")
  return x.sum(0)

x = torch.randn(3, 1)
lst = [0, 1, 2, 3]

result = vmap(f, in_dims=(0, None))(x, lst)
```

----------------------------------------

TITLE: Calling aten.clone.default (Python)
DESCRIPTION: Creates a copy of a tensor. Useful when an operation might modify the tensor in-place but the original values are still needed, or to break computation graphs. Examples show cloning int64 tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_10

LANGUAGE: Python
CODE:
```
((T([8, 128], i64),), {})
```

LANGUAGE: Python
CODE:
```
((T([8, 127], i64, stride=(128, 1)),), {})
```

----------------------------------------

TITLE: Using Functorch's Batch Norm Patching Utility in PyTorch
DESCRIPTION: Applies functorch's utility function to automatically replace all BatchNorm modules in a network with versions that don't track running statistics, modifying the network in-place.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/batch_norm.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
from functorch.experimental import replace_all_batch_norm_modules_
replace_all_batch_norm_modules_(net)
```

----------------------------------------

TITLE: Disabling BF16 Reduced Precision Reduction (Python)
DESCRIPTION: Shows the Python code to disable reduced precision reductions within BF16 matrix multiplications performed by the CUDA backend. Disabling this might improve numerical stability if issues are observed.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_7

LANGUAGE: python
CODE:
```
torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False
```

----------------------------------------

TITLE: Configuring PyTorch Build with Env Vars (Bash)
DESCRIPTION: Demonstrates setting environment variables before running `python setup.py develop` to control which PyTorch components are built. This example disables several components (`DEBUG=1`, `USE_DISTRIBUTED=0`, etc.) to speed up compilation for specific development scenarios.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_41

LANGUAGE: bash
CODE:
```
DEBUG=1 USE_DISTRIBUTED=0 USE_MKLDNN=0 USE_CUDA=0 BUILD_TEST=0 USE_FBGEMM=0 USE_NNPACK=0 USE_QNNPACK=0 USE_XNNPACK=0 python setup.py develop
```

----------------------------------------

TITLE: Batch Normalization Backward Pass
DESCRIPTION: Backward propagation for batch normalization with gradient calculations. Uses both f16 and f32 tensors for running statistics.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_efficientnet_b0_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
((T([128, 1280, 7, 7], f16), T([128, 1280, 7, 7], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f32), T([1280], f32), True, 0.001, [True, True, True]), {})
```

----------------------------------------

TITLE: Scalar Tensor Subtraction with rsub in PyTorch (Python)
DESCRIPTION: Applies aten.rsub which allows the subtraction of a scalar from each element in a tensor. It's useful in adjusting values throughout the elements of a tensor by a constant amount.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_22

LANGUAGE: Python
CODE:
```
aten.rsub.Scalar
cnt: 1, ((T([16, 1, 1, 512], f16), 1.0), {})
```

----------------------------------------

TITLE: Softmax Operations in PyTorch
DESCRIPTION: Implementation of forward and backward softmax operations with 16-bit floating point tensors of shape [8, 12, 512, 512].
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_DistilBert_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
aten._softmax.default(T([8, 12, 512, 512], f16), -1, False)
aten._softmax_backward_data.default(T([8, 12, 512, 512], f16), T([8, 12, 512, 512], f16), -1, f16)
```

----------------------------------------

TITLE: Batch Normalization Operations in PyTorch
DESCRIPTION: Batch normalization forward operations with batch size 128 and varying feature dimensions. Uses half-precision (f16) tensors with momentum 0.1 and epsilon 0.001.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_efficientnet_b0_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
((T([128, 144, 28, 28], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f16), True, 0.1, 0.001), {})
```

----------------------------------------

TITLE: Locating oneDNN Graph Tensor Implementation - Bash
DESCRIPTION: These paths point to the header and source files responsible for implementing the LlgaTensorImpl, which handles tensor interactions between PyTorch and the oneDNN Graph backend.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/onednn/README.md#_snippet_3

LANGUAGE: bash
CODE:
```
torch/csrc/jit/codegen/onednn/LlgaTensorImpl.h\ntorch/csrc/jit/codegen/onednn/LlgaTensorImpl.cpp
```

----------------------------------------

TITLE: Generating function documentation for window functions in reStructuredText
DESCRIPTION: Auto-summary directive to generate documentation for various window functions available in the torch.signal.windows module, creating separate documentation pages for each function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/signal.rst#2025-04-22_snippet_3

LANGUAGE: reStructuredText
CODE:
```
.. autosummary::
    :toctree: generated
    :nosignatures:

    bartlett
    blackman
    cosine
    exponential
    gaussian
    general_cosine
    general_hamming
    hamming
    hann
    kaiser
    nuttall
```

----------------------------------------

TITLE: Sphinx RST API Documentation Structure for PyTorch Compiler
DESCRIPTION: Sphinx documentation structure defining the torch.compiler module API reference, including autosummary directives for compiler functions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_api.rst#2025-04-22_snippet_0

LANGUAGE: rst
CODE:
```
.. currentmodule:: torch.compiler

.. automodule:: torch.compiler

.. _torch.compiler_api:

torch.compiler API reference
============================

For a quick overview of ``torch.compiler``, see :ref:`torch.compiler_overview`.

.. autosummary::
    :toctree: generated
    :nosignatures:

     compile
     reset
     allow_in_graph
     substitute_in_graph
     assume_constant_result
     list_backends
     disable
     set_stance
     cudagraph_mark_step_begin
     is_compiling
     is_dynamo_compiling
     is_exporting
```

----------------------------------------

TITLE: Fast Numerical Evaluation of Complex Gradients in PyTorch
DESCRIPTION: Mathematical formulation showing how PyTorch computes scalar quantities for fast backward gradcheck with complex inputs, avoiding full Jacobian matrix reconstruction while maintaining correctness.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/gradcheck.rst#2025-04-22_snippet_1

LANGUAGE: math
CODE:
```
\begin{aligned}
    s &= 2 * v^T (real(CW) ur + i * imag(CW) ui) \\
      &= 2 * v^T (\frac{1}{2} * \frac{\partial y}{\partial a} ur + i * \frac{1}{2} * \frac{\partial y}{\partial b} ui) \\
      &= v^T (\frac{\partial y}{\partial a} ur + i * \frac{\partial y}{\partial b} ui) \\
      &= v^T ((\frac{\partial y}{\partial a} ur) + i * (\frac{\partial y}{\partial b} ui))
\end{aligned}
```

----------------------------------------

TITLE: vmap with Different Randomness Behavior
DESCRIPTION: This example demonstrates using vmap with 'different' randomness flag, where random values are generated independently for each element in the batch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/ux_limitations.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
def add_noise(x):
  y = torch.randn(())  # y will be different across the batch
  return x + y

x = torch.ones(3)
result = vmap(add_noise, randomness="different")(x)  # we get 3 different values
```

----------------------------------------

TITLE: Using Distributed Autograd Context in PyTorch
DESCRIPTION: Shows how to use the distributed autograd context manager to properly setup and execute distributed backward pass. The context ensures that all send and recv functions are properly stored.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/distributed_autograd.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch.distributed.autograd as dist_autograd
with dist_autograd.context() as context_id:
  loss = model.forward()
  dist_autograd.backward(context_id, loss)
```

----------------------------------------

TITLE: Analyzing NLL Loss Operations in PyTorch
DESCRIPTION: This snippet shows the usage of nll_loss_backward.default and nll_loss_forward.default operators with specific tensor shapes and parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/swsl_resnext101_32x16d_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([], f16), T([32, 1000], f16), T([32], i64), None, 1, -100, T([], f16)), {})
cnt: 1, ((T([32, 1000], f16), T([32], i64), None, 1, -100), {})
```

----------------------------------------

TITLE: Running Sparse Matrix-Matrix Multiplication Benchmark
DESCRIPTION: Command line usage for running SPMM benchmarks using matmul_bench.py. Supports sparse@sparse and sparse@dense operations with optional CUDA and backward testing.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/sparse/dlmc/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
matmul_bench.py --operation sparse@sparse|sparse@dense --backward-test --with-cuda
```

----------------------------------------

TITLE: Implementing pow Kernel for Tensor-Tensor Operations in PyTorch
DESCRIPTION: This CUDA kernel implements the power operation for tensor-tensor operations in PyTorch. It operates on TensorIteratorBase to handle various tensor shapes and types.
SOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_42

LANGUAGE: C++
CODE:
```
_ZN2at6native51_GLOBAL__N__e9b6561f_12_PowKernel_cu_40e48458_3413224pow_tensor_tensor_kernelERNS_18TensorIteratorBaseE
```

----------------------------------------

TITLE: Computing NLL Loss Forward Pass in PyTorch
DESCRIPTION: This snippet shows the tensor shapes and parameters for the forward pass of the negative log-likelihood loss in a PyTorch model. It includes predictions and target labels.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_inception_v3_training.txt#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})
```

----------------------------------------

TITLE: PyTorch Tensor View and Reshape Operations
DESCRIPTION: Series of tensor view/reshape operations using unsafe_view to modify tensor dimensions while maintaining the underlying data.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
aten._unsafe_view.default((T([1024, 16, 8, 8, 2, 2], f16), [1024, 16, 64, 4]))
aten._unsafe_view.default((T([128, 384, 2, 2, 12, 12], f16), [1024, 48, 4, 144]))
```

----------------------------------------

TITLE: Including CSV Table for Core Aten IR Operations
DESCRIPTION: This snippet includes a CSV table containing information about Core Aten IR operations. It uses the csv-table directive in reStructuredText to reference an external CSV file.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_ir.rst#2025-04-22_snippet_1

LANGUAGE: reStructuredText
CODE:
```
.. csv-table::
   :file: ../build/ir/aten_ops.csv
   :widths: auto
   :header-rows: 1
```

----------------------------------------

TITLE: Nesting InferenceMode States in C++
DESCRIPTION: Demonstration of how InferenceMode states can be nested with different boolean values, allowing for flexible toggling of inference mode within code blocks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/inference_mode.rst#2025-04-22_snippet_1

LANGUAGE: cpp
CODE:
```
{
  InferenceMode guard(true);
  // InferenceMode is on
  {
    InferenceMode guard(false);
    // InferenceMode is off
  }
  // InferenceMode is on
}
// InferenceMode is off
```

----------------------------------------

TITLE: In-place Operations with Named Tensors in PyTorch
DESCRIPTION: Demonstrates how in-place operations propagate names from a named tensor to an unnamed tensor. When adding a named tensor to an unnamed tensor in-place, the unnamed tensor inherits the dimension names from the named tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/name_inference.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> x = torch.randn(3, 3)
>>> y = torch.randn(3, 3, names=('N', 'C'))
>>> x.names
(None, None)

>>> x += y
>>> x.names
('N', 'C')
```

----------------------------------------

TITLE: PyTorch Tensor Copy Operation
DESCRIPTION: This snippet shows a tensor copy operation, where a tensor of shape [128, 3, 224, 224] with f16 precision is copied to another tensor of the same shape.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
Operator: aten.copy_.default
cnt: 1, ((T([128, 3, 224, 224], f16), T([128, 3, 224, 224], f16)), {})
```

----------------------------------------

TITLE: Optimizers with Complex Parameters
DESCRIPTION: Shows how to use optimizers with complex parameters, demonstrating equivalence with real parameter optimization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/complex_numbers.rst#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
params = [torch.rand(2, 3, dtype=torch.complex64) for _ in range(5)]
real_params = [torch.view_as_real(p) for p in params]

complex_optim = torch.optim.AdamW(params)
real_optim = torch.optim.AdamW(real_params)
```

----------------------------------------

TITLE: Analyzing Tensor Copy Operations in PyTorch
DESCRIPTION: This snippet shows a tensor copy operation, which is used to create a new tensor with the same data as an existing tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_10

LANGUAGE: Python
CODE:
```
Operator: aten.copy_.default
cnt: 1, ((T([128, 3, 224, 224], f16), T([128, 3, 224, 224], f16)), {})
```

----------------------------------------

TITLE: Analyzing PyTorch Operator Usage Patterns
DESCRIPTION: This snippet demonstrates the usage patterns of various PyTorch operators in a deep learning model. It includes information about tensor shapes, data types (primarily float16), and the number of times each operator is called with specific configurations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilevit_s_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([64, 1000], f16), 1, False), {})

Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})

Operator: aten._softmax.default
cnt: 2, ((T([256, 4, 256, 256], f16), -1, False), {})
cnt: 4, ((T([256, 4, 64, 64], f16), -1, False), {})
cnt: 3, ((T([256, 4, 16, 16], f16), -1, False), {})

Operator: aten._softmax_backward_data.default
cnt: 3, ((T([256, 4, 16, 16], f16), T([256, 4, 16, 16], f16), -1, f16), {})
cnt: 4, ((T([256, 4, 64, 64], f16), T([256, 4, 64, 64], f16), -1, f16), {})
cnt: 2, ((T([256, 4, 256, 256], f16), T([256, 4, 256, 256], f16), -1, f16), {})

Operator: aten._unsafe_view.default
cnt: 2, ((T([147456, 16, 2, 2], f16), [64, 144, 256, 4]), {})
cnt: 2, ((T([64, 4, 256, 144], f16), [256, 256, 144]), {})
cnt: 6, ((T([256, 4, 256, 36], f16), [1024, 256, 36]), {})
# ... (more entries)

Operator: aten.add.Tensor
cnt: 32, ((T([], i64), 1), {})
cnt: 4, ((T([64, 64, 64, 64], f16), T([64, 64, 64, 64], f16)), {})
cnt: 8, ((T([256, 256, 144], f16), T([256, 256, 144], f16)), {})
# ... (more entries)

Operator: aten.addmm.default
cnt: 2, ((T([432], f16), T([65536, 144], f16), T([144, 432], f16, stride=(1, 144))), {})
cnt: 2, ((T([144], f16), T([65536, 144], f16), T([144, 144], f16, stride=(1, 144))), {})
# ... (more entries)

Operator: aten.bmm.default
cnt: 2, ((T([1024, 256, 36], f16), T([1024, 36, 256], f16)), {})
cnt: 2, ((T([1024, 256, 256], f16), T([1024, 256, 36], f16)), {})
# ... (more entries)

Operator: aten.cat.default
cnt: 1, (([T([64, 96, 32, 32], f16), T([64, 96, 32, 32], f16)], 1), {})
cnt: 1, (([T([64, 128, 16, 16], f16), T([64, 128, 16, 16], f16)], 1), {})
cnt: 1, (([T([64, 160, 8, 8], f16), T([64, 160, 8, 8], f16)], 1), {})

Operator: aten.clone.default
cnt: 1, ((T([64, 3, 256, 256], f16),), {})
cnt: 1, ((T([64, 16, 128, 128], f16),), {})
# ... (more entries)

Operator: aten.convolution.default
cnt: 1, ((T([64, 3, 256, 256], f16), T([16, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([64, 16, 128, 128], f16), T([64, 16, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
# ... (more entries)
```

----------------------------------------

TITLE: Backward Operations for Select and Slice in PyTorch
DESCRIPTION: This snippet shows backward operations for select and slice operations on tensors with various shapes and strides.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
Operator: aten.select_backward.default
cnt: 1, ((T([64, 384], f16), [64, 197, 384], 1, 0), {})

Operator: aten.slice_backward.default
cnt: 1, ((T([64, 196, 384], f16), [64, 197, 384], 1, 1, 9223372036854775807, 1), {})
cnt: 8, ((T([64, 197, 384], f16), [64, 197, 384], 0, 0, 9223372036854775807, 1), {})
cnt: 2, ((T([64, 196, 384], f16, stride=(75648, 384, 1)), [64, 197, 384], 1, 1, 9223372036854775807, 1), {})
cnt: 2, ((T([64, 1, 384], f16), [64, 1, 384], 2, 0, 9223372036854775807, 1), {})
cnt: 4, ((T([64, 1, 384], f16), [64, 197, 384], 1, 0, 1, 1), {})
```

----------------------------------------

TITLE: Configuring Distributed Support for PyTorch Python
DESCRIPTION: Sets up distributed computing support for PyTorch Python bindings, including optional dependencies like NCCL and MPI.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_12

LANGUAGE: CMake
CODE:
```
if(USE_DISTRIBUTED)
    if(WIN32)
      append_filelist("libtorch_python_distributed_core_sources" TORCH_PYTHON_SRCS)
    else()
      append_filelist("libtorch_python_distributed_sources" TORCH_PYTHON_SRCS)
    endif()
    # Disable certain warnings for GCC-9.X
    if(CMAKE_COMPILER_IS_GNUCXX)
      set_source_files_properties(${TORCH_SRC_DIR}/csrc/distributed/autograd/init.cpp PROPERTIES COMPILE_FLAGS "-Wno-cast-function-type")
      set_source_files_properties(${TORCH_SRC_DIR}/csrc/distributed/rpc/testing/init.cpp PROPERTIES COMPILE_FLAGS "-Wno-cast-function-type")
      set_source_files_properties(${TORCH_SRC_DIR}/csrc/distributed/c10d/init.cpp PROPERTIES COMPILE_FLAGS "-Wno-cast-function-type")
    endif()
    # NCCL is a private dependency of libtorch, but libtorch_python includes
    # some private headers of libtorch, which in turn include NCCL. As a hacky
    # alternative to making NCCL a public dependency of libtorch, we make it
    # a private dependency of libtorch_python as well.
    if(USE_NCCL)
      list(APPEND TORCH_PYTHON_LINK_LIBRARIES __caffe2_nccl)
    endif()
    # Same for MPI.
    if(USE_MPI)
      list(APPEND TORCH_PYTHON_LINK_LIBRARIES MPI::MPI_CXX)
    endif()
    list(APPEND TORCH_PYTHON_COMPILE_DEFINITIONS USE_C10D)

endif()
```

----------------------------------------

TITLE: PyTorch Operator Patterns - Tensor Operations
DESCRIPTION: Collection of PyTorch tensor operations including log_softmax, addition, convolution and their corresponding backward passes. Shows detailed tensor shapes, strides and hyperparameters for each operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/legacy_senet154_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
# Example convolution operation
aten.convolution.default((T([32, 3, 224, 224], f16), T([64, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1))

# Example tensor addition
aten.add.Tensor((T([32, 256, 56, 56], f16), T([32, 256, 56, 56], f16)))

# Example backward pass
aten.convolution_backward.default((T([32, 2048, 1, 1], f16), T([32, 128, 1, 1], f16), T([2048, 128, 1, 1], f16), [2048], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]))
```

----------------------------------------

TITLE: Creating AOT Model Compiler with Torch Library Linkage
DESCRIPTION: Creates the Ahead-of-Time model compiler binary target and explicitly links it with the torch library.
SOURCE: https://github.com/pytorch/pytorch/blob/main/binaries/CMakeLists.txt#2025-04-22_snippet_6

LANGUAGE: cmake
CODE:
```
caffe2_binary_target(aot_model_compiler "aot_model_compiler.cc")
target_link_libraries(aot_model_compiler torch)
```

----------------------------------------

TITLE: Scripted Function Calling Traced Function with Disabled Autocast
DESCRIPTION: Example showing a limitation where disabling autocast in a traced function is ignored when called from a scripted function with autocast enabled.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/JIT-AUTOCAST.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
import torch
from torch.cpu.amp import autocast

def helper(a, b):
    with autocast(enabled=False):
        return torch.mm(a, b) * 2.0

traced = torch.jit.trace(helper, (x, y))

@torch.jit.script
def fn(a, b):
    with autocast(enabled=True):
        return traced(a, b)
```

----------------------------------------

TITLE: Tracking Tensor Copy and Clone Operations via ATen in PyTorch (Python)
DESCRIPTION: Monitors usage of ATen copy_ and clone operations to record memory duplication and assignment for model tensors. Dependencies: PyTorch, at least two tensors of matching shapes/dtypes for copy, one for clone. Expects tensors as input and outputs copied or cloned tensors of the same shape/type, highlighting memory management steps in model code. Limitation: Only metadata, not the source implementation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet18_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
Operator: aten.copy_.default
cnt: 1, ((T([16, 3, 224, 224], f16), T([16, 3, 224, 224], f16)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.clone.default
cnt: 1, ((T([16, 3, 224, 224], f16),), {})
```

----------------------------------------

TITLE: Analyzing PyTorch Batch Normalization Operations
DESCRIPTION: This snippet shows the usage patterns of the native_batch_norm_backward operator in PyTorch. It details the tensor shapes, data types, and hyperparameters used across different layers of a neural network.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_13

LANGUAGE: Python
CODE:
```
Operator: aten.native_batch_norm_backward.default
cnt: 1, ((T([32, 1280, 7, 7], f16), T([32, 1280, 7, 7], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f32), T([1280], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([32, 320, 7, 7], f16), T([32, 320, 7, 7], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f32), T([320], f32), False, 1e-05, [True, True, True]), {})
cnt: 8, ((T([32, 1152, 7, 7], f16), T([32, 1152, 7, 7], f16), T([1152], f16), T([1152], f16), T([1152], f16), T([1152], f32), T([1152], f32), False, 1e-05, [True, True, True]), {})
# ... (truncated for brevity)
```

----------------------------------------

TITLE: Exporting a Model using torch.cond
DESCRIPTION: Shows how to export a model that uses torch.cond for further transformations and deployment, including dynamic shape handling.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cond.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
inp = torch.randn(4, 3)
dim_batch = torch.export.Dim("batch", min=2)
ep = torch.export.export(DynamicShapeCondPredicate(), (inp,), {}, dynamic_shapes={"x": {0: dim_batch}})
print(ep)
```

----------------------------------------

TITLE: Concat Loop with Conditional Load
DESCRIPTION: Example demonstrating a potential issue with eager evaluation in concat operations where out-of-bounds access could occur without proper lazy evaluation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/tensorexpr/ConditionalsInTE.md#2025-04-22_snippet_1

LANGUAGE: cpp
CODE:
```
for i = 0 ... N
    store ((i < 16) ? load A[i] : load B[i-16]), C
```

----------------------------------------

TITLE: Specifying Matplotlib Version (Python)
DESCRIPTION: Specifies the required version for the Matplotlib library. Although primarily a plotting library, it may be a dependency for Sphinx extensions or custom scripts used during the documentation build process.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/requirements.txt#_snippet_4

LANGUAGE: Python
CODE:
```
matplotlib==3.5.3
```

----------------------------------------

TITLE: Analyzing ATen Tensor Addition in PyTorch
DESCRIPTION: Tracks operations involving the \"aten.add.Tensor\" operator, which performs element-wise addition of tensors, listing various tensor shapes and data types involved.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_6

LANGUAGE: pseudocode
CODE:
```
Operator: aten.add.Tensor
cnt: 1, ((T([128], i64), 1), {})
cnt: 1, ((T([8, 128, 1024], f16), T([128, 1024], f16)), {})
cnt: 12, ((T([8, 16, 128, 128], f16), T([8, 1, 128, 128], f16)), {})
cnt: 72, ((T([8, 128, 1024], f16), T([8, 128, 1024], f16)), {})
cnt: 1, ((T([50265, 1024], f16), T([50265, 1024], f16)), {})
```

----------------------------------------

TITLE: Comparing HVP Implementations in PyTorch
DESCRIPTION: Verifies that both HVP implementations (forward-reverse and reverse-reverse) produce the same result by comparing their outputs on the same inputs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_25

LANGUAGE: python
CODE:
```
result_hvp_revrev = hvp_revrev(f, (x,), (tangent,))
assert torch.allclose(result, result_hvp_revrev[0])
```

----------------------------------------

TITLE: NLL Loss Operations
DESCRIPTION: Negative log likelihood loss calculations for classification tasks with 1000 output classes. Handles both forward and backward passes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_efficientnet_b0_training.txt#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
((T([128, 1000], f16), T([128], i64), None, 1, -100), {})
```

----------------------------------------

TITLE: Analyzing ATen GELU Backward Operations in PyTorch
DESCRIPTION: Records instances of \"aten.gelu_backward.default\", detailing how gradients are calculated during backpropagation for GELU activated layers.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_14

LANGUAGE: pseudocode
CODE:
```
Operator: aten.gelu_backward.default
cnt: 12, ((T([8, 128, 4096], f16), T([8, 128, 4096], f16)), {})
```

----------------------------------------

TITLE: Analyzing ATen Native Layer Normalization in PyTorch
DESCRIPTION: Tracks \"aten.native_layer_norm.default\", detailing its computation and use of additional parameters for tensor normalization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_19

LANGUAGE: pseudocode
CODE:
```
Operator: aten.native_layer_norm.default
cnt: 25, ((T([8, 128, 1024], f16), [1024], T([1024], f16), T([1024], f16), 1e-05), {})
```

----------------------------------------

TITLE: Installing Linux Dependencies (Triton) Bash
DESCRIPTION: Builds and installs the Triton compiler using 'make triton', which is an optional but recommended dependency for enabling the torch.compile feature with backends like Inductor. Must be run from the PyTorch source directory.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_6

LANGUAGE: Bash
CODE:
```
# (optional) If using torch.compile with inductor/triton, install the matching version of triton
# Run from the pytorch directory after cloning
# For Intel GPU support, please explicitly `export USE_XPU=1` before running command.
make triton
```

----------------------------------------

TITLE: Documenting Pure Function Schema in PyTorch JIT
DESCRIPTION: Defines a JIT FunctionSchema for a pure `add` operation. The lack of annotations on the `Tensor` types indicates that the function produces a fresh, read-only Tensor output, creating no aliases and performing no mutations on inputs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_34

LANGUAGE: JIT FunctionSchema
CODE:
```
add(Tensor a, Tensor b) -> Tensor
```

----------------------------------------

TITLE: Enabling Broadcasting Backward Compatibility Warning in Python
DESCRIPTION: Shows how to enable a warning to identify potential backward incompatibilities caused by the introduction of broadcasting. Setting `torch.utils.backcompat.broadcast_warning.enabled` to `True` will generate a `UserWarning` when an operation involves tensors that are not the same shape but are broadcastable and have the same number of elements, indicating a potential behavior change compared to older PyTorch versions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/broadcasting.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
>>> torch.utils.backcompat.broadcast_warning.enabled=True
>>> torch.add(torch.ones(4,1), torch.ones(4))
__main__:1: UserWarning: self and other do not have the same shape, but are broadcastable, and have the same number of elements.
Changing behavior in a backwards incompatible manner to broadcasting rather than viewing as 1-dimensional.
```

----------------------------------------

TITLE: Importing Python Extension Error Example
DESCRIPTION: Demonstrates an ImportError that occurs when importing a C++ extension without first importing torch, showing undefined symbol errors from PyTorch/ATen.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/faq.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
>>> import extension
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ImportError: /home/user/.pyenv/versions/3.7.1/lib/python3.7/site-packages/extension.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZN2at19UndefinedTensorImpl10_singletonE
```

----------------------------------------

TITLE: Configuring C++ and C Standards (CMake)
DESCRIPTION: Sets the required C++ standard to 17 and the C standard to 11 for the project, caching these values. It also includes a warning if C++ standard flags are found in environment variables, advising the user to remove them to ensure the build uses the enforced standard.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#_snippet_3

LANGUAGE: CMake
CODE:
```
# check and set CMAKE_CXX_STANDARD
string(FIND "${CMAKE_CXX_FLAGS}" "-std=c++" env_cxx_standard)
if(env_cxx_standard GREATER -1)
  message(
    WARNING
      "C++ standard version definition detected in environment variable."
      "PyTorch requires -std=c++17. Please remove -std=c++ settings in your environment."
  )
endif()
set(CMAKE_CXX_STANDARD
    17
    CACHE STRING
          "The C++ standard whose features are requested to build this target.")
set(CMAKE_C_STANDARD
    11
    CACHE STRING
          "The C standard whose features are requested to build this target.")
```

----------------------------------------

TITLE: Optimized MulConstant with Gradient Materialization Control
DESCRIPTION: Enhanced version of MulConstant that optimizes gradient computation by controlling gradient materialization and handling None gradients.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
class MulConstant(Function):
    @staticmethod
    def forward(tensor, constant):
        return tensor * constant

    @staticmethod
    def setup_context(ctx, inputs, output):
        tensor, constant = inputs
        ctx.set_materialize_grads(False)
        ctx.constant = constant

    @staticmethod
    def backward(ctx, grad_output):
        # Here we must handle None grad_output tensor. In this case we
        # can skip unnecessary computations and just return None.
        if grad_output is None:
            return None, None

        # We return as many input gradients as there were arguments.
        # Gradients of non-Tensor arguments to forward must be None.
        return grad_output * ctx.constant, None
```

----------------------------------------

TITLE: Configuring BatchNorm2d Without Running Stats in PyTorch
DESCRIPTION: Creates a BatchNorm2d layer with track_running_stats set to False, preventing the module from using running mean and variance statistics that cause issues with functorch's vmap.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/batch_norm.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
BatchNorm2d(64, track_running_stats=False)
```

----------------------------------------

TITLE: Reshaping Tensors with aten._unsafe_view.default
DESCRIPTION: Records usage of the _unsafe_view.default operator for fast tensor reshaping operations. This operator is primarily used to reshape feature maps for YOLO object detection, converting 5D tensors to 4D tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
Operator: aten._unsafe_view.default
cnt: 1, ((T([8, 3, 85, 48, 64], f16), [8, 255, 48, 64]), {})
cnt: 1, ((T([8, 3, 85, 24, 32], f16), [8, 255, 24, 32]), {})
cnt: 1, ((T([8, 3, 85, 12, 16], f16), [8, 255, 12, 16]), {})
```

----------------------------------------

TITLE: Enabling Full FP16 Accumulation CuBLAS (C++)
DESCRIPTION: Provides the C++ code snippet to enable full FP16 accumulation specifically for FP16 matrix multiplications within the CuBLAS backend via the global context. This setting can improve performance on compatible hardware.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_10

LANGUAGE: cpp
CODE:
```
at::globalContext().setAllowFP16AccumulationCuBLAS(true);
```

----------------------------------------

TITLE: Evaluating Disk Savings for Sparsified DLRM Models in Python
DESCRIPTION: Python script to evaluate disk savings by sparsifying the DLRM model with various configurations. It generates sparsified model checkpoints and metadata.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/benchmarks/README.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
python evaluate_disk_savings.py --model-path=<path_to_model_checkpoint> --sparsified-model-dump-path=<path_to_dump_sparsified_models>
```

----------------------------------------

TITLE: Analyzing ATen Tensor Addition Operations in PyTorch
DESCRIPTION: This code snippet describes the use of the ATen add tensor operation, which sums two tensors element-wise. It indicates different scenarios with varying dimensions and shapes, showcasing its frequent invocation in handling tensor data, especially in computation involving neural networks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistilBertForMaskedLM_training.txt#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
Operator: aten.add.Tensor
cnt: 1, ((T([16, 128, 768], f16), T([1, 128, 768], f16)), {})
cnt: 36, ((T([16, 128, 768], f16), T([16, 128, 768], f16)), {})
cnt: 1, ((T([30522, 768], f16), T([30522, 768], f16)), {})
```

----------------------------------------

TITLE: Enabling Health Check Server in LocalElasticAgent
DESCRIPTION: Sets up an environment variable to enable a health check monitoring server in LocalElasticAgent. The TORCHELASTIC_HEALTH_CHECK_PORT variable must be defined with a port number.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/agent.rst#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
import os

os.environ["TORCHELASTIC_HEALTH_CHECK_PORT"] = "8080"
```

----------------------------------------

TITLE: Loading Data from Map-style Dataset without Batching
DESCRIPTION: Demonstrates the equivalent operation of loading individual samples from a map-style dataset when automatic batching is disabled.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/data.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
for index in sampler:
    yield collate_fn(dataset[index])
```

----------------------------------------

TITLE: Simplified TensorOptions with Free Functions
DESCRIPTION: Demonstrates the shorter syntax using free functions for tensor options.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_9

LANGUAGE: cpp
CODE:
```
torch::ones(10, torch::dtype(torch::kFloat32))
```

----------------------------------------

TITLE: Creating Symbolic Link for libtorch_cpu (Bash)
DESCRIPTION: This bash command sequence navigates to the `torch/lib` directory, creates a symbolic link from the built `libtorch_cpu` library in the `build` directory back to `torch/lib`, and then returns to the original directory. This allows changes to the C++ library to be reflected without `python setup.py develop` after every change.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_1

LANGUAGE: bash
CODE:
```
pushd torch/lib; sh -c "ln -sf ../../build/lib/libtorch_cpu.* ."; popd
```

----------------------------------------

TITLE: Unsupported Write Indexing with Implicit Broadcasting (Python)
DESCRIPTION: Shows an unsupported tensor indexing pattern for writing data where the `new_data` requires implicit broadcasting that the ONNX exporter cannot handle. The workaround is to explicitly expand the `new_data` to the expected shape before assignment.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_13

LANGUAGE: python
CODE:
```
data[torch.tensor([[0, 2], [1, 1]]), 1:3] = new_data
# Workarounds: expand new_data explicitly.
# Example:
#   data shape: [3, 4, 5]
#   new_data shape: [5]
#   expected new_data shape after broadcasting: [2, 2, 2, 5]
```

----------------------------------------

TITLE: Mixed Dimension Transposition in PyTorch
DESCRIPTION: Demonstrates working with mixed positional and first-class dimensions during transposition.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
B = torch.rand(3, 4, 5)
B_T = B[i, j].order(j, i)
assert torch.allclose(B.permute(1, 0, 2), B_T)
```

----------------------------------------

TITLE: Clear cuFFT Plan Cache PyTorch CUDA
DESCRIPTION: Clears the cuFFT plan cache for the default CUDA device. This can be used to free up memory or reset the cache state.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_32

LANGUAGE: python
CODE:
```
torch.backends.cuda.cufft_plan_cache.clear()
```

----------------------------------------

TITLE: Downloading MKL/MAGMA and Setting Environment Variables (BAT)
DESCRIPTION: This snippet provides batch commands to download necessary MKL and MAGMA libraries using `curl` and `7z`, and then sets environment variables like `CMAKE_INCLUDE_PATH`, `LIB`, and `MAGMA_HOME` to point to the downloaded locations for use during the PyTorch build process. Requires `7z` and `curl` to be installed.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#_snippet_0

LANGUAGE: bat
CODE:
```
REM Make sure you have 7z and curl installed.

REM Download MKL files
curl https://s3.amazonaws.com/ossci-windows/mkl_2020.2.254.7z -k -O
7z x -aoa mkl_2020.2.254.7z -omkl

REM Download MAGMA files
REM version available:
REM 2.5.4 (CUDA 10.1 10.2 11.0 11.1) x (Debug Release)
REM 2.5.3 (CUDA 10.1 10.2 11.0) x (Debug Release)
REM 2.5.2 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release)
REM 2.5.1 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release)
set "CUDA_PREFIX=cuda102"
set "CONFIG=release"
set "HOST=https://s3.amazonaws.com/ossci-windows"
curl -k "%HOST%/magma_2.5.4_%CUDA_PREFIX%_%CONFIG%.7z" -o magma.7z
7z x -aoa magma.7z -omagma

REM Setting essential environment variables
set "CMAKE_INCLUDE_PATH=%cd%\mkl\include"
set "LIB=%cd%\mkl\lib;%LIB%"
set "MAGMA_HOME=%cd%\magma"
```

----------------------------------------

TITLE: Limitation with Runtime Autocast Enable Argument
DESCRIPTION: Example demonstrating that runtime values cannot be used for the 'enabled' parameter in autocast context managers within JIT scripted functions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/JIT-AUTOCAST.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
import torch
from torch.cpu.amp import autocast

@torch.jit.script
def fn(a, b, use_amp: bool):
    # runtime values for autocast enable argument are not supported
    with autocast(enabled=use_amp):
        return torch.mm(a, b)
```

----------------------------------------

TITLE: Packaging a TorchScript Module
DESCRIPTION: Demonstrates how to package a TorchScript model using the same save_pickle and load_pickle APIs as with any other object, including support for TorchScript objects as attributes or submodules.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
# save TorchScript just like any other object
with PackageExporter(file_name) as e:
    e.save_pickle("res", "script_model.pkl", scripted_model)
    e.save_pickle("res", "mixed_model.pkl", python_model_with_scripted_submodule)
# load as normal
importer = PackageImporter(file_name)
loaded_script = importer.load_pickle("res", "script_model.pkl")
loaded_mixed = importer.load_pickle("res", "mixed_model.pkl")
```

----------------------------------------

TITLE: Setting API Usage Logger in PyTorch C++
DESCRIPTION: This C++ snippet sets up an API usage logger for PyTorch, allowing developers to track which APIs are invoked. By calling c10::SetAPIUsageHandler, developers can register a logging callback function that outputs API use events to standard error output. This is particularly useful for monitoring and auditing API usage in managed environments.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/large_scale_deployments.rst#2025-04-22_snippet_1

LANGUAGE: cpp
CODE:
```
SetAPIUsageLogger([](const std::string& event_name) {
    std::cerr << "API was used: " << event_name << std::endl;
});
```

----------------------------------------

TITLE: Syntactic Sugar for Catch-All Function Kernel Registration (C++)
DESCRIPTION: Demonstrates a simplified syntax for registering a catch-all kernel function (`my_kernel_cpu`). Instead of using `.options().catchAllKernel()`, the function pointer is passed directly as the second argument to `.op()`. This implicitly registers the function as a catch-all kernel.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_3

LANGUAGE: cpp
CODE:
```
namespace { Tensor my_kernel_cpu(const Tensor& a, const Tensor& b) {...}

static auto registry = torch::RegisterOperators()
 .op("my_namespace::my_op", &my_kernel_cpu);
```

----------------------------------------

TITLE: Triggering Custom Backend Error (Python)
DESCRIPTION: This snippet defines a simple PyTorch model, a custom backend compiler `toy_compiler` that asserts False if `torch.relu` is found in the graph, and a test function that uses `torch.relu`. It compiles the test function using `torch.compile` with the custom backend to demonstrate how to trigger errors specifically within a non-TorchInductor backend compilation process for debugging. Requires `torch` and `torch._dynamo`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting_old.rst#_snippet_3

LANGUAGE: python
CODE:
```
import torch

import torch._dynamo as dynamo

model = torch.nn.Sequential(*[torch.nn.Linear(200, 200) for _ in range(5)])
# toy compiler which fails if graph contains relu
def toy_compiler(gm: torch.fx.GraphModule, _):
    for node in gm.graph.nodes:
        if node.target == torch.relu:
            assert False

    return gm


def test_backend_error():
    y = torch.ones(200, 200)
    x = torch.ones(200, 200)
    z = x + y
    a = torch.relu(z)
    return model(a)


compiled_test_backend_error = torch.compile(test_backend_error, backend=toy_compiler)
compiled_test_backend_error()
```

----------------------------------------

TITLE: Foreach Operations List in RestructuredText
DESCRIPTION: A list of PyTorch's foreach operations that apply functions to multiple tensors simultaneously for better performance. These operations are in beta and do not support Forward-mode automatic differentiation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.rst#2025-04-22_snippet_5

LANGUAGE: restructuredtext
CODE:
```
.. warning::
    This API is in beta and subject to future changes.
    Forward-mode AD is not supported.

.. autosummary::
    :toctree: generated
    :nosignatures:

    _foreach_abs
    _foreach_abs_
    _foreach_acos
    _foreach_acos_
    _foreach_asin
    _foreach_asin_
    _foreach_atan
    _foreach_atan_
    _foreach_ceil
    _foreach_ceil_
    _foreach_cos
    _foreach_cos_
    _foreach_cosh
    _foreach_cosh_
    _foreach_erf
    _foreach_erf_
    _foreach_erfc
    _foreach_erfc_
    _foreach_exp
    _foreach_exp_
    _foreach_expm1
    _foreach_expm1_
    _foreach_floor
    _foreach_floor_
    _foreach_log
    _foreach_log_
    _foreach_log10
    _foreach_log10_
    _foreach_log1p
    _foreach_log1p_
    _foreach_log2
    _foreach_log2_
    _foreach_neg
    _foreach_neg_
    _foreach_tan
    _foreach_tan_
    _foreach_sin
    _foreach_sin_
    _foreach_sinh
    _foreach_sinh_
    _foreach_round
    _foreach_round_
    _foreach_sqrt
    _foreach_sqrt_
    _foreach_lgamma
    _foreach_lgamma_
    _foreach_frac
    _foreach_frac_
    _foreach_reciprocal
    _foreach_reciprocal_
    _foreach_sigmoid
    _foreach_sigmoid_
    _foreach_trunc
    _foreach_trunc_
    _foreach_zero_
```

----------------------------------------

TITLE: Tensor Concatenation Operations in PyTorch (14x14 Feature Maps)
DESCRIPTION: PyTorch's tensor concatenation operations for 14x14 feature maps along dimension 1 (channel dimension). These progressively growing concatenation operations show the dense layer patterns where each new 32-channel feature map is added to an increasingly larger collection.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
cnt: 1, (([T([64, 256, 14, 14], f16)], 1), {})
cnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})
cnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})
cnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})
cnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})
cnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})
cnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})
cnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})
cnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})
cnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})
cnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})
cnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})
cnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})
cnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})
cnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})
cnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})
cnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})
```

----------------------------------------

TITLE: Embedding Operations with aten.embedding
DESCRIPTION: Embeddings of input indices into higher dimensional spaces using aten.embedding.default rely on structured matrices. Requires PyTorch support to map indices into rows of an embedding matrix, resulting in projected outputs maintaining index matrix shapes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForConditionalGeneration_training.txt#2025-04-22_snippet_11

LANGUAGE: Python
CODE:
```
Operator: aten.embedding.default
cnt: 2, ((T([50005, 768], f16), T([8, 128], i64), 1), {})
cnt: 2, ((T([1026, 768], f16), T([8, 128], i64)), {})
```

----------------------------------------

TITLE: Defining CUDA Function in c10/cuda
DESCRIPTION: Example of defining a CUDA function in c10/cuda namespace, which will be transpiled to c10/hip for AMD GPU builds. This demonstrates the namespace and function declaration pattern.
SOURCE: https://github.com/pytorch/pytorch/blob/main/c10/cuda/README.md#2025-04-22_snippet_0

LANGUAGE: C++
CODE:
```
// c10/cuda/CUDAFoo.h
namespace c10 { namespace cuda {

void my_func();

}}
```

----------------------------------------

TITLE: FileCheck Builder API Example
DESCRIPTION: Shows how to use the FileCheck builder API to programmatically create test assertions instead of using string annotations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/HowToWriteTestsUsingFileCheck.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
FileCheck().check("aten::mul")     \
           .check_not("aten::mul") \
           .check("return")        \
           .run(optimized)
```

----------------------------------------

TITLE: Creating PyTorch MemPool with Custom Allocator
DESCRIPTION: This simple snippet demonstrates how to instantiate a `torch.cuda.MemPool` object by passing the custom pluggable allocator obtained from the previous steps. This pool will manage memory allocations using the specified allocator functions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_27

LANGUAGE: python
CODE:
```
pool = torch.cuda.MemPool(allocator)
```

----------------------------------------

TITLE: Handling Unsafe Globals in NumPy >= 1.25 for PyTorch Serialization
DESCRIPTION: Example of an error message when encountering unsafe globals with NumPy >= 1.25 during PyTorch serialization, and how to allowlist the problematic type.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
WeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`,
but got <class 'numpy.dtypes.Float32DType'>
```

----------------------------------------

TITLE: Accessing Gradient Storage in PyTorch
DESCRIPTION: Example showing how to access both the data and gradient storage of a tensor that requires gradients. Demonstrates that tensors with gradients maintain two separate storage objects.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/storage.rst#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
t = torch.zeros(3, requires_grad=True)
t.sum().backward()
assert list(t.untyped_storage()) == [0] * 12  # the storage of the tensor is just 0s
assert list(t.grad.untyped_storage()) != [0] * 12  # the storage of the gradient isn't
```

----------------------------------------

TITLE: Enabling Watchdog in LocalElasticAgent
DESCRIPTION: Sets up environment variables to enable a named pipe based watchdog in LocalElasticAgent. The TORCHELASTIC_ENABLE_FILE_TIMER variable must be set to 1, and optionally TORCHELASTIC_TIMER_FILE can specify a unique file name for the named pipe.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/agent.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
import os

os.environ["TORCHELASTIC_ENABLE_FILE_TIMER"] = "1"
os.environ["TORCHELASTIC_TIMER_FILE"] = "unique_pipe_name"  # Optional
```

----------------------------------------

TITLE: Defining DTypeConfig for PyTorch Quantization Backend - Python
DESCRIPTION: This snippet demonstrates how to create DTypeConfig objects in PyTorch using simple dtypes and the more detailed DTypeWithConstraints class. DTypeConfig specifies the allowed data types (and optional constraints like quantization and scale ranges) for quantized inputs, outputs, weights, and biases for operators in a quantization backend, determining if a QConfig is valid.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/backend_config/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
import torch
from torch.ao.quantization.backend import DTypeConfig, DTypeWithConstraints

dtype_config = DTypeConfig(
    input_dtype=torch.quint8,
    output_dtype=torch.quint8,
    weight_dtype=torch.qint8,
    bias_dtype=torch.float)

dtype_config_with_constraints = DTypeConfig(
    input_dtype=DTypeWithConstraints(
        dtype=torch.quint8,
        quant_min_lower_bound=0,
        quant_max_upper_bound=255,
        scale_min_lower_bound=2 ** -12,
    ),
    output_dtype=DTypeWithConstraints(
        dtype=torch.quint8,
        quant_min_lower_bound=0,
        quant_max_upper_bound=255,
        scale_min_lower_bound=2 ** -12,
    ),
    weight_dtype=DTypeWithConstraints(
        dtype=torch.qint8,
        quant_min_lower_bound=-128,
        quant_max_upper_bound=127,
        scale_min_lower_bound=2 ** -12,
    ),
    bias_dtype=torch.float)
```

----------------------------------------

TITLE: Performing Backward Pass for Batch Normalization
DESCRIPTION: Computes the gradients of the inputs for the backward pass of batch normalization, essential for training neural networks. It requires input and gradient tensors with matching dimensions and appropriate momentum and epsilon settings. Outputs are gradient tensors used for updating weights based on the backward propagation algorithm.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
Operator: aten.native_batch_norm_backward.default
cnt: 2, ((T([1, 1024, 128, 128], f16), T([1, 1024, 128, 128], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), False, 1e-05, [True, True, True]), {})
cnt: 2, ((T([1, 2048, 64, 64], f16), T([1, 2048, 64, 64], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f32), T([2048], f32), False, 1e-05, [True, True, True]), {})
cnt: 13, ((T([1, 4096, 32, 32], f16), T([1, 4096, 32, 32], f16), T([4096], f16), T([4096], f16), T([4096], f16), T([4096], f32), T([4096], f32), False, 1e-05, [True, True, True]), {})
```

----------------------------------------

TITLE: Basic Arithmetic Operations: Addition and Multiplication in PyTorch (Python)
DESCRIPTION: These represent batched elementwise add and mul operations between PyTorch tensors, as well as in-place variants (add_). Inputs and outputs preserve dtype and shape where relevant; supported dtypes include FP16 and integer. Operations may influence gradient flow if used in model parameter updates.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
Operator: aten.add.Tensor
cnt: 1, ((T([4, 128], i32), 0), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.add.Tensor
cnt: 1, ((T([4, 128], i64), 0), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.add.Tensor
cnt: 73, ((T([4, 128, 768], f16), T([4, 128, 768], f16)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.add.Tensor
cnt: 12, ((T([4, 12, 128, 128], f16), T([4, 1, 1, 128], f16)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.add.Tensor
cnt: 1, ((T([30522, 768], f16), T([30522, 768], f16)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.add_.Tensor
cnt: 1, ((T([4, 128, 768], f16), T([4, 128, 768], f16)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.mul.Tensor
cnt: 1, ((T([4, 1, 1, 128], f16), -65504.0), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.mul.Tensor
cnt: 1, ((T([4, 128], i32), T([4, 128], i32)), {})
```

----------------------------------------

TITLE: Inspecting Shapes and Type of Extracted Model Parameters
DESCRIPTION: Iterates through the `params` tuple (containing the model's learnable parameters extracted by `make_functional_with_buffers`) and prints the shape of each parameter tensor (weights and biases). It also prints the Python type of the `params` variable itself, confirming it is a tuple.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
for x in params:
  print(f"{x.shape}")

print(f"\n{type(params)}")
```

----------------------------------------

TITLE: Matrix Multiplication Operation in PyTorch Neural Network
DESCRIPTION: Summary of matrix multiplication operation (aten.addmm.default) in the model, showing count, tensor shapes and strides. This likely represents a fully connected layer processing the final features into class probabilities.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_regnet_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
Operator: aten.addmm.default
cnt: 1, ((T([1000], f16), T([32, 2240], f16), T([2240, 1000], f16, stride=(1, 2240))), {})
```

----------------------------------------

TITLE: Generated Triton Kernel with Injected Error
DESCRIPTION: The Triton kernel code generated with an intentionally injected error in the ReLU operation, demonstrating the syntax error that will be caught by the minifier.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_aot_inductor_minifier.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
@triton.jit
def triton_poi_fused_addmm_relu_sigmoid_0(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = xindex % 16
    tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tmp0 + tmp1
    tmp3 = compile error!
    tmp4 = tl.sigmoid(tmp3)
    tl.store(in_out_ptr0 + (x2), tmp4, xmask)
```

----------------------------------------

TITLE: Registering custom sharding with experimental register_sharding
DESCRIPTION: Registers a custom sharding function for a specific operator using the experimental register_sharding decorator.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.tensor.rst#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
@register_sharding(torch.nn.Linear)
def sharded_linear(types, args, kwargs, default_sharding):
    # Custom sharding logic here
```

----------------------------------------

TITLE: Defining Distributed Autograd Test Executable in CMake
DESCRIPTION: This CMake script conditionally defines and configures the 'test_dist_autograd' executable if USE_DISTRIBUTED is enabled and the target platform is not Windows (WIN32). It specifies the source files, includes necessary directories (ATen_CPU_INCLUDE), links against 'torch' and 'gtest' libraries. It also conditionally adds a 'USE_CUDA' compile definition if USE_CUDA is enabled. Finally, if INSTALL_TEST is set, it configures installation rules for the executable and its PDB file (on MSVC).
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/dist_autograd/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
if(USE_DISTRIBUTED AND NOT WIN32)
  set(DIST_AUTOGRAD_TEST_DIR "${TORCH_ROOT}/test/cpp/dist_autograd")
  set(DIST_AUTOGRAD_TEST_SOURCES
    ${TORCH_ROOT}/test/cpp/common/main.cpp
    ${DIST_AUTOGRAD_TEST_DIR}/test_dist_autograd.cpp
  )

  add_executable(test_dist_autograd ${DIST_AUTOGRAD_TEST_SOURCES})
  target_include_directories(test_dist_autograd PRIVATE ${ATen_CPU_INCLUDE})
  target_link_libraries(test_dist_autograd PRIVATE torch gtest)

  if(USE_CUDA)
    target_compile_definitions(test_dist_autograd PRIVATE "USE_CUDA")
  endif()

  if(INSTALL_TEST)
    set_target_properties(test_dist_autograd PROPERTIES INSTALL_RPATH "${CMAKE_INSTALL_RPATH}:${_rpath_portable_origin}/../lib")
    install(TARGETS test_dist_autograd DESTINATION bin)
    # Install PDB files for MSVC builds
    if(MSVC AND BUILD_SHARED_LIBS)
      install(FILES $<TARGET_PDB_FILE:test_dist_autograd> DESTINATION bin OPTIONAL)
    endif()
  endif()
endif()
```

----------------------------------------

TITLE: Tracing a Function with jit.trace in PyTorch
DESCRIPTION: This code snippet demonstrates how to use torch.jit.trace to create a traced version of the 'add_two_maybe' function. It also shows a case where the trace works correctly for the initial inputs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
t = torch.ones(1)
maybe_false = torch.BoolTensor([0])
good_inputs = (t, maybe_false)
jit = torch.jit.trace(add_two_maybe, good_inputs)
# let's check that the results match with eager
assert jit(*good_inputs) == add_two_maybe(*good_inputs)
```

----------------------------------------

TITLE: Exploring Package Contents Using Unzip Command
DESCRIPTION: Shows how to use the unzip command to extract and inspect torch package contents from the command line. The example demonstrates extracting a package and viewing its directory structure.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
$ unzip my_package.pt && tree my_package
my_package
├── .data
│   ├── 94304870911616.storage
│   ├── 94304900784016.storage
│   ├── extern_modules
│   └── version
├── models
│   └── model_1.pkl
└── torchvision
    └── models
        ├── resnet.py
        └── utils.py
~ cd my_package && cat torchvision/models/resnet.py
...
```

----------------------------------------

TITLE: Apply aten.nll_loss_forward in Python
DESCRIPTION: Utilizes the negative log-likelihood loss function in PyTorch for forward operations, often applicable in classification tasks. Essential dependencies include the PyTorch framework. It handles a softmax probability output against labels, providing a single loss scalar value. This operation is typical in training neural networks using cross-entropy loss.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 1000], f16), T([64], i64), None, 1, -100), {})
```

----------------------------------------

TITLE: PyTorch Leaky ReLU Operations
DESCRIPTION: Leaky ReLU activations with slope 0.1 applied to tensors of various sizes. Includes both forward and backward operations for gradient computation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
((T([8, 32, 384, 512], f16), 0.1), {})
```

----------------------------------------

TITLE: Creating PyTorch Test Script (Python)
DESCRIPTION: Provides a simple Python script designed to test and profile a specific PyTorch operation, `torch.add` in this case. The operation is repeated many times to ensure sufficient samples for profiling statistics.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_37

LANGUAGE: python
CODE:
```
import torch

t1 = torch.tensor([[1, 1], [1, 1.]])
t2 = torch.tensor([[0, 0], [0, 0.]])

for _ in range(1000000):
    torch.add(t1, t2)
```

----------------------------------------

TITLE: torch.full Behavior in PyTorch 1.7
DESCRIPTION: Shows how torch.full infers the returned tensor's dtype from the fill value in PyTorch 1.7.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
# PyTorch 1.7
>>> torch.full((3,), 1)
tensor([1, 1, 1])

>>> torch.full((3,), True)
tensor([True, True, True])

>>> torch.full((3,), 1.)
tensor([1., 1., 1.])

>>> torch.full((3,), 1 + 1j)
tensor([1.+1.j, 1.+1.j, 1.+1.j])
```

----------------------------------------

TITLE: Simple Data Sparsification Example in PyTorch
DESCRIPTION: Basic usage of a data sparsifier to sparsify a regular tensor and a parameter. Shows how to add data to the sparsifier, compute the mask, and apply the mask to the data.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/README.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
tensor1 = torch.randn(100, 100)
param1 = nn.Parameter(torch.randn(200, 32))

my_sparsifier = ImplementedDataSparsifier(threshold=0.2)
my_sparsifier.add_data(name='tensor1', data=tensor1, threshold=0.5)
my_sparsifier.add_data(name='param1', data=param1)

my_sparsifier.step()  # computes mask

my_sparsifier.squash_mask()  # applies and removes mask
```

----------------------------------------

TITLE: Customizing Class Packaging with __reduce_package__
DESCRIPTION: Demonstrates how to customize the packaging of a class by implementing __reduce_package__ and a corresponding unpackaging function. This allows for custom serialization behavior for class instances.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
# foo.py [Example of customizing how class Foo is packaged]
from torch.package import PackageExporter, PackageImporter
import time


class Foo:
    def __init__(self, my_string: str):
        super().__init__()
        self.my_string = my_string
        self.time_imported = 0
        self.time_exported = 0

    def __reduce_package__(self, exporter: PackageExporter):
        """
        Called by ``torch.package.PackageExporter``'s Pickler's ``persistent_id`` when
        saving an instance of this object. This method should do the work to save this
        object inside of the ``torch.package`` archive.

        Returns function w/ arguments to load the object from a
        ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function.
        """

        # use this pattern to ensure no naming conflicts with normal dependencies,
        # anything saved under this module name shouldn't conflict with other
        # items in the package
        generated_module_name = f"foo-generated._{exporter.get_unique_id()}"
        exporter.save_text(
            generated_module_name,
            "foo.txt",
            self.my_string + ", with exporter modification!",
        )
        time_exported = time.clock_gettime(1)

        # returns de-packaging function w/ arguments to invoke with
        return (unpackage_foo, (generated_module_name, time_exported,))


def unpackage_foo(
    importer: PackageImporter, generated_module_name: str, time_exported: float
) -> Foo:
    """
    Called by ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function
    when depickling a Foo object.
    Performs work of loading and returning a Foo instance from a ``torch.package`` archive.
    """
    time_imported = time.clock_gettime(1)
    foo = Foo(importer.load_text(generated_module_name, "foo.txt"))
    foo.time_imported = time_imported
    foo.time_exported = time_exported
    return foo
```

----------------------------------------

TITLE: Performance Comparison Helper Function for Benchmarks - Python
DESCRIPTION: Defines 'get_perf' to compare timing results between two torch.benchmark Timer runs. Prints percent improvement for the faster method. Parameters: two timing objects and descriptor strings. Prerequisite: requires Torch Benchmark library.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
def get_perf(first, first_descriptor, second, second_descriptor):
  """  takes torch.benchmark objects and compares delta of second vs first. """
  faster = second.times[0]
  slower = first.times[0]
  gain = (slower-faster)/slower
  if gain < 0: gain *=-1 
  final_gain = gain*100
  print(f" Performance delta: {final_gain:.4f} percent improvement with {second_descriptor} ")
```

----------------------------------------

TITLE: Saving Resources to a Package
DESCRIPTION: Demonstrates how to save various types of resources (pickle objects, text, and binary data) to a torch package using save_pickle, save_text, and save_binary methods.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
with torch.PackageExporter("package.pt") as exporter:
    # Pickles the object and saves to `my_resources/tensor.pkl` in the archive.
    exporter.save_pickle("my_resources", "tensor.pkl", torch.randn(4))
    exporter.save_text("config_stuff", "words.txt", "a sample string")
    exporter.save_binary("raw_data", "binary", my_bytes)
```

----------------------------------------

TITLE: Reporting ATen Operator Usage Patterns - PyTorch - Python
DESCRIPTION: Lists a structured tally of PyTorch ATen operator invocations, including tensor shapes, data types, and parameters per operator instance. This covers elementwise (add, div), memory ops (clone, copy), convolutions, normalizations, and activations, showing usage diversity and architectural depth. Inputs specify tensor type abbreviations (T), shapes, dtypes, and parameter lists, supporting downstream analysis for optimization, kernel testing, or code generation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_CycleGAN_and_pix2pix_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
Operator: aten.add.Tensor
cnt: 18, ((T([1, 256, 64, 64], f16), T([1, 256, 64, 64], f16)), {})
Operator: aten.clone.default
cnt: 1, ((T([1, 3, 256, 256], f16),), {})
Operator: aten.convolution.default
cnt: 1, ((T([1, 3, 262, 262], f16), T([64, 3, 7, 7], f16), T([64], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([1, 64, 256, 256], f16), T([128, 64, 3, 3], f16), T([128], f16), [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([1, 128, 128, 128], f16), T([256, 128, 3, 3], f16), T([256], f16), [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 18, ((T([1, 256, 66, 66], f16), T([256, 256, 3, 3], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([1, 256, 64, 64], f16), T([256, 128, 3, 3], f16), T([128], f16), [2, 2], [1, 1], [1, 1], True, [1, 1], 1), {})
cnt: 1, ((T([1, 128, 128, 128], f16), T([128, 64, 3, 3], f16), T([64], f16), [2, 2], [1, 1], [1, 1], True, [1, 1], 1), {})
cnt: 1, ((T([1, 64, 262, 262], f16), T([3, 64, 7, 7], f16), T([3], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
Operator: aten.convolution_backward.default
cnt: 1, ((T([1, 3, 256, 256], f16), T([1, 64, 262, 262], f16), T([3, 64, 7, 7], f16), [3], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([1, 64, 256, 256], f16), T([1, 128, 128, 128], f16), T([128, 64, 3, 3], f16), [64], [2, 2], [1, 1], [1, 1], True, [1, 1], 1, [True, True, True]), {})
cnt: 1, ((T([1, 128, 128, 128], f16), T([1, 256, 64, 64], f16), T([256, 128, 3, 3], f16), [128], [2, 2], [1, 1], [1, 1], True, [1, 1], 1, [True, True, True]), {})
cnt: 18, ((T([1, 256, 64, 64], f16), T([1, 256, 66, 66], f16), T([256, 256, 3, 3], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([1, 256, 64, 64], f16), T([1, 128, 128, 128], f16), T([256, 128, 3, 3], f16), [256], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([1, 128, 128, 128], f16), T([1, 64, 256, 256], f16), T([128, 64, 3, 3], f16), [128], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([1, 64, 256, 256], f16), T([1, 3, 262, 262], f16), T([64, 3, 7, 7], f16), [64], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [False, True, True]), {})
Operator: aten.copy_.default
cnt: 1, ((T([1, 3, 256, 256], f16), T([1, 3, 256, 256], f16)), {})
cnt: 2, ((T([64, 256, 256], f16), T([64, 256, 256], f16)), {})
cnt: 4, ((T([1, 64, 256, 256], f16), T([1, 64, 256, 256], f16)), {})
cnt: 2, ((T([128, 128, 128], f16), T([128, 128, 128], f16)), {})
cnt: 4, ((T([1, 128, 128, 128], f16), T([1, 128, 128, 128], f16)), {})
cnt: 10, ((T([256, 64, 64], f16), T([256, 64, 64], f16)), {})
cnt: 20, ((T([1, 256, 64, 64], f16), T([1, 256, 64, 64], f16)), {})
Operator: aten.div.Tensor
cnt: 2, ((T([], f16), 196608), {})
Operator: aten.native_batch_norm.default
cnt: 2, ((T([1, 64, 256, 256], f16), None, None, None, None, True, 0.1, 1e-05), {})
cnt: 2, ((T([1, 128, 128, 128], f16), None, None, None, None, True, 0.1, 1e-05), {})
cnt: 19, ((T([1, 256, 64, 64], f16), None, None, None, None, True, 0.1, 1e-05), {})
Operator: aten.native_batch_norm_backward.default
cnt: 2, ((T([1, 64, 256, 256], f16), T([1, 64, 256, 256], f16), None, None, None, T([64], f32), T([64], f32), True, 1e-05, [True, False, False]), {})
cnt: 2, ((T([1, 128, 128, 128], f16), T([1, 128, 128, 128], f16), None, None, None, T([128], f32), T([128], f32), True, 1e-05, [True, False, False]), {})
cnt: 19, ((T([1, 256, 64, 64], f16), T([1, 256, 64, 64], f16), None, None, None, T([256], f32), T([256], f32), True, 1e-05, [True, False, False]), {})
Operator: aten.new_empty_strided.default
cnt: 2, ((T([1, 64, 256, 256], f16), [1, 64, 256, 256], [4194304, 65536, 256, 1]), {})
cnt: 2, ((T([1, 128, 128, 128], f16), [1, 128, 128, 128], [2097152, 16384, 128, 1]), {})
cnt: 10, ((T([1, 256, 64, 64], f16), [1, 256, 64, 64], [1048576, 4096, 64, 1]), {})
Operator: aten.new_zeros.default
cnt: 2, ((T([64, 256, 256], f16), [4194304]), {})
cnt: 2, ((T([128, 128, 128], f16), [2097152]), {})
cnt: 10, ((T([256, 64, 64], f16), [1048576]), {})
Operator: aten.reflection_pad2d.default
cnt: 1, ((T([1, 3, 256, 256], f16), [3, 3, 3, 3]), {})
cnt: 18, ((T([1, 256, 64, 64], f16), [1, 1, 1, 1]), {})
cnt: 1, ((T([1, 64, 256, 256], f16), [3, 3, 3, 3]), {})
Operator: aten.reflection_pad2d_backward.default
cnt: 1, ((T([1, 64, 262, 262], f16), T([1, 64, 256, 256], f16), [3, 3, 3, 3]), {})
cnt: 18, ((T([1, 256, 66, 66], f16), T([1, 256, 64, 64], f16), [1, 1, 1, 1]), {})
Operator: aten.relu_.default
cnt: 2, ((T([1, 64, 256, 256], f16),), {})
cnt: 2, ((T([1, 128, 128, 128], f16),), {})
cnt: 10, ((T([1, 256, 64, 64], f16),), {})
Operator: aten.sum.default
cnt: 1, ((T([1, 3, 256, 256], f16),), {})
Operator: aten.tanh.default
cnt: 1, ((T([1, 3, 256, 256], f16),), {})
Operator: aten.tanh_backward.default
cnt: 1, ((T([1, 3, 256, 256], f16, stride=(0, 0, 0, 0)), T([1, 3, 256, 256], f16)), {})
Operator: aten.threshold_backward.default
cnt: 2, ((T([1, 64, 256, 256], f16), T([1, 64, 256, 256], f16), 0), {})
cnt: 2, ((T([1, 128, 128, 128], f16), T([1, 128, 128, 128], f16), 0), {})
cnt: 10, ((T([1, 256, 64, 64], f16), T([1, 256, 64, 64], f16), 0), {})
```

----------------------------------------

TITLE: Dimension Index Arithmetic in PyTorch
DESCRIPTION: Creates a matrix mask using dimension indices to generate an upper triangular matrix pattern.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from torchdim import dims
i, j = dims(sizes=[4, 4])
print(i <= j)
```

----------------------------------------

TITLE: Tensor Slicing Input Tuples for aten.slice_backward.default - Python
DESCRIPTION: Samples for testing 'aten.slice_backward.default', each tuple contains a tensor, slice shape, dimension, and slicing indices (start, stop, step). These parameters validate gradient propagation through slicing, ensuring shape/dtype compatibility, especially for high-dimensional tensors; errors may arise if dimensions/indices are not legal.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
Operator: aten.slice_backward.default
cnt: 4, ((T([128, 960, 7, 7], f16), [128, 960, 7, 7], 3, 0, 9223372036854775807, 1), {})
```

LANGUAGE: python
CODE:
```
cnt: 4, ((T([128, 960, 7, 7], f16), [128, 960, 7, 7], 2, 0, 9223372036854775807, 1), {})
```

----------------------------------------

TITLE: Cloning Tensors with PyTorch
DESCRIPTION: This example shows 'aten.clone.default', a PyTorch operation to create a copy of a tensor. It's often used to ensure tensor manipulations do not alter the original data. It supports cloning tensors with various dimensions prevalent in model layers.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v2_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
Operator: aten.clone.default
cnt: 1, ((T([96, 3, 224, 224], f16),), {})
cnt: 2, ((T([96, 32, 112, 112], f16),), {})
cnt: 1, ((T([96, 96, 112, 112], f16),), {})
cnt: 1, ((T([96, 96, 56, 56], f16),), {})
cnt: 3, ((T([96, 144, 56, 56], f16),), {})
cnt: 1, ((T([96, 144, 28, 28], f16),), {})
cnt: 5, ((T([96, 192, 28, 28], f16),), {})
cnt: 1, ((T([96, 192, 14, 14], f16),), {})
cnt: 8, ((T([96, 384, 14, 14], f16),), {})
cnt: 5, ((T([96, 576, 14, 14], f16),), {})
cnt: 1, ((T([96, 576, 7, 7], f16),), {})
cnt: 6, ((T([96, 960, 7, 7], f16),), {})
cnt: 1, ((T([96, 1280, 7, 7], f16),), {})
```

----------------------------------------

TITLE: Max Pooling Operations in PyTorch with Half-Precision Tensors
DESCRIPTION: Max pooling operations with different input shapes and kernel sizes. Operations use half-precision (f16) tensors and include stride settings.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/inception_v3_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
((T([128, 64, 147, 147], f16), [3, 3], [2, 2]), {})
```

----------------------------------------

TITLE: Utilize aten.split_with_sizes in Python
DESCRIPTION: Splits a tensor into parts of specified sizes along a defined dimension using PyTorch. Prerequisites are compatible tensor operations provided by PyTorch. This function needs a tensor, a sizes list indicating partition extents, and a dimension index. The result is a tuple of segmented tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_11

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 32, 112, 112], f16), [16, 16], 1), {})
```

----------------------------------------

TITLE: Configuring QAT module swap (Python)
DESCRIPTION: Shows how to use `BackendPatternConfig` to define the mapping for QAT module swapping. This configuration specifies that instances of the fused float module `nn.intrinsic.LinearReLU` should be replaced with their QAT counterparts, `nn.intrinsic.qat.LinearReLU`, during the `prepare_fx/prepare_qat_fx` process.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
BackendPatternConfig(nni.LinearReLU)
    .set_qat_module(nniqat.LinearReLU)
```

----------------------------------------

TITLE: Building PyTorch for Android with Custom Flags
DESCRIPTION: Example of building PyTorch for Android with additional CMAKE flags to enable binary compilation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
bash scripts/build_android.sh -DBUILD_BINARY=ON
```

----------------------------------------

TITLE: Analyzing ATen Embedding Operations in PyTorch
DESCRIPTION: Details instances of \"aten.embedding.default\" summarizing its use in looking up embeddings for indices in a tensor, frequently used in language models.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_11

LANGUAGE: pseudocode
CODE:
```
Operator: aten.embedding.default
cnt: 1, ((T([50265, 1024], f16), T([8, 128], i64), 0), {})
cnt: 1, ((T([1024, 1024], f16), T([128], i64)), {})
```

----------------------------------------

TITLE: Testing torch.cond with Different Input Shapes
DESCRIPTION: Demonstrates how to test the DynamicShapeCondPredicate model with different input shapes to verify the conditional behavior.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cond.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
inp = torch.randn(3)
inp2 = torch.randn(5)
assert torch.equal(dyn_shape_mod(inp), false_fn(inp))
assert torch.equal(dyn_shape_mod(inp2), true_fn(inp2))
```

----------------------------------------

TITLE: Exposing Custom PyTorch Operators to Caffe2 CUDA
DESCRIPTION: This code shows how to make a custom PyTorch operator available to the Caffe2 frontend for CUDA execution. It uses a macro to expose the CUDA implementation of the operator to Caffe2.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_13

LANGUAGE: cpp
CODE:
```
C10_EXPORT_C10_OP_TO_CAFFE2_CUDA(
    MyCaffe2OperatorName, "my_namespace::my_op")
```

----------------------------------------

TITLE: Disabling NumPy Tracing for Debugging
DESCRIPTION: Shows how to disable tracing through NumPy functions for debugging purposes using torch._dynamo configuration.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
from torch._dynamo import config
config.trace_numpy = False
```

----------------------------------------

TITLE: Basic Implementation of a Custom Data Sparsifier in PyTorch
DESCRIPTION: Example implementation of a custom data sparsifier that zeros out tensor values below a threshold. This sparsifier inherits from BaseDataSparsifier and implements the required update_mask method.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/README.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
class ImplementedDataSparsifier(BaseDataSparsifier):
    def __init__(self, threshold):
        super().__init__(threshold=threshold)

    def update_mask(self, name, data, threshold):
        mask = self.get_mask(name)
        mask[torch.abs(data) < threshold] = 0.0
```

----------------------------------------

TITLE: Defining Expression Lists in TorchScript
DESCRIPTION: Specifies the syntax for expression lists and starred items in TorchScript. Starred items can only appear on the left side of assignments.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_24

LANGUAGE: python
CODE:
```
expression_list ::=  expression (',' expression)* [',']
starred_item    ::=  '*' primary
```

----------------------------------------

TITLE: Example Python Function with For Loop
DESCRIPTION: A simple Python function illustrating a typical `for` loop structure that iterates a specific number of times based on the size of a tensor's dimension and modifies a value (`z`) inside the loop. This serves as the source code example for the subsequent JIT IR translation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_5

LANGUAGE: python
CODE:
```
def f(x):
    z = x
    for i in range(x.size(0)):
        z = z * z
    return z
```

----------------------------------------

TITLE: Cloning Tensor in PyTorch
DESCRIPTION: This snippet demonstrates the cloning of a tensor, which is often used to create a copy of the input data for further processing or to preserve the original data.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
aten.clone.default(T([64, 3, 224, 224], f16),)
```

----------------------------------------

TITLE: Patching BatchNorm Modules In-Place with Functorch - PyTorch (Python)
DESCRIPTION: Applies functorch's in-place patch function to replace all BatchNorm modules in a model such that they do not use running statistics. This is achieved with the replace_all_batch_norm_modules_ utility from torch.func. The module 'net' is modified directly to ensure compatibility with vmapped operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.batch_norm.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
from torch.func import replace_all_batch_norm_modules_
replace_all_batch_norm_modules_(net)
```

----------------------------------------

TITLE: Analyzing PyTorch Operator Usage
DESCRIPTION: This code snippet represents a breakdown of PyTorch operator usage in a neural network model. It includes operator names, counts, and tensor shapes for various operations such as convolutions, activations, and other common deep learning operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv2_100_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([128, 1000], f16), 1, False), {})
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})
Operator: aten.add.Tensor
cnt: 52, ((T([], i64), 1), {})
cnt: 2, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16)), {})
cnt: 4, ((T([128, 32, 28, 28], f16), T([128, 32, 28, 28], f16)), {})
cnt: 6, ((T([128, 64, 14, 14], f16), T([128, 64, 14, 14], f16)), {})
cnt: 4, ((T([128, 96, 14, 14], f16), T([128, 96, 14, 14], f16)), {})
cnt: 4, ((T([128, 160, 7, 7], f16), T([128, 160, 7, 7], f16)), {})
Operator: aten.addmm.default
cnt: 1, ((T([1000], f16), T([128, 1280], f16), T([1280, 1000], f16, stride=(1, 1280))), {})
Operator: aten.clone.default
cnt: 1, ((T([128, 3, 224, 224], f16),), {})
cnt: 2, ((T([128, 32, 112, 112], f16),), {})
cnt: 1, ((T([128, 96, 112, 112], f16),), {})
cnt: 1, ((T([128, 96, 56, 56], f16),), {})
cnt: 3, ((T([128, 144, 56, 56], f16),), {})
cnt: 1, ((T([128, 144, 28, 28], f16),), {})
cnt: 5, ((T([128, 192, 28, 28], f16),), {})
cnt: 1, ((T([128, 192, 14, 14], f16),), {})
cnt: 8, ((T([128, 384, 14, 14], f16),), {})
cnt: 5, ((T([128, 576, 14, 14], f16),), {})
cnt: 1, ((T([128, 576, 7, 7], f16),), {})
cnt: 6, ((T([128, 960, 7, 7], f16),), {})
cnt: 1, ((T([128, 1280, 7, 7], f16),), {})
Operator: aten.convolution.default
cnt: 1, ((T([128, 3, 224, 224], f16), T([32, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 32, 112, 112], f16), T([32, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 32), {})
cnt: 1, ((T([128, 32, 112, 112], f16), T([16, 32, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 16, 112, 112], f16), T([96, 16, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 96, 112, 112], f16), T([96, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 96), {})
cnt: 1, ((T([128, 96, 56, 56], f16), T([24, 96, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 24, 56, 56], f16), T([144, 24, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 144, 56, 56], f16), T([144, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 144), {})
cnt: 1, ((T([128, 144, 56, 56], f16), T([24, 144, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 144, 56, 56], f16), T([144, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 144), {})
cnt: 1, ((T([128, 144, 28, 28], f16), T([32, 144, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([128, 32, 28, 28], f16), T([192, 32, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 192, 28, 28], f16), T([192, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 192), {})
cnt: 2, ((T([128, 192, 28, 28], f16), T([32, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 192, 28, 28], f16), T([192, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 192), {})
cnt: 1, ((T([128, 192, 14, 14], f16), T([64, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 64, 14, 14], f16), T([384, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([128, 384, 14, 14], f16), T([384, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 384), {})
cnt: 3, ((T([128, 384, 14, 14], f16), T([64, 384, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 384, 14, 14], f16), T([96, 384, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([128, 96, 14, 14], f16), T([576, 96, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 576, 14, 14], f16), T([576, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 576), {})
cnt: 2, ((T([128, 576, 14, 14], f16), T([96, 576, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 576, 14, 14], f16), T([576, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 576), {})
cnt: 1, ((T([128, 576, 7, 7], f16), T([160, 576, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([128, 160, 7, 7], f16), T([960, 160, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([128, 960, 7, 7], f16), T([960, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 960), {})
cnt: 2, ((T([128, 960, 7, 7], f16), T([160, 960, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 960, 7, 7], f16), T([320, 960, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 320, 7, 7], f16), T([1280, 320, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
Operator: aten.convolution_backward.default
cnt: 1, ((T([128, 1280, 7, 7], f16), T([128, 320, 7, 7], f16), T([1280, 320, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 320, 7, 7], f16), T([128, 960, 7, 7], f16), T([320, 960, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 3, ((T([128, 960, 7, 7], f16), T([128, 960, 7, 7], f16), T([960, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 960, [True, True, False]), {})
cnt: 3, ((T([128, 960, 7, 7], f16), T([128, 160, 7, 7], f16), T([960, 160, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 160, 7, 7], f16), T([128, 960, 7, 7], f16), T([160, 960, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 160, 7, 7], f16), T([128, 576, 7, 7], f16), T([160, 576, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 576, 7, 7], f16), T([128, 576, 14, 14], f16), T([576, 1, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 576, [True, True, False]), {})
cnt: 3, ((T([128, 576, 14, 14], f16), T([128, 96, 14, 14], f16), T([576, 96, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 96, 14, 14], f16), T([128, 576, 14, 14], f16), T([96, 576, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 576, 14, 14], f16), T([128, 576, 14, 14], f16), T([576, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 576, [True, True, False]), {})
cnt: 1, ((T([128, 96, 14, 14], f16), T([128, 384, 14, 14], f16), T([96, 384, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 4, ((T([128, 384, 14, 14], f16), T([128, 384, 14, 14], f16), T([384, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 384, [True, True, False]), {})
cnt: 4, ((T([128, 384, 14, 14], f16), T([128, 64, 14, 14], f16), T([384, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 3, ((T([128, 64, 14, 14], f16), T([128, 384, 14, 14], f16), T([64, 384, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 64, 14, 14], f16), T([128, 192, 14, 14], f16), T([64, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 192, 14, 14], f16), T([128, 192, 28, 28], f16), T([192, 1, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 192, [True, True, False]), {})
cnt: 3, ((T([128, 192, 28, 28], f16), T([128, 32, 28, 28], f16), T([192, 32, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 32, 28, 28], f16), T([128, 192, 28, 28], f16), T([32, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 192, 28, 28], f16), T([128, 192, 28, 28], f16), T([192, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 192, [True, True, False]), {})
cnt: 1, ((T([128, 32, 28, 28], f16), T([128, 144, 28, 28], f16), T([32, 144, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 144, 28, 28], f16), T([128, 144, 56, 56], f16), T([144, 1, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 144, [True, True, False]), {})
cnt: 2, ((T([128, 144, 56, 56], f16), T([128, 24, 56, 56], f16), T([144, 24, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 24, 56, 56], f16), T([128, 144, 56, 56], f16), T([24, 144, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 144, 56, 56], f16), T([128, 144, 56, 56], f16), T([144, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 144, [True, True, False]), {})
cnt: 1, ((T([128, 24, 56, 56], f16), T([128, 96, 56, 56], f16), T([24, 96, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 96, 56, 56], f16), T([128, 96, 112, 112], f16), T([96, 1, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 96, [True, True, False]), {})
cnt: 1, ((T([128, 96, 112, 112], f16), T([128, 16, 112, 112], f16), T([96, 16, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 16, 112, 112], f16), T([128, 32, 112, 112], f16), T([16, 32, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 32, 112, 112], f16), T([128, 32, 112, 112], f16), T([32, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 32, [True, True, False]), {})
cnt: 1, ((T([128, 32, 112, 112], f16), T([128, 3, 224, 224], f16), T([32, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})
Operator: aten.copy_.default
cnt: 1, ((T([128, 3, 224, 224], f16), T([128, 3, 224, 224], f16)), {})
Operator: aten.div.Scalar
cnt: 1, ((T([128, 1280, 7, 7], f16, stride=(1280, 1, 0, 0)), 49), {})
Operator: aten.hardtanh_.default
```

----------------------------------------

TITLE: Marking Unused Functions in TorchScript with @torch.jit.unused Decorator
DESCRIPTION: Decorator that indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception. Allows exporting models with non-TorchScript compatible code.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_42

LANGUAGE: python
CODE:
```
@torch.jit.unused
```

----------------------------------------

TITLE: Initial TorchScript LSTM Graph (Post Specialization)
DESCRIPTION: Shows the TorchScript graph representation of an LSTM layer immediately after input specialization. It uses standard `aten` and `prim` operators to perform matrix multiplications, additions, chunking, unpacking, and applying activation functions (sigmoid, tanh) to compute the next hidden and cell states based on inputs and weights.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_24

LANGUAGE: TorchScript
CODE:
```
graph(%x : Float(*, *),
      %hx : Float(*, *),
      %cx : Float(*, *),
      %w_ih : Float(*, *),
      %w_hh : Float(*, *),
      %b_ih : Float(*),
      %b_hh : Float(*)):
  %7 : int = prim::Constant[value=4]()
  %8 : int = prim::Constant[value=1]()
  %9 : Tensor = aten::t(%w_ih)
  %10 : Tensor = aten::mm(%x, %9)
  %11 : Tensor = aten::t(%w_hh)
  %12 : Tensor = aten::mm(%hx, %11)
  %13 : Tensor = aten::add(%10, %12, %8)
  %14 : Tensor = aten::add(%13, %b_ih, %8)
  %gates : Tensor = aten::add(%14, %b_hh, %8)
  %16 : Tensor[] = aten::chunk(%gates, %7, %8)
  %ingate.1 : Tensor, %forgetgate.1 : Tensor, %cellgate.1 : Tensor, %outgate.1 : Tensor = prim::ListUnpack(%16)
  %ingate : Tensor = aten::sigmoid(%ingate.1)
  %forgetgate : Tensor = aten::sigmoid(%forgetgate.1)
  %cellgate : Tensor = aten::tanh(%cellgate.1)
  %outgate : Tensor = aten::sigmoid(%outgate.1)
  %25 : Tensor = aten::mul(%forgetgate, %cx)
  %26 : Tensor = aten::mul(%ingate, %cellgate)
  %cy : Tensor = aten::add(%25, %26, %8)
  %28 : Tensor = aten::tanh(%cy)
  %hy : Tensor = aten::mul(%outgate, %28)
  %30 : (Tensor, Tensor) = prim::TupleConstruct(%hy, %cy)
  return (%30)
```

----------------------------------------

TITLE: Defining Shifting Operations in TorchScript
DESCRIPTION: Specifies the syntax for shift expressions in TorchScript, which can use << and >> operators with int or Tensor arguments. Shifting is defined in terms of division or multiplication by powers of 2.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
shift_expr ::=  a_expr | shift_expr ( '<<' | '>>' ) a_expr
```

----------------------------------------

TITLE: Usage Example for aten.nll_loss_forward.default
DESCRIPTION: Logs the forward pass for Negative Log Likelihood loss (`aten.nll_loss_forward.default`). Computes the loss based on input probabilities ([128, 1000] float16), target labels ([128] int64), optional weight (None), reduction type (1 for mean), and ignore index (-100).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/regnety_002_training.txt#2025-04-22_snippet_7

LANGUAGE: plaintext
CODE:
```
Operator: aten.nll_loss_forward.default
cnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})
```

----------------------------------------

TITLE: Nearest Neighbor Upsampling Backward Pass in PyTorch
DESCRIPTION: This snippet computes the gradient for nearest neighbor upsampling during the backward pass. It's essential for training neural networks that use upsampling operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_20

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([8, 128, 48, 64], f16, stride=(1179648, 3072, 64, 1)), None, [8, 128, 24, 32], [2.0, 2.0]), {})
cnt: 1, ((T([8, 256, 24, 32], f16, stride=(589824, 768, 32, 1)), None, [8, 256, 12, 16], [2.0, 2.0]), {})
```

----------------------------------------

TITLE: Importing DataPipe and Typing Libraries in Python
DESCRIPTION: Imports necessary classes and types for DataPipe typing, including IterDataPipe and various typing constructs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/typing.ipynb#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from torch.utils.data import IterDataPipe
from typing import Any, Iterator, List, Tuple, TypeVar, Set, Union

T_co = TypeVar('T_co', covariant=True)
```

----------------------------------------

TITLE: Demonstrating jit.trace Failure with Changed Inputs in PyTorch
DESCRIPTION: This snippet shows how jit.trace fails when the input conditions change, highlighting the limitations of jit.trace with dynamic control flow.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
maybe_true = torch.BoolTensor([1])
assert jit(t, maybe_true) == add_two_maybe(t, maybe_true)
```

----------------------------------------

TITLE: Collecting HuggingFace Models Training Data
DESCRIPTION: Command to collect matrix multiplication training data from HuggingFace models using TorchInductor
SOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/mm/README.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
TORCHINDUCTOR_AUTOHEURISTIC_USE="" TORCHINDUCTOR_AUTOHEURISTIC_COLLECT="mm" TORCHINDUCTOR_AUTOHEURISTIC_LOG_PATH="hf_train_mm.txt" TORCHINDUCTOR_MAX_AUTOTUNE=1 time python ../../../benchmarks/dynamo/huggingface.py --ci --performance --timing --explain --inductor --device cuda --train --amp
```

----------------------------------------

TITLE: Adding Test Directory and Installing Headers
DESCRIPTION: Adds the test subdirectory and configures the installation of C10 HIP headers and the generated macros file.
SOURCE: https://github.com/pytorch/pytorch/blob/main/c10/hip/CMakeLists.txt#2025-04-22_snippet_4

LANGUAGE: cmake
CODE:
```
add_subdirectory(test)

# ---[ Installation
install(DIRECTORY ${CMAKE_CURRENT_LIST_DIR}
        DESTINATION include
        FILES_MATCHING PATTERN "*.h")
install(FILES ${CMAKE_BINARY_DIR}/c10/hip/impl/hip_cmake_macros.h
  DESTINATION include/c10/hip/impl)
```

----------------------------------------

TITLE: Defining Python Class 'LongWithShortDocstring' With Insufficient Docstring
DESCRIPTION: This snippet defines a Python class named 'LongWithShortDocstring' with a docstring that is too short (only 10 characters). The docstring contains only a TODO placeholder.
SOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/more_python_code.py.txt.before.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
class LongWithShortDocstring:
    """TODO"""
```

----------------------------------------

TITLE: Maintainer Approval Process in Markdown
DESCRIPTION: Outlines the decision criteria and requirements for approving a new maintainer addition PR.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/community/build_ci_governance.rst#2025-04-22_snippet_1

LANGUAGE: markdown
CODE:
```
* Not earlier than two business days passed before merging (ensure the majority of the contributors have seen it)
* PR has the correct label (`module: ci`)
* There are no objections from the current maintainers
* There are at least three net *thumbs up* from current maintainers (or all maintainers vote *thumbs up* when the module has less than 3 maintainers).
```

----------------------------------------

TITLE: Initializing PyTorch DataLoader with Various Options
DESCRIPTION: Constructor signature for DataLoader class, showing all available parameters for configuring data loading behavior.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/data.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,
           batch_sampler=None, num_workers=0, collate_fn=None,
           pin_memory=False, drop_last=False, timeout=0,
           worker_init_fn=None, *, prefetch_factor=2,
           persistent_workers=False)
```

----------------------------------------

TITLE: Example of FSDP2 Application to nn.Linear
DESCRIPTION: Illustrates how FSDP2 is applied to a specific PyTorch module (nn.Linear). It shows that calling fully_shard on a Linear module creates a new FSDPLinear class.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.fsdp.fully_shard.rst#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
fully_shard(linear)
```

----------------------------------------

TITLE: Configuring Compiler Flags for Non-MSVC Compilers
DESCRIPTION: Adds compiler flags to ignore certain warnings for non-MSVC compilers.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: CMake
CODE:
```
if(NOT MSVC)
  string(APPEND CMAKE_CXX_FLAGS " -Wno-ignored-qualifiers")
  string(APPEND CMAKE_C_FLAGS " -Wno-ignored-qualifiers")
  string(APPEND CMAKE_CXX_FLAGS " -Wno-absolute-value")
  string(APPEND CMAKE_C_FLAGS " -Wno-absolute-value")
endif(NOT MSVC)
```

----------------------------------------

TITLE: Square Root Operation in PyTorch
DESCRIPTION: The example for aten.sqrt.default shows computing the square root of a scalar tensor in float32, a basic mathematical operation common in data normalization processes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_14

LANGUAGE: Python
CODE:
```
Operator: aten.sqrt.default
cnt: 24, ((T([], f32),), {})
```

----------------------------------------

TITLE: Profiling aten.add.Tensor Calls in PyTorch Text Trace
DESCRIPTION: Lists call signatures for the aten.add.Tensor operator, detailing argument tensor shapes and data types. The trace captures patterns in element-wise addition operations typically appearing in residual connections or intermediate computations in neural network layers. No external code dependencies are required for reading this trace, but it assumes a PyTorch context for operators and tensor representations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_0

LANGUAGE: text
CODE:
```
Operator: aten.add.Tensor
cnt: 2, ((T([32, 24, 56, 56], f16), T([32, 24, 56, 56], f16)), {})
cnt: 2, ((T([32, 40, 28, 28], f16), T([32, 40, 28, 28], f16)), {})
cnt: 4, ((T([32, 80, 14, 14], f16), T([32, 80, 14, 14], f16)), {})
cnt: 4, ((T([32, 112, 14, 14], f16), T([32, 112, 14, 14], f16)), {})
cnt: 6, ((T([32, 192, 7, 7], f16), T([32, 192, 7, 7], f16)), {})
cnt: 4, ((T([32, 1152, 7, 7], f16), T([32, 1152, 7, 7], f16)), {})
cnt: 1, ((T([32, 672, 7, 7], f16), T([32, 672, 7, 7], f16)), {})
cnt: 2, ((T([32, 672, 14, 14], f16), T([32, 672, 14, 14], f16)), {})
cnt: 3, ((T([32, 480, 14, 14], f16), T([32, 480, 14, 14], f16)), {})
cnt: 1, ((T([32, 240, 14, 14], f16), T([32, 240, 14, 14], f16)), {})
cnt: 1, ((T([32, 240, 28, 28], f16), T([32, 240, 28, 28], f16)), {})
cnt: 1, ((T([32, 144, 28, 28], f16), T([32, 144, 28, 28], f16)), {})
cnt: 1, ((T([32, 144, 56, 56], f16), T([32, 144, 56, 56], f16)), {})
cnt: 1, ((T([32, 96, 56, 56], f16), T([32, 96, 56, 56], f16)), {})
cnt: 1, ((T([32, 32, 112, 112], f16), T([32, 32, 112, 112], f16)), {})
```

----------------------------------------

TITLE: PyTorch GELU Backward Function
DESCRIPTION: These snippets show the backward pass of the GELU activation function for various tensor shapes, using float16 precision.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_12

LANGUAGE: Python
CODE:
```
((T([128, 3072, 6, 6], f16), T([128, 3072, 6, 6], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 768, 6, 6], f16), T([128, 768, 6, 6], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 1536, 6, 6], f16), T([128, 1536, 6, 6], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 768, 12, 12], f16), T([128, 768, 12, 12], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 1536, 12, 12], f16), T([128, 1536, 12, 12], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 768, 24, 24], f16), T([128, 768, 24, 24], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 512, 24, 24], f16), T([128, 512, 24, 24], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 256, 24, 24], f16), T([128, 256, 24, 24], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 256, 48, 48], f16), T([128, 256, 48, 48], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 128, 48, 48], f16), T([128, 128, 48, 48], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 64, 96, 96], f16), T([128, 64, 96, 96], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 32, 96, 96], f16), T([128, 32, 96, 96], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 16, 96, 96], f16), T([128, 16, 96, 96], f16)), {})
```

----------------------------------------

TITLE: Initializing a Python Class with Method Docstring
DESCRIPTION: This snippet defines a class named 'LintInitInit' with an __init__ method that includes a docstring and placeholder comments.
SOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/more_python_code.py.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
class LintInitInit:
    def __init__(self) -> None:
        """This is a very long comment, a very long comment"""

        # Lots of lines!
        # Lots of lines!
        pass
```

----------------------------------------

TITLE: Example of Weight Tensor Before and After Pruning
DESCRIPTION: Demonstrates how a weight tensor looks before and after applying unstructured pruning by zeroing out the lowest absolute value elements.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/pruner/README.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
W = [[1 2 3]
     [4 5 6]
     [7 1 9]]
```

----------------------------------------

TITLE: Importing PyTorch Distributed Elastic Events Module
DESCRIPTION: This snippet shows how to import the events module from PyTorch's distributed elastic package. It's a prerequisite for using the events API.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/events.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch.distributed.elastic.events
```

----------------------------------------

TITLE: Batch Normalization Backward Operation Statistics
DESCRIPTION: Logs of backward pass batch normalization operations (aten.native_batch_norm_backward.default) showing gradient computation details
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 1280, 7, 7], f16), T([128, 1280, 7, 7], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f32), T([1280], f32), True, 1e-05, [True, True, True]), {})
```

----------------------------------------

TITLE: Aggregation Function Implementation in PyTorch
DESCRIPTION: Example implementation of an aggregation function that combines two tensors by addition.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/activation_sparsifier/README.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
def aggregate_fn(tensor1, tensor2):
    return tensor1 + tensor2
```

----------------------------------------

TITLE: Adding Custom Command for Source Code Generation in CMake
DESCRIPTION: Defines a custom command in CMake that executes the previously defined `GEN_COMMAND` (Python script) to generate C++ source files. Specifies the output files (`GEN_COMMAND_sources`), the command itself, dependencies (Python scripts, YAML files, templates), and the working directory. This command runs during the build process before compiling the sources.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/edge/CMakeLists.txt#2025-04-22_snippet_2

LANGUAGE: cmake
CODE:
```
add_custom_command(
        COMMENT "Generating sources"
        OUTPUT ${GEN_COMMAND_sources}
        COMMAND ${GEN_COMMAND}
        DEPENDS
        ${all_python}
        ${TORCH_ROOT}/aten/src/ATen/native/native_functions.yaml
        ${TORCH_ROOT}/aten/src/ATen/native/tags.yaml
        ${TEST_ROOT}/templates/Functions.h
        ${TEST_ROOT}/templates/NativeFunctions.h
        ${TEST_ROOT}/templates/RegisterCodegenUnboxedKernels.cpp
        ${TEST_ROOT}/templates/RegisterDispatchKeyCustomOps.cpp
        WORKING_DIRECTORY ${TORCH_ROOT}
)
```

----------------------------------------

TITLE: Profiling ATen Threshold Backward Operations for ReLU Gradients in PyTorch (Python)
DESCRIPTION: Tracks ATen threshold_backward operator calls, corresponding to gradient computations for ReLU during backward passes. Dependencies: PyTorch. Inputs are tensors of same shape (input and grad_output) plus the threshold scalar; output is the computed gradient. Matches each activation backward with input/output tensor shapes, for correct gradient flow.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet18_training.txt#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
Operator: aten.threshold_backward.default
cnt: 4, ((T([16, 512, 7, 7], f16), T([16, 512, 7, 7], f16), 0), {})
cnt: 4, ((T([16, 256, 14, 14], f16), T([16, 256, 14, 14], f16), 0), {})
cnt: 4, ((T([16, 128, 28, 28], f16), T([16, 128, 28, 28], f16), 0), {})
cnt: 4, ((T([16, 64, 56, 56], f16), T([16, 64, 56, 56], f16), 0), {})
cnt: 1, ((T([16, 64, 112, 112], f16), T([16, 64, 112, 112], f16), 0), {})
```

----------------------------------------

TITLE: Configure Emscripten Flags (CMake)
DESCRIPTION: Sets specific compiler flags for Emscripten builds, including warning suppression and disabling exception catching.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#_snippet_30

LANGUAGE: CMake
CODE:
```
if(EMSCRIPTEN)
  string(
    APPEND
    CMAKE_CXX_FLAGS
    " -Wno-implicit-function-declaration -DEMSCRIPTEN -s DISABLE_EXCEPTION_CATCHING=0"
  )
endif()
```

----------------------------------------

TITLE: Addition using aten.add.Tensor in PyTorch
DESCRIPTION: Illustrates the application of aten.add.Tensor for element-wise addition across different tensor configurations, heavily relying on f16 type. Necessary infrastructures include PyTorch for tensorated computations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MobileBertForMaskedLM_training.txt#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
aten.add.Tensor, ((T([16, 128, 512], f16), T([1, 128, 512], f16)), {})
```

----------------------------------------

TITLE: Constructing and Recording a Rendezvous Event in PyTorch
DESCRIPTION: This function constructs and records a rendezvous event in the PyTorch distributed elastic system. It's specifically designed for handling rendezvous-related events.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/events.rst#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
torch.distributed.elastic.events.construct_and_record_rdzv_event
```

----------------------------------------

TITLE: Defining a Class with String Literal Not Used as Docstring
DESCRIPTION: A class with methods where a string literal appears between methods rather than as a docstring.
SOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/python_code.py.txt#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
class NotDocstring:
    def short1(self):
        pass

    """This is not a docstring"""

    def short2(self):
        pass

    def short3(self):
        pass
```

----------------------------------------

TITLE: Running Code Coverage for Multiple Tests and Folders
DESCRIPTION: This example demonstrates how to run the code coverage tool for multiple tests and generate reports for specific folders of interest.
SOURCE: https://github.com/pytorch/pytorch/blob/main/tools/code_coverage/README.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
python oss_coverage.py --run-only=atest c10_logging_test --interest-only aten/src/Aten c10/core
```

----------------------------------------

TITLE: Tensor Cloning in PyTorch
DESCRIPTION: Creates a copy of the input tensor with the same content and size but may have a different storage.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
aten.clone.default((T([64, 3, 224, 224], f16),), {})
```

----------------------------------------

TITLE: Enabling Logging for Fusion Pass in Python
DESCRIPTION: This command enables logging for the fusion pass in PyTorch, useful for verifying if fusion is occurring in a script model.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/cuda/README.md#2025-04-22_snippet_9

LANGUAGE: Bash
CODE:
```
PYTORCH_JIT_LOG_LEVEL="graph_fuser"
```

----------------------------------------

TITLE: Basic Mask Computation in PyTorch
DESCRIPTION: Demonstrates the high-level process of computing a mask by aggregating activations, reducing them, and applying a mask function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/activation_sparsifier/README.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
aggregated_tensor = aggregate_fn([activation for activation in all_activations])
reduced_tensor = reduce_fn(aggregated_tensor)
mask = mask_fn(reduced_tensor)
```

----------------------------------------

TITLE: Matrix Multiplication Operations
DESCRIPTION: Matrix multiplication operations between tensors of compatible shapes, typically used in fully connected layers. Uses float16 precision.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
((T([64, 1000], f16), T([1000, 1536], f16)), {})
```

----------------------------------------

TITLE: Analyzing _softmax Operator Calls in PyTorch with Various Tensor Dimensions
DESCRIPTION: Records of aten._softmax.default operator calls with different tensor shapes. All operations use half-precision (f16) tensors with dimension 1 as the softmax dimension and False for the third parameter.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_resnest_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
Operator: aten._softmax.default
cnt: 1, ((T([32, 2, 1, 64], f16), 1, False), {})
cnt: 1, ((T([32, 2, 1, 128], f16), 1, False), {})
cnt: 1, ((T([32, 2, 1, 256], f16), 1, False), {})
cnt: 1, ((T([32, 2, 1, 512], f16), 1, False), {})
```

----------------------------------------

TITLE: PyTorch Tensor Copy Operation
DESCRIPTION: This snippet shows a tensor copy operation between two float16 tensors with the same shape.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
((T([128, 3, 192, 192], f16), T([128, 3, 192, 192], f16)), {})
```

----------------------------------------

TITLE: Defining Dispatch for Native Functions in YAML
DESCRIPTION: Shows how to specify dispatch information for native functions in native_functions.yaml, including backend-specific implementations and composite kernels.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_13

LANGUAGE: YAML
CODE:
```
dispatch:
  CPU: kernel_cpu
  CUDA: kernel_cuda
  QuantizedCPU: kernel_quantized_cpu
```

LANGUAGE: YAML
CODE:
```
dispatch:
  CompositeExplicitAutograd: kernel
```

----------------------------------------

TITLE: Using Loss Parallelism Context Manager in Python
DESCRIPTION: This snippet demonstrates the usage of the loss_parallel context manager for parallelized cross-entropy loss computation in PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.tensor.parallel.rst#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
from torch.distributed.tensor.parallel import loss_parallel
```

----------------------------------------

TITLE: Profiling specific FastRNNs implementations with nvprof
DESCRIPTION: Command to profile specific RNN implementations (ATEN and JIT) using NVIDIA's nvprof tool.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/fastrnns/README.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
python -m fastrnns.profile --rnns aten jit
```

----------------------------------------

TITLE: Importing checkpoint module in PyTorch
DESCRIPTION: Shows how to import the checkpoint module from torch.utils. This module provides functions and classes for implementing checkpointing in PyTorch models.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/checkpoint.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
torch.utils.checkpoint
```

----------------------------------------

TITLE: Analyzing Element-wise Operations in PyTorch
DESCRIPTION: This snippet shows element-wise operations like division and activation functions (hardsigmoid, hardswish) applied to tensors of various shapes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
Operator: aten.div.Scalar
cnt: 1, ((T([128, 1344, 7, 7], f16, stride=(1344, 1, 0, 0)), 49), {})
cnt: 1, ((T([128, 1104, 7, 7], f16, stride=(1104, 1, 0, 0)), 49), {})

Operator: aten.hardsigmoid.default
cnt: 5, ((T([128, 120, 1, 1], f16),), {})
cnt: 6, ((T([128, 360, 1, 1], f16),), {})

Operator: aten.hardswish_.default
cnt: 3, ((T([128, 16, 112, 112], f16),), {})
cnt: 1, ((T([128, 64, 112, 112], f16),), {})
```

----------------------------------------

TITLE: Disabling NVFuser in PyTorch
DESCRIPTION: These snippets show three methods to disable NVFuser in PyTorch, including environment variables and a Python API call.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/cuda/README.md#2025-04-22_snippet_10

LANGUAGE: Bash
CODE:
```
export PYTORCH_JIT_USE_NNC_NOT_NVFUSER=1
```

LANGUAGE: Python
CODE:
```
torch._C._jit_set_nvfuser_enabled(False)
```

LANGUAGE: Bash
CODE:
```
export PYTORCH_JIT_ENABLE_NVFUSER=0
```

----------------------------------------

TITLE: Mocking Modules with Exclusions
DESCRIPTION: Example of using the mock action with exclusion patterns to mock all modules except those matching the exclusion patterns.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
exporter.mock("**", exclude=["torchvision.**"])
```

----------------------------------------

TITLE: Tracking Convolution Forward Operations in Neural Network
DESCRIPTION: Lists all convolution operations in the forward pass, showing kernel sizes, strides, padding, and tensor shapes. The network uses a combination of standard convolutions, depthwise separable convolutions (with groups parameter), and 1x1 pointwise convolutions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hardcorenas_a_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
Operator: aten.convolution.default
cnt: 1, ((T([128, 3, 224, 224], f16), T([32, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 32, 112, 112], f16), T([32, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 32), {})
cnt: 1, ((T([128, 32, 112, 112], f16), T([16, 32, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 16, 112, 112], f16), T([48, 16, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 48, 112, 112], f16), T([48, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 48), {})
cnt: 1, ((T([128, 48, 56, 56], f16), T([24, 48, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 24, 56, 56], f16), T([72, 24, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 72, 56, 56], f16), T([72, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 72), {})
cnt: 1, ((T([128, 72, 1, 1], f16), T([24, 72, 1, 1], f16), T([24], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 24, 1, 1], f16), T([72, 24, 1, 1], f16), T([72], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 72, 56, 56], f16), T([24, 72, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 72, 56, 56], f16), T([72, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 72), {})
cnt: 1, ((T([128, 72, 28, 28], f16), T([40, 72, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 40, 28, 28], f16), T([240, 40, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 240, 28, 28], f16), T([240, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 240), {})
cnt: 2, ((T([128, 240, 1, 1], f16), T([64, 240, 1, 1], f16), T([64], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 64, 1, 1], f16), T([240, 64, 1, 1], f16), T([240], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 240, 28, 28], f16), T([40, 240, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 240, 28, 28], f16), T([240, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 240), {})
cnt: 1, ((T([128, 240, 14, 14], f16), T([80, 240, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 80, 14, 14], f16), T([480, 80, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 480, 14, 14], f16), T([480, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 480), {})
cnt: 2, ((T([128, 480, 1, 1], f16), T([120, 480, 1, 1], f16), T([120], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 120, 1, 1], f16), T([480, 120, 1, 1], f16), T([480], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 480, 14, 14], f16), T([80, 480, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 480, 14, 14], f16), T([112, 480, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 112, 14, 14], f16), T([672, 112, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 672, 14, 14], f16), T([672, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 672), {})
cnt: 2, ((T([128, 672, 1, 1], f16), T([168, 672, 1, 1], f16), T([168], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 168, 1, 1], f16), T([672, 168, 1, 1], f16), T([672], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 672, 14, 14], f16), T([112, 672, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 672, 14, 14], f16), T([672, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 672), {})
cnt: 1, ((T([128, 672, 7, 7], f16), T([192, 672, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 192, 7, 7], f16), T([1152, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 1152, 7, 7], f16), T([1152, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 1152), {})
cnt: 1, ((T([128, 1152, 1, 1], f16), T([288, 1152, 1, 1], f16), T([288], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 288, 1, 1], f16), T([1152, 288, 1, 1], f16), T([1152], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 1152, 7, 7], f16), T([192, 1152, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 192, 7, 7], f16), T([960, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 960, 1, 1], f16), T([1280, 960, 1, 1], f16), T([1280], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

----------------------------------------

TITLE: Backward Pass for Tanh Activation in PyTorch (Python)
DESCRIPTION: Computes the derivatives for the tanh function during backpropagation with aten.tanh_backward, crucial for adjusting weights based on the tanh activation response during training.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_28

LANGUAGE: Python
CODE:
```
aten.tanh_backward.default
cnt: 1, ((T([16, 768], f16), T([16, 768], f16)), {})
```

----------------------------------------

TITLE: Graph Visualization with AOT Function
DESCRIPTION: Implements a graph drawer compiler that saves forward and backward graphs as SVG files using the draw_graph utility.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/COMPILE_README.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
def f(x):
    return x.cos().cos()

def graph_drawer(name):
    def f(fx_g: fx.GraphModule, inps):
        draw_graph(fx_g, name)
        return fx_g
    return f

aot_function(f, fw_compiler=graph_drawer("forward"), bw_compiler=graph_drawer("backward"))(torch.randn(3, requires_grad=True))
```

----------------------------------------

TITLE: Calling aten.sum.dim_IntList (Python)
DESCRIPTION: Computes the sum of tensor elements over a list of specified dimensions. Example shows summing a boolean tensor along dimension 1.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_31

LANGUAGE: Python
CODE:
```
((T([8, 128], b8), [1]), {})
```

----------------------------------------

TITLE: Stack Operation in PyTorch
DESCRIPTION: This snippet shows stack operations on tensors with different shapes and strides. It combines multiple tensors along a new dimension.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
Operator: aten.stack.default
cnt: 2, (([T([64, 12, 197, 32], f16, stride=(75648, 6304, 1, 197)), T([64, 12, 197, 32], f16)],), {})
cnt: 14, (([T([64, 12, 196, 32], f16), T([64, 12, 196, 32], f16, stride=(75264, 6272, 1, 196)), T([64, 12, 196, 32], f16)],), {})
```

----------------------------------------

TITLE: Tensor Multiplication Operations in PyTorch
DESCRIPTION: Element-wise multiplication between tensors of matching shapes, using half-precision format. Operations include both spatial and channel-wise multiplications.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
aten.mul.Tensor
cnt: 10, ((T([128, 120, 28, 28], f16), T([128, 120, 1, 1], f16)), {})
cnt: 12, ((T([128, 360, 14, 14], f16), T([128, 360, 1, 1], f16)), {})
```

----------------------------------------

TITLE: TorchScript LSTM Graph (After Required Passes)
DESCRIPTION: Represents the TorchScript graph after applying required graph transformations and inference passes. These passes ensure graph legality for the interpreter and propagate information like ranks, dtypes, and device information. The graph structure for the LSTM computation is largely maintained, but explicit type/shape annotations appear on tensor values.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_25

LANGUAGE: TorchScript
CODE:
```
graph(%x : Float(*, *),
      %hx : Float(*, *),
      %cx : Float(*, *),
      %w_ih : Float(*, *),
      %w_hh : Float(*, *),
      %b_ih : Float(*),
      %b_hh : Float(*)):
  %8 : int = prim::Constant[value=1]()
  %9 : Float(*, *) = aten::t(%w_ih)
  %10 : Float(*, *) = aten::mm(%x, %9)
  %11 : Float(*, *) = aten::t(%w_hh)
  %12 : Float(*, *) = aten::mm(%hx, %11)
  %13 : Float(*, *) = aten::add(%10, %12, %8)
  %14 : Float(*, *) = aten::add(%13, %b_ih, %8)
  %gates : Float(*, *) = aten::add(%14, %b_hh, %8)
  %31 : Float(*, *), %32 : Float(*, *), %33 : Float(*, *), %34 : Float(*, *) = prim::ConstantChunk[chunks=4, dim=1](%gates)
  %ingate : Float(*, *) = aten::sigmoid(%31)
  %forgetgate : Float(*, *) = aten::sigmoid(%32)
  %cellgate : Float(*, *) = aten::tanh(%33)
  %outgate : Float(*, *) = aten::sigmoid(%34)
  %25 : Float(*, *) = aten::mul(%forgetgate, %cx)
  %26 : Float(*, *) = aten::mul(%ingate, %cellgate)
  %cy : Float(*, *) = aten::add(%25, %26, %8)
  %28 : Float(*, *) = aten::tanh(%cy)
  %hy : Float(*, *) = aten::mul(%outgate, %28)
  %30 : (Float(*, *), Float(*, *)) = prim::TupleConstruct(%hy, %cy)
  return (%30)
```

----------------------------------------

TITLE: Run Specific PyTorch Python Test File (bash)
DESCRIPTION: Executes a single Python test file, `test/test_jit.py`, which contains tests for the TorchScript JIT compiler. This allows running only the tests within a specific test suite file.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_14

LANGUAGE: bash
CODE:
```
python test/test_jit.py
```

----------------------------------------

TITLE: PyTorch Sum Operation with Symbolic Dimensions
DESCRIPTION: This snippet shows a sum operation on a tensor of shape [128, 1000] with f16 precision along dimension 0, keeping the dimension in the result (keepdim=True). The operation uses symbolic dimensions (SymInt).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([128, 1000], f16), [0], True), {})
```

----------------------------------------

TITLE: Summation Across Dimensions with aten.sum.SymInt in PyTorch (Python)
DESCRIPTION: Examples of summing tensors over the first axis across several shapes, returning reduced tensors for further processing. Used for pooling, aggregation, or normalization. Accepts input tensor, dimension list, and keepdim flag; operates exclusively on FP16 inputs for all documented usages.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([512, 30522], f16), [0], True), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.sum.SymInt
cnt: 61, ((T([512, 768], f16), [0], True), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.sum.SymInt
cnt: 12, ((T([512, 3072], f16), [0], True), {})
```

----------------------------------------

TITLE: Manually Configuring CMake Build Options on Linux Bash
DESCRIPTION: Sets the CMAKE_PREFIX_PATH environment variable and runs the PyTorch setup script with the '--cmake-only' flag to generate CMake build files. Subsequently, opens the ccmake or cmake-gui interactive tool to allow manual adjustment of build options.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_17

LANGUAGE: Bash
CODE:
```
export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
python setup.py build --cmake-only
ccmake build  # or cmake-gui build
```

----------------------------------------

TITLE: Configuring Mobile NNC Test Sources and Executable in CMake
DESCRIPTION: Sets up the source files and creates an executable for mobile NNC tests. It links against the torch library and gtest, includes necessary directories, and defines the USE_GTEST compile definition.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/mobile/nnc/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
set(MOBILE_NNC_TEST_ROOT ${TORCH_ROOT}/test/mobile/nnc)

set(MOBILE_NNC_TEST_SRCS
  ${MOBILE_NNC_TEST_ROOT}/test_context.cpp
  ${MOBILE_NNC_TEST_ROOT}/test_nnc_backend.cpp
  ${MOBILE_NNC_TEST_ROOT}/test_registry.cpp
)

add_executable(test_mobile_nnc
  ${TORCH_ROOT}/test/cpp/lite_interpreter_runtime/main.cpp
  ${MOBILE_NNC_TEST_SRCS}
)

target_link_libraries(test_mobile_nnc PRIVATE torch gtest)
target_include_directories(test_mobile_nnc PRIVATE ${ATen_CPU_INCLUDE})
target_compile_definitions(test_mobile_nnc PRIVATE USE_GTEST)
```

----------------------------------------

TITLE: Testing Backward Pass for ReLU/Threshold with aten.threshold_backward.default - Python
DESCRIPTION: Snippets provide inputs to the backward pass for thresholded activations (e.g., ReLU), specifying input/output tensors and threshold value for 'aten.threshold_backward.default'. Used for validating gradient computation and operator support for f16 tensors on typical activation shapes. Requires forward pass context and shape/type matching.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
Operator: aten.threshold_backward.default
cnt: 1, ((T([128, 1280, 1, 1], f16), T([128, 1280, 1, 1], f16), 0), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 960, 7, 7], f16), T([128, 960, 7, 7], f16), 0), {})
```

----------------------------------------

TITLE: Analyzing ATen Softmax Operations in PyTorch
DESCRIPTION: The snippet details the use of the ATen softmax operator applied on a tensor, illustrating dimensional and data type configurations. The softmax function is applied with specific parameters indicating dimension and boolean settings, reflecting its utilization across multiple instances.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistilBertForMaskedLM_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
Operator: aten._softmax.default
cnt: 6, ((T([16, 12, 128, 128], f16), -1, False), {})
```

----------------------------------------

TITLE: Examining Matrix Multiplication and Tensor Clone Operations in PyTorch
DESCRIPTION: This snippet shows statistics for addmm (matrix multiplication with addition) and clone operations in the model. The addmm operation is used for the final classification layer, while clone operations are used throughout the network on tensors of various shapes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tinynet_a_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
Operator: aten.addmm.default
cnt: 1, ((T([1000], f16), T([128, 1280], f16), T([1280, 1000], f16, stride=(1, 1280))), {})
Operator: aten.clone.default
cnt: 1, ((T([128, 3, 192, 192], f16),), {})
cnt: 2, ((T([128, 32, 96, 96], f16),), {})
cnt: 1, ((T([128, 8, 1, 1], f16),), {})
cnt: 1, ((T([128, 96, 96, 96], f16),), {})
cnt: 1, ((T([128, 96, 48, 48], f16),), {})
cnt: 1, ((T([128, 4, 1, 1], f16),), {})
cnt: 3, ((T([128, 144, 48, 48], f16),), {})
cnt: 2, ((T([128, 6, 1, 1], f16),), {})
cnt: 1, ((T([128, 144, 24, 24], f16),), {})
cnt: 3, ((T([128, 240, 24, 24], f16),), {})
cnt: 2, ((T([128, 10, 1, 1], f16),), {})
cnt: 1, ((T([128, 240, 12, 12], f16),), {})
cnt: 8, ((T([128, 480, 12, 12], f16),), {})
cnt: 4, ((T([128, 20, 1, 1], f16),), {})
cnt: 7, ((T([128, 672, 12, 12], f16),), {})
cnt: 4, ((T([128, 28, 1, 1], f16),), {})
cnt: 1, ((T([128, 672, 6, 6], f16),), {})
cnt: 10, ((T([128, 1152, 6, 6], f16),), {})
cnt: 5, ((T([128, 48, 1, 1], f16),), {})
cnt: 1, ((T([128, 1280, 6, 6], f16),), {})
```

----------------------------------------

TITLE: Calling aten._unsafe_view.default (Python)
DESCRIPTION: Reshapes a tensor to a new shape without checking for memory contiguousness or aliasing, potentially leading to performance benefits or issues if not used carefully. Examples show various reshaping operations on float16 tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_5

LANGUAGE: Python
CODE:
```
((T([8, 128, 16, 64], f16), [8, 128, 1024]), {})
```

LANGUAGE: Python
CODE:
```
((T([1024, 50265], f16), [8, 128, 50265]), {})
```

LANGUAGE: Python
CODE:
```
((T([8, 16, 128, 64], f16), [128, 128, 64]), {})
```

LANGUAGE: Python
CODE:
```
((T([8, 128, 1024], f16), [1024, 1024]), {})
```

----------------------------------------

TITLE: Creating CFFI Extension with Libraries (Python)
DESCRIPTION: This Python snippet demonstrates how to define a CFFI extension using `create_extension` in PyTorch, specifically highlighting the need to explicitly list required libraries like 'ATen' and '_C' in the `libraries` argument for successful compilation on Windows due to experimental support. Additional CUDA libraries might also be necessary depending on the extension.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#_snippet_2

LANGUAGE: python
CODE:
```
ffi = create_extension(
    '_ext.my_lib',
    headers=headers,
    sources=sources,
    define_macros=defines,
    relative_to=__file__,
    with_cuda=with_cuda,
    extra_compile_args=["-std=c99"],
    libraries=['ATen', '_C'] # Append cuda libraries when necessary, like cudart
)
```

----------------------------------------

TITLE: Mathematical Formulation of Quantization Process in PyTorch
DESCRIPTION: This snippet defines the mathematical formulation of how data is quantized and dequantized in PyTorch's quantization framework, including the equations for scale and zero point calculation in both symmetric and asymmetric quantization schemes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization-support.rst#2025-04-22_snippet_0

LANGUAGE: LaTeX
CODE:
```
\begin{aligned}
    \text{Quantization:}&\\
    &Q_\text{out} = \text{clamp}(x_\text{input}/s+z, Q_\text{min}, Q_\text{max})\\
    \text{Dequantization:}&\\
    &x_\text{out} = (Q_\text{input}-z)*s
\end{aligned}
```

----------------------------------------

TITLE: Python-Printed TorchScript Module Code
DESCRIPTION: Presents the Python-like code generated by the TorchScript Python Printer from the module's internal graph. This output demonstrates the serialization mechanism where the graph's structure is translated back into readable Python syntax, which can then be re-imported to reconstruct the original graph.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_43

LANGUAGE: Python
CODE:
```
def forward(self,
    x: Tensor,
    y: int,
    z: float) -> Tensor:
  if torch.gt(y, 2):
    x0 = torch.add(x, z, 1)
  else:
    x0 = torch.add(x, y, 1)
  return x0
```

----------------------------------------

TITLE: Analyzing ATen Log Softmax Operations in PyTorch
DESCRIPTION: This snippet records the usage patterns of the \"aten._log_softmax.default\" operator in PyTorch, detailing tensor shapes, dimensions, and parameters used in the operation. This operator computes the log of the softmax values for given inputs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_0

LANGUAGE: pseudocode
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([1024, 50265], f16), 1, False), {})
```

----------------------------------------

TITLE: Run Specific PyTorch Python Test Method (bash)
DESCRIPTION: Executes a single test method (`test_Sequential`) from a specific test class (`TestJit`) within the `test/test_jit.py` file. This command is useful for targeting individual tests for focused debugging or verification.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_15

LANGUAGE: bash
CODE:
```
python test/test_jit.py TestJit.test_Sequential
```

----------------------------------------

TITLE: Native Layer Normalization Backward - PyTorch Aten
DESCRIPTION: Computes the gradient for Layer Normalization. This internal operator is used during the backward pass of `native_layer_norm`. It requires the gradient of the output, the input tensor, the normalized shape, saved mean and variance from the forward pass, optional weight and bias, and a mask indicating which gradients to compute.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BartForConditionalGeneration_training.txt#_snippet_19

LANGUAGE: Python
CODE:
```
import torch

grad_output = torch.randn(2, 1024, 1024, dtype=torch.float16)
input_tensor = torch.randn(2, 1024, 1024, dtype=torch.float16)
normalized_shape = [1024]
mean = torch.randn(2, 1024, 1, dtype=torch.float32)
var = torch.randn(2, 1024, 1, dtype=torch.float32)
weight = torch.randn(1024, dtype=torch.float16)
bias = torch.randn(1024, dtype=torch.float16)
grads = torch.native_layer_norm_backward(grad_output, input_tensor, normalized_shape, mean, var, weight, bias, [True, True, True])
```

----------------------------------------

TITLE: Backward Layer Norm Calculation with native_layer_norm in PyTorch (Python)
DESCRIPTION: Executes the backward computation for layer normalization via aten.native_layer_norm_backward, computing gradients necessary for parameter updates within the layer normalization step.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_19

LANGUAGE: Python
CODE:
```
aten.native_layer_norm_backward.default
cnt: 25, ((T([16, 512, 768], f16), T([16, 512, 768], f16), [768], T([16, 512, 1], f32), T([16, 512, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})
```

----------------------------------------

TITLE: Defining Boolean Operations in TorchScript
DESCRIPTION: Specifies the syntax for boolean operations (and, or, not) in TorchScript. These operators work on boolean values and can be customized for user-defined objects.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_22

LANGUAGE: python
CODE:
```
or_test  ::=  and_test | or_test 'or' and_test
and_test ::=  not_test | and_test 'and' not_test
not_test ::=  'bool' '(' or_expr ')' | comparison | 'not' not_test
```

----------------------------------------

TITLE: Running Tests and Generating Coverage Report Separately
DESCRIPTION: This snippet demonstrates how to run tests separately and then generate a coverage report without re-running the tests, useful for accelerating development.
SOURCE: https://github.com/pytorch/pytorch/blob/main/tools/code_coverage/README.md#2025-04-22_snippet_8

LANGUAGE: bash
CODE:
```
# run tests when you are developing a new feature, assume the test is `test_nn.py`
python oss_coverage.py --run-only=test_nn.py
# or you can run it yourself
cd test/ && python test_nn.py
# then you want to learn about code coverage, you can just run:
python oss_coverage.py --run-only=test_nn.py --export --summary
```

----------------------------------------

TITLE: Copy Tensor Content in PyTorch
DESCRIPTION: Data on aten.copy_.default reveals operator usage for copying data from one tensor to another, especially applicable in preserving tensor values through transformations in operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
Operator: aten.copy_.default
cnt: 1, ((T([1, 512], i64), T([1, 512], i64)), {})
```

----------------------------------------

TITLE: Registering Schema-Only PyTorch Operators
DESCRIPTION: This snippet shows how to register an operator without implementing a kernel. This approach is useful for defining the interface of an operator before implementing its functionality.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_11

LANGUAGE: cpp
CODE:
```
static auto registry = torch::RegisterOperators()
   .op("my_namespace::my_op(Tensor a, Tensor b) -> Tensor");
```

----------------------------------------

TITLE: Demonstrating Tensor Promotion in TorchScript If Statements
DESCRIPTION: Shows how 1-dimensional tensors are promoted to bool in if statements, while multi-dimensional tensors raise an error.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_28

LANGUAGE: python
CODE:
```
import torch

@torch.jit.script
def fn(x: torch.Tensor):
    if x: # The tensor gets promoted to bool
        return True
    return False
print(fn(torch.rand(1)))
```

----------------------------------------

TITLE: Understanding PyTorch Tensors Retrieval Mechanism
DESCRIPTION: This Python code snippet provides an insight into how PyTorch packs and unpacks tensors during autograd operations. Using '.exp()' function, the code shows that the unpacked tensor retrieved through 'grad_fn._saved_result' might be a different object while maintaining the same storage. Requires 'torch' library.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/autograd.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
x = torch.randn(5, requires_grad=True)
y = x.exp()
print(y.equal(y.grad_fn._saved_result))  # True
print(y is y.grad_fn._saved_result)  # False
```

----------------------------------------

TITLE: PyTorch Bilinear Upsampling Operations
DESCRIPTION: Usage of aten.upsample_bilinear2d.vec operator with various input tensor shapes and stride configurations. Includes upsampling to different target sizes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_15

LANGUAGE: python
CODE:
```
Operator: aten.upsample_bilinear2d.vec
cnt: 1, ((T([1, 3, 427, 640], f16, stride=(3, 1, 1920, 3)), [799, 1199], False, None), {})
cnt: 1, ((T([1, 3, 612, 612], f16, stride=(3, 1, 1836, 3)), [800, 800], False, None), {})
cnt: 1, ((T([1, 3, 640, 443], f16, stride=(3, 1, 1329, 3)), [1155, 800], False, None), {})
cnt: 1, ((T([1, 3, 459, 640], f16, stride=(3, 1, 1920, 3)), [799, 1115], False, None), {})
```

----------------------------------------

TITLE: Creating Conda Environment for PyTorch (Bash)
DESCRIPTION: Sets up a dedicated conda environment named `pytorch-myfeature` to isolate a specific PyTorch build. It then activates the new environment before running `python setup.py develop`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_39

LANGUAGE: bash
CODE:
```
conda create -n pytorch-myfeature
source activate pytorch-myfeature
# if you run python now, torch will NOT be installed
python setup.py develop
```

----------------------------------------

TITLE: Implementing Relative Positional Embedding with First-Class Dimensions in Python
DESCRIPTION: This snippet demonstrates how to implement relative positional embeddings using first-class dimensions in PyTorch. It uses dimensions for indexing arithmetic and embedding lookup.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
def relative_positional_embedding(q, k, distance_embedding_weight):
    batch, query_sequence, key_sequence, heads, features = dims(5)
    q = q[batch, query_sequence, [heads, features]]
    k = k[batch, key_sequence, [heads, features]]

    distance = query_sequence - key_sequence
    n_embeddings = distance_embedding_weight.size(0)
    index_bias = n_embeddings // 2

    assert key_sequence.size + bias <= n_embeddings

    # indexing with dims
    positional_embedding = distance_embedding_weight[distance + index_bias, features]

    # matrix multiplies with dims
    relative_position_scores_query = (q*positional_embedding).sum(features)
    relative_position_scores_key = (k*positional_embedding).sum(features)
    return  (relative_position_scores_query + relative_position_scores_key).order(batch, heads, key_sequence, query_sequence)
```

----------------------------------------

TITLE: PyTorch Slice Operations
DESCRIPTION: Tensor slicing operations with different dimension specifications and stride patterns
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_botnext26ts_256_training.txt#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
((T([4096, 8, 8], f16), [4096, 8, 15], 2, 7, 9223372036854775807, 1), {})
```

----------------------------------------

TITLE: Downloading Mixed MM Datasets for A100/H100
DESCRIPTION: Downloads the pre-existing datasets for A100 and H100 GPUs used in mixed matrix multiplication heuristics
SOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/mixed_mm/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
bash get_mixedmm_dataset.sh
```

----------------------------------------

TITLE: Importing torch.overrides Module in Python
DESCRIPTION: This snippet shows how to import the torch.overrides module in Python. It sets the current module context for the subsequent documentation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.overrides.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
.. currentmodule:: torch.overrides
```

----------------------------------------

TITLE: Tensor Equality Comparisons
DESCRIPTION: Element-wise equality comparisons between tensors of different shapes and data types.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
((T([5000, 4], f32), T([4], f16)), {})
```

----------------------------------------

TITLE: Analyzing Tensor Shapes and Strides in PyTorch
DESCRIPTION: This code snippet represents a collection of tensor shape analyses in PyTorch. Each line shows the count of occurrences, tensor shapes, data types, and stride information. The tensors vary in dimensions from 4D to 2D, primarily using float16 (f16) data type.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2net101_26w_4s_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([64, 2048, 7, 7], f16), T([64, 2048, 7, 7], f16), 0), {})
cnt: 5, ((T([64, 208, 7, 7], f16, stride=(40768, 49, 7, 1)), T([64, 208, 7, 7], f16), 0), {})
cnt: 4, ((T([64, 208, 7, 7], f16), T([64, 208, 7, 7], f16), 0), {})
cnt: 2, ((T([64, 832, 7, 7], f16), T([64, 832, 7, 7], f16), 0), {})
cnt: 1, ((T([64, 832, 14, 14], f16), T([64, 832, 14, 14], f16), 0), {})
cnt: 23, ((T([64, 1024, 14, 14], f16), T([64, 1024, 14, 14], f16), 0), {})
cnt: 25, ((T([64, 104, 14, 14], f16, stride=(81536, 196, 14, 1)), T([64, 104, 14, 14], f16), 0), {})
cnt: 44, ((T([64, 104, 14, 14], f16), T([64, 104, 14, 14], f16), 0), {})
cnt: 22, ((T([64, 416, 14, 14], f16), T([64, 416, 14, 14], f16), 0), {})
cnt: 1, ((T([64, 416, 28, 28], f16), T([64, 416, 28, 28], f16), 0), {})
cnt: 4, ((T([64, 512, 28, 28], f16), T([64, 512, 28, 28], f16), 0), {})
cnt: 6, ((T([64, 52, 28, 28], f16, stride=(163072, 784, 28, 1)), T([64, 52, 28, 28], f16), 0), {})
cnt: 6, ((T([64, 52, 28, 28], f16), T([64, 52, 28, 28], f16), 0), {})
cnt: 3, ((T([64, 208, 28, 28], f16), T([64, 208, 28, 28], f16), 0), {})
cnt: 1, ((T([64, 208, 56, 56], f16), T([64, 208, 56, 56], f16), 0), {})
cnt: 3, ((T([64, 256, 56, 56], f16), T([64, 256, 56, 56], f16), 0), {})
cnt: 5, ((T([64, 26, 56, 56], f16, stride=(326144, 3136, 56, 1)), T([64, 26, 56, 56], f16), 0), {})
cnt: 4, ((T([64, 26, 56, 56], f16), T([64, 26, 56, 56], f16), 0), {})
cnt: 3, ((T([64, 104, 56, 56], f16), T([64, 104, 56, 56], f16), 0), {})
cnt: 1, ((T([64, 64, 112, 112], f16), T([64, 64, 112, 112], f16), 0), {})
```

----------------------------------------

TITLE: Invoking Log Softmax Operator in PyTorch
DESCRIPTION: Demonstrates the use of the aten._log_softmax.default operator for performing log-softmax operations along a specified dimension on a float16 tensor. This operator is essential in scenarios requiring the computation of log probabilities where numerical stability is crucial.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaForQuestionAnswering_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten._log_softmax.default
cnt: 2, ((T([4, 512], f16), 1, False), {})
```

----------------------------------------

TITLE: Generating Final Heuristic
DESCRIPTION: Command to generate the final heuristic after collecting all training data
SOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/mm/README.md#2025-04-22_snippet_5

LANGUAGE: bash
CODE:
```
bash generate_heuristic_mm.sh generate
```

----------------------------------------

TITLE: Backward Pass for Tanh Activation in PyTorch
DESCRIPTION: Computes the gradient for the backward pass of the tanh function, integrating into broader backpropagation processes. Requires input tensors and their associated gradients. Outputs are tensors prepared for further gradient calculations, contributing to efficient backpropagation and training.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_17

LANGUAGE: Python
CODE:
```
Operator: aten.tanh_backward.default
cnt: 1, ((T([16, 3, 128, 128], f16, stride=(0, 0, 0, 0)), T([16, 3, 128, 128], f16)), {})
```

----------------------------------------

TITLE: Conditionally Adding UBSAN Link Libraries for test_jit CMake
DESCRIPTION: Adds the `-fsanitize=undefined` link flag to the `test_jit` executable target. This ensures the necessary sanitizer libraries are linked when `USE_ASAN` is enabled.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#_snippet_14

LANGUAGE: CMake
CODE:
```
if(USE_ASAN)
  target_link_libraries(test_jit PRIVATE "-fsanitize=undefined")
endif()
```

----------------------------------------

TITLE: Applying Native Batch Normalization in PyTorch
DESCRIPTION: The native_batch_norm operator normalizes tensor outputs across the specified dimensions. It configures training mode and momentum, factors crucial for adapting model parameters based on batch statistics.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/legacy_senet154_training.txt#2025-04-22_snippet_6

LANGUAGE: text
CODE:
```
cnt: 2, ((T([32, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})
```

LANGUAGE: text
CODE:
```
cnt: 1, ((T([32, 128, 112, 112], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})
```

----------------------------------------

TITLE: Gram Matrix Implementation with Dimensions
DESCRIPTION: Implements a Gram matrix calculation using dimension objects for style transfer applications.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
def gram_matrix_new(y):
    b, c, c2, h, w = dims()
    r = (y[b, c, h, w] * y[b, c2, h, w]).sum((h, w))
    r = r / (h.size * w.size)
    return r.order(b, c, c2)

gram_matrix_new(torch.rand(1, 2, 3, 4))
```

----------------------------------------

TITLE: Configuring reStructuredText Hidden Role
DESCRIPTION: Defines a custom hidden role for documentation styling with an associated CSS class.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/_templates/autosummary/classnoinheritance.rst#2025-04-22_snippet_0

LANGUAGE: rst
CODE:
```
.. role:: hidden
    :class: hidden-section
```

----------------------------------------

TITLE: Invoking aten.native_layer_norm_backward.default Operator (Log)
DESCRIPTION: Log entries detailing calls to the PyTorch `aten.native_layer_norm_backward.default` operator. Each line shows the invocation count (`cnt`) and the specific arguments passed, including input tensor shapes, data types (f16, f32), normalized shape dimensions, mean/variance tensors, weight/bias tensors, and boolean flags indicating gradient requirements.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/crossvit_9_240_training.txt#2025-04-22_snippet_2

LANGUAGE: text
CODE:
```
Operator: aten.native_layer_norm_backward.default
cnt: 22, ((T([64, 197, 256], f16), T([64, 197, 256], f16), [256], T([64, 197, 1], f32), T([64, 197, 1], f32), T([256], f16), T([256], f16), [True, True, True]), {})
cnt: 10, ((T([64, 401, 128], f16), T([64, 401, 128], f16), [128], T([64, 401, 1], f32), T([64, 401, 1], f32), T([128], f16), T([128], f16), [True, True, True]), {})
cnt: 3, ((T([64, 1, 128], f16), T([64, 1, 128], f16), [128], T([64, 1, 1], f32), T([64, 1, 1], f32), T([128], f16), T([128], f16), [True, True, True]), {})
cnt: 3, ((T([64, 1, 256], f16), T([64, 1, 256], f16), [256], T([64, 1, 1], f32), T([64, 1, 1], f32), T([256], f16), T([256], f16), [True, True, True]), {})
cnt: 3, ((T([64, 1, 256], f16), T([64, 1, 256], f16, stride=(50432, 256, 1)), [256], T([64, 1, 1], f32), T([64, 1, 1], f32), T([256], f16), T([256], f16), [True, True, True]), {})
cnt: 3, ((T([64, 1, 128], f16), T([64, 1, 128], f16, stride=(51328, 128, 1)), [128], T([64, 1, 1], f32), T([64, 1, 1], f32), T([128], f16), T([128], f16), [True, True, True]), {})
```

----------------------------------------

TITLE: PyTorch Tensor Shape Patterns and Batch Normalization
DESCRIPTION: Log entries showing tensor shape patterns used in batch normalization operations, including input tensors, running statistics, and parameters. Shows repeated patterns with varying channel depths (64 to 2048) and spatial dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet50_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
cnt: 5, ((T([32, 512, 7, 7], f16), T([32, 512, 7, 7], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), False, 1e-05, [True, True, True]), {})
```

----------------------------------------

TITLE: Defining Custom Op with ONNX-script (Part 5: Exporting Model)
DESCRIPTION: Exports the PyTorch model to ONNX, specifying the target opset version and providing a custom opset mapping for the "onnx-script" domain. This export process will utilize the registered custom_selu symbolic, which internally uses the ONNX-script Selu function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_29

LANGUAGE: Python
CODE:
```
torch.onnx.export(
    model,
    x,
    "model.onnx",
    opset_version=opset_version,
    # only needed if you want to specify an opset version > 1.
    custom_opsets={"onnx-script": 2}
)
```

----------------------------------------

TITLE: Tensor Multiplication Operations in PyTorch
DESCRIPTION: Profiling data for tensor multiplication operations showing count and tensor shapes. These elementwise multiplications are used throughout the network, including operations for squeeze-and-excitation blocks where channel attention is applied.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
Operator: aten.mul.Tensor
cnt: 2, ((T([32, 32, 112, 112], f16), T([32, 32, 1, 1], f16)), {})
cnt: 2, ((T([32, 96, 56, 56], f16), T([32, 96, 1, 1], f16)), {})
cnt: 2, ((T([32, 144, 56, 56], f16), T([32, 144, 1, 1], f16)), {})
cnt: 2, ((T([32, 144, 28, 28], f16), T([32, 144, 1, 1], f16)), {})
cnt: 2, ((T([32, 240, 28, 28], f16), T([32, 240, 1, 1], f16)), {})
cnt: 2, ((T([32, 240, 14, 14], f16), T([32, 240, 1, 1], f16)), {})
cnt: 6, ((T([32, 480, 14, 14], f16), T([32, 480, 1, 1], f16)), {})
cnt: 4, ((T([32, 672, 14, 14], f16), T([32, 672, 1, 1], f16)), {})
cnt: 2, ((T([32, 672, 7, 7], f16), T([32, 672, 1, 1], f16)), {})
cnt: 8, ((T([32, 1152, 7, 7], f16), T([32, 1152, 1, 1], f16)), {})
cnt: 4, ((T([32, 1152, 7, 7], f16), T([32, 1152, 7, 7], f16)), {})
cnt: 1, ((T([32, 672, 7, 7], f16), T([32, 672, 7, 7], f16)), {})
cnt: 2, ((T([32, 672, 14, 14], f16), T([32, 672, 14, 14], f16)), {})
cnt: 3, ((T([32, 480, 14, 14], f16), T([32, 480, 14, 14], f16)), {})
cnt: 1, ((T([32, 240, 14, 14], f16), T([32, 240, 14, 14], f16)), {})
cnt: 1, ((T([32, 240, 28, 28], f16), T([32, 240, 28, 28], f16)), {})
cnt: 1, ((T([32, 144, 28, 28], f16), T([32, 144, 28, 28], f16)), {})
cnt: 1, ((T([32, 144, 56, 56], f16), T([32, 144, 56, 56], f16)), {})
cnt: 1, ((T([32, 96, 56, 56], f16), T([32, 96, 56, 56], f16)), {})
cnt: 1, ((T([32, 32, 112, 112], f16), T([32, 32, 112, 112], f16)), {})
```

----------------------------------------

TITLE: Tracking PyTorch Softmax Operations in Deep Learning Model
DESCRIPTION: Details of log_softmax and its backward operation usage with 128 batch size and 1000 output classes (likely classification model). Shows tensor shapes and data types (f16/half precision).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hardcorenas_a_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([128, 1000], f16), 1, False), {})
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})
```

----------------------------------------

TITLE: Defining a FakeTensor Data Structure in PyTorch (Python)
DESCRIPTION: This Python class outlines the structure for a FakeTensor, which stores tensor metadata (such as size, dtype, device, and optionally dimension order) for use in symbolic or fake tracing contexts. All fields are explicitly typed, with dim_order denoted as a work-in-progress feature. The class facilitates shape/dtype inferences for graph construction without real data.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
class FakeTensor:
  size: List[SymInt]
  dtype: torch.dtype
  device: torch.device
  dim_order: List[int]  # This doesn't exist yet
```

----------------------------------------

TITLE: PyTorch Tensor Sum Operation with Symbol Dimensions
DESCRIPTION: This section documents a symbolic integer sum operation in PyTorch, operating on a tensor with 64 samples and 1000 output features (likely a classification model). The sum is performed along dimension 0 with keepdim=True to preserve dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([64, 1000], f16), [0], True), {})
```

----------------------------------------

TITLE: Aggregate Sum along Dimensional Indices with ATen
DESCRIPTION: Performs summation of elements over specified dimensions. Requires tensor inputs with dimensional lists, like [0]. Outputs minimized tensors with relevant elements' sum across dimensions, beneficial for dimensionality reduction in pre-processing tasks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_15

LANGUAGE: Python
CODE:
```
Operator: aten.sum.dim_IntList
cnt: 4, ((T([16, 64], f16), [0]), {})
cnt: 4, ((T([16, 128], f16), [0]), {})
cnt: 26, ((T([16, 256], f16), [0]), {})
```

----------------------------------------

TITLE: PyTorch ReLU In-place Operations
DESCRIPTION: Log entries for in-place ReLU activation operations on tensors of various shapes. Shows the operation being applied to feature maps with different channel depths and spatial dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet50_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
Operator: aten.relu_.default
cnt: 1, ((T([32, 64, 112, 112], f16),), {})
cnt: 6, ((T([32, 64, 56, 56], f16),), {})
```

----------------------------------------

TITLE: Analyzing ATen GELU Operations in PyTorch
DESCRIPTION: Tracks the invocation of \"aten.gelu.default\", which applies the Gaussian Error Linear Unit activation function to input tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_13

LANGUAGE: pseudocode
CODE:
```
Operator: aten.gelu.default
cnt: 12, ((T([8, 128, 4096], f16),), {})
```

----------------------------------------

TITLE: Utilizing ATen Upsample Bilinear2d in PyTorch: Python
DESCRIPTION: This snippet explores the use of the 'upsample_bilinear2d' operator and its backward variant in PyTorch. It describes how bi-linear up-sampling is performed on tensors of various shapes, emphasizing the scaling factors and the optionality in parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_16

LANGUAGE: Python
CODE:
```
Operator: aten.upsample_bilinear2d.vec
cnt: 2, ((T([3, 256, 128, 128], f16), None, True, [2.0, 2.0]), {})
cnt: 1, ((T([3, 128, 256, 256], f16), None, True, [2.0, 2.0]), {})
cnt: 1, ((T([3, 256, 256, 256], f16), None, True, [2.0, 2.0]), {})
Operator: aten.upsample_bilinear2d_backward.vec
cnt: 1, ((T([3, 256, 512, 512], f16), None, [3, 256, 256, 256], True, [2.0, 2.0]), {})
```

----------------------------------------

TITLE: Representing LSTM Cell as JIT Graph
DESCRIPTION: This snippet shows the graph representation of a single LSTM cell computation within the PyTorch JIT. It details the sequence of operations (e.g., aten::mm, aten::add, aten::chunk) performed on input tensors to produce the output tensors, including constant values and list unpacking.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_20

LANGUAGE: JIT Graph
CODE:
```
graph(%x : Tensor,
      %hx : Tensor,
      %cx : Tensor,
      %w_ih : Tensor,
      %w_hh : Tensor,
      %b_ih : Tensor,
      %b_hh : Tensor):
  %7 : int = prim::Constant[value=4]()
  %8 : int = prim::Constant[value=1]()
  %9 : Tensor = aten::t(%w_ih)
  %10 : Tensor = aten::mm(%x, %9)
  %11 : Tensor = aten::t(%w_hh)
  %12 : Tensor = aten::mm(%hx, %11)
  %13 : Tensor = aten::add(%10, %12, %8)
  %14 : Tensor = aten::add(%13, %b_ih, %8)
  %gates : Tensor = aten::add(%14, %b_hh, %8)
  %16 : Tensor[] = aten::chunk(%gates, %7, %8)
  %ingate.1 : Tensor, %forgetgate.1 : Tensor, %cellgate.1 : Tensor, %outgate.1 : Tensor = prim::ListUnpack(%16)
  %ingate : Tensor = aten::sigmoid(%ingate.1)
  %forgetgate : Tensor = aten::sigmoid(%forgetgate.1)
  %cellgate : Tensor = aten::tanh(%cellgate.1)
  %outgate : Tensor = aten::sigmoid(%outgate.1)
  %25 : Tensor = aten::mul(%forgetgate, %cx)
  %26 : Tensor = aten::mul(%ingate, %cellgate)
  %cy : Tensor = aten::add(%25, %26, %8)
  %28 : Tensor = aten::tanh(%cy)
  %hy : Tensor = aten::mul(%outgate, %28)
  %30 : (Tensor, Tensor) = prim::TupleConstruct(%hy, %cy)
  return (%30)
```

----------------------------------------

TITLE: Loss Function Computation for Model Training
DESCRIPTION: Shows the negative log-likelihood loss calculation used for training the classification model. This operation takes logits of shape [2, 1000] and target labels of shape [2], ignoring entries with value -100.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
Operator: aten.nll_loss_backward.default
cnt: 1, ((T([], f16), T([2, 1000], f16), T([2], i64), None, 1, -100, T([], f16)), {})
Operator: aten.nll_loss_forward.default
cnt: 1, ((T([2, 1000], f16), T([2], i64), None, 1, -100), {})
```

----------------------------------------

TITLE: Defining Custom Op with ONNX-script (Part 2: ONNX-script Function)
DESCRIPTION: Defines the Selu function using ONNX-script. This function implements the SELU operation using a combination of standard ONNX operators like CastLike, Exp, multiplication, and Where, grouped under a custom opset domain.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_26

LANGUAGE: Python
CODE:
```
custom_opset = onnxscript.values.Opset(domain="onnx-script", version=1)

@onnxscript.script(custom_opset)
def Selu(X):
    alpha = 1.67326  # auto wrapped as Constants
    gamma = 1.0507
    alphaX = op.CastLike(alpha, X)
    gammaX = op.CastLike(gamma, X)
    neg = gammaX * (alphaX * op.Exp(X) - alphaX)
    pos = gammaX * X
    zero = op.CastLike(0, X)
    return op.Where(X <= zero, neg, pos)
```

----------------------------------------

TITLE: Python Timer Creation for TorchScript Mode
DESCRIPTION: Example of creating a Timer instance for TorchScript mode in Python. It includes statement code to call the JIT model and setup code that loads the model and warms it up.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/instruction_counts/README.md#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
Timer(
    stmt="""
        y = jit_model(x, w)
    """,
    setup=""",
        # benchmark.setup.py_setup
        # jit_model = torch.jit.load(...)
        # Warm up jit_model
    """,
)
```

----------------------------------------

TITLE: Implementing Custom Rendezvous Handler for TorchElastic in Python
DESCRIPTION: This snippet shows how to implement a custom rendezvous handler by extending the RendezvousHandler class and passing it to the worker specification when creating an agent.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/customization.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
spec = WorkerSpec(
    rdzv_handler=MyRendezvousHandler(params),
    ...
)
elastic_agent = LocalElasticAgent(spec, start_method=start_method)
elastic_agent.run(spec.role)
```

----------------------------------------

TITLE: Generating Coverage Summary for Different Folder
DESCRIPTION: This example shows how to generate a coverage summary for a different folder of interest without re-running the tests.
SOURCE: https://github.com/pytorch/pytorch/blob/main/tools/code_coverage/README.md#2025-04-22_snippet_7

LANGUAGE: bash
CODE:
```
# after run this command
python oss_coverage.py --run-only=atest --interest-only=aten
# you may then want to learn atest's coverage over c10, instead of running the test again, you can:
python oss_coverage.py --run-only=atest --interest-only=c10 --summary
```

----------------------------------------

TITLE: Applying ReLU Activation in PyTorch
DESCRIPTION: This snippet demonstrates the usage of ReLU (Rectified Linear Unit) activation function on tensors with various shapes. It shows the count of operations for each unique tensor configuration.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_15

LANGUAGE: Python
CODE:
```
Operator: aten.relu_.default
cnt: 2, ((T([32, 64, 112, 112], f16),), {})
cnt: 6, ((T([32, 128, 56, 56], f16),), {})
cnt: 1, ((T([32, 256, 56, 56], f16),), {})
cnt: 5, ((T([32, 160, 28, 28], f16),), {})
cnt: 1, ((T([32, 512, 28, 28], f16),), {})
cnt: 10, ((T([32, 192, 14, 14], f16),), {})
cnt: 2, ((T([32, 768, 14, 14], f16),), {})
cnt: 10, ((T([32, 224, 7, 7], f16),), {})
cnt: 2, ((T([32, 1024, 7, 7], f16),), {})
```

----------------------------------------

TITLE: Warning Message in reStructuredText
DESCRIPTION: Displays a warning message about distributed optimizer not being supported with CUDA tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.optim.rst#2025-04-22_snippet_1

LANGUAGE: reStructuredText
CODE:
```
.. warning ::
    Distributed optimizer is not currently supported when using CUDA tensors
```

----------------------------------------

TITLE: Applying Sigmoid Activation in PyTorch
DESCRIPTION: This snippet applies the sigmoid activation function to tensors. Sigmoid is commonly used in neural networks to squash values between 0 and 1, often in the output layer for binary classification or as gates in LSTM cells.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_13

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([8, 3, 12, 16, 2], f16, stride=(48960, 16320, 1360, 85, 1)),), {})
cnt: 1, ((T([8, 3, 24, 32, 2], f16, stride=(195840, 65280, 2720, 85, 1)),), {})
cnt: 1, ((T([8, 3, 48, 64, 2], f16, stride=(783360, 261120, 5440, 85, 1)),), {})
```

----------------------------------------

TITLE: Analyzing Tensor Copy Operations in PyTorch
DESCRIPTION: This snippet shows the usage of tensor copy operations in the model. It includes the shapes of the tensors being copied and the frequency of these operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
Operator: aten.copy_.default
cnt: 1, ((T([128, 3, 224, 224], f16), T([128, 3, 224, 224], f16)), {})
cnt: 15, ((T([128, 160, 7, 7], f16), T([128, 160, 7, 7], f16)), {})
cnt: 6, ((T([128, 112, 14, 14], f16), T([128, 112, 14, 14], f16)), {})
cnt: 12, ((T([128, 80, 14, 14], f16), T([128, 80, 14, 14], f16)), {})
```

----------------------------------------

TITLE: Creating Benchmarking Function
DESCRIPTION: Implements a benchmarking utility to measure forward and backward pass latencies
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/aot_autograd_optimizations.ipynb#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import time
import statistics

def bench(fn, args, prefix):
    warmup = 10
    iterations = 100

    for _ in range(warmup):
        ref = fn(*args)
        ref.sum().backward()
    
    fw_latencies = []
    bw_latencies = []
    for _ in range(iterations):
        for arg in args:
            arg.grad = None

        fw_begin = time.perf_counter()
        ref = fn(*args)
        fw_end = time.perf_counter()

        loss = ref.sum() 

        bw_begin = time.perf_counter()
        loss.backward()
        bw_end = time.perf_counter()

        fw_latencies.append(fw_end - fw_begin)
        bw_latencies.append(bw_end - bw_begin)
    
    avg_fw_latency = statistics.mean(fw_latencies) * 10**6
    avg_bw_latency = statistics.mean(bw_latencies) * 10**6
    print(prefix, "Fwd = " + str(avg_fw_latency) + " us", "Bwd = " + str(avg_bw_latency) + " us", sep=', ')
```

----------------------------------------

TITLE: Setting Up PyTorch Development Environment (Bash)
DESCRIPTION: This snippet provides a series of bash commands to clone the PyTorch repository, navigate into it, add the upstream remote, set up the development environment using `make setup-env` (with options for CUDA/ROCm), and activate the Python virtual environment. It requires Git and Make.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_0

LANGUAGE: bash
CODE:
```
git clone git@github.com:<USERNAME>/pytorch.git
cd pytorch
git remote add upstream git@github.com:pytorch/pytorch.git

make setup-env
# Or run `make setup-env-cuda` for pre-built CUDA binaries
# Or run `make setup-env-rocm` for pre-built ROCm binaries
source venv/bin/activate  # or `& .\venv\Scripts\Activate.ps1` on Windows
```

----------------------------------------

TITLE: Installing py-spy Profiler (Bash)
DESCRIPTION: Installs the py-spy profiling tool using pip. py-spy is a sampling profiler for Python with native code profiling capabilities. Requires pip to be installed and accessible.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_36

LANGUAGE: bash
CODE:
```
pip install py-spy
```

----------------------------------------

TITLE: Backpropagation Operations in PyTorch CNN Model
DESCRIPTION: This code shows the backpropagation operations for the convolutional layers of the model. It demonstrates gradient flow through convolutions with various kernel sizes, handling residual connections, and propagating gradients across different feature scales.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/visformer_small_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
Operator: aten.convolution_backward.default
cnt: 4, ((T([128, 768, 7, 7], f16), T([128, 3072, 7, 7], f16), T([768, 3072, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 4, ((T([128, 3072, 7, 7], f16), T([128, 768, 7, 7], f16), T([3072, 768, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 4, ((T([128, 768, 7, 7], f16), T([128, 768, 7, 7], f16), T([768, 768, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 4, ((T([128, 2304, 7, 7], f16), T([128, 768, 7, 7], f16), T([2304, 768, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 768, 7, 7], f16), T([128, 384, 14, 14], f16), T([768, 384, 2, 2], f16), [768], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 4, ((T([128, 384, 14, 14], f16), T([128, 1536, 14, 14], f16), T([384, 1536, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 4, ((T([128, 1536, 14, 14], f16), T([128, 384, 14, 14], f16), T([1536, 384, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 4, ((T([128, 384, 14, 14], f16), T([128, 384, 14, 14], f16), T([384, 384, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 4, ((T([128, 1152, 14, 14], f16), T([128, 384, 14, 14], f16), T([1152, 384, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 384, 14, 14], f16), T([128, 192, 28, 28], f16), T([384, 192, 2, 2], f16), [384], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 7, ((T([128, 192, 28, 28], f16), T([128, 384, 28, 28], f16), T([192, 384, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 7, ((T([128, 384, 28, 28], f16), T([128, 384, 28, 28], f16), T([384, 48, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 8, [True, True, False]), {})
cnt: 7, ((T([128, 384, 28, 28], f16), T([128, 192, 28, 28], f16), T([384, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 192, 28, 28], f16), T([128, 32, 112, 112], f16), T([192, 32, 4, 4], f16), [192], [4, 4], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 1, ((T([128, 32, 112, 112], f16), T([128, 3, 224, 224], f16), T([32, 3, 7, 7], f16), [0], [2, 2], [3, 3], [1, 1], False, [0, 0], 1, [False, True, False]), {})
```

----------------------------------------

TITLE: Recording an Event in PyTorch Distributed Elastic
DESCRIPTION: This function is used to record an event in the distributed elastic system. It's part of the events API and likely takes parameters to describe the event being recorded.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/events.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
torch.distributed.elastic.events.record
```

----------------------------------------

TITLE: Replacing Add with Mul and Sub Using PyTorch FX Transformer
DESCRIPTION: This transformer replaces a single add operation with a multiplication followed by a subtraction in a PyTorch FX graph.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
class ReplaceAddWithMulSub(torch.fx.Transformer):
    """
    Original:
        def f(x, y):
            return x + y

    After pass:
        def f(x, y):
            z = x * y
            return z - y
    """
    def call_function(self, target, args, kwargs):
        if target != torch.ops.aten.add.Tensor:
            return super().call_function(target, args, kwargs)

        x, y = args

        mul_res = super().call_function(torch.ops.aten.mul.Tensor, args, {})
        return super().call_function(torch.ops.aten.sub.Tensor, (mul_res, y), {})

transformed_graph_module = ReplaceAddWithMulSub(graph_module).transform()
```

----------------------------------------

TITLE: Concatenating Tensors with ATen Cat Operator
DESCRIPTION: Illustrates `aten.cat`, used to concatenate tensors along a specified dimension, typically for merging feature maps. Examples use tensors like [64, 512, 8, 8], demonstrating its utility in increasing feature space dimensionality.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_10

LANGUAGE: plaintext
CODE:
```
Operator: aten.cat.default
cnt: 1, (([T([64, 512, 8, 8], f16), T([64, 512, 8, 8], f16), T([64, 512, 8, 8], f16)], 1), {})
```

----------------------------------------

TITLE: Configuring Intel XPU Support for PyTorch Python
DESCRIPTION: Sets up Intel XPU (GPU/accelerator) support for PyTorch Python bindings when the USE_XPU option is enabled.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_9

LANGUAGE: CMake
CODE:
```
if(USE_XPU)
    include(${TORCH_ROOT}/cmake/public/xpu.cmake)
    append_filelist("libtorch_python_xpu_sources" TORCH_PYTHON_SRCS)

    list(APPEND TORCH_PYTHON_COMPILE_DEFINITIONS USE_XPU)
endif()
```

----------------------------------------

TITLE: Conditional Contiguous Tensor Handling - C++
DESCRIPTION: This snippet adapts tensor handling based on condition checks by either calling `newContiguous` or retaining the tensor directly. It employs the conditional freeing strategy to ensure proper reference management, suitable for optimizing tensor operations based on stride conditions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/README.md#2025-04-22_snippet_3

LANGUAGE: cpp
CODE:
```
  if (!(k_->stride(3) == 1) || !(k_->stride[2] == k_->size(3))) {
    kernel = THTensor_(newContiguous)(k_);
  } else {
    THTensor_(retain)(k_);
    kernel = k_;
  }
  ...
  c10::raw::intrusive_ptr::decref(kernel);
```

----------------------------------------

TITLE: Defining Graph Node List Structure in Python
DESCRIPTION: This snippet defines the fundamental structure for a Graph in Export IR using Python, where a graph contains a list of Node instances. It prescribes the base schema for representing graphs within torch.fx and PyTorch's Export IR internals. There are no external dependencies needed beyond standard Python for the class skeleton; actual usages assume torch.fx.Node compatibility. The Graph expects a non-empty list of Node objects as its nodes property.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
class Graph:
  nodes: List[Node]
```

----------------------------------------

TITLE: Registering FakeTensor Kernel in PyTorch
DESCRIPTION: This code snippet references the documentation for registering a FakeTensor kernel (meta kernel) in PyTorch, which is used for reasoning about input/output shapes of operators.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_18

LANGUAGE: reStructuredText
CODE:
```
:func:`torch.library.register_fake`
```

----------------------------------------

TITLE: CapabilityBasedPartitioner Usage Example
DESCRIPTION: Demonstrates using CapabilityBasedPartitioner with custom operator support for add and mul operations
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
from torch.fx.passes.infra.partitioner import CapabilityBasedPartitioner
from torch.fx.passes.operator_support import any_chain, OperatorSupportBase

class AddMulOperatorSupport(OperatorSupportBase):
    def is_node_supported(self, submodules, node: torch.fx.Node) -> bool:
        return node.op == "call_function" and node.target in [
            torch.ops.aten.add.Tensor, torch.ops.aten.mul.Tensor,
        ]

capability_partitioner = CapabilityBasedPartitioner(
    graph_module,
    op_support,
)

# Returns a list of partitions (list of nodes that belong in each partition)
partition_list = capability_partitioner.propose_partitions()
# Fuses the partitions into graph modules and inserts `call_module` nodes in the graph
fused_graph_module = capability_partitioner.fuse_partitions(partition_list)
```

----------------------------------------

TITLE: Analyzing Batch Normalization Tensor Shapes in PyTorch
DESCRIPTION: This snippet shows the tensor shapes and parameters for batch normalization operations in a PyTorch model. It includes input tensors, running mean and variance, and other batch norm parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_inception_v3_training.txt#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([128, 96, 17, 17], f16), T([128, 96, 17, 17], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), True, 0.001, [True, True, True]), {})
```

----------------------------------------

TITLE: Calling aten.mm.default (Python)
DESCRIPTION: Performs matrix multiplication (mat1 @ mat2). A fundamental operation in linear algebra and neural networks. Examples show various matrix dimensions and strides with float16 data type.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_22

LANGUAGE: Python
CODE:
```
((T([1024, 1024], f16), T([1024, 50265], f16, stride=(1, 1024))), {})
```

LANGUAGE: Python
CODE:
```
((T([50265, 1024], f16, stride=(1, 50265)), T([1024, 1024], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([1024, 50265], f16), T([50265, 1024], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([1024, 1024], f16), T([1024, 4096], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([1024, 1024], f16, stride=(1, 1024)), T([1024, 4096], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([1024, 4096], f16), T([4096, 1024], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([4096, 1024], f16, stride=(1, 4096)), T([1024, 1024], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([1024, 1024], f16), T([1024, 1024], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([1024, 1024], f16, stride=(1, 1024)), T([1024, 1024], f16)), {})
```

----------------------------------------

TITLE: Tensor Operations with Convolutions and Element-wise Functions
DESCRIPTION: Series of tensor operations showing convolutions with varying input/output channels and spatial dimensions. Operations include convolutions, hardtanh activations, mean pooling and backward passes using half-precision (f16) tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
# Convolution operation examples
((T([128, 106, 14, 14], f16), T([128, 570, 14, 14], f16), T([106, 570, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})

# Element-wise hardtanh operation
((T([128, 32, 112, 112], f16), 0.0, 6.0), {})

# Mean pooling operation
((T([128, 228, 28, 28], f16), [2, 3], True), {})
```

----------------------------------------

TITLE: Configuring CMake for PyTorch Android Native Build
DESCRIPTION: This CMake configuration demonstrates how to set up include directories and link libraries for a PyTorch Android native build. It locates the necessary headers and libraries extracted from the AAR.
SOURCE: https://github.com/pytorch/pytorch/blob/main/android/README.md#2025-04-22_snippet_5

LANGUAGE: CMake
CODE:
```
# Relative path of gradle build directory to CMakeLists.txt
set(build_DIR ${CMAKE_SOURCE_DIR}/build)

file(GLOB PYTORCH_INCLUDE_DIRS "${build_DIR}/pytorch_android*.aar/headers")
file(GLOB PYTORCH_LINK_DIRS "${build_DIR}/pytorch_android*.aar/jni/${ANDROID_ABI}")

set(BUILD_SUBDIR ${ANDROID_ABI})
target_include_directories(${PROJECT_NAME} PRIVATE
  ${PYTORCH_INCLUDE_DIRS}
)

find_library(PYTORCH_LIBRARY pytorch_jni
  PATHS ${PYTORCH_LINK_DIRS}
  NO_CMAKE_FIND_ROOT_PATH)

find_library(FBJNI_LIBRARY fbjni
  PATHS ${PYTORCH_LINK_DIRS}
  NO_CMAKE_FIND_ROOT_PATH)

target_link_libraries(${PROJECT_NAME}
  ${PYTORCH_LIBRARY})
  ${FBJNI_LIBRARY})
```

----------------------------------------

TITLE: Linking Prebuilt libtorch Library in Gradle
DESCRIPTION: This Gradle configuration shows how to link a prebuilt libtorch library from a PyTorch Android gradle dependency. It includes task setup for extracting AAR contents and configuring the native build.
SOURCE: https://github.com/pytorch/pytorch/blob/main/android/README.md#2025-04-22_snippet_4

LANGUAGE: Groovy
CODE:
```
android {
...
    configurations {
       extractForNativeBuild
    }
...
    compileOptions {
        externalNativeBuild {
            cmake {
                arguments "-DANDROID_STL=c++_shared"
            }
        }
    }
...
    externalNativeBuild {
        cmake {
            path "CMakeLists.txt"
        }
    }
}

dependencies {
    extractForNativeBuild('org.pytorch:pytorch_android:1.10.0')
}

task extractAARForNativeBuild {
    doLast {
        configurations.extractForNativeBuild.files.each {
            def file = it.absoluteFile
            copy {
                from zipTree(file)
                into "$buildDir/$file.name"
                include "headers/**"
                include "jni/**"
            }
        }
    }
}

tasks.whenTaskAdded { task ->
  if (task.name.contains('externalNativeBuild')) {
    task.dependsOn(extractAARForNativeBuild)
  }
}
```

----------------------------------------

TITLE: Profiling Backward Operator Invocation for aten.threshold_backward (Python)
DESCRIPTION: This snippet represents a profiling record for the "aten.threshold_backward.default" operator, showing the invocation count and the float16 input/output tensor shapes used in a backward pass for a threshold operation. The snippet depends on PyTorch's operator dispatch and a profiling mechanism. Inputs are the profiles of gradient and input tensors, shape and dtype information, outputs are used for analyzing backward pass shapes and performance. The structure is formatted for batch profiling, without execution logic but useful for kernel optimization analysis.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dpn107_training.txt#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
cnt: 5, ((T([32, 1600, 7, 7], f16), T([32, 1600, 7, 7], f16), 0), {})
```

----------------------------------------

TITLE: Suppress Non-MSVC Compiler Warnings (CMake)
DESCRIPTION: Suppresses specific compiler warnings that are not desired for the FBGEMM build on non-MSVC platforms, checking if the flags are supported.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#_snippet_12

LANGUAGE: CMake
CODE:
```
append_cxx_flag_if_supported("-Wno-missing-field-initializers"
                               CMAKE_CXX_FLAGS)
  append_cxx_flag_if_supported("-Wno-unknown-pragmas" CMAKE_CXX_FLAGS)
  append_cxx_flag_if_supported("-Wno-unused-parameter" CMAKE_CXX_FLAGS)
  append_cxx_flag_if_supported("-Wno-strict-overflow" CMAKE_CXX_FLAGS)
  append_cxx_flag_if_supported("-Wno-strict-aliasing" CMAKE_CXX_FLAGS)
  append_cxx_flag_if_supported("-Wno-stringop-overflow" CMAKE_CXX_FLAGS)
  append_cxx_flag_if_supported("-Wvla-extension" CMAKE_CXX_FLAGS)
  append_cxx_flag_if_supported("-Wsuggest-override" CMAKE_CXX_FLAGS)
  append_cxx_flag_if_supported("-Wnewline-eof" CMAKE_CXX_FLAGS)
  append_cxx_flag_if_supported("-Winconsistent-missing-override"
                               CMAKE_CXX_FLAGS)
  append_cxx_flag_if_supported("-Winconsistent-missing-destructor-override"
                               CMAKE_CXX_FLAGS)
```

----------------------------------------

TITLE: Max Pooling with Indices in PyTorch
DESCRIPTION: Applies 2D max pooling over input tensors, returning both the result and the indices of the max values.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
aten.max_pool2d_with_indices.default((T([64, 64, 224, 224], f16), [2, 2], [2, 2]), {})
```

LANGUAGE: Python
CODE:
```
aten.max_pool2d_with_indices.default((T([64, 128, 112, 112], f16), [2, 2], [2, 2]), {})
```

----------------------------------------

TITLE: Checkout PyTorch Nightly with ROCm & venv (bash)
DESCRIPTION: Uses the `./tools/nightly.py` script to checkout a new branch (`-b`) and install PyTorch nightly binaries built specifically with ROCm support (`--rocm`) into a `venv` environment. This requires a ROCm-capable system; a subsequent `source` command activates the environment (use `& .\venv\Scripts\Activate.ps1` on Windows).
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_10

LANGUAGE: bash
CODE:
```
./tools/nightly.py checkout -b my-nightly-branch --rocm
source venv/bin/activate
```

----------------------------------------

TITLE: Configuring Multiple PyTorch Operators for Benchmarking
DESCRIPTION: This snippet demonstrates how to configure multiple PyTorch operators (abs and acos) for benchmarking using op_bench.config_list and op_bench.op_list.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
import operator_benchmark as op_bench
import torch

unary_ops_configs = op_bench.config_list(
    attrs=[
        [128, 128],
        [256, 256],
        [1024, 1024],
    ],
    attr_names=["M", "N"],
    tags=["short"]
)

unary_ops_list = op_bench.op_list(
    attr_names=["op_name", "op_func"],
    attrs=[
        ["abs", torch.abs],
        ["acos", torch.acos],
    ],
)
```

----------------------------------------

TITLE: Installing Common Dependencies Bash
DESCRIPTION: Installs essential build tools like cmake and ninja using conda, and then installs Python dependencies specified in the requirements.txt file using pip. These dependencies are generally required regardless of the operating system or hardware backend.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_3

LANGUAGE: Bash
CODE:
```
conda install cmake ninja
# Run this command from the PyTorch directory after cloning the source code using the “Get the PyTorch Source“ section below
pip install -r requirements.txt
```

----------------------------------------

TITLE: Running Flake8 on Changes Bash
DESCRIPTION: Shows a bash command that uses `git diff --name-only` to find files modified in the current branch compared to the common merge base with the `main` branch, and then pipes these filenames as arguments to the `flake8` linter. This facilitates running the linter only on the code relevant to the current pull request or changes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_63

LANGUAGE: Bash
CODE:
```
flake8 $(git diff --name-only $(git merge-base --fork-point main))
```

----------------------------------------

TITLE: Importing PyTorch Distributed Elastic Multiprocessing Module
DESCRIPTION: This snippet shows how to import the PyTorch distributed elastic multiprocessing module. This module provides functionality for managing distributed processes in PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/multiprocessing.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
import torch.distributed.elastic.multiprocessing
```

----------------------------------------

TITLE: C++ Timer Creation for Eager Mode
DESCRIPTION: Example of creating a Timer instance for the eager forward mode in C++. It uses the benchmark's C++ forward statement and setup code, with language specified as 'cpp'.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/instruction_counts/README.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
Timer(
    stmt=benchmark.cpp_fwd_stmt,
    setup=benchmark.setup.cpp_setup,
    language="cpp",
)
```

----------------------------------------

TITLE: Gradient Descent Update Rules
DESCRIPTION: Mathematical formulation of gradient descent update rules for complex variables in real and complex spaces.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/autograd.rst#2025-04-22_snippet_6

LANGUAGE: math
CODE:
```
\begin{aligned}
    x_{n+1} &= x_n - (\alpha/2) * \frac{\partial L}{\partial x}  \\
    y_{n+1} &= y_n - (\alpha/2) * \frac{\partial L}{\partial y}
\end{aligned}
```

----------------------------------------

TITLE: Log Entry for Tensor Pair (Shape [128, 32, 56, 56], f16, Stride)
DESCRIPTION: Logs the occurrence (count 5) of a tensor pair. The first tensor has shape [128, 32, 56, 56], dtype f16, and specific strides (401408, 3136, 56, 1). The second tensor has the same shape and dtype but default strides. Likely generated during PyTorch execution.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2next50_training.txt#2025-04-22_snippet_10

LANGUAGE: text
CODE:
```
cnt: 5, ((T([128, 32, 56, 56], f16, stride=(401408, 3136, 56, 1)), T([128, 32, 56, 56], f16), 0), {})
```

----------------------------------------

TITLE: Summing Tensors using ATen in Python
DESCRIPTION: Uses the \"aten.sum.SymInt\" operator in ATen to sum across specified dimensions for tensors with different shapes. The operation is demonstrated with f16 tensors, showcasing summation with a keepdim parameter. Dependencies include a working PyTorch setup with accessible ATen functionality.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/visformer_small_training.txt#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([128, 1000], f16), [0], True), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([128, 768, 7, 7], f16), [0], True), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([128, 384, 14, 14], f16), [0], True), {})
```

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([128, 192, 28, 28], f16), [0], True), {})
```

----------------------------------------

TITLE: Analyzing aten.sum.SymInt Operations in PyTorch
DESCRIPTION: This code snippet shows various configurations of the aten.sum.SymInt operator in PyTorch. It includes tensor shapes, dimensions for summation, and whether to keep dimensions. The operations are primarily on f16 (float16) tensors with different shapes and summation axes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_13

LANGUAGE: Python
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([64, 1000], f16), [0], True), {})
cnt: 3, ((T([64, 1584, 7, 7], f16), [2, 3], True), {})
cnt: 1, ((T([64, 960, 7, 7], f16), [2, 3], True), {})
cnt: 3, ((T([64, 480, 14, 14], f16), [2, 3], True), {})
cnt: 4, ((T([64, 624, 14, 14], f16), [2, 3], True), {})
cnt: 1, ((T([64, 336, 14, 14], f16), [2, 3], True), {})
cnt: 3, ((T([64, 336, 28, 28], f16), [2, 3], True), {})
cnt: 1, ((T([64, 240, 28, 28], f16), [2, 3], True), {})
```

----------------------------------------

TITLE: ReLU Activation Operations
DESCRIPTION: In-place ReLU activation operations on tensors of various sizes, processing feature maps with batch size 64 and different spatial dimensions (112x112, 56x56, 28x28, 14x14).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
((T([64, 64, 112, 112], f16),), {})
((T([64, 64, 56, 56], f16),), {})
((T([64, 128, 56, 56], f16),), {})
```

----------------------------------------

TITLE: Analyzing Batch Normalization Operations in PyTorch
DESCRIPTION: This snippet shows multiple instances of batch normalization operations with different tensor shapes and channel sizes. It includes input tensors, running mean and variance, and other parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hardcorenas_a_training.txt#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([128, 672, 7, 7], f16), T([128, 672, 7, 7], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f32), T([672], f32), True, 1e-05, [True, True, True]), {})
```

----------------------------------------

TITLE: Intel ITT Integration Documentation
DESCRIPTION: RST documentation for Intel Instrumentation and Tracing Technology (ITT) related functions in PyTorch profiler.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/profiler.rst#2025-04-22_snippet_1

LANGUAGE: rst
CODE:
```
Intel Instrumentation and Tracing Technology APIs
-------------------------------------------------

.. autofunction:: torch.profiler.itt.is_available

.. autofunction:: torch.profiler.itt.mark

.. autofunction:: torch.profiler.itt.range_push

.. autofunction:: torch.profiler.itt.range_pop

.. This module needs to be documented. Adding here in the meantime
.. for tracking purposes
.. py:module:: torch.profiler.itt
.. py:module:: torch.profiler.profiler
.. py:module:: torch.profiler.python_tracer
```

----------------------------------------

TITLE: Illustrating Immutable Function in Python for FX Graph Pass
DESCRIPTION: This code snippet demonstrates a simple function that returns a clone of the input tensor. It's used to illustrate that such operations cannot be turned into no-ops in graph passes, as it would change the semantics of the compiled graph.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/_inductor/fx_passes/README.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
def f(x: Tensor):
    return x.clone()
```

----------------------------------------

TITLE: Inspecting Compiled Subgraph Functions in Dynamo
DESCRIPTION: This code inspects the source code of compiled subgraph functions generated by Dynamo. It prints the source code for the main compiled function and the resume functions that handle graph breaks, providing insight into how Dynamo transforms the original function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_overview.rst#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
print("source code of __compiled_fn_0:")
print(innermost_fn(__compiled_fn_0).__self__.code)
print("=" * 60)
print("source code of __resume_at_30_1:")
print(decompile(__resume_at_30_1))
print("=" * 60)
print("source code of __resume_at_38_2:")
print(decompile(__resume_at_38_2))
```

----------------------------------------

TITLE: Recording Flame Graph with py-spy (Bash)
DESCRIPTION: Runs the py-spy profiler on a specified Python script (`test_tensor_tensor_add.py`) to generate an SVG flame graph output. The `--native` flag enables profiling of native C++ code called from Python.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_38

LANGUAGE: bash
CODE:
```
py-spy record -o profile.svg --native -- python test_tensor_tensor_add.py
```

----------------------------------------

TITLE: Initializing PyTorch DataPipe Imports
DESCRIPTION: Basic setup for working with PyTorch DataPipes by importing required modules.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/dataframes_pipes.ipynb#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
from importlib import reload
import torch
reload(torch)
from torch.utils.data import IterDataPipe
```

----------------------------------------

TITLE: Building PyTorch Docker Image (Bash)
DESCRIPTION: Command to build the PyTorch Docker image. It uses the `make` utility with the `-f` flag to specify `docker.Makefile` as the build file, which orchestrates the Docker build process. Requires Docker and Make installed.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_21

LANGUAGE: Bash
CODE:
```
make -f docker.Makefile
```

----------------------------------------

TITLE: Registering Quantized Operator Implementation using TORCH_LIBRARY_IMPL in C++
DESCRIPTION: Shows how to register the C++ function `quantized_xand` as the implementation for the "xand" operator specifically for the `QuantizedCPU` dispatch key. This registration is done using `TORCH_LIBRARY_IMPL` and `TORCH_FN`, linking the defined schema to the concrete kernel function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/README.md#2025-04-22_snippet_2

LANGUAGE: c++
CODE:
```
TORCH_LIBRARY_IMPL(quantized, QuantizedCPU, m) {
  m.impl("xand", TORCH_FN(quantized_xand));
}
```

----------------------------------------

TITLE: Building and Serving PyTorch Documentation (Bash)
DESCRIPTION: Navigates to the documentation directory, installs Python dependencies from `requirements.txt` using pip, builds HTML documentation using `make html`, and serves it locally using `make serve`. Requires Sphinx, `pip`, and project documentation dependencies.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_22

LANGUAGE: Bash
CODE:
```
cd docs/
pip install -r requirements.txt
make html
make serve
```

----------------------------------------

TITLE: Debugging MemPool State with use_count and snapshot in Python
DESCRIPTION: This snippet illustrates how to use the `use_count()` method to track the number of references to the memory pool and the `snapshot()` method to view the allocated memory segments within the pool. It performs allocations within the pool and asserts the expected changes in use count and segment count, demonstrating how these methods can be used for debugging memory pool behavior.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_31

LANGUAGE: python
CODE:
```
pool = torch.cuda.MemPool(allocator)

# pool's use count should be 1 at this point as MemPool object
# holds a reference
assert pool.use_count() == 1

nelem_1mb = 1024 * 1024 // 4

with torch.cuda.use_mem_pool(pool):
    out_0 = torch.randn(nelem_1mb, device="cuda")

    # pool's use count should be 2 at this point as use_mem_pool
    # holds a reference
    assert pool.use_count() == 2

# pool's use count should be back to 1 at this point as use_mem_pool
# released its reference
assert pool.use_count() == 1

with torch.cuda.use_mem_pool(pool):
    # pool should have 1 segment since we made a small allocation (1 MB)
    # above and so the CUDACachingAllocator packed it into a 2 MB buffer
    assert len(pool.snapshot()) == 1

    out_1 = torch.randn(nelem_1mb, device="cuda")

    # pool should still have 1 segment since we made another small allocation
    # (1 MB) that got packed into the existing 2 MB buffer
    assert len(pool.snapshot()) == 1

    out_2 = torch.randn(nelem_1mb, device="cuda")

    # pool now should have 2 segments since the CUDACachingAllocator had
    # to make a new 2 MB buffer to accomodate out_2
    assert len(pool.snapshot()) == 2
```

----------------------------------------

TITLE: Average Pooling Operations in PyTorch
DESCRIPTION: PyTorch's average pooling operations with 2x2 kernels and stride 2. These operations reduce the spatial dimensions of the input tensors by a factor of 2, commonly used in CNN architectures for downsampling.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
Operator: aten.avg_pool2d.default
cnt: 1, ((T([64, 128, 56, 56], f16), [2, 2], [2, 2]), {})
cnt: 1, ((T([64, 256, 28, 28], f16), [2, 2], [2, 2]), {})
cnt: 1, ((T([64, 512, 14, 14], f16), [2, 2], [2, 2]), {})
```

----------------------------------------

TITLE: PyTorch Matrix Multiplication
DESCRIPTION: These snippets show matrix multiplication operations between tensors of different shapes, using float16 precision.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_14

LANGUAGE: Python
CODE:
```
((T([128, 1000], f16, stride=(0, 0)), T([1000, 3072], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([1000, 128], f16, stride=(0, 0)), T([128, 3072], f16)), {})
```

----------------------------------------

TITLE: Demonstrating DataPipe Type Attribute in Python
DESCRIPTION: Shows how to access and print the 'type' attribute of DataPipe classes and instances.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/typing.ipynb#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
def print_helper(cls, obj):
    print(f"DataPipe[{cls.type}]\nInstance type: {obj.type}")
```

LANGUAGE: python
CODE:
```
class DP(IterDataPipe[List[int]]):
    def __iter__(self) -> Iterator[List[int]]:
        pass
print_helper(DP, DP())
```

LANGUAGE: python
CODE:
```
class DP(IterDataPipe[Any]):
    def __iter__(self) -> Iterator[Any]:
        pass
print_helper(DP, DP())
```

LANGUAGE: python
CODE:
```
class DP(IterDataPipe[tuple]):
    def __iter__(self) -> Iterator[tuple]:
        pass
print_helper(DP, DP())
```

----------------------------------------

TITLE: Defining Custom Symbolic for prim::PythonOp (Part 1: Autograd Function)
DESCRIPTION: Defines a simple MyRelu operation using torch.autograd.Function. This function performs a clamp operation and saves the input for the backward pass. This class is used as an example within a prim::PythonOp.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_20

LANGUAGE: Python
CODE:
```
class MyRelu(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input.clamp(min=0)
```

----------------------------------------

TITLE: Invoking aten.silu_backward.default SiLU Backward Pass in PyTorch ATen
DESCRIPTION: Documents observed calls to the backward pass for the SiLU activation function (`aten.silu_backward.default`) in PyTorch ATen. It lists the different gradient output and original input tensor shapes (all f16) and their respective call counts.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_22

LANGUAGE: text
CODE:
```
Operator: aten.silu_backward.default
cnt: 1, ((T([128, 2304, 7, 7], f16), T([128, 2304, 7, 7], f16)), {})
cnt: 8, ((T([128, 384, 7, 7], f16), T([128, 384, 7, 7], f16)), {})
cnt: 2, ((T([128, 1536, 7, 7], f16), T([128, 1536, 7, 7], f16)), {})
cnt: 18, ((T([128, 384, 14, 14], f16), T([128, 384, 14, 14], f16)), {})
cnt: 6, ((T([128, 1536, 14, 14], f16), T([128, 1536, 14, 14], f16)), {})
cnt: 1, ((T([128, 384, 28, 28], f16), T([128, 384, 28, 28], f16)), {})
cnt: 2, ((T([128, 512, 28, 28], f16), T([128, 512, 28, 28], f16)), {})
cnt: 5, ((T([128, 128, 28, 28], f16), T([128, 128, 28, 28], f16)), {})
cnt: 2, ((T([128, 128, 56, 56], f16), T([128, 128, 56, 56], f16)), {})
cnt: 1, ((T([128, 256, 56, 56], f16), T([128, 256, 56, 56], f16)), {})
cnt: 3, ((T([128, 64, 56, 56], f16), T([128, 64, 56, 56], f16)), {})
cnt: 1, ((T([128, 64, 112, 112], f16), T([128, 64, 112, 112], f16)), {})
cnt: 1, ((T([128, 32, 112, 112], f16), T([128, 32, 112, 112], f16)), {})
cnt: 1, ((T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16)), {})
```

----------------------------------------

TITLE: Benchmark Output - System Information and Performance Metrics
DESCRIPTION: Sample benchmark output showing PyTorch version, CUDA configuration, GPU topology matrix, and training performance metrics across 8 trainer nodes. Includes percentile-based timing measurements for each trainer and aggregate statistics.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/distributed/benchmarks/README.md#2025-04-22_snippet_0

LANGUAGE: text
CODE:
```
---------- Info ---------

* PyTorch version: 1.7.0
* CUDA version: 9.2.0

---------- nvidia-smi topo -m ---------

    GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU     Affinity
    GPU0     X      NV2     NV1     NV2     NV1     NODE    NODE    NODE    0-19,40-59
    GPU1    NV2      X      NV2     NV1     NODE    NV1     NODE    NODE    0-19,40-59
    GPU2    NV1     NV2      X      NV1     NODE    NODE    NV2     NODE    0-19,40-59
    GPU3    NV2     NV1     NV1      X      NODE    NODE    NODE    NV2     0-19,40-59
    GPU4    NV1     NODE    NODE    NODE     X      NV2     NV1     NV2     0-19,40-59
    GPU5    NODE    NV1     NODE    NODE    NV2      X      NV2     NV1     0-19,40-59
    GPU6    NODE    NODE    NV2     NODE    NV1     NV2      X      NV1     0-19,40-59
    GPU7    NODE    NODE    NODE    NV2     NV2     NV1     NV1      X      0-19,40-59

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing a single PCIe switch
  NV#  = Connection traversing a bonded set of # NVLinks

------------------  PyTorch Distributed Benchmark (DDP and RPC) ---------------------

                    sec/epoch  epoch/sec    sec/epoch  epoch/sec    sec/epoch  epoch/sec    sec/epoch  epoch/sec
    Trainer0:  p50:  0.376s     185/s  p75:  0.384s     182/s  p90:  0.390s     179/s  p95:  0.396s     176/s
    Trainer1:  p50:  0.377s     204/s  p75:  0.384s     200/s  p90:  0.389s     197/s  p95:  0.393s     195/s
    Trainer2:  p50:  0.377s     175/s  p75:  0.384s     172/s  p90:  0.390s     169/s  p95:  0.395s     166/s
    Trainer3:  p50:  0.377s     161/s  p75:  0.384s     158/s  p90:  0.390s     156/s  p95:  0.393s     155/s
    Trainer4:  p50:  0.377s     172/s  p75:  0.383s     169/s  p90:  0.389s     166/s  p95:  0.395s     164/s
    Trainer5:  p50:  0.377s     180/s  p75:  0.383s     177/s  p90:  0.389s     174/s  p95:  0.395s     172/s
    Trainer6:  p50:  0.377s     204/s  p75:  0.384s     200/s  p90:  0.390s     197/s  p95:  0.394s     195/s
    Trainer7:  p50:  0.377s     185/s  p75:  0.384s     182/s  p90:  0.389s     179/s  p95:  0.394s     177/s
         All:  p50:  0.377s    1470/s  p75:  0.384s    1443/s  p90:  0.390s    1421/s  p95:  0.396s    1398/s
```

----------------------------------------

TITLE: Analyzing Backward Operations in PyTorch
DESCRIPTION: This snippet shows backward operations for hardsigmoid and hardswish activations, which are used in the backpropagation process during training.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
Operator: aten.hardsigmoid_backward.default
cnt: 1, ((T([128, 1104, 1, 1], f16), T([128, 1104, 1, 1], f16)), {})
cnt: 5, ((T([128, 736, 1, 1], f16), T([128, 736, 1, 1], f16)), {})

Operator: aten.hardswish_backward.default
cnt: 1, ((T([128, 1984, 1, 1], f16), T([128, 1984, 1, 1], f16)), {})
cnt: 1, ((T([128, 1344, 7, 7], f16), T([128, 1344, 7, 7], f16)), {})
```

----------------------------------------

TITLE: Summing Tensor Elements in PyTorch
DESCRIPTION: This snippet shows the operation for summing elements along a specific dimension of a tensor, commonly used in loss calculation or feature aggregation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/adv_inception_v3_training.txt#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([128, 1000], f16), [0], True), {})
```

----------------------------------------

TITLE: Log Entry for Tensor Pair (Shape [128, 64, 28, 28], f16, Stride)
DESCRIPTION: Logs the occurrence (count 6) of a tensor pair. The first tensor has shape [128, 64, 28, 28], dtype f16, and specific strides (200704, 784, 28, 1). The second tensor has the same shape and dtype but default strides. Likely generated during PyTorch execution.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2next50_training.txt#2025-04-22_snippet_6

LANGUAGE: text
CODE:
```
cnt: 6, ((T([128, 64, 28, 28], f16, stride=(200704, 784, 28, 1)), T([128, 64, 28, 28], f16), 0), {})
```

----------------------------------------

TITLE: Tensor Cloning in PyTorch
DESCRIPTION: This snippet shows a tensor clone operation used to create a copy of the input tensor. This is typically done to ensure the tensor is contiguous in memory or to prevent unexpected in-place modifications.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
Operator: aten.clone.default
cnt: 1, ((T([96, 9, 128, 128], f16),), {})
```

----------------------------------------

TITLE: C++ Timer Creation for TorchScript Mode
DESCRIPTION: Example of creating a Timer instance for TorchScript mode in C++. It includes statement code to prepare inputs as IValues and call the model's forward method, with setup code that loads the model.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/instruction_counts/README.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
Timer(
    stmt="""
        std::vector<torch::jit::IValue> ivalue_inputs(
            torch::jit::IValue({x}),
            torch::jit::IValue({w})
        );
        auto y = jit_model.forward(ivalue_inputs);
    """,
    setup="""
        # benchmark.setup.cpp_setup
        # jit_model = torch::jit::load(...)
        # Warm up jit_model
    """,
)
```

----------------------------------------

TITLE: Calling aten.add.Tensor (Python)
DESCRIPTION: Performs element-wise addition between two tensors. Examples show addition of tensors with various shapes and data types (int64 and float16). Includes broadcasting cases.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_6

LANGUAGE: Python
CODE:
```
((T([8, 128], i64, stride=(0, 1)), 2), {})
```

LANGUAGE: Python
CODE:
```
((T([8, 128, 1024], f16), T([8, 128, 1024], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([128], i64), 1), {})
```

LANGUAGE: Python
CODE:
```
((T([8, 16, 128, 128], f16), T([8, 1, 128, 128], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([8, 128, 50265], f16), T([1, 50265], f16)), {})
```

LANGUAGE: Python
CODE:
```
((T([50265, 1024], f16), T([50265, 1024], f16)), {})
```

----------------------------------------

TITLE: PyTorch Batch Normalization
DESCRIPTION: These snippets show batch normalization operations applied to tensors of various shapes, using float16 precision.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_16

LANGUAGE: Python
CODE:
```
((T([1, 16, 27], f16), T([16], f16), None, None, None, True, 0.0, 1e-05), {})
```

LANGUAGE: Python
CODE:
```
((T([1, 32, 144], f16), T([32], f16), None, None, None, True, 0.0, 1e-05), {})
```

LANGUAGE: Python
CODE:
```
((T([1, 64, 288], f16), T([64], f16), None, None, None, True, 0.0, 1e-05), {})
```

LANGUAGE: Python
CODE:
```
((T([1, 128, 576], f16), T([128], f16), None, None, None, True, 0.0, 1e-05), {})
```

LANGUAGE: Python
CODE:
```
((T([1, 256, 128], f16), T([256], f16), None, None, None, True, 0.0, 1e-05), {})
```

----------------------------------------

TITLE: In-Place Addition of Tensors in PyTorch - Python
DESCRIPTION: Illustrates the use of aten.add_.Tensor for in-place addition of tensors. Modifies the original tensor by adding another tensor. Useful for operations where memory optimization is crucial. Limitations include maintaining compatible tensor shapes for in-place operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v3_large_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
Operator: aten.add_.Tensor
cnt: 1, ((T([32, 16, 112, 112], f16), T([32, 16, 112, 112], f16)), {})
cnt: 1, ((T([32, 24, 56, 56], f16), T([32, 24, 56, 56], f16)), {})
cnt: 2, ((T([32, 40, 28, 28], f16), T([32, 40, 28, 28], f16)), {})
cnt: 3, ((T([32, 80, 14, 14], f16), T([32, 80, 14, 14], f16)), {})
cnt: 1, ((T([32, 112, 14, 14], f16), T([32, 112, 14, 14], f16)), {})
cnt: 2, ((T([32, 160, 7, 7], f16), T([32, 160, 7, 7], f16)), {})

```

----------------------------------------

TITLE: PyTorch Operator Usage Statistics
DESCRIPTION: This snippet provides a comprehensive list of PyTorch operators used in a neural network model, along with their call counts and tensor shapes. It includes operations like convolutions, activations, and tensor manipulations, which are typical for deep learning models.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ese_vovnet19b_dw_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([128, 1000], f16), 1, False), {})
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})
Operator: aten.add.Tensor
cnt: 23, ((T([], i64), 1), {})
cnt: 1, ((T([128, 1024, 7, 7], f16), T([128, 1024, 7, 7], f16)), {})
# ... (truncated for brevity)
Operator: aten.hardsigmoid_backward.default
cnt: 1, ((T([128, 1024, 1, 1], f16), T([128, 1024, 1, 1], f16)), {})
cnt: 1, ((T([128, 768, 1, 1], f16), T([128, 768, 1, 1], f16)), {})
```

----------------------------------------

TITLE: Analyzing PyTorch Operator Usage for MobileNetV2
DESCRIPTION: This code snippet shows the usage statistics of various PyTorch operators in a MobileNetV2 implementation. It includes tensor shapes, data types, and occurrence counts for operations like hardtanh, batch normalization, and matrix multiplication.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v2_training.txt#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
Operator: aten.hardtanh.default
cnt: 5, ((T([96, 192, 28, 28], f16), 0.0, 6.0), {})
cnt: 1, ((T([96, 192, 14, 14], f16), 0.0, 6.0), {})
cnt: 8, ((T([96, 384, 14, 14], f16), 0.0, 6.0), {})
cnt: 5, ((T([96, 576, 14, 14], f16), 0.0, 6.0), {})
cnt: 1, ((T([96, 576, 7, 7], f16), 0.0, 6.0), {})
cnt: 6, ((T([96, 960, 7, 7], f16), 0.0, 6.0), {})
cnt: 1, ((T([96, 1280, 7, 7], f16), 0.0, 6.0), {})

Operator: aten.hardtanh_backward.default
cnt: 1, ((T([96, 1280, 7, 7], f16), T([96, 1280, 7, 7], f16), 0.0, 6.0), {})
cnt: 6, ((T([96, 960, 7, 7], f16), T([96, 960, 7, 7], f16), 0.0, 6.0), {})
cnt: 1, ((T([96, 576, 7, 7], f16), T([96, 576, 7, 7], f16), 0.0, 6.0), {})
cnt: 5, ((T([96, 576, 14, 14], f16), T([96, 576, 14, 14], f16), 0.0, 6.0), {})
cnt: 8, ((T([96, 384, 14, 14], f16), T([96, 384, 14, 14], f16), 0.0, 6.0), {})
cnt: 1, ((T([96, 192, 14, 14], f16), T([96, 192, 14, 14], f16), 0.0, 6.0), {})
cnt: 5, ((T([96, 192, 28, 28], f16), T([96, 192, 28, 28], f16), 0.0, 6.0), {})
cnt: 1, ((T([96, 144, 28, 28], f16), T([96, 144, 28, 28], f16), 0.0, 6.0), {})
cnt: 3, ((T([96, 144, 56, 56], f16), T([96, 144, 56, 56], f16), 0.0, 6.0), {})
cnt: 1, ((T([96, 96, 56, 56], f16), T([96, 96, 56, 56], f16), 0.0, 6.0), {})
cnt: 1, ((T([96, 96, 112, 112], f16), T([96, 96, 112, 112], f16), 0.0, 6.0), {})
cnt: 2, ((T([96, 32, 112, 112], f16), T([96, 32, 112, 112], f16), 0.0, 6.0), {})

Operator: aten.mean.dim
cnt: 1, ((T([96, 1280, 7, 7], f16), [-1, -2], True), {})

Operator: aten.mm.default
cnt: 1, ((T([96, 1000], f16, stride=(0, 0)), T([1000, 1280], f16)), {})
cnt: 1, ((T([1000, 96], f16, stride=(0, 0)), T([96, 1280], f16)), {})

Operator: aten.native_batch_norm.default
cnt: 2, ((T([96, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([96, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([96, 96, 112, 112], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([96, 96, 56, 56], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), False, 0.1, 1e-05), {})
cnt: 2, ((T([96, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), False, 0.1, 1e-05), {})
cnt: 3, ((T([96, 144, 56, 56], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([96, 144, 28, 28], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f16), False, 0.1, 1e-05), {})
cnt: 3, ((T([96, 32, 28, 28], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), False, 0.1, 1e-05), {})
cnt: 5, ((T([96, 192, 28, 28], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([96, 192, 14, 14], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), False, 0.1, 1e-05), {})
cnt: 4, ((T([96, 64, 14, 14], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})
cnt: 8, ((T([96, 384, 14, 14], f16), T([384], f16), T([384], f16), T([384], f16), T([384], f16), False, 0.1, 1e-05), {})
cnt: 3, ((T([96, 96, 14, 14], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), False, 0.1, 1e-05), {})
cnt: 5, ((T([96, 576, 14, 14], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([96, 576, 7, 7], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f16), False, 0.1, 1e-05), {})
cnt: 3, ((T([96, 160, 7, 7], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f16), False, 0.1, 1e-05), {})
cnt: 6, ((T([96, 960, 7, 7], f16), T([960], f16), T([960], f16), T([960], f16), T([960], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([96, 320, 7, 7], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([96, 1280, 7, 7], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f16), False, 0.1, 1e-05), {})

Operator: aten.native_batch_norm_backward.default
cnt: 1, ((T([96, 1280, 7, 7], f16), T([96, 1280, 7, 7], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f32), T([1280], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([96, 320, 7, 7], f16), T([96, 320, 7, 7], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f32), T([320], f32), False, 1e-05, [True, True, True]), {})
cnt: 6, ((T([96, 960, 7, 7], f16), T([96, 960, 7, 7], f16), T([960], f16), T([960], f16), T([960], f16), T([960], f32), T([960], f32), False, 1e-05, [True, True, True]), {})
cnt: 3, ((T([96, 160, 7, 7], f16), T([96, 160, 7, 7], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f32), T([160], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([96, 576, 7, 7], f16), T([96, 576, 7, 7], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f32), T([576], f32), False, 1e-05, [True, True, True]), {})
cnt: 5, ((T([96, 576, 14, 14], f16), T([96, 576, 14, 14], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f32), T([576], f32), False, 1e-05, [True, True, True]), {})
cnt: 3, ((T([96, 96, 14, 14], f16), T([96, 96, 14, 14], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), False, 1e-05, [True, True, True]), {})
cnt: 8, ((T([96, 384, 14, 14], f16), T([96, 384, 14, 14], f16), T([384], f16), T([384], f16), T([384], f16), T([384], f32), T([384], f32), False, 1e-05, [True, True, True]), {})
cnt: 4, ((T([96, 64, 14, 14], f16), T([96, 64, 14, 14], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([96, 192, 14, 14], f16), T([96, 192, 14, 14], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), False, 1e-05, [True, True, True]), {})
cnt: 5, ((T([96, 192, 28, 28], f16), T([96, 192, 28, 28], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), False, 1e-05, [True, True, True]), {})
cnt: 3, ((T([96, 32, 28, 28], f16), T([96, 32, 28, 28], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([96, 144, 28, 28], f16), T([96, 144, 28, 28], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f32), T([144], f32), False, 1e-05, [True, True, True]), {})
cnt: 3, ((T([96, 144, 56, 56], f16), T([96, 144, 56, 56], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f32), T([144], f32), False, 1e-05, [True, True, True]), {})
cnt: 2, ((T([96, 24, 56, 56], f16), T([96, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f32), T([24], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([96, 96, 56, 56], f16), T([96, 96, 56, 56], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([96, 96, 112, 112], f16), T([96, 96, 112, 112], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([96, 16, 112, 112], f16), T([96, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f32), T([16], f32), False, 1e-05, [True, True, True]), {})
cnt: 2, ((T([96, 32, 112, 112], f16), T([96, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), False, 1e-05, [True, True, True]), {})

Operator: aten.sum.SymInt
cnt: 1, ((T([96, 1000], f16, stride=(0, 0)), [0], True), {})

Operator: aten.sum.default
cnt: 1, ((T([96, 1000], f16),), {})
```

----------------------------------------

TITLE: Cloning Tensors with aten.clone
DESCRIPTION: The clone operation duplicates tensors, ensuring they are distinct from the original. Dependent on PyTorch, it manages tensor duplication processes, producing a new tensor with the same shape, type, and data as the original input.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForConditionalGeneration_training.txt#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
Operator: aten.clone.default
cnt: 3, ((T([8, 128], i64),), {})
cnt: 1, ((T([8, 127], i64, stride=(128, 1)),), {})
```

----------------------------------------

TITLE: PyTorch HardSwish Activation Operations
DESCRIPTION: HardSwish activation function operations applied to tensors of different shapes throughout the network layers.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hardcorenas_a_training.txt#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
((T([128, 32, 112, 112], f16),), {})
```

----------------------------------------

TITLE: Configuring ATen Activation and Backward Operator Scenarios - Python
DESCRIPTION: This set configures testing of activation functions (Hardsigmoid, Hardswish) and their backward counterparts via tensor shape presets. The tuples specify varying tensor sizes designed for neural net feature maps, covering typical batched activation inputs and targets for both in-place and out-of-place operations. Prerequisites include correct tensor shape and data type support in PyTorch, with test coverage extending to both forward and gradient-backward passes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
Operator: aten.hardsigmoid.default
cnt: 1, ((T([128, 72, 1, 1], f16),), {})
```

LANGUAGE: python
CODE:
```
cnt: 2, ((T([128, 120, 1, 1], f16),), {})
```

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 480, 1, 1], f16),), {})
```

LANGUAGE: python
CODE:
```
cnt: 2, ((T([128, 672, 1, 1], f16),), {})
```

LANGUAGE: python
CODE:
```
cnt: 2, ((T([128, 960, 1, 1], f16),), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.hardsigmoid_backward.default
cnt: 2, ((T([128, 960, 1, 1], f16), T([128, 960, 1, 1], f16)), {})
```

----------------------------------------

TITLE: PyTorch Max Pooling Operations
DESCRIPTION: Max pooling operations with different kernel sizes (5x5, 9x9, 13x13) and corresponding padding and stride configurations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
((T([8, 512, 12, 16], f16), [5, 5], [1, 1], [2, 2]), {})
```

----------------------------------------

TITLE: Analyzing Convolution Operations in PyTorch
DESCRIPTION: This snippet shows various convolution operations with different tensor shapes, strides, and padding. The operations are represented as tuples containing tensor shapes and parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
cnt: 5, ((T([128, 736, 7, 7], f16), T([128, 184, 7, 7], f16), T([736, 184, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 184, 7, 7], f16), T([128, 720, 7, 7], f16), T([184, 720, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 720, 1, 1], f16), T([128, 32, 1, 1], f16), T([720, 32, 1, 1], f16), [720], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
```

----------------------------------------

TITLE: Ensuring Consistent Randomness Across Torch and Triton
DESCRIPTION: Enables a configuration flag in torch._inductor to ensure that random number generation is consistent between PyTorch's default behavior and the code generated for Triton kernels. This helps in debugging accuracy issues related to randomness.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting_old.rst#_snippet_9

LANGUAGE: python
CODE:
```
torch._inductor.config.fallback_random = True
```

----------------------------------------

TITLE: Tensor Copy Operations
DESCRIPTION: Copy operations between tensors with different shapes and strides, primarily working with half-precision (f16) data type.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
((T([3, 799, 1199], f16, stride=(1439744, 1216, 1)), T([3, 799, 1199], f16)), {})
```

----------------------------------------

TITLE: Batch Normalization Operation Statistics
DESCRIPTION: Logs of batch normalization operations (aten.native_batch_norm.default) showing input tensor shapes, normalization parameters and running statistics
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
cnt: 2, ((T([128, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), True, 0.1, 1e-05), {})
```

----------------------------------------

TITLE: Concatenating Tensors along a Specified Dimension in Python
DESCRIPTION: The function concatenates a list of tensors along the specified dimension. Dependencies include input tensors of shapes [16, 3, 128, 128] and [16, 5, 128, 128] with data type f16. This is useful for combining feature maps in neural networks. The operation outputs a tensor with increased size along the concatenation dimension.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
Operator: aten.cat.default
cnt: 1, (([T([16, 3, 128, 128], f16), T([16, 5, 128, 128], f16)], 1), {})
```

----------------------------------------

TITLE: Analyzing PyTorch SiLU Activation Operations
DESCRIPTION: This snippet shows the usage of the SiLU (Swish) activation function in PyTorch. It includes both the forward (silu_) and backward (silu_backward) operations, detailing tensor shapes and data types across various layers.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_15

LANGUAGE: Python
CODE:
```
Operator: aten.silu_.default
cnt: 2, ((T([32, 32, 112, 112], f16),), {})
cnt: 1, ((T([32, 8, 1, 1], f16),), {})
cnt: 1, ((T([32, 96, 112, 112], f16),), {})
# ... (truncated for brevity)

Operator: aten.silu_backward.default
cnt: 1, ((T([32, 1280, 7, 7], f16), T([32, 1280, 7, 7], f16)), {})
cnt: 4, ((T([32, 48, 1, 1], f16), T([32, 48, 1, 1], f16)), {})
# ... (truncated for brevity)
```

----------------------------------------

TITLE: PyTorch GELU Activation Function
DESCRIPTION: These snippets show the application of the GELU (Gaussian Error Linear Unit) activation function to tensors of various shapes, using float16 precision.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_11

LANGUAGE: Python
CODE:
```
((T([128, 16, 96, 96], f16),), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 32, 96, 96], f16),), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 64, 96, 96], f16),), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 128, 48, 48], f16),), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 256, 48, 48], f16),), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 256, 24, 24], f16),), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 512, 24, 24], f16),), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 768, 24, 24], f16),), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 768, 12, 12], f16),), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 1536, 12, 12], f16),), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 768, 6, 6], f16),), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 1536, 6, 6], f16),), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 3072, 6, 6], f16),), {})
```

----------------------------------------

TITLE: Running Store and Process Group Wrapper Tests in Python
DESCRIPTION: Commands to run unit tests for the Store component and Process Group Wrapper in PyTorch Distributed using Python.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/distributed/CONTRIBUTING.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
python test/distributed/test_store.py
python test/distributed/test_pg_wrapper.py
```

----------------------------------------

TITLE: Analyzing PyTorch Operator Usage
DESCRIPTION: This code snippet shows the usage patterns of various PyTorch operators, including their input tensor shapes, data types, and occurrence counts. It covers operators like threshold_backward, topk, unbind, upsample_nearest2d, and where.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_11

LANGUAGE: Python
CODE:
```
cnt: 4, ((T([1, 88, 10, 10, 3], f16), [-1]), {})
cnt: 4, ((T([1, 88, 5, 5, 2], f16), [-1]), {})
Operator: aten.threshold_backward.default
cnt: 20, ((T([2], f16), T([2], f16), 0), {})
cnt: 12, ((T([3], f16), T([3], f16), 0), {})
Operator: aten.topk.default
cnt: 1, ((T([1, 6905250], f16), 5000, 1), {})
Operator: aten.unbind.int
cnt: 2, ((T([5000, 4], f32), 1), {})
cnt: 1, ((T([1, 100, 6], f32, stride=(0, 0, 0)),), {})
cnt: 4, ((T([1, 88, 5, 5, 2], f16, stride=(2200, 25, 5, 1, 0)), -1), {})
cnt: 4, ((T([1, 88, 10, 10, 3], f16, stride=(8800, 100, 10, 1, 0)), -1), {})
cnt: 4, ((T([1, 88, 20, 20, 3], f16, stride=(35200, 400, 20, 1, 0)), -1), {})
cnt: 4, ((T([1, 88, 40, 40, 3], f16, stride=(140800, 1600, 40, 1, 0)), -1), {})
cnt: 4, ((T([1, 88, 80, 80, 2], f16, stride=(563200, 6400, 80, 1, 0)), -1), {})
cnt: 4, ((T([1, 88, 40, 40, 2], f16, stride=(140800, 1600, 40, 1, 0)), -1), {})
cnt: 4, ((T([1, 88, 20, 20, 2], f16, stride=(35200, 400, 20, 1, 0)), -1), {})
cnt: 4, ((T([1, 88, 10, 10, 2], f16, stride=(8800, 100, 10, 1, 0)), -1), {})
Operator: aten.upsample_nearest2d.vec
cnt: 4, ((T([1, 88, 5, 5], f16), [10, 10], None), {})
cnt: 4, ((T([1, 88, 10, 10], f16), [20, 20], None), {})
cnt: 4, ((T([1, 88, 20, 20], f16), [40, 40], None), {})
cnt: 4, ((T([1, 88, 40, 40], f16), [80, 80], None), {})
Operator: aten.upsample_nearest2d_backward.vec
cnt: 4, ((T([1, 88, 80, 80], f16), [80, 80], [1, 88, 40, 40], None), {})
cnt: 4, ((T([1, 88, 40, 40], f16), [40, 40], [1, 88, 20, 20], None), {})
cnt: 4, ((T([1, 88, 20, 20], f16), [20, 20], [1, 88, 10, 10], None), {})
cnt: 4, ((T([1, 88, 10, 10], f16), [10, 10], [1, 88, 5, 5], None), {})
Operator: aten.where.self
cnt: 1, ((T([5000, 4], b8), T([5000, 4], f32), T([5000, 4], f32)), {})
cnt: 1, ((T([5000, 4], b8), T([5000, 4], f32), T([], f32)), {})
```

----------------------------------------

TITLE: Basic FakeTensorMode Usage in PyTorch
DESCRIPTION: Demonstrates how to create and use fake tensors outside of PT2 context. Shows creation of FakeTensorMode, converting real tensors to fake tensors, and performing operations within the fake mode context.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_fake_tensor.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
# Create a fake mode
from torch._subclasses.fake_tensor import FakeTensorMode
fake_mode = FakeTensorMode()
converter = fake_mode.fake_tensor_converter
# Fakeify some real tensors
fake_x = converter.from_real_tensor(fake_mode, x)
with fake_mode:
    # Do some operations on the fake tensors
    fake_y = fake_x * 2
    # Factory operations automatically get fakeified in the context manager
    fake_z = torch.empty(20)
```

----------------------------------------

TITLE: Tensor Addition Operations with Strides in PyTorch
DESCRIPTION: Collection of tensor addition operations with various strides. These operations show different tensor shapes and stride patterns for 16-bit floating point tensors, likely from a neural network computation graph.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
cnt: 1, ((T([64, 64, 56, 56], f16, stride=(802816, 3136, 56, 1)), T([64, 64, 56, 56], f16, stride=(702464, 3136, 56, 1))), {})
cnt: 5, ((T([64, 32, 56, 56], f16, stride=(802816, 3136, 56, 1)), T([64, 32, 56, 56], f16, stride=(702464, 3136, 56, 1))), {})
cnt: 1, ((T([64, 64, 56, 56], f16), T([64, 64, 56, 56], f16, stride=(602112, 3136, 56, 1))), {})
cnt: 4, ((T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16, stride=(602112, 3136, 56, 1))), {})
cnt: 1, ((T([64, 64, 56, 56], f16), T([64, 64, 56, 56], f16, stride=(501760, 3136, 56, 1))), {})
cnt: 3, ((T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16, stride=(501760, 3136, 56, 1))), {})
cnt: 1, ((T([64, 64, 56, 56], f16), T([64, 64, 56, 56], f16, stride=(401408, 3136, 56, 1))), {})
cnt: 2, ((T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16, stride=(401408, 3136, 56, 1))), {})
cnt: 1, ((T([64, 64, 56, 56], f16), T([64, 64, 56, 56], f16, stride=(301056, 3136, 56, 1))), {})
cnt: 1, ((T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16, stride=(301056, 3136, 56, 1))), {})
cnt: 1, ((T([64, 64, 56, 56], f16), T([64, 64, 56, 56], f16)), {})
```

----------------------------------------

TITLE: Type Conversion and Copy in PyTorch Python
DESCRIPTION: These snippets signify the 'aten._to_copy' operator usage, indicating a tensor's type conversion with possibly specified properties like dtype, layout, device, and memory pinning. The operation converts input tensor types, outputs a copied tensor in the new type, and depends on PyTorch. Precise attributes guide the conversion process, crucial for computational efficiency in varied hardware environments.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
Operator: aten._to_copy.default
cnt: 1, ((T([5000, 4], f16),), {'dtype': f32})
cnt: 1, ((T([5000], f16),), {'dtype': f32})
cnt: 1, ((T([5000], i64),), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda', 'pin_memory': False})
```

----------------------------------------

TITLE: Configuring Installation Directories for libshm in CMake
DESCRIPTION: Sets up the installation directories for libshm binaries and libraries if not already defined. This ensures proper placement of compiled artifacts.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/lib/libshm_windows/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
if(NOT LIBSHM_INSTALL_LIB_SUBDIR)
  set(LIBSHM_INSTALL_BIN_SUBDIR "bin" CACHE PATH "libshm install binary directory")
  set(LIBSHM_INSTALL_LIB_SUBDIR "lib" CACHE PATH "libshm install library directory")
endif()
```

----------------------------------------

TITLE: Calling aten.sum.SymInt (Python)
DESCRIPTION: Computes the sum of tensor elements over specified dimensions. Used for reducing tensor size or aggregation. Examples show summing float16 tensors along dimension 0.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_30

LANGUAGE: Python
CODE:
```
((T([1024, 1024], f16), [0], True), {})
```

LANGUAGE: Python
CODE:
```
((T([1024, 4096], f16), [0], True), {})
```

----------------------------------------

TITLE: Tracking Type Conversion Operations with aten._to_copy.default
DESCRIPTION: Shows usages of the _to_copy.default operator which handles tensor type conversion. Statistics include tensor shape conversion patterns, primarily between float16 and float32 data types, with many operations involving CUDA device transfers.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
Operator: aten._to_copy.default
cnt: 1, ((T([1, 1, 12, 16, 2], i64),), {'dtype': f32})
cnt: 3, ((T([3, 2], f32),), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda'})
cnt: 3, ((T([1, 3, 1, 1, 2], f32),), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda'})
cnt: 1, ((T([1, 1, 24, 32, 2], i64),), {'dtype': f32})
cnt: 1, ((T([1, 1, 48, 64, 2], i64),), {'dtype': f32})
cnt: 2, ((T([8, 3, 48, 64, 2], f16),), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda'})
cnt: 2, ((T([8, 3, 48, 64, 2], f32),), {'dtype': f16})
cnt: 2, ((T([8, 3, 24, 32, 2], f16),), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda'})
cnt: 2, ((T([8, 3, 24, 32, 2], f32),), {'dtype': f16})
cnt: 2, ((T([8, 3, 12, 16, 2], f16),), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda'})
cnt: 2, ((T([8, 3, 12, 16, 2], f32),), {'dtype': f16})
```

----------------------------------------

TITLE: Registering PyTorch Kernel with Multiple Outputs (C++)
DESCRIPTION: Illustrates how a C++ kernel function can return multiple values by returning a `std::tuple`. The types within the tuple correspond to the multiple outputs of the operator. The operator schema will be inferred to reflect these multiple return values unless explicitly specified.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_9

LANGUAGE: cpp
CODE:
```
namespace {
  std::tuple<Tensor, int64_t, Tensor>
     my_kernel_cpu(const Tensor& a, const Tensor& b, int64_t c) {...}
}

static auto registry = torch::RegisterOperators()
   .op("my_namespace::my_op", torch::RegisterOperators::options()
       .kernel<decltype(my_kernel_cpu), &my_kernel_cpu>(CPU()));
```

----------------------------------------

TITLE: Input Tensor Cloning in PyTorch MobileNetV3
DESCRIPTION: Operation to clone the input tensor. This is typically done to ensure the input isn't modified during processing or to create a separate copy for operations that might modify the tensor in-place.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
Operator: aten.clone.default
cnt: 1, ((T([128, 3, 224, 224], f16),), {})
```

----------------------------------------

TITLE: Describing PyTorch Operator Arguments and Tensor Shapes - Python
DESCRIPTION: This snippet lists structured tuples detailing tensor shapes and primitives required for various PyTorch operator invocations, such as convolutions, pooling, normalization, and arithmetic. Inputs are PyTorch tensor annotations (with dimensions, type, strides) plus operator-specific parameter lists. The outputs are expected to match operand signatures for PyTorch operators, supporting benchmarking, profiling, or shape-based validation utilities. Dependencies: PyTorch (aten namespace), understanding of tensor layout/stride options and operator arity. Limitations: Not standalone executable; requires higher-level orchestration or test harness for functional execution.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_botnext26ts_256_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 512, 8, 8], f16), T([128, 2048, 8, 8], f16), T([512, 2048, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 2048, 8, 8], f16), T([128, 1024, 16, 16], f16), T([2048, 1024, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 640, 16, 16], f16), T([128, 512, 16, 16], f16), T([640, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 512, 16, 16], f16), T([128, 1024, 16, 16], f16), T([512, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 1024, 16, 16], f16), T([128, 256, 16, 16], f16), T([1024, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 384, 16, 16], f16), T([128, 256, 16, 16], f16), T([384, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 256, 16, 16], f16), T([128, 1024, 16, 16], f16), T([256, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 1024, 16, 16], f16), T([128, 512, 32, 32], f16), T([1024, 512, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 1, 256], f16), T([128, 1, 256], f16), T([1, 1, 5], f16), [0], [1], [2], [1], False, [0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 256, 16, 16], f16), T([128, 256, 32, 32], f16), T([256, 16, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 16, [True, True, False]), {})
cnt: 1, ((T([128, 256, 32, 32], f16), T([128, 512, 32, 32], f16), T([256, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 512, 32, 32], f16), T([128, 128, 32, 32], f16), T([512, 128, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 1, 128], f16), T([128, 1, 128], f16), T([1, 1, 5], f16), [0], [1], [2], [1], False, [0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 128, 32, 32], f16), T([128, 128, 32, 32], f16), T([128, 16, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 8, [True, True, False]), {})
cnt: 1, ((T([128, 128, 32, 32], f16), T([128, 512, 32, 32], f16), T([128, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 512, 32, 32], f16), T([128, 256, 64, 64], f16), T([512, 256, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 128, 32, 32], f16), T([128, 128, 64, 64], f16), T([128, 16, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 8, [True, True, False]), {})
cnt: 1, ((T([128, 128, 64, 64], f16), T([128, 256, 64, 64], f16), T([128, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 3, ((T([128, 256, 64, 64], f16), T([128, 64, 64, 64], f16), T([256, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 1, 64], f16), T([128, 1, 64], f16), T([1, 1, 3], f16), [0], [1], [1], [1], False, [0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 64, 64, 64], f16), T([128, 64, 64, 64], f16), T([64, 16, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 4, [True, True, False]), {})
cnt: 1, ((T([128, 64, 64, 64], f16), T([128, 256, 64, 64], f16), T([64, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 64, 64, 64], f16), T([128, 64, 64, 64], f16), T([64, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 64, 128, 128], f16), T([128, 32, 128, 128], f16), T([64, 32, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 32, 128, 128], f16), T([128, 24, 128, 128], f16), T([32, 24, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 24, 128, 128], f16), T([128, 3, 256, 256], f16), T([24, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})
```

----------------------------------------

TITLE: Loop-Level Analogy Example in PyTorch
DESCRIPTION: Simple loop example showing basic iteration over batch channels with corresponding values.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
for channel in range(batch.size):
    result[channel] = channel + 1000
```

----------------------------------------

TITLE: Calling aten.lt.Tensor (Python)
DESCRIPTION: Performs element-wise less than comparison between two tensors, resulting in a boolean tensor. Example shows comparing an int64 tensor with a broadcasted int64 tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_20

LANGUAGE: Python
CODE:
```
((T([128], i64), T([128, 1], i64)), {})
```

----------------------------------------

TITLE: Convolution Backward Pass in PyTorch
DESCRIPTION: Computes gradients for convolution operations. Includes input gradients, weight gradients, and bias gradients for various layer configurations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
aten.convolution_backward.default((T([64, 512, 14, 14], f16), T([64, 512, 14, 14], f16), T([512, 512, 3, 3], f16), [512], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, True]), {})
```

LANGUAGE: Python
CODE:
```
aten.convolution_backward.default((T([64, 512, 28, 28], f16), T([64, 512, 28, 28], f16), T([512, 512, 3, 3], f16), [512], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, True]), {})
```

----------------------------------------

TITLE: Profiling PyTorch Split With Sizes Operator - Python
DESCRIPTION: This snippet lists input argument tuples for ATen split_with_sizes.default, testing splits of high-dimensional tensors with specific shapes, strides, and split sizes along the last dimension. Used for validating tensor partitioning logic in forward/backward computation graphs for large models. Inputs are tensors and split size lists. Requires only PyTorch as dependency.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
cnt: 1, ((T([1024, 4, 144, 48], f16, stride=(27648, 144, 1, 576)), [16, 32], -1), {})
cnt: 1, ((T([1024, 4, 144, 80], f16, stride=(46080, 144, 1, 576)), [16, 64], -1), {})
cnt: 1, ((T([1024, 1, 144, 80], f16, stride=(11520, 144, 1, 144)), [16, 64], -1), {})
```

----------------------------------------

TITLE: PyTorch Matrix Multiplication Operations
DESCRIPTION: Matrix multiplication operations between tensors with different shapes and strides using half-precision format.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_xception65_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
((T([32, 1000], f16), T([1000, 2048], f16)), {})
((T([1000, 32], f16, stride=(1, 1000)), T([32, 2048], f16)), {})
```

----------------------------------------

TITLE: Analyzing Convolution Operations in PyTorch
DESCRIPTION: Records of aten.convolution.default operator calls with various tensor shapes and convolution parameters. These operations represent different convolutional layers in a neural network with different filter sizes, strides, and padding configurations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_resnest_training.txt#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
Operator: aten.convolution.default
cnt: 1, ((T([32, 3, 224, 224], f16), T([32, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 32, 112, 112], f16), T([32, 32, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 32, 112, 112], f16), T([64, 32, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 64, 56, 56], f16), T([64, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 64, 56, 56], f16), T([128, 32, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 2), {})
cnt: 1, ((T([32, 64, 1, 1], f16), T([32, 64, 1, 1], f16), T([32], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 32, 1, 1], f16), T([128, 32, 1, 1], f16), T([128], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([32, 64, 56, 56], f16), T([256, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 256, 56, 56], f16), T([128, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 128, 56, 56], f16), T([256, 64, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 2), {})
cnt: 1, ((T([32, 128, 1, 1], f16), T([64, 128, 1, 1], f16), T([64], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 64, 1, 1], f16), T([256, 64, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 128, 28, 28], f16), T([512, 128, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 256, 28, 28], f16), T([512, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 512, 28, 28], f16), T([256, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 256, 28, 28], f16), T([512, 128, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 2), {})
cnt: 1, ((T([32, 256, 1, 1], f16), T([128, 256, 1, 1], f16), T([128], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 128, 1, 1], f16), T([512, 128, 1, 1], f16), T([512], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 256, 14, 14], f16), T([1024, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 512, 14, 14], f16), T([1024, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 1024, 14, 14], f16), T([512, 1024, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 512, 14, 14], f16), T([1024, 256, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 2), {})
cnt: 1, ((T([32, 512, 1, 1], f16), T([256, 512, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 256, 1, 1], f16), T([1024, 256, 1, 1], f16), T([1024], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 512, 7, 7], f16), T([2048, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 1024, 7, 7], f16), T([2048, 1024, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

----------------------------------------

TITLE: Debugging PyTorch Indexing with LLDB (Initial)
DESCRIPTION: Shows how to use LLDB to set a breakpoint and launch a Python script that performs tensor indexing in PyTorch. Demonstrates the default debug information level, which provides function addresses but not source file/line or variable values.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_51

LANGUAGE: Bash
CODE:
```
% lldb -o "b applySelect" -o "process launch" -- python3 -c "import torch;print(torch.rand(5)[3])"
(lldb) target create "python"
Current executable set to '/usr/bin/python3' (arm64).
(lldb) settings set -- target.run-args  "-c" "import torch;print(torch.rand(5)[3])"
(lldb) b applySelect
Breakpoint 1: no locations (pending).
WARNING:  Unable to resolve breakpoint to any actual locations.
(lldb) process launch
2 locations added to breakpoint 1
Process 87729 stopped
* thread #1, queue = 'com.apple.main-thread', stop reason = breakpoint 1.1
    frame #0: 0x00000001023d55a8 libtorch_python.dylib`at::indexing::impl::applySelect(at::Tensor const&, long long, c10::SymInt, long long, c10::Device const&, std::__1::optional<c10::ArrayRef<c10::SymInt>> const&)
libtorch_python.dylib`at::indexing::impl::applySelect:
->  0x1023d55a8 <+0>:  sub    sp, sp, #0xd0
    0x1023d55ac <+4>:  stp    x24, x23, [sp, #0x90]
    0x1023d55b0 <+8>:  stp    x22, x21, [sp, #0xa0]
    0x1023d55b4 <+12>: stp    x20, x19, [sp, #0xb0]
Target 0: (python) stopped.
Process 87729 launched: '/usr/bin/python' (arm64)
```

----------------------------------------

TITLE: Analyzing ATen Unsafe View Operations in PyTorch
DESCRIPTION: This snippet documents instances where the ATen unsafe view operator is utilized to reshape tensors without regard to memory layout security. Examples show transformations on tensors of various dimensional forms, indicating frequent tensor manipulation and reshaping in computational processes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistilBertForMaskedLM_training.txt#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
Operator: aten._unsafe_view.default
cnt: 18, ((T([16, 12, 128, 64], f16), [192, 128, 64]), {})
cnt: 6, ((T([16, 12, 64, 128], f16), [192, 64, 128]), {})
cnt: 6, ((T([192, 128, 128], f16), [16, 12, 128, 128]), {})
cnt: 6, ((T([192, 128, 64], f16), [16, 12, 128, 64]), {})
cnt: 12, ((T([16, 128, 12, 64], f16), [16, 128, 768]), {})
cnt: 6, ((T([16, 128, 768], f16), [2048, 768]), {})
```

----------------------------------------

TITLE: PyTorch Threshold Backward Operations
DESCRIPTION: This snippet shows backward operations for threshold functions (likely ReLU backward) applied to tensors of shapes [128, 1984, 7, 7] and [128, 1104, 7, 7] with f16 precision, using a threshold value of 0.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_17

LANGUAGE: python
CODE:
```
Operator: aten.threshold_backward.default
cnt: 1, ((T([128, 1984, 7, 7], f16), T([128, 1984, 7, 7], f16), 0), {})
cnt: 8, ((T([128, 1104, 7, 7], f16), T([128, 1104, 7, 7], f16), 0), {})
```

----------------------------------------

TITLE: Applying PyTorch aten.add_.Tensor Operator in Python
DESCRIPTION: This snippet showcases the 'aten.add_.Tensor' operator from PyTorch, which performs in-place element-wise addition of tensors. It processes tensors with the same shape and half-precision float format (f16), reducing memory footprint by modifying the original tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnext50_32x4d_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
cnt: 3, ((T([8, 256, 56, 56], f16), T([8, 256, 56, 56], f16)), {})
cnt: 4, ((T([8, 512, 28, 28], f16), T([8, 512, 28, 28], f16)), {})
cnt: 6, ((T([8, 1024, 14, 14], f16), T([8, 1024, 14, 14], f16)), {})
cnt: 3, ((T([8, 2048, 7, 7], f16), T([8, 2048, 7, 7], f16)), {})
```

----------------------------------------

TITLE: Subtracting Scalar from Tensor using aten.rsub.Scalar - Python
DESCRIPTION: Illustrates calls to aten.rsub.Scalar to subtract a scalar from all elements of the tensor, covering both regular and strided tensors in f16 and f32 formats. Used in residual or normalization connections in neural networks. Requirements: PyTorch, appropriate tensor and scalar type compatibility.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
Operator: aten.rsub.Scalar
cnt: 24, ((T([2, 1, 1, 1024], f16), 1.0), {})
cnt: 24, ((T([2, 12, 64, 448], f32), 1.0), {})
cnt: 12, ((T([2, 1, 12, 64, 192], f16), 1.0), {})
cnt: 24, ((T([2, 1, 1, 1, 64], f16, stride=(1024, 1024, 1024, 64, 1)), 1.0), {})
cnt: 12, ((T([2, 12, 12, 64, 192], f32, stride=(2064384, 172032, 12288, 192, 1)), 1.0), {})
```

----------------------------------------

TITLE: Installing functorch Development Dependencies
DESCRIPTION: Bash commands for installing additional requirements for AOTAutograd and testing.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
pip install networkx
```

----------------------------------------

TITLE: Triggering TorchDynamo AssertionError (Python)
DESCRIPTION: This Python snippet demonstrates how to trigger a specific error within TorchDynamo by attempting to compile a function that uses a PyTorch tensor as a dictionary key, which is not supported by the tracer. It compiles the function using `torch.compile` with the `"eager"` backend to ensure the error originates from Dynamo itself. Running this code will result in an `AssertionError` during the compilation process.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting_old.rst#_snippet_0

LANGUAGE: Python
CODE:
```
import torch

import torch._dynamo as dynamo


def test_assertion_error():
    y = torch.ones(200, 200)
    z = {y: 5}
    return z

compiled_test_assertion_error = torch.compile(test_assertion_error, backend="eager")

compiled_test_assertion_error()
```

----------------------------------------

TITLE: Conducting Log Softmax Backward Data Operation in PyTorch
DESCRIPTION: Applies the backward phase of the log softmax on a tensor with the same shape and data type as the input. Utilizes gradient data of type f16 to update model parameters based on error propagation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/XGLMForCausalLM_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
"""Operator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([256, 256008], f16), T([256, 256008], f16), 1, f16), {})"""
```

----------------------------------------

TITLE: Invoking aten._log_softmax Operator - PyTorch - Python
DESCRIPTION: This snippet documents the invocation of the aten._log_softmax operator, which computes the logarithm of the softmax over a specified dimension of a floating-point tensor of shape [4, 2] (dtype: f16). There are no additional kwargs. Key parameters include the input tensor, axis, and boolean for contiguous memory. The output is a log-probability tensor of the same shape. Dependencies: torch, with half-precision support. Limitations: f16 precision and specific axis constraints.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GPT2ForSequenceClassification_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([4, 2], f16), 1, False), {})
```

----------------------------------------

TITLE: Creating New Tensor Filled with Zeros - PyTorch Aten
DESCRIPTION: Creates a new tensor filled with zeros with a specified size and properties (like dtype, layout, device) derived from a source tensor or provided explicitly. This internal operator is used for allocating new zero-initialized tensors. It takes a source tensor and the desired size of the new tensor, plus options.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BartForConditionalGeneration_training.txt#_snippet_20

LANGUAGE: Python
CODE:
```
import torch

input_tensor = torch.randn(2, 1024, dtype=torch.int64)
new_tensor = input_tensor.new_zeros([2, 1024], dtype=torch.int64, device='cuda')
```

----------------------------------------

TITLE: Running CSAN Detection via Command Line
DESCRIPTION: Command to run the CUDA Sanitizer on a Python script to detect synchronization issues.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cuda._sanitizer.rst#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
TORCH_CUDA_SANITIZER=1 python example_error.py
```

----------------------------------------

TITLE: Defining PyTorch Build Options (CMake)
DESCRIPTION: A comprehensive block defining various build options using `option` for boolean flags and `cmake_dependent_option` for flags dependent on others. These options control which parts of PyTorch are built (binaries, tests, JNI), whether specific features are enabled (CUDA, ROCm, cuDNN, sanitizers), how dependencies are handled (custom protobuf, static runtime), and other build characteristics.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#_snippet_8

LANGUAGE: CMake
CODE:
```
# ---[ Options. Note to developers: if you add an option below, make sure you
# also add it to cmake/Summary.cmake so that the summary prints out the option
# values.
include(CMakeDependentOption)
option(ATEN_NO_TEST "Do not build ATen test binaries" OFF)
option(BUILD_BINARY "Build C++ binaries" OFF)
option(BUILD_CUSTOM_PROTOBUF
       "Build and use Caffe2's own protobuf under third_party" ON)
option(BUILD_PYTHON "Build Python binaries" ON)
option(BUILD_LITE_INTERPRETER "Master flag to build Lite Interpreter" OFF)
option(BUILD_SHARED_LIBS "Build libcaffe2.so" ON)
cmake_dependent_option(
  CAFFE2_LINK_LOCAL_PROTOBUF "If set, build protobuf inside libcaffe2.so." ON
  "BUILD_SHARED_LIBS AND BUILD_CUSTOM_PROTOBUF" OFF)
cmake_dependent_option(
  CAFFE2_USE_MSVC_STATIC_RUNTIME "Using MSVC static runtime libraries" ON
  "NOT BUILD_SHARED_LIBS" OFF)
option(BUILD_TEST "Build C++ test binaries (need gtest and gbenchmark)" OFF)
option(BUILD_AOT_INDUCTOR_TEST "Build C++ test binaries for aot-inductor" OFF)
option(BUILD_STATIC_RUNTIME_BENCHMARK
       "Build C++ binaries for static runtime benchmarks (need gbenchmark)" OFF)
option(
  BUILD_MOBILE_BENCHMARK
  "Build C++ test binaries for mobile (ARM) targets(need gtest and gbenchmark)"
  OFF)
option(
  BUILD_MOBILE_TEST
  "Build C++ test binaries for mobile (ARM) targets(need gtest and gbenchmark)"
  OFF)
option(BUILD_JNI "Build JNI bindings" OFF)
option(BUILD_MOBILE_AUTOGRAD
       "Build autograd function in mobile build (in development)" OFF)
cmake_dependent_option(INSTALL_TEST "Install test binaries if BUILD_TEST is on" ON "BUILD_TEST" OFF)
option(USE_CPP_CODE_COVERAGE "Compile C/C++ with code coverage flags" OFF)
option(USE_COLORIZE_OUTPUT "Colorize output during compilation" ON)
option(USE_ASAN "Use Address+Undefined Sanitizers" OFF)
option(USE_TSAN "Use Thread Sanitizer" OFF)
option(USE_CUDA "Use CUDA" ON)
option(USE_XPU "Use XPU" ON)
cmake_dependent_option(
  BUILD_LAZY_CUDA_LINALG "Build cuda linalg ops as separate library" ON
  "USE_CUDA AND LINUX AND BUILD_PYTHON" OFF)
cmake_dependent_option(USE_ROCM "Use ROCm" ON "LINUX" OFF)
option(CAFFE2_STATIC_LINK_CUDA "Statically link CUDA libraries" OFF)
cmake_dependent_option(USE_CUDNN "Use cuDNN" ON "USE_CUDA" OFF)
cmake_dependent_option(USE_STATIC_CUDNN "Use cuDNN static libraries" OFF
                       "USE_CUDNN" OFF)
cmake_dependent_option(USE_CUSPARSELT "Use cuSPARSELt" ON "USE_CUDA" OFF)
cmake_dependent_option(USE_CUDSS "Use cuDSS" ON "USE_CUDA" OFF)
```

----------------------------------------

TITLE: Run Sphinx Documentation Tests (Bash)
DESCRIPTION: These commands are used to execute the documentation tests configured using the Sphinx Doctest Extension. Navigating to the `docs` directory and running `make doctest` triggers Sphinx to find and run code snippets embedded within the docstrings and `.rst` source files as automated unit tests.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_35

LANGUAGE: bash
CODE:
```
cd docs
make doctest
```

----------------------------------------

TITLE: Importing jacrev and jacfwd for Reverse and Forward Mode - Python
DESCRIPTION: Imports functorch.jacrev and jacfwd, exposing both the reverse-mode and forward-mode Jacobian transform APIs. These are used for later function and performance comparisons. No input/output except module imports.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
from functorch import jacrev, jacfwd
```

----------------------------------------

TITLE: Handling Unsafe Globals in NumPy < 1.25 for PyTorch Serialization
DESCRIPTION: Example of an error message when encountering unsafe globals with NumPy < 1.25 during PyTorch serialization, and how to allowlist the problematic type.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
WeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`,
but got <class 'numpy.dtype[float32]'>
```

----------------------------------------

TITLE: Summing Tensor Elements - PyTorch Aten
DESCRIPTION: Computes the sum of all elements in a tensor or the sum along specified dimensions. This internal operator is used for reduction operations. It takes the input tensor, a list of dimensions to sum over, and a boolean flag to keep the dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BartForConditionalGeneration_training.txt#_snippet_23

LANGUAGE: Python
CODE:
```
import torch

input_tensor = torch.randn(2048, 1024, dtype=torch.float16)
sum_result = torch.sum(input_tensor, dim=[0], keepdim=True)
```

----------------------------------------

TITLE: Defining a Short Class with Proper Docstring
DESCRIPTION: A minimal class that includes a proper docstring despite having no implementation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/python_code.py.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
class ShortWithDocstring:
    """This docstring, while short, is enough"""
    pass
```

----------------------------------------

TITLE: Configuring Gradle for Local PyTorch Android Build
DESCRIPTION: This Gradle configuration demonstrates how to use locally built PyTorch Android AAR files in an Android project. It includes repository setup and dependency declarations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/android/README.md#2025-04-22_snippet_3

LANGUAGE: Groovy
CODE:
```
allprojects {
    repositories {
        flatDir {
            dirs 'libs'
        }
    }
}

dependencies {
    implementation(name:'pytorch_android', ext:'aar')
    implementation(name:'pytorch_android_torchvision', ext:'aar')
    ...
    implementation 'com.facebook.soloader:nativeloader:0.10.5'
    implementation 'com.facebook.fbjni:fbjni-java-only:0.2.2'
}
```

----------------------------------------

TITLE: Waiting for Asynchronous Tasks with torch.jit.wait() in TorchScript
DESCRIPTION: Forces completion of a torch.jit.Future[T] asynchronous task, returning the result of the task.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_37

LANGUAGE: python
CODE:
```
torch.jit.wait()
```

----------------------------------------

TITLE: Indexing and Copying Operations: clone, copy_, cumsum in PyTorch (Python)
DESCRIPTION: Shows standard tensor deep copy (clone), value replacement (copy_), and cumulative sum (cumsum) ops, taking integer or float tensors with single or multi-dimensional shapes. cumsum requires a dimension argument; copy_ operates in-place, and clone produces an object with the same content but a new memory address.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
Operator: aten.clone.default
cnt: 2, ((T([4, 128], i64),), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.copy_.default
cnt: 2, ((T([4, 128], i64), T([4, 128], i64)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.cumsum.default
cnt: 1, ((T([4, 128], i32), 1), {})
```

----------------------------------------

TITLE: Supported In-place Operations with vmap
DESCRIPTION: This example shows a case where in-place operations are supported by vmap, specifically when the tensor being modified has the same number of elements as the result of the operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/ux_limitations.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
def f(x, y):
  x.add_(y)
  return x

x = torch.randn(3)
y = torch.randn(3)
expected = x + y

# Does not raise an error because x and y have the same number of elements.
vmap(f, in_dims=(0, 0))(x, y)
assert torch.allclose(x, expected)
```

----------------------------------------

TITLE: Sparsifying Training Data with Data Sparsifier in PyTorch
DESCRIPTION: Example demonstrating how to sparsify input training data before passing it to a model. This approach can be useful when the input data itself can benefit from sparsification.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/README.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
model = SomeModel()

data_sparsifier = ImplementedDataSparsifier(threshold=0.2)

data_name = 'train_data'

for x, y in train_data_loader:
    x = data_sparsifier.add_data(name=data_name, data=x)
    ...
    y_out = model(x)
    ...
    data_sparsifier.step()
```

----------------------------------------

TITLE: Layer Normalization Operations in PyTorch
DESCRIPTION: This snippet demonstrates the usage of aten.native_layer_norm.default and aten.native_layer_norm_backward.default operators for layer normalization in neural networks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/attention_is_all_you_need_pytorch_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
Operator: aten.native_layer_norm.default
cnt: 13, ((T([256, 33, 512], f16), [512], T([512], f16), T([512], f16), 1e-06), {})
cnt: 19, ((T([256, 31, 512], f16), [512], T([512], f16), T([512], f16), 1e-06), {})
Operator: aten.native_layer_norm_backward.default
cnt: 19, ((T([256, 31, 512], f16), T([256, 31, 512], f16), [512], T([256, 31, 1], f32), T([256, 31, 1], f32), T([512], f16), T([512], f16), [True, True, True]), {})
cnt: 13, ((T([256, 33, 512], f16), T([256, 33, 512], f16), [512], T([256, 33, 1], f32), T([256, 33, 1], f32), T([512], f16), T([512], f16), [True, True, True]), {})
```

----------------------------------------

TITLE: Usage Examples for aten.sigmoid.default
DESCRIPTION: Logs Sigmoid activation function calls (`aten.sigmoid.default`). These are out-of-place operations applied to 4D float16 tensors where the spatial dimensions are singletons ([128, C, 1, 1]).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/regnety_002_training.txt#2025-04-22_snippet_10

LANGUAGE: plaintext
CODE:
```
Operator: aten.sigmoid.default
cnt: 1, ((T([128, 24, 1, 1], f16),), {})
cnt: 1, ((T([128, 56, 1, 1], f16),), {})
cnt: 4, ((T([128, 152, 1, 1], f16),), {})
cnt: 7, ((T([128, 368, 1, 1], f16),), {})
```

----------------------------------------

TITLE: Setting JIT Fusion Strategy (Python)
DESCRIPTION: This Python snippet demonstrates how to configure the PyTorch JIT's fusion strategy using `torch._C._jit_set_fusion_strategy()`. It takes a list of tuples specifying the type of compilation ('STATIC' or 'DYNAMIC') and the number of attempts for each type before falling back, controlling how the graph executor handles shape specialization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_23

LANGUAGE: Python
CODE:
```
torch._C._jit_set_fusion_strategy([
    ("STATIC", 2),
    ("DYNAMIC", 20),
])
```

----------------------------------------

TITLE: Documenting LSTM Optimization Conditions in reStructuredText
DESCRIPTION: This snippet outlines the conditions required for selecting a persistent algorithm to improve LSTM performance in PyTorch. It specifies requirements related to cuDNN, GPU usage, data type, hardware, and input format.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cudnn_persistent_rnn.rst#2025-04-22_snippet_0

LANGUAGE: reStructuredText
CODE:
```
.. note::

    If the following conditions are satisfied:
    1) cudnn is enabled,
    2) input data is on the GPU
    3) input data has dtype ``torch.float16``
    4) V100 GPU is used,
    5) input data is not in ``PackedSequence`` format
    persistent algorithm can be selected to improve performance.
```

----------------------------------------

TITLE: Run Specific PyTorch C++ Test (bash)
DESCRIPTION: Executes a compiled C++ test binary (`test_jit`) located in the `build/bin` directory after building PyTorch from source. The `--gtest_filter` flag is used to run only a specific test method (`MayContainAlias`) within a designated test suite (`ContainerAliasingTest`) using the Google Test framework.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_19

LANGUAGE: bash
CODE:
```
./build/bin/test_jit --gtest_filter=ContainerAliasingTest.MayContainAlias
```

----------------------------------------

TITLE: Running Single Test in Distributed Spawn Suite
DESCRIPTION: Command to run a specific test (DDP profiling with torch profiler) in the distributed spawn test suite, setting up environment variables for backend and world size.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/distributed/CONTRIBUTING.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
touch /tmp/barrier && TEMP_DIR="/tmp" BACKEND="nccl" WORLD_SIZE="2" python test/distributed/test_distributed_spawn.py -v TestDistBackendWithSpawn.test_ddp_profiling_torch_profiler
```

----------------------------------------

TITLE: Pull PyTorch Nightly Updates with conda (bash)
DESCRIPTION: Uses the `./tools/nightly.py` script with the `pull` command to fetch the latest nightly commits and reinstall dependencies, including binaries, into a specified `conda` environment (`-p`). This updates the environment and binaries in the repository directory; a subsequent `source` command activates the environment (use `& .\venv\Scripts\Activate.ps1` on Windows).
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_11

LANGUAGE: bash
CODE:
```
./tools/nightly.py pull -p my-env
source my-env/bin/activate
```

----------------------------------------

TITLE: Illustrating Unreachable Code Removal - Python
DESCRIPTION: Demonstrates a Python `if/else` block where one branch raises an exception and the other returns. This example illustrates how the PyTorch JIT compiler can identify and remove unreachable code (the `print` statement) following the block, inserting `prim::Uninitialized` for values in branches that exit prematurely.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_14

LANGUAGE: python
CODE:
```
if i < 0:
  raise Exception("Negative input")
else:
  return math.sqrt(i)
print(i)  # unreachable code
```

----------------------------------------

TITLE: Accessing fill_uninitialized_memory Attribute in PyTorch
DESCRIPTION: This snippet demonstrates how to access the fill_uninitialized_memory attribute in the torch.utils.deterministic module. When set to True, it causes uninitialized memory to be filled with known values when using deterministic algorithms. This affects performance but ensures deterministic behavior.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/deterministic.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
torch.utils.deterministic.fill_uninitialized_memory
```

----------------------------------------

TITLE: Running All Benchmarks with Thread Control
DESCRIPTION: Executes all benchmarks with explicit control over OpenMP and MKL threading.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_7

LANGUAGE: bash
CODE:
```
python -m benchmark_all_test --omp-num-threads 1 --mkl-num-threads 1
```

----------------------------------------

TITLE: Matrix Multiplication in PyTorch
DESCRIPTION: Matrix multiplication operations, likely used in fully connected layers or attention mechanisms. These operations involve reshaping tensors and performing dot products.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
cnt: 2, ((T([262144, 16], f16), T([16, 23], f16, stride=(1, 16))), {})
```

----------------------------------------

TITLE: Running Default Code Coverage for All Tests
DESCRIPTION: This snippet shows how to run the code coverage tool with default settings, which will run all tests and collect coverage over the entire PyTorch folder.
SOURCE: https://github.com/pytorch/pytorch/blob/main/tools/code_coverage/README.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
python oss_coverage.py
```

----------------------------------------

TITLE: Running PyTorch Op Tests with Pytest
DESCRIPTION: Provides basic pytest commands to execute operator tests, including running all tests, filtering by a specific operator name (`-k ceil`), or filtering by a functional neural network operator (`-k nn_functional_scaled_dot_product_attention`).
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/onnx/torchlib/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
# All
python -m pytest test_ops.py

# To run tests on a specific operator (e.g. torch.ceil):
python -m pytest test_ops.py -k ceil

# To run tests on a nn operator (e.g. nn.functional.scaled_dot_product_attention):
python -m pytest test_ops.py -k nn_functional_scaled_dot_product_attention
```

----------------------------------------

TITLE: PyTorch CPU Stream Classes
DESCRIPTION: Documentation section listing stream-related classes for CPU operations in PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cpu.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
Stream
```

----------------------------------------

TITLE: Running PyTorch Benchmarks with Command-Line Help
DESCRIPTION: Shows how to access help documentation for benchmark command-line options.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_6

LANGUAGE: bash
CODE:
```
python benchmark_runner.py --help
```

----------------------------------------

TITLE: Calling aten.ne.Scalar (Python)
DESCRIPTION: Performs element-wise not equal comparison between a tensor and a scalar value, resulting in a boolean tensor. Example shows checking if elements in an int64 tensor are not equal to 1.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_26

LANGUAGE: Python
CODE:
```
((T([8, 128], i64), 1), {})
```

----------------------------------------

TITLE: Analyzing Element-wise Multiplication in PyTorch
DESCRIPTION: This snippet shows the usage of element-wise multiplication operations in the model. It includes the shapes of the tensors being multiplied and the frequency of these operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
Operator: aten.mul.Tensor
cnt: 2, ((T([128, 72, 28, 28], f16), T([128, 72, 1, 1], f16)), {})
cnt: 2, ((T([128, 120, 28, 28], f16), T([128, 120, 1, 1], f16)), {})
cnt: 2, ((T([128, 480, 14, 14], f16), T([128, 480, 1, 1], f16)), {})
cnt: 2, ((T([128, 672, 14, 14], f16), T([128, 672, 1, 1], f16)), {})
```

----------------------------------------

TITLE: Configure Apple Specific Flags (CMake)
DESCRIPTION: Sets compiler flags and linker flags for Apple platforms, specifically enabling MPS support with related definitions and weak framework linking.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#_snippet_28

LANGUAGE: CMake
CODE:
```
if(APPLE)
  if(USE_MPS)
    string(APPEND CMAKE_OBJCXX_FLAGS " -DUSE_MPS -fno-objc-arc")
    string(APPEND CMAKE_CXX_FLAGS " -DUSE_MPS")
    string(
      APPEND
      CMAKE_SHARED_LINKER_FLAGS
      " -weak_framework Foundation -weak_framework MetalPerformanceShaders -weak_framework MetalPerformanceShadersGraph -weak_framework Metal"
    )
    # To suppress MPSGraph availability warnings
    append_cxx_flag_if_supported("-Wno-unguarded-availability-new"
                                 CMAKE_OBJCXX_FLAGS)
  endif()
  append_cxx_flag_if_supported("-Wno-missing-braces" CMAKE_CXX_FLAGS)
endif()
```

----------------------------------------

TITLE: PyTorch Mean Operation
DESCRIPTION: These snippets show the mean operation applied to tensors along specific dimensions, using float16 precision.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_13

LANGUAGE: Python
CODE:
```
((T([128, 256, 48, 48], f16), [2, 3], True), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 512, 24, 24], f16), [2, 3], True), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 1536, 12, 12], f16), [2, 3], True), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 1536, 6, 6], f16), [2, 3], True), {})
```

LANGUAGE: Python
CODE:
```
((T([128, 3072, 6, 6], f16), [-1, -2], True), {})
```

----------------------------------------

TITLE: Using GPUDirect Storage in PyTorch
DESCRIPTION: This example demonstrates how to use the GdsFile class from torch.cuda.gds for direct memory access transfers between GPU memory and storage. It requires CUDA 12.6 or higher and proper system configuration for GPUDirect Storage.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cuda.rst#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
from torch.cuda.gds import GdsFile

# Example usage of GdsFile
# (Placeholder code, actual implementation may vary)
```

----------------------------------------

TITLE: Importing the NumPy Compatibility Layer for Debugging
DESCRIPTION: Example of how to use the compatibility layer in eager mode by replacing the standard NumPy import with torch._numpy for debugging purposes. This allows determining if issues are caused by dynamo or the compatibility layer itself.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/_numpy/README.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
import torch._numpy as np
```

----------------------------------------

TITLE: Using torch-tensor-repr in GDB for Tensor Info
DESCRIPTION: Shows a GDB debugging session where a breakpoint is hit inside a PyTorch `at::Tensor` method. It demonstrates how the custom `torch-tensor-repr` command is used to print a user-friendly representation of an `at::Tensor` object (`*this`) compared to GDB's default output.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_54

LANGUAGE: GDB
CODE:
```
$ gdb python
GNU gdb (GDB) 9.2
[...]
(gdb) # insert a breakpoint when we call .neg()
(gdb) break at::Tensor::neg
Function "at::Tensor::neg" not defined.
Make breakpoint pending on future shared library load? (y or [n]) y
Breakpoint 1 (at::Tensor::neg) pending.

(gdb) run
[...]
>>> import torch
>>> t = torch.tensor([1, 2, 3, 4], dtype=torch.float64)
>>> t
tensor([1., 2., 3., 4.], dtype=torch.float64)
>>> t.neg()

Thread 1 "python" hit Breakpoint 1, at::Tensor::neg (this=0x7ffb118a9c88) at aten/src/ATen/core/TensorBody.h:3295
3295    inline at::Tensor Tensor::neg() const {
(gdb) # the default repr of 'this' is not very useful
(gdb) p this
$1 = (const at::Tensor * const) 0x7ffb118a9c88
(gdb) p *this
$2 = {impl_ = {target_ = 0x55629b5cd330}}
(gdb) torch-tensor-repr *this
Python-level repr of *this:
tensor([1., 2., 3., 4.], dtype=torch.float64)
```

----------------------------------------

TITLE: Tensor Multiplication with aten.addmm in PyTorch
DESCRIPTION: Applies aten.addmm.default for performing matrix multiplication followed by addition in f16 precision, essential for neural network layer computations. Dependencies include PyTorch and potentially CUDA for better performance.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MobileBertForMaskedLM_training.txt#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
aten.addmm.default, ((T([512], f16), T([2048, 384], f16), T([384, 512], f16, stride=(1, 384))), {})
```

----------------------------------------

TITLE: Patching Code into a Package
DESCRIPTION: Demonstrates how to use PackageExporter's save_source_string() method to save arbitrary Python source code to a module in the package, allowing for code patching and overriding.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
with PackageExporter(f) as exporter:
    # Save the my_module.foo available in your current Python environment.
    exporter.save_module("my_module.foo")

    # This saves the provided string to my_module/foo.py in the package archive.
    # It will override the my_module.foo that was previously saved.
    exporter.save_source_string("my_module.foo", textwrap.dedent(
        """
        def my_function():
            print('hello world')
        """
    ))

    # If you want to treat my_module.bar as a package
    # (e.g. save to `my_module/bar/__init__.py` instead of `my_module/bar.py)
    # pass is_package=True,
    exporter.save_source_string("my_module.bar",
                                "def foo(): print('hello')\n",
                                is_package=True)

importer = PackageImporter(f)
importer.import_module("my_module.foo").my_function()  # prints 'hello world'
```

----------------------------------------

TITLE: Matrix Multiplication Operation in PyTorch
DESCRIPTION: PyTorch's addmm operation that performs a matrix multiplication followed by an addition. This specific operation uses 16-bit floating point tensors and is likely part of a fully connected layer in a neural network.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
Operator: aten.addmm.default
cnt: 1, ((T([1000], f16), T([64, 1024], f16), T([1024, 1000], f16, stride=(1, 1024))), {})
```

----------------------------------------

TITLE: Matrix Multiplication in PyTorch
DESCRIPTION: Performs matrix multiplication between two tensors of compatible shapes. Used in fully connected layers and their backward passes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
aten.mm.default((T([64, 1000], f16, stride=(0, 0)), T([1000, 4096], f16)), {})
```

LANGUAGE: Python
CODE:
```
aten.mm.default((T([1000, 64], f16, stride=(0, 0)), T([64, 4096], f16)), {})
```

----------------------------------------

TITLE: Implementing Basic Unsqueeze Batching Rule in C++
DESCRIPTION: Demonstrates implementation of a basic batching rule for the unsqueeze operator. The rule handles moving batch dimensions and adjusting the unsqueeze dimension parameter to account for the batch dimension.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/writing_batching_rules.md#2025-04-22_snippet_0

LANGUAGE: cpp
CODE:
```
std::tuple<Tensor,optional<int64_t>> unsqueeze_batch_rule(
    const Tensor& self,
    optional<int64_t> self_bdim,
    int64_t dim) {
  auto self_ = moveBatchDimToFront(self, self_bdim);
  auto rank = rankWithoutBatchDim(self, self_bdim);
  dim = maybe_wrap_dim(dim, rank + 1) + 1;
  return std::make_tuple(self_.unsqueeze(dim), 0);
}
```

----------------------------------------

TITLE: Implementing Iterator for MapperIterDataPipe
DESCRIPTION: Implementation of the __iter__ method for MapperIterDataPipe that consumes data from source DataPipe and applies the mapping function before yielding results.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/datapipes/README.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
class MapperIterDataPipe(IterDataPipe):
    ...

    def __iter__(self):
        for d in self.dp:
            yield self.fn(d)
```

----------------------------------------

TITLE: NTK Computation using Vector Products
DESCRIPTION: Alternative implementation of empirical NTK computation using NTK-vector products method, which can be more efficient for certain model architectures.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/neural_tangent_kernels.ipynb#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
def empirical_ntk_ntk_vps(func, params, x1, x2, compute='full'):
    def get_ntk(x1, x2):
        def func_x1(params):
            return func(params, x1)

        def func_x2(params):
            return func(params, x2)

        output, vjp_fn = vjp(func_x1, params)

        def get_ntk_slice(vec):
            vjps = vjp_fn(vec)
            _, jvps = jvp(func_x2, (params,), vjps)
            return jvps

        basis = torch.eye(output.numel(), dtype=output.dtype, device=output.device).view(output.numel(), -1)
        return vmap(get_ntk_slice)(basis)
        
    result = vmap(vmap(get_ntk, (None, 0)), (0, None))(x1, x2)
    
    if compute == 'full':
        return result
    if compute == 'trace':
        return torch.einsum('NMKK->NM', result)
    if compute == 'diagonal':
        return torch.einsum('NMKK->NMK', result)
```

----------------------------------------

TITLE: Old API for Constants in TorchScript
DESCRIPTION: Shows the older approach for defining constants in TorchScript modules using the __constants__ class variable.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_14

LANGUAGE: python
CODE:
```
class MyModule(torch.jit.ScriptModule):
    __constants__ = ['my_constant']

    def __init__(self):
        super().__init__()
        self.my_constant = 2

    def forward(self):
        pass
m = MyModule()
```

----------------------------------------

TITLE: Computing Gradients using functorch.grad and make_functional in Python
DESCRIPTION: Demonstrates the legacy `functorch` approach to calculate gradients for model parameters. It uses `functorch.make_functional` to obtain a stateless version of the model (`fmodel`) and its parameters (`params`). A loss function `compute_loss` is defined, taking these parameters as input. Finally, `functorch.grad` is applied to this loss function to compute the gradients with respect to the parameters. Requires `torch` and `functorch` libraries.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.migrating.rst#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
# ---------------
# using functorch
# ---------------
import torch
import functorch
inputs = torch.randn(64, 3)
targets = torch.randn(64, 3)
model = torch.nn.Linear(3, 3)

fmodel, params = functorch.make_functional(model)

def compute_loss(params, inputs, targets):
    prediction = fmodel(params, inputs)
    return torch.nn.functional.mse_loss(prediction, targets)

grads = functorch.grad(compute_loss)(params, inputs, targets)
```

----------------------------------------

TITLE: Example of a FakeTensor Instance Representation (Python)
DESCRIPTION: This one-liner in Python demonstrates how to instantiate and display a FakeTensor object, showing typical initialization parameters such as data type, size, and device. Useful for conveying expected output from metadata propagation in symbolic tracing graphs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
FakeTensor(dtype=torch.int, size=[2,], device=CPU)
```

----------------------------------------

TITLE: Listing PyTorch Models with Batch Sizes
DESCRIPTION: This snippet provides a comprehensive list of PyTorch models and their corresponding batch sizes. The batch sizes are likely used for training or inference, and may represent optimal or maximum values for efficiency or memory constraints.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/torchbench_models_list.txt#2025-04-22_snippet_0

LANGUAGE: plaintext
CODE:
```
BERT_pytorch,128
Background_Matting, 16
LearningToPaint,1024
alexnet,1024
dcgan,1024
densenet121,64
hf_Albert,32
hf_Bart,16
hf_Bert,16
hf_GPT2,16
hf_T5,4
mnasnet1_0,256
mobilenet_v2,128
mobilenet_v3_large,256
nvidia_deeprecommender,1024
pytorch_unet,8
resnet18,512
resnet50,128
resnext50_32x4d,128
shufflenet_v2_x1_0,512
squeezenet1_1,512
timm_nfnet,256
timm_efficientnet,128
timm_regnet,128
timm_resnest,256
timm_vision_transformer,256
timm_vovnet,128
vgg16,128
```

----------------------------------------

TITLE: Using AutoDispatchBelowADInplaceOrView in Custom Autograd Functions
DESCRIPTION: Example of using AutoDispatchBelowADInplaceOrView (formerly AutoNonVariableTypeMode) in a custom autograd function to redispatch under Autograd dispatch keys for a ROI align operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/inference_mode.rst#2025-04-22_snippet_2

LANGUAGE: cpp
CODE:
```
class ROIAlignFunction : public torch::autograd::Function<ROIAlignFunction> {
 public:
  static torch::autograd::variable_list forward(
      torch::autograd::AutogradContext* ctx,
      const torch::autograd::Variable& input,
      const torch::autograd::Variable& rois,
      double spatial_scale,
      int64_t pooled_height,
      int64_t pooled_width,
      int64_t sampling_ratio,
      bool aligned) {
    ctx->saved_data["spatial_scale"] = spatial_scale;
    ctx->saved_data["pooled_height"] = pooled_height;
    ctx->saved_data["pooled_width"] = pooled_width;
    ctx->saved_data["sampling_ratio"] = sampling_ratio;
    ctx->saved_data["aligned"] = aligned;
    ctx->saved_data["input_shape"] = input.sizes();
    ctx->save_for_backward({rois});
    // Used to be at::AutoNonVariableTypeMode g;
    at::AutoDispatchBelowADInplaceOrView guard;
    auto result = roi_align(
        input, rois, spatial_scale, pooled_height,
        pooled_width, sampling_ratio, aligned);
    return {result};
  }
```

----------------------------------------

TITLE: Calling aten._log_softmax.default (Python)
DESCRIPTION: Performs the log-softmax operation on a tensor, typically used for calculating log probabilities in classification tasks. The snippet shows a call with a large float16 tensor, specifying the dimension and whether to include the original input in the gradient calculation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_0

LANGUAGE: Python
CODE:
```
((T([1024, 50265], f16), 1, False), {})
```

----------------------------------------

TITLE: Invoking aten.threshold_backward.default with Tensor Arguments (Text)
DESCRIPTION: This section logs calls to `aten.threshold_backward.default`, which computes gradients for the `threshold` operation (commonly used in ReLU's backward pass). It takes the gradient of the output and the input to the original threshold function. The examples show calls with float16 (f16) tensors of shapes like [128, 768, 1, 1] and [128, 128, 1, 1]. The third argument `0` represents the threshold value used in the forward pass.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_23

LANGUAGE: text
CODE:
```
Operator: aten.threshold_backward.default
```

LANGUAGE: text
CODE:
```
cnt: 9, ((T([128, 768, 1, 1], f16), T([128, 768, 1, 1], f16), 0), {})
```

LANGUAGE: text
CODE:
```
cnt: 2, ((T([128, 256, 1, 1], f16), T([128, 256, 1, 1], f16), 0), {})
```

LANGUAGE: text
CODE:
```
cnt: 1, ((T([128, 128, 1, 1], f16), T([128, 128, 1, 1], f16), 0), {})
```

----------------------------------------

TITLE: Configuring PyTorch Logging with TORCH_LOGS Environment Variable Examples
DESCRIPTION: These examples demonstrate setting the `TORCH_LOGS` environment variable in a shell to configure PyTorch's logging behavior. The `+` prefix decreases the log level (more verbose, e.g., DEBUG), while the `-` prefix increases it (less verbose, e.g., ERROR). Specifying an artifact name enables its output. Settings are comma-separated and apply to components like `dynamo`, `aot`, `inductor`, or custom modules.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/logging.rst#2025-04-22_snippet_0

LANGUAGE: shell
CODE:
```
TORCH_LOGS="+dynamo,aot"
```

LANGUAGE: shell
CODE:
```
TORCH_LOGS="-dynamo,+inductor"
```

LANGUAGE: shell
CODE:
```
TORCH_LOGS="aot_graphs"
```

LANGUAGE: shell
CODE:
```
TORCH_LOGS="+dynamo,schedule"
```

LANGUAGE: shell
CODE:
```
TORCH_LOGS="+some.random.module,schedule"
```

----------------------------------------

TITLE: Analyzing ATen Less Than Operations in PyTorch
DESCRIPTION: This snippet logs usage of the \"aten.lt.Tensor\" which performs element-wise comparisons between tensors to check if one tensor is less than another.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_15

LANGUAGE: pseudocode
CODE:
```
Operator: aten.lt.Tensor
cnt: 1, ((T([128], i64), T([128, 1], i64)), {})
```

----------------------------------------

TITLE: Average Pooling Operations in PyTorch
DESCRIPTION: This snippet shows average pooling operation used to downsample feature maps. The operation is performed on 4x4 spatial dimensions with a kernel size of 4x4, effectively reducing spatial dimensions to 1x1.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
Operator: aten.avg_pool2d.default
cnt: 1, ((T([96, 512, 4, 4], f16), [4, 4]), {})
```

----------------------------------------

TITLE: Analyzing ATen Matrix Multiplications in PyTorch
DESCRIPTION: Logs instances of \"aten.mm.default\" which perform matrix multiplication on various shaped float16 tensors with possible memory strides.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_17

LANGUAGE: pseudocode
CODE:
```
Operator: aten.mm.default
cnt: 1, ((T([1024, 1024], f16), T([1024, 50265], f16, stride=(1, 1024))), {})
cnt: 1, ((T([50265, 1024], f16, stride=(1, 50265)), T([1024, 1024], f16)), {})
cnt: 1, ((T([1024, 50265], f16), T([50265, 1024], f16)), {})
cnt: 12, ((T([1024, 1024], f16), T([1024, 4096], f16)), {})
cnt: 12, ((T([1024, 1024], f16, stride=(1, 1024)), T([1024, 4096], f16)), {})
cnt: 12, ((T([1024, 4096], f16), T([4096, 1024], f16)), {})
cnt: 12, ((T([4096, 1024], f16, stride=(1, 4096)), T([1024, 1024], f16)), {})
cnt: 48, ((T([1024, 1024], f16), T([1024, 1024], f16)), {})
cnt: 48, ((T([1024, 1024], f16, stride=(1, 1024)), T([1024, 1024], f16)), {})
```

----------------------------------------

TITLE: Resizing a Non-Quantized Tensor in PyTorch
DESCRIPTION: This code snippet demonstrates the use of the resize_() method on a PyTorch tensor. When fill_uninitialized_memory is True and deterministic algorithms are enabled, this operation will fill uninitialized memory for non-quantized tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/deterministic.rst#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
torch.Tensor.resize_()
```

----------------------------------------

TITLE: Performing Softmax Operations with Backward Pass Support
DESCRIPTION: This snippet illustrates both the forward (aten._softmax.default) and backward (aten._softmax_backward_data.default) operations performed on a tensor using the softmax function. It supports half-precision float tensors, emphasizing compatibility for optimized compute scenarios within deep learning models.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaForQuestionAnswering_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
Operator: aten._softmax.default
cnt: 12, ((T([4, 12, 512, 512], f16), -1, False), {})
```

LANGUAGE: Python
CODE:
```
Operator: aten._softmax_backward_data.default
cnt: 12, ((T([4, 12, 512, 512], f16), T([4, 12, 512, 512], f16), -1, f16), {})
```

----------------------------------------

TITLE: Activation Functions: GELU and LayerNorm (Forward and Backward) in PyTorch (Python)
DESCRIPTION: Demonstrates use of the Gaussian Error Linear Unit (gelu) and native_layer_norm (plus their backwards), common in transformer and modern deep models. Forward ops take tensors with compatible feature size; backward ops require additional gradients and scale tensors. All inputs are in fp16, and intermediate results are also fp16 or fp32.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
Operator: aten.gelu.default
cnt: 12, ((T([4, 128, 3072], f16),), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.gelu.default
cnt: 1, ((T([4, 128, 768], f16),), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.gelu_backward.default
cnt: 1, ((T([4, 128, 768], f16), T([4, 128, 768], f16)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.gelu_backward.default
cnt: 12, ((T([4, 128, 3072], f16), T([4, 128, 3072], f16)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.native_layer_norm.default
cnt: 26, ((T([4, 128, 768], f16), [768], T([768], f16), T([768], f16), 1e-12), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.native_layer_norm_backward.default
cnt: 26, ((T([4, 128, 768], f16), T([4, 128, 768], f16), [768], T([4, 128, 1], f32), T([4, 128, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})
```

----------------------------------------

TITLE: Usage Examples for aten.relu_.default
DESCRIPTION: Logs in-place Rectified Linear Unit (ReLU) activation function calls (`aten.relu_.default`). These operations modify the input tensor directly. Applied to float16 tensors, mostly 4D feature maps but also some 4D tensors with singleton spatial dimensions (e.g., [128, C, 1, 1]).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/regnety_002_training.txt#2025-04-22_snippet_9

LANGUAGE: plaintext
CODE:
```
Operator: aten.relu_.default
cnt: 1, ((T([128, 32, 112, 112], f16),), {})
cnt: 1, ((T([128, 24, 112, 112], f16),), {})
cnt: 1, ((T([128, 24, 56, 56], f16),), {})
cnt: 1, ((T([128, 8, 1, 1], f16),), {})
cnt: 1, ((T([128, 56, 56, 56], f16),), {})
cnt: 1, ((T([128, 56, 28, 28], f16),), {})
cnt: 1, ((T([128, 6, 1, 1], f16),), {})
cnt: 1, ((T([128, 152, 28, 28], f16),), {})
cnt: 7, ((T([128, 152, 14, 14], f16),), {})
cnt: 1, ((T([128, 14, 1, 1], f16),), {})
cnt: 4, ((T([128, 38, 1, 1], f16),), {})
cnt: 1, ((T([128, 368, 14, 14], f16),), {})
cnt: 13, ((T([128, 368, 7, 7], f16),), {})
cnt: 6, ((T([128, 92, 1, 1], f16),), {})
```

----------------------------------------

TITLE: Using ATen LogSoftmax Operator in PyTorch
DESCRIPTION: This operator applies the log softmax function along the specified dimension of the input tensor. Dependencies include PyTorch with ATen support. Key parameters are the input tensor shape, dimension along which softmax is computed, and data type. Output is a tensor with log softmax applied. It requires proper input dimension specification to avoid errors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/ElectraForCausalLM_training.txt#2025-04-22_snippet_0

LANGUAGE: plaintext
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([511, 30522], f16), 1, False), {})
```

----------------------------------------

TITLE: Tensor Concatenation Operations in PyTorch
DESCRIPTION: This snippet shows tensor concatenation operations (cat) used to combine tensors along specified dimensions. These operations are used to merge feature maps or detection results from different network stages.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
Operator: aten.cat.default
cnt: 4, (([T([269952, 4], f16), T([67488, 4], f16), T([16872, 4], f16), T([4218, 4], f16), T([1083, 4], f16)],), {})
cnt: 1, (([T([4, 269952, 1], f16), T([4, 67488, 1], f16), T([4, 16872, 1], f16), T([4, 4218, 1], f16), T([4, 1083, 1], f16)], 1), {})
cnt: 1, (([T([4, 269952, 4], f16), T([4, 67488, 4], f16), T([4, 16872, 4], f16), T([4, 4218, 4], f16), T([4, 1083, 4], f16)], 1), {})
cnt: 1, (([T([359613, 4], f16), T([359613, 4], f16), T([359613, 4], f16), T([359613, 4], f16)],), {})
cnt: 1, (([T([269952], i64), T([67488], i64), T([16872], i64), T([4218], i64), T([1083], i64)],), {})
cnt: 1, (([T([4, 1000], i64), T([4, 1000], i64), T([4, 1000], i64), T([4, 1000], i64), T([4, 1000], i64)], 1), {})
cnt: 3, (([T([0, 4], f16), T([0, 4], f16), T([0, 4], f16), T([0, 4], f16)],), {})
cnt: 2, (([T([0, 1], f16), T([0, 1], f16), T([0, 1], f16), T([0, 1], f16)],), {})
cnt: 2, (([T([0, 1], f16), T([0, 4], f16)], 1), {})
cnt: 2, (([T([0], f32), T([0], f32), T([0], f32), T([0], f32)],), {})
cnt: 1, (([T([0], i64), T([0], i64), T([0], i64), T([0], i64)],), {})
cnt: 1, (([T([0, 91], f16), T([0, 91], f16), T([0, 91], f16), T([0, 91], f16)],), {})
cnt: 1, (([T([0, 364], f16), T([0, 364], f16), T([0, 364], f16), T([0, 364], f16)],), {})
```

----------------------------------------

TITLE: Configure cuFFT Plan Cache for Specific Device PyTorch CUDA
DESCRIPTION: Accesses and configures the cuFFT plan cache for a non-default CUDA device (specified by index or device object). This allows setting properties like max_size per device.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_33

LANGUAGE: python
CODE:
```
torch.backends.cuda.cufft_plan_cache[1].max_size = 10
```

----------------------------------------

TITLE: Convolution Backward Operations in PyTorch
DESCRIPTION: This snippet shows backward pass operations for convolutions, used during gradient computation. These operations propagate gradients from output tensors back to input tensors and weights, following the same pattern as the forward convolutions but in reverse.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
Operator: aten.convolution_backward.default
cnt: 3, ((T([96, 512, 4, 4], f16), T([96, 512, 4, 4], f16), T([512, 512, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([96, 512, 4, 4], f16), T([96, 256, 8, 8], f16), T([512, 256, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([96, 512, 4, 4], f16), T([96, 256, 8, 8], f16), T([512, 256, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 3, ((T([96, 256, 8, 8], f16), T([96, 256, 8, 8], f16), T([256, 256, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([96, 256, 8, 8], f16), T([96, 128, 16, 16], f16), T([256, 128, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([96, 256, 8, 8], f16), T([96, 128, 16, 16], f16), T([256, 128, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 3, ((T([96, 128, 16, 16], f16), T([96, 128, 16, 16], f16), T([128, 128, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([96, 128, 16, 16], f16), T([96, 64, 32, 32], f16), T([128, 64, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([96, 128, 16, 16], f16), T([96, 64, 32, 32], f16), T([128, 64, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 3, ((T([96, 64, 32, 32], f16), T([96, 64, 32, 32], f16), T([64, 64, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([96, 64, 32, 32], f16), T([96, 64, 64, 64], f16), T([64, 64, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([96, 64, 32, 32], f16), T([96, 64, 64, 64], f16), T([64, 64, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([96, 64, 64, 64], f16), T([96, 9, 128, 128], f16), T([64, 9, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})
```

----------------------------------------

TITLE: Preparing PyTorch Build for ROCm on Linux Bash
DESCRIPTION: Runs a specific Python script located in the PyTorch source tree that performs necessary pre-build setup steps only when compiling PyTorch with support for AMD ROCm GPUs on Linux.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_11

LANGUAGE: Bash
CODE:
```
# Only run this if you're compiling for ROCm
python tools/amd_build/build_amd.py
```

----------------------------------------

TITLE: Batch Normalization Operations
DESCRIPTION: Batch normalization operations with momentum 0.1 and epsilon 1e-05 applied to different tensor shapes. Includes running mean and variance tracking.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
((T([64, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})
```

----------------------------------------

TITLE: PyTorch Batch Normalization
DESCRIPTION: Batch normalization operations on tensors with different channel dimensions. Includes running mean/variance tracking and momentum parameter of 0.1.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_efficientnet_b0_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
native_batch_norm.default((T([128, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 0.001))
```

----------------------------------------

TITLE: Batch Normalization Operations in PyTorch
DESCRIPTION: Profiling data for batch normalization operations showing count, tensor shapes, and parameters. These operations normalize activations across batches with learnable parameters, using a momentum of 0.1 and epsilon of 1e-05.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
Operator: aten.native_batch_norm.default
cnt: 2, ((T([32, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([32, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([32, 96, 112, 112], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([32, 96, 56, 56], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), False, 0.1, 1e-05), {})
cnt: 2, ((T([32, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), False, 0.1, 1e-05), {})
cnt: 3, ((T([32, 144, 56, 56], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([32, 144, 28, 28], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f16), False, 0.1, 1e-05), {})
cnt: 2, ((T([32, 40, 28, 28], f16), T([40], f16), T([40], f16), T([40], f16), T([40], f16), False, 0.1, 1e-05), {})
cnt: 3, ((T([32, 240, 28, 28], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([32, 240, 14, 14], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f16), False, 0.1, 1e-05), {})
cnt: 3, ((T([32, 80, 14, 14], f16), T([80], f16), T([80], f16), T([80], f16), T([80], f16), False, 0.1, 1e-05), {})
cnt: 6, ((T([32, 480, 14, 14], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f16), False, 0.1, 1e-05), {})
cnt: 3, ((T([32, 112, 14, 14], f16), T([112], f16), T([112], f16), T([112], f16), T([112], f16), False, 0.1, 1e-05), {})
cnt: 5, ((T([32, 672, 14, 14], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([32, 672, 7, 7], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), False, 0.1, 1e-05), {})
```

----------------------------------------

TITLE: Analyzing PyTorch Operator Usage Patterns
DESCRIPTION: This code snippet provides a detailed breakdown of PyTorch operator usage, including counts and parameter structures for various operations. It covers tensor shapes, data types, and specific configurations for operations like layer normalization, loss functions, and activations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilevit_s_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 512, 8, 8], f16), T([64, 512, 8, 8], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([64, 512, 16, 16], f16), T([64, 512, 16, 16], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), True, 1e-05, [True, True, True]), {})
cnt: 4, ((T([64, 128, 16, 16], f16), T([64, 128, 16, 16], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})
# ... (truncated for brevity)

Operator: aten.native_layer_norm.default
cnt: 5, ((T([256, 256, 144], f16), [144], T([144], f16), T([144], f16), 1e-05), {})
cnt: 9, ((T([256, 64, 192], f16), [192], T([192], f16), T([192], f16), 1e-05), {})
cnt: 7, ((T([256, 16, 240], f16), [240], T([240], f16), T([240], f16), 1e-05), {})

Operator: aten.native_layer_norm_backward.default
cnt: 7, ((T([256, 16, 240], f16), T([256, 16, 240], f16), [240], T([256, 16, 1], f32), T([256, 16, 1], f32), T([240], f16), T([240], f16), [True, True, True]), {})
cnt: 9, ((T([256, 64, 192], f16), T([256, 64, 192], f16), [192], T([256, 64, 1], f32), T([256, 64, 1], f32), T([192], f16), T([192], f16), [True, True, True]), {})
cnt: 5, ((T([256, 256, 144], f16), T([256, 256, 144], f16), [144], T([256, 256, 1], f32), T([256, 256, 1], f32), T([144], f16), T([144], f16), [True, True, True]), {})

Operator: aten.nll_loss_backward.default
cnt: 1, ((T([], f16), T([64, 1000], f16), T([64], i64), None, 1, -100, T([], f16)), {})

Operator: aten.nll_loss_forward.default
cnt: 1, ((T([64, 1000], f16), T([64], i64), None, 1, -100), {})

Operator: aten.silu.default
cnt: 2, ((T([256, 256, 288], f16),), {})
cnt: 4, ((T([256, 64, 384], f16),), {})
cnt: 3, ((T([256, 16, 480], f16),), {})

Operator: aten.silu_.default
cnt: 1, ((T([64, 16, 128, 128], f16),), {})
cnt: 2, ((T([64, 64, 128, 128], f16),), {})
cnt: 1, ((T([64, 128, 128, 128], f16),), {})
# ... (truncated for brevity)

Operator: aten.silu_backward.default
cnt: 1, ((T([64, 640, 8, 8], f16), T([64, 640, 8, 8], f16)), {})
cnt: 2, ((T([64, 160, 8, 8], f16), T([64, 160, 8, 8], f16)), {})
cnt: 1, ((T([64, 160, 8, 8], f16, stride=(20480, 64, 8, 1)), T([64, 160, 8, 8], f16)), {})
# ... (truncated for brevity)

Operator: aten.stack.default
cnt: 3, (([T([256, 4, 16, 60], f16), T([256, 4, 16, 60], f16, stride=(3840, 960, 1, 16)), T([256, 4, 16, 60], f16)],), {})
cnt: 4, (([T([256, 4, 64, 48], f16), T([256, 4, 64, 48], f16, stride=(12288, 3072, 1, 64)), T([256, 4, 64, 48], f16)],), {})
cnt: 2, (([T([256, 4, 256, 36], f16), T([256, 4, 256, 36], f16, stride=(36864, 9216, 1, 256)), T([256, 4, 256, 36], f16)],), {})

Operator: aten.sum.SymInt
cnt: 1, ((T([64, 1000], f16), [0], True), {})
cnt: 6, ((T([4096, 240], f16), [0], True), {})
cnt: 3, ((T([4096, 480], f16), [0], True), {})
# ... (truncated for brevity)

Operator: aten.unbind.int
cnt: 2, ((T([3, 256, 4, 256, 36], f16, stride=(144, 110592, 36, 432, 1)),), {})
cnt: 4, ((T([3, 256, 4, 64, 48], f16, stride=(192, 36864, 48, 576, 1)),), {})
cnt: 3, ((T([3, 256, 4, 16, 60], f16, stride=(240, 11520, 60, 720, 1)),), {})
```

----------------------------------------

TITLE: Start Simple HTTP Server (Bash)
DESCRIPTION: This command uses Python's built-in `http.server` module to start a lightweight web server. It runs on port 8000 and serves files from the directory specified by `<path_to_html_output>`, which should be the directory containing the built documentation HTML files. This is useful on a remote machine for previewing docs via a tunnel.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_33

LANGUAGE: bash
CODE:
```
python -m http.server 8000 <path_to_html_output>
```

----------------------------------------

TITLE: Log Softmax Backward Data - PyTorch Aten
DESCRIPTION: Computes the gradient of the log-softmax function with respect to the input data. This is the internal operator used during the backward pass of a log-softmax operation. It requires the gradient of the output, the output tensor itself, the dimension, and the desired dtype for the gradient.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BartForConditionalGeneration_training.txt#_snippet_1

LANGUAGE: Python
CODE:
```
import torch

grad_output = torch.randn(2048, 50265, dtype=torch.float16)
output = torch.randn(2048, 50265, dtype=torch.float16)
data_grad = torch._log_softmax_backward_data(grad_output, output, dim=1, dtype=torch.float16)
```

----------------------------------------

TITLE: Complete PyTorch Add Operator Benchmark Implementation
DESCRIPTION: Full implementation of a benchmark for torch.add showing input configuration, tensor creation, and the forward computation. Includes both 'short' and 'long' configurations using different helper methods.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
import operator_benchmark as op_bench
import torch

add_long_configs = op_bench.cross_product_configs(
    M=[8, 64, 128],
    N=range(2, 10, 3),
    K=[2 ** x for x in range(0, 3)],
    tags=["long"]
)

add_short_configs = op_bench.config_list(
    attr_names=["M", "N", "K"],
    attrs=[
        [8, 16, 32],
        [16, 16, 64],
        [64, 64, 128],
    ],
    tags=["short"],
)

class AddBenchmark(op_bench.TorchBenchmarkBase):
    def init(self, M, N, K, device):
        self.inputs = {
            "input_one": torch.rand(M, N, K, device=device, requires_grad=self.auto_set()),
            "input_two": torch.rand(M, N, K, device=device, requires_grad=self.auto_set())
        }
        self.set_module_name("add")

    def forward(self, input_one, input_two):
        return torch.add(input_one, input_two)

op_bench.generate_pt_test(add_long_configs + add_short_configs, AddBenchmark)

if __name__ == "__main__":
    op_bench.benchmark_runner.main()
```

----------------------------------------

TITLE: Creating Tensors Filled with Ones using aten.new_ones - Python
DESCRIPTION: Demonstrates usage of the aten.new_ones.default operator to initialize tensors filled with ones, specifying the required shape, dtype, device, and memory layout. Handles multi-dimensional tensors and adapts to CUDA settings. Dependencies: PyTorch, acceptance of 'f16' and 'f32' types, and device configuration.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
Operator: aten.new_ones.default
cnt: 24, ((T([2, 1, 1, 1024], f16), [2, 1, 1, 192]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda', 'pin_memory': False})
cnt: 24, ((T([2, 12, 14, 64, 192], f32), [2, 12, 64, 256]), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda', 'pin_memory': False})
```

----------------------------------------

TITLE: Exponentiating Tensors with Scalar using aten.pow.Tensor_Scalar - Python
DESCRIPTION: Shows several uses of the aten.pow.Tensor_Scalar operator to raise tensors to a scalar power, supporting both integer and floating-point exponents over multi-dimensional tensors. Relies on tensor broadcasting and matching shapes, using half-precision (f16) data. Requires PyTorch. Scalar parameter indicates the exponent.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
Operator: aten.pow.Tensor_Scalar
cnt: 12, ((T([2, 1024, 3072], f16), 3.0), {})
cnt: 1, ((T([2, 1024, 768], f16), 3.0), {})
cnt: 1, ((T([2, 1024, 768], f16), 2.0), {})
cnt: 12, ((T([2, 1024, 3072], f16), 2.0), {})
```

----------------------------------------

TITLE: Enable Tunable Operation in C++
DESCRIPTION: Demonstrates enabling the tunable operation feature in PyTorch using C++ syntax. This requires including the ATen CUDA tunable header and accessing the tuning context.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/cuda/tunable/README.md#2025-04-22_snippet_4

LANGUAGE: C++
CODE:
```
#include <ATen/cuda/tunable/Tunable.h>

at::cuda::tunable::getTuningContext()->EnableTunableOp(true);
```

----------------------------------------

TITLE: Indexing Implementation in PyTorch Python
DESCRIPTION: This snippet reflects the usage of the 'aten._index_put_impl_' operator, which handles in-place assignments to a tensor based on index arrays. It requires the PyTorch library for execution. The input includes a target tensor, index arrays, the values to assign, and flags dictating operational behavior. The operation outputs the modified tensor. This method is efficient for sparse update operations on tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten._index_put_impl_.default
cnt: 1, ((T([5000, 1], f32), [T([100], i64)], T([100, 1], f32, stride=(0, 0)), True, True), {})
cnt: 1, ((T([5000, 4], f32), [T([100], i64)], T([100, 4], f32), True, True), {})
```

----------------------------------------

TITLE: Invoking aten.mul.Tensor in PyTorch ATen
DESCRIPTION: Logs calls to the `aten.mul.Tensor` operator, performing element-wise multiplication between two tensors. The examples show operations on 4D tensors (common in convolutional layers) with `f16` data type, including cases potentially involving broadcasting (e.g., multiplying a `[128, 128, 7, 7]` tensor by a `[128, 128, 1, 1]` tensor). The `cnt` indicates the frequency of each call pattern.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/lcnet_050_training.txt#2025-04-22_snippet_2

LANGUAGE: plaintext
CODE:
```
Operator: aten.mul.Tensor
cnt: 2, ((T([128, 128, 7, 7], f16), T([128, 128, 1, 1], f16)), {})
cnt: 2, ((T([128, 256, 7, 7], f16), T([128, 256, 1, 1], f16)), {})
cnt: 1, ((T([128, 256, 7, 7], f16), T([128, 256, 7, 7], f16)), {})
cnt: 1, ((T([128, 128, 7, 7], f16), T([128, 128, 7, 7], f16)), {})
```

----------------------------------------

TITLE: PyTorch Nearest Neighbor Upsampling Operations
DESCRIPTION: Usage of aten.upsample_nearest2d.vec operator showing progressive upsampling of feature maps with specified output sizes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
Operator: aten.upsample_nearest2d.vec
cnt: 1, ((T([4, 256, 37, 38], f16), [74, 76], None), {})
cnt: 1, ((T([4, 256, 74, 76], f16), [148, 152], None), {})
cnt: 1, ((T([4, 256, 148, 152], f16), [296, 304], None), {})
```

----------------------------------------

TITLE: Analyzing PyTorch Batch Normalization Backward Operations
DESCRIPTION: This snippet shows the usage of the aten.native_batch_norm_backward.default operator for computing gradients in batch normalization layers with various shapes and parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
Operator: aten.native_batch_norm_backward.default
cnt: 4, ((T([3, 64, 512, 512], f16), T([3, 64, 512, 512], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})
cnt: 4, ((T([3, 128, 256, 256], f16), T([3, 128, 256, 256], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), False, 1e-05, [True, True, True]), {})
# ... (truncated for brevity)
```

----------------------------------------

TITLE: Batch Normalization Operations in PyTorch
DESCRIPTION: Records of batch normalization operations with various tensor shapes. Each operation includes input tensor, scale, bias, running mean, running variance, training mode flag, momentum, and epsilon parameters for normalization.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/spnasnet_100_training.txt#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
Operator: aten.native_batch_norm.default
cnt: 2, ((T([128, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 48, 112, 112], f16), T([48], f16), T([48], f16), T([48], f16), T([48], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 48, 56, 56], f16), T([48], f16), T([48], f16), T([48], f16), T([48], f16), True, 0.1, 1e-05), {})
cnt: 3, ((T([128, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), True, 0.1, 1e-05), {})
cnt: 4, ((T([128, 72, 56, 56], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 144, 56, 56], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 144, 28, 28], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f16), True, 0.1, 1e-05), {})
cnt: 4, ((T([128, 40, 28, 28], f16), T([40], f16), T([40], f16), T([40], f16), T([40], f16), True, 0.1, 1e-05), {})
cnt: 6, ((T([128, 120, 28, 28], f16), T([120], f16), T([120], f16), T([120], f16), T([120], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 240, 28, 28], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f16), True, 0.1, 1e-05), {})
cnt: 7, ((T([128, 240, 14, 14], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f16), True, 0.1, 1e-05), {})
cnt: 4, ((T([128, 80, 14, 14], f16), T([80], f16), T([80], f16), T([80], f16), T([80], f16), True, 0.1, 1e-05), {})
cnt: 2, ((T([128, 480, 14, 14], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f16), True, 0.1, 1e-05), {})
cnt: 4, ((T([128, 96, 14, 14], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), True, 0.1, 1e-05), {})
cnt: 6, ((T([128, 288, 14, 14], f16), T([288], f16), T([288], f16), T([288], f16), T([288], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 576, 14, 14], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 576, 7, 7], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f16), True, 0.1, 1e-05), {})
cnt: 4, ((T([128, 192, 7, 7], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), True, 0.1, 1e-05), {})
cnt: 8, ((T([128, 1152, 7, 7], f16), T([1152], f16), T([1152], f16), T([1152], f16), T([1152], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 320, 7, 7], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 1280, 7, 7], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f16), True, 0.1, 1e-05), {})
```

----------------------------------------

TITLE: Threshold Backward Operations in PyTorch with Float16 Tensors
DESCRIPTION: Records of threshold backward operations used in the backward pass of ReLU activations with threshold value 0. Each entry shows the gradient input tensor, output tensor (both with the same shape), and the threshold value.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnext50_32x4d_training.txt#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
Operator: aten.threshold_backward.default
cnt: 3, ((T([8, 2048, 7, 7], f16), T([8, 2048, 7, 7], f16), 0), {})
cnt: 5, ((T([8, 1024, 7, 7], f16), T([8, 1024, 7, 7], f16), 0), {})
cnt: 7, ((T([8, 1024, 14, 14], f16), T([8, 1024, 14, 14], f16), 0), {})
cnt: 11, ((T([8, 512, 14, 14], f16), T([8, 512, 14, 14], f16), 0), {})
cnt: 5, ((T([8, 512, 28, 28], f16), T([8, 512, 28, 28], f16), 0), {})
cnt: 7, ((T([8, 256, 28, 28], f16), T([8, 256, 28, 28], f16), 0), {})
cnt: 4, ((T([8, 256, 56, 56], f16), T([8, 256, 56, 56], f16), 0), {})
cnt: 6, ((T([8, 128, 56, 56], f16), T([8, 128, 56, 56], f16), 0), {})
cnt: 1, ((T([8, 64, 112, 112], f16), T([8, 64, 112, 112], f16), 0), {})
```

----------------------------------------

TITLE: Analyzing PyTorch Operator Usage Patterns in Deep Learning Model
DESCRIPTION: This snippet presents a statistical analysis of PyTorch operator calls in a neural network, showing call counts and tensor shapes. The data reveals a pattern typical of a convolutional neural network with progressively decreasing spatial dimensions and increasing channel counts, using half-precision (f16) tensors throughout.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tinynet_a_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([128, 1000], f16), 1, False), {})
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})
Operator: aten.add.Tensor
cnt: 58, ((T([], i64), 1), {})
cnt: 2, ((T([128, 24, 48, 48], f16), T([128, 24, 48, 48], f16)), {})
cnt: 2, ((T([128, 40, 24, 24], f16), T([128, 40, 24, 24], f16)), {})
cnt: 6, ((T([128, 80, 12, 12], f16), T([128, 80, 12, 12], f16)), {})
cnt: 6, ((T([128, 112, 12, 12], f16), T([128, 112, 12, 12], f16)), {})
cnt: 8, ((T([128, 192, 6, 6], f16), T([128, 192, 6, 6], f16)), {})
cnt: 5, ((T([128, 1152, 6, 6], f16), T([128, 1152, 6, 6], f16)), {})
cnt: 1, ((T([128, 672, 6, 6], f16), T([128, 672, 6, 6], f16)), {})
cnt: 3, ((T([128, 672, 12, 12], f16), T([128, 672, 12, 12], f16)), {})
cnt: 4, ((T([128, 480, 12, 12], f16), T([128, 480, 12, 12], f16)), {})
cnt: 1, ((T([128, 240, 12, 12], f16), T([128, 240, 12, 12], f16)), {})
cnt: 1, ((T([128, 240, 24, 24], f16), T([128, 240, 24, 24], f16)), {})
cnt: 1, ((T([128, 144, 24, 24], f16), T([128, 144, 24, 24], f16)), {})
cnt: 1, ((T([128, 144, 48, 48], f16), T([128, 144, 48, 48], f16)), {})
cnt: 1, ((T([128, 96, 48, 48], f16), T([128, 96, 48, 48], f16)), {})
cnt: 1, ((T([128, 32, 96, 96], f16), T([128, 32, 96, 96], f16)), {})
```

----------------------------------------

TITLE: PyTorch Batch Normalization Backward Operations
DESCRIPTION: This snippet shows backward operations for batch normalization on tensors with various shapes in f16 precision. Each operation includes input, gradient, scale, bias, running statistics, and flags for computing gradients.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
Operator: aten.native_batch_norm_backward.default
cnt: 1, ((T([128, 1984, 7, 7], f16), T([128, 1984, 7, 7], f16), T([1984], f16), T([1984], f16), T([1984], f16), T([1984], f32), T([1984], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 352, 7, 7], f16), T([128, 352, 7, 7], f16), T([352], f16), T([352], f16), T([352], f16), T([352], f32), T([352], f32), True, 1e-05, [True, True, True]), {})
cnt: 8, ((T([128, 1104, 7, 7], f16), T([128, 1104, 7, 7], f16), T([1104], f16), T([1104], f16), T([1104], f16), T([1104], f32), T([1104], f32), True, 1e-05, [True, True, True]), {})
cnt: 4, ((T([128, 184, 7, 7], f16), T([128, 184, 7, 7], f16), T([184], f16), T([184], f16), T([184], f16), T([184], f32), T([184], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 672, 7, 7], f16), T([128, 672, 7, 7], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f32), T([672], f32), True, 1e-05, [True, True, True]), {})
cnt: 5, ((T([128, 672, 14, 14], f16), T([128, 672, 14, 14], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f32), T([672], f32), True, 1e-05, [True, True, True]), {})
cnt: 4, ((T([128, 112, 14, 14], f16), T([128, 112, 14, 14], f16), T([112], f16), T([112], f16), T([112], f16), T([112], f32), T([112], f32), True, 1e-05, [True, True, True]), {})
cnt: 2, ((T([128, 336, 14, 14], f16), T([128, 336, 14, 14], f16), T([336], f16), T([336], f16), T([336], f16), T([336], f32), T([336], f32), True, 1e-05, [True, True, True]), {})
cnt: 6, ((T([128, 384, 14, 14], f16), T([128, 384, 14, 14], f16), T([384], f16), T([384], f16), T([384], f16), T([384], f32), T([384], f32), True, 1e-05, [True, True, True]), {})
cnt: 4, ((T([128, 64, 14, 14], f16), T([128, 64, 14, 14], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})
cnt: 3, ((T([128, 192, 14, 14], f16), T([128, 192, 14, 14], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), True, 1e-05, [True, True, True]), {})
cnt: 5, ((T([128, 192, 28, 28], f16), T([128, 192, 28, 28], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), True, 1e-05, [True, True, True]), {})
cnt: 4, ((T([128, 32, 28, 28], f16), T([128, 32, 28, 28], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), True, 1e-05, [True, True, True]), {})
cnt: 2, ((T([128, 96, 28, 28], f16), T([128, 96, 28, 28], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 144, 28, 28], f16), T([128, 144, 28, 28], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f32), T([144], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 144, 56, 56], f16), T([128, 144, 56, 56], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f32), T([144], f32), True, 1e-05, [True, True, True]), {})
cnt: 7, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f32), T([24], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 96, 56, 56], f16), T([128, 96, 56, 56], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 96, 112, 112], f16), T([128, 96, 112, 112], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), True, 1e-05, [True, True, True]), {})
cnt: 4, ((T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f32), T([16], f32), True, 1e-05, [True, True, True]), {})
```

----------------------------------------

TITLE: Profiling ATen Matrix Multiplication and Aggregation Operators in PyTorch (Python)
DESCRIPTION: Details usage of core ATen matrix multiplication (mm) and aggregation (sum, mean) operations as recorded during the model run. Dependencies: PyTorch, tensor inputs with compatible shapes/dtypes. Expects 2D or 1D tensors for multiplication/sum/mean; returns aggregated/multiplied tensors. Observes parameterization for summing/mean over dimensions and optional keepdim/keep parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet18_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
Operator: aten.mm.default
cnt: 1, ((T([16, 1000], f16, stride=(0, 0)), T([1000, 512], f16)), {})
cnt: 1, ((T([1000, 16], f16, stride=(0, 0)), T([16, 512], f16)), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([16, 1000], f16, stride=(0, 0)), [0], True), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.sum.default
cnt: 1, ((T([16, 1000], f16),), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.mean.dim
cnt: 1, ((T([16, 512, 7, 7], f16), [-1, -2], True), {})
```

----------------------------------------

TITLE: Profiling aten.threshold_backward.default Calls - PyTorch - Python
DESCRIPTION: Analyzes backward threshold operations (aten.threshold_backward.default) for various tensor shapes, strides, and value arguments. Requires base and gradient tensors with threshold value, most often used in backpropagation for activation layers like ReLU. Addresses both default and strided tensors as inputs; outputs are gradient tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/selecsls42b_training.txt#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
Operator: aten.threshold_backward.default
cnt: 1, ((T([128, 1024, 4, 4], f16), T([128, 1024, 4, 4], f16), 0), {})
cnt: 1, ((T([128, 1280, 4, 4], f16), T([128, 1280, 4, 4], f16), 0), {})
cnt: 1, ((T([128, 1024, 7, 7], f16), T([128, 1024, 7, 7], f16), 0), {})
cnt: 1, ((T([128, 960, 7, 7], f16), T([128, 960, 7, 7], f16), 0), {})
cnt: 1, ((T([128, 480, 14, 14], f16), T([128, 480, 14, 14], f16), 0), {})
cnt: 1, ((T([128, 152, 14, 14], f16, stride=(178752, 196, 14, 1)), T([128, 152, 14, 14], f16), 0), {})
cnt: 7, ((T([128, 304, 14, 14], f16), T([128, 304, 14, 14], f16), 0), {})
cnt: 2, ((T([128, 152, 14, 14], f16), T([128, 152, 14, 14], f16), 0), {})
cnt: 1, ((T([128, 152, 14, 14], f16, stride=(119168, 196, 14, 1)), T([128, 152, 14, 14], f16), 0), {})
cnt: 1, ((T([128, 288, 28, 28], f16), T([128, 288, 28, 28], f16), 0), {})
cnt: 1, ((T([128, 72, 28, 28], f16, stride=(338688, 784, 28, 1)), T([128, 72, 28, 28], f16), 0), {})
cnt: 7, ((T([128, 144, 28, 28], f16), T([128, 144, 28, 28], f16), 0), {})
cnt: 2, ((T([128, 72, 28, 28], f16), T([128, 72, 28, 28], f16), 0), {})
cnt: 1, ((T([128, 72, 28, 28], f16, stride=(225792, 784, 28, 1)), T([128, 72, 28, 28], f16), 0), {})
cnt: 1, ((T([128, 128, 56, 56], f16), T([128, 128, 56, 56], f16), 0), {})
cnt: 1, ((T([128, 32, 56, 56], f16, stride=(602112, 3136, 56, 1)), T([128, 32, 56, 56], f16), 0), {})
cnt: 7, ((T([128, 64, 56, 56], f16), T([128, 64, 56, 56], f16), 0), {})
cnt: 2, ((T([128, 32, 56, 56], f16), T([128, 32, 56, 56], f16), 0), {})
cnt: 1, ((T([128, 32, 56, 56], f16, stride=(401408, 3136, 56, 1)), T([128, 32, 56, 56], f16), 0), {})
cnt: 1, ((T([128, 32, 112, 112], f16), T([128, 32, 112, 112], f16), 0), {})
```

----------------------------------------

TITLE: PyTorch Tensor Convolution Operations
DESCRIPTION: Series of convolution operations using PyTorch tensors with half precision (f16). Operations include forward and backward passes with varying input shapes, kernel sizes, strides, and padding configurations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dpn107_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 2048, 14, 14], f16), T([800, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 2112, 14, 14], f16), T([800, 2112, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

----------------------------------------

TITLE: Configuring QNNPACK Benchmark Executables in CMake
DESCRIPTION: Sets up multiple benchmark executables for QNNPACK operations, including compiler flags, include directories, and linking libraries. This configuration is applied when PYTORCH_QNNPACK_BUILD_BENCHMARKS is enabled.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_17

LANGUAGE: CMake
CODE:
```
add_executable(add-bench bench/add.cc)
set_target_properties(add-bench PROPERTIES
  CXX_STANDARD 14
  CXX_STANDARD_REQUIRED YES
  CXX_EXTENSIONS NO)
target_link_libraries(add-bench PRIVATE pytorch_qnnpack benchmark)
```

----------------------------------------

TITLE: Sum Along Dimensions in PyTorch
DESCRIPTION: Using aten.sum.SymInt, sums elements along dimension 0 of tensor shaped [512, 2], the operation allows selective aggregation along desired axes, typically for loss calculations or reductions in complex models.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_15

LANGUAGE: Python
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([512, 2], f16), [0], True), {})
```

----------------------------------------

TITLE: Configuring CUDA Support for PyTorch Python
DESCRIPTION: Sets up CUDA support for PyTorch Python bindings, including dependencies like cuDNN, cuSPARSELt and NVToolsExt when available.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_7

LANGUAGE: CMake
CODE:
```
if(USE_CUDA)
    include(${TORCH_ROOT}/cmake/public/cuda.cmake)
    append_filelist("libtorch_python_cuda_core_sources" TORCH_PYTHON_SRCS)
    list(APPEND TORCH_PYTHON_SRCS ${GENERATED_THNN_CXX_CUDA})

    list(APPEND TORCH_PYTHON_COMPILE_DEFINITIONS USE_CUDA)
    if(USE_CUDNN)
        list(APPEND TORCH_PYTHON_LINK_LIBRARIES torch::cudnn)
        list(APPEND TORCH_PYTHON_COMPILE_DEFINITIONS USE_CUDNN)
    endif()
    if(USE_CUSPARSELT)
        list(APPEND TORCH_PYTHON_LINK_LIBRARIES torch::cusparselt)
        list(APPEND TORCH_PYTHON_COMPILE_DEFINITIONS USE_CUSPARSELT)
    endif()
    if(USE_CUFILE)
        list(APPEND TORCH_PYTHON_LINK_LIBRARIES torch::cufile)
        list(APPEND TORCH_PYTHON_COMPILE_DEFINITIONS USE_CUFILE)
    endif()

    if(TARGET torch::nvtx3)
      list(APPEND TORCH_PYTHON_LINK_LIBRARIES torch::nvtx3)
    else()
      if(TARGET torch::nvtoolsext)
        list(APPEND TORCH_PYTHON_LINK_LIBRARIES torch::nvtoolsext)
      endif()
    endif()
endif()
```

----------------------------------------

TITLE: Wirtinger Derivative Chain Rule
DESCRIPTION: Chain rule expressions relating partial derivatives with respect to real and imaginary components to Wirtinger derivatives.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/autograd.rst#2025-04-22_snippet_5

LANGUAGE: math
CODE:
```
\begin{aligned}
    \frac{\partial }{\partial z} &= 1/2 * \left(\frac{\partial }{\partial x} - 1j * \frac{\partial }{\partial y}\right)   \\
    \frac{\partial }{\partial z^*} &= 1/2 * \left(\frac{\partial }{\partial x} + 1j * \frac{\partial }{\partial y}\right)
\end{aligned}
```

----------------------------------------

TITLE: Building and Benchmarking Caffe2/QNNPACK on Raspberry Pi/Host - Bash
DESCRIPTION: Provides instructions for cloning the PyTorch repository (which includes QNNPACK as a submodule), updating the QNNPACK submodule, building Caffe2 with binaries for the host system (like a Raspberry Pi) while limiting build jobs to save memory, downloading a pre-trained quantized MobileNet v2 model, and running the Caffe2 `speed_benchmark` tool.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/README.md#_snippet_0

LANGUAGE: Bash
CODE:
```
# Clone PyTorch 1.0 repo
git clone --recursive https://github.com/pytorch/pytorch.git
cd pytorch

# Optional: update QNNPACK submodule to latest revision
git submodule update --remote third_party/QNNPACK

# Build Caffe2 (including binaries) for the host system
# Use only 1 thread for build to avoid out-of-memory failures
MAX_JOBS=1 scripts/build_local.sh -DBUILD_BINARY=ON -DBUILD_PYTHON=OFF \
    -DUSE_OBSERVERS=OFF -DUSE_DISTRIBUTED=OFF

# Download model weights
wget https://s3.amazonaws.com/download.caffe2.ai/models/mobilenet_v2_1.0_224_quant/init_net.pb  # @lint-ignore

# Download model graph
wget https://s3.amazonaws.com/download.caffe2.ai/models/mobilenet_v2_1.0_224_quant/predict_net.pb  # @lint-ignore

# Run speed benchmark with 50 warm-up iterations and 10 measurement iterations
build/bin/speed_benchmark --net predict_net.pb --init_net init_net.pb \
    --input data --input_dims 1,3,224,224 --input_type float \
    --warmup 50 --iter 10
```

----------------------------------------

TITLE: Installing py-spy for PyTorch Benchmarking
DESCRIPTION: Command to install py-spy, a sampling profiler for Python programs, which is required for generating flame graphs in the benchmarking process.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/overrides_benchmark/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip install py-spy
```

----------------------------------------

TITLE: Using experimental context_parallel feature
DESCRIPTION: Applies context-based parallelism to a module using the experimental context_parallel function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.tensor.rst#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
context_parallel(module, device_mesh, parallel_mode="tensor", sharding_strategy=None)
```

----------------------------------------

TITLE: Batch Normalization in PyTorch
DESCRIPTION: Batch normalization operations with input tensors, running mean and variance, and learnable parameters. These are used to normalize activations and improve training stability.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([128, 24, 128, 128], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), True, 0.1, 1e-05), {})
```

----------------------------------------

TITLE: Evaluating Forward Time for Sparsified DLRM Models in Python
DESCRIPTION: Python script to evaluate forward pass execution time for sparsified DLRM models, comparing performance with and without torch.sparse tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/benchmarks/README.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
python evaluate_forward_time.py --raw-data-file=<path_to_raw_data_txt_file> --processed-data-file=<path_to_kaggleAdDisplayChallenge_processed.npz> --sparse-model-metadata=<path_to_sparse_model_metadata_csv>
```

----------------------------------------

TITLE: Implementing vmap for Custom PyTorch Autograd Function
DESCRIPTION: This snippet shows the implementation of vmap support for a custom PyTorch autograd Function. It handles expanding batched dimensions and applies the NumpyTake operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.func.rst#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
return x.expand(info.batch_size, *x.shape)
return x.movedim(x_bdim, 0)

# If the Tensor doesn't have the dimension being vmapped over,
# expand it out. Otherwise, move it to the front of the Tensor
x = maybe_expand_bdim_at_front(x, x_bdim)
ind = maybe_expand_bdim_at_front(ind, ind_bdim)
ind_inv = maybe_expand_bdim_at_front(ind_inv, ind_inv_bdim)

# The return is a tuple (output, out_dims). Since output is a Tensor,
# then out_dims is an Optional[int] (instead of being a Tuple).
return NumpyTake.apply(x, ind, ind_inv, dim + 1), 0
```

----------------------------------------

TITLE: Implementing numpy_sort Function with PyTorch vmap
DESCRIPTION: This snippet defines a numpy_sort function that uses a custom NumpySort operation and demonstrates its usage with torch.vmap for vectorized operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.func.rst#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
def numpy_sort(x, dim=-1):
    result, _, _ = NumpySort.apply(x, dim)
    return result

x = torch.randn(2, 3)
result = torch.vmap(numpy_sort)(x)
assert torch.allclose(result, numpy_sort(result, 1))
```

----------------------------------------

TITLE: Batch Normalization Operations
DESCRIPTION: Forward and backward batch normalization operations with running statistics and gradient computation for different tensor shapes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/maml_omniglot_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
aten.native_batch_norm.default((T([5, 64, 26, 26], f16, stride=(43264, 1, 1664, 64)), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 1.0, 1e-05))
aten.native_batch_norm_backward.default((T([5, 64, 26, 26], f16, stride=(43264, 1, 1664, 64)), T([5, 64, 26, 26], f16, stride=(43264, 1, 1664, 64)), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]))
```

----------------------------------------

TITLE: PyTorch Batch Normalization Forward Operations
DESCRIPTION: Batch normalization forward pass operations with half-precision tensors, using momentum 0.1 and epsilon 1e-05. Each operation processes tensors of different shapes with their corresponding running statistics.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hardcorenas_a_training.txt#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
((T([128, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})
```

----------------------------------------

TITLE: PyTorch Model Batch Size Configuration List
DESCRIPTION: A comprehensive listing of deep learning models with their recommended batch sizes. The configuration includes various model architectures such as inception, transformer-based models (ViT, Swin), convolutional networks (ResNet variants), and hybrid models with batch sizes ranging from 4 to 1024 depending on model complexity and memory requirements.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/timm_models_list.txt#2025-04-22_snippet_0

LANGUAGE: plaintext
CODE:
```
adv_inception_v3 128
beit_base_patch16_224 128
botnet26t_256 128
cait_m36_384 4
coat_lite_mini 128
convit_base 128
convmixer_768_32 64
convnext_base 128
crossvit_9_240 256
cspdarknet53 128
deit_base_distilled_patch16_224 128
dla102 128
dm_nfnet_f0 128
dpn107 64
eca_botnext26ts_256 128
eca_halonext26ts 128
ese_vovnet19b_dw 256
fbnetc_100 512
fbnetv3_b 256
gernet_l 128
ghostnet_100 512
gluon_inception_v3 256
gmixer_24_224 128
gmlp_s16_224 128
hrnet_w18 128
inception_v3 128
jx_nest_base 128
lcnet_050 256
levit_128 1024
mixer_b16_224 128
mixnet_l 128
mnasnet_100 512
mobilenetv2_100 128
mobilenetv3_large_100 512
mobilevit_s 128
nfnet_l0 128
pit_b_224 64
pnasnet5large 32
poolformer_m36 128
regnety_002 1024
repvgg_a2 128
res2net101_26w_4s 128
res2net50_14w_8s 128
res2next50 128
resmlp_12_224 128
resnest101e 128
rexnet_100 256
sebotnet33ts_256 64
selecsls42b 128
spnasnet_100 128
swin_base_patch4_window7_224 128
swsl_resnext101_32x16d 64
tf_efficientnet_b0 128
tf_mixnet_l 128
tinynet_a 128
tnt_s_patch16_224 128
twins_pcpvt_base 128
visformer_small 128
vit_base_patch16_224 128
volo_d1_224 128
xcit_large_24_p8_224 16
```

----------------------------------------

TITLE: Feature-based Mask Computation in PyTorch
DESCRIPTION: Shows how masks are computed when working with specific features or channels in the activation tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/activation_sparsifier/README.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
# when features = None, mask is a tensor computed on the entire activation tensor
# otherwise, mask is a list of tensors of length = len(features), computed on each feature of activations

# On a high level, this is how the mask is computed if features is not None
for i in range(len(features)):
   aggregated_tensor_feature = aggregate_fn([activation[features[i]] for activation in all_activations])
   mask[i] = mask_fn(reduce_fn(aggregated_tensor_feature))
```

----------------------------------------

TITLE: Analyzing PyTorch Operator Usage in Transformer Model
DESCRIPTION: This code snippet represents a summary of PyTorch operator usage in a deep learning model. It includes tensor operations, loss calculations, and various tensor manipulations typical in transformer architectures. The snippet shows the operator name, count of usage, and the input parameters for each operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BigBird_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
cnt: 26, ((T([1, 1024, 768], f16), T([1, 1024, 768], f16), [768], T([1, 1024, 1], f32), T([1, 1024, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})
Operator: aten.new_empty_strided.default
cnt: 36, ((T([144, 64, 64], f16), [144, 64, 64], [4096, 64, 1]), {})
Operator: aten.new_ones.default
cnt: 24, ((T([1, 1, 1, 1024], f16), [1, 1, 1, 192]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda', 'pin_memory': False})
cnt: 24, ((T([1, 12, 14, 64, 192], f32), [1, 12, 64, 256]), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda', 'pin_memory': False})
Operator: aten.new_zeros.default
cnt: 12, ((T([12, 12, 64, 64], f16, stride=(64, 49152, 768, 1)), [589824]), {})
cnt: 24, ((T([504, 64, 64], f16), [192, 64, 64]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})
Operator: aten.nll_loss_backward.default
cnt: 1, ((T([], f16), T([1024, 50358], f16), T([1024], i64), None, 1, -100, T([], f16)), {})
Operator: aten.nll_loss_forward.default
cnt: 1, ((T([1024, 50358], f16), T([1024], i64), None, 1, -100), {})
Operator: aten.pow.Tensor_Scalar
cnt: 12, ((T([1, 1024, 3072], f16), 3.0), {})
cnt: 1, ((T([1, 1024, 768], f16), 3.0), {})
cnt: 1, ((T([1, 1024, 768], f16), 2.0), {})
cnt: 12, ((T([1, 1024, 3072], f16), 2.0), {})
Operator: aten.rsub.Scalar
cnt: 24, ((T([1, 1, 1, 1024], f16), 1.0), {})
cnt: 24, ((T([1, 12, 64, 448], f32), 1.0), {})
cnt: 12, ((T([1, 1, 12, 64, 192], f16), 1.0), {})
cnt: 24, ((T([1, 1, 1, 1, 64], f16), 1.0), {})
cnt: 12, ((T([1, 12, 12, 64, 192], f32, stride=(2064384, 172032, 12288, 192, 1)), 1.0), {})
Operator: aten.select_backward.default
cnt: 24, ((T([1, 12, 64, 64], f16), [1, 12, 16, 64, 64], 2, -1), {})
cnt: 12, ((T([1, 12, 64, 64], f16), [1, 12, 16, 64, 64], 2, -2), {})
cnt: 12, ((T([1, 12, 192, 64], f16, stride=(344064, 28672, 64, 1)), [1, 12, 14, 192, 64], 2, -1), {})
cnt: 24, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [1, 12, 16, 64, 64], 2, -1), {})
cnt: 12, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [1, 12, 16, 64, 64], 2, -2), {})
cnt: 12, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [1, 12, 16, 64, 64], 2, -3), {})
cnt: 24, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [1, 12, 16, 64, 64], 2, 0), {})
cnt: 12, ((T([1, 12, 192, 64], f16, stride=(344064, 28672, 1, 448)), [1, 12, 14, 192, 64], 2, -1), {})
cnt: 24, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [1, 12, 16, 64, 64], 2, -1), {})
cnt: 12, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [1, 12, 16, 64, 64], 2, -2), {})
cnt: 12, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [1, 12, 16, 64, 64], 2, -3), {})
cnt: 24, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [1, 12, 16, 64, 64], 2, 0), {})
cnt: 24, ((T([1, 12, 64, 64], f16), [1, 12, 16, 64, 64], 2, 0), {})
cnt: 12, ((T([1, 12, 64, 64], f16, stride=(64, 4096, 1, 64)), [1, 12, 16, 64, 64], 2, -1), {})
cnt: 12, ((T([1, 12, 64, 64], f16, stride=(64, 4096, 1, 64)), [1, 12, 16, 64, 64], 2, 0), {})
cnt: 12, ((T([1, 12, 64, 64], f16), [1, 12, 16, 64, 64], 2, 1), {})
cnt: 12, ((T([1, 12, 192, 64], f16, stride=(344064, 28672, 64, 1)), [1, 12, 14, 192, 64], 2, 0), {})
cnt: 12, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [1, 12, 16, 64, 64], 2, 2), {})
cnt: 12, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [1, 12, 16, 64, 64], 2, 1), {})
cnt: 12, ((T([1, 12, 192, 64], f16, stride=(344064, 28672, 1, 448)), [1, 12, 14, 192, 64], 2, 0), {})
cnt: 12, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [1, 12, 16, 64, 64], 2, 2), {})
cnt: 12, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [1, 12, 16, 64, 64], 2, 1), {})
Operator: aten.slice_backward.default
cnt: 372, ((T([1, 12, 16, 64, 64], f16), [1, 12, 16, 64, 64], 1, 0, 9223372036854775807, 1), {})
cnt: 372, ((T([1, 12, 16, 64, 64], f16), [1, 12, 16, 64, 64], 0, 0, 9223372036854775807, 1), {})
cnt: 72, ((T([1, 12, 14, 192, 64], f16), [1, 12, 14, 192, 64], 1, 0, 9223372036854775807, 1), {})
cnt: 72, ((T([1, 12, 14, 192, 64], f16), [1, 12, 14, 192, 64], 0, 0, 9223372036854775807, 1), {})
cnt: 12, ((T([1, 12, 12, 64, 64], f16), [1, 12, 12, 64, 512], 4, -64, 9223372036854775807, 1), {})
cnt: 48, ((T([1, 12, 12, 64, 512], f16), [1, 12, 12, 64, 512], 3, 0, 9223372036854775807, 1), {})
cnt: 48, ((T([1, 12, 12, 64, 512], f16), [1, 12, 12, 64, 512], 2, 0, 9223372036854775807, 1), {})
cnt: 48, ((T([1, 12, 12, 64, 512], f16), [1, 12, 12, 64, 512], 1, 0, 9223372036854775807, 1), {})
cnt: 48, ((T([1, 12, 12, 64, 512], f16), [1, 12, 12, 64, 512], 0, 0, 9223372036854775807, 1), {})
cnt: 12, ((T([1, 12, 12, 64, 64], f16), [1, 12, 12, 64, 512], 4, 0, 64, 1), {})
cnt: 12, ((T([1, 12, 12, 192, 64], f16), [1, 12, 14, 192, 64], 2, 1, -1, 1), {})
cnt: 12, ((T([1, 12, 12, 64, 192], f16), [1, 12, 12, 64, 512], 4, 256, -64, 1), {})
cnt: 12, ((T([1, 12, 12, 64, 192], f16), [1, 12, 12, 64, 512], 4, 64, 256, 1), {})
cnt: 12, ((T([1, 12, 12, 192, 64], f16, stride=(1769472, 147456, 12288, 1, 192)), [1, 12, 14, 192, 64], 2, 1, -1, 1), {})
cnt: 12, ((T([1, 12, 12, 64, 64], f16), [1, 12, 16, 64, 64], 2, 2, -2, 1), {})
cnt: 12, ((T([1, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 64, 1)), [1, 12, 16, 64, 64], 2, 3, -1, 1), {})
cnt: 12, ((T([1, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 64, 1)), [1, 12, 16, 64, 64], 2, 2, -2, 1), {})
cnt: 12, ((T([1, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 64, 1)), [1, 12, 16, 64, 64], 2, 1, -3, 1), {})
cnt: 12, ((T([1, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 1, 192)), [1, 12, 16, 64, 64], 2, 3, -1, 1), {})
cnt: 12, ((T([1, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 1, 192)), [1, 12, 16, 64, 64], 2, 2, -2, 1), {})
cnt: 12, ((T([1, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 1, 192)), [1, 12, 16, 64, 64], 2, 1, -3, 1), {})
Operator: aten.stack.default
cnt: 12, (([T([504, 64], f32)],), {})
Operator: aten.sum.SymInt
cnt: 1, ((T([1024, 50358], f16), [0], True), {})
cnt: 49, ((T([1024, 768], f16), [0], True), {})
cnt: 12, ((T([1024, 3072], f16), [0], True), {})
cnt: 12, ((T([1024, 768], f16, stride=(1, 1024)), [0], True), {})
Operator: aten.tanh.default
cnt: 12, ((T([1, 1024, 3072], f16),), {})
cnt: 1, ((T([1, 768], f16),), {})
cnt: 1, ((T([1, 1024, 768], f16),), {})
Operator: aten.tanh_backward.default
cnt: 1, ((T([1, 1024, 768], f16), T([1, 1024, 768], f16)), {})
cnt: 12, ((T([1, 1024, 3072], f16), T([1, 1024, 3072], f16)), {})
Operator: aten.unbind.int
cnt: 12, ((T([1, 16, 64], f32),), {})
cnt: 12, ((T([1, 12, 14, 3], i64),), {})
Operator: aten.unsqueeze_.default
cnt: 1, ((T([1, 12, 64, 192], f32), 1), {})
cnt: 12, ((T([12, 14, 3], i64), 0), {})
cnt: 48, ((T([1, 12, 64, 64], f16), 2), {})
```

----------------------------------------

TITLE: PyTorch Sigmoid Operations
DESCRIPTION: Sigmoid activation and its backward pass on tensors with varying channel dimensions (64, 128, 256)
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_botnext26ts_256_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
((T([128, 1, 64], f16),), {})
```

----------------------------------------

TITLE: Clearing cuFFT Plan Cache in PyTorch CUDA Backend
DESCRIPTION: This code snippet shows how to clear the cuFFT plan cache using the clear() method.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/backends.rst#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
clear()
```

----------------------------------------

TITLE: Calling aten._log_softmax_backward_data.default (Python)
DESCRIPTION: Computes the gradient of the log-softmax operation with respect to the input data. It requires the gradient from the subsequent layer and the output of the log-softmax forward pass, along with the dimension and data type.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt#_snippet_1

LANGUAGE: Python
CODE:
```
((T([1024, 50265], f16), T([1024, 50265], f16), 1, f16), {})
```

----------------------------------------

TITLE: Computing SQNR using torch.ao.ns.fx.utils in Python
DESCRIPTION: This code snippet demonstrates the function signature for computing Signal-to-Quantization-Noise Ratio (SQNR) between two tensors x and y using the torch.ao.ns.fx.utils module.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.ao.ns._numeric_suite_fx.rst#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
torch.ao.ns.fx.utils.compute_sqnr(x, y)
```

----------------------------------------

TITLE: CUDA Operations in C++
DESCRIPTION: Function declarations for CUDA-related operations, including device properties, memory allocation, and CUBLAS functions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_9

LANGUAGE: C++
CODE:
```
_ZN2at4cuda14get_p2p_accessEii
_ZN2at4cuda19getDevicePropertiesEl
_ZN2at4cuda22getCUDADeviceAllocatorEv
_ZN2at4cuda26getCurrentDevicePropertiesEv
_ZN2at4cuda6detail12_GLOBAL__N_118_hasPrimaryContextEa
_ZN2at4cuda6detail5nvrtcEv
_ZN2at4cuda9warp_sizeEv
_ZN2at6detail10empty_cudaEN3c108ArrayRefIlEENS1_10ScalarTypeESt8optionalINS1_6DeviceEES5_INS1_12MemoryFormatEE
_ZN2at6detail10empty_cudaEN3c108ArrayRefIlEERKNS1_13TensorOptionsE
_ZN2at6detail10empty_cudaEN3c108ArrayRefIlEESt8optionalINS1_10ScalarTypeEES4_INS1_6LayoutEES4_INS1_6DeviceEES4_IbES4_INS1_12MemoryFormatEE
```

----------------------------------------

TITLE: Invoking aten.native_batch_norm_backward.default in PyTorch ATen
DESCRIPTION: Logs calls to the `aten.native_batch_norm_backward.default` operator, computing gradients for batch normalization. Arguments include the gradient w.r.t the output (4D, f16), the original input tensor (4D, f16), weight (1D, f16), running mean (1D, f16), running variance (1D, f16), saved mean (1D, f32), saved inverse std deviation (1D, f32), training mode (`True`), epsilon (`1e-05`), and a list of booleans indicating which inputs require gradients (`[True, True, True]`). The `cnt` indicates call frequency.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/lcnet_050_training.txt#2025-04-22_snippet_4

LANGUAGE: plaintext
CODE:
```
Operator: aten.native_batch_norm_backward.default
cnt: 3, ((T([128, 256, 7, 7], f16), T([128, 256, 7, 7], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 128, 7, 7], f16), T([128, 128, 7, 7], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})
cnt: 11, ((T([128, 128, 14, 14], f16), T([128, 128, 14, 14], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 64, 14, 14], f16), T([128, 64, 14, 14], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})
cnt: 3, ((T([128, 64, 28, 28], f16), T([128, 64, 28, 28], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 32, 28, 28], f16), T([128, 32, 28, 28], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), True, 1e-05, [True, True, True]), {})
cnt: 3, ((T([128, 32, 56, 56], f16), T([128, 32, 56, 56], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 16, 56, 56], f16), T([128, 16, 56, 56], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f32), T([16], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f32), T([16], f32), True, 1e-05, [True, True, True]), {})
cnt: 2, ((T([128, 8, 112, 112], f16), T([128, 8, 112, 112], f16), T([8], f16), T([8], f16), T([8], f16), T([8], f32), T([8], f32), True, 1e-05, [True, True, True]), {})
```

----------------------------------------

TITLE: Disabling Vmap Fallback Warning in Functorch (Python)
DESCRIPTION: This Python code snippet demonstrates how to disable the verbose vmap fallback warnings in PyTorch's functorch module. Calling `torch._C._functorch._set_vmap_fallback_warning_enabled(False)` suppresses these warnings, which can be helpful if they become excessive during development or debugging.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/README.md#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
`torch._C._functorch._set_vmap_fallback_warning_enabled(False)`
```

----------------------------------------

TITLE: Configuring C++ Standard and Build Verbosity in CMake
DESCRIPTION: Includes standard installation directories, sets the required C++ standard to C++17, enables verbose output during the make process, and displays the status of the `ANDROID_STL` variable.
SOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: cmake
CODE:
```
include(GNUInstallDirs)

set(CMAKE_CXX_STANDARD 17 CACHE STRING "The C++ standard whose features are requested to build this target.")
set(CMAKE_VERBOSE_MAKEFILE ON)
message(STATUS "ANDROID_STL:${ANDROID_STL}")
```

----------------------------------------

TITLE: Example Function for Suffix Truncation
DESCRIPTION: Sample function demonstrating how suffix truncation works by removing operations from the end of the graph while preserving the failure condition.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/minifier.ipynb#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
def f(a):
    b = x * 2
    c = b + 3
    d = c / 4
    return d
```

LANGUAGE: python
CODE:
```
def f(a):
    b = x * 2
    c = b + 3
    return c
```

----------------------------------------

TITLE: Distinguishing Between Packaged and Non-Packaged Code
DESCRIPTION: Demonstrates how to use torch.package.is_from_package() to check if an object's code is from a torch.package, with exceptions for extern modules and stdlib.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
importer = PackageImporter(f)
mod = importer.import_module('foo')
obj = importer.load_pickle('model', 'model.pkl')
txt = importer.load_text('text', 'my_test.txt')

assert is_from_package(mod)
assert is_from_package(obj)
assert not is_from_package(txt) # str is from stdlib, so this will return False
```

----------------------------------------

TITLE: Documenting CallgrindStats Class in PyTorch Benchmark Utils
DESCRIPTION: This snippet documents the CallgrindStats class from the torch.utils.benchmark module, including all its members.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/benchmark_utils.rst#2025-04-22_snippet_4

LANGUAGE: reStructuredText
CODE:
```
.. autoclass:: CallgrindStats
    :members:
```

----------------------------------------

TITLE: Configuring QNNPACK Build Targets with CMake - CMake
DESCRIPTION: This CMake script sets up the QNNPACK library build process, organizing source files into groups by functionality and architecture, setting compile flags according to target processor, and specifying how to build (static, shared, or default) the library. It ensures correct linking with required dependencies such as clog, cpuinfo, pthreadpool, and fxdiv, and adds appropriate directories for header inclusion. The script uses CMake conditionals to adjust build configurations per platform and manages dependency subdirectory integration or system library detection.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_3

LANGUAGE: cmake
CODE:
```
# ---[ QNNPACK library
set(PYTORCH_QNNPACK_INIT_SRCS
  src/init.c
  src/add.c
  src/average-pooling.c
  src/channel-shuffle.c
  src/clamp.c
  src/conv-prepack.cc
  src/convolution.c
  src/deconvolution.c
  src/fc-prepack.cc
  src/fully-connected.c
  src/fully-connected-sparse.c
  src/global-average-pooling.c
  src/hardsigmoid.c
  src/hardswish.c
  src/leaky-relu.c
  src/max-pooling.c
  src/sigmoid.c
  src/softargmax.c
  src/tanh.c
  src/operator-delete.c)

set(PYTORCH_QNNPACK_EXEC_SRCS
  src/conv-run.cc
  src/deconv-run.cc
  src/fc-run.cc
  src/fc-unpack.cc
  src/fc-dynamic-run.cc
  src/indirection.c
  src/operator-run.c)

set(PYTORCH_QNNPACK_SCALAR_UKERNELS
  src/u8lut32norm/scalar.c
  src/x8lut/scalar.c)

set(PYTORCH_QNNPACK_PSIMD_UKERNELS
  src/sgemm/6x8-psimd.c)

set(PYTORCH_QNNPACK_ARM_NEON_UKERNELS
  src/q8avgpool/mp8x9p8q-neon.c
  src/q8avgpool/up8x9-neon.c
  src/q8avgpool/up8xm-neon.c
  src/q8conv/4x8-neon.c
  src/q8conv/8x8-neon.c
  src/q8dwconv/mp8x25-neon.c
  src/q8dwconv/mp8x25-neon-per-channel.c
  src/q8dwconv/mp8x27-neon.c
  src/q8dwconv/up8x9-neon.c
  src/q8dwconv/up8x9-neon-per-channel.c
  src/q8gavgpool/mp8x7p7q-neon.c
  src/q8gavgpool/up8x7-neon.c
  src/q8gavgpool/up8xm-neon.c
  src/q8gemm/4x-sumrows-neon.c
  src/q8gemm/4x8-neon.c
  src/q8gemm/4x8-dq-neon.c
  src/q8gemm/4x8c2-xzp-neon.c
  src/q8gemm/6x4-neon.c
  src/q8gemm/8x8-neon.c
  src/q8vadd/neon.c
  src/sgemm/5x8-neon.c
  src/sgemm/6x8-neon.c
  src/u8clamp/neon.c
  src/u8maxpool/16x9p8q-neon.c
  src/u8maxpool/sub16-neon.c
  src/u8rmax/neon.c
  src/x8zip/x2-neon.c
  src/x8zip/x3-neon.c
  src/x8zip/x4-neon.c
  src/x8zip/xm-neon.c)

set(PYTORCH_QNNPACK_AARCH32_ASM_UKERNELS
  src/hgemm/8x8-aarch32-neonfp16arith.S
  src/q8conv/4x8-aarch32-neon.S
  src/q8dwconv/up8x9-aarch32-neon.S
  src/q8dwconv/up8x9-aarch32-neon-per-channel.S
  src/q8gemm/4x8-aarch32-neon.S
  src/q8gemm/4x8-dq-aarch32-neon.S
  src/q8gemm/4x8c2-xzp-aarch32-neon.S
  src/q8gemm_sparse/4x4-packA-aarch32-neon.S
  src/q8gemm_sparse/4x8c1x4-dq-packedA-aarch32-neon.S
  src/q8gemm_sparse/4x8c8x1-dq-packedA-aarch32-neon.S)

set(PYTORCH_QNNPACK_AARCH64_ASM_UKERNELS
  src/q8conv/8x8-aarch64-neon.S
  src/q8gemm/8x8-aarch64-neon.S
  src/q8gemm/8x8-dq-aarch64-neon.S
  src/q8gemm_sparse/8x4-packA-aarch64-neon.S
  src/q8gemm_sparse/8x8c1x4-dq-packedA-aarch64-neon.S
  src/q8gemm_sparse/8x8c8x1-dq-packedA-aarch64-neon.S)

set(PYTORCH_QNNPACK_X86_SSE2_UKERNELS
  src/q8avgpool/mp8x9p8q-sse2.c
  src/q8avgpool/up8x9-sse2.c
  src/q8avgpool/up8xm-sse2.c
  src/q8conv/4x4c2-sse2.c
  src/q8dwconv/mp8x25-sse2.c
  src/q8dwconv/mp8x25-sse2-per-channel.c
  src/q8dwconv/mp8x27-sse2.c
  src/q8dwconv/up8x9-sse2.c
  src/q8dwconv/up8x9-sse2-per-channel.c
  src/q8gavgpool/mp8x7p7q-sse2.c
  src/q8gavgpool/up8x7-sse2.c
  src/q8gavgpool/up8xm-sse2.c
  src/q8gemm/2x4c8-sse2.c
  src/q8gemm/4x4c2-dq-sse2.c
  src/q8gemm/4x4c2-sse2.c
  src/q8gemm_sparse/8x4c1x4-dq-packedA-sse2.c
  src/q8gemm_sparse/8x4-packA-sse2.c
  src/q8vadd/sse2.c
  src/u8clamp/sse2.c
  src/u8maxpool/16x9p8q-sse2.c
  src/u8maxpool/sub16-sse2.c
  src/u8rmax/sse2.c
  src/x8zip/x2-sse2.c
  src/x8zip/x3-sse2.c
  src/x8zip/x4-sse2.c
  src/x8zip/xm-sse2.c)

set(PYTORCH_QNNPACK_UKERNELS ${PYTORCH_QNNPACK_SCALAR_UKERNELS} ${PYTORCH_QNNPACK_PSIMD_UKERNELS})
if(CMAKE_SYSTEM_PROCESSOR MATCHES "^armv[5-8]" OR IOS_ARCH MATCHES "^armv7")
  list(APPEND PYTORCH_QNNPACK_UKERNELS ${PYTORCH_QNNPACK_ARM_NEON_UKERNELS})
  list(APPEND PYTORCH_QNNPACK_UKERNELS ${PYTORCH_QNNPACK_AARCH32_ASM_UKERNELS})
endif()
if(PYTORCH_QNNPACK_TARGET_PROCESSOR MATCHES "^(aarch64|arm64)$" OR IOS_ARCH MATCHES "^arm64.*")
  list(APPEND PYTORCH_QNNPACK_UKERNELS ${PYTORCH_QNNPACK_ARM_NEON_UKERNELS})
  list(APPEND PYTORCH_QNNPACK_UKERNELS ${PYTORCH_QNNPACK_AARCH64_ASM_UKERNELS})
endif()
if(PYTORCH_QNNPACK_TARGET_PROCESSOR MATCHES "^(i[3-6]86|x86_64)$" OR IOS_ARCH MATCHES "^(i386|x86_64)$")
  list(APPEND PYTORCH_QNNPACK_UKERNELS ${PYTORCH_QNNPACK_X86_SSE2_UKERNELS})
endif()

if(PYTORCH_QNNPACK_LIBRARY_TYPE STREQUAL "default")
  add_library(pytorch_qnnpack ${PYTORCH_QNNPACK_INIT_SRCS} ${PYTORCH_QNNPACK_EXEC_SRCS} ${PYTORCH_QNNPACK_UKERNELS})
elseif(PYTORCH_QNNPACK_LIBRARY_TYPE STREQUAL "shared")
  add_library(pytorch_qnnpack SHARED ${PYTORCH_QNNPACK_INIT_SRCS} ${PYTORCH_QNNPACK_EXEC_SRCS} ${PYTORCH_QNNPACK_UKERNELS})
elseif(PYTORCH_QNNPACK_LIBRARY_TYPE STREQUAL "static")
  add_library(pytorch_qnnpack STATIC ${PYTORCH_QNNPACK_INIT_SRCS} ${PYTORCH_QNNPACK_EXEC_SRCS} ${PYTORCH_QNNPACK_UKERNELS})
else()
  message(FATAL_ERROR "Unsupported QNNPACK library type \"${PYTORCH_QNNPACK_LIBRARY_TYPE}\". Must be \"static\", \"shared\", or \"default\"")
endif()
set_target_properties(pytorch_qnnpack PROPERTIES
  CXX_STANDARD 14
  C_STANDARD 11
  C_EXTENSIONS YES)
if(CMAKE_SYSTEM_PROCESSOR MATCHES "^armv[5-8]" OR IOS_ARCH MATCHES "^armv7")
  set_property(SOURCE ${PYTORCH_QNNPACK_ARM_NEON_UKERNELS} APPEND_STRING PROPERTY COMPILE_FLAGS " -O2 -marm -mfpu=neon ")
  if(IOS)
    set_property(SOURCE ${PYTORCH_QNNPACK_AARCH32_ASM_UKERNELS} APPEND_STRING PROPERTY COMPILE_FLAGS " -arch ${IOS_ARCH} ")
  endif()
endif()
if(PYTORCH_QNNPACK_TARGET_PROCESSOR MATCHES "^(aarch64|arm64)$" OR IOS_ARCH MATCHES "^arm64.*")
  set_property(SOURCE ${PYTORCH_QNNPACK_ARM_NEON_UKERNELS} APPEND_STRING PROPERTY COMPILE_FLAGS " -O2 ")
  if(IOS)
    set_property(SOURCE ${PYTORCH_QNNPACK_AARCH64_ASM_UKERNELS} APPEND_STRING PROPERTY COMPILE_FLAGS " -arch ${IOS_ARCH} ")
  endif()
endif()
if(PYTORCH_QNNPACK_TARGET_PROCESSOR MATCHES "^(i[3-6]86|x86_64)$" OR IOS_ARCH MATCHES "^(i386|x86_64)$")
  set_property(SOURCE ${PYTORCH_QNNPACK_X86_SSE2_UKERNELS} APPEND_STRING PROPERTY COMPILE_FLAGS " -O2 -msse2 ")
endif()
if(CMAKE_SYSTEM_PROCESSOR MATCHES "^armv[5-8]" OR IOS_ARCH MATCHES "^armv7")
  set_property(SOURCE ${PYTORCH_QNNPACK_PSIMD_UKERNELS} APPEND_STRING PROPERTY COMPILE_FLAGS " -O2 -marm -mfpu=neon ")
  set_property(SOURCE ${PYTORCH_QNNPACK_SCALAR_UKERNELS} APPEND_STRING PROPERTY COMPILE_FLAGS " -O2 -marm ")
else()
  set_property(SOURCE ${PYTORCH_QNNPACK_PSIMD_UKERNELS} APPEND_STRING PROPERTY COMPILE_FLAGS " -O2 ")
  set_property(SOURCE ${PYTORCH_QNNPACK_SCALAR_UKERNELS} APPEND_STRING PROPERTY COMPILE_FLAGS " -O2 ")
endif()
set_property(SOURCE ${PYTORCH_QNNPACK_INIT_SRCS} APPEND_STRING PROPERTY COMPILE_FLAGS " -Os ")
if(NOT CMAKE_BUILD_TYPE STREQUAL "Debug")
  set_property(SOURCE ${PYTORCH_QNNPACK_OPERATOR_SRCS} APPEND_STRING PROPERTY COMPILE_FLAGS " -O2 ")
endif()
target_include_directories(pytorch_qnnpack PUBLIC include)
target_include_directories(pytorch_qnnpack PUBLIC src)
set_target_properties(pytorch_qnnpack PROPERTIES PUBLIC_HEADER include/pytorch_qnnpack.h)
set_target_properties(pytorch_qnnpack PROPERTIES PUBLIC_HEADER include/qnnpack_func.h)

# ---[ Configure clog
if(NOT TARGET clog)
  set(CLOG_BUILD_TESTS OFF CACHE BOOL "")
  set(CLOG_RUNTIME_TYPE "${CPUINFO_RUNTIME_TYPE}" CACHE STRING "")
  add_subdirectory(
    "${CLOG_SOURCE_DIR}"
    "${CONFU_DEPENDENCIES_BINARY_DIR}/clog")
  # We build static version of clog but a dynamic library may indirectly depend on it
  set_property(TARGET clog PROPERTY POSITION_INDEPENDENT_CODE ON)
endif()
target_link_libraries(pytorch_qnnpack PUBLIC clog)

# ---[ Configure cpuinfo
if(NOT TARGET cpuinfo AND USE_SYSTEM_CPUINFO)
  add_library(cpuinfo SHARED IMPORTED)
  find_library(CPUINFO_LIBRARY cpuinfo)
  if(NOT CPUINFO_LIBRARY)
    message(FATAL_ERROR "Cannot find cpuinfo")
  endif()
  message("Found cpuinfo: ${CPUINFO_LIBRARY}")
  set_target_properties(cpuinfo PROPERTIES IMPORTED_LOCATION "${CPUINFO_LIBRARY}")
elseif(NOT TARGET cpuinfo)
  set(CPUINFO_BUILD_TOOLS OFF CACHE BOOL "")
  set(CPUINFO_BUILD_UNIT_TESTS OFF CACHE BOOL "")
  set(CPUINFO_BUILD_MOCK_TESTS OFF CACHE BOOL "")
  set(CPUINFO_BUILD_BENCHMARKS OFF CACHE BOOL "")
  add_subdirectory(
    "${CPUINFO_SOURCE_DIR}"
    "${CONFU_DEPENDENCIES_BINARY_DIR}/cpuinfo")
endif()
target_link_libraries(pytorch_qnnpack PRIVATE cpuinfo)

# ---[ Configure pthreadpool
if(NOT TARGET pthreadpool AND NOT USE_SYSTEM_PTHREADPOOL)
  set(PTHREADPOOL_BUILD_TESTS OFF CACHE BOOL "")
  set(PTHREADPOOL_BUILD_BENCHMARKS OFF CACHE BOOL "")
  add_subdirectory(
    "${PTHREADPOOL_SOURCE_DIR}"
    "${CONFU_DEPENDENCIES_BINARY_DIR}/pthreadpool")
elseif(NOT TARGET pthreadpool AND USE_SYSTEM_PTHREADPOOL)
  add_library(pthreadpool SHARED IMPORTED)
  find_library(PTHREADPOOL_LIBRARY pthreadpool)
  if(NOT PTHREADPOOL_LIBRARY)
    message(FATAL_ERROR "Cannot find pthreadpool")
  endif()
  message("-- Found pthreadpool: ${PTHREADPOOL_LIBRARY}")
  set_target_properties(pthreadpool PROPERTIES
    IMPORTED_LOCATION "${PTHREADPOOL_LIBRARY}")
  add_library(pthreadpool_interface INTERFACE)
endif()
target_link_libraries(pytorch_qnnpack PUBLIC pthreadpool)

# ---[ Configure FXdiv
if(NOT TARGET fxdiv AND NOT USE_SYSTEM_FXDIV)
  set(FXDIV_BUILD_TESTS OFF CACHE BOOL "")
  set(FXDIV_BUILD_BENCHMARKS OFF CACHE BOOL "")
  add_subdirectory(
    "${FXDIV_SOURCE_DIR}"
    "${CONFU_DEPENDENCIES_BINARY_DIR}/fxdiv")
elseif(NOT TARGET fxdiv AND USE_SYSTEM_FXDIV)
  find_file(FXDIV_HDR fxdiv.h PATH_SUFFIXES include)
  if(NOT FXDIV_HDR)
    message(FATAL_ERROR "Cannot find fxdiv")
  endif()
  add_library(fxdiv STATIC "${FXDIV_HDR}")
  set_property(TARGET fxdiv PROPERTY LINKER_LANGUAGE C)
endif()
target_link_libraries(pytorch_qnnpack PRIVATE fxdiv)

# -- [ CMake-4 compat mode
if(CMAKE_VERSION VERSION_GREATER_EQUAL "4.0.0" AND NOT (USE_SYSTEM_PSIMD OR USE_SYSTEM_FP16))
  message(WARNING "Ancient psimd/FP16 forces CMake compatibility")
  set(CMAKE_POLICY_VERSION_MINIMUM 3.5)
endif()

```

----------------------------------------

TITLE: Accessing cuFFT Plan Cache in PyTorch CUDA Backend
DESCRIPTION: This code snippet demonstrates how to access the cuFFT plan cache for a specific CUDA device using torch.backends.cuda.cufft_plan_cache.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/backends.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
torch.backends.cuda.cufft_plan_cache[i]
```

----------------------------------------

TITLE: Modifying Bwd Kernels Bash CMake
DESCRIPTION: Executes the `add_make_kernel_pt.sh` Bash script using `bash -c`, passing the backward blob list (`bwd_blob_list.txt`) as an argument. This script is intended to modify the generated kernel files, specifically changing 'make_kernel' to 'make_kernel_pt' for the backward pass. The return code is stored in `ret`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/transformers/hip/flash_attn/ck/CMakeLists.txt#_snippet_10

LANGUAGE: CMake
CODE:
```
execute_process(
  COMMAND bash -c "${CMAKE_CURRENT_LIST_DIR}/add_make_kernel_pt.sh ${CMAKE_CURRENT_LIST_DIR}/bwd_blob_list.txt"
  RESULT_VARIABLE ret)
```

----------------------------------------

TITLE: Analyzing PyTorch Operator Usage in Vision Transformer
DESCRIPTION: A statistical breakdown of PyTorch operators used in a Vision Transformer model, including tensor shapes, data types (primarily float16), and call counts. This represents an execution trace or profiling output showing the internal operations of a ViT model processing batches of 8 images with 197 tokens (14×14 + 1 class token) and 6 attention heads.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vision_transformer_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
Operator: aten._softmax.default
cnt: 12, ((T([8, 6, 197, 197], f16), -1, False), {})
Operator: aten._softmax_backward_data.default
cnt: 12, ((T([8, 6, 197, 197], f16), T([8, 6, 197, 197], f16), -1, f16), {})
Operator: aten._unsafe_view.default
cnt: 36, ((T([8, 6, 197, 64], f16), [48, 197, 64]), {})
cnt: 12, ((T([8, 6, 64, 197], f16), [48, 64, 197]), {})
cnt: 12, ((T([48, 197, 197], f16), [8, 6, 197, 197]), {})
cnt: 12, ((T([48, 197, 64], f16), [8, 6, 197, 64]), {})
cnt: 12, ((T([8, 197, 6, 64], f16), [8, 197, 384]), {})
cnt: 12, ((T([8, 197, 3, 6, 64], f16), [8, 197, 1152]), {})
Operator: aten.add.Tensor
cnt: 1, ((T([8, 197, 384], f16), T([1, 197, 384], f16)), {})
cnt: 48, ((T([8, 197, 384], f16), T([8, 197, 384], f16)), {})
Operator: aten.addmm.default
cnt: 12, ((T([1152], f16), T([1576, 384], f16), T([384, 1152], f16, stride=(1, 384))), {})
cnt: 12, ((T([384], f16), T([1576, 384], f16), T([384, 384], f16, stride=(1, 384))), {})
cnt: 12, ((T([1536], f16), T([1576, 384], f16), T([384, 1536], f16, stride=(1, 384))), {})
cnt: 12, ((T([384], f16), T([1576, 1536], f16), T([1536, 384], f16, stride=(1, 1536))), {})
cnt: 1, ((T([1000], f16), T([8, 384], f16, stride=(75648, 1)), T([384, 1000], f16, stride=(1, 384))), {})
Operator: aten.bmm.default
cnt: 12, ((T([48, 197, 64], f16), T([48, 64, 197], f16)), {})
cnt: 12, ((T([48, 197, 197], f16), T([48, 197, 64], f16)), {})
cnt: 12, ((T([48, 197, 197], f16, stride=(38809, 1, 197)), T([48, 197, 64], f16)), {})
cnt: 12, ((T([48, 197, 64], f16), T([48, 64, 197], f16, stride=(12608, 1, 64))), {})
cnt: 12, ((T([48, 64, 197], f16, stride=(12608, 1, 64)), T([48, 197, 197], f16)), {})
cnt: 12, ((T([48, 197, 197], f16), T([48, 197, 64], f16, stride=(12608, 1, 197))), {})
Operator: aten.cat.default
cnt: 1, (([T([8, 1, 384], f16, stride=(0, 384, 1)), T([8, 196, 384], f16, stride=(75264, 1, 196))], 1), {})
Operator: aten.clone.default
cnt: 1, ((T([8, 3, 224, 224], f16),), {})
Operator: aten.convolution.default
cnt: 1, ((T([8, 3, 224, 224], f16), T([384, 3, 16, 16], f16), T([384], f16), [16, 16], [0, 0], [1, 1], False, [0, 0], 1), {})
Operator: aten.convolution_backward.default
cnt: 1, ((T([8, 384, 14, 14], f16, stride=(75648, 1, 5376, 384)), T([8, 3, 224, 224], f16), T([384, 3, 16, 16], f16), [384], [16, 16], [0, 0], [1, 1], False, [0, 0], 1, [False, True, True]), {})
Operator: aten.copy_.default
cnt: 1, ((T([8, 3, 224, 224], f16), T([8, 3, 224, 224], f16)), {})
Operator: aten.div.Tensor
cnt: 2, ((T([], f16), 8000), {})
Operator: aten.gelu.default
cnt: 12, ((T([8, 197, 1536], f16),), {})
Operator: aten.gelu_backward.default
cnt: 12, ((T([8, 197, 1536], f16), T([8, 197, 1536], f16)), {})
Operator: aten.mm.default
cnt: 1, ((T([8, 1000], f16, stride=(0, 0)), T([1000, 384], f16)), {})
cnt: 1, ((T([1000, 8], f16, stride=(0, 0)), T([8, 384], f16, stride=(75648, 1))), {})
cnt: 12, ((T([1576, 384], f16), T([384, 1536], f16)), {})
cnt: 12, ((T([384, 1576], f16, stride=(1, 384)), T([1576, 1536], f16)), {})
cnt: 12, ((T([1576, 1536], f16), T([1536, 384], f16)), {})
cnt: 12, ((T([1536, 1576], f16, stride=(1, 1536)), T([1576, 384], f16)), {})
cnt: 12, ((T([1576, 384], f16), T([384, 384], f16)), {})
cnt: 12, ((T([384, 1576], f16, stride=(1, 384)), T([1576, 384], f16)), {})
cnt: 12, ((T([1576, 1152], f16), T([1152, 384], f16)), {})
cnt: 12, ((T([1152, 1576], f16, stride=(1, 1152)), T([1576, 384], f16)), {})
Operator: aten.mul.Tensor
cnt: 24, ((T([8, 6, 197, 197], f16), 0.125), {})
Operator: aten.native_layer_norm.default
cnt: 25, ((T([8, 197, 384], f16), [384], T([384], f16), T([384], f16), 1e-06), {})
Operator: aten.native_layer_norm_backward.default
cnt: 25, ((T([8, 197, 384], f16), T([8, 197, 384], f16), [384], T([8, 197, 1], f32), T([8, 197, 1], f32), T([384], f16), T([384], f16), [True, True, True]), {})
Operator: aten.select_backward.default
cnt: 1, ((T([8, 384], f16), [8, 197, 384], 1, 0), {})
Operator: aten.slice_backward.default
cnt: 1, ((T([8, 197, 384], f16), [8, 197, 384], 0, 0, 9223372036854775807, 1), {})
Operator: aten.stack.default
cnt: 12, (([T([8, 6, 197, 64], f16), T([8, 6, 197, 64], f16, stride=(75648, 12608, 1, 197)), T([8, 6, 197, 64], f16)],), {})
Operator: aten.sum.SymInt
cnt: 1, ((T([8, 1000], f16, stride=(0, 0)), [0], True), {})
cnt: 24, ((T([1576, 384], f16), [0], True), {})
cnt: 12, ((T([1576, 1536], f16), [0], True), {})
cnt: 12, ((T([1576, 1152], f16), [0], True), {})
cnt: 1, ((T([8, 197, 384], f16), [0], True), {})
cnt: 1, ((T([8, 1, 384], f16, stride=(75648, 384, 1)), [0], True), {})
Operator: aten.sum.default
cnt: 1, ((T([8, 1000], f16),), {})
Operator: aten.unbind.int
cnt: 12, ((T([3, 8, 6, 197, 64], f16, stride=(384, 226944, 64, 1152, 1)),), {})
```

----------------------------------------

TITLE: PyTorch NLL Loss Backward Operation
DESCRIPTION: NLL Loss backward operation with half-precision (f16) tensors on CUDA device. Takes input tensors of shapes [128, 1000] and [128] for gradients calculation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/levit_128_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})
```

----------------------------------------

TITLE: Defining Installation Rules for JNI Library in CMake
DESCRIPTION: Specifies installation rules for the built JNI target (`${PYTORCH_JNI_TARGET}`). It installs the archive (static lib, if applicable), library (shared lib/DLL), and runtime (executable, or DLL on Windows) components to standard destinations defined by `CMAKE_INSTALL_LIBDIR` and `CMAKE_INSTALL_BINDIR` from the `GNUInstallDirs` module.
SOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_18

LANGUAGE: cmake
CODE:
```
install(TARGETS ${PYTORCH_JNI_TARGET}
  ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}
  LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}
  RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}) #For windows
```

----------------------------------------

TITLE: Manipulating Tensors with ATen _unsafe_view Operator
DESCRIPTION: Shows the usage of `aten._unsafe_view` operator. It creates views over existing tensor data with new shapes like [256, 32, 1024] or [256, 1024, 1024], given the original tensor size. It is crucial for changing data layout without loading data into memory. Dependency is primarily on matching element sizes between the original and new shape.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_4

LANGUAGE: plaintext
CODE:
```
Operator: aten._unsafe_view.default
cnt: 3, ((T([64, 128, 32, 32], f16), [256, 32, 1024]), {})
cnt: 1, ((T([256, 1024, 1024], f16), [256, 1024, 1024]), {})
cnt: 2, ((T([256, 32, 32, 32], f16), [262144, 32]), {})
```

----------------------------------------

TITLE: Applying LayerNorm Backward with aten Operators - Python
DESCRIPTION: This snippet captures calls to the aten.native_layer_norm_backward.default operator in PyTorch, used for the backward pass of a layer normalization operation. The inputs include input, grad_output, normalized_shape, mean, rstd, and the weight/bias tensors, crucial for computing gradients; outputs track flags for gradients needed. Dependencies include PyTorch and proper tensor geometry. Inputs are typically multiple tensors matching a normalized shape and appropriate data types (f16, f32). The pattern assumes correct dimension alignment.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
Operator: aten.native_layer_norm_backward.default
cnt: 26, ((T([2, 1024, 768], f16), T([2, 1024, 768], f16), [768], T([2, 1024, 1], f32), T([2, 1024, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})
```

----------------------------------------

TITLE: PyTorch Convolution Backward Operations
DESCRIPTION: Backward pass for convolution operations including gradient computation for weights and inputs
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
aten.convolution_backward.default(T([128, 1280, 1, 1], f16), T([128, 960, 1, 1], f16), T([1280, 960, 1, 1], f16), [1280], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True])
```

----------------------------------------

TITLE: PyTorch Tensor Operations for Deep Learning Model
DESCRIPTION: This code snippet represents a collection of PyTorch tensor operations used in a deep learning model. It includes various operators such as softmax, matrix multiplication, convolution, and layer normalization. The operations are presented with their input tensor shapes, data types (primarily float16), and usage count.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tnt_s_patch16_224_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([64, 1000], f16), 1, False), {})

Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})

Operator: aten._softmax.default
cnt: 12, ((T([12544, 4, 16, 16], f16), -1, False), {})
cnt: 12, ((T([64, 6, 197, 197], f16), -1, False), {})

Operator: aten._softmax_backward_data.default
cnt: 12, ((T([64, 6, 197, 197], f16), T([64, 6, 197, 197], f16), -1, f16), {})
cnt: 12, ((T([12544, 4, 16, 16], f16), T([12544, 4, 16, 16], f16), -1, f16), {})

# ... (truncated for brevity)

Operator: aten.unbind.int
cnt: 12, ((T([2, 12544, 4, 16, 6], f16, stride=(24, 768, 6, 48, 1)),), {})
cnt: 12, ((T([2, 64, 6, 197, 64], f16, stride=(384, 151296, 64, 768, 1)),), {})
```

----------------------------------------

TITLE: Invoking aten._unsafe_view Operator for Tensor Reshaping - PyTorch - Python
DESCRIPTION: These snippets log the usage of aten._unsafe_view, an internal PyTorch tensor view operation for reshaping tensors without copying data. Inputs are tensors of various shapes and the desired new shapes. Outputs are views sharing memory with the original if possible. Risks include unsafe reshaping if tensor strides/layouts are incompatible. Prerequisites: torch, understanding of tensor memory layout.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GPT2ForSequenceClassification_training.txt#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
Operator: aten._unsafe_view.default
cnt: 36, ((T([4, 12, 1024, 64], f16), [48, 1024, 64]), {})
```

LANGUAGE: Python
CODE:
```
Operator: aten._unsafe_view.default
cnt: 12, ((T([4, 12, 64, 1024], f16), [48, 64, 1024]), {})
```

LANGUAGE: Python
CODE:
```
Operator: aten._unsafe_view.default
cnt: 12, ((T([48, 1024, 1024], f16), [4, 12, 1024, 1024]), {})
```

LANGUAGE: Python
CODE:
```
Operator: aten._unsafe_view.default
cnt: 12, ((T([48, 1024, 64], f16), [4, 12, 1024, 64]), {})
```

LANGUAGE: Python
CODE:
```
Operator: aten._unsafe_view.default
cnt: 1, ((T([4096, 2], f16), [4, 1024, 2]), {})
```

LANGUAGE: Python
CODE:
```
Operator: aten._unsafe_view.default
cnt: 24, ((T([4, 1024, 12, 64], f16), [4, 1024, 768]), {})
```

----------------------------------------

TITLE: PT2 Pre-AOTAutograd FakeTensor Usage
DESCRIPTION: Shows how to handle fake tensors in PT2 before AOTAutograd, including detecting fake mode and converting arguments to fake tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_fake_tensor.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
# Fake mode is not enabled!
from torch._guards import detect_fake_mode
fake_mode = detect_fake_mode(args)
# if fake_mode isn't None
converter = fake_mode.fake_tensor_converter
fake_args = [converter.from_real_tensor(fake_mode, arg) for arg in args]
with fake_mode:
    ... # do stuff with the fake args, if needed ...
```

----------------------------------------

TITLE: Cloning Tensor - PyTorch Aten
DESCRIPTION: Creates a deep copy of a tensor, including its data. This internal operator ensures that modifications to the new tensor do not affect the original. It takes the input tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BartForConditionalGeneration_training.txt#_snippet_10

LANGUAGE: Python
CODE:
```
import torch

input_tensor = torch.arange(20).reshape(2, 10)
clone_tensor = torch.clone(input_tensor)
```

----------------------------------------

TITLE: Registering Catch-All Custom PyTorch Operator Kernel (C++)
DESCRIPTION: Illustrates registering a catch-all kernel function (`my_kernel_fallback`) for a custom operator (`my_namespace::my_op`). This kernel will be used for all backends, disabling dispatch for this operator. Registration uses `torch::RegisterOperators::options().catchAllKernel()` with `decltype` and a function pointer.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_2

LANGUAGE: cpp
CODE:
```
namespace { Tensor my_kernel_fallback(Tensor a, Tensor b) {...} }

static auto registry = torch::RegisterOperators()
   .op("my_namespace::my_op", torch::RegisterOperators::options()
       .catchAllKernel<decltype(my_kernel_fallback), &my_kernel_fallback>());
```

----------------------------------------

TITLE: Including Quantized Source Files in Build System (CMakeLists.txt)
DESCRIPTION: Shows how to update the `CMakeLists.txt` file (`caffe2/aten/src/ATen/CMakeLists.txt`) to include new quantized source files using `FILE(GLOB ...)`. This ensures that files in specified paths (like `"native/quantized/cpu/*.cpp"`) are included in the CMake build process, necessary if custom file locations are used.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/README.md#2025-04-22_snippet_7

LANGUAGE: bash
CODE:
```
FILE(GLOB native_quantized_cpp
          "native/quantized/*.cpp"
          "native/quantized/cpu/*.cpp")
```

----------------------------------------

TITLE: Invoking aten.mul and aten.mul.Scalar for Elementwise and Scalar Multiplication - PyTorch - Python
DESCRIPTION: These snippets document both elementwise (tensor-tensor) and scalar (tensor-scalar) multiplication of f16 tensors using aten.mul.Tensor and aten.mul.Scalar. Parameters include input tensors or broadcasting scalars. Output matches broadcasting rules, producing f16 tensors of original or broadcasted shape. Dependencies: torch, careful management of precision and broadcasting semantics.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GPT2ForSequenceClassification_training.txt#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
Operator: aten.mul.Scalar
cnt: 12, ((T([4, 1024, 3072], f16), 3.0), {})
```

LANGUAGE: Python
CODE:
```
Operator: aten.mul.Tensor
cnt: 24, ((T([4, 1024, 3072], f16), 0.5), {})
```

LANGUAGE: Python
CODE:
```
Operator: aten.mul.Tensor
cnt: 24, ((T([4, 1024, 3072], f16), 0.044715), {})
```

LANGUAGE: Python
CODE:
```
Operator: aten.mul.Tensor
cnt: 24, ((T([4, 1024, 3072], f16), 0.7978845608028654), {})
```

LANGUAGE: Python
CODE:
```
Operator: aten.mul.Tensor
cnt: 48, ((T([4, 1024, 3072], f16), T([4, 1024, 3072], f16)), {})
```

----------------------------------------

TITLE: Analyzing Softmax Operations in PyTorch
DESCRIPTION: This snippet shows the usage of aten._softmax.default and aten._softmax_backward_data.default operators with various tensor shapes and data types.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/attention_is_all_you_need_pytorch_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten._softmax.default
cnt: 6, ((T([256, 8, 33, 33], f16), -1, False), {})
cnt: 6, ((T([256, 8, 31, 31], f16), -1, False), {})
cnt: 6, ((T([256, 8, 31, 33], f16), -1, False), {})
Operator: aten._softmax_backward_data.default
cnt: 6, ((T([256, 8, 31, 33], f16), T([256, 8, 31, 33], f16), -1, f16), {})
cnt: 6, ((T([256, 8, 31, 31], f16), T([256, 8, 31, 31], f16), -1, f16), {})
cnt: 6, ((T([256, 8, 33, 33], f16), T([256, 8, 33, 33], f16), -1, f16), {})
```

----------------------------------------

TITLE: Building and Configuring C10 Library Target
DESCRIPTION: Creates the c10 library target, sets compile options, version properties, and visibility settings. This defines how the library should be built and what properties it should have.
SOURCE: https://github.com/pytorch/pytorch/blob/main/c10/CMakeLists.txt#2025-04-22_snippet_3

LANGUAGE: CMake
CODE:
```
if(NOT BUILD_LIBTORCHLESS)
  add_library(c10 ${C10_SRCS} ${C10_HEADERS})
  torch_compile_options(c10)
  if(HAVE_SOVERSION)
    set_target_properties(c10 PROPERTIES
        VERSION ${TORCH_VERSION} SOVERSION ${TORCH_SOVERSION})
  endif()
  # If building shared library, set dllimport/dllexport proper.
  target_compile_options(c10 PRIVATE "-DC10_BUILD_MAIN_LIB")
  # Enable hidden visibility if compiler supports it.
  if(${COMPILER_SUPPORTS_HIDDEN_VISIBILITY})
    target_compile_options(c10 PRIVATE "-fvisibility=hidden")
  endif()
```

----------------------------------------

TITLE: Tensor Addition in PyTorch (Python)
DESCRIPTION: Involves the aten.add operation which adds corresponding elements of two tensors. This operation is widely utilized in neural network operations, particularly when aggregating weighted inputs to neurons.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
aten.add.Tensor
cnt: 1, ((T([16, 512, 768], f16), T([1, 512, 768], f16)), {})
cnt: 79, ((T([16, 512, 768], f16), T([16, 512, 768], f16)), {})
cnt: 12, ((T([16, 12, 512, 512], f16), T([16, 1, 1, 512], f16)), {})
cnt: 2, ((T([1024, 768], f16), T([1024, 768], f16)), {})
```

----------------------------------------

TITLE: Installing and Running Functorch Benchmarks
DESCRIPTION: Commands for installing functorch (either stable or from source) and running benchmarks with functorch integration.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/functional_autograd_benchmark/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
# Install stable functorch:
pip install functorch
# or install from source:
pip install git+https://github.com/pytorch/functorch

# Run the benchmark for the base
# This will use the GPU if available.
pushd benchmarks/functional_autograd_benchmark
python functional_autograd_benchmark.py --output bench-with-functorch.txt
```

----------------------------------------

TITLE: Executing Softmax in PyTorch
DESCRIPTION: Computes the softmax of a tensor along the specified dimension, essential for probability distributions in classification problems. This function operates on FP16 tensors and allows specification of the dimension for computation. The result is a tensor with values normalized between 0 and 1.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/Speech2Text2ForCausalLM_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
aten._softmax.default(T([256, 128, 128], f16), -1, False)
```

----------------------------------------

TITLE: Logging Aten Operator: aten.select_backward.default (Text)
DESCRIPTION: Log entry for an invocation of the 'aten.select_backward.default' operator. Arguments include an input tensor (T) with shape and data type (f16), followed by dimensions and indices.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/pit_b_224_training.txt#2025-04-22_snippet_3

LANGUAGE: text
CODE:
```
Operator: aten.select_backward.default
cnt: 1, ((T([64, 1024], f16), [64, 1, 1024], 1, 0), {})
```

----------------------------------------

TITLE: Batch Normalization Operations
DESCRIPTION: Batch normalization operations on tensors with various channel dimensions (32 to 448), using momentum 0.1 and epsilon 0.001
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_inception_v3_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
((T([128, 32, 149, 149], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 0.001), {})
```

----------------------------------------

TITLE: Log Softmax Operations in PyTorch
DESCRIPTION: Log softmax forward and backward operations on tensors with shape [32, 1000] using float16 precision. These are typically used in classification tasks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convmixer_768_32_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
aten._log_softmax.default((T([32, 1000], f16), 1, False), {})
aten._log_softmax_backward_data.default((T([32, 1000], f16), T([32, 1000], f16), 1, f16), {})
```

----------------------------------------

TITLE: Defining the Test Executable for Edge Operator Registration in CMake
DESCRIPTION: Creates an executable target named `test_edge_op_registration`. It compiles the specified C++ source files (`test_operator_registration.cpp`, `test_main.cpp`). Defines the `USE_GTEST` preprocessor macro privately for this target, indicating its use of the Google Test framework.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/edge/CMakeLists.txt#2025-04-22_snippet_5

LANGUAGE: cmake
CODE:
```
add_executable(test_edge_op_registration
        ${TEST_ROOT}/test_operator_registration.cpp
        ${TEST_ROOT}/test_main.cpp
        )

target_compile_definitions(test_edge_op_registration PRIVATE USE_GTEST)
```

----------------------------------------

TITLE: Renaming Generated Files Bash CMake
DESCRIPTION: Executes a Bash command using `bash -c` to iterate through all `.cpp` files in the current directory and rename them to have a `.hip` extension. This prepares the generated kernel files for compilation with a HIP compiler. The return code is stored in `ret`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/transformers/hip/flash_attn/ck/CMakeLists.txt#_snippet_12

LANGUAGE: CMake
CODE:
```
execute_process(COMMAND bash -c "for file in ${CMAKE_CURRENT_LIST_DIR}/*.cpp; do mv -- \"$file\" \"\${file%.cpp}.hip\"; done"
  RESULT_VARIABLE ret
)
```

----------------------------------------

TITLE: Running PyTorch Add Operator Benchmark
DESCRIPTION: Executes the benchmark for the torch.add operator with single-threaded configuration. The flags control OpenMP and MKL thread counts.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
cd pytorch/benchmarks/operator_benchmark
python -m pt.add_test --omp-num-threads 1 --mkl-num-threads 1
```

----------------------------------------

TITLE: Applying Sigmoid Activation in PyTorch
DESCRIPTION: The sigmoid operator applies the sigmoid activation function on a tensor, mapping input values into the (0, 1) interval. This function is commonly used in classification tasks to form probability distributions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/legacy_senet154_training.txt#2025-04-22_snippet_9

LANGUAGE: text
CODE:
```
cnt: 3, ((T([32, 256, 1, 1], f16),), {})
```

LANGUAGE: text
CODE:
```
cnt: 8, ((T([32, 512, 1, 1], f16),), {})
```

----------------------------------------

TITLE: Tracking NLL Loss Calculations in PyTorch for Classification
DESCRIPTION: Records function calls for negative log-likelihood loss in forward and backward passes. The operations use a batch size of 128 with 1000 classes, indicating a standard image classification task.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
Operator: aten.nll_loss_backward.default
cnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})
Operator: aten.nll_loss_forward.default
cnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})
```

----------------------------------------

TITLE: Matrix Multiplication for Final Classification in PyTorch MobileNetV3
DESCRIPTION: The addmm operation used in the final fully connected layer for classification. It takes the flattened features ([128, 1984]) and multiplies them with weights ([1984, 1000]) to produce logits for 1000 classes, adding a bias term ([1000]).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
Operator: aten.addmm.default
cnt: 1, ((T([1000], f16), T([128, 1984], f16), T([1984, 1000], f16, stride=(1, 1984))), {})
```

----------------------------------------

TITLE: Defining Command for Generating Unboxing Kernels in CMake
DESCRIPTION: Constructs the command (`GEN_COMMAND`) to execute the Python module `torchgen.gen_executorch` for generating C++ source files related to unboxing kernels. Specifies source paths, output directory, YAML configuration files, and flags. Also defines the list of expected output source files (`GEN_COMMAND_sources`).
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/edge/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: cmake
CODE:
```
# Generate unboxing kernels
set(GEN_COMMAND
        "${Python_EXECUTABLE}" -m torchgen.gen_executorch
        --source-path=${TEST_ROOT}
        --install-dir=${OUTPUT_DIRECTORY}
        --tags-path=${TORCH_ROOT}/aten/src/ATen/native/tags.yaml
        --aten-yaml-path=${TORCH_ROOT}/aten/src/ATen/native/native_functions.yaml
        --use-aten-lib
        --op-selection-yaml-path=${TEST_ROOT}/selected_operators.yaml
        --custom-ops-yaml-path=${TEST_ROOT}/custom_ops.yaml
        )
set(GEN_COMMAND_sources
        ${OUTPUT_DIRECTORY}/RegisterCodegenUnboxedKernelsEverything.cpp
        ${OUTPUT_DIRECTORY}/RegisterCPUCustomOps.cpp
        ${OUTPUT_DIRECTORY}/Functions.h
        ${OUTPUT_DIRECTORY}/NativeFunctions.h
        ${OUTPUT_DIRECTORY}/CustomOpsNativeFunctions.h
        )
message(STATUS "Generating sources for unboxing kernels ${GEN_COMMAND}")
```

----------------------------------------

TITLE: Defining Shared Libraries CMake
DESCRIPTION: Provides a basic CMakeLists.txt file defining two shared libraries, `foo` and `bar`, where `bar` links against `foo`. It highlights the need for explicit symbol export (`__declspec(dllexport)`) for symbols from `foo` to be visible and usable by `bar` when building with MSVC on Windows.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_57

LANGUAGE: CMake
CODE:
```
project(myproject CXX)
set(CMAKE_CXX_STANDARD 14)
add_library(foo SHARED foo.cpp)
add_library(bar SHARED bar.cpp)
# NB: don't forget to __declspec(dllexport) at least one symbol from foo,
# otherwise foo.lib will not be created.
target_link_libraries(bar PUBLIC foo)
```

----------------------------------------

TITLE: Execute aten.relu_ Activation in Python
DESCRIPTION: Performs the in-place rectified linear unit (ReLU) activation on tensors using PyTorch. Commonly used in constructing neural network layers for introducing non-linearity. Dependencies require PyTorch’s deep learning capabilities. The snippet accepts tensors, modifying them to zero out negative values, enhancing feature representations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
cnt: 2, ((T([64, 32, 112, 112], f16),), {})
```

----------------------------------------

TITLE: PyTorch Pooling Operations
DESCRIPTION: Max pooling operations with indices, including forward and backward passes, using 3x3 kernel and 2x2 stride
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/densenet121_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
((T([4, 64, 112, 112], f16), [3, 3], [2, 2], [1, 1]), {})
```

----------------------------------------

TITLE: Remove RST Files for Faster Doc Builds (Bash)
DESCRIPTION: These commands provide a tip to speed up documentation builds by temporarily removing most `.rst` source files. It navigates to the `docs/source` directory and uses standard Unix tools (`find`, `grep`, `xargs rm`) to delete all `.rst` files except for `index.rst` and a specified file (e.g., `jit.rst`) you are actively working on.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_28

LANGUAGE: bash
CODE:
```
cd docs/source
find . -type f | grep rst | grep -v index | grep -v jit | xargs rm
```

----------------------------------------

TITLE: Configuring NNAPI Backend in CMake
DESCRIPTION: This snippet adds the Android NNAPI delegate library, linking it with the appropriate dependencies. It's skipped for MacOS due to build issues.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_18

LANGUAGE: CMake
CODE:
```
if(NOT ${CMAKE_SYSTEM_NAME} MATCHES "Darwin")
  add_library(nnapi_backend SHARED
          ${TORCH_SRC_DIR}/csrc/jit/backends/nnapi/nnapi_backend_lib.cpp
          ${TORCH_SRC_DIR}/csrc/jit/backends/nnapi/nnapi_backend_preprocess.cpp
          )
  if(BUILD_LIBTORCHLESS)
    target_link_libraries(nnapi_backend PRIVATE ${TORCH_LIB} torch_python pybind::pybind11)
  else()
    target_link_libraries(nnapi_backend PRIVATE torch torch_python pybind::pybind11)
  endif()
endif()
```

----------------------------------------

TITLE: Installing Linux Dependencies (MAGMA CUDA) Bash
DESCRIPTION: Runs a script to install MAGMA, a library for GPU-accelerated linear algebra, which is necessary for enabling certain CUDA features in PyTorch builds on Linux. Requires specifying the CUDA version.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_5

LANGUAGE: Bash
CODE:
```
# CUDA only: Add LAPACK support for the GPU if needed
# magma installation: run with active conda environment. specify CUDA version to install
.ci/docker/common/install_magma_conda.sh 12.4
```

----------------------------------------

TITLE: Running Sparse Matrix-Vector Multiplication Benchmark
DESCRIPTION: Command line usage for running SPMV benchmarks using matmul_bench.py.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/sparse/dlmc/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
matmul_bench.py --operation sparse@vector
```

----------------------------------------

TITLE: In-place Tensor Addition Operations in PyTorch
DESCRIPTION: This snippet shows the in-place tensor addition operations (aten.add_.Tensor) with various tensor shapes. These operations modify tensors in-place without creating new ones, used at different spatial resolutions in the model.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
Operator: aten.add_.Tensor
cnt: 2, ((T([96, 64, 32, 32], f16), T([96, 64, 32, 32], f16)), {})
cnt: 2, ((T([96, 128, 16, 16], f16), T([96, 128, 16, 16], f16)), {})
cnt: 2, ((T([96, 256, 8, 8], f16), T([96, 256, 8, 8], f16)), {})
cnt: 2, ((T([96, 512, 4, 4], f16), T([96, 512, 4, 4], f16)), {})
```

----------------------------------------

TITLE: Running FastRNNs benchmarks with default settings
DESCRIPTION: Command to run the FastRNNs benchmarking suite with default settings, which compares all available RNN implementations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/fastrnns/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
python -m fastrnns.bench
```

----------------------------------------

TITLE: Operator Frequency and Parameter Analysis in PyTorch
DESCRIPTION: This section presents different PyTorch operators with their execution count and the shape of tensors they operate on. It includes operators like `aten.add`, `aten.mul`, and `aten.mm`, analyzing their frequency with various tensor shapes and data types, such as `f16` for half-precision floating points. This structured information is crucial for performance analysis and optimization in machine learning models using PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GoogleFnet_training.txt#2025-04-22_snippet_0

LANGUAGE: pseudo
CODE:
```
Operator: aten._fft_c2c.default\ncnt: 12, ((T([1, 512, 768], c32), [1, 2], 0, True), {})\nOperator: aten._log_softmax.default\ncnt: 1, ((T([512, 32000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([512, 32000], f16), T([512, 32000], f16), 1, f16), {})\nOperator: aten._to_copy.default\ncnt: 12, ((T([1, 512, 768], f16),), {'dtype': c32})\nOperator: aten.add.Tensor\ncnt: 28, ((T([1, 512, 768], f16), T([1, 512, 768], f16)), {})\ncnt: 24, ((T([1, 512, 768], f16), T([1, 512, 768], f16, stride=(786432, 1536, 2))), {})\ncnt: 36, ((T([1, 512, 3072], f16), T([1, 512, 3072], f16)), {})\ncnt: 12, ((T([1, 512, 3072], f16), 1.0), {})\ncnt: 1, ((T([1, 512, 768], f16), 1.0), {})\ncnt: 1, ((T([32000, 768], f16), T([32000, 768], f16)), {})...\n
```

----------------------------------------

TITLE: Max Pooling Backward Pass in PyTorch
DESCRIPTION: Computes the gradients of max pooling operations, using the stored indices from the forward pass.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
aten.max_pool2d_with_indices_backward.default((T([64, 512, 7, 7], f16), T([64, 512, 14, 14], f16), [2, 2], [2, 2], [0, 0], [1, 1], False, T([64, 512, 7, 7], i64)), {})
```

LANGUAGE: Python
CODE:
```
aten.max_pool2d_with_indices_backward.default((T([64, 512, 14, 14], f16), T([64, 512, 28, 28], f16), [2, 2], [2, 2], [0, 0], [1, 1], False, T([64, 512, 14, 14], i64)), {})
```

----------------------------------------

TITLE: Applying PyTorch relu_ In-place Activation on Tensors - Python
DESCRIPTION: These code blocks provide example argument patterns for in-place relu_ operator calls on float16 tensors of various shapes. Cases include large input tensors for batch processing (e.g., images or features). Useful for verifying in-place modification and broadcasting in PyTorch for relu_ activation. Requires float16 tensor support and basic PyTorch invocation compatibility.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/resnest101e_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
Operator: aten.relu_.default
cnt: 2, ((T([32, 64, 128, 128], f16),), {})
cnt: 1, ((T([32, 128, 128, 128], f16),), {})
cnt: 3, ((T([32, 64, 64, 64], f16),), {})
cnt: 4, ((T([32, 128, 64, 64], f16),), {})
cnt: 3, ((T([32, 32, 1, 1], f16),), {})
cnt: 4, ((T([32, 256, 64, 64], f16),), {})
cnt: 4, ((T([32, 64, 1, 1], f16),), {})
cnt: 5, ((T([32, 512, 32, 32], f16),), {})
cnt: 3, ((T([32, 128, 32, 32], f16),), {})
cnt: 4, ((T([32, 256, 32, 32], f16),), {})
cnt: 23, ((T([32, 128, 1, 1], f16),), {})
cnt: 24, ((T([32, 1024, 16, 16], f16),), {})
cnt: 22, ((T([32, 256, 16, 16], f16),), {})
cnt: 23, ((T([32, 512, 16, 16], f16),), {})
cnt: 3, ((T([32, 256, 1, 1], f16),), {})
cnt: 3, ((T([32, 2048, 8, 8], f16),), {})
cnt: 2, ((T([32, 512, 8, 8], f16),), {})
cnt: 2, ((T([32, 1024, 8, 8], f16),), {})
```

----------------------------------------

TITLE: Analyzing ATen Masked Fill Operations in PyTorch
DESCRIPTION: Records occurrence of the \"aten.masked_fill_.Scalar\", used to fill tensors with a scalar value based on a mask.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_16

LANGUAGE: pseudocode
CODE:
```
Operator: aten.masked_fill_.Scalar
cnt: 1, ((T([128, 128], f32), T([128, 128], b8), 0), {})
```

----------------------------------------

TITLE: Configuring IPython to Hide Error Tracebacks
DESCRIPTION: Sets up IPython to hide error tracebacks, showing only exception messages. This is used for cleaner error output in the examples.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/typing.ipynb#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
# Hide traceback of Error
import functools
ipython = get_ipython()
def showtraceback(self, exc_tuple=None, filename=None, tb_offset=None,
                  exception_only=False, running_compiled_code=False):
    try:
        try:
            etype, value, tb = self._get_exc_info(exc_tuple)
        except ValueError:
            print('No traceback available to show.', file=sys.stderr)
            return

        # Hide traceback
        stb = self.InteractiveTB.get_exception_only(etype, value)

        self._showtraceback(etype, value, stb)

    except KeyboardInterrupt:
        print('\n' + self.get_exception_only(), file=sys.stderr)
ipython.showtraceback = functools.partial(showtraceback, ipython)
```

----------------------------------------

TITLE: Configuring Caffe2 MPS Tests - CMake
DESCRIPTION: Configures and builds Caffe2 MPS (Metal Performance Shaders) test executables if `USE_MPS` is enabled. It finds Metal and Foundation libraries, links them along with `torch_library` and `gtest_main`, sets include directories, adds tests, and optionally installs the test executable and its PDB file (on MSVC).
SOURCE: https://github.com/pytorch/pytorch/blob/main/caffe2/CMakeLists.txt#_snippet_58

LANGUAGE: CMake
CODE:
```
  if(USE_MPS)
    foreach(test_src ${Caffe2_MPS_TEST_SRCS})
      get_filename_component(test_name ${test_src} NAME_WE)
      add_executable(${test_name} "${test_src}")
      find_library(metal NAMES Metal)
      find_library(foundation NAMES Foundation)
      target_link_libraries(${test_name} torch_library gtest_main ${metal} ${foundation})
      target_include_directories(${test_name} PRIVATE $<INSTALL_INTERFACE:include>)
      target_include_directories(${test_name} PRIVATE $<BUILD_INTERFACE:${CMAKE_BINARY_DIR}/include>)
      target_include_directories(${test_name} PRIVATE ${Caffe2_CPU_INCLUDE})
      add_test(NAME ${test_name} COMMAND $<TARGET_FILE:${test_name}>)
      if(INSTALL_TEST)
        set_target_properties(${test_name} PROPERTIES INSTALL_RPATH "${CMAKE_INSTALL_RPATH}:${_rpath_portable_origin}/../lib")
        install(TARGETS ${test_name} DESTINATION test)
        # Install PDB files for MSVC builds
        if(MSVC AND BUILD_SHARED_LIBS)
          install(FILES $<TARGET_PDB_FILE:${test_name}> DESTINATION test OPTIONAL)
        endif()
      endif()
    endforeach()
  endif()
```

----------------------------------------

TITLE: Submit Changes with ghstack (Bash)
DESCRIPTION: This command is used with the `ghstack` workflow tool after making commits, potentially including those generated by the CI update script. It submits the local changes as a stack of pull requests to GitHub, facilitating review.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_22

LANGUAGE: bash
CODE:
```
ghstack submit
```

----------------------------------------

TITLE: Configuring PyTorch Gradient Operations for Benchmarking
DESCRIPTION: This code demonstrates how to configure PyTorch gradient operations for benchmarking by enabling requires_grad and using generate_pt_gradient_test.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_19

LANGUAGE: python
CODE:
```
self.input_one = torch.rand(M, N, K, requires_grad=True)
generate_pt_gradient_test(long_configs + short_configs, TorchAddBenchmark)
```

----------------------------------------

TITLE: Adaptive Average Pooling in PyTorch
DESCRIPTION: Performs 2D adaptive average pooling on input tensors. The forward and backward operations are shown with their respective input shapes and data types.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
aten._adaptive_avg_pool2d.default((T([64, 512, 7, 7], f16), [7, 7]), {})
```

LANGUAGE: Python
CODE:
```
aten._adaptive_avg_pool2d_backward.default((T([64, 512, 7, 7], f16), T([64, 512, 7, 7], f16)), {})
```

----------------------------------------

TITLE: PyTorch Batch Normalization Operations
DESCRIPTION: Batch normalization operations with various tensor shapes and configurations. Uses momentum of 0.03 and epsilon of 0.0001 across different channel dimensions ranging from 32 to 1024.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
((T([8, 32, 384, 512], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), False, 0.03, 0.0001), {})
```

----------------------------------------

TITLE: Copying Tensors with ATen
DESCRIPTION: This function copies data from the source tensor to the destination tensor. Dependencies include two tensors with identical shapes, e.g., [16, 3, 128, 128], and data type f16. The main constraint is that source and destination tensors must have the same shapes and types to execute a successful copy operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
Operator: aten.copy_.default
cnt: 1, ((T([16, 3, 128, 128], f16), T([16, 3, 128, 128], f16)), {})
cnt: 1, ((T([16, 5], f16), T([16, 5], f16)), {})
cnt: 4, ((T([64], f16), T([64], f16)), {})
cnt: 4, ((T([128], f16), T([128], f16)), {})
cnt: 26, ((T([256], f16), T([256], f16)), {})
cnt: 4, ((T([16, 64, 128, 128], f16), T([16, 64, 128, 128], f16)), {})
cnt: 2, ((T([1, 1024, 128, 128], f16), T([1, 1024, 128, 128], f16)), {})
cnt: 4, ((T([16, 128, 64, 64], f16), T([16, 128, 64, 64], f16)), {})
cnt: 2, ((T([1, 2048, 64, 64], f16), T([1, 2048, 64, 64], f16)), {})
cnt: 14, ((T([16, 256, 32, 32], f16), T([16, 256, 32, 32], f16)), {})
cnt: 7, ((T([1, 4096, 32, 32], f16), T([1, 4096, 32, 32], f16)), {})
```

----------------------------------------

TITLE: Configuring CCache Max Files (Bash)
DESCRIPTION: Configures the maximum number of files stored in the ccache directory. Setting to 0 allows an unlimited number of cached files, recommended for comprehensive caching.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_46

LANGUAGE: bash
CODE:
```
ccache -F 0
```

----------------------------------------

TITLE: Computing Convolution Gradients with aten.convolution_backward.default
DESCRIPTION: Shows the gradient computation for convolution operations during backpropagation. This includes gradients for weights and input tensors across various network layers, with particular focus on multi-scale feature extraction and detection heads.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
Operator: aten.convolution_backward.default
cnt: 1, ((T([8, 255, 48, 64], f16), T([8, 256, 48, 64], f16), T([255, 256, 1, 1], f16), [255], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 11, ((T([8, 256, 48, 64], f16), T([8, 128, 48, 64], f16), T([256, 128, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 10, ((T([8, 128, 48, 64], f16), T([8, 256, 48, 64], f16), T([128, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([8, 128, 48, 64], f16), T([8, 384, 48, 64], f16), T([128, 384, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([8, 128, 24, 32], f16), T([8, 256, 24, 32], f16), T([128, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([8, 255, 24, 32], f16), T([8, 512, 24, 32], f16), T([255, 512, 1, 1], f16), [255], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 11, ((T([8, 512, 24, 32], f16), T([8, 256, 24, 32], f16), T([512, 256, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 10, ((T([8, 256, 24, 32], f16), T([8, 512, 24, 32], f16), T([256, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([8, 256, 24, 32], f16), T([8, 768, 24, 32], f16), T([256, 768, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([8, 256, 12, 16], f16), T([8, 512, 12, 16], f16), T([256, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([8, 255, 12, 16], f16), T([8, 1024, 12, 16], f16), T([255, 1024, 1, 1], f16), [255], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
cnt: 7, ((T([8, 1024, 12, 16], f16), T([8, 512, 12, 16], f16), T([1024, 512, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 7, ((T([8, 512, 12, 16], f16), T([8, 1024, 12, 16], f16), T([512, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([8, 512, 12, 16], f16), T([8, 2048, 12, 16], f16), T([512, 2048, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([8, 1024, 12, 16], f16), T([8, 512, 24, 32], f16), T([1024, 512, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([8, 512, 24, 32], f16), T([8, 256, 48, 64], f16), T([512, 256, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([8, 256, 48, 64], f16), T([8, 128, 96, 128], f16), T([256, 128, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([8, 128, 96, 128], f16), T([8, 64, 96, 128], f16), T([128, 64, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([8, 64, 96, 128], f16), T([8, 128, 96, 128], f16), T([64, 128, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([8, 128, 96, 128], f16), T([8, 64, 192, 256], f16), T([128, 64, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([8, 64, 192, 256], f16), T([8, 32, 192, 256], f16), T([64, 32, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([8, 32, 192, 256], f16), T([8, 64, 192, 256], f16), T([32, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([8, 64, 192, 256], f16), T([8, 32, 384, 512], f16), T([64, 32, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([8, 32, 384, 512], f16), T([8, 3, 384, 512], f16), T([32, 3, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})
```

----------------------------------------

TITLE: Batch Matrix Multiplication Using BMM in PyTorch
DESCRIPTION: The aten.bmm.default operator is highlighted, showcasing operations on 3D tensors [24, 512, 64] and [24, 64, 512] with float16 precision. It allows efficient batch-wise matrix multiplications, key for batched data processes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
Operator: aten.bmm.default
cnt: 24, ((T([24, 512, 64], f16), T([24, 64, 512], f16, stride=(32768, 1, 64))), {})
```

----------------------------------------

TITLE: Setting Android NDK Path for PyTorch Build
DESCRIPTION: Sets the ANDROID_NDK environment variable to specify the location of Android NDK required for building PyTorch on Android.
SOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
export ANDROID_NDK=YOUR_NDK_PATH
```

----------------------------------------

TITLE: Showing TensorExpr Benchmark Documentation
DESCRIPTION: Command to display the help documentation for TensorExpr benchmarks module.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/tensorexpr/HowToRun.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
python -m benchmarks.tensorexpr --help
```

----------------------------------------

TITLE: Analyzing ATen Tensor Cloning Operations in PyTorch
DESCRIPTION: The snippet examines the use of the ATen clone operator, emphasizing instances where tensors are duplicated. Cloning is key when mutable operations are performed to maintain the original data structure, enabling controlled manipulation in workflows.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistilBertForMaskedLM_training.txt#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
Operator: aten.clone.default
cnt: 2, ((T([16, 128], i64),), {})
```

----------------------------------------

TITLE: Defining and Linking Lazy Test Dependencies in CMake
DESCRIPTION: Sets a CMake variable `LAZY_TEST_DEPENDENCIES` to list the required libraries (`torch`, `gtest`). Then, it links these libraries privately to the `test_lazy` target using `target_link_libraries`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/lazy/CMakeLists.txt#2025-04-22_snippet_5

LANGUAGE: cmake
CODE:
```
set(LAZY_TEST_DEPENDENCIES torch gtest)

target_link_libraries(test_lazy PRIVATE ${LAZY_TEST_DEPENDENCIES})
```

----------------------------------------

TITLE: Configuring CCache Max Size (Bash)
DESCRIPTION: Configures the maximum size of the ccache directory on disk. Setting a larger size, like 25Gi, is recommended for large projects like PyTorch to maximize cache effectiveness. Set to 0 for unlimited size.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_45

LANGUAGE: bash
CODE:
```
ccache -M 25Gi
```

----------------------------------------

TITLE: Creating Type Stub Targets for PyTorch Python
DESCRIPTION: Sets up the targets and commands to generate Python type stubs (.pyi files) for the PyTorch Python API, which enables better IDE support and type checking.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_11

LANGUAGE: CMake
CODE:
```
add_custom_target(torch_python_stubs DEPENDS
    "${TORCH_SRC_DIR}/_C/__init__.pyi"
    "${TORCH_SRC_DIR}/_C/_VariableFunctions.pyi"
    "${TORCH_SRC_DIR}/nn/functional.pyi"
    "${TORCH_SRC_DIR}/utils/data/datapipes/datapipe.pyi"
)

file(GLOB_RECURSE torchgen_python "${PROJECT_SOURCE_DIR}/torchgen/*.py")
file(GLOB_RECURSE autograd_python "${TOOLS_PATH}/autograd/*.py")
file(GLOB_RECURSE pyi_python "${TOOLS_PATH}/pyi/*.py")
add_custom_command(
    OUTPUT
    "${TORCH_SRC_DIR}/_C/__init__.pyi"
    "${TORCH_SRC_DIR}/_C/_VariableFunctions.pyi"
    "${TORCH_SRC_DIR}/nn/functional.pyi"
    COMMAND
    "${Python_EXECUTABLE}" -mtools.pyi.gen_pyi
      --native-functions-path "aten/src/ATen/native/native_functions.yaml"
      --tags-path "aten/src/ATen/native/tags.yaml"
      --deprecated-functions-path "tools/autograd/deprecated.yaml"
    DEPENDS
      "${TORCH_SRC_DIR}/_C/__init__.pyi.in"
      "${TORCH_SRC_DIR}/_C/_VariableFunctions.pyi.in"
      "${TORCH_SRC_DIR}/nn/functional.pyi.in"
      "${TORCH_ROOT}/aten/src/ATen/native/native_functions.yaml"
      "${TORCH_ROOT}/aten/src/ATen/native/tags.yaml"
      "${TORCH_ROOT}/tools/autograd/deprecated.yaml"
      "${TORCH_ROOT}/torch/_torch_docs.py"
      "${TORCH_ROOT}/torch/_tensor_docs.py"
      ${pyi_python}
      ${autograd_python}
      ${torchgen_python}
    WORKING_DIRECTORY
    "${TORCH_ROOT}"
)
file(GLOB_RECURSE datapipe_files "${TORCH_SRC_DIR}/utils/data/datapipes/*.py")
add_custom_command(
    OUTPUT
    "${TORCH_SRC_DIR}/utils/data/datapipes/datapipe.pyi"
    COMMAND
    "${Python_EXECUTABLE}" ${TORCH_SRC_DIR}/utils/data/datapipes/gen_pyi.py
    DEPENDS
    "${TORCH_SRC_DIR}/utils/data/datapipes/datapipe.pyi.in"
    ${datapipe_files}
    WORKING_DIRECTORY
    "${TORCH_ROOT}"
)
```

----------------------------------------

TITLE: Profiling ATen Convolution and Convolution Backward Operators in PyTorch (Python)
DESCRIPTION: Tracks calls to ATen convolution and its backward variants, with detailed tensor dimensions and convolution settings (stride, padding, dilation, groups, etc). Dependencies: PyTorch, model containing convolutional layers, mixed-precision support. Inputs include weight, input, and optional bias tensors; outputs correspond to forward or backward pass results. Constraints: Only operator calls are shown, not model source code.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet18_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
Operator: aten.convolution.default
cnt: 1, ((T([16, 3, 224, 224], f16), T([64, 3, 7, 7], f16), None, [2, 2], [3, 3], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([16, 64, 56, 56], f16), T([64, 64, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([16, 64, 56, 56], f16), T([128, 64, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([16, 128, 28, 28], f16), T([128, 128, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([16, 64, 56, 56], f16), T([128, 64, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([16, 128, 28, 28], f16), T([256, 128, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([16, 256, 14, 14], f16), T([256, 256, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([16, 128, 28, 28], f16), T([256, 128, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([16, 256, 14, 14], f16), T([512, 256, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([16, 512, 7, 7], f16), T([512, 512, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([16, 256, 14, 14], f16), T([512, 256, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
```

LANGUAGE: python
CODE:
```
Operator: aten.convolution_backward.default
cnt: 3, ((T([16, 512, 7, 7], f16), T([16, 512, 7, 7], f16), T([512, 512, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([16, 512, 7, 7], f16), T([16, 256, 14, 14], f16), T([512, 256, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([16, 512, 7, 7], f16), T([16, 256, 14, 14], f16), T([512, 256, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 3, ((T([16, 256, 14, 14], f16), T([16, 256, 14, 14], f16), T([256, 256, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([16, 256, 14, 14], f16), T([16, 128, 28, 28], f16), T([256, 128, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([16, 256, 14, 14], f16), T([16, 128, 28, 28], f16), T([256, 128, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 3, ((T([16, 128, 28, 28], f16), T([16, 128, 28, 28], f16), T([128, 128, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([16, 128, 28, 28], f16), T([16, 64, 56, 56], f16), T([128, 64, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([16, 128, 28, 28], f16), T([16, 64, 56, 56], f16), T([128, 64, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 4, ((T([16, 64, 56, 56], f16), T([16, 64, 56, 56], f16), T([64, 64, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([16, 64, 112, 112], f16), T([16, 3, 224, 224], f16), T([64, 3, 7, 7], f16), [0], [2, 2], [3, 3], [1, 1], False, [0, 0], 1, [False, True, False]), {})
```

----------------------------------------

TITLE: CUDA Graph Profiling Fix
DESCRIPTION: Workaround for CUDA graph profiling issues on older driver versions, showing initialization setup.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_profiling_torch_compile.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
import torch

torch.profiler._utils._init_for_cuda_graphs()

# ... rest of program
```

----------------------------------------

TITLE: Adding Tensors - PyTorch Aten
DESCRIPTION: Performs element-wise addition between two tensors. This fundamental internal operator handles standard tensor addition. It takes two tensors as input.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BartForConditionalGeneration_training.txt#_snippet_6

LANGUAGE: Python
CODE:
```
import torch

tensor_a = torch.randn(2, 1024, 1024, dtype=torch.float16)
tensor_b = torch.randn(2, 1024, 1024, dtype=torch.float16)
result = torch.add(tensor_a, tensor_b)
```

----------------------------------------

TITLE: Layer Normalization Operations
DESCRIPTION: Forward and backward passes for layer normalization, operating on tensors of shape [8, 512, 768] with epsilon value of 1e-12.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_DistilBert_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
aten.native_layer_norm.default(T([8, 512, 768], f16), [768], T([768], f16), T([768], f16), 1e-12)
aten.native_layer_norm_backward.default(T([8, 512, 768], f16), T([8, 512, 768], f16), [768], T([8, 512, 1], f32), T([8, 512, 1], f32), T([768], f16), T([768], f16), [True, True, True])
```

----------------------------------------

TITLE: Invoking aten._log_softmax_backward_data Operator - PyTorch - Python
DESCRIPTION: This snippet traces the backward computation for the log_softmax operation using aten._log_softmax_backward_data. It takes two f16 tensors of shape [4, 2], an axis specification, and outputs the gradient with respect to the input for use in autograd. All tensors use half precision. Dependencies: torch autograd engine. Limitations: f16 requires appropriate hardware.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GPT2ForSequenceClassification_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([4, 2], f16), T([4, 2], f16), 1, f16), {})
```

----------------------------------------

TITLE: Implementing Unsafe View in PyTorch
DESCRIPTION: Facilitates reshaping tensors without copying data, which may lead to unsafe memory access due to unfixed strides. Its use cases are mainly in scenarios where reshaping operations must be fast, with the side effect of possible data corruption. It requires PyTorch, input tensors, and desired shape dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/Speech2Text2ForCausalLM_training.txt#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
aten._unsafe_view.default(T([64, 128, 4, 64], f16), [64, 128, 256])
```

LANGUAGE: Python
CODE:
```
aten._unsafe_view.default(T([8192, 10000], f16), [64, 128, 10000])
```

LANGUAGE: Python
CODE:
```
aten._unsafe_view.default(T([64, 4, 128, 64], f16), [256, 128, 64])
```

LANGUAGE: Python
CODE:
```
aten._unsafe_view.default(T([64, 128, 256], f16), [8192, 256])
```

----------------------------------------

TITLE: Python Timer Creation for Eager Mode
DESCRIPTION: Example of creating a Timer instance for the eager forward mode in Python. It uses the benchmark's Python forward statement and setup code.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/instruction_counts/README.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
Timer(
    stmt=benchmark.py_fwd_stmt,
    setup=benchmark.setup.py_setup,
)
```

----------------------------------------

TITLE: Adding RPATH for Apple Builds in CMake
DESCRIPTION: For Apple platforms (macOS, iOS), adds a custom post-build command to the `pytorch_jni` target. This command uses `install_name_tool` to add `@loader_path` to the runtime path (rpath) of the built library, helping it find its dependencies relative to its own location at runtime.
SOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_8

LANGUAGE: cmake
CODE:
```
if(APPLE)
  # Need to add rpath so dlopen can find dependencies.
  add_custom_command(TARGET pytorch_jni
      POST_BUILD COMMAND
      ${CMAKE_INSTALL_NAME_TOOL} -add_rpath "@loader_path"
        $<TARGET_FILE:pytorch_jni>)
endif()
```

----------------------------------------

TITLE: Configuring ROCm Support for PyTorch Python
DESCRIPTION: Sets up AMD ROCm support for PyTorch Python bindings when the USE_ROCM option is enabled.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_8

LANGUAGE: CMake
CODE:
```
if(USE_ROCM)
    append_filelist("libtorch_python_cuda_core_sources" TORCH_PYTHON_SRCS)
    list(APPEND TORCH_PYTHON_SRCS ${GENERATED_THNN_CXX_CUDA})

    list(APPEND TORCH_PYTHON_COMPILE_DEFINITIONS
      USE_ROCM
      __HIP_PLATFORM_AMD__
      )
    if(NOT WIN32)
      list(APPEND TORCH_PYTHON_LINK_LIBRARIES ${ROCM_ROCTX_LIB})
    endif()
endif()
```

----------------------------------------

TITLE: Profiling FastRNNs models with nvprof
DESCRIPTION: Command to profile all FastRNNs models using NVIDIA's nvprof tool, generating performance profiling files.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/fastrnns/README.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
python -m fastrnns.profile
```

----------------------------------------

TITLE: Comparing Python and C++ Tensor Creation
DESCRIPTION: Compares tensor creation syntax between Python and C++ APIs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
torch.randn(3, 4, dtype=torch.float32, device=torch.device('cuda', 1), requires_grad=True)
```

LANGUAGE: cpp
CODE:
```
torch::randn({3, 4}, torch::dtype(torch::kFloat32).device(torch::kCUDA, 1).requires_grad(true))
```

----------------------------------------

TITLE: Registering Custom Backend with Decorator
DESCRIPTION: Demonstrates how to register a custom backend using the register_backend decorator, allowing the backend to be referenced by name in torch.compile.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_custom_backends.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
from torch._dynamo import register_backend

@register_backend
def my_compiler(gm, example_inputs):
    ...
```

----------------------------------------

TITLE: Complete Distributed Autograd Example (Incomplete)
DESCRIPTION: Beginning of a complete example showing how to use distributed autograd with the RPC framework. This snippet sets up the imports and defines the add function but is incomplete in the provided text.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/distributed_autograd.rst#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
import torch
import torch.distributed.autograd as dist_autograd
import torch.distributed.rpc as rpc

def my_add(t1, t2):
  return torch.add(t1, t2)

# On worker 0:
```

----------------------------------------

TITLE: Running TorchInductor Benchmark Command
DESCRIPTION: Command to run benchmark script for model profiling with specific environment variables enabled for kernel naming and benchmarking.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_inductor_profiling.rst#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1 TORCHINDUCTOR_BENCHMARK_KERNEL=1 python -u benchmarks/dynamo/timm_models.py --backend inductor --amp --performance --dashboard --only mixnet_l --disable-cudagraphs --training
```

----------------------------------------

TITLE: True Division with torch.div in PyTorch 1.7
DESCRIPTION: Shows how torch.div performs true division in PyTorch 1.7, similar to Python 3 division.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
# PyTorch 1.7
>>> a = torch.tensor(5)
>>> b = torch.tensor(3)
>>> a / b
tensor(1.6667)
```

----------------------------------------

TITLE: Running Add Benchmark with Long Tag Filter
DESCRIPTION: Runs torch.add benchmarks using only the test cases tagged as 'long'.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_11

LANGUAGE: bash
CODE:
```
python -m pt.add_test --tag-filter long
```

----------------------------------------

TITLE: Traced Function Calling Scripted Function with Autocast
DESCRIPTION: Example showing that calling a scripted function from a traced function with autocast enabled is not supported in PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/JIT-AUTOCAST.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
import torch
from torch.cpu.amp import autocast

@torch.jit.script
def fn(a, b):
    return torch.mm(a, b)

def traced(a, b):
    with autocast(enabled=True):
        return fn(a, b)

# running TorchScript with Autocast enabled is not supported
torch.jit.trace(traced, (x, y))
```

----------------------------------------

TITLE: Using CUDA Packed Tensor Accessors in C++
DESCRIPTION: Example of creating and using CUDA packed tensor accessors for efficient element-wise access in GPU kernels. Packed accessors copy tensor metadata rather than pointing to it, making them suitable for use in CUDA device code.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_basics.rst#2025-04-22_snippet_2

LANGUAGE: cpp
CODE:
```
__global__ void packed_accessor_kernel(
    torch::PackedTensorAccessor64<float, 2> foo,
    float* trace) {
  int i = threadIdx.x;
  gpuAtomicAdd(trace, foo[i][i]);
}

torch::Tensor foo = torch::rand({12, 12});

// assert foo is 2-dimensional and holds floats.
auto foo_a = foo.packed_accessor64<float,2>();
float trace = 0;

packed_accessor_kernel<<<1, 12>>>(foo_a, &trace);
```

----------------------------------------

TITLE: PyTorch Matrix Operations
DESCRIPTION: Matrix multiplication and transformation operations including bmm (batch matrix multiply), mm (matrix multiply), and addmm (matrix multiply with addition). Operations handle various tensor shapes and use float16 precision.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/AllenaiLongformerBase_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
# Matrix multiplication operations
aten.bmm.default(T([36, 512, 64], f16), T([36, 64, 512], f16))
aten.mm.default(T([1024, 768], f16), T([768, 3072], f16))
aten.addmm.default(T([768], f16), T([1024, 768], f16), T([768, 768], f16))
```

----------------------------------------

TITLE: Creating Zero-Initialized Tensors in PyTorch
DESCRIPTION: This snippet creates zero-initialized tensors with specified shapes. It's commonly used for initializing tensors that will be filled with computed values later in the neural network process.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_12

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([8, 3, 48, 64, 4], f16), [6266880]), {})
cnt: 1, ((T([8, 3, 24, 32, 4], f16), [1566720]), {})
cnt: 1, ((T([8, 3, 12, 16, 4], f16), [391680]), {})
```

----------------------------------------

TITLE: Calculating Normalized L2 Error with torch.ao.ns.fx.utils in Python
DESCRIPTION: This snippet shows the function signature for computing the normalized L2 error between two tensors x and y using the torch.ao.ns.fx.utils module.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.ao.ns._numeric_suite_fx.rst#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
torch.ao.ns.fx.utils.compute_normalized_l2_error(x, y)
```

----------------------------------------

TITLE: Evaluating Model Quality for Sparsified DLRM Models in Python
DESCRIPTION: Python script to evaluate model quality metrics for sparsified DLRM models. It uses raw and processed data files along with sparse model metadata.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/benchmarks/README.md#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
python evaluate_model_metrics.py --raw-data-file=<path_to_raw_data_txt_file> --processed-data-file=<path_to_kaggleAdDisplayChallenge_processed.npz> --sparse-model-metadata=<path_to_sparse_model_metadata_csv>
```

----------------------------------------

TITLE: Registering Optimized Kernels for CPU Dispatch in C++
DESCRIPTION: This snippet details the registration of a CPU kernel with divergence in AVX2 and AVX512 performance: using REGISTER_DISPATCH for general or inferior AVX512 performance, or ALSO_REGISTER_AVX512_DISPATCH when optimized. Benchmarks on performance can be determined by environment variables to enforce specific instruction sets during runtime.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/cpu/README.md#2025-04-22_snippet_2

LANGUAGE: C++
CODE:
```
REGISTER_DISPATCH(fnNameImpl, &your_kernel)
```

LANGUAGE: C++
CODE:
```
ALSO_REGISTER_AVX512_DISPATCH(fnNameImpl, &your_kernel)
```

----------------------------------------

TITLE: Old API for Module Attributes in TorchScript
DESCRIPTION: Demonstrates the older approach for defining attributes in TorchScript modules by inheriting from torch.jit.ScriptModule and using torch.jit.Attribute.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
from typing import Dict
import torch

class MyModule(torch.jit.ScriptModule):
    def __init__(self):
        super().__init__()
        self.my_dict = torch.jit.Attribute({}, Dict[str, int])
        self.my_int = torch.jit.Attribute(20, int)

m = MyModule()
```

----------------------------------------

TITLE: PyTorch Tensor Operations - Softmax and Log Softmax
DESCRIPTION: Collection of softmax and log softmax operations with various tensor shapes and half precision (f16) data type. Includes both forward and backward operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
aten._log_softmax.default((T([128, 1000], f16), 1, False))
aten._log_softmax_backward_data.default((T([128, 1000], f16), T([128, 1000], f16), 1, f16))
aten._softmax.default((T([1024, 4, 64, 144], f16), -1, False))
```

----------------------------------------

TITLE: Implementing BUILD_LIST Bytecode in Python Symbolic Execution
DESCRIPTION: This code snippet shows the implementation of the BUILD_LIST bytecode in Dynamo's symbolic executor. It pops a specified number of elements from the stack and pushes a new ListVariable to the stack.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
def BUILD_LIST(self, inst):
    items = self.popn(inst.argval)
    self.push(ListVariable(items, mutation_type=ValueMutationNew()))
```

----------------------------------------

TITLE: Tensor Conversion with Copy in PyTorch (Python)
DESCRIPTION: Utilizes aten._to_copy to copy tensor data while optionally changing its dtype, noted by the conversion from f32 to f16, a common practice for memory and computation efficiency during training pipeline.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
aten._to_copy.default
cnt: 1, ((T([16, 1, 1, 512], f32),), {"dtype": f16})
```

----------------------------------------

TITLE: Profiling PyTorch nll_loss Forward and Backward Operators - Python
DESCRIPTION: This snippet enumerates example input arguments for the PyTorch ATen nll_loss_forward.default and nll_loss_backward.default operators. Inputs include tensors representing logits/probabilities, target indices, optional weights, reduction/fill parameters, and labels. These are relevant for loss and gradient checking in multiclass classification setups. Knowledge of PyTorch ATen interface and tensor dtype/shape conventions is required.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
cnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})
cnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})
```

----------------------------------------

TITLE: Determining Cosine Similarity using torch.ao.ns.fx.utils in Python
DESCRIPTION: This code snippet illustrates the function signature for computing the cosine similarity between two tensors x and y using the torch.ao.ns.fx.utils module.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.ao.ns._numeric_suite_fx.rst#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
torch.ao.ns.fx.utils.compute_cosine_similarity(x, y)
```

----------------------------------------

TITLE: Enabling Custom OpInfo Test in Test List
DESCRIPTION: Shows the Python code required to register a custom `OpInfo` (like "ops.aten.slice_scatter") in the `TESTED_TORCHLIB_OPS` list using `TorchLibOpInfo`, linking it to the corresponding implementation (`core_ops.aten_slice_scatter`) for inclusion in the ONNX Runtime test suite.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/onnx/torchlib/README.md#_snippet_5

LANGUAGE: python
CODE:
```
TorchLibOpInfo("ops.aten.slice_scatter", core_ops.aten_slice_scatter)
```

----------------------------------------

TITLE: Describing PyTorch nll_loss_forward and nll_loss_backward Calls - Python
DESCRIPTION: These snippets specify argument patterns for negative log-likelihood loss operators in PyTorch, showing combinations of predicted logits, target indices, optional weights, reduction type, ignore index, and output tensors. Arguments include float16 tensors for prediction and output, int64 tensors for indices, and constants matching operator signatures. Intended for coverage of varied NLL loss forward and backward usages in PyTorch. Assumes dependency on PyTorch operators, with shapes matching typical classification tasks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/resnest101e_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
Operator: aten.nll_loss_backward.default
cnt: 1, ((T([], f16), T([32, 1000], f16), T([32], i64), None, 1, -100, T([], f16)), {})
Operator: aten.nll_loss_forward.default
cnt: 1, ((T([32, 1000], f16), T([32], i64), None, 1, -100), {})
```

----------------------------------------

TITLE: Defining Python Package Dependencies for PyTorch Development
DESCRIPTION: Specifies required Python packages and their version constraints for PyTorch development environment. Includes essential packages for building, testing, and development like cmake, ninja, numpy, and other utility packages. Some dependencies have specific version requirements and platform constraints.
SOURCE: https://github.com/pytorch/pytorch/blob/main/requirements.txt#2025-04-22_snippet_0

LANGUAGE: plaintext
CODE:
```
astunparse
cmake
expecttest>=0.3.0
filelock
fsspec
hypothesis
jinja2
lintrunner ; platform_machine != "s390x"
networkx
ninja
numpy
optree>=0.13.0
packaging
psutil
pyyaml
requests
setuptools>=62.3.0,<75.9
sympy>=1.13.3
types-dataclasses
typing-extensions>=4.10.0
```

----------------------------------------

TITLE: Add Unix DL Dependency (CMake)
DESCRIPTION: Adds the 'dl' library dependency for Caffe2 on non-Apple Unix systems.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#_snippet_33

LANGUAGE: CMake
CODE:
```
if(NOT APPLE AND UNIX)
  list(APPEND Caffe2_DEPENDENCY_LIBS dl)
endif()
```

----------------------------------------

TITLE: Selecting JNI Source Files Based on Build Type in CMake
DESCRIPTION: Uses `file(GLOB ...)` to populate the `pytorch_android_SOURCES` variable with the appropriate C++ source files. If `BUILD_LITE_INTERPRETER` is true, it includes `pytorch_jni_lite.cpp`; otherwise, it includes `pytorch_jni_jit.cpp`. Both configurations include the common JNI files.
SOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_6

LANGUAGE: cmake
CODE:
```
if(BUILD_LITE_INTERPRETER)
  file(GLOB pytorch_android_SOURCES
    ${pytorch_android_DIR}/pytorch_jni_lite.cpp
    ${pytorch_android_DIR}/pytorch_jni_common.cpp
    ${pytorch_android_DIR}/pytorch_jni_common.h
  )
else()
  file(GLOB pytorch_android_SOURCES
    ${pytorch_android_DIR}/pytorch_jni_jit.cpp
    ${pytorch_android_DIR}/pytorch_jni_common.cpp
    ${pytorch_android_DIR}/pytorch_jni_common.h
  )
endif()
```

----------------------------------------

TITLE: Dumping Tensor Dispatch Keys with Functorch (Python)
DESCRIPTION: This Python code snippet refers to the `torch._C._functorch.dump_tensor` function, which is used for debugging within PyTorch's functorch module. It dumps the dispatch keys currently on the stack, aiding in understanding the dispatch mechanism.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/README.md#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
`torch._C._functorch.dump_tensor`
```

----------------------------------------

TITLE: Complete Minifier Usage Example
DESCRIPTION: Full example demonstrating how to use the minifier with a serialized graph and custom checker function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/minifier.ipynb#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
import torch
import torch.fx as fx
from functorch.compile import minifier, check_nvfuser_subprocess, check_nvfuser_correctness_subprocess

inps = [(torch.Size([3]), torch.float32), (torch.Size([3]), torch.float32)]
inps = [torch.ones(shape, dtype=dtype) for (shape, dtype) in inps]
from foo import FxModule
mod = FxModule()

minifier(fx.symbolic_trace(mod), inps, pass_checker)
```

----------------------------------------

TITLE: Example of Unstructured Pruned Weight Tensor
DESCRIPTION: Shows a weight tensor after unstructured pruning where individual elements with the lowest values have been zeroed out while preserving the tensor's shape.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/pruner/README.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
W_pruned = [[0 0 3]
            [4 5 6]
            [7 0 9]]
```

----------------------------------------

TITLE: Adding fbjni Subdirectory Dependency in CMake
DESCRIPTION: Sets the path to the fbjni library source (`fbjni_DIR`) and defines a build directory for it (`fbjni_BUILD_DIR`) based on the main build subdirectory (`BUILD_SUBDIR`). It then includes the fbjni library build process as a subdirectory using `add_subdirectory`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_11

LANGUAGE: cmake
CODE:
```
set(fbjni_DIR ${CMAKE_CURRENT_LIST_DIR}/../libs/fbjni/)
set(fbjni_BUILD_DIR ${CMAKE_BINARY_DIR}/fbjni/${BUILD_SUBDIR})

add_subdirectory(${fbjni_DIR} ${fbjni_BUILD_DIR})
```

----------------------------------------

TITLE: Preparing AMD ROCm Build for PyTorch
DESCRIPTION: This snippet shows how to prepare the PyTorch build environment for AMD ROCm support. It involves running a Python script to set up the necessary configurations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/libtorch.rst#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
cd <pytorch_root>

# Only run this if you're compiling for ROCm
python tools/amd_build/build_amd.py
```

----------------------------------------

TITLE: Python Package Dependencies List
DESCRIPTION: A list of required Python packages needed for the PyTorch project. Includes PyGithub for GitHub API interactions and tqdm for displaying progress bars.
SOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release_notes/requirements.txt#2025-04-22_snippet_0

LANGUAGE: text
CODE:
```
PyGithub
tqdm
```

----------------------------------------

TITLE: Parallel addition with aten.add_.Tensor in PyTorch
DESCRIPTION: The aten.add_.Tensor operator handles in-place tensor addition, modifying one tensor by adding another to it, with focus on large f16-based tensor dimensions. The operation requires PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MobileBertForMaskedLM_training.txt#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
aten.add_.Tensor, ((T([16, 128, 30522], f16), T([30522], f16)), {})
```

----------------------------------------

TITLE: Implementing Thread-Safe Metrics Collection in PyTorch
DESCRIPTION: Uses a semaphore to ensure thread-safe writing to metrics_dict in metrics_thread and gpu_utilization_thread. This change improves the accuracy and reliability of performance metrics collection.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/inference/CHANGELOG.md#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
semaphore.acquire()
try:
    # Write to metrics_dict
finally:
    semaphore.release()
```

----------------------------------------

TITLE: Install Specific Katex Version (Bash)
DESCRIPTION: This command demonstrates how to install a specific version (`0.13.18`) of the `katex` tool globally using the npm package manager. This is noted as a version compatible with a specific `nodejs` version (`6.13.1`) that is known to work for documentation builds.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_24

LANGUAGE: bash
CODE:
```
npm install -g katex@0.13.18
```

----------------------------------------

TITLE: Set Up Conda Environment on Linux for Source Build
DESCRIPTION: This code snippet provides the Bash commands to set up a dedicated Conda environment on Linux. This environment is required as a prerequisite for building PyTorch from source.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_0

LANGUAGE: Bash
CODE:
```
$ source <CONDA_INSTALL_DIR>/bin/activate
$ conda create -y -n <CONDA_NAME>
$ conda activate <CONDA_NAME>
```

----------------------------------------

TITLE: Computing Convolutions with aten.convolution.default
DESCRIPTION: Documents convolution operations in a neural network. Operations include initial feature extraction, downsampling through strided convolutions, and detection head layers for a YOLO-like architecture, with most operations using half-precision (float16) data.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
Operator: aten.convolution.default
cnt: 1, ((T([8, 3, 384, 512], f16), T([32, 3, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([8, 32, 384, 512], f16), T([64, 32, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([8, 64, 192, 256], f16), T([32, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([8, 32, 192, 256], f16), T([64, 32, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([8, 64, 192, 256], f16), T([128, 64, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([8, 128, 96, 128], f16), T([64, 128, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([8, 64, 96, 128], f16), T([128, 64, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([8, 128, 96, 128], f16), T([256, 128, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 10, ((T([8, 256, 48, 64], f16), T([128, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 11, ((T([8, 128, 48, 64], f16), T([256, 128, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([8, 256, 48, 64], f16), T([512, 256, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 10, ((T([8, 512, 24, 32], f16), T([256, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 11, ((T([8, 256, 24, 32], f16), T([512, 256, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([8, 512, 24, 32], f16), T([1024, 512, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 7, ((T([8, 1024, 12, 16], f16), T([512, 1024, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 7, ((T([8, 512, 12, 16], f16), T([1024, 512, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([8, 2048, 12, 16], f16), T([512, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([8, 1024, 12, 16], f16), T([255, 1024, 1, 1], f16), T([255], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([8, 512, 12, 16], f16), T([256, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([8, 768, 24, 32], f16), T([256, 768, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([8, 512, 24, 32], f16), T([255, 512, 1, 1], f16), T([255], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([8, 256, 24, 32], f16), T([128, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([8, 384, 48, 64], f16), T([128, 384, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([8, 256, 48, 64], f16), T([255, 256, 1, 1], f16), T([255], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

----------------------------------------

TITLE: PyTorch Sum Operation
DESCRIPTION: Shows the symbolic integer summation operation (aten.sum.SymInt) on a specific tensor shape with dimension preservation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2net50_14w_8s_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([128, 1000], f16), [0], True), {})
```

----------------------------------------

TITLE: Exposing Custom PyTorch Operators to Caffe2 CPU
DESCRIPTION: This code demonstrates how to make a custom PyTorch operator available to the Caffe2 frontend for CPU execution. It uses a macro to expose the operator under a specified name in Caffe2.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_12

LANGUAGE: cpp
CODE:
```
// Expose "my_namespace::my_op" custom operator to caffe2.
// In caffe2, the operator will be called "MyCaffe2OperatorName".
C10_EXPORT_C10_OP_TO_CAFFE2_CPU(
    MyCaffe2OperatorName, "my_namespace::my_op")
```

----------------------------------------

TITLE: Pip-compile Command for Generating Bazel Requirements
DESCRIPTION: The pip-compile command that was used to generate this requirements file. It specifies the --allow-unsafe flag and --generate-hashes option while compiling the requirements.in file from the tools/build/bazel directory.
SOURCE: https://github.com/pytorch/pytorch/blob/main/tools/build/bazel/requirements.txt#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
pip-compile --allow-unsafe --generate-hashes tools/build/bazel/requirements.in
```

----------------------------------------

TITLE: NLL Loss Calculations in PyTorch
DESCRIPTION: Forward and backward negative log likelihood loss computations with ignore_index=-100, operating on classification outputs in half-precision format.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
aten.nll_loss_forward.default
cnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})
```

----------------------------------------

TITLE: Running Benchmarks for Specific Operator
DESCRIPTION: Executes benchmarks for a specific operator (add) with thread control.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_10

LANGUAGE: bash
CODE:
```
python -m benchmark_all_test --operators add --omp-num-threads 1 --mkl-num-threads 1
```

----------------------------------------

TITLE: Printing Optimized Graph of jit-traced Function in PyTorch
DESCRIPTION: This code prints the optimized graph of the jit-traced function, revealing that the if statement has been optimized out, leading to incorrect behavior with changed inputs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
print(torch.jit.last_executed_optimized_graph())

# graph(%t : Tensor,
#       %maybe : Tensor):
#   %2 : Tensor = prim::profile[profiled_type=Float(1, strides=[1], requires_grad=0, device=cpu), seen_none=0](%t)
#    = prim::profile()
#   return (%2)
```

----------------------------------------

TITLE: Defining and Loading NCCL Custom Allocator in Python
DESCRIPTION: This snippet defines C++ functions `nccl_alloc_plug` and `nccl_free_plug` that wrap NCCL's memory allocation and deallocation functions. It then uses `torch.utils.cpp_extension.load_inline` to compile and load this C++ code into a Python module. Finally, it wraps the loaded allocator functions in a `CUDAPluggableAllocator` instance and initializes PyTorch distributed communication to access the backend's allocator.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#_snippet_26

LANGUAGE: python
CODE:
```
import os

import torch
import torch.distributed as dist
from torch.cuda.memory import CUDAPluggableAllocator
from torch.distributed.distributed_c10d import _get_default_group
from torch.utils import cpp_extension


# create allocator
nccl_allocator_source = """
#include <nccl.h>
#include <iostream>
extern "C" {

void* nccl_alloc_plug(size_t size, int device, void* stream) {
  std::cout << "Using ncclMemAlloc" << std::endl;
  void* ptr;
  ncclResult_t err = ncclMemAlloc(&ptr, size);
  return ptr;

}

void nccl_free_plug(void* ptr, size_t size, int device, void* stream) {
  std::cout << "Using ncclMemFree" << std::endl;
  ncclResult_t err = ncclMemFree(ptr);
}

}
"""
nccl_allocator_libname = "nccl_allocator"
nccl_allocator = torch.utils.cpp_extension.load_inline(
    name=nccl_allocator_libname,
    cpp_sources=nccl_allocator_source,
    with_cuda=True,
    extra_ldflags=["-lnccl"],
    verbose=True,
    is_python_module=False,
    build_directory="./",
)

allocator = CUDAPluggableAllocator(
    f"./{nccl_allocator_libname}.so", "nccl_alloc_plug", "nccl_free_plug"
).allocator()

# setup distributed
rank = int(os.getenv("RANK"))
local_rank = int(os.getenv("LOCAL_RANK"))
world_size = int(os.getenv("WORLD_SIZE"))
torch.cuda.set_device(local_rank)
dist.init_process_group(backend="nccl")
device = torch.device(f"cuda:{local_rank}")
default_pg = _get_default_group()
backend = default_pg._get_backend(device)

# Note: for convenience, ProcessGroupNCCL backend provides
# the ncclMemAlloc allocator as backend.mem_allocator
allocator = backend.mem_allocator
```

----------------------------------------

TITLE: Defining Conditional Logic in Python
DESCRIPTION: This Python function `f` takes three arguments and demonstrates a simple conditional structure (if/else) based on the value of `c`. It is provided as the source code example that is translated into the PyTorch IR representation shown subsequently.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_1

LANGUAGE: Python
CODE:
```
def f(a, b, c):
    d = a + b
    if c:
        e = d + d
    else:
        e = b + d
    return e
```

----------------------------------------

TITLE: Defining Custom C++ Operator Symbolic (Part 1: Symbolic Function)
DESCRIPTION: Defines the custom symbolic function symbolic_foo_forward using symbolic_helper.parse_args to handle inputs and attributes. It creates an ONNX operator custom_domain::Foo with specified inputs and attributes, mapping the PyTorch C++ op call to its ONNX representation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#_snippet_30

LANGUAGE: Python
CODE:
```
from torch.onnx import symbolic_helper


# Define custom symbolic function
@symbolic_helper.parse_args("v", "v", "f", "i")
def symbolic_foo_forward(g, input1, input2, attr1, attr2):
    return g.op("custom_domain::Foo", input1, input2, attr1_f=attr1, attr2_i=attr2)
```

----------------------------------------

TITLE: Tensor Operations with Layer Normalization
DESCRIPTION: Collection of tensor operations showing various configurations of layer normalization with different tensor shapes and strides. Operations include forward and backward passes with half-precision (f16) tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convnext_base_training.txt#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 56, 56, 128], f16, stride=(401408, 56, 1, 3136)), [128], T([128], f16), T([128], f16), 1e-06), {})
cnt: 3, ((T([32, 56, 56, 128], f16), [128], T([128], f16), T([128], f16), 1e-06), {})
cnt: 3, ((T([32, 28, 28, 256], f16), [256], T([256], f16), T([256], f16), 1e-06), {})
```

----------------------------------------

TITLE: Running TensorExpr GPU Benchmark
DESCRIPTION: Example command for running TensorExpr benchmark on GPU with forward mode, trace JIT mode, and TensorExpr CUDA fuser.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/tensorexpr/HowToRun.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
python -m benchmarks.tensorexpr --device gpu --mode fwd --jit-mode trace --cuda-fuser=te
```

----------------------------------------

TITLE: Visualizing Pattern Matching in PyTorch Computational Graph
DESCRIPTION: This code snippet illustrates how a specific fusion pattern (nn.ReLU, (operator.add, MatchAllNode, (nn.BatchNorm2d, nn.Conv2d))) would match a computational graph in PyTorch. It shows the structure of the graph and how different operations are connected.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/pattern.md#2025-04-22_snippet_1

LANGUAGE: text
CODE:
```
tensor_1            tensor_2
 |                    |
 *(MatchAllNode)  nn.Conv2d
 |                    |
 |             nn.BatchNorm2d
 \                  /
  -- operator.add --
         |
      nn.ReLU
```

----------------------------------------

TITLE: Adding Dynamic CUDA Dependencies in CMake
DESCRIPTION: This snippet executes when the environment variable `ATEN_STATIC_CUDA` is *not* set. It appends the standard CUDA libraries (`${CUDA_LIBRARIES}`) and dynamic versions of cuSPARSE and cuFFT to the `ATen_CUDA_DEPENDENCY_LIBS` list. If `BUILD_LAZY_CUDA_LINALG` is not enabled, it also appends the dynamic cuSolver library.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_13

LANGUAGE: cmake
CODE:
```
  else()
    list(APPEND ATen_CUDA_DEPENDENCY_LIBS
      ${CUDA_LIBRARIES}
      CUDA::cusparse
      CUDA::cufft
    )
   if(NOT BUILD_LAZY_CUDA_LINALG)
     list(APPEND ATen_CUDA_DEPENDENCY_LIBS
       CUDA::cusolver
     )
   endif()
  endif()
```

----------------------------------------

TITLE: Applying MSVC Conforming Lambda Flag - CMake
DESCRIPTION: Applies the `/Zc:lambda` compile flag to specific memory-efficient attention CUDA source files when building with MSVC. This flag enables the conforming lambda processor to support capturing `constexpr` in lambdas.
SOURCE: https://github.com/pytorch/pytorch/blob/main/caffe2/CMakeLists.txt#_snippet_63

LANGUAGE: CMake
CODE:
```
if(MSVC)
  # This is used to enable the conforming lambda processor in MSVC
  # Which allows us to capture constexpr in lambdas
  # Note that this will be turned on by default for std=c++20 and above
  # This should be applied globally when https://github.com/pytorch/pytorch/issues/92600 is fixed
  foreach(tmp ${MEM_EFF_ATTENTION_CUDA_SOURCES})
    # MEM_EFF_ATTENTION_CUDA is populated in pytorch/aten/src/ATen/CMakeLists.txt
    # We iterate over these files, updating paths and adding the compile flag
    FILE(RELATIVE_PATH tmp_path "${PROJECT_SOURCE_DIR}" "${tmp}")
    SET(tmp_path "../${tmp_path}")
    set_source_files_properties(${tmp_path} PROPERTIES COMPILE_FLAGS "-Xcompiler /Zc:lambda")
  endforeach()
endif()
```

----------------------------------------

TITLE: Adding CUTLASS Includes and Conditional Static CUDA Dependencies in CMake
DESCRIPTION: Appends CUTLASS include directories to the ATen CUDA include path list. Conditionally, if the environment variable `ATEN_STATIC_CUDA` is set, it appends static CUDA libraries (cuSPARSE, cuFFT, cuSolver, LAPACK) to the `ATen_CUDA_DEPENDENCY_LIBS` list. The specific cuSolver and LAPACK libraries depend on the CUDA version (<=11 or >=12) and whether `BUILD_LAZY_CUDA_LINALG` is disabled.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_12

LANGUAGE: cmake
CODE:
```
  list(APPEND ATen_CUDA_INCLUDE ${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/cutlass/include)
  list(APPEND ATen_CUDA_INCLUDE ${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/cutlass/tools/util/include)
  if($ENV{ATEN_STATIC_CUDA})
    list(APPEND ATen_CUDA_DEPENDENCY_LIBS
      ${CUDA_LIBRARIES}
      CUDA::cusparse_static
      CUDA::cufft_static_nocallback
    )
   if(NOT BUILD_LAZY_CUDA_LINALG)
     if(CUDA_VERSION_MAJOR LESS_EQUAL 11)
       list(APPEND ATen_CUDA_DEPENDENCY_LIBS
         CUDA::cusolver_static
         ${CUDAToolkit_LIBRARY_DIR}/liblapack_static.a     # needed for libcusolver_static
       )
     elseif(CUDA_VERSION_MAJOR GREATER_EQUAL 12)
       list(APPEND ATen_CUDA_DEPENDENCY_LIBS
         CUDA::cusolver_static
         ${CUDAToolkit_LIBRARY_DIR}/libcusolver_lapack_static.a     # needed for libcusolver_static
       )
     endif()
   endif()
```

----------------------------------------

TITLE: Tracking Native Batch Normalization Calls (aten.native_batch_norm) - PyTorch - Python
DESCRIPTION: This snippet enumerates call configurations of aten.native_batch_norm, capturing input and parameter tensor shapes and flags for training mode, epsilon, and momentum. Useful in determining how normalization layers are applied per-channel. Relevant for half precision workflows and input validation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_13

LANGUAGE: Python
CODE:
```
Operator: aten.native_batch_norm.default
cnt: 1, ((T([1, 16, 27], f16), T([16], f16), None, None, None, True, 0.0, 1e-05), {})
cnt: 1, ((T([1, 32, 144], f16), T([32], f16), None, None, None, True, 0.0, 1e-05), {})
cnt: 1, ((T([1, 64, 288], f16), T([64], f16), None, None, None, True, 0.0, 1e-05), {})
cnt: 5, ((T([1, 128, 576], f16), T([128], f16), None, None, None, True, 0.0, 1e-05), {})
cnt: 1, ((T([1, 256, 128], f16), T([256], f16), None, None, None, True, 0.0, 1e-05), {})
cnt: 1, ((T([1, 64, 128], f16), T([64], f16), None, None, None, True, 0.0, 1e-05), {})
cnt: 2, ((T([1, 64, 576], f16), T([64], f16), None, None, None, True, 0.0, 1e-05), {})
cnt: 1, ((T([1, 256, 64], f16), T([256], f16), None, None, None, True, 0.0, 1e-05), {})
cnt: 1, ((T([1, 512, 256], f16), T([512], f16), None, None, None, True, 0.0, 1e-05), {})
cnt: 1, ((T([1, 128, 256], f16), T([128], f16), None, None, None, True, 0.0, 1e-05), {})
cnt: 2, ((T([1, 512, 128], f16), T([512], f16), None, None, None, True, 0.0, 1e-05), {})
cnt: 1, ((T([1, 128, 512], f16), T([128], f16), None, None, None, True, 0.0, 1e-05), {})
cnt: 1, ((T([1, 1536, 512], f16), T([1536], f16), None, None, None, True, 0.0, 1e-05), {})
cnt: 1, ((T([1, 384, 512], f16), T([384], f16), None, None, None, True, 0.0, 1e-05), {})
cnt: 18, ((T([1, 384, 576], f16), T([384], f16), None, None, None, True, 0.0, 1e-05), {})
cnt: 9, ((T([1, 1536, 384], f16), T([1536], f16), None, None, None, True, 0.0, 1e-05), {})
cnt: 8, ((T([1, 384, 1536], f16), T([384], f16), None, None, None, True, 0.0, 1e-05), {})
cnt: 1, ((T([1, 1536, 1536], f16), T([1536], f16), None, None, None, True, 0.0, 1e-05), {})
cnt: 1, ((T([1, 2304, 1536], f16), T([2304], f16), None, None, None, True, 0.0, 1e-05), {})
```

----------------------------------------

TITLE: Conditionally Add FXdiv Subdirectory CMake
DESCRIPTION: Includes the FXdiv subdirectory for building, but only if the compiler is not MSVC and the `USE_XNNPACK` flag is enabled. It also ensures the target `fxdiv` doesn't already exist and disables its tests and benchmarks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/caffe2/CMakeLists.txt#_snippet_8

LANGUAGE: CMake
CODE:
```
if(NOT MSVC AND USE_XNNPACK)
  if(NOT TARGET fxdiv)
    set(FXDIV_BUILD_TESTS OFF CACHE BOOL "")
    set(FXDIV_BUILD_BENCHMARKS OFF CACHE BOOL "")
    add_subdirectory(
      "${FXDIV_SOURCE_DIR}"
      "${CMAKE_BINARY_DIR}/FXdiv")
  endif()
endif()
```

----------------------------------------

TITLE: Configuring CMake with CCache Launcher (Bash)
DESCRIPTION: Exports environment variables (`CMAKE_C_COMPILER_LAUNCHER`, `CMAKE_CXX_COMPILER_LAUNCHER`, `CMAKE_CUDA_COMPILER_LAUNCHER`) to instruct CMake and `setup.py` to use ccache as the compiler launcher. This integrates ccache into the build process to cache compilation outputs.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_47

LANGUAGE: bash
CODE:
```
export CMAKE_C_COMPILER_LAUNCHER=ccache
export CMAKE_CXX_COMPILER_LAUNCHER=ccache
export CMAKE_CUDA_COMPILER_LAUNCHER=ccache
python setup.py develop
```

----------------------------------------

TITLE: Analyzing PyTorch Operator Usage in ResNet-like Model
DESCRIPTION: This code snippet represents a summary of PyTorch operator usage in a deep learning model, likely a ResNet variant. It includes convolution, batch normalization, pooling, and activation operations with their respective tensor shapes and data types.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_resnest_training.txt#2025-04-22_snippet_10

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([32, 32, 112, 112], f16), T([32, 32, 112, 112], f16), T([32, 32, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([32, 32, 112, 112], f16), T([32, 3, 224, 224], f16), T([32, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})
Operator: aten.copy_.default
cnt: 1, ((T([32, 3, 224, 224], f16), T([32, 3, 224, 224], f16)), {})
Operator: aten.div.Scalar
cnt: 1, ((T([32, 2048, 7, 7], f16, stride=(2048, 1, 0, 0)), 49), {})
cnt: 1, ((T([32, 512, 14, 14], f16, stride=(512, 1, 0, 0)), 196), {})
cnt: 1, ((T([32, 256, 28, 28], f16, stride=(256, 1, 0, 0)), 784), {})
cnt: 1, ((T([32, 128, 56, 56], f16, stride=(128, 1, 0, 0)), 3136), {})
cnt: 1, ((T([32, 64, 56, 56], f16, stride=(64, 1, 0, 0)), 3136), {})
Operator: aten.div.Tensor
cnt: 2, ((T([], f16), 32000), {})
Operator: aten.max_pool2d_with_indices.default
cnt: 1, ((T([32, 64, 112, 112], f16), [3, 3], [2, 2], [1, 1]), {})
Operator: aten.max_pool2d_with_indices_backward.default
cnt: 1, ((T([32, 64, 56, 56], f16), T([32, 64, 112, 112], f16), [3, 3], [2, 2], [1, 1], [1, 1], False, T([32, 64, 56, 56], i64)), {})
Operator: aten.mean.dim
cnt: 1, ((T([32, 64, 56, 56], f16), [2, 3], True), {})
cnt: 1, ((T([32, 128, 56, 56], f16), [2, 3], True), {})
cnt: 1, ((T([32, 256, 28, 28], f16), [2, 3], True), {})
cnt: 1, ((T([32, 512, 14, 14], f16), [2, 3], True), {})
cnt: 1, ((T([32, 2048, 7, 7], f16), [-1, -2], True), {})
Operator: aten.mm.default
cnt: 1, ((T([32, 1000], f16, stride=(0, 0)), T([1000, 2048], f16)), {})
cnt: 1, ((T([1000, 32], f16, stride=(0, 0)), T([32, 2048], f16)), {})
Operator: aten.mul.Tensor
cnt: 1, ((T([32, 2, 64, 56, 56], f16), T([32, 2, 64, 1, 1], f16)), {})
cnt: 1, ((T([32, 2, 128, 56, 56], f16), T([32, 2, 128, 1, 1], f16)), {})
cnt: 1, ((T([32, 2, 256, 28, 28], f16), T([32, 2, 256, 1, 1], f16)), {})
cnt: 1, ((T([32, 2, 512, 14, 14], f16), T([32, 2, 512, 1, 1], f16)), {})
cnt: 1, ((T([32, 2, 512, 14, 14], f16, stride=(100352, 0, 196, 14, 1)), T([32, 2, 512, 14, 14], f16)), {})
cnt: 1, ((T([32, 2, 512, 14, 14], f16, stride=(100352, 0, 196, 14, 1)), T([32, 2, 512, 1, 1], f16)), {})
cnt: 1, ((T([32, 2, 256, 28, 28], f16, stride=(200704, 0, 784, 28, 1)), T([32, 2, 256, 28, 28], f16)), {})
cnt: 1, ((T([32, 2, 256, 28, 28], f16, stride=(200704, 0, 784, 28, 1)), T([32, 2, 256, 1, 1], f16)), {})
cnt: 1, ((T([32, 2, 128, 56, 56], f16, stride=(401408, 0, 3136, 56, 1)), T([32, 2, 128, 56, 56], f16)), {})
cnt: 1, ((T([32, 2, 128, 56, 56], f16, stride=(401408, 0, 3136, 56, 1)), T([32, 2, 128, 1, 1], f16)), {})
cnt: 1, ((T([32, 2, 64, 56, 56], f16, stride=(200704, 0, 3136, 56, 1)), T([32, 2, 64, 56, 56], f16)), {})
cnt: 1, ((T([32, 2, 64, 56, 56], f16, stride=(200704, 0, 3136, 56, 1)), T([32, 2, 64, 1, 1], f16)), {})
Operator: aten.native_batch_norm.default
cnt: 2, ((T([32, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([32, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([32, 64, 56, 56], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})
cnt: 2, ((T([32, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([32, 32, 1, 1], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), False, 0.1, 1e-05), {})
cnt: 3, ((T([32, 256, 56, 56], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([32, 64, 1, 1], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})
cnt: 3, ((T([32, 512, 28, 28], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([32, 256, 28, 28], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([32, 128, 1, 1], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), False, 0.1, 1e-05), {})
cnt: 3, ((T([32, 1024, 14, 14], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([32, 512, 14, 14], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([32, 256, 1, 1], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), False, 0.1, 1e-05), {})
cnt: 2, ((T([32, 2048, 7, 7], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f16), False, 0.1, 1e-05), {})
Operator: aten.native_batch_norm_backward.default
cnt: 2, ((T([32, 2048, 7, 7], f16), T([32, 2048, 7, 7], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f32), T([2048], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([32, 256, 1, 1], f16), T([32, 256, 1, 1], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), False, 1e-05, [True, True, True]), {})
cnt: 3, ((T([32, 1024, 14, 14], f16), T([32, 1024, 14, 14], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([32, 512, 14, 14], f16), T([32, 512, 14, 14], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([32, 128, 1, 1], f16), T([32, 128, 1, 1], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), False, 1e-05, [True, True, True]), {})
cnt: 3, ((T([32, 512, 28, 28], f16), T([32, 512, 28, 28], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([32, 256, 28, 28], f16), T([32, 256, 28, 28], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([32, 64, 1, 1], f16), T([32, 64, 1, 1], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})
cnt: 3, ((T([32, 256, 56, 56], f16), T([32, 256, 56, 56], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), False, 1e-05, [True, True, True]), {})
cnt: 2, ((T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([32, 32, 1, 1], f16), T([32, 32, 1, 1], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([32, 64, 56, 56], f16), T([32, 64, 56, 56], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([32, 64, 112, 112], f16), T([32, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})
cnt: 2, ((T([32, 32, 112, 112], f16), T([32, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), False, 1e-05, [True, True, True]), {})
Operator: aten.relu_.default
cnt: 2, ((T([32, 32, 112, 112], f16),), {})
cnt: 1, ((T([32, 64, 112, 112], f16),), {})
cnt: 1, ((T([32, 64, 56, 56], f16),), {})
cnt: 2, ((T([32, 128, 56, 56], f16),), {})
cnt: 1, ((T([32, 32, 1, 1], f16),), {})
cnt: 2, ((T([32, 256, 56, 56], f16),), {})
cnt: 1, ((T([32, 64, 1, 1], f16),), {})
cnt: 2, ((T([32, 512, 28, 28], f16),), {})
cnt: 1, ((T([32, 256, 28, 28], f16),), {})
cnt: 1, ((T([32, 128, 1, 1], f16),), {})
cnt: 2, ((T([32, 1024, 14, 14], f16),), {})
cnt: 1, ((T([32, 512, 14, 14], f16),), {})
cnt: 1, ((T([32, 256, 1, 1], f16),), {})
cnt: 1, ((T([32, 2048, 7, 7], f16),), {})
Operator: aten.sum.SymInt
cnt: 1, ((T([32, 1000], f16, stride=(0, 0)), [0], True), {})
cnt: 1, ((T([32, 2, 512, 14, 14], f16), [3, 4], True), {})
cnt: 1, ((T([32, 2, 256, 28, 28], f16), [3, 4], True), {})
cnt: 1, ((T([32, 2, 128, 56, 56], f16), [3, 4], True), {})
cnt: 1, ((T([32, 2, 64, 56, 56], f16), [3, 4], True), {})
Operator: aten.sum.default
cnt: 1, ((T([32, 1000], f16),), {})
Operator: aten.sum.dim_IntList
cnt: 2, ((T([32, 2, 64, 56, 56], f16), [1]), {})
cnt: 2, ((T([32, 2, 128, 56, 56], f16), [1]), {})
cnt: 2, ((T([32, 2, 256, 28, 28], f16), [1]), {})
cnt: 2, ((T([32, 2, 512, 14, 14], f16), [1]), {})
Operator: aten.threshold_backward.default
cnt: 1, ((T([32, 2048, 7, 7], f16), T([32, 2048, 7, 7], f16), 0), {})
cnt: 1, ((T([32, 256, 1, 1], f16), T([32, 256, 1, 1], f16), 0), {})
cnt: 2, ((T([32, 1024, 14, 14], f16), T([32, 1024, 14, 14], f16), 0), {})
cnt: 1, ((T([32, 512, 14, 14], f16), T([32, 512, 14, 14], f16), 0), {})
cnt: 1, ((T([32, 128, 1, 1], f16), T([32, 128, 1, 1], f16), 0), {})
cnt: 2, ((T([32, 512, 28, 28], f16), T([32, 512, 28, 28], f16), 0), {})
cnt: 1, ((T([32, 256, 28, 28], f16), T([32, 256, 28, 28], f16), 0), {})
cnt: 1, ((T([32, 64, 1, 1], f16), T([32, 64, 1, 1], f16), 0), {})
cnt: 2, ((T([32, 256, 56, 56], f16), T([32, 256, 56, 56], f16), 0), {})
cnt: 2, ((T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16), 0), {})
cnt: 1, ((T([32, 32, 1, 1], f16), T([32, 32, 1, 1], f16), 0), {})
cnt: 1, ((T([32, 64, 56, 56], f16), T([32, 64, 56, 56], f16), 0), {})
cnt: 1, ((T([32, 64, 112, 112], f16), T([32, 64, 112, 112], f16), 0), {})
cnt: 2, ((T([32, 32, 112, 112], f16), T([32, 32, 112, 112], f16), 0), {})
```

----------------------------------------

TITLE: Applying In-place ReLU Activation in PyTorch
DESCRIPTION: This snippet shows the tensor shapes for in-place ReLU activation in a PyTorch model. It demonstrates the operation on tensors of various sizes, likely corresponding to different layers in the network.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_inception_v3_training.txt#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([128, 32, 149, 149], f16),), {})
```

----------------------------------------

TITLE: Invoking aten.sum.SymInt Sum Reduction in PyTorch ATen
DESCRIPTION: Documents observed calls to the sum reduction operation (`aten.sum.SymInt`) in PyTorch ATen. It lists different input tensor shapes (f16), the dimensions to sum over (e.g., [0] or [2, 3]), whether to keep the dimensions (True), and the respective call counts.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_23

LANGUAGE: text
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([128, 1000], f16), [0], True), {})
cnt: 3, ((T([128, 1536, 7, 7], f16), [2, 3], True), {})
cnt: 6, ((T([128, 1536, 14, 14], f16), [2, 3], True), {})
cnt: 2, ((T([128, 512, 28, 28], f16), [2, 3], True), {})
cnt: 1, ((T([128, 256, 56, 56], f16), [2, 3], True), {})
```

----------------------------------------

TITLE: Analyzing ReLU Activation Usage in PyTorch
DESCRIPTION: This code snippet shows the input tensor shapes for the ReLU activation function in PyTorch. It includes counts of how many times each unique shape is used, both for the default ReLU and the in-place ReLU_ operation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v3_large_training.txt#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
Operator: aten.relu.default
cnt: 1, ((T([32, 24, 1, 1], f16),), {})
cnt: 2, ((T([32, 32, 1, 1], f16),), {})
cnt: 1, ((T([32, 120, 1, 1], f16),), {})
cnt: 2, ((T([32, 168, 1, 1], f16),), {})
cnt: 2, ((T([32, 240, 1, 1], f16),), {})

Operator: aten.relu_.default
cnt: 1, ((T([32, 16, 112, 112], f16),), {})
cnt: 1, ((T([32, 64, 112, 112], f16),), {})
cnt: 1, ((T([32, 64, 56, 56], f16),), {})
cnt: 3, ((T([32, 72, 56, 56], f16),), {})
cnt: 1, ((T([32, 72, 28, 28], f16),), {})
cnt: 4, ((T([32, 120, 28, 28], f16),), {})
```

----------------------------------------

TITLE: Executing Tensor Addition Using ATen
DESCRIPTION: Tensor addition operations are carried out using the aten.add.Tensor operator, requiring PyTorch. This operator performs element-wise or scalar addition on tensors, resulting in a tensor with the modified dimensionality or values based on input operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForConditionalGeneration_training.txt#2025-04-22_snippet_5

LANGUAGE: Python
CODE:
```
Operator: aten.add.Tensor
cnt: 2, ((T([8, 128], i64, stride=(0, 1)), 2), {})
cnt: 97, ((T([8, 128, 768], f16), T([8, 128, 768], f16)), {})
cnt: 1, ((T([128], i64), 1), {})
cnt: 6, ((T([8, 12, 128, 128], f16), T([8, 1, 128, 128], f16)), {})
cnt: 1, ((T([8, 128, 50005], f16), T([1, 50005], f16)), {})
cnt: 2, ((T([50005, 768], f16), T([50005, 768], f16)), {})
```

----------------------------------------

TITLE: Installing MacOS Dependencies (MKL Intel x86) Bash
DESCRIPTION: Installs Intel MKL static and include files specifically for macOS systems running on Intel x86 processors, required for optimized numerical operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_7

LANGUAGE: Bash
CODE:
```
# Add this package on intel x86 processor machines only
pip install mkl-static mkl-include
```

----------------------------------------

TITLE: Add Non-MSVC Error-Promoting Compiler Warnings (CMake)
DESCRIPTION: Adds specific warning flags that are often treated as errors to the C++ compiler flags for non-MSVC builds, only if the compiler supports them.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#_snippet_11

LANGUAGE: CMake
CODE:
```
append_cxx_flag_if_supported("-Werror=return-type" CMAKE_CXX_FLAGS)
  append_cxx_flag_if_supported("-Werror=non-virtual-dtor" CMAKE_CXX_FLAGS)
  append_cxx_flag_if_supported("-Werror=braced-scalar-init" CMAKE_CXX_FLAGS)
  append_cxx_flag_if_supported("-Werror=range-loop-construct" CMAKE_CXX_FLAGS)
  append_cxx_flag_if_supported("-Werror=bool-operation" CMAKE_CXX_FLAGS)
  append_cxx_flag_if_supported("-Wnarrowing" CMAKE_CXX_FLAGS)
```

----------------------------------------

TITLE: Merging Multi-GPU Training Data
DESCRIPTION: Python command to merge training data collected from multiple GPUs into a single file
SOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/mm/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
python torchgen/_autuoheuristic/merge_data.py mm_train.txt data_6.txt data_7.txt
```

----------------------------------------

TITLE: Tensor Operations with Convolutions
DESCRIPTION: Tensor operations involving convolutions with different input shapes, strides and parameters. Operations use float16 (f16) data type and include various tensor transformations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/twins_pcpvt_base_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
T([32, 320, 14, 14], f16), T([32, 128, 28, 28], f16), T([320, 128, 2, 2], f16)
```

----------------------------------------

TITLE: Analyzing PyTorch Operator Usage Patterns in Neural Network Execution
DESCRIPTION: This data represents a profiling report of PyTorch operators used during model execution, showing tensor shapes, counts, and data types. The statistics include common neural network operations like convolutions, softmax, pooling, and matrix multiplications with their specific tensor dimensions and parameters.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/botnet26t_256_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([128, 1000], f16), 1, False), {})
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})
Operator: aten._softmax.default
cnt: 2, ((T([512, 256, 256], f16), -1, False), {})
cnt: 1, ((T([512, 64, 64], f16), -1, False), {})
Operator: aten._softmax_backward_data.default
cnt: 1, ((T([512, 64, 64], f16), T([512, 64, 64], f16), -1, f16), {})
cnt: 2, ((T([512, 256, 256], f16), T([512, 256, 256], f16), -1, f16), {})
Operator: aten._unsafe_view.default
cnt: 3, ((T([128, 256, 16, 16], f16), [512, 64, 256]), {})
cnt: 2, ((T([512, 256, 256], f16), [512, 256, 256]), {})
cnt: 2, ((T([512, 16, 16, 64], f16), [131072, 64]), {})
cnt: 4, ((T([131072, 31], f16), [512, 16, 16, 31]), {})
cnt: 2, ((T([512, 16, 16, 16, 16], f16), [512, 256, 256]), {})
cnt: 1, ((T([512, 256, 64], f16), [512, 256, 64]), {})
cnt: 3, ((T([512, 64, 256], f16), [128, 256, 16, 16]), {})
cnt: 3, ((T([128, 512, 16, 16], f16), [512, 128, 256]), {})
cnt: 2, ((T([512, 16, 16, 128], f16), [131072, 128]), {})
cnt: 1, ((T([512, 256, 128], f16), [512, 256, 128]), {})
cnt: 3, ((T([512, 128, 256], f16), [128, 512, 16, 16]), {})
cnt: 3, ((T([128, 512, 8, 8], f16), [512, 128, 64]), {})
cnt: 1, ((T([512, 64, 64], f16), [512, 64, 64]), {})
cnt: 2, ((T([512, 8, 8, 128], f16), [32768, 128]), {})
cnt: 2, ((T([32768, 15], f16), [512, 8, 8, 15]), {})
cnt: 1, ((T([512, 8, 8, 8, 8], f16), [512, 64, 64]), {})
cnt: 1, ((T([512, 64, 128], f16), [512, 64, 128]), {})
cnt: 3, ((T([512, 128, 64], f16), [128, 512, 8, 8]), {})
cnt: 1, ((T([512, 8, 8, 128], f16), [512, 64, 128]), {})
cnt: 1, ((T([512, 16, 16, 128], f16), [512, 256, 128]), {})
cnt: 1, ((T([512, 16, 16, 64], f16), [512, 256, 64]), {})
Operator: aten.add.Tensor
cnt: 31, ((T([], i64), 1), {})
cnt: 4, ((T([128, 256, 64, 64], f16), T([128, 256, 64, 64], f16)), {})
cnt: 4, ((T([128, 512, 32, 32], f16), T([128, 512, 32, 32], f16)), {})
cnt: 4, ((T([128, 1024, 16, 16], f16), T([128, 1024, 16, 16], f16)), {})
cnt: 2, ((T([512, 16, 16, 16, 16], f16, stride=(8432, 31, 527, 1, 0)), T([512, 16, 16, 16, 16], f16, stride=(8432, 527, 31, 0, 1))), {})
cnt: 2, ((T([512, 256, 256], f16), T([512, 256, 256], f16)), {})
cnt: 3, ((T([128, 2048, 8, 8], f16), T([128, 2048, 8, 8], f16)), {})
cnt: 1, ((T([512, 8, 8, 8, 8], f16, stride=(1080, 15, 135, 1, 0)), T([512, 8, 8, 8, 8], f16, stride=(1080, 135, 15, 0, 1))), {})
cnt: 1, ((T([512, 64, 64], f16), T([512, 64, 64], f16)), {})
cnt: 1, ((T([512, 8, 8, 128], f16, stride=(8192, 128, 1024, 1)), T([512, 8, 8, 128], f16)), {})
cnt: 1, ((T([512, 64, 128], f16), T([512, 64, 128], f16)), {})
cnt: 1, ((T([512, 16, 16, 128], f16, stride=(32768, 128, 2048, 1)), T([512, 16, 16, 128], f16)), {})
cnt: 1, ((T([512, 256, 128], f16), T([512, 256, 128], f16)), {})
cnt: 1, ((T([512, 16, 16, 64], f16, stride=(16384, 64, 1024, 1)), T([512, 16, 16, 64], f16)), {})
cnt: 1, ((T([512, 256, 64], f16), T([512, 256, 64], f16)), {})
cnt: 1, ((T([128, 64, 64, 64], f16), T([128, 64, 64, 64], f16)), {})
Operator: aten.addmm.default
cnt: 1, ((T([1000], f16), T([128, 2048], f16), T([2048, 1000], f16, stride=(1, 2048))), {})
Operator: aten.avg_pool2d.default
cnt: 1, ((T([128, 512, 16, 16], f16), [2, 2], [2, 2]), {})
Operator: aten.avg_pool2d_backward.default
cnt: 1, ((T([128, 512, 8, 8], f16), T([128, 512, 16, 16], f16), [2, 2], [2, 2], [0, 0], False, True, None), {})
Operator: aten.bmm.default
cnt: 2, ((T([512, 256, 64], f16, stride=(16384, 1, 256)), T([512, 64, 256], f16)), {})
cnt: 2, ((T([512, 256, 256], f16), T([512, 256, 64], f16, stride=(16384, 1, 256))), {})
cnt: 2, ((T([512, 256, 128], f16, stride=(32768, 1, 256)), T([512, 128, 256], f16)), {})
cnt: 2, ((T([512, 256, 256], f16), T([512, 256, 128], f16, stride=(32768, 1, 256))), {})
cnt: 2, ((T([512, 64, 128], f16, stride=(8192, 1, 64)), T([512, 128, 64], f16)), {})
cnt: 2, ((T([512, 64, 64], f16), T([512, 64, 128], f16, stride=(8192, 1, 64))), {})
cnt: 1, ((T([512, 64, 64], f16, stride=(4096, 1, 64)), T([512, 64, 128], f16, stride=(8192, 1, 64))), {})
cnt: 1, ((T([512, 128, 64], f16), T([512, 64, 64], f16)), {})
cnt: 1, ((T([512, 256, 256], f16, stride=(65536, 1, 256)), T([512, 256, 128], f16, stride=(32768, 1, 256))), {})
cnt: 1, ((T([512, 128, 256], f16), T([512, 256, 256], f16)), {})
cnt: 1, ((T([512, 256, 256], f16, stride=(65536, 1, 256)), T([512, 256, 64], f16, stride=(16384, 1, 256))), {})
cnt: 1, ((T([512, 64, 256], f16), T([512, 256, 256], f16)), {})
Operator: aten.cat.default
cnt: 1, (([T([128, 512, 8, 8], f16), T([128, 512, 8, 8], f16), T([128, 512, 8, 8], f16)], 1), {})
cnt: 1, (([T([128, 512, 16, 16], f16), T([128, 512, 16, 16], f16), T([128, 512, 16, 16], f16)], 1), {})
cnt: 1, (([T([128, 256, 16, 16], f16), T([128, 256, 16, 16], f16), T([128, 256, 16, 16], f16)], 1), {})
Operator: aten.clone.default
cnt: 1, ((T([128, 3, 256, 256], f16),), {})
Operator: aten.constant_pad_nd.default
cnt: 4, ((T([8192, 16, 31], f16), [0, 1], 0.0), {})
cnt: 4, ((T([8192, 512], f16), [0, 15], 0.0), {})
cnt: 2, ((T([4096, 8, 15], f16), [0, 1], 0.0), {})
cnt: 2, ((T([4096, 128], f16), [0, 7], 0.0), {})
cnt: 2, ((T([4096, 135], f16), [0, -7]), {})
cnt: 2, ((T([4096, 8, 16], f16), [0, -1]), {})
cnt: 4, ((T([8192, 527], f16), [0, -15]), {})
cnt: 4, ((T([8192, 16, 32], f16), [0, -1]), {})
Operator: aten.convolution.default
cnt: 1, ((T([128, 3, 256, 256], f16), T([24, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 24, 128, 128], f16), T([32, 24, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 32, 128, 128], f16), T([64, 32, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 64, 64, 64], f16), T([64, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 64, 64, 64], f16), T([64, 64, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([128, 64, 64, 64], f16), T([256, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 256, 64, 64], f16), T([64, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 256, 64, 64], f16), T([128, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 128, 64, 64], f16), T([128, 128, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 128, 32, 32], f16), T([512, 128, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 256, 64, 64], f16), T([512, 256, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 512, 32, 32], f16), T([128, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 128, 32, 32], f16), T([128, 128, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 512, 32, 32], f16), T([256, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 256, 32, 32], f16), T([256, 256, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 256, 16, 16], f16), T([1024, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 512, 32, 32], f16), T([1024, 512, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 1024, 16, 16], f16), T([256, 1024, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 256, 16, 16], f16), T([768, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 1024, 16, 16], f16), T([512, 1024, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 512, 16, 16], f16), T([1536, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 512, 8, 8], f16), T([2048, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 1024, 16, 16], f16), T([2048, 1024, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 2048, 8, 8], f16), T([512, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 512, 8, 8], f16), T([1536, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
Operator: aten.convolution_backward.default
cnt: 2, ((T([128, 2048, 8, 8], f16), T([128, 512, 8, 8], f16), T([2048, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 1536, 8, 8], f16), T([128, 512, 8, 8], f16), T([1536, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 512, 8, 8], f16), T([128, 2048, 8, 8], f16), T([512, 2048, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 2048, 8, 8], f16), T([128, 1024, 16, 16], f16), T([2048, 1024, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 1536, 16, 16], f16), T([128, 512, 16, 16], f16), T([1536, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 512, 16, 16], f16), T([128, 1024, 16, 16], f16), T([512, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 1024, 16, 16], f16), T([128, 256, 16, 16], f16), T([1024, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 768, 16, 16], f16), T([128, 256, 16, 16], f16), T([768, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 256, 16, 16], f16), T([128, 1024, 16, 16], f16), T([256, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 1024, 16, 16], f16), T([128, 512, 32, 32], f16), T([1024, 512, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

----------------------------------------

TITLE: Backward Convolution Operations in PyTorch MobileNetV3
DESCRIPTION: Statistics for backward pass convolution operations used during training. These compute gradients with respect to inputs and weights for all the convolution layers in the network, following the same structure as the forward pass but in reverse order.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
Operator: aten.convolution_backward.default
cnt: 1, ((T([128, 1984, 7, 7], f16), T([128, 352, 7, 7], f16), T([1984, 352, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 352, 7, 7], f16), T([128, 1104, 7, 7], f16), T([352, 1104, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 1104, 7, 7], f16), T([128, 1104, 7, 7], f16), T([1104, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1104, [True, True, False]), {})
cnt: 4, ((T([128, 1104, 7, 7], f16), T([128, 184, 7, 7], f16), T([1104, 184, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 3, ((T([128, 184, 7, 7], f16), T([128, 1104, 7, 7], f16), T([184, 1104, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 3, ((T([128, 1104, 7, 7], f16), T([128, 1104, 7, 7], f16), T([1104, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 1104, [True, True, False]), {})
cnt: 1, ((T([128, 184, 7, 7], f16), T([128, 672, 7, 7], f16), T([184, 672, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 672, 7, 7], f16), T([128, 672, 14, 14], f16), T([672, 1, 5, 5], f16), [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 672, [True, True, False]), {})
cnt: 3, ((T([128, 672, 14, 14], f16), T([128, 112, 14, 14], f16), T([672, 112, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 112, 14, 14], f16), T([128, 336, 14, 14], f16), T([112, 336, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 336, 14, 14], f16), T([128, 336, 14, 14], f16), T([336, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 336, [True, True, False]), {})
cnt: 1, ((T([128, 336, 14, 14], f16), T([128, 112, 14, 14], f16), T([336, 112, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 112, 14, 14], f16), T([128, 672, 14, 14], f16), T([112, 672, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 672, 14, 14], f16), T([128, 672, 14, 14], f16), T([672, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 672, [True, True, False]), {})
cnt: 1, ((T([128, 112, 14, 14], f16), T([128, 384, 14, 14], f16), T([112, 384, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 3, ((T([128, 384, 14, 14], f16), T([128, 384, 14, 14], f16), T([384, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 384, [True, True, False]), {})
cnt: 3, ((T([128, 384, 14, 14], f16), T([128, 64, 14, 14], f16), T([384, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 64, 14, 14], f16), T([128, 384, 14, 14], f16), T([64, 384, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 64, 14, 14], f16), T([128, 192, 14, 14], f16), T([64, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 192, 14, 14], f16), T([128, 192, 14, 14], f16), T([192, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 192, [True, True, False]), {})
cnt: 1, ((T([128, 192, 14, 14], f16), T([128, 64, 14, 14], f16), T([192, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 192, 14, 14], f16), T([128, 192, 28, 28], f16), T([192, 1, 5, 5], f16), [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 192, [True, True, False]), {})
cnt: 3, ((T([128, 192, 28, 28], f16), T([128, 32, 28, 28], f16), T([192, 32, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 32, 28, 28], f16), T([128, 192, 28, 28], f16), T([32, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 192, 28, 28], f16), T([128, 192, 28, 28], f16), T([192, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 192, [True, True, False]), {})
cnt: 1, ((T([128, 192, 28, 28], f16), T([128, 192, 28, 28], f16), T([192, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 192, [True, True, False]), {})
```

----------------------------------------

TITLE: Implementing Nested Functions with Conditional Logic in Python
DESCRIPTION: This code snippet defines a function 'top' that creates nested functions based on input numbers. It demonstrates complex nesting and conditional function definitions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/block_names.py.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
def top(number):
    if number == 0:

        def fun():
            if number == 10:
                def sab():
                    return 1
            else:
                def sub():
                    return 2
            return sub

    elif number == 1:

        def fun():
            if number == 11:
                def sub():
                    return 3
            else:
                def sub():
                    return 4
            return sub

    elif number == 2:

        def fun():
            if number == 12:
                def sub():
                    return 5
            else:
                def sab():
                    return 6
            return sub

    elif number == 3:

        def run():
            if number == 12:
                def sub():
                    return 5
            else:
                def sub():
                    return 6
            return sub
```

----------------------------------------

TITLE: Handling Python Errors in C++ Code
DESCRIPTION: This snippet demonstrates how to handle errors when writing Python bindings by hand. It uses HANDLE_TH_ERRORS and END_HANDLE_TH_ERRORS macros to catch exceptions and convert them to appropriate Python signals.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/README.md#2025-04-22_snippet_0

LANGUAGE: C++
CODE:
```
// Entry point from Python interpreter
PyObject* run(PyObject* arg) {
  HANDLE_TH_ERRORS
  ...
  if (!x) throw python_error();
  // From c10/Exception.h
  TORCH_CHECK(cond, "cond was false here");
  TORCH_WARN("Warning message");
  ...
  END_HANDLE_TH_ERRORS
}
```

----------------------------------------

TITLE: Tensor Reduction Operations
DESCRIPTION: Sum operations across specified dimensions with various tensor shapes, mainly used for spatial dimension reduction in convolution networks.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_efficientnet_b0_training.txt#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
((T([128, 1000], f16), [0], True), {})
```

----------------------------------------

TITLE: Analyzing ATen Log Softmax Operations in PyTorch
DESCRIPTION: This snippet captures the use of the ATen log softmax operator on a tensor with specified dimensions and data type. The operation is applied with a dimension and a flag indicating certain preferences. The snippet focuses on a function's application rather than its complete definition or algorithmic details.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistilBertForMaskedLM_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([2048, 30522], f16), 1, False), {})
```

----------------------------------------

TITLE: Analyzing Convolution Operations in PyTorch MobileNet Model
DESCRIPTION: This snippet shows convolution operations with different tensor shapes in a MobileNet model. Each line shows a count (cnt) followed by input/output tensor shapes, filter dimensions, stride, padding and other convolution parameters in f16 precision.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 32, 28, 28], f16), T([128, 96, 28, 28], f16), T([32, 96, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 96, 28, 28], f16), T([128, 96, 28, 28], f16), T([96, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 96, [True, True, False]), {})
cnt: 1, ((T([128, 96, 28, 28], f16), T([128, 32, 28, 28], f16), T([96, 32, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 32, 28, 28], f16), T([128, 144, 28, 28], f16), T([32, 144, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 144, 28, 28], f16), T([128, 144, 56, 56], f16), T([144, 1, 5, 5], f16), [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 144, [True, True, False]), {})
cnt: 1, ((T([128, 144, 56, 56], f16), T([128, 24, 56, 56], f16), T([144, 24, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 4, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16), T([24, 24, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16), T([24, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 24, [True, True, False]), {})
cnt: 1, ((T([128, 24, 56, 56], f16), T([128, 96, 56, 56], f16), T([24, 96, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 96, 56, 56], f16), T([128, 96, 112, 112], f16), T([96, 1, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 96, [True, True, False]), {})
cnt: 1, ((T([128, 96, 112, 112], f16), T([128, 16, 112, 112], f16), T([96, 16, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 2, ((T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16), T([16, 16, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16), T([16, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 16, [True, True, False]), {})
cnt: 1, ((T([128, 16, 112, 112], f16), T([128, 3, 224, 224], f16), T([16, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})
```

----------------------------------------

TITLE: Tracking Convolution Backward Operations in PyTorch
DESCRIPTION: Documentation of backward pass operations for convolutions with their gradient tensors, weights, and configuration parameters. These operations handle both 3x3 and 1x1 convolutions at different spatial resolutions with half-precision (f16) tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/densenet121_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
Operator: aten.convolution_backward.default
cnt: 1, ((T([4, 32, 7, 7], f16, stride=(50176, 49, 7, 1)), T([4, 128, 7, 7], f16), T([32, 128, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

----------------------------------------

TITLE: Profiling PyTorch Unfold Backward Operator - Python
DESCRIPTION: This snippet defines batched test cases for the ATen unfold_backward.default operator, focusing on unfolded views on higher-dimensional tensors (common in convolutional backprop). Each case specifies input/output tensor shapes, dimensions to unfold, size, and step, generally targeting gradient recovery for im2col-like operations. Inputs are tuples of shapes/parameters. Requires PyTorch ATen backend.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_16

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 640, 1, 1, 12, 12], f16), [128, 640, 1, 12, 12], 3, 12, 8), {})
cnt: 1, ((T([128, 640, 1, 12, 12], f16), [128, 640, 12, 12], 2, 12, 8), {})
cnt: 1, ((T([128, 640, 2, 2, 12, 12], f16), [128, 640, 2, 20, 12], 3, 12, 8), {})
cnt: 1, ((T([128, 640, 2, 20, 12], f16), [128, 640, 20, 20], 2, 12, 8), {})
cnt: 1, ((T([128, 384, 2, 2, 12, 12], f16), [128, 384, 2, 20, 12], 3, 12, 8), {})
cnt: 1, ((T([128, 384, 2, 20, 12], f16), [128, 384, 20, 20], 2, 12, 8), {})
```

----------------------------------------

TITLE: Defining TorchScript Type System Grammar
DESCRIPTION: Specifies the grammar for TorchScript's type system, including primitive types, structural types, and nominal types. It defines the syntax for various type constructs like tuples, lists, dictionaries, and custom classes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_17

LANGUAGE: TorchScript
CODE:
```
TSPrimitiveType ::= "int" | "float" | "double" | "complex" | "bool" | "str" | "None"

TSStructuralType ::= TSTuple | TSNamedTuple | TSList | TSDict | TSOptional |
                     TSUnion | TSFuture | TSRRef | TSAwait
TSTuple         ::= "Tuple" "[" (TSType ",")* TSType "]"
TSNamedTuple    ::= "namedtuple" "(" (TSType ",")* TSType ")"
TSList          ::= "List" "[" TSType "]"
TSOptional      ::= "Optional" "[" TSType "]"
TSUnion         ::= "Union" "[" (TSType ",")* TSType "]"
TSFuture        ::= "Future" "[" TSType "]"
TSRRef          ::= "RRef" "[" TSType "]"
TSAwait         ::= "Await" "[" TSType "]"
TSDict          ::= "Dict" "[" KeyType "," TSType "]"
KeyType         ::= "str" | "int" | "float" | "bool" | TensorType | "Any"

TSNominalType   ::= TSBuiltinClasses | TSCustomClass | TSEnum
TSBuiltinClass  ::= TSTensor | "torch.device" | "torch.stream"|
                    "torch.dtype" | "torch.nn.ModuleList" |
                    "torch.nn.ModuleDict" | ...
TSTensor        ::= "torch.tensor" and subclasses
```

----------------------------------------

TITLE: Install Python Doc Prerequisites (Bash)
DESCRIPTION: These commands navigate to the `docs` directory and install Python dependencies required for building the documentation using pip and the `requirements.txt` file. It also includes instructions for installing the `katex` tool globally or locally using npm, which is necessary for rendering mathematical formulas.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_23

LANGUAGE: bash
CODE:
```
cd docs
pip install -r requirements.txt
# `katex` must also be available in your PATH.
# You can either install katex globally if you have properly configured npm:
# npm install -g katex
# Or if you prefer an uncontaminated global executable environment or do not want to go through the node configuration:
# npm install katex && export PATH="$PATH:$(pwd)/node_modules/.bin"
```

----------------------------------------

TITLE: Embedding Dense Backward - PyTorch Aten
DESCRIPTION: Computes the gradient for the embedding layer in dense mode. This internal operator is part of the backward pass for the embedding lookup operation. It requires the gradient of the output, the index tensor, the number of embeddings, padding index, and a flag for scaling gradients.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BartForConditionalGeneration_training.txt#_snippet_13

LANGUAGE: Python
CODE:
```
import torch

grad_output = torch.randn(2, 1024, 1024, dtype=torch.float16)
indices = torch.randint(0, 50265, (2, 1024), dtype=torch.int64)
num_embeddings = 50265
weight_grad = torch.embedding_dense_backward(grad_output, indices, num_embeddings, padding_idx=1, scale_grad_by_freq=False)
```

----------------------------------------

TITLE: Configuring Custom TorchScript Operation Library
DESCRIPTION: Builds a shared library for custom TorchScript operations with conditional CUDA/ROCm support and links against LibTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/aoti_inference/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
add_library(aoti_custom_class SHARED aoti_custom_class.cpp)
set_target_properties(aoti_custom_class PROPERTIES
    LIBRARY_OUTPUT_DIRECTORY ${CMAKE_CURRENT_BINARY_DIR})
if(USE_CUDA)
  target_compile_definitions(aoti_custom_class PRIVATE USE_CUDA)
elseif(USE_ROCM)
    target_compile_definitions(aoti_custom_class PRIVATE USE_ROCM)
endif()
target_link_libraries(aoti_custom_class torch)
```

----------------------------------------

TITLE: Conditional Installation of Test Executable in CMake
DESCRIPTION: Conditionally installs the `test_lite_interpreter_runtime` executable target to the `bin` destination directory if the `INSTALL_TEST` CMake option is enabled. Additionally, for MSVC builds using shared libraries (`MSVC` and `BUILD_SHARED_LIBS` are true), it installs the corresponding PDB debug symbol file. Requires the `test_lite_interpreter_runtime` target to exist.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/lite_interpreter_runtime/CMakeLists.txt#2025-04-22_snippet_7

LANGUAGE: cmake
CODE:
```
if(INSTALL_TEST)
  install(TARGETS test_lite_interpreter_runtime DESTINATION bin)
  # Install PDB files for MSVC builds
  if(MSVC AND BUILD_SHARED_LIBS)
    install(
      FILES $<TARGET_PDB_FILE:test_lite_interpreter_runtime>
      DESTINATION bin OPTIONAL)
  endif()
endif()
```

----------------------------------------

TITLE: Negative Log-Likelihood Loss Backward Pass in PyTorch
DESCRIPTION: The aten.nll_loss_backward.default example captures the backpropagation pass for negative log-likelihood loss operations on models, handling float16 tensor inputs and integer targets to compute gradients.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_13

LANGUAGE: Python
CODE:
```
Operator: aten.nll_loss_backward.default
cnt: 2, ((T([], f16), T([1, 512], f16), T([1], i64), None, 1, 512, T([], f16)), {})
```

----------------------------------------

TITLE: Installing CCache Compiler Cache (Bash)
DESCRIPTION: Provides commands to install the ccache compiler cache using various package managers (conda, apt, yum, brew). ccache speeds up rebuilds by caching previous compilation results for identical source files.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_44

LANGUAGE: bash
CODE:
```
conda install ccache -c conda-forge
```

LANGUAGE: bash
CODE:
```
sudo apt install ccache
```

LANGUAGE: bash
CODE:
```
sudo yum install ccache
```

LANGUAGE: bash
CODE:
```
brew install ccache
```

----------------------------------------

TITLE: Generating Custom GPU Mixed MM Heuristics
DESCRIPTION: Processes collected training data to generate new mixed matrix multiplication heuristics for custom GPU configurations
SOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/mixed_mm/README.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
bash generate_heuristic.sh generate
```

----------------------------------------

TITLE: Running Specific Test Method Using Pytest
DESCRIPTION: Command to run a specific test method (multi limit single dtype) in the ProcessGroup gloo/nccl test using pytest.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/distributed/CONTRIBUTING.md#2025-04-22_snippet_4

LANGUAGE: bash
CODE:
```
pytest -vs test/distributed/test_c10d_common.py -k test_multi_limit_single_dtype
```

----------------------------------------

TITLE: Generate Commit for Specific CI Jobs (bash)
DESCRIPTION: Uses the `./tools/testing/explicit_ci_jobs.py` script to generate a commit or tag that restricts the CI system to run only the specified job(s) (`--job`) and filtered GitHub Actions workflows (`--filter-gha`). This is useful for targeting specific CI configurations for debugging or rerunning.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_20

LANGUAGE: bash
CODE:
```
# --job: specify one or more times to filter to a specific job + its dependencies
# --filter-gha: specify github actions workflows to keep
./tools/testing/explicit_ci_jobs.py --job ci-runner-linux-bionic-cuda11.6-py3.9 --filter-gha pull
```

----------------------------------------

TITLE: Matrix Multiplication in PyTorch with Half-Precision Tensors
DESCRIPTION: Matrix multiplication operations with different input shapes. Operations use half-precision (f16) tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/inception_v3_training.txt#2025-04-22_snippet_4

LANGUAGE: Python
CODE:
```
((T([128, 1000], f16), T([1000, 2048], f16)), {})
```

----------------------------------------

TITLE: Using ATen Softmax Backward Data Operator in PyTorch
DESCRIPTION: This operator computes the gradient of the softmax operation using the output tensor from a forward pass. Dependencies are PyTorch and ATen extensions. Parameters include input tensor shapes, the specific dimension, and data type specifications. It outputs the gradient tensor, computed based on the forward softmax output. Ensuring correct dimensions and data types is critical for gradient accuracy.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/ElectraForCausalLM_training.txt#2025-04-22_snippet_3

LANGUAGE: plaintext
CODE:
```
Operator: aten._softmax_backward_data.default
cnt: 12, ((T([1, 4, 512, 512], f16), T([1, 4, 512, 512], f16), -1, f16), {})
```

----------------------------------------

TITLE: Backward Convolution Operations in PyTorch
DESCRIPTION: Multiple backward convolution operations corresponding to the forward passes. These operations compute gradients with respect to inputs, weights, and biases for the convolution layers in the neural network.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
((T([128, 1280, 1, 1], f16), T([128, 960, 1, 1], f16), T([1280, 960, 1, 1], f16), [1280], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})
```

----------------------------------------

TITLE: Run C++ Doxygen Check (Bash)
DESCRIPTION: This command executes a shell script located in the `docs/cpp/source` directory. The script performs a check on the C++ documentation source files to verify that the Doxygen commands and syntax used are valid before building the documentation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_30

LANGUAGE: bash
CODE:
```
./check-doxygen.sh
```

----------------------------------------

TITLE: Tensor Copy and Unsafe View Operations in PyTorch
DESCRIPTION: This snippet showcases `aten._to_copy.default` and `aten._unsafe_view.default` operations. `aten._to_copy.default` is used for copying a tensor to different settings like dtype and layout. `aten._unsafe_view.default` reshapes tensors into specified views. It requires specification of data shape and types.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForCausalLM_training.txt#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
Operator: aten._to_copy.default
cnt: 1, ((T([128, 128], f32),), {\"dtype\": f16})
cnt: 1, ((T([16, 1, 128, 128], f16, stride=(0, 16384, 128, 1)),), {\"dtype\": f16, \"layout\": torch.strided, \"device\": \"cuda\"})
```

LANGUAGE: Python
CODE:
```
Operator: aten._unsafe_view.default
cnt: 18, ((T([16, 128, 12, 64], f16), [16, 128, 768]), {})
cnt: 1, ((T([2048, 50005], f16), [16, 128, 50005]), {})
cnt: 6, ((T([16, 12, 128, 64], f16), [192, 128, 64]), {})
cnt: 6, ((T([16, 128, 768], f16), [2048, 768]), {})
```

----------------------------------------

TITLE: Data Collection Command for AutoHeuristic
DESCRIPTION: Shell command to collect training data for AutoHeuristic. Specifies log path and collection target using environment variables.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/README.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
TORCHINDUCTOR_AUTOHEURISTIC_LOG_PATH="train.txt" \
  TORCHINDUCTOR_AUTOHEURISTIC_COLLECT="pad_mm" python run.py
```

----------------------------------------

TITLE: PyTorch IR Expression (Expr) Definition
DESCRIPTION: Defines the structure of Expression nodes in the PyTorch IR. Expressions represent computations, variables, buffers, and mathematical operations. They serve as building blocks for more complex statements and can be nested within other expressions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/tensorexpr/IRSpecification.md#2025-04-22_snippet_1

LANGUAGE: plaintext
CODE:
```
Expr
= Var()
| Buf(base_handle_ = Var, dims = [Expr], qscale_ = Expr, qzero_ = Expr)
| Term(variables_ = [Expr], scalar_ = Expr)
| Polynomial(variables_ = [Term], scalar_ = Expr)
| MaxTerm(variables_ = [Term], scalar_ = Expr)
| MinTerm(variables_ = [Term], scalar_ = Expr)
| Cast(src_value_ = Expr)
| BitCast(src_value_ = Expr)
| BinaryOpNode(lhs_ = Expr, rhs_ = Expr)
| ImmInt/ImmFloat/etc.()
| Ramp(base_ = Expr, stride_ = Expr)
| Load(buf_ = Buf, indices = [Expr], mask_ = Expr)
| Broadcast(value_ = Expr, lanes_ = int)
| IfThenElse(condition_ = Expr, true_ = Expr, false_ = Expr)
| Intrinsics(op_type_ = {kSin, kPow, kExp, ...}, params_ = [Expr])
| CompareSelect(lhs_ = Expr, rhs_ = Expr, ret_val1_ = Expr, ret_val2_ = Expr, compare_op_ = {kEQ, kGT, kGE, ...}, bias_ = {kUnbiased, kLikely, kUnlikely})
| ReduceOp(body_ = Expr, reduce_args_ = [Var], reducer = Reducer)
```

----------------------------------------

TITLE: Tensor Element-wise Multiplication Statistics
DESCRIPTION: Logs of element-wise multiplication operations (aten.mul.Tensor) between tensors of matching shapes using float16 data type
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
cnt: 2, ((T([128, 228, 28, 28], f16), T([128, 228, 1, 1], f16)), {})
cnt: 2, ((T([128, 300, 28, 28], f16), T([128, 300, 1, 1], f16)), {})
```

----------------------------------------

TITLE: PyTorch Batch Normalization Operations
DESCRIPTION: This snippet shows batch normalization operations on tensors with various shapes in f16 precision. Each operation includes input tensor, scale, bias, running mean, running variance, training mode flag, momentum (0.1), and epsilon (1e-05).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
Operator: aten.native_batch_norm.default
cnt: 4, ((T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 96, 112, 112], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 96, 56, 56], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), True, 0.1, 1e-05), {})
cnt: 7, ((T([128, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 144, 56, 56], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 144, 28, 28], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f16), True, 0.1, 1e-05), {})
cnt: 4, ((T([128, 32, 28, 28], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})
cnt: 2, ((T([128, 96, 28, 28], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), True, 0.1, 1e-05), {})
cnt: 5, ((T([128, 192, 28, 28], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), True, 0.1, 1e-05), {})
cnt: 3, ((T([128, 192, 14, 14], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), True, 0.1, 1e-05), {})
cnt: 4, ((T([128, 64, 14, 14], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})
cnt: 6, ((T([128, 384, 14, 14], f16), T([384], f16), T([384], f16), T([384], f16), T([384], f16), True, 0.1, 1e-05), {})
cnt: 4, ((T([128, 112, 14, 14], f16), T([112], f16), T([112], f16), T([112], f16), T([112], f16), True, 0.1, 1e-05), {})
cnt: 5, ((T([128, 672, 14, 14], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), True, 0.1, 1e-05), {})
cnt: 2, ((T([128, 336, 14, 14], f16), T([336], f16), T([336], f16), T([336], f16), T([336], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 672, 7, 7], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), True, 0.1, 1e-05), {})
cnt: 4, ((T([128, 184, 7, 7], f16), T([184], f16), T([184], f16), T([184], f16), T([184], f16), True, 0.1, 1e-05), {})
cnt: 8, ((T([128, 1104, 7, 7], f16), T([1104], f16), T([1104], f16), T([1104], f16), T([1104], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 352, 7, 7], f16), T([352], f16), T([352], f16), T([352], f16), T([352], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 1984, 7, 7], f16), T([1984], f16), T([1984], f16), T([1984], f16), T([1984], f16), True, 0.1, 1e-05), {})
```

----------------------------------------

TITLE: Implementing Local Timer Client in PyTorch Distributed Elastic
DESCRIPTION: Class for implementing a local timer client using multiprocess.Queue. It is part of the server/client implementations provided by torchelastic.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/timer.rst#2025-04-22_snippet_3

LANGUAGE: Python
CODE:
```
LocalTimerClient
```

----------------------------------------

TITLE: Declaring PyTorch Distribution Modules using py:module Directive
DESCRIPTION: These reStructuredText directives use `py:module` to declare specific Python modules belonging to the `torch.distributions` package. This allows Sphinx, the documentation generator, to recognize these modules, generate appropriate documentation pages, and manage cross-references within the PyTorch documentation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributions.rst#2025-04-22_snippet_0

LANGUAGE: rst
CODE:
```
.. py:module:: torch.distributions.multinomial
```

LANGUAGE: rst
CODE:
```
.. py:module:: torch.distributions.multivariate_normal
```

LANGUAGE: rst
CODE:
```
.. py:module:: torch.distributions.negative_binomial
```

LANGUAGE: rst
CODE:
```
.. py:module:: torch.distributions.normal
```

LANGUAGE: rst
CODE:
```
.. py:module:: torch.distributions.one_hot_categorical
```

LANGUAGE: rst
CODE:
```
.. py:module:: torch.distributions.pareto
```

LANGUAGE: rst
CODE:
```
.. py:module:: torch.distributions.poisson
```

LANGUAGE: rst
CODE:
```
.. py:module:: torch.distributions.relaxed_bernoulli
```

LANGUAGE: rst
CODE:
```
.. py:module:: torch.distributions.relaxed_categorical
```

LANGUAGE: rst
CODE:
```
.. py:module:: torch.distributions.studentT
```

LANGUAGE: rst
CODE:
```
.. py:module:: torch.distributions.transformed_distribution
```

LANGUAGE: rst
CODE:
```
.. py:module:: torch.distributions.uniform
```

LANGUAGE: rst
CODE:
```
.. py:module:: torch.distributions.utils
```

LANGUAGE: rst
CODE:
```
.. py:module:: torch.distributions.von_mises
```

LANGUAGE: rst
CODE:
```
.. py:module:: torch.distributions.weibull
```

LANGUAGE: rst
CODE:
```
.. py:module:: torch.distributions.wishart
```

----------------------------------------

TITLE: Measuring CUDA Kernel Effective Memory Bandwidth (Python)
DESCRIPTION: Presents a Python script utilizing `torch.utils.benchmark.Timer` to measure the execution time of a CUDA kernel (`uniform_`). It calculates and prints the effective memory bandwidth based on the execution time, tensor size, and the number of bytes read/written per element.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_56

LANGUAGE: Python
CODE:
```
import torch
from torch.utils.benchmark import Timer
size = 128*512
nrep = 100
nbytes_read_write = 4 # this is number of bytes read + written by a kernel. Change this to fit your kernel.

for i in range(10):
    a=torch.empty(size).cuda().uniform_()
    torch.cuda.synchronize()
    out = a.uniform_()
    torch.cuda.synchronize()
    t = Timer(stmt="a.uniform_()", globals=globals())
    res = t.blocked_autorange()
    timec = res.median
    print("uniform, size, elements", size, "forward", timec, "bandwidth (GB/s)", size*(nbytes_read_write)*1e-9/timec)
    size *=2
```

----------------------------------------

TITLE: Profiling aten.nll_loss_backward.default Calls - PyTorch - Python
DESCRIPTION: Provides argument patterns for the backward negative log-likelihood loss operator (aten.nll_loss_backward.default), including tensors and scalar arguments. Dependencies are correct tensor types and shapes for loss gradients, logits, targets, and ignore index. Inputs must match PyTorch NLL loss specifications; the output is the gradient tensor, aiding in profiling training steps.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/selecsls42b_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
Operator: aten.nll_loss_backward.default
cnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})
```

----------------------------------------

TITLE: Running Distributed Spawn Tests in Python
DESCRIPTION: Command to run the distributed spawn test suite, which includes tests for Distributed Data Parallel, using Python's test runner.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/distributed/CONTRIBUTING.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
python test/run_test.py --verbose -i distributed/test_distributed_spawn
```

----------------------------------------

TITLE: Performing Tensor Addition with ATen Add Operator
DESCRIPTION: Illustrates the `aten.add.Tensor` operator for tensor addition. Supports various data types like i64 and f16 with shapes like [64, 256, 64, 64]. The operator requires correct dimensional alignment and data type compatibility between input tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_5

LANGUAGE: plaintext
CODE:
```
Operator: aten.add.Tensor
cnt: 38, ((T([], i64), 1), {})
cnt: 4, ((T([64, 256, 64, 64], f16), T([64, 256, 64, 64], f16)), {})
cnt: 6, ((T([64, 512, 32, 32], f16), T([64, 512, 32, 32], f16)), {})
```

----------------------------------------

TITLE: Building and Running with ASAN Bash
DESCRIPTION: Provides bash function definitions (`build_with_asan`, `run_with_asan`) that configure environment variables and execute build/run commands for PyTorch with AddressSanitizer (ASAN) enabled. It sets up necessary flags and paths for using Clang, the ASAN runtime library via `LD_PRELOAD`, and disables specific sanitizers or features for compatibility.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_60

LANGUAGE: Bash
CODE:
```
LLVM_ROOT=<wherever your llvm install is>
PYTORCH_ROOT=<wherever your pytorch checkout is>

LIBASAN_RT="$LLVM_ROOT/lib/clang/8.0.0/lib/linux/libclang_rt.asan-x86_64.so"
build_with_asan()
{
  LD_PRELOAD=${LIBASAN_RT} \
  CC="$LLVM_ROOT/bin/clang" \
  CXX="$LLVM_ROOT/bin/clang++" \
  LDSHARED="clang --shared" \
  LDFLAGS="-stdlib=libstdc++" \
  CFLAGS="-fsanitize=address -fno-sanitize-recover=all -shared-libasan -pthread" \
  CXX_FLAGS="-pthread" \
  USE_CUDA=0 USE_OPENMP=0 USE_DISTRIBUTED=0 DEBUG=1 \
  python setup.py develop
}

run_with_asan()
{
  LD_PRELOAD=${LIBASAN_RT} $@
}

# you can look at build-asan.sh to find the latest options the CI uses
export ASAN_OPTIONS=detect_leaks=0:symbolize=1:strict_init_order=true
export UBSAN_OPTIONS=print_stacktrace=1:suppressions=$PYTORCH_ROOT/ubsan.supp
export ASAN_SYMBOLIZER_PATH=$LLVM_ROOT/bin/llvm-symbolizer
```

----------------------------------------

TITLE: Python Command for Running MAML Examples
DESCRIPTION: Basic command line instruction for executing any of the MAML example scripts.
SOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/examples/maml_omniglot/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
python {filename}
```

----------------------------------------

TITLE: Training DLRM Model with Criteo Dataset in Bash
DESCRIPTION: Bash script to train the DLRM model using the Criteo Kaggle dataset. It saves the trained model and processes the dataset for further use.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/benchmarks/README.md#2025-04-22_snippet_0

LANGUAGE: bash
CODE:
```
./bench/dlrm_s_criteo_kaggle.sh --save-model=./models/criteo_model.ckpt [--use-gpu]
```

----------------------------------------

TITLE: PyTorch Mean Operation Along Dimensions
DESCRIPTION: This snippet shows a mean operation that reduces a tensor of shape [128, 1984, 7, 7] with f16 precision along its last two dimensions (-1, -2), keeping the dimensions in the result (keepdim=True).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_10

LANGUAGE: python
CODE:
```
Operator: aten.mean.dim
cnt: 1, ((T([128, 1984, 7, 7], f16), [-1, -2], True), {})
```

----------------------------------------

TITLE: Real and Imaginary Component Expressions
DESCRIPTION: Expressions for calculating real and imaginary components of a complex number in terms of z and its conjugate.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/autograd.rst#2025-04-22_snippet_4

LANGUAGE: math
CODE:
```
\begin{aligned}
    \mathrm{Re}(z) &= \frac {z + z^*}{2} \\
    \mathrm{Im}(z) &= \frac {z - z^*}{2j}
\end{aligned}
```

----------------------------------------

TITLE: Mask Function Implementation in PyTorch
DESCRIPTION: Example implementation of a mask function that creates a binary mask based on a threshold value.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/activation_sparsifier/README.md#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
def mask_fn(tensor, threshold):  # threshold is the sparse config here
    mask = torch.ones_like(tensor)
    mask[torch.abs(tensor) < threshold] = 0.0
    return mask
```

----------------------------------------

TITLE: Temporarily Disabling FakeTensor Mode
DESCRIPTION: Shows how to temporarily disable fake tensor mode using unset_fake_temporarily context manager, useful for cases like constant propagation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_fake_tensor.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
from torch._subclasses.fake_tensor import unset_fake_temporarily
with unset_fake_temporarily():
    ... # fake mode is disabled here, you can do real tensor compute
```

----------------------------------------

TITLE: Profiling PyTorch Operators in ConvNeXt Model
DESCRIPTION: This snippet shows the operator usage statistics of a ConvNeXt model implementation in PyTorch, displaying the call counts and tensor dimensions for forward and backward operations. It provides insight into the model architecture with downsampling pattern and increasing channel dimensions from 32 to 2560.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gernet_l_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([128, 1000], f16), 1, False), {})
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})
Operator: aten.add.Tensor
cnt: 57, ((T([], i64), 1), {})
cnt: 2, ((T([128, 128, 64, 64], f16), T([128, 128, 64, 64], f16)), {})
cnt: 4, ((T([128, 192, 32, 32], f16), T([128, 192, 32, 32], f16)), {})
cnt: 12, ((T([128, 640, 16, 16], f16), T([128, 640, 16, 16], f16)), {})
cnt: 17, ((T([128, 640, 8, 8], f16), T([128, 640, 8, 8], f16)), {})
cnt: 1, ((T([128, 32, 128, 128], f16), T([128, 32, 128, 128], f16)), {})
Operator: aten.addmm.default
cnt: 1, ((T([1000], f16), T([128, 2560], f16), T([2560, 1000], f16, stride=(1, 2560))), {})
Operator: aten.clone.default
cnt: 1, ((T([128, 3, 256, 256], f16),), {})
Operator: aten.convolution.default
cnt: 1, ((T([128, 3, 256, 256], f16), T([32, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 32, 128, 128], f16), T([128, 32, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 128, 64, 64], f16), T([128, 128, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 32, 128, 128], f16), T([128, 32, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 128, 64, 64], f16), T([192, 128, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([128, 192, 32, 32], f16), T([192, 192, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 128, 64, 64], f16), T([192, 128, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 192, 32, 32], f16), T([160, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 160, 32, 32], f16), T([160, 160, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 6, ((T([128, 160, 16, 16], f16), T([640, 160, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 192, 32, 32], f16), T([640, 192, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 5, ((T([128, 640, 16, 16], f16), T([160, 640, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 5, ((T([128, 160, 16, 16], f16), T([160, 160, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 640, 16, 16], f16), T([1920, 640, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 1920, 16, 16], f16), T([1920, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1920), {})
cnt: 9, ((T([128, 1920, 8, 8], f16), T([640, 1920, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 640, 16, 16], f16), T([640, 640, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 8, ((T([128, 640, 8, 8], f16), T([1920, 640, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 8, ((T([128, 1920, 8, 8], f16), T([1920, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1920), {})
cnt: 1, ((T([128, 640, 8, 8], f16), T([2560, 640, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
Operator: aten.convolution_backward.default
cnt: 1, ((T([128, 2560, 8, 8], f16), T([128, 640, 8, 8], f16), T([2560, 640, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 9, ((T([128, 640, 8, 8], f16), T([128, 1920, 8, 8], f16), T([640, 1920, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 8, ((T([128, 1920, 8, 8], f16), T([128, 1920, 8, 8], f16), T([1920, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1920, [True, True, False]), {})
cnt: 8, ((T([128, 1920, 8, 8], f16), T([128, 640, 8, 8], f16), T([1920, 640, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 640, 8, 8], f16), T([128, 640, 16, 16], f16), T([640, 640, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 1920, 8, 8], f16), T([128, 1920, 16, 16], f16), T([1920, 1, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1920, [True, True, False]), {})
cnt: 1, ((T([128, 1920, 16, 16], f16), T([128, 640, 16, 16], f16), T([1920, 640, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 6, ((T([128, 640, 16, 16], f16), T([128, 160, 16, 16], f16), T([640, 160, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 5, ((T([128, 160, 16, 16], f16), T([128, 160, 16, 16], f16), T([160, 160, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 5, ((T([128, 160, 16, 16], f16), T([128, 640, 16, 16], f16), T([160, 640, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 640, 16, 16], f16), T([128, 192, 32, 32], f16), T([640, 192, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 160, 16, 16], f16), T([128, 160, 32, 32], f16), T([160, 160, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 160, 32, 32], f16), T([128, 192, 32, 32], f16), T([160, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 3, ((T([128, 192, 32, 32], f16), T([128, 192, 32, 32], f16), T([192, 192, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 192, 32, 32], f16), T([128, 128, 64, 64], f16), T([192, 128, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 192, 32, 32], f16), T([128, 128, 64, 64], f16), T([192, 128, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 128, 64, 64], f16), T([128, 32, 128, 128], f16), T([128, 32, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 128, 64, 64], f16), T([128, 128, 64, 64], f16), T([128, 128, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 128, 64, 64], f16), T([128, 32, 128, 128], f16), T([128, 32, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 32, 128, 128], f16), T([128, 3, 256, 256], f16), T([32, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})
Operator: aten.copy_.default
cnt: 1, ((T([128, 3, 256, 256], f16), T([128, 3, 256, 256], f16)), {})
Operator: aten.div.Scalar
cnt: 1, ((T([128, 2560, 8, 8], f16, stride=(2560, 1, 0, 0)), 64), {})
Operator: aten.lift_fresh_copy.default
cnt: 1, ((T([128], i64),), {})
Operator: aten.mean.dim
cnt: 1, ((T([128, 2560, 8, 8], f16), [-1, -2], True), {})
Operator: aten.mm.default
cnt: 1, ((T([128, 1000], f16), T([1000, 2560], f16)), {})
cnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 2560], f16)), {})
Operator: aten.native_batch_norm.default
cnt: 1, ((T([128, 32, 128, 128], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})
cnt: 3, ((T([128, 128, 64, 64], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})
cnt: 5, ((T([128, 192, 32, 32], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 160, 32, 32], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f16), True, 0.1, 1e-05), {})
cnt: 11, ((T([128, 160, 16, 16], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f16), True, 0.1, 1e-05), {})
cnt: 7, ((T([128, 640, 16, 16], f16), T([640], f16), T([640], f16), T([640], f16), T([640], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 1920, 16, 16], f16), T([1920], f16), T([1920], f16), T([1920], f16), T([1920], f16), True, 0.1, 1e-05), {})
cnt: 17, ((T([128, 1920, 8, 8], f16), T([1920], f16), T([1920], f16), T([1920], f16), T([1920], f16), True, 0.1, 1e-05), {})
cnt: 10, ((T([128, 640, 8, 8], f16), T([640], f16), T([640], f16), T([640], f16), T([640], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 2560, 8, 8], f16), T([2560], f16), T([2560], f16), T([2560], f16), T([2560], f16), True, 0.1, 1e-05), {})
Operator: aten.native_batch_norm_backward.default
cnt: 1, ((T([128, 2560, 8, 8], f16), T([128, 2560, 8, 8], f16), T([2560], f16), T([2560], f16), T([2560], f16), T([2560], f32), T([2560], f32), True, 1e-05, [True, True, True]), {})
cnt: 10, ((T([128, 640, 8, 8], f16), T([128, 640, 8, 8], f16), T([640], f16), T([640], f16), T([640], f16), T([640], f32), T([640], f32), True, 1e-05, [True, True, True]), {})
cnt: 17, ((T([128, 1920, 8, 8], f16), T([128, 1920, 8, 8], f16), T([1920], f16), T([1920], f16), T([1920], f16), T([1920], f32), T([1920], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 1920, 16, 16], f16), T([128, 1920, 16, 16], f16), T([1920], f16), T([1920], f16), T([1920], f16), T([1920], f32), T([1920], f32), True, 1e-05, [True, True, True]), {})
cnt: 7, ((T([128, 640, 16, 16], f16), T([128, 640, 16, 16], f16), T([640], f16), T([640], f16), T([640], f16), T([640], f32), T([640], f32), True, 1e-05, [True, True, True]), {})
cnt: 11, ((T([128, 160, 16, 16], f16), T([128, 160, 16, 16], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f32), T([160], f32), True, 1e-05, [True, True, True]), {})
cnt: 1, ((T([128, 160, 32, 32], f16), T([128, 160, 32, 32], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f32), T([160], f32), True, 1e-05, [True, True, True]), {})
cnt: 5, ((T([128, 192, 32, 32], f16), T([128, 192, 32, 32], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), True, 1e-05, [True, True, True]), {})
cnt: 3, ((T([128, 128, 64, 64], f16), T([128, 128, 64, 64], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})
```

----------------------------------------

TITLE: Referencing PyTorch Pipeline Package
DESCRIPTION: Shows the package import path for PyTorch's pipeline parallelism functionality
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/distributed/pipelining/README.md#2025-04-22_snippet_0

LANGUAGE: markdown
CODE:
```
torch.distributed.pipelining
```

----------------------------------------

TITLE: Documenting Wildcard Alias in PyTorch JIT List Select Schema
DESCRIPTION: Defines a JIT FunctionSchema for a `list_select` operation. The `(*)` annotation on the output indicates it belongs to the wildcard alias set, used conservatively when the exact alias relationship isn't precisely known, such as when extracting an element from a list.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_37

LANGUAGE: JIT FunctionSchema
CODE:
```
list_select(Tensor[] list, int idx) -> Tensor(*)
```

----------------------------------------

TITLE: Summation with Tensor SymInt in PyTorch (Python)
DESCRIPTION: Executes aten.sum with a symbolic integer which performs an element-wise summation over the specified axes of a tensor—a fundamental operation in loss calculations and metric aggregations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_26

LANGUAGE: Python
CODE:
```
aten.sum.SymInt
cnt: 1, ((T([16, 2], f16), [0], True), {})
cnt: 1, ((T([16, 768], f16), [0], True), {})
cnt: 60, ((T([8192, 768], f16), [0], True), {})
cnt: 12, ((T([8192, 3072], f16), [0], True), {})
cnt: 1, ((T([16, 512, 768], f16), [0], True), {})
```

----------------------------------------

TITLE: Checkout PyTorch Nightly Branch with venv (bash)
DESCRIPTION: Uses the `./tools/nightly.py` script to checkout or create a new PyTorch development branch (`-b`) and sets up a Python virtual environment (`venv`). The command requires git and Python with `venv` installed; a subsequent `source` command activates the environment (use `& .\venv\Scripts\Activate.ps1` on Windows).
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_7

LANGUAGE: bash
CODE:
```
./tools/nightly.py checkout -b my-nightly-branch
source venv/bin/activate
```

----------------------------------------

TITLE: Sum Operations with Symbolic Indices in PyTorch
DESCRIPTION: Records of sum operations performed along specific symbolic dimensions of tensors. This operation computes the sum along dimension 0 of a tensor with shape [128, 1000], keeping the dimension in the result (keepdim=True).
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/spnasnet_100_training.txt#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([128, 1000], f16), [0], True), {})
```

----------------------------------------

TITLE: Build Python HTML Docs (Bash)
DESCRIPTION: This command executes the `make html` target, typically run from the `docs` directory after installing prerequisites. It invokes Sphinx to process the `.rst` source files and generate the final HTML documentation output, which is placed in the `docs/build/html` directory.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_27

LANGUAGE: bash
CODE:
```
make html
```

----------------------------------------

TITLE: Representing Conditional Logic in PyTorch IR
DESCRIPTION: This snippet shows the PyTorch Intermediate Representation (IR) for the corresponding Python function `f`. It illustrates the use of the `prim::If` node to represent the conditional statement, utilizing `block0` (true branch) and `block1` (false branch) sub-blocks. The outputs of the branches merge to become the output of the `prim::If` node, similar to an SSA Phi function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_2

LANGUAGE: PyTorch IR
CODE:
```
graph(%a : Dynamic,
      %b : Dynamic,
      %c : Dynamic):
  %2 : int = prim::Constant[value=1]()
  %3 : Dynamic = aten::add(%a, %b, %2)
  %5 : Dynamic = prim::If(%c)
    block0():
      %6 : int = prim::Constant[value=1]()
      %7 : Dynamic = aten::add(%3, %3, %6)
      -> (%7)
    }
    block1():
      %8 : int = prim::Constant[value=1]()
      %9 : Dynamic = aten::add(%b, %3, %8)
      -> (%9)
  return (%5)
```

----------------------------------------

TITLE: Copy Built Docs Locally with rsync (Bash)
DESCRIPTION: These commands provide an alternative method for previewing documentation built on a remote machine. They create local directories (`build`, `cpp/build`) and then use `rsync` with the archive (`-a`) and compress (`-z`) flags to efficiently copy the generated HTML documentation directories (`docs/build/html` and `docs/cpp/build/html`) from the remote machine to the local filesystem.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_34

LANGUAGE: bash
CODE:
```
mkdir -p build cpp/build
rsync -az me@my_machine:/path/to/pytorch/docs/build/html build
rsync -az me@my_machine:/path/to/pytorch/docs/cpp/build/html cpp/build
```

----------------------------------------

TITLE: Checking Fwd Kernel Modification Status CMake
DESCRIPTION: Checks the `ret` variable for the return code of the preceding `execute_process`. If `ret` is set and non-zero, it indicates failure in modifying the forward kernels, causing CMake configuration to abort with a fatal error message.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/transformers/hip/flash_attn/ck/CMakeLists.txt#_snippet_9

LANGUAGE: CMake
CODE:
```
if(ret AND NOT ret EQUAL 0)
  message( FATAL_ERROR "CK Tile FMHA FAILED to change make_kernel to make_kernel_pt for the fwd pass")
endif()
```

----------------------------------------

TITLE: Logging ATen Operator Invocation in PyTorch - Text
DESCRIPTION: This snippet logs the invocation summary of ATen operators used in PyTorch models. It records operator names, argument shapes, operand types (e.g., 'f16'), key parameters, and number of occurrences ('cnt') in a text format. The logging aids in profiling, trace analysis, and optimization by providing per-operator utilization statistics. Inputs include tensor shapes and parameters; the output is a human-readable summary without executable code. There are no dependencies except access to operator invocation events.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/swin_base_patch4_window7_224_training.txt#2025-04-22_snippet_0

LANGUAGE: Text
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([64, 1000], f16), 1, False), {})
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})
Operator: aten._softmax.default
cnt: 2, ((T([4096, 4, 49, 49], f16), -1, False), {})
cnt: 2, ((T([1024, 8, 49, 49], f16), -1, False), {})
cnt: 18, ((T([256, 16, 49, 49], f16), -1, False), {})
cnt: 2, ((T([64, 32, 49, 49], f16), -1, False), {})
Operator: aten._softmax_backward_data.default
cnt: 2, ((T([64, 32, 49, 49], f16), T([64, 32, 49, 49], f16), -1, f16), {})
cnt: 18, ((T([256, 16, 49, 49], f16), T([256, 16, 49, 49], f16), -1, f16), {})
cnt: 2, ((T([1024, 8, 49, 49], f16), T([1024, 8, 49, 49], f16), -1, f16), {})
cnt: 2, ((T([4096, 4, 49, 49], f16), T([4096, 4, 49, 49], f16), -1, f16), {})
Operator: aten._unsafe_view.default
cnt: 6, ((T([4096, 4, 49, 32], f16), [16384, 49, 32]), {})
cnt: 2, ((T([4096, 4, 32, 49], f16), [16384, 32, 49]), {})
cnt: 2, ((T([16384, 49, 49], f16), [4096, 4, 49, 49]), {})
cnt: 2, ((T([16384, 49, 32], f16), [4096, 4, 49, 32]), {})
cnt: 2, ((T([4096, 49, 4, 32], f16), [4096, 49, 128]), {})
cnt: 1, ((T([50176, 256], f16), [64, 784, 256]), {})
cnt: 6, ((T([1024, 8, 49, 32], f16), [8192, 49, 32]), {})
cnt: 2, ((T([1024, 8, 32, 49], f16), [8192, 32, 49]), {})
cnt: 2, ((T([8192, 49, 49], f16), [1024, 8, 49, 49]), {})
cnt: 2, ((T([8192, 49, 32], f16), [1024, 8, 49, 32]), {})
cnt: 2, ((T([1024, 49, 8, 32], f16), [1024, 49, 256]), {})
cnt: 1, ((T([12544, 512], f16), [64, 196, 512]), {})
cnt: 54, ((T([256, 16, 49, 32], f16), [4096, 49, 32]), {})
cnt: 18, ((T([256, 16, 32, 49], f16), [4096, 32, 49]), {})
cnt: 18, ((T([4096, 49, 49], f16), [256, 16, 49, 49]), {})
cnt: 18, ((T([4096, 49, 32], f16), [256, 16, 49, 32]), {})
cnt: 18, ((T([256, 49, 16, 32], f16), [256, 49, 512]), {})
cnt: 1, ((T([3136, 1024], f16), [64, 49, 1024]), {})
cnt: 6, ((T([64, 32, 49, 32], f16), [2048, 49, 32]), {})
cnt: 2, ((T([64, 32, 32, 49], f16), [2048, 32, 49]), {})
cnt: 2, ((T([2048, 49, 49], f16), [64, 32, 49, 49]), {})
cnt: 2, ((T([2048, 49, 32], f16), [64, 32, 49, 32]), {})
cnt: 2, ((T([64, 49, 32, 32], f16), [64, 49, 1024]), {})
cnt: 2, ((T([64, 49, 3, 32, 32], f16), [64, 49, 3072]), {})
cnt: 18, ((T([64, 2, 2, 7, 7, 512], f16), [256, 7, 7, 512]), {})
cnt: 18, ((T([256, 49, 3, 16, 32], f16), [256, 49, 1536]), {})
cnt: 18, ((T([64, 2, 7, 2, 7, 512], f16), [64, 14, 14, 512]), {})
cnt: 2, ((T([64, 4, 4, 7, 7, 256], f16), [1024, 7, 7, 256]), {})
cnt: 2, ((T([1024, 49, 3, 8, 32], f16), [1024, 49, 768]), {})
cnt: 2, ((T([64, 4, 7, 4, 7, 256], f16), [64, 28, 28, 256]), {})
cnt: 2, ((T([64, 8, 8, 7, 7, 128], f16), [4096, 7, 7, 128]), {})
cnt: 2, ((T([4096, 49, 3, 4, 32], f16), [4096, 49, 384]), {})
cnt: 2, ((T([64, 8, 7, 8, 7, 128], f16), [64, 56, 56, 128]), {})
Operator: aten.add.Tensor
cnt: 2, ((T([4096, 4, 49, 49], f16), T([1, 4, 49, 49], f16)), {})
cnt: 8, ((T([64, 3136, 128], f16), T([64, 3136, 128], f16)), {})
cnt: 1, ((T([64, 64, 4, 49, 49], f16), T([1, 64, 1, 49, 49], f16)), {})
cnt: 2, ((T([1024, 8, 49, 49], f16), T([1, 8, 49, 49], f16)), {})
cnt: 8, ((T([64, 784, 256], f16), T([64, 784, 256], f16)), {})
cnt: 1, ((T([64, 16, 8, 49, 49], f16), T([1, 16, 1, 49, 49], f16)), {})
cnt: 18, ((T([256, 16, 49, 49], f16), T([1, 16, 49, 49], f16)), {})
cnt: 72, ((T([64, 196, 512], f16), T([64, 196, 512], f16)), {})
cnt: 9, ((T([64, 4, 16, 49, 49], f16), T([1, 4, 1, 49, 49], f16)), {})
cnt: 2, ((T([64, 32, 49, 49], f16), T([1, 32, 49, 49], f16)), {})
cnt: 8, ((T([64, 49, 1024], f16), T([64, 49, 1024], f16)), {})
cnt: 3, ((T([64, 14, 14, 512], f16), T([64, 14, 14, 512], f16)), {})
cnt: 3, ((T([64, 28, 28, 256], f16), T([64, 28, 28, 256], f16)), {})
cnt: 3, ((T([64, 56, 56, 128], f16), T([64, 56, 56, 128], f16)), {})
Operator: aten.addmm.default
cnt: 2, ((T([384], f16), T([200704, 128], f16), T([128, 384], f16, stride=(1, 128))), {})
cnt: 2, ((T([128], f16), T([200704, 128], f16), T([128, 128], f16, stride=(1, 128))), {})
cnt: 2, ((T([512], f16), T([200704, 128], f16), T([128, 512], f16, stride=(1, 128))), {})
cnt: 2, ((T([128], f16), T([200704, 512], f16), T([512, 128], f16, stride=(1, 512))), {})
cnt: 2, ((T([768], f16), T([50176, 256], f16), T([256, 768], f16, stride=(1, 256))), {})
cnt: 2, ((T([256], f16), T([50176, 256], f16), T([256, 256], f16, stride=(1, 256))), {})
cnt: 2, ((T([1024], f16), T([50176, 256], f16), T([256, 1024], f16, stride=(1, 256))), {})
cnt: 2, ((T([256], f16), T([50176, 1024], f16), T([1024, 256], f16, stride=(1, 1024))), {})
cnt: 18, ((T([1536], f16), T([12544, 512], f16), T([512, 1536], f16, stride=(1, 512))), {})
cnt: 18, ((T([512], f16), T([12544, 512], f16), T([512, 512], f16, stride=(1, 512))), {})
cnt: 18, ((T([2048], f16), T([12544, 512], f16), T([512, 2048], f16, stride=(1, 512))), {})
cnt: 18, ((T([512], f16), T([12544, 2048], f16), T([2048, 512], f16, stride=(1, 2048))), {})
cnt: 2, ((T([3072], f16), T([3136, 1024], f16), T([1024, 3072], f16, stride=(1, 1024))), {})
cnt: 2, ((T([1024], f16), T([3136, 1024], f16), T([1024, 1024], f16, stride=(1, 1024))), {})
cnt: 2, ((T([4096], f16), T([3136, 1024], f16), T([1024, 4096], f16, stride=(1, 1024))), {})
cnt: 2, ((T([1024], f16), T([3136, 4096], f16), T([4096, 1024], f16, stride=(1, 4096))), {})
cnt: 1, ((T([1000], f16), T([64, 1024], f16), T([1024, 1000], f16, stride=(1, 1024))), {})
Operator: aten.bernoulli_.float
cnt: 2, ((T([64, 1, 1], f16), 0.9956521736457944), {})
cnt: 2, ((T([64, 1, 1], f16), 0.9913043472915888), {})
cnt: 2, ((T([64, 1, 1], f16), 0.9869565209373832), {})
cnt: 2, ((T([64, 1, 1], f16), 0.9826086945831776), {})
cnt: 2, ((T([64, 1, 1], f16), 0.9782608672976494), {})
cnt: 2, ((T([64, 1, 1], f16), 0.9739130418747663), {})
cnt: 2, ((T([64, 1, 1], f16), 0.9695652164518833), {})
cnt: 2, ((T([64, 1, 1], f16), 0.9652173891663551), {})
cnt: 2, ((T([64, 1, 1], f16), 0.960869561880827), {})
cnt: 2, ((T([64, 1, 1], f16), 0.9565217345952988), {})
cnt: 2, ((T([64, 1, 1], f16), 0.9521739110350609), {})
cnt: 2, ((T([64, 1, 1], f16), 0.9478260837495327), {})
cnt: 2, ((T([64, 1, 1], f16), 0.9434782564640045), {})
cnt: 2, ((T([64, 1, 1], f16), 0.9391304329037666), {})
cnt: 2, ((T([64, 1, 1], f16), 0.9347826093435287), {})
cnt: 2, ((T([64, 1, 1], f16), 0.9304347857832909), {})
cnt: 2, ((T([64, 1, 1], f16), 0.9260869547724724), {})
cnt: 2, ((T([64, 1, 1], f16), 0.9217391312122345), {})
cnt: 2, ((T([64, 1, 1], f16), 0.917391300201416), {})
cnt: 2, ((T([64, 1, 1], f16), 0.9130434766411781), {})
cnt: 2, ((T([64, 1, 1], f16), 0.9086956530809402), {})
cnt: 2, ((T([64, 1, 1], f16), 0.9043478220701218), {})
cnt: 2, ((T([64, 1, 1], f16), 0.8999999985098839), {})
Operator: aten.bmm.default
cnt: 2, ((T([16384, 49, 32], f16), T([16384, 32, 49], f16)), {})
cnt: 2, ((T([16384, 49, 49], f16), T([16384, 49, 32], f16)), {})
cnt: 2, ((T([8192, 49, 32], f16), T([8192, 32, 49], f16)), {})
cnt: 2, ((T([8192, 49, 49], f16), T([8192, 49, 32], f16)), {})
cnt: 18, ((T([4096, 49, 32], f16), T([4096, 32, 49], f16)), {})
cnt: 18, ((T([4096, 49, 49], f16), T([4096, 49, 32], f16)), {})
cnt: 2, ((T([2048, 49, 32], f16), T([2048, 32, 49], f16)), {})
cnt: 2, ((T([2048, 49, 49], f16), T([2048, 49, 32], f16)), {})
cnt: 2, ((T([2048, 49, 49], f16, stride=(2401, 1, 49)), T([2048, 49, 32], f16)), {})
cnt: 2, ((T([2048, 49, 32], f16), T([2048, 32, 49], f16, stride=(1568, 1, 32))), {})
cnt: 2, ((T([2048, 32, 49], f16, stride=(1568, 1, 32)), T([2048, 49, 49], f16)), {})
cnt: 2, ((T([2048, 49, 49], f16), T([2048, 49, 32], f16, stride=(1568, 1, 49))), {})
cnt: 18, ((T([4096, 49, 49], f16, stride=(2401, 1, 49)), T([4096, 49, 32], f16)), {})
cnt: 18, ((T([4096, 49, 32], f16), T([4096, 32, 49], f16, stride=(1568, 1, 32))), {})
cnt: 18, ((T([4096, 32, 49], f16, stride=(1568, 1, 32)), T([4096, 49, 49], f16)), {})
cnt: 18, ((T([4096, 49, 49], f16), T([4096, 49, 32], f16, stride=(1568, 1, 49))), {})
cnt: 2, ((T([8192, 49, 49], f16, stride=(2401, 1, 49)), T([8192, 49, 32], f16)), {})
cnt: 2, ((T([8192, 49, 32], f16), T([8192, 32, 49], f16, stride=(1568, 1, 32))), {})
cnt: 2, ((T([8192, 32, 49], f16, stride=(1568, 1, 32)), T([8192, 49, 49], f16)), {})
cnt: 2, ((T([8192, 49, 49], f16), T([8192, 49, 32], f16, stride=(1568, 1, 49))), {})
cnt: 2, ((T([16384, 49, 49], f16, stride=(2401, 1, 49)), T([16384, 49, 32], f16)), {})
cnt: 2, ((T([16384, 49, 32], f16), T([16384, 32, 49], f16, stride=(1568, 1, 32))), {})
cnt: 2, ((T([16384, 32, 49], f16, stride=(1568, 1, 32)), T([16384, 49, 49], f16)), {})
cnt: 2, ((T([16384, 49, 49], f16), T([16384, 49, 32], f16, stride=(1568, 1, 49))), {})
Operator: aten.cat.default
cnt: 1, (([T([64, 28, 28, 128], f16, stride=(401408, 14336, 256, 1)), T([64, 28, 28, 128], f16, stride=(401408, 14336, 256, 1)), T([64, 28, 28, 128], f16, stride=(401408, 14336, 256, 1)), T([64, 28, 28, 128], f16, stride=(401408, 14336, 256, 1))], -1), {})
cnt: 1, (([T([64, 14, 14, 256], f16, stride=(200704, 14336, 512, 1)), T([64, 14, 14, 256], f16, stride=(200704, 14336, 512, 1)), T([64, 14, 14, 256], f16, stride=(200704, 14336, 512, 1)), T([64, 14, 14, 256], f16, stride=(200704, 14336, 512, 1))], -1), {})
cnt: 1, (([T([64, 7, 7, 512], f16, stride=(100352, 14336, 1024, 1)), T([64, 7, 7, 512], f16, stride=(100352, 14336, 1024, 1)), T([64, 7, 7, 512], f16, stride=(100352, 14336, 1024, 1)), T([64, 7, 7, 512], f16, stride=(100352, 14336, 1024, 1))], -1), {})
Operator: aten.clone.default
cnt: 1, ((T([64, 3, 224, 224], f16),), {})
Operator: aten.convolution.default
cnt: 1, ((T([64, 3, 224, 224], f16), T([128, 3, 4, 4], f16), T([128], f16), [4, 4], [0, 0], [1, 1], False, [0, 0], 1), {})
Operator: aten.convolution_backward.default
cnt: 1, ((T([64, 128, 56, 56], f16, stride=(401408, 1, 7168, 128)), T([64, 3, 224, 224], f16), T([128, 3, 4, 4], f16), [128], [4, 4], [0, 0], [1, 1], False, [0, 0], 1, [False, True, True]), {})
Operator: aten.copy_.default
cnt: 1, ((T([64, 3, 224, 224], f16), T([64, 3, 224, 224], f16)), {})
Operator: aten.div.Scalar
cnt: 1, ((T([64, 49, 1024], f16, stride=(1024, 0, 1)), 49), {})
```

----------------------------------------

TITLE: Initializing TensorImpl in C++
DESCRIPTION: This function initializes a TensorImpl object with storage, dispatch key set, and type metadata. It's a core part of PyTorch's tensor implementation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_0

LANGUAGE: C++
CODE:
```
_ZN3c1010TensorImplC1ENS0_8ImplTypeEONS_7StorageENS_14DispatchKeySetEN6caffe28TypeMetaE
```

----------------------------------------

TITLE: Initializing Convolution Operation in PyTorch
DESCRIPTION: This snippet represents the configuration setup for a convolution operation in PyTorch using half-precision (f16) tensors. It details shape, kernel size, stride, padding, dilation, and group count parameters necessary for the convolution operation. Expected outputs include convolved tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/pnasnet5large_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
cnt: 2, ((T([16, 1080, 42, 42], f16), T([432, 1080, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

LANGUAGE: Python
CODE:
```
cnt: 5, ((T([16, 2160, 21, 21], f16), T([432, 2160, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

----------------------------------------

TITLE: Tracing Operator Execution in PyTorch JIT C++
DESCRIPTION: A C++ snippet from the PyTorch JIT tracer demonstrating how individual operator calls are recorded during tracing. It checks the tracing state, creates a corresponding `Node` in the graph, records source information, adds inputs and outputs to the node, and inserts the node into the graph being built.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_10

LANGUAGE: cpp
CODE:
```
torch::jit::Node* node = nullptr;
std::shared_ptr<jit::tracer::TracingState> tracer_state;
if (jit::tracer::isTracing()) {
        tracer_state = jit::tracer::getTracingState();
        at::Symbol op_name;
        op_name = jit::Symbol::fromQualString("aten::__ilshift__");
        node = tracer_state->graph->create(op_name, /*num_outputs=*/0);
        jit::tracer::recordSourceLocation(node);
        jit::tracer::addInputs(node, "self", self);
        jit::tracer::addInputs(node, "other", other);
        tracer_state->graph->insertNode(node);

        jit::tracer::setTracingState(nullptr);
}
TypeDefault::__ilshift__(self, other);
if (tracer_state) {
        jit::tracer::setTracingState(std::move(tracer_state));
        jit::tracer::addOutput(node, self);
}
```

----------------------------------------

TITLE: Unbinding Tensors using ATen in Python
DESCRIPTION: Demonstrates the ATen \"aten.unbind.int\" operator to unbind tensors along a specified dimension, effectively splitting a tensor into smaller ones. Focused on f16 tensors with varied strides. PyTorch with ATen support is required to execute these operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/visformer_small_training.txt#2025-04-22_snippet_9

LANGUAGE: Python
CODE:
```
cnt: 4, ((T([3, 128, 6, 196, 64], f16, stride=(75264, 225792, 12544, 1, 196)),), {})
```

LANGUAGE: Python
CODE:
```
cnt: 4, ((T([3, 128, 6, 49, 128], f16, stride=(37632, 112896, 6272, 1, 49)),), {})
```

----------------------------------------

TITLE: Logging Aten Operator: aten.unbind.int (Text)
DESCRIPTION: Log entries showing example invocations of the 'aten.unbind.int' operator. Each line ('cnt') represents a call signature, detailing the input tensor (T) to be unbound, including its shape, data type (f16), and stride information. The dimension to unbind along is implicitly the first one (dim=0) unless otherwise specified in the operator's definition or context not shown here.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/pit_b_224_training.txt#2025-04-22_snippet_7

LANGUAGE: text
CODE:
```
Operator: aten.unbind.int
cnt: 3, ((T([3, 64, 4, 962, 64], f16, stride=(256, 738816, 64, 768, 1)),), {})
cnt: 6, ((T([3, 64, 8, 257, 64], f16, stride=(512, 394752, 64, 1536, 1)),), {})
cnt: 4, ((T([3, 64, 16, 65, 64], f16, stride=(1024, 199680, 64, 3072, 1)),), {})
```

----------------------------------------

TITLE: Checking Timer Expiration in PyTorch Distributed Elastic
DESCRIPTION: Function to check if a timer has expired. It is part of the client methods for interacting with the timer system.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/timer.rst#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
torch.distributed.elastic.timer.expires
```

----------------------------------------

TITLE: Documenting torch.testing Module in reStructuredText
DESCRIPTION: This snippet defines the documentation structure for the torch.testing module in PyTorch. It sets up the module reference and includes autofunction directives for three key functions: assert_close, make_tensor, and assert_allclose.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/testing.rst#2025-04-22_snippet_0

LANGUAGE: reStructuredText
CODE:
```
torch.testing
=============

.. automodule:: torch.testing
.. currentmodule:: torch.testing

.. autofunction:: assert_close
.. autofunction:: make_tensor
.. autofunction:: assert_allclose
```

----------------------------------------

TITLE: Profiling aten.addmm.default Calls in PyTorch Text Trace
DESCRIPTION: Presents invocation details for the aten.addmm.default operator, typically used for a fully-connected (linear) layer combining a bias tensor and a matrix-matrix multiplication. The entry encodes the tensor shapes, memory stride info, and datatype (f16). This format is suitable for reviewing memory access patterns and linear layer characteristics in traced deep learning models.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_1

LANGUAGE: text
CODE:
```
Operator: aten.addmm.default
cnt: 1, ((T([1000], f16), T([32, 1280], f16), T([1280, 1000], f16, stride=(1, 1280))), {})
```

----------------------------------------

TITLE: Invoking aten.sum.SymInt with Tensor Arguments (Text)
DESCRIPTION: This section logs calls to the `aten.sum.SymInt` operator, which computes the sum of tensor elements over specified dimensions. The examples show summing float16 (f16) tensors of various shapes (e.g., [128, 1000], [128, 1536, 6, 6]) along different dimensions (e.g., [0], [2, 3]). The `True` argument likely corresponds to `keepdim=True`. Note the explicit stride information `stride=(0, 0)` in one example, indicating a potentially non-contiguous tensor or a specific layout.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_21

LANGUAGE: text
CODE:
```
Operator: aten.sum.SymInt
```

LANGUAGE: text
CODE:
```
cnt: 1, ((T([128, 1000], f16, stride=(0, 0)), [0], True), {})
```

LANGUAGE: text
CODE:
```
cnt: 3, ((T([128, 1536, 6, 6], f16), [2, 3], True), {})
```

LANGUAGE: text
CODE:
```
cnt: 6, ((T([128, 1536, 12, 12], f16), [2, 3], True), {})
```

LANGUAGE: text
CODE:
```
cnt: 2, ((T([128, 512, 24, 24], f16), [2, 3], True), {})
```

LANGUAGE: text
CODE:
```
cnt: 1, ((T([128, 256, 48, 48], f16), [2, 3], True), {})
```

----------------------------------------

TITLE: Performing Multi-GPU Distributed Offline GEMM Tuning with PyTorch (Python)
DESCRIPTION: Python script for performing offline GEMM tuning distributed across multiple GPUs. It requires the main logic to be within `if __name__ == "__main__":` due to using `concurrent.futures`. It calls `tunable.mgpu_tune_gemm_in_file`, providing a wildcard pattern for input untuned CSV files (`tunableop_untuned?.csv`) and the number of GPUs (`num_gpus`) to use. The function gathers unique GEMMs, distributes tuning across the specified GPUs, collects results, and saves them to aggregated files (e.g., `tunableop_results_full0.csv`) and per-GPU files.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/cuda/tunable/README.md#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
if __name__ == "__main__":
    num_gpus = 8 # number of GPUs that will be used during the tuning process
    tunable.mgpu_tune_gemm_in_file("tunableop_untuned?.csv", num_gpus)
```

----------------------------------------

TITLE: Generating Reproduction Report for Failed Test
DESCRIPTION: Demonstrates how to set the `CREATE_REPRODUCTION_REPORT` environment variable and run a specific test case (`div_mode_int`) using pytest to generate a markdown report for debugging failing inputs in ONNX Runtime tests.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/onnx/torchlib/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
CREATE_REPRODUCTION_REPORT=1 python -m pytest test/onnx/torchlib/test_ops.py -k div_mode_int
```

----------------------------------------

TITLE: Using MaybeOwned<Tensor> in Tensor::expect_contiguous method
DESCRIPTION: A canonical example of MaybeOwned<Tensor> usage in a Tensor's expect_contiguous method. The method returns a borrowed self-reference when the tensor is already contiguous and an owned reference to a newly created contiguous tensor otherwise.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/maybe_owned.rst#2025-04-22_snippet_0

LANGUAGE: cpp
CODE:
```
inline c10::MaybeOwned<Tensor> Tensor::expect_contiguous(MemoryFormat memory_format) const & {
  if (is_contiguous(memory_format)) {
    return c10::MaybeOwned<Tensor>::borrowed(*this);
  } else {
    return c10::MaybeOwned<Tensor>::owned(__dispatch_contiguous(memory_format));
  }
}
```

----------------------------------------

TITLE: Defining and Using C-style Logging in C
DESCRIPTION: This C code snippet demonstrates how to set up and use a logging library for logging errors, warnings, and debug information with printf-style formatting. It includes macro definitions for setting log levels and provides logging functions to report various statuses and events in a program. Dependencies include the clog library, and the code is intended for environments with C99 or C++ compatibility. Inputs are status codes and variable values, and outputs are formatted log messages. The snippet shows usage in a hypothetical function some_function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/deps/clog/README.md#2025-04-22_snippet_0

LANGUAGE: C
CODE:
```
#include <clog.h>\n\n#ifndef MYMODULE_LOG_LEVEL\n    #define MYMODULE_LOG_LEVEL CLOG_DEBUG\n#endif\n\nCLOG_DEFINE_LOG_DEBUG(mymodule_, \"My Module\", MYMODULE_LOG_LEVEL);\nCLOG_DEFINE_LOG_INFO(mymodule_, \"My Module\", MYMODULE_LOG_LEVEL);\nCLOG_DEFINE_LOG_WARNING(mymodule_, \"My Module\", MYMODULE_LOG_LEVEL);\nCLOG_DEFINE_LOG_ERROR(mymodule_, \"My Module\", MYMODULE_LOG_LEVEL);\n\n...\n\nvoid some_function(...) {\n    int status = ...\n    if (status != 0) {\n        mymodule_log_error(\n            \"something really bad happened: \"\n            \"operation failed with status %d\", status);\n    }\n\n    uint32_t expected_zero = ...\n    if (expected_zero != 0) {\n        mymodule_log_warning(\n            \"something suspicious happened (var = %\"PRIu32\"), \"\n            \"fall back to generic implementation\", expected_zero);\n    }\n\n    void* usually_non_null = ...\n    if (usually_non_null == NULL) {\n        mymodule_log_info(\n            \"something unusual, but common, happened: \"\n            \"enabling work-around\");\n    }\n\n    float a = ...\n    mymodule_log_debug(\"computed a = %.7f\", a);\n}
```

----------------------------------------

TITLE: Invoking aten.addmm.default in PyTorch ATen
DESCRIPTION: Logs calls to the `aten.addmm.default` operator, which performs an add and matrix multiplication operation (C = beta*M + alpha*(mat1 @ mat2)). The logs show tensor arguments represented as `T([shape], dtype, optional_stride)`, indicating the dimensions, data type (f16), and sometimes memory layout (stride) of the input tensors. The `cnt` field indicates the frequency of each specific call signature.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/lcnet_050_training.txt#2025-04-22_snippet_1

LANGUAGE: plaintext
CODE:
```
Operator: aten.addmm.default
cnt: 1, ((T([128, 1000], f16), T([1000, 1280], f16)), {})
cnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 1280], f16)), {})
```

----------------------------------------

TITLE: Checking SSL Certificate Validity Dates (Bash)
DESCRIPTION: This bash command uses `openssl` to display the validity period of an SSL certificate file. This is used to diagnose issues with Git submodule updates failing due to expired proxy SSL certificates.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_6

LANGUAGE: bash
CODE:
```
openssl x509 -noout -in <cert_file> -dates
```

----------------------------------------

TITLE: Profiling aten.relu_.default Calls - PyTorch - Python
DESCRIPTION: Displays argument layouts for in-place ReLU (aten.relu_.default) operator calls with a variety of float16 tensors. Relies on CUDA or CPU PyTorch kernels with ability to mutate tensor values in-place. Inputs are float16 tensors, outputs are the modified tensor after ReLU, with repeated batch and spatial dimensions observed.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/selecsls42b_training.txt#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
Operator: aten.relu_.default
cnt: 1, ((T([128, 32, 112, 112], f16),), {})
cnt: 7, ((T([128, 64, 56, 56], f16),), {})
cnt: 4, ((T([128, 32, 56, 56], f16),), {})
cnt: 1, ((T([128, 128, 56, 56], f16),), {})
cnt: 7, ((T([128, 144, 28, 28], f16),), {})
cnt: 4, ((T([128, 72, 28, 28], f16),), {})
cnt: 1, ((T([128, 288, 28, 28], f16),), {})
cnt: 7, ((T([128, 304, 14, 14], f16),), {})
cnt: 4, ((T([128, 152, 14, 14], f16),), {})
cnt: 1, ((T([128, 480, 14, 14], f16),), {})
cnt: 1, ((T([128, 960, 7, 7], f16),), {})
cnt: 1, ((T([128, 1024, 7, 7], f16),), {})
cnt: 1, ((T([128, 1280, 4, 4], f16),), {})
cnt: 1, ((T([128, 1024, 4, 4], f16),), {})
```

----------------------------------------

TITLE: Python Pseudo-code for PyTorch JIT prim::Loop Semantics
DESCRIPTION: Provides Python-like pseudo-code to clarify the execution semantics of the `prim::Loop` node. It shows the initialization of loop-carried variables, the loop condition logic combining the initial/iteration condition and the trip count, and how values are passed into and updated within the loop body.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_4

LANGUAGE: python
CODE:
```
y_1, ..., y_r = x_1, ..., x_r
condition = initial_condition
i = 0
while condition and i < max_trip_count:
    a_1, ..., a_r = y_1, ..., y_r

    ############################################################
    # Actual body of the loop
    b_1, ..., b_m = some::node(a_value_from_outside_of_the_loop, a_1)
    iter_condition = some::node(a_2)
    ############################################################

    y_1, ..., y_r = b_1, ..., b_r
    condition = iter_condition
    i += 1
```

----------------------------------------

TITLE: Documenting Diffs for CUTLASS Extension Headers (diff, C++)
DESCRIPTION: This snippet is a unified diff, included in a Markdown code block, showing all changes between original FasterTransformer C++ header files and their PyTorch-adapted versions. It illustrates changes in constructor signatures, added helper functions, comment toggling, include path alterations, and various compatibility edits. It assumes basic familiarity with C++ and CUDA, as well as CUTLASS and PyTorch extension layouts. No build or runtime dependencies are required to interpret the diff, but understanding the context requires knowledge of C++ headers and CUDA programming. Inputs: original and modified file versions; output: a diff code block for documentation and traceability.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/cuda/cutlass_extensions/README.md#2025-04-22_snippet_0

LANGUAGE: diff
CODE:
```
Only in FasterTransformer/src/fastertransformer/cutlass_extensions/include/cutlass_extensions: compute_occupancy.h
Only in FasterTransformer/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/epilogue: epilogue_quant_helper.h
Only in FasterTransformer/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/epilogue: threadblock
diff -r FasterTransformer/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/gemm/kernel/fpA_intB_gemm.h pytorch/aten/src/ATen/native/cuda/cutlass_extensions/gemm/kernel/fpA_intB_gemm.h
157c157,158
<     struct Params {
---
>     struct Params
>     {
183d183
<         CUTLASS_HOST_DEVICE
186d185
<         CUTLASS_HOST_DEVICE
188,190c187,188
<                cutlass::gemm::GemmCoord const& grid_tiled_shape,
<                const int                       gemm_k_size,
<                void*                           workspace = nullptr):
---
>                int                             device_sms,
>                int                             sm_occupancy):
192d189
<             grid_tiled_shape(grid_tiled_shape),
205,206d201
<             semaphore(static_cast<int*>(workspace)),
<             gemm_k_size(gemm_k_size),
210a206,227
>             ThreadblockSwizzle swizzle;
>             grid_tiled_shape = swizzle.get_tiled_shape(
>                 args.problem_size,
>                 {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
>                 args.batch_count);
>
>             gemm_k_size = args.problem_size.k();
>         }
>
>         size_t get_workspace_size() const
>         {
>             return 0;
>         }
>
>         Status init_workspace(void *workspace,cudaStream_t stream = nullptr)
>         {
>             return Status::kSuccess;
>         }
>
>         dim3 get_grid_dims() const
>         {
>             return ThreadblockSwizzle().get_grid_shape(grid_tiled_shape);
278,283d294
<     static size_t get_extra_workspace_size(Arguments const& args, cutlass::gemm::GemmCoord const& grid_tiled_shape)
<     {
<
<         return 0;
<     }
<
464a476,482
>     CUTLASS_DEVICE
>     static void invoke(Params const &params, SharedStorage &shared_storage)
>     {
>         GemmFpAIntB op;
>         op(params, shared_storage);
>     }
>
492c510
< }  // namespace cutlass
\ No newline at end of file
---
> }  // namespace cutlass
Only in FasterTransformer/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/gemm/kernel: gemm_moe_problem_visitor.h
Only in FasterTransformer/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/gemm/kernel: gemm_with_epilogue_visitor.h
Only in FasterTransformer/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/gemm/kernel: moe_cutlass_kernel.h
Only in FasterTransformer/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/gemm/kernel: moe_problem_visitor.h
diff -r FasterTransformer/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/gemm/warp/mma_tensorop_dequantizer.h pytorch/aten/src/ATen/native/cuda/cutlass_extensions/gemm/warp/mma_tensorop_dequantizer.h
55c55,58
< #include <src/fastertransformer/utils/cuda_bf16_wrapper.h>
---
> //#include <src/fastertransformer/utils/cuda_bf16_wrapper.h>
> //#ifdef ENABLE_BF16
> #include <cuda_bf16.h>
> //#endif
155c158,159
< #if (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800) && defined(ENABLE_BF16))
---
> //#if (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800) && defined(ENABLE_BF16))
> #if (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800))
470c474
< ////////////////////////////////////////////////////////////////////////////////
\ No newline at end of file
---
> ////////////////////////////////////////////////////////////////////////////////

```

----------------------------------------

TITLE: Legacy Device Construction Using Device Ordinals in PyTorch
DESCRIPTION: This code snippet shows the legacy method of constructing a device by providing a single device ordinal, which is treated as the current accelerator type (typically CUDA).
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_6

LANGUAGE: python
CODE:
```
>>> torch.device(1)
device(type='cuda', index=1)
```

----------------------------------------

TITLE: Syntactic Sugar for Catch-All Lambda Kernel Registration (C++)
DESCRIPTION: Shows a simplified syntax for registering a stateless lambda as a catch-all kernel. The lambda is passed directly as the second argument to `.op()`, implicitly registering it as a catch-all kernel without needing `.options().catchAllKernel()`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_4

LANGUAGE: cpp
CODE:
```
static auto registry = torch::RegisterOperators()
 .op("my_namespace::my_op", [] (Tensor a, Tensor b) {...});
```

----------------------------------------

TITLE: NVFuser Graph Fusion Debugging Command
DESCRIPTION: Shell command to enable detailed fusion graph logging for debugging NVFuser behavior.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/cuda/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
PYTORCH_JIT_LOG_LEVEL="graph_fuser" python <your pytorch script>
```

----------------------------------------

TITLE: Listing Available Benchmark Tests
DESCRIPTION: Displays all supported benchmark tests in the PyTorch operator benchmark suite.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
python -m pt.add_test --list-tests
```

----------------------------------------

TITLE: Adding test_jit Executable Target CMake
DESCRIPTION: Creates the main executable target named `test_jit`. This binary compiles and links all the specified JIT test source files (`JIT_TEST_SRCS`) along with a common test `main.cpp` file to produce the final test runner.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#_snippet_12

LANGUAGE: CMake
CODE:
```
add_executable(test_jit
  ${TORCH_ROOT}/test/cpp/common/main.cpp
  ${JIT_TEST_SRCS}
)
```

----------------------------------------

TITLE: Batch Normalization Operations in PyTorch with Float16 Tensors
DESCRIPTION: Records of batch normalization operations with various tensor shapes and parameters. Each entry shows the count (cnt) of operations with specific tensor configurations, including input tensors, running statistics, and epsilon value of 1e-05.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnext50_32x4d_training.txt#2025-04-22_snippet_4

LANGUAGE: python
CODE:
```
cnt: 8, ((T([8, 1024, 14, 14], f16), T([8, 1024, 14, 14], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), False, 1e-05, [True, True, True]), {})
cnt: 11, ((T([8, 512, 14, 14], f16), T([8, 512, 14, 14], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), False, 1e-05, [True, True, True]), {})
cnt: 6, ((T([8, 512, 28, 28], f16), T([8, 512, 28, 28], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), False, 1e-05, [True, True, True]), {})
cnt: 7, ((T([8, 256, 28, 28], f16), T([8, 256, 28, 28], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), False, 1e-05, [True, True, True]), {})
cnt: 5, ((T([8, 256, 56, 56], f16), T([8, 256, 56, 56], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), False, 1e-05, [True, True, True]), {})
cnt: 6, ((T([8, 128, 56, 56], f16), T([8, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), False, 1e-05, [True, True, True]), {})
cnt: 1, ((T([8, 64, 112, 112], f16), T([8, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})
```

----------------------------------------

TITLE: Running Tests with Pytest Shell
DESCRIPTION: The shell command is used to execute all tests in the specified Python test script related to distributed tensors in PyTorch using pytest. The test can be executed on either CPU or GPU. The command requires pytest to be installed and should be run from the project root.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/distributed/tensor/README.md#2025-04-22_snippet_0

LANGUAGE: shell
CODE:
```
pytest test/distributed/tensor/test_dtensor.py
```

----------------------------------------

TITLE: Running TorchBench Performance Benchmarks (Python)
DESCRIPTION: Commands to run TorchBench performance benchmarks for both training and inference using TorchInductor backend. The commands specify device, precision, and output format.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/README.md#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
./benchmarks/dynamo/torchbench.py --performance --training --amp --backend=inductor --output=torchbench_training.csv
./benchmarks/dynamo/torchbench.py --performance --inference --bfloat16 --backend=inductor --output=torchbench_inference.csv
```

----------------------------------------

TITLE: Running Code Coverage for Specific Test and Folder
DESCRIPTION: This example demonstrates how to run the code coverage tool for a specific test (atest) and generate reports only for the 'aten' folder.
SOURCE: https://github.com/pytorch/pytorch/blob/main/tools/code_coverage/README.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
python oss_coverage.py --run-only=atest --interest-only=aten
```

----------------------------------------

TITLE: Configuring NVCC Dependency Fix Wrapper (Bash)
DESCRIPTION: Exports `CMAKE_CUDA_COMPILER_LAUNCHER` to use a Python script wrapper (`tools/nvcc_fix_deps.py`) in conjunction with ccache. This wrapper fixes a potential bug in nvcc regarding header dependencies that can cause unnecessary CUDA file recompilations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_50

LANGUAGE: bash
CODE:
```
export CMAKE_CUDA_COMPILER_LAUNCHER="python;`pwd`/tools/nvcc_fix_deps.py;ccache"
python setup.py develop
```

----------------------------------------

TITLE: Padding Operations for Convolutional Neural Networks
DESCRIPTION: This snippet shows constant padding operations used to add zero-padding around feature maps in a convolutional neural network. Each operation adds 1 pixel of padding on all sides of half-precision (f16) tensors.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_7

LANGUAGE: python
CODE:
```
Operator: aten.constant_pad_nd.default
cnt: 4, ((T([0, 1, 28, 28], f16), [1, 1, 1, 1], 0.0), {})
```

----------------------------------------

TITLE: Configuring CMake Macros for C10 HIP
DESCRIPTION: Sets up the C10_HIP_BUILD_SHARED_LIBS variable and generates the hip_cmake_macros.h file from a template.
SOURCE: https://github.com/pytorch/pytorch/blob/main/c10/hip/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
set(C10_HIP_BUILD_SHARED_LIBS ${BUILD_SHARED_LIBS}) # used in cmake_macros.h.in
configure_file(
    ${CMAKE_CURRENT_LIST_DIR}/impl/hip_cmake_macros.h.in
    ${CMAKE_BINARY_DIR}/c10/hip/impl/hip_cmake_macros.h)
```

----------------------------------------

TITLE: Analyzing PyTorch Operator Usage in Deep Learning Model
DESCRIPTION: This code snippet provides a comprehensive overview of PyTorch operators used in a deep learning model. It includes operator names, call counts, input tensor shapes, and parameters for each operation. The analysis covers various operations like softmax, convolutions, pooling, and matrix multiplications, which are crucial for understanding the model's architecture and computation flow.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_botnext26ts_256_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([128, 1000], f16), 1, False), {})
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})
Operator: aten._softmax.default
cnt: 2, ((T([512, 256, 256], f16), -1, False), {})
cnt: 1, ((T([512, 64, 64], f16), -1, False), {})
Operator: aten._softmax_backward_data.default
cnt: 1, ((T([512, 64, 64], f16), T([512, 64, 64], f16), -1, f16), {})
cnt: 2, ((T([512, 256, 256], f16), T([512, 256, 256], f16), -1, f16), {})
Operator: aten._unsafe_view.default
cnt: 4, ((T([128, 64, 16, 16], f16), [512, 16, 256]), {})
cnt: 1, ((T([128, 256, 16, 16], f16), [512, 64, 256]), {})
cnt: 2, ((T([512, 256, 256], f16), [512, 256, 256]), {})
cnt: 4, ((T([512, 16, 16, 16], f16), [131072, 16]), {})
cnt: 4, ((T([131072, 31], f16), [512, 16, 16, 31]), {})
cnt: 2, ((T([512, 16, 16, 16, 16], f16), [512, 256, 256]), {})
cnt: 1, ((T([512, 256, 64], f16), [512, 256, 64]), {})
cnt: 2, ((T([512, 64, 256], f16), [128, 256, 16, 16]), {})
cnt: 1, ((T([128, 512, 16, 16], f16), [512, 128, 256]), {})
cnt: 1, ((T([512, 256, 128], f16), [512, 256, 128]), {})
cnt: 2, ((T([512, 128, 256], f16), [128, 512, 16, 16]), {})
cnt: 2, ((T([128, 64, 8, 8], f16), [512, 16, 64]), {})
cnt: 1, ((T([128, 512, 8, 8], f16), [512, 128, 64]), {})
cnt: 1, ((T([512, 64, 64], f16), [512, 64, 64]), {})
cnt: 2, ((T([512, 8, 8, 16], f16), [32768, 16]), {})
cnt: 2, ((T([32768, 15], f16), [512, 8, 8, 15]), {})
cnt: 1, ((T([512, 8, 8, 8, 8], f16), [512, 64, 64]), {})
cnt: 1, ((T([512, 64, 128], f16), [512, 64, 128]), {})
cnt: 2, ((T([512, 128, 64], f16), [128, 512, 8, 8]), {})
cnt: 1, ((T([512, 8, 8, 16], f16), [512, 64, 16]), {})
cnt: 1, ((T([512, 16, 64], f16), [128, 64, 8, 8]), {})
cnt: 2, ((T([512, 16, 16, 16], f16), [512, 256, 16]), {})
cnt: 2, ((T([512, 16, 256], f16), [128, 64, 16, 16]), {})
Operator: aten.add.Tensor
cnt: 31, ((T([], i64), 1), {})
cnt: 4, ((T([128, 256, 64, 64], f16), T([128, 256, 64, 64], f16)), {})
cnt: 4, ((T([128, 512, 32, 32], f16), T([128, 512, 32, 32], f16)), {})
cnt: 4, ((T([128, 1024, 16, 16], f16), T([128, 1024, 16, 16], f16)), {})
cnt: 2, ((T([512, 16, 16, 16, 16], f16, stride=(8432, 31, 527, 1, 0)), T([512, 16, 16, 16, 16], f16, stride=(8432, 527, 31, 0, 1))), {})
cnt: 2, ((T([512, 256, 256], f16), T([512, 256, 256], f16)), {})
cnt: 3, ((T([128, 2048, 8, 8], f16), T([128, 2048, 8, 8], f16)), {})
cnt: 1, ((T([512, 8, 8, 8, 8], f16, stride=(1080, 15, 135, 1, 0)), T([512, 8, 8, 8, 8], f16, stride=(1080, 135, 15, 0, 1))), {})
cnt: 1, ((T([512, 64, 64], f16), T([512, 64, 64], f16)), {})
cnt: 1, ((T([512, 8, 8, 16], f16, stride=(1024, 16, 128, 1)), T([512, 8, 8, 16], f16)), {})
cnt: 1, ((T([512, 64, 16], f16), T([512, 64, 16], f16)), {})
cnt: 2, ((T([512, 16, 16, 16], f16, stride=(4096, 16, 256, 1)), T([512, 16, 16, 16], f16)), {})
cnt: 2, ((T([512, 256, 16], f16), T([512, 256, 16], f16)), {})
cnt: 1, ((T([128, 256, 16, 16], f16), T([128, 256, 16, 16], f16)), {})
cnt: 2, ((T([128, 128, 32, 32], f16), T([128, 128, 32, 32], f16)), {})
cnt: 3, ((T([128, 64, 64, 64], f16), T([128, 64, 64, 64], f16)), {})
Operator: aten.addmm.default
cnt: 1, ((T([1000], f16), T([128, 2048], f16), T([2048, 1000], f16, stride=(1, 2048))), {})
Operator: aten.avg_pool2d.default
cnt: 1, ((T([128, 512, 16, 16], f16), [2, 2], [2, 2]), {})
Operator: aten.avg_pool2d_backward.default
cnt: 1, ((T([128, 512, 8, 8], f16), T([128, 512, 16, 16], f16), [2, 2], [2, 2], [0, 0], False, True, None), {})
Operator: aten.bmm.default
cnt: 2, ((T([512, 256, 16], f16, stride=(4096, 1, 256)), T([512, 16, 256], f16)), {})
cnt: 1, ((T([512, 256, 256], f16), T([512, 256, 64], f16, stride=(16384, 1, 256))), {})
cnt: 1, ((T([512, 256, 256], f16), T([512, 256, 128], f16, stride=(32768, 1, 256))), {})
cnt: 1, ((T([512, 64, 16], f16, stride=(1024, 1, 64)), T([512, 16, 64], f16)), {})
cnt: 1, ((T([512, 64, 64], f16), T([512, 64, 128], f16, stride=(8192, 1, 64))), {})
cnt: 1, ((T([512, 64, 64], f16, stride=(4096, 1, 64)), T([512, 64, 128], f16, stride=(8192, 1, 64))), {})
cnt: 1, ((T([512, 64, 128], f16, stride=(8192, 1, 64)), T([512, 128, 64], f16)), {})
cnt: 1, ((T([512, 16, 64], f16), T([512, 64, 64], f16)), {})
cnt: 1, ((T([512, 64, 64], f16), T([512, 64, 16], f16, stride=(1024, 1, 64))), {})
cnt: 1, ((T([512, 256, 256], f16, stride=(65536, 1, 256)), T([512, 256, 128], f16, stride=(32768, 1, 256))), {})
cnt: 1, ((T([512, 256, 128], f16, stride=(32768, 1, 256)), T([512, 128, 256], f16)), {})
cnt: 2, ((T([512, 16, 256], f16), T([512, 256, 256], f16)), {})
cnt: 2, ((T([512, 256, 256], f16), T([512, 256, 16], f16, stride=(4096, 1, 256))), {})
cnt: 1, ((T([512, 256, 256], f16, stride=(65536, 1, 256)), T([512, 256, 64], f16, stride=(16384, 1, 256))), {})
cnt: 1, ((T([512, 256, 64], f16, stride=(16384, 1, 256)), T([512, 64, 256], f16)), {})
Operator: aten.cat.default
cnt: 1, (([T([128, 64, 8, 8], f16), T([128, 64, 8, 8], f16), T([128, 512, 8, 8], f16)], 1), {})
cnt: 1, (([T([128, 64, 16, 16], f16), T([128, 64, 16, 16], f16), T([128, 512, 16, 16], f16)], 1), {})
cnt: 1, (([T([128, 64, 16, 16], f16), T([128, 64, 16, 16], f16), T([128, 256, 16, 16], f16)], 1), {})
Operator: aten.clone.default
cnt: 1, ((T([128, 3, 256, 256], f16),), {})
cnt: 1, ((T([128, 24, 128, 128], f16),), {})
cnt: 1, ((T([128, 32, 128, 128], f16),), {})
cnt: 1, ((T([128, 64, 128, 128], f16),), {})
cnt: 4, ((T([128, 64, 64, 64], f16),), {})
cnt: 2, ((T([128, 256, 64, 64], f16),), {})
cnt: 1, ((T([128, 128, 64, 64], f16),), {})
cnt: 3, ((T([128, 128, 32, 32], f16),), {})
cnt: 2, ((T([128, 512, 32, 32], f16),), {})
cnt: 1, ((T([128, 256, 32, 32], f16),), {})
cnt: 3, ((T([128, 256, 16, 16], f16),), {})
cnt: 2, ((T([128, 1024, 16, 16], f16),), {})
cnt: 1, ((T([128, 512, 16, 16], f16),), {})
cnt: 3, ((T([128, 512, 8, 8], f16),), {})
cnt: 2, ((T([128, 2048, 8, 8], f16),), {})
Operator: aten.constant_pad_nd.default
cnt: 4, ((T([8192, 16, 31], f16), [0, 1], 0.0), {})
cnt: 4, ((T([8192, 512], f16), [0, 15], 0.0), {})
cnt: 2, ((T([4096, 8, 15], f16), [0, 1], 0.0), {})
cnt: 2, ((T([4096, 128], f16), [0, 7], 0.0), {})
cnt: 2, ((T([4096, 135], f16), [0, -7]), {})
cnt: 2, ((T([4096, 8, 16], f16), [0, -1]), {})
cnt: 4, ((T([8192, 527], f16), [0, -15]), {})
cnt: 4, ((T([8192, 16, 32], f16), [0, -1]), {})
Operator: aten.convolution.default
cnt: 1, ((T([128, 3, 256, 256], f16), T([24, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 24, 128, 128], f16), T([32, 24, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 32, 128, 128], f16), T([64, 32, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 64, 64, 64], f16), T([64, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 64, 64, 64], f16), T([64, 16, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 4), {})
cnt: 2, ((T([128, 1, 64], f16), T([1, 1, 3], f16), None, [1], [1], [1], False, [0], 1), {})
cnt: 3, ((T([128, 64, 64, 64], f16), T([256, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 256, 64, 64], f16), T([64, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 256, 64, 64], f16), T([128, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 128, 64, 64], f16), T([128, 16, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 8), {})
cnt: 2, ((T([128, 1, 128], f16), T([1, 1, 5], f16), None, [1], [2], [1], False, [0], 1), {})
cnt: 2, ((T([128, 128, 32, 32], f16), T([512, 128, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 256, 64, 64], f16), T([512, 256, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 512, 32, 32], f16), T([128, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 128, 32, 32], f16), T([128, 16, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 8), {})
cnt: 1, ((T([128, 512, 32, 32], f16), T([256, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 256, 32, 32], f16), T([256, 16, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 16), {})
cnt: 1, ((T([128, 1, 256], f16), T([1, 1, 5], f16), None, [1], [2], [1], False, [0], 1), {})
cnt: 2, ((T([128, 256, 16, 16], f16), T([1024, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 512, 32, 32], f16), T([1024, 512, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 1024, 16, 16], f16), T([256, 1024, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 256, 16, 16], f16), T([384, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 1024, 16, 16], f16), T([512, 1024, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 512, 16, 16], f16), T([640, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([128, 512, 8, 8], f16), T([2048, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 1024, 16, 16], f16), T([2048, 1024, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 2048, 8, 8], f16), T([512, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([128, 512, 8, 8], f16), T([640, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
Operator: aten.convolution_backward.default
cnt: 2, ((T([128, 2048, 8, 8], f16), T([128, 512, 8, 8], f16), T([2048, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([128, 640, 8, 8], f16), T([128, 512, 8, 8], f16), T([640, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
```

----------------------------------------

TITLE: Analyzing Batch Normalization Parameters in PyTorch
DESCRIPTION: This code snippet represents a series of batch normalization parameter sets for various tensor shapes. Each line shows the count of occurrences, tensor shape, data type, and other parameters used in batch normalization operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/densenet121_training.txt#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
cnt: 24, ((T([4, 128, 14, 14], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([4, 288, 14, 14], f16), T([288], f16), T([288], f16), T([288], f16), T([288], f16), False, 0.1, 1e-05), {})
cnt: 1, ((T([4, 320, 14, 14], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f16), False, 0.1, 1e-05), {})
# ... (truncated for brevity)
cnt: 1, ((T([4, 800, 14, 14], f16), T([800], f16), T([800], f16), T([800], f16), T([800], f16), False, 0.1, 1e-05), {})
```

----------------------------------------

TITLE: PyTorch Batch Normalization Operations
DESCRIPTION: Native batch normalization forward and backward operation calls with tensor shapes, running statistics, and gradient computation parameters. Shows the usage pattern in a neural network with varying channel dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/levit_128_training.txt#2025-04-22_snippet_2

LANGUAGE: python
CODE:
```
Operator: aten.native_batch_norm.default
cnt: 1, ((T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), True, 0.1, 1e-05), {})
cnt: 1, ((T([128, 32, 56, 56], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})
```

----------------------------------------

TITLE: Configuring Link Libraries for PyTorch Python Binding
DESCRIPTION: Sets up the required libraries to link against when building the PyTorch Python extension. Includes Python, pybind11, and other dependencies.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_4

LANGUAGE: CMake
CODE:
```
set(TORCH_PYTHON_LINK_LIBRARIES
    Python::Module
    pybind::pybind11
    opentelemetry::api
    httplib
    nlohmann
    shm
    fmt::fmt-header-only
    ATEN_CPU_FILES_GEN_LIB)

if(USE_ASAN AND TARGET Sanitizer::address)
  list(APPEND TORCH_PYTHON_LINK_LIBRARIES Sanitizer::address)
endif()
if(USE_ASAN AND TARGET Sanitizer::undefined)
  list(APPEND TORCH_PYTHON_LINK_LIBRARIES Sanitizer::undefined)
endif()
if(USE_TSAN AND TARGET Sanitizer::thread)
  list(APPEND TORCH_PYTHON_LINK_LIBRARIES Sanitizer::thread)
endif()
```

----------------------------------------

TITLE: Performing Sum Operations in PyTorch
DESCRIPTION: This snippet shows the usage of sum operations on tensors. It includes both symbolic and default sum operations, with details on input tensor shapes and configurations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_16

LANGUAGE: Python
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([32, 1000], f16, stride=(0, 0)), [0], True), {})
Operator: aten.sum.default
cnt: 1, ((T([32, 1000], f16),), {})
```

----------------------------------------

TITLE: Computing Log Softmax Backward Data in PyTorch
DESCRIPTION: Calculates the gradient of the log softmax operation, critical for backpropagation in neural networks. This operation requires the input tensor, the output gradient tensor, and the dimension to perform the operation, all in FP16 format. It outputs a gradient tensor necessary for optimizing models via methods like SGD.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/Speech2Text2ForCausalLM_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
aten._log_softmax_backward_data.default(T([8192, 10000], f16), T([8192, 10000], f16), 1, f16)
```

----------------------------------------

TITLE: Add Stringop Overflow Warning Suppression (CMake)
DESCRIPTION: Appends the '-Wno-stringop-overflow' flag to C++ compiler flags if supported, suppressing related warnings.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#_snippet_31

LANGUAGE: CMake
CODE:
```
append_cxx_flag_if_supported("-Wno-stringop-overflow" CMAKE_CXX_FLAGS)
```

----------------------------------------

TITLE: Documenting Sample Arguments for aten.upsample_bilinear2d_backward.vec Operator - PyTorch - Python
DESCRIPTION: This snippet captures input and output tensor shapes for aten.upsample_bilinear2d_backward.vec, the backward pass for bilinear upsampling in PyTorch used for gradient computation. Each sample includes the grad_output tensor, optional placeholder (None), the input spatial size, the align_corners argument (True), and the scale factors. These samples are ideal for test coverage and behavioral checks during gradient calculations in f16 precision workflows.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_unet_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
Operator: aten.upsample_bilinear2d_backward.vec
cnt: 1, ((T([1, 64, 640, 958], f16), None, [1, 64, 320, 479], True, [2.0, 2.0]), {})
cnt: 1, ((T([1, 128, 320, 478], f16), None, [1, 128, 160, 239], True, [2.0, 2.0]), {})
cnt: 1, ((T([1, 256, 160, 238], f16), None, [1, 256, 80, 119], True, [2.0, 2.0]), {})
cnt: 1, ((T([1, 512, 80, 118], f16), None, [1, 512, 40, 59], True, [2.0, 2.0]), {})
```

----------------------------------------

TITLE: Tracking ReLU Backward Operations in PyTorch
DESCRIPTION: Records threshold_backward operations used during the backpropagation of ReLU activations. These operations handle gradient flow by zeroing out gradients where the corresponding inputs were negative.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
Operator: aten.threshold_backward.default
cnt: 2, ((T([128, 240, 1, 1], f16), T([128, 240, 1, 1], f16), 0), {})
cnt: 2, ((T([128, 168, 1, 1], f16), T([128, 168, 1, 1], f16), 0), {})
cnt: 1, ((T([128, 120, 1, 1], f16), T([128, 120, 1, 1], f16), 0), {})
cnt: 2, ((T([128, 32, 1, 1], f16), T([128, 32, 1, 1], f16), 0), {})
cnt: 4, ((T([128, 120, 28, 28], f16), T([128, 120, 28, 28], f16), 0), {})
cnt: 1, ((T([128, 24, 1, 1], f16), T([128, 24, 1, 1], f16), 0), {})
cnt: 1, ((T([128, 72, 28, 28], f16), T([128, 72, 28, 28], f16), 0), {})
cnt: 3, ((T([128, 72, 56, 56], f16), T([128, 72, 56, 56], f16), 0), {})
cnt: 1, ((T([128, 64, 56, 56], f16), T([128, 64, 56, 56], f16), 0), {})
cnt: 1, ((T([128, 64, 112, 112], f16), T([128, 64, 112, 112], f16), 0), {})
cnt: 1, ((T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16), 0), {})
```

----------------------------------------

TITLE: Specifying Target Operator in OpInfo with op=
DESCRIPTION: Illustrates how to use the `op=` parameter in an `OpInfo` definition when the OpInfo name needs to be globally unique for the test suite but differs from the actual PyTorch operator being tested (e.g., naming an OpInfo "ops.aten.bernoulli.p_deterministic" while testing `torch.ops.aten.bernoulli.p`).
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/onnx/torchlib/README.md#_snippet_3

LANGUAGE: python
CODE:
```
    opinfo_core.OpInfo(
        "ops.aten.bernoulli.p_deterministic",
        op=torch.ops.aten.bernoulli.p,

```

----------------------------------------

TITLE: Tracking SiLU Activation Operations in PyTorch
DESCRIPTION: Records of SiLU (Swish) activation forward and backward operations with various tensor shapes. Shows the inplace operations (_) and corresponding backward passes for different feature maps.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tinynet_a_training.txt#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
Operator: aten.silu_.default
cnt: 2, ((T([128, 32, 96, 96], f16),), {})
cnt: 1, ((T([128, 8, 1, 1], f16),), {})
cnt: 1, ((T([128, 96, 96, 96], f16),), {})
cnt: 1, ((T([128, 96, 48, 48], f16),), {})
cnt: 1, ((T([128, 4, 1, 1], f16),), {})
cnt: 3, ((T([128, 144, 48, 48], f16),), {})
cnt: 2, ((T([128, 6, 1, 1], f16),), {})
cnt: 1, ((T([128, 144, 24, 24], f16),), {})
cnt: 3, ((T([128, 240, 24, 24], f16),), {})
cnt: 2, ((T([128, 10, 1, 1], f16),), {})
cnt: 1, ((T([128, 240, 12, 12], f16),), {})
cnt: 8, ((T([128, 480, 12, 12], f16),), {})
cnt: 4, ((T([128, 20, 1, 1], f16),), {})
cnt: 7, ((T([128, 672, 12, 12], f16),), {})
cnt: 4, ((T([128, 28, 1, 1], f16),), {})
cnt: 1, ((T([128, 672, 6, 6], f16),), {})
cnt: 10, ((T([128, 1152, 6, 6], f16),), {})
cnt: 5, ((T([128, 48, 1, 1], f16),), {})
cnt: 1, ((T([128, 1280, 6, 6], f16),), {})
Operator: aten.silu_backward.default
cnt: 1, ((T([128, 1280, 6, 6], f16), T([128, 1280, 6, 6], f16)), {})
cnt: 5, ((T([128, 48, 1, 1], f16), T([128, 48, 1, 1], f16)), {})
cnt: 10, ((T([128, 1152, 6, 6], f16), T([128, 1152, 6, 6], f16)), {})
cnt: 4, ((T([128, 28, 1, 1], f16), T([128, 28, 1, 1], f16)), {})
cnt: 1, ((T([128, 672, 6, 6], f16), T([128, 672, 6, 6], f16)), {})
cnt: 7, ((T([128, 672, 12, 12], f16), T([128, 672, 12, 12], f16)), {})
cnt: 4, ((T([128, 20, 1, 1], f16), T([128, 20, 1, 1], f16)), {})
cnt: 8, ((T([128, 480, 12, 12], f16), T([128, 480, 12, 12], f16)), {})
cnt: 2, ((T([128, 10, 1, 1], f16), T([128, 10, 1, 1], f16)), {})
cnt: 1, ((T([128, 240, 12, 12], f16), T([128, 240, 12, 12], f16)), {})
cnt: 3, ((T([128, 240, 24, 24], f16), T([128, 240, 24, 24], f16)), {})
cnt: 2, ((T([128, 6, 1, 1], f16), T([128, 6, 1, 1], f16)), {})
cnt: 1, ((T([128, 144, 24, 24], f16), T([128, 144, 24, 24], f16)), {})
cnt: 3, ((T([128, 144, 48, 48], f16), T([128, 144, 48, 48], f16)), {})
cnt: 1, ((T([128, 4, 1, 1], f16), T([128, 4, 1, 1], f16)), {})
cnt: 1, ((T([128, 96, 48, 48], f16), T([128, 96, 48, 48], f16)), {})
cnt: 1, ((T([128, 96, 96, 96], f16), T([128, 96, 96, 96], f16)), {})
cnt: 1, ((T([128, 8, 1, 1], f16), T([128, 8, 1, 1], f16)), {})
cnt: 2, ((T([128, 32, 96, 96], f16), T([128, 32, 96, 96], f16)), {})
```

----------------------------------------

TITLE: Disabling JIT Optimizations via C++ API
DESCRIPTION: Presents the C++ function call to globally disable most runtime optimizations within the TorchScript JIT graph executor for the current process. This allows developers to bypass advanced optimizations like fusion and JIT autodiff for debugging purposes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_30

LANGUAGE: C++
CODE:
```
torch::jit::setGraphExecutorOptimize(false);
```

----------------------------------------

TITLE: Compiler-Specific Options Configuration
DESCRIPTION: Sets compiler-specific options to handle warnings and errors, particularly for non-MSVC compilers. Includes special handling for GCC 12+ and various warning suppressions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/api/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: cmake
CODE:
```
if(NOT MSVC)
  target_compile_options_if_supported(test_api "-Wno-missing-braces")
  target_compile_options_if_supported(test_api "-Wno-maybe-uninitialized")
  target_compile_options_if_supported(test_api "-Wno-unused-but-set-parameter")

  if(CMAKE_CXX_COMPILER_ID STREQUAL "GNU" AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 12)
    target_compile_options_if_supported(test_api "-Wno-error=nonnull")
  endif()
endif()
```

----------------------------------------

TITLE: Cross-Compiling and Benchmarking Caffe2/QNNPACK on ARMv7 Android - Bash
DESCRIPTION: Details the process for cloning the PyTorch repository and updating QNNPACK, cross-compiling Caffe2 with binaries for ARMv7 Android using clang, pushing the compiled `speed_benchmark` executable and the MobileNet v2 quantized model files to an Android device via `adb`, and executing the benchmark remotely using `adb shell`.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/README.md#_snippet_1

LANGUAGE: Bash
CODE:
```
# Clone PyTorch 1.0 repo
git clone --recursive https://github.com/pytorch/pytorch.git
cd pytorch

# Optional: update QNNPACK submodule to latest revision
git submodule update --remote third_party/QNNPACK

# Build Caffe2 (including binaries) for Android, and push to device
scripts/build_android.sh -DANDROID_TOOLCHAIN=clang -DBUILD_BINARY=ON
adb push build_android/bin/speed_benchmark /data/local/tmp/speed_benchmark

# Download model weights and copy them to Android device
wget https://s3.amazonaws.com/download.caffe2.ai/models/mobilenet_v2_1.0_224_quant/init_net.pb  # @lint-ignore
adb push init_net.pb /data/local/tmp/init_net.pb

# Download model graph and copy it to Android device
wget https://s3.amazonaws.com/download.caffe2.ai/models/mobilenet_v2_1.0_224_quant/predict_net.pb  # @lint-ignore
adb push predict_net.pb /data/local/tmp/predict_net.pb

# Run speed benchmark with 50 warm-up iterations and 10 measurement iterations
adb shell /data/local/tmp/speed_benchmark \
    --net /data/local/tmp/predict_net.pb \
    --init_net /data/local/tmp/init_net.pb \
    --input data --input_dims 1,3,224,224 --input_type float \
    --warmup 50 --iter 10
```

----------------------------------------

TITLE: Profiling aten.convolution.default Calls in PyTorch Text Trace
DESCRIPTION: Shows invocation signatures for forward convolution operations, with detailed arguments such as input/output tensor shapes, kernel shapes, strides, paddings, groups, and half-precision usage. These traces reflect the model's convolutional layer configuration and parameter counts, allowing for layer-wise architectural analysis or profiling. All data is shown as plain text, requiring reader familiarity with tensor and convolution conventions in PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_3

LANGUAGE: text
CODE:
```
Operator: aten.convolution.default
cnt: 1, ((T([32, 3, 224, 224], f16), T([32, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 32, 112, 112], f16), T([32, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 32), {})
cnt: 1, ((T([32, 32, 1, 1], f16), T([8, 32, 1, 1], f16), T([8], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 8, 1, 1], f16), T([32, 8, 1, 1], f16), T([32], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 32, 112, 112], f16), T([16, 32, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 16, 112, 112], f16), T([96, 16, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 96, 112, 112], f16), T([96, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 96), {})
cnt: 1, ((T([32, 96, 1, 1], f16), T([4, 96, 1, 1], f16), T([4], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 4, 1, 1], f16), T([96, 4, 1, 1], f16), T([96], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 96, 56, 56], f16), T([24, 96, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([32, 24, 56, 56], f16), T([144, 24, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 144, 56, 56], f16), T([144, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 144), {})
cnt: 2, ((T([32, 144, 1, 1], f16), T([6, 144, 1, 1], f16), T([6], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([32, 6, 1, 1], f16), T([144, 6, 1, 1], f16), T([144], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 144, 56, 56], f16), T([24, 144, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 144, 56, 56], f16), T([144, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 144), {})
cnt: 1, ((T([32, 144, 28, 28], f16), T([40, 144, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([32, 40, 28, 28], f16), T([240, 40, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 240, 28, 28], f16), T([240, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 240), {})
cnt: 2, ((T([32, 240, 1, 1], f16), T([10, 240, 1, 1], f16), T([10], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([32, 10, 1, 1], f16), T([240, 10, 1, 1], f16), T([240], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 240, 28, 28], f16), T([40, 240, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 240, 28, 28], f16), T([240, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 240), {})
cnt: 1, ((T([32, 240, 14, 14], f16), T([80, 240, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([32, 80, 14, 14], f16), T([480, 80, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([32, 480, 14, 14], f16), T([480, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 480), {})
cnt: 3, ((T([32, 480, 1, 1], f16), T([20, 480, 1, 1], f16), T([20], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([32, 20, 1, 1], f16), T([480, 20, 1, 1], f16), T([480], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([32, 480, 14, 14], f16), T([80, 480, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 480, 14, 14], f16), T([480, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 480), {})
cnt: 1, ((T([32, 480, 14, 14], f16), T([112, 480, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([32, 112, 14, 14], f16), T([672, 112, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([32, 672, 14, 14], f16), T([672, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 672), {})
cnt: 3, ((T([32, 672, 1, 1], f16), T([28, 672, 1, 1], f16), T([28], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([32, 28, 1, 1], f16), T([672, 28, 1, 1], f16), T([672], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 2, ((T([32, 672, 14, 14], f16), T([112, 672, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 672, 14, 14], f16), T([672, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 672), {})
cnt: 1, ((T([32, 672, 7, 7], f16), T([192, 672, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([32, 192, 7, 7], f16), T([1152, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([32, 1152, 7, 7], f16), T([1152, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 1152), {})
cnt: 4, ((T([32, 1152, 1, 1], f16), T([48, 1152, 1, 1], f16), T([48], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 4, ((T([32, 48, 1, 1], f16), T([1152, 48, 1, 1], f16), T([1152], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 3, ((T([32, 1152, 7, 7], f16), T([192, 1152, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 1152, 7, 7], f16), T([1152, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1152), {})
cnt: 1, ((T([32, 1152, 7, 7], f16), T([320, 1152, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
cnt: 1, ((T([32, 320, 7, 7], f16), T([1280, 320, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})
```

----------------------------------------

TITLE: Tensor Backward Softmax Operation in PyTorch
DESCRIPTION: This snippet highlights the use of `aten._log_softmax_backward_data.default`, which performs the backward operation of log softmax. It requires tensors for the input and gradient, along with the dimension and data type.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForCausalLM_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([2048, 50005], f16), T([2048, 50005], f16), 1, f16), {})
```

----------------------------------------

TITLE: Generating TorchScript Model Data
DESCRIPTION: Creates custom commands to generate TorchScript model data for both CPU and CUDA versions using Python scripts.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/aoti_inference/CMakeLists.txt#2025-04-22_snippet_1

LANGUAGE: cmake
CODE:
```
add_custom_command(
    OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/script_data.pt
           ${CMAKE_CURRENT_BINARY_DIR}/script_model_cpu.pt
           ${CMAKE_CURRENT_BINARY_DIR}/script_model_cuda.pt
    COMMAND python ${AOT_INDUCTOR_TEST_ROOT}/compile_model.py
    DEPENDS compile_model.py
)
add_custom_target(aoti_script_model ALL
    DEPENDS ${CMAKE_CURRENT_BINARY_DIR}/script_data.pt
    DEPENDS ${CMAKE_CURRENT_BINARY_DIR}/script_model_cpu.pt
    DEPENDS ${CMAKE_CURRENT_BINARY_DIR}/script_model_cuda.pt
)
add_dependencies(aoti_script_model aoti_custom_class)
```

----------------------------------------

TITLE: Implementing CUDA Tensor Multiplication in PyTorch
DESCRIPTION: This function implements element-wise tensor multiplication for CUDA devices in PyTorch. It takes two input tensors and performs the multiplication.
SOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_15

LANGUAGE: C++
CODE:
```
_ZN2at4_ops10mul_Tensor4callERKNS_6TensorES4_
```

----------------------------------------

TITLE: Describing aten._log_softmax_backward_data.default Usage - PyTorch - Python
DESCRIPTION: Documents the backward operation for _log_softmax, taking gradient output and input tensors, dimension argument and data type. It's crucial for propagating gradients through log_softmax layers. Deprecated usage or unsupported types should be verified for compatibility with custom backends.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ecaresnet101d_training.txt#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})
```

----------------------------------------

TITLE: Non-Set Examples in Python
DESCRIPTION: This snippet demonstrates examples that are not sets but might be confused with set syntax. It includes dictionary initialization, multi-line strings containing the word 'set', and a class with a 'set' method.
SOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/set_linter_testdata/python_code.py.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
d = {}
long_string = """ set()
set() set x.set set()
\""""

class A:
    def set(self, x):
        self.x = x

set = A().set
```

----------------------------------------

TITLE: Analytical Evaluation of Complex Gradients in PyTorch
DESCRIPTION: Mathematical formulation showing the analytical approach to compute the same scalar quantity as the numerical method for complex input gradcheck in PyTorch.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/gradcheck.rst#2025-04-22_snippet_2

LANGUAGE: math
CODE:
```
\begin{aligned}
    s &= 2 * v^T (real(CW) ur + i * imag(CW) ui) \\
      &= v^T real(2 * CW) ur + i * v^T imag(2 * CW) ui) \\
      &= real(v^T (2 * CW)) ur + i * imag(v^T (2 * CW)) ui
\end{aligned}
```

----------------------------------------

TITLE: Documenting Timer Class in PyTorch Benchmark Utils
DESCRIPTION: This snippet documents the Timer class from the torch.utils.benchmark module, including all its members.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/benchmark_utils.rst#2025-04-22_snippet_2

LANGUAGE: reStructuredText
CODE:
```
.. autoclass:: Timer
    :members:
```

----------------------------------------

TITLE: Invoking aten.sigmoid_backward.default with Tensor Arguments (Text)
DESCRIPTION: This section logs calls to the `aten.sigmoid_backward.default` operator, used during backpropagation to compute gradients for the sigmoid function. It takes the gradient of the output and the output of the original sigmoid function as inputs. The examples show calls with tensors of shapes like [128, 1536, 1, 1] and [128, 256, 1, 1], using the float16 (f16) data type for both arguments.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_20

LANGUAGE: text
CODE:
```
Operator: aten.sigmoid_backward.default
```

LANGUAGE: text
CODE:
```
cnt: 9, ((T([128, 1536, 1, 1], f16), T([128, 1536, 1, 1], f16)), {})
```

LANGUAGE: text
CODE:
```
cnt: 2, ((T([128, 512, 1, 1], f16), T([128, 512, 1, 1], f16)), {})
```

LANGUAGE: text
CODE:
```
cnt: 1, ((T([128, 256, 1, 1], f16), T([128, 256, 1, 1], f16)), {})
```

----------------------------------------

TITLE: Importing Control Plane Module in Python
DESCRIPTION: This snippet shows how to import the control_plane module from torch.distributed.elastic. It sets up the current module for documentation purposes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/control_plane.rst#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
.. automodule:: torch.distributed.elastic.control_plane
.. currentmodule:: torch.distributed.elastic.control_plane
```

----------------------------------------

TITLE: Defining Custom OpInfo in PyTorch/ONNXRuntime Tests
DESCRIPTION: Shows how to declare a new `OpInfo` entry for a PyTorch operator (`ops.aten.slice_scatter`) that doesn't have a suitable existing OpInfo. It specifies the operator name, supported data types (`dtypes`), a reference to the sample input function (`sample_inputs_func`), and indicates lack of support for `out=` arguments.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/onnx/torchlib/README.md#_snippet_2

LANGUAGE: python
CODE:
```
opinfo_core.OpInfo(
    "ops.aten.slice_scatter",
    aten_name="slice_scatter",
    dtypes=common_dtype.all_types_and(torch.bfloat16, torch.half, torch.bool),
    sample_inputs_func=sample_inputs_slice_scatter,
    supports_out=False,
),
```

----------------------------------------

TITLE: Analyzing Softmax Operations in PyTorch
DESCRIPTION: Shows the usage of softmax and log_softmax operations with their tensor shapes and parameters. The counts indicate these are primarily used in attention mechanisms with 16 attention heads and sequence lengths of 576 or 577.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([2, 1000], f16), 1, False), {})
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([2, 1000], f16), T([2, 1000], f16), 1, f16), {})
Operator: aten._softmax.default
cnt: 36, ((T([2, 16, 576, 576], f16, stride=(5308416, 1, 9216, 16)), -1, False), {})
cnt: 2, ((T([2, 16, 1, 577], f16), -1, False), {})
Operator: aten._softmax_backward_data.default
cnt: 2, ((T([2, 16, 1, 577], f16), T([2, 16, 1, 577], f16), -1, f16), {})
cnt: 36, ((T([2, 16, 576, 576], f16, stride=(5308416, 1, 9216, 16)), T([2, 16, 576, 576], f16), -1, f16), {})
```

----------------------------------------

TITLE: Summarizing ATen Operator Usage - PyTorch - Python
DESCRIPTION: This snippet catalogs calls to different low-level ATen operators used by PyTorch models, describing for each operator the argument structure and how often that signature appears during traced execution. It is assumed to be Python-generated inventory, possibly output of a tracing/profiling tool. Operators involve tensor algebra, data conversion, and neural network primitives, each listed with shapes, types, and optional argument dictionaries, but no code for execution. The list is for diagnostic/reporting purposes and does not execute transformations or model code directly.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForQuestionAnswering_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten._log_softmax.default
cnt: 2, ((T([64, 128], f16), 1, False), {})
Operator: aten._log_softmax_backward_data.default
cnt: 2, ((T([64, 128], f16), T([64, 128], f16), 1, f16), {})
Operator: aten._softmax.default
cnt: 12, ((T([64, 12, 128, 128], f16), -1, False), {})
Operator: aten._softmax_backward_data.default
cnt: 12, ((T([64, 12, 128, 128], f16), T([64, 12, 128, 128], f16), -1, f16), {})
Operator: aten._to_copy.default
cnt: 1, ((T([64, 1, 1, 128], f32),), {'dtype': f16})
cnt: 1, ((T([64, 128], b8),), {'dtype': i32})
cnt: 1, ((T([64, 128], i64),), {'dtype': i32, 'layout': torch.strided, 'device': 'cuda'})
cnt: 1, ((T([64, 128], i32),), {'dtype': i64})
Operator: aten._unsafe_view.default
cnt: 36, ((T([64, 12, 128, 64], f16), [768, 128, 64]), {})
cnt: 12, ((T([64, 12, 64, 128], f16), [768, 64, 128]), {})
cnt: 12, ((T([768, 128, 128], f16), [64, 12, 128, 128]), {})
cnt: 12, ((T([768, 128, 64], f16), [64, 12, 128, 64]), {})
cnt: 24, ((T([64, 128, 12, 64], f16), [64, 128, 768]), {})
cnt: 12, ((T([64, 128, 768], f16), [8192, 768]), {})
Operator: aten.add.Tensor
cnt: 1, ((T([64, 128], i32), 0), {})
cnt: 1, ((T([64, 128], i64), 0), {})
cnt: 73, ((T([64, 128, 768], f16), T([64, 128, 768], f16)), {})
cnt: 12, ((T([64, 12, 128, 128], f16), T([64, 1, 1, 128], f16)), {})
cnt: 1, ((T([], f16), T([], f16)), {})
Operator: aten.add_.Tensor
cnt: 1, ((T([64, 128, 768], f16), T([64, 128, 768], f16)), {})
Operator: aten.addmm.default
cnt: 48, ((T([768], f16), T([8192, 768], f16), T([768, 768], f16, stride=(1, 768))), {})
cnt: 12, ((T([3072], f16), T([8192, 768], f16), T([768, 3072], f16, stride=(1, 768))), {})
cnt: 12, ((T([768], f16), T([8192, 3072], f16), T([3072, 768], f16, stride=(1, 3072))), {})
cnt: 1, ((T([2], f16), T([8192, 768], f16), T([768, 2], f16, stride=(1, 768))), {})
Operator: aten.bmm.default
cnt: 12, ((T([768, 128, 64], f16), T([768, 64, 128], f16)), {})
cnt: 12, ((T([768, 128, 128], f16), T([768, 128, 64], f16)), {})
cnt: 12, ((T([768, 128, 128], f16, stride=(16384, 1, 128)), T([768, 128, 64], f16)), {})
cnt: 12, ((T([768, 128, 64], f16), T([768, 64, 128], f16, stride=(8192, 1, 64))), {})
cnt: 12, ((T([768, 64, 128], f16, stride=(8192, 1, 64)), T([768, 128, 128], f16)), {})
cnt: 12, ((T([768, 128, 128], f16), T([768, 128, 64], f16, stride=(8192, 1, 128))), {})
Operator: aten.cat.default
cnt: 1, (([T([64, 128, 1], f16), T([64, 128, 1], f16)], 2), {})
Operator: aten.clamp.default
cnt: 2, ((T([64], i64), 0, 128), {})
Operator: aten.clone.default
cnt: 1, ((T([64, 128], i64),), {})
cnt: 2, ((T([64], i64),), {})
Operator: aten.copy_.default
cnt: 1, ((T([64, 128], i64), T([64, 128], i64)), {})
cnt: 2, ((T([64], i64), T([64], i64)), {})
Operator: aten.cumsum.default
cnt: 1, ((T([64, 128], i32), 1), {})
Operator: aten.div.Tensor
cnt: 24, ((T([64, 12, 128, 128], f16), 8.0), {})
cnt: 2, ((T([], f16), 2), {})
Operator: aten.embedding.default
cnt: 1, ((T([30522, 768], f16), T([64, 128], i64), 0), {})
cnt: 1, ((T([2, 768], f16), T([64, 128], i64, stride=(0, 1))), {})
cnt: 1, ((T([512, 768], f16), T([64, 128], i64), 0), {})
Operator: aten.embedding_dense_backward.default
cnt: 1, ((T([64, 128, 768], f16), T([64, 128], i64), 512, 0, False), {})
cnt: 1, ((T([64, 128, 768], f16), T([64, 128], i64, stride=(0, 1)), 2, -1, False), {})
cnt: 1, ((T([64, 128, 768], f16), T([64, 128], i64), 30522, 0, False), {})
Operator: aten.gelu.default
cnt: 12, ((T([64, 128, 3072], f16),), {})
Operator: aten.gelu_backward.default
cnt: 12, ((T([64, 128, 3072], f16), T([64, 128, 3072], f16)), {})
Operator: aten.mm.default
cnt: 1, ((T([8192, 2], f16), T([2, 768], f16)), {})
cnt: 1, ((T([2, 8192], f16, stride=(1, 2)), T([8192, 768], f16)), {})
cnt: 12, ((T([8192, 768], f16), T([768, 3072], f16)), {})
cnt: 12, ((T([768, 8192], f16, stride=(1, 768)), T([8192, 3072], f16)), {})
cnt: 12, ((T([8192, 3072], f16), T([3072, 768], f16)), {})
cnt: 12, ((T([3072, 8192], f16, stride=(1, 3072)), T([8192, 768], f16)), {})
cnt: 48, ((T([8192, 768], f16), T([768, 768], f16)), {})
cnt: 48, ((T([768, 8192], f16, stride=(1, 768)), T([8192, 768], f16)), {})
Operator: aten.mul.Tensor
cnt: 1, ((T([64, 1, 1, 128], f16), -65504.0), {})
cnt: 1, ((T([64, 128], i32), T([64, 128], i32)), {})
Operator: aten.native_layer_norm.default
cnt: 25, ((T([64, 128, 768], f16), [768], T([768], f16), T([768], f16), 1e-12), {})
Operator: aten.native_layer_norm_backward.default
cnt: 25, ((T([64, 128, 768], f16), T([64, 128, 768], f16), [768], T([64, 128, 1], f32), T([64, 128, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})
Operator: aten.ne.Scalar
cnt: 1, ((T([64, 128], i64), 0), {})
Operator: aten.nll_loss_backward.default
cnt: 2, ((T([], f16), T([64, 128], f16), T([64], i64), None, 1, 128, T([], f16)), {})
Operator: aten.nll_loss_forward.default
cnt: 2, ((T([64, 128], f16), T([64], i64), None, 1, 128), {})
Operator: aten.rsub.Scalar
cnt: 1, ((T([64, 1, 1, 128], f16), 1.0), {})
Operator: aten.split.Tensor
cnt: 1, ((T([64, 128, 2], f16), 1, -1), {})
Operator: aten.sum.SymInt
cnt: 1, ((T([8192, 2], f16), [0], True), {})
cnt: 60, ((T([8192, 768], f16), [0], True), {})
cnt: 12, ((T([8192, 3072], f16), [0], True), {})
```

----------------------------------------

TITLE: Analyzing PyTorch Embedding Bag Operation Parameters
DESCRIPTION: Records of aten._embedding_bag.default operator calls with input tensors of shape [965, 192] in float16, various index tensors, offset tensor of shape [1024], and associated parameters. Each entry shows the count of occurrences for that specific parameter combination.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_0

LANGUAGE: python
CODE:
```
Operator: aten._embedding_bag.default
cnt: 2, ((T([965, 192], f16), T([54824], i64), T([1024], i64), False, 0, True, T([54824], f16)), {})
cnt: 2, ((T([965, 192], f16), T([54798], i64), T([1024], i64), False, 0, True, T([54798], f16)), {})
```

----------------------------------------

TITLE: Defining and Analyzing ATen Operator Call Patterns in PyTorch (Python)
DESCRIPTION: This snippet represents structured documentation and statistics on calls to PyTorch's ATen operators in Python projects, focusing on input/output tensor shapes, types, and invocation details. It is used for profiling code, optimizing tensor operations, or for backend dispatching and kernel generation. Inputs are described with rich shape/type/stride semantics, outputs and key parameters (e.g., device, dtype, layout) are annotated, and the information assumes knowledge of PyTorch's computational graph and operator schema.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForCausalLM_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten._log_softmax.default
cnt: 1, ((T([2048, 50265], f16), 1, False), {})
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([2048, 50265], f16), T([2048, 50265], f16), 1, f16), {})
Operator: aten._softmax.default
cnt: 12, ((T([256, 128, 128], f16), -1, False), {})
Operator: aten._softmax_backward_data.default
cnt: 12, ((T([256, 128, 128], f16), T([256, 128, 128], f16), -1, f16), {})
Operator: aten._to_copy.default
cnt: 1, ((T([128, 128], f32),), {'dtype': f16})
cnt: 1, ((T([16, 1, 128, 128], f16, stride=(0, 16384, 128, 1)),), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})
Operator: aten._unsafe_view.default
cnt: 36, ((T([16, 128, 16, 64], f16), [16, 128, 1024]), {})
cnt: 1, ((T([2048, 50265], f16), [16, 128, 50265]), {})
cnt: 12, ((T([16, 16, 128, 64], f16), [256, 128, 64]), {})
cnt: 12, ((T([16, 128, 1024], f16), [2048, 1024]), {})
Operator: aten.add.Tensor
cnt: 1, ((T([128], i64), 1), {})
cnt: 1, ((T([16, 128], i64, stride=(0, 1)), 2), {})
cnt: 73, ((T([16, 128, 1024], f16), T([16, 128, 1024], f16)), {})
cnt: 12, ((T([16, 16, 128, 128], f16), T([16, 1, 128, 128], f16)), {})
cnt: 1, ((T([50265, 1024], f16), T([50265, 1024], f16)), {})
Operator: aten.addmm.default
cnt: 48, ((T([1024], f16), T([2048, 1024], f16), T([1024, 1024], f16, stride=(1, 1024))), {})
cnt: 12, ((T([4096], f16), T([2048, 1024], f16), T([1024, 4096], f16, stride=(1, 1024))), {})
cnt: 12, ((T([1024], f16), T([2048, 4096], f16), T([4096, 1024], f16, stride=(1, 4096))), {})
Operator: aten.bmm.default
cnt: 24, ((T([256, 128, 64], f16), T([256, 64, 128], f16, stride=(8192, 1, 64))), {})
cnt: 24, ((T([256, 128, 128], f16), T([256, 128, 64], f16)), {})
cnt: 12, ((T([256, 128, 128], f16, stride=(16384, 1, 128)), T([256, 128, 64], f16)), {})
cnt: 12, ((T([256, 64, 128], f16, stride=(8192, 1, 64)), T([256, 128, 128], f16)), {})
Operator: aten.clone.default
cnt: 2, ((T([16, 128], i64),), {})
Operator: aten.copy_.default
cnt: 2, ((T([16, 128], i64), T([16, 128], i64)), {})
Operator: aten.embedding.default
cnt: 1, ((T([50265, 1024], f16), T([16, 128], i64), 1), {})
cnt: 1, ((T([1026, 1024], f16), T([16, 128], i64)), {})
Operator: aten.embedding_dense_backward.default
cnt: 1, ((T([16, 128, 1024], f16), T([16, 128], i64), 1026, -1, False), {})
cnt: 1, ((T([16, 128, 1024], f16), T([16, 128], i64), 50265, 1, False), {})
Operator: aten.gelu.default
cnt: 12, ((T([16, 128, 4096], f16),), {})
Operator: aten.gelu_backward.default
cnt: 12, ((T([16, 128, 4096], f16), T([16, 128, 4096], f16)), {})
Operator: aten.lt.Tensor
cnt: 1, ((T([128], i64), T([128, 1], i64)), {})
Operator: aten.masked_fill_.Scalar
cnt: 1, ((T([128, 128], f32), T([128, 128], b8), 0), {})
Operator: aten.mm.default
cnt: 1, ((T([2048, 1024], f16), T([1024, 50265], f16, stride=(1, 1024))), {})
cnt: 1, ((T([50265, 2048], f16, stride=(1, 50265)), T([2048, 1024], f16)), {})
cnt: 1, ((T([2048, 50265], f16), T([50265, 1024], f16)), {})
cnt: 12, ((T([2048, 1024], f16), T([1024, 4096], f16)), {})
cnt: 12, ((T([1024, 2048], f16, stride=(1, 1024)), T([2048, 4096], f16)), {})
cnt: 12, ((T([2048, 4096], f16), T([4096, 1024], f16)), {})
cnt: 12, ((T([4096, 2048], f16, stride=(1, 4096)), T([2048, 1024], f16)), {})
cnt: 48, ((T([2048, 1024], f16), T([1024, 1024], f16)), {})
cnt: 48, ((T([1024, 2048], f16, stride=(1, 1024)), T([2048, 1024], f16)), {})
Operator: aten.mul.Tensor
cnt: 2, ((T([16, 128, 1024], f16), 1.0), {})
cnt: 24, ((T([16, 128, 1024], f16), 0.125), {})
Operator: aten.native_layer_norm.default
cnt: 26, ((T([16, 128, 1024], f16), [1024], T([1024], f16), T([1024], f16), 1e-05), {})
Operator: aten.native_layer_norm_backward.default
cnt: 26, ((T([16, 128, 1024], f16), T([16, 128, 1024], f16), [1024], T([16, 128, 1], f32), T([16, 128, 1], f32), T([1024], f16), T([1024], f16), [True, True, True]), {})
Operator: aten.nll_loss_backward.default
cnt: 1, ((T([], f16), T([2048, 50265], f16), T([2048], i64), None, 1, -100, T([], f16)), {})
Operator: aten.nll_loss_forward.default
cnt: 1, ((T([2048, 50265], f16), T([2048], i64), None, 1, -100), {})
Operator: aten.sum.SymInt
cnt: 60, ((T([2048, 1024], f16), [0], True), {})
cnt: 12, ((T([2048, 4096], f16), [0], True), {})
```

----------------------------------------

TITLE: Registering Test Module for Operator Changes in PyTorch
DESCRIPTION: Example of registering a test module instance as a key with the corresponding changed operator name as value in the ALL_MODULES dictionary. This ensures the test model covers everything needed for testing backward compatibility.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/operator_upgraders/README.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
# key: test module instance, value: changed operator name
ALL_MODULES = {
    TestVersionedLinspaceV7(): "aten::linspace",
}
```

----------------------------------------

TITLE: Configuring C10 Library Basic Properties in CMake
DESCRIPTION: Sets up the minimum CMake version, project name, and basic build properties for the C10 library. Establishes C++17 as the standard and enables export of compile commands.
SOURCE: https://github.com/pytorch/pytorch/blob/main/c10/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
cmake_minimum_required(VERSION 3.18 FATAL_ERROR)
project(c10 CXX)

set(CMAKE_CXX_STANDARD 17 CACHE STRING "The C++ standard whose features are requested to build this target.")
set(CMAKE_EXPORT_COMPILE_COMMANDS ON)
```

----------------------------------------

TITLE: TorchScript LSTM Graph (After Derivative Preserving Opts)
DESCRIPTION: The TorchScript graph after applying derivative-preserving optimizations like dead code elimination, common subexpression elimination, and algebraic rewrites. These passes optimize the graph's efficiency without altering its ability to compute gradients correctly if required. In this specific example, the graph structure appears identical to the previous stage, suggesting no significant structural changes were triggered by these passes.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_26

LANGUAGE: TorchScript
CODE:
```
graph(%x : Float(*, *),
      %hx : Float(*, *),
      %cx : Float(*, *),
      %w_ih : Float(*, *),
      %w_hh : Float(*, *),
      %b_ih : Float(*),
      %b_hh : Float(*)):
  %8 : int = prim::Constant[value=1]()
  %9 : Float(*, *) = aten::t(%w_ih)
  %10 : Float(*, *) = aten::mm(%x, %9)
  %11 : Float(*, *) = aten::t(%w_hh)
  %12 : Float(*, *) = aten::mm(%hx, %11)
  %13 : Float(*, *) = aten::add(%10, %12, %8)
  %14 : Float(*, *) = aten::add(%13, %b_ih, %8)
  %gates : Float(*, *) = aten::add(%14, %b_hh, %8)
  %31 : Float(*, *), %32 : Float(*, *), %33 : Float(*, *), %34 : Float(*, *) = prim::ConstantChunk[chunks=4, dim=1](%gates)
  %ingate : Float(*, *) = aten::sigmoid(%31)
  %forgetgate : Float(*, *) = aten::sigmoid(%32)
  %cellgate : Float(*, *) = aten::tanh(%33)
  %outgate : Float(*, *) = aten::sigmoid(%34)
  %25 : Float(*, *) = aten::mul(%forgetgate, %cx)
  %26 : Float(*, *) = aten::mul(%ingate, %cellgate)
  %cy : Float(*, *) = aten::add(%25, %26, %8)
  %28 : Float(*, *) = aten::tanh(%cy)
  %hy : Float(*, *) = aten::mul(%outgate, %28)
  %30 : (Float(*, *), Float(*, *)) = prim::TupleConstruct(%hy, %cy)
  return (%30)
```

----------------------------------------

TITLE: Enumerating Inputs for aten.threshold_backward.default Operator - PyTorch - Python
DESCRIPTION: This snippet provides multiple example arguments for the aten.threshold_backward.default operator in PyTorch, wherein each entry details input and output tensor shapes (all using f16), alongside a threshold parameter (set to 0). The patterns illustrate varying spatial dimensions and channel sizes, helpful for validating correct broadcasting and thresholding logic. Each tuple packs inputs in a consistent order for reproducibility in automated test scripts or documentation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_unet_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
cnt: 4, ((T([1, 64, 640, 959], f16), T([1, 64, 640, 959], f16), 0), {})
cnt: 1, ((T([1, 64, 320, 479], f16), T([1, 64, 320, 479], f16), 0), {})
cnt: 3, ((T([1, 128, 320, 479], f16), T([1, 128, 320, 479], f16), 0), {})
cnt: 1, ((T([1, 128, 160, 239], f16), T([1, 128, 160, 239], f16), 0), {})
cnt: 3, ((T([1, 256, 160, 239], f16), T([1, 256, 160, 239], f16), 0), {})
cnt: 1, ((T([1, 256, 80, 119], f16), T([1, 256, 80, 119], f16), 0), {})
cnt: 3, ((T([1, 512, 80, 119], f16), T([1, 512, 80, 119], f16), 0), {})
cnt: 2, ((T([1, 512, 40, 59], f16), T([1, 512, 40, 59], f16), 0), {})
```

----------------------------------------

TITLE: Analyzing ATen Negative Log Likelihood Loss Forward in PyTorch
DESCRIPTION: Records usage of \"aten.nll_loss_forward.default\", for forward pass applications of NLL Loss computation in network models.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_22

LANGUAGE: pseudocode
CODE:
```
Operator: aten.nll_loss_forward.default
cnt: 1, ((T([1024, 50265], f16), T([1024], i64), None, 1, -100), {})
```

----------------------------------------

TITLE: Undoing CMAKE Compatibility Mode for PyTorch QNNPACK
DESCRIPTION: Handles CMAKE compatibility by unsetting the policy version minimum if using CMAKE version 4.0.0 or greater. This ensures the build process is compatible with different versions of CMAKE.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_6

LANGUAGE: CMake
CODE:
```
# -- [ Undo cmake-4 compat mode
if(CMAKE_VERSION VERSION_GREATER_EQUAL "4.0.0")
  unset(CMAKE_POLICY_VERSION_MINIMUM)
endif()
```

----------------------------------------

TITLE: Executing aten.nll_loss_forward Operator in PyTorch
DESCRIPTION: In this snippet, Aten’s nll_loss_forward.default operator is utilized to compute the negative log likelihood loss using input tensor and target indices. Dependencies involve PyTorch’s tensor operations, and inputs include a prediction tensor and index tensor of labels. The sponsor will output a loss value for optimization purposes, serving as a loss function for classification tasks. It requires consistent input shapes and indices range matching label count.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_10

LANGUAGE: Python
CODE:
```
cnt: 1, ((T([64, 1000], f16), T([64], i64), None, 1, -100), {})
```

----------------------------------------

TITLE: Profiling PyTorch SiLU (Swish) and SiLU Backward Operators - Python
DESCRIPTION: This snippet collects input shapes for the aten.silu_.default and aten.silu_backward.default operators, presenting in-place forward activation and their backward gradient operations. Inputs demonstrate varied shapes and dimensions, using float16 for simulated large activation maps. Useful for activation kernel regression and correctness validation, with dependencies limited to PyTorch and ATen conventions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 24, 128, 128], f16),), {})
cnt: 1, ((T([128, 32, 128, 128], f16),), {})
cnt: 1, ((T([128, 64, 128, 128], f16),), {})
cnt: 4, ((T([128, 64, 64, 64], f16),), {})
cnt: 2, ((T([128, 256, 64, 64], f16),), {})
cnt: 1, ((T([128, 128, 64, 64], f16),), {})
cnt: 3, ((T([128, 128, 32, 32], f16),), {})
cnt: 2, ((T([128, 512, 32, 32], f16),), {})
cnt: 1, ((T([128, 256, 32, 32], f16),), {})
cnt: 3, ((T([128, 256, 16, 16], f16),), {})
cnt: 2, ((T([128, 1024, 16, 16], f16),), {})
cnt: 1, ((T([128, 512, 16, 16], f16),), {})
cnt: 3, ((T([128, 512, 8, 8], f16),), {})
cnt: 2, ((T([128, 2048, 8, 8], f16),), {})
cnt: 2, ((T([128, 2048, 8, 8], f16), T([128, 2048, 8, 8], f16)), {})
cnt: 3, ((T([128, 512, 8, 8], f16), T([128, 512, 8, 8], f16)), {})
cnt: 1, ((T([128, 512, 16, 16], f16), T([128, 512, 16, 16], f16)), {})
cnt: 2, ((T([128, 1024, 16, 16], f16), T([128, 1024, 16, 16], f16)), {})
cnt: 3, ((T([128, 256, 16, 16], f16), T([128, 256, 16, 16], f16)), {})
cnt: 1, ((T([128, 256, 32, 32], f16), T([128, 256, 32, 32], f16)), {})
cnt: 2, ((T([128, 512, 32, 32], f16), T([128, 512, 32, 32], f16)), {})
cnt: 3, ((T([128, 128, 32, 32], f16), T([128, 128, 32, 32], f16)), {})
cnt: 1, ((T([128, 128, 64, 64], f16), T([128, 128, 64, 64], f16)), {})
cnt: 2, ((T([128, 256, 64, 64], f16), T([128, 256, 64, 64], f16)), {})
cnt: 4, ((T([128, 64, 64, 64], f16), T([128, 64, 64, 64], f16)), {})
cnt: 1, ((T([128, 64, 128, 128], f16), T([128, 64, 128, 128], f16)), {})
cnt: 1, ((T([128, 32, 128, 128], f16), T([128, 32, 128, 128], f16)), {})
cnt: 1, ((T([128, 24, 128, 128], f16), T([128, 24, 128, 128], f16)), {})
```

----------------------------------------

TITLE: Defining a CUDA-Specific Test in PyTorch C++ Frontend (C++)
DESCRIPTION: Defines a GoogleTest test case intended to run only on systems with CUDA available. The `_CUDA` suffix appended to the test case name signals the test runner (specifically logic in `main.cpp`) to execute this test only if CUDA devices are detected. This relies on the GoogleTest framework and PyTorch's custom test filtering.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/api/README.md#2025-04-22_snippet_0

LANGUAGE: cpp
CODE:
```
```cpp
TEST(MyTestSuite, MyTestCase_CUDA) { }
```
```

----------------------------------------

TITLE: Defining Matrix Multiplication Benchmark with GroupedStmts
DESCRIPTION: Example of defining a benchmark using GroupedStmts to test matrix multiplication in both Python and C++. It includes setup code, specifies a function signature, and enables TorchScript and autograd variants.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/instruction_counts/README.md#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
# `GroupedStmts` is an alias of `GroupedBenchmark.init_from_stmts`
benchmark = GroupedStmts(
    py_stmt=r"y = x * w",
    cpp_stmt=r"auto y = x * w;",

    setup=GroupedSetup(
        py_setup="""
            x = torch.ones((4, 4))
            w = torch.ones((4, 4), requires_grad=True)
        """,
        cpp_setup="""
            auto x = torch::ones((4, 4));
            auto w = torch::ones((4, 4));
            w.set_requires_grad(true);
        """,
    ),

    signature="f(x, w) -> y",
    torchscript=True,
    autograd=True,
),
```

----------------------------------------

TITLE: Analyzing ATen Negative Log Likelihood Loss Backward in PyTorch
DESCRIPTION: Logs occurrences of \"aten.nll_loss_backward.default\", used during training to compute gradients for NLL Loss functions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_21

LANGUAGE: pseudocode
CODE:
```
Operator: aten.nll_loss_backward.default
cnt: 1, ((T([], f16), T([1024, 50265], f16), T([1024], i64), None, 1, -100, T([], f16)), {})
```

----------------------------------------

TITLE: Analyzing PyTorch Slice Backward Operation Calls
DESCRIPTION: Logs of aten.slice_backward.default operations showing gradients for sliced tensors. Each call includes the tensor shape, original shape, dimension, start index, end index, and stride information.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_9

LANGUAGE: python
CODE:
```
Operator: aten.slice_backward.default
cnt: 1, ((T([128, 11, 7, 7], f16, stride=(9065, 49, 7, 1)), [128, 185, 7, 7], 1, 174, 9223372036854775807, 1), {})
cnt: 2, ((T([128, 185, 7, 7], f16), [128, 185, 7, 7], 0, 0, 9223372036854775807, 1), {})
cnt: 1, ((T([128, 174, 7, 7], f16, stride=(9065, 49, 7, 1)), [128, 185, 7, 7], 1, 0, 174, 1), {})
cnt: 1, ((T([128, 12, 7, 7], f16, stride=(8526, 49, 7, 1)), [128, 174, 7, 7], 1, 162, 9223372036854775807, 1), {})
cnt: 2, ((T([128, 174, 7, 7], f16), [128, 174, 7, 7], 0, 0, 9223372036854775807, 1), {})
cnt: 1, ((T([128, 162, 7, 7], f16, stride=(8526, 49, 7, 1)), [128, 174, 7, 7], 1, 0, 162, 1), {})
cnt: 1, ((T([128, 11, 7, 7], f16, stride=(7938, 49, 7, 1)), [128, 162, 7, 7], 1, 151, 9223372036854775807, 1), {})
cnt: 2, ((T([128, 162, 7, 7], f16), [128, 162, 7, 7], 0, 0, 9223372036854775807, 1), {})
cnt: 1, ((T([128, 151, 7, 7], f16, stride=(7938, 49, 7, 1)), [128, 162, 7, 7], 1, 0, 151, 1), {})
cnt: 1, ((T([128, 11, 7, 7], f16, stride=(7399, 49, 7, 1)), [128, 151, 7, 7], 1, 140, 9223372036854775807, 1), {})
cnt: 2, ((T([128, 151, 7, 7], f16), [128, 151, 7, 7], 0, 0, 9223372036854775807, 1), {})
cnt: 1, ((T([128, 140, 7, 7], f16, stride=(7399, 49, 7, 1)), [128, 151, 7, 7], 1, 0, 140, 1), {})
cnt: 1, ((T([128, 11, 14, 14], f16, stride=(25088, 196, 14, 1)), [128, 128, 14, 14], 1, 117, 9223372036854775807, 1), {})
cnt: 2, ((T([128, 128, 14, 14], f16), [128, 128, 14, 14], 0, 0, 9223372036854775807, 1), {})
cnt: 1, ((T([128, 117, 14, 14], f16, stride=(25088, 196, 14, 1)), [128, 128, 14, 14], 1, 0, 117, 1), {})
cnt: 1, ((T([128, 11, 14, 14], f16, stride=(22932, 196, 14, 1)), [128, 117, 14, 14], 1, 106, 9223372036854775807, 1), {})
cnt: 2, ((T([128, 117, 14, 14], f16), [128, 117, 14, 14], 0, 0, 9223372036854775807, 1), {})
cnt: 1, ((T([128, 106, 14, 14], f16, stride=(22932, 196, 14, 1)), [128, 117, 14, 14], 1, 0, 106, 1), {})
cnt: 1, ((T([128, 11, 14, 14], f16, stride=(20776, 196, 14, 1)), [128, 106, 14, 14], 1, 95, 9223372036854775807, 1), {})
cnt: 2, ((T([128, 106, 14, 14], f16), [128, 106, 14, 14], 0, 0, 9223372036854775807, 1), {})
cnt: 1, ((T([128, 95, 14, 14], f16, stride=(20776, 196, 14, 1)), [128, 106, 14, 14], 1, 0, 95, 1), {})
cnt: 1, ((T([128, 11, 14, 14], f16, stride=(18620, 196, 14, 1)), [128, 95, 14, 14], 1, 84, 9223372036854775807, 1), {})
cnt: 2, ((T([128, 95, 14, 14], f16), [128, 95, 14, 14], 0, 0, 9223372036854775807, 1), {})
cnt: 1, ((T([128, 84, 14, 14], f16, stride=(18620, 196, 14, 1)), [128, 95, 14, 14], 1, 0, 84, 1), {})
cnt: 1, ((T([128, 12, 14, 14], f16, stride=(16464, 196, 14, 1)), [128, 84, 14, 14], 1, 72, 9223372036854775807, 1), {})
cnt: 2, ((T([128, 84, 14, 14], f16), [128, 84, 14, 14], 0, 0, 9223372036854775807, 1), {})
cnt: 1, ((T([128, 72, 14, 14], f16, stride=(16464, 196, 14, 1)), [128, 84, 14, 14], 1, 0, 72, 1), {})
cnt: 1, ((T([128, 11, 28, 28], f16, stride=(47824, 784, 28, 1)), [128, 61, 28, 28], 1, 50, 9223372036854775807, 1), {})
cnt: 2, ((T([128, 61, 28, 28], f16), [128, 61, 28, 28], 0, 0, 9223372036854775807, 1), {})
cnt: 1, ((T([128, 50, 28, 28], f16, stride=(47824, 784, 28, 1)), [128, 61, 28, 28], 1, 0, 50, 1), {})
cnt: 1, ((T([128, 11, 56, 56], f16, stride=(119168, 3136, 56, 1)), [128, 38, 56, 56], 1, 27, 9223372036854775807, 1), {})
cnt: 2, ((T([128, 38, 56, 56], f16), [128, 38, 56, 56], 0, 0, 9223372036854775807, 1), {})
cnt: 1, ((T([128, 27, 56, 56], f16, stride=(119168, 3136, 56, 1)), [128, 38, 56, 56], 1, 0, 27, 1), {})
```

----------------------------------------

TITLE: Profiling Tensor Copy Operations in PyTorch
DESCRIPTION: Log of operations that create fresh copies of tensors, detached from computational graphs. This single operation creates a fresh copy of a 64-element integer tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_18

LANGUAGE: python
CODE:
```
Operator: aten.lift_fresh_copy.default
cnt: 1, ((T([64], i64),), {})
```

----------------------------------------

TITLE: Analyzing Tensor Shape Patterns in PyTorch Operations
DESCRIPTION: Logs showing tensor shapes appearing in PyTorch operations. Each line shows call count, tensor shapes, and data type information. These represent input tensor specifications across different network layers.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_12

LANGUAGE: python
CODE:
```
cnt: 1, ((T([128, 570, 14, 14], f16),), {})
cnt: 1, ((T([128, 636, 14, 14], f16),), {})
cnt: 1, ((T([128, 702, 14, 14], f16),), {})
cnt: 1, ((T([128, 768, 14, 14], f16),), {})
cnt: 1, ((T([128, 840, 7, 7], f16),), {})
cnt: 1, ((T([128, 906, 7, 7], f16),), {})
cnt: 1, ((T([128, 972, 7, 7], f16),), {})
cnt: 1, ((T([128, 1044, 7, 7], f16),), {})
cnt: 1, ((T([128, 1280, 7, 7], f16),), {})
```

----------------------------------------

TITLE: Promoting Wheels to PyPI
DESCRIPTION: Script to promote wheels to PyPI. This is a one-way operation that cannot be undone. Requires PyPI repository access.
SOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release/README.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
promote/wheel_to_pypi.sh
```

----------------------------------------

TITLE: PyTorch Tensor Operations Analysis
DESCRIPTION: Detailed breakdown of PyTorch operator usage showing tensor shapes, data types, and operation counts. Includes operations like batch normalization, ReLU activation, tensor multiplication, and division across different tensor dimensions.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_regnet_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
cnt: 1, ((T([32, 224, 112, 112], f16), T([32, 32, 112, 112], f16), T([224, 32, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})
cnt: 1, ((T([32, 32, 112, 112], f16), T([32, 3, 224, 224], f16), T([32, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})
Operator: aten.copy_.default
cnt: 1, ((T([32, 3, 224, 224], f16), T([32, 3, 224, 224], f16)), {})
```

----------------------------------------

TITLE: Running Code Coverage for Specific Test
DESCRIPTION: This snippet shows how to run the code coverage tool for a specific test (atest) and generate reports for the entire PyTorch folder.
SOURCE: https://github.com/pytorch/pytorch/blob/main/tools/code_coverage/README.md#2025-04-22_snippet_2

LANGUAGE: bash
CODE:
```
python oss_coverage.py --run-only=atest
```

----------------------------------------

TITLE: Configuring PyTorch CPU Test Sources and Includes with CMake
DESCRIPTION: This CMake snippet glob's test files, appends specific source files to the CPU sources list, adds include directories, and sets variables in the parent scope. It includes third-party miniz library and custom adapter files.
SOURCE: https://github.com/pytorch/pytorch/blob/main/caffe2/serialize/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: CMake
CODE:
```
file(GLOB tmp *_test.cc)

set(Caffe2_CPU_TEST_SRCS ${Caffe2_CPU_TEST_SRCS} ${tmp})
list(APPEND Caffe2_CPU_SRCS
  ${PROJECT_SOURCE_DIR}/third_party/miniz-3.0.2/miniz.c
  ${CMAKE_CURRENT_SOURCE_DIR}/inline_container.cc
  ${CMAKE_CURRENT_SOURCE_DIR}/istream_adapter.cc
  ${CMAKE_CURRENT_SOURCE_DIR}/file_adapter.cc
  ${CMAKE_CURRENT_SOURCE_DIR}/crc.cc
  ${CMAKE_CURRENT_SOURCE_DIR}/read_adapter_interface.cc)
list(APPEND Caffe2_CPU_INCLUDE ${PROJECT_SOURCE_DIR}/third_party/miniz-3.0.2)

set(Caffe2_CPU_TEST_SRCS ${Caffe2_CPU_TEST_SRCS} PARENT_SCOPE)
set(Caffe2_CPU_SRCS ${Caffe2_CPU_SRCS} PARENT_SCOPE)
set(Caffe2_CPU_INCLUDE ${Caffe2_CPU_INCLUDE} PARENT_SCOPE)
```

----------------------------------------

TITLE: Using GLOBAL Opcode in Pickle Format
DESCRIPTION: Example of a GLOBAL opcode in pickle format that is used to identify the implementation of an object's type during the dependency resolution process.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_16

LANGUAGE: text
CODE:
```
GLOBAL 'torchvision.models.resnet Resnet`
```

----------------------------------------

TITLE: Configuring MPI Tests and Example
DESCRIPTION: Sets up MPI-specific tests and configures an allreduce example for Linux systems with Gloo support.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/c10d/CMakeLists.txt#2025-04-22_snippet_4

LANGUAGE: cmake
CODE:
```
if(USE_MPI AND USE_C10D_MPI)
  add_definitions(-DMPIEXEC=${MPIEXEC})
  c10d_add_test(ProcessGroupMPITest.cpp LINK_LIBRARIES torch_cpu MPI::MPI_CXX INSTALL_TEST ${INSTALL_TEST})
endif()

if(LINUX AND USE_GLOO AND USE_C10D_GLOO)
  add_executable(example_allreduce example/allreduce.cpp)
  target_include_directories(example_allreduce PRIVATE $<BUILD_INTERFACE:${TORCH_SRC_DIR}/csrc/distributed>)
  target_link_libraries(example_allreduce torch_cpu)
  if(USE_CUDA)
    target_link_libraries(example_allreduce torch_cuda)
  endif()
endif()
```

----------------------------------------

TITLE: Training Commit Classifier in Python
DESCRIPTION: Commands for training and using a machine learning classifier for commit categorization
SOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release_notes/README.md#2025-04-22_snippet_3

LANGUAGE: bash
CODE:
```
python classifier.py --train
python categorize.py --use_classifier
```

----------------------------------------

TITLE: Using Annotations for Tensor Mutation and Aliasing
DESCRIPTION: This code snippet illustrates the use of annotations to denote mutation and aliasing of Tensor objects. It covers different cases such as in-place operations and output arguments and explains the requirements for using annotations like '(a!)'. Examples of function schemas with annotations are provided, detailing changes as PyTorch moves towards schema unification.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_4

LANGUAGE: C++
CODE:
```
abs(Tensor self) -> Tensor
```

LANGUAGE: C++
CODE:
```
abs_(Tensor(a!) self) -> Tensor(a!)
```

LANGUAGE: C++
CODE:
```
abs(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
```

LANGUAGE: C++
CODE:
```
transpose(Tensor(a) self, int dim0, int dim1) -> Tensor(a)
```

LANGUAGE: C++
CODE:
```
func: chunk(Tensor(a -> *) self, int chunks, int dim=0) -> Tensor(a)[]
```

----------------------------------------

TITLE: Representing Differentiable Subgraph in TorchScript Graph
DESCRIPTION: Shows the structure of a TorchScript graph after derivative splitting. It defines a main graph calling a `prim::DifferentiableGraph` node, which encapsulates operations with known symbolic gradients, demonstrating the explicit forward pass computation within the JIT IR.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_28

LANGUAGE: TorchScript Graph
CODE:
```
graph(%x : Float(*, *),
      %hx : Float(*, *),
      %cx : Float(*, *),
      %w_ih : Float(*, *),
      %w_hh : Float(*, *),
      %b_ih : Float(*),
      %b_hh : Float(*)):
  %8 : int = prim::Constant[value=1]()
  %hy : Float(*, *), %cy : Float(*, *) = prim::DifferentiableGraph_0(%cx, %b_hh, %b_ih, %hx, %w_hh, %x, %w_ih)
  %30 : (Float(*, *), Float(*, *)) = prim::TupleConstruct(%hy, %cy)
  return (%30)
with prim::DifferentiableGraph_0 = graph(%13 : Float(*, *),
      %29 : Float(*),
      %33 : Float(*),
      %40 : Float(*, *),
      %43 : Float(*, *),
      %45 : Float(*, *),
      %48 : Float(*, *)):
  %49 : Float(*, *) = aten::t(%48)
  %47 : Float(*, *) = aten::mm(%45, %49)
  %44 : Float(*, *) = aten::t(%43)
  %42 : Float(*, *) = aten::mm(%40, %44)
  %38 : int = prim::Constant[value=1]()
  %39 : Float(*, *) = aten::add(%47, %42, %38)
  %35 : Float(*, *) = aten::add(%39, %33, %38)
  %gates : Float(*, *) = aten::add(%35, %29, %38)
  %24 : Float(*, *), %25 : Float(*, *), %26 : Float(*, *), %27 : Float(*, *) = prim::ConstantChunk[chunks=4, dim=1](%gates)
  %ingate : Float(*, *) = aten::sigmoid(%24)
  %forgetgate : Float(*, *) = aten::sigmoid(%25)
  %cellgate : Float(*, *) = aten::tanh(%26)
  %outgate : Float(*, *) = aten::sigmoid(%27)
  %14 : Float(*, *) = aten::mul(%forgetgate, %13)
  %11 : Float(*, *) = aten::mul(%ingate, %cellgate)
  %cy : Float(*, *) = aten::add(%14, %11, %38)
  %4 : Float(*, *) = aten::tanh(%cy)
  %hy : Float(*, *) = aten::mul(%outgate, %4)
  return (%hy, %cy)
```

----------------------------------------

TITLE: Configuring PyTorch Vision JNI Library Build
DESCRIPTION: Configures CMake build settings for a shared library that provides JNI bindings for PyTorch Vision. Sets C++17 standard, enables verbose build output, configures source files, and sets up compilation options for exception handling.
SOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android_torchvision/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
cmake_minimum_required(VERSION 3.4.1)
project(pytorch_vision_jni CXX)
set(CMAKE_CXX_STANDARD 17 CACHE STRING "The C++ standard whose features are requested to build this target.")
set(CMAKE_VERBOSE_MAKEFILE ON)

set(pytorch_vision_cpp_DIR ${CMAKE_CURRENT_LIST_DIR}/src/main/cpp)

file(GLOB pytorch_vision_SOURCES
  ${pytorch_vision_cpp_DIR}/pytorch_vision_jni.cpp
)

add_library(pytorch_vision_jni SHARED
    ${pytorch_vision_SOURCES}
)

target_compile_options(pytorch_vision_jni PRIVATE
  -fexceptions
)

set(BUILD_SUBDIR ${ANDROID_ABI})

target_link_libraries(pytorch_vision_jni)
```

----------------------------------------

TITLE: Accessing PyTorch Version Information in C++
DESCRIPTION: Example code demonstrating how to access PyTorch version information using version macros. Shows both accessing individual version components (major, minor, patch) and the complete version string using TORCH_VERSION macro.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/versioning.rst#2025-04-22_snippet_0

LANGUAGE: cpp
CODE:
```
#include <torch/torch.h>
#include <iostream>

int main() {
  std::cout << "PyTorch version from parts: "
    << TORCH_VERSION_MAJOR << "."
    << TORCH_VERSION_MINOR << "."
    << TORCH_VERSION_PATCH << std::endl;
  std::cout << "PyTorch version: " << TORCH_VERSION << std::endl;
}
```

----------------------------------------

TITLE: Including Vulkan Dependency Definitions in CMake
DESCRIPTION: If the `USE_VULKAN` flag is ON, this block initializes empty `Vulkan_LIBS` and `Vulkan_INCLUDES` variables and then includes the `VulkanDependencies.cmake` file. This external file is expected to populate the variables with the necessary Vulkan libraries and include paths.
SOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_12

LANGUAGE: cmake
CODE:
```
# ---[ Vulkan deps
if(USE_VULKAN)
  set(Vulkan_LIBS)
  set(Vulkan_INCLUDES)
  include(${CMAKE_CURRENT_LIST_DIR}/../../cmake/VulkanDependencies.cmake)
endif()
```

----------------------------------------

TITLE: Conditionally Add ITT Profiler Sources - CMake
DESCRIPTION: Adds C++ source files related to the Intel Trace Analyzer and Collector (ITT) profiler integration if the `USE_ITT` flag is enabled. These files provide the necessary wrapper and stub implementations for ITT profiling.
SOURCE: https://github.com/pytorch/pytorch/blob/main/caffe2/CMakeLists.txt#_snippet_21

LANGUAGE: CMake
CODE:
```
if(${USE_ITT})
  list(APPEND TORCH_SRCS
    ${TORCH_SRC_DIR}/csrc/itt_wrapper.cpp
    ${TORCH_SRC_DIR}/csrc/profiler/stubs/itt.cpp
  )
endif()
```

----------------------------------------

TITLE: Configure Android Debug Symbol Stripping (CMake)
DESCRIPTION: Adds flags to strip debug symbols for Android release builds, handling different compilers (GCC/Clang) and linkers.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#_snippet_32

LANGUAGE: CMake
CODE:
```
if(ANDROID AND (NOT ANDROID_DEBUG_SYMBOLS))
  if(CMAKE_COMPILER_IS_GNUCXX)
    string(APPEND CMAKE_CXX_FLAGS " -s")
  elseif("${CMAKE_CXX_COMPILER_ID}" MATCHES "Clang")
    string(APPEND CMAKE_CXX_FLAGS " -g0")
  else()
    string(APPEND CMAKE_EXE_LINKER_FLAGS " -s")
  endif()
endif()
```

----------------------------------------

TITLE: Setting Environment Variables for Clang Compiler
DESCRIPTION: This snippet demonstrates how to set environment variables for using the Clang compiler with the coverage tool. It sets the compiler type and the path to LLVM tools.
SOURCE: https://github.com/pytorch/pytorch/blob/main/tools/code_coverage/README.md#2025-04-22_snippet_1

LANGUAGE: bash
CODE:
```
# set compiler type, the default is auto detected, you can check it at the start of log.txt
export COMPILER_TYPE="CLANG"
# set llvm path for clang, by default is /usr/local/opt/llvm/bin
export LLVM_TOOL_PATH=...
```

----------------------------------------

TITLE: Configuring PyTorch Python Extension in CMake
DESCRIPTION: This snippet sets up the torch_python shared library, including source files, compile options, and dependencies. It also configures platform-specific settings and handles various build options.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_15

LANGUAGE: CMake
CODE:
```
add_library(torch_python SHARED ${TORCH_PYTHON_SRCS})
torch_compile_options(torch_python)
if(APPLE)
  target_compile_options(torch_python PRIVATE
      $<$<COMPILE_LANGUAGE:CXX>: -fvisibility=default>)
endif()

if(CAFFE2_USE_MKL AND BUILD_LIBTORCHLESS)
  set(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE)
  target_link_libraries(torch_python PRIVATE caffe2::mkl)
endif()

add_dependencies(torch_python onnx_proto)
if(USE_NUMPY)
  target_link_libraries(torch_python PRIVATE Python::NumPy)
  target_compile_definitions(torch_python PRIVATE USE_NUMPY)
endif()
```

----------------------------------------

TITLE: Initializing CMake Project for PyTorch JNI
DESCRIPTION: Sets the minimum required CMake version, defines an option `BUILD_LITE_INTERPRETER` to control which JNI variant is built (defaulting to ON), displays the status of this option, and sets the project name and target variable (`PYTORCH_JNI_TARGET`) based on the option's value.
SOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_0

LANGUAGE: cmake
CODE:
```
cmake_minimum_required(VERSION 3.4.1)
option(BUILD_LITE_INTERPRETER "Master flag to build pytorch_jni_lite" ON)
message(
  STATUS
  "BUILD_LITE_INTERPRETER (pytorch_jni_lite): ${BUILD_LITE_INTERPRETER}")

if(BUILD_LITE_INTERPRETER)
  project(pytorch_jni_lite CXX)
  set(PYTORCH_JNI_TARGET pytorch_jni_lite)
else()
  project(pytorch_jni CXX)
  set(PYTORCH_JNI_TARGET pytorch_jni)
endif()
```

----------------------------------------

TITLE: PyTorch Split Operations
DESCRIPTION: Tensor splitting operations along channel dimension with specified size chunks
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_botnext26ts_256_training.txt#2025-04-22_snippet_8

LANGUAGE: python
CODE:
```
((T([128, 384, 16, 16], f16), [64, 64, 256], 1), {})
```

----------------------------------------

TITLE: Installing Windows Dependencies (Distributed) Bash
DESCRIPTION: Installs libuv using conda-forge, a package required for enabling the experimental distributed training support (torch.distributed) feature on Windows.
SOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#_snippet_10

LANGUAGE: Bash
CODE:
```
# Add these packages if torch.distributed is needed.
# Distributed package support on Windows is a prototype feature and is subject to changes.
conda install -c conda-forge libuv=1.39
```

----------------------------------------

TITLE: Fixing Intel OpenMP Issue with Conda (BAT)
DESCRIPTION: This batch command specifically addresses an `ImportError: DLL load failed: The operating system cannot run %1` issue that can occur when initializing a conda environment with the `conda-forge` channel. It forces the installation of `intel-openmp` from the default channels using the `-f` flag to fix the conflicting library version.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#_snippet_5

LANGUAGE: bat
CODE:
```
conda install -c defaults intel-openmp -f
```

----------------------------------------

TITLE: Documenting Compare Class in PyTorch Benchmark Utils
DESCRIPTION: This snippet documents the Compare class from the torch.utils.benchmark module, including all its members.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/benchmark_utils.rst#2025-04-22_snippet_6

LANGUAGE: reStructuredText
CODE:
```
.. autoclass:: Compare
    :members:
```

----------------------------------------

TITLE: Documenting Additional PyTorch Benchmark Modules
DESCRIPTION: This snippet documents additional modules related to PyTorch benchmarking, including examples, op_fuzzers, utils, and valgrind_wrapper.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/benchmark_utils.rst#2025-04-22_snippet_7

LANGUAGE: reStructuredText
CODE:
```
.. py:module:: torch.utils.benchmark.examples
.. py:module:: torch.utils.benchmark.op_fuzzers
.. py:module:: torch.utils.benchmark.utils
.. py:module:: torch.utils.benchmark.utils.valgrind_wrapper
```

----------------------------------------

TITLE: PyTorch Activation Functions in Neural Network
DESCRIPTION: Shows the usage of GELU activation functions and their backward passes in the neural network. These activations are applied after various linear transformations throughout the network architecture.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convnext_base_training.txt#2025-04-22_snippet_5

LANGUAGE: python
CODE:
```
Operator: aten.gelu.default
cnt: 3, ((T([32, 56, 56, 512], f16),), {})
cnt: 3, ((T([32, 28, 28, 1024], f16),), {})
cnt: 27, ((T([32, 14, 14, 2048], f16),), {})
cnt: 3, ((T([32, 7, 7, 4096], f16),), {})
Operator: aten.gelu_backward.default
cnt: 3, ((T([32, 7, 7, 4096], f16), T([32, 7, 7, 4096], f16)), {})
cnt: 27, ((T([32, 14, 14, 2048], f16), T([32, 14, 14, 2048], f16)), {})
cnt: 3, ((T([32, 28, 28, 1024], f16), T([32, 28, 28, 1024], f16)), {})
cnt: 3, ((T([32, 56, 56, 512], f16), T([32, 56, 56, 512], f16)), {})
```

----------------------------------------

TITLE: Retrieving Logging Handler for PyTorch Distributed Elastic Events
DESCRIPTION: This function returns a logging handler for the events system in PyTorch's distributed elastic package. It's used to manage how events are logged.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/events.rst#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
torch.distributed.elastic.events.get_logging_handler
```

----------------------------------------

TITLE: Adding Quantized Dispatch to Existing Operator in native_functions.yaml
DESCRIPTION: Illustrates how to add a quantized CPU implementation (`quantized_xand`) to an existing operator definition (e.g., `xand`) in `ATen/native/native_functions.yaml`. It involves adding a new key-value pair (`QuantizedCPU: quantized_xand`) under the `dispatch` section for the relevant `func` entry.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/README.md#2025-04-22_snippet_4

LANGUAGE: yaml
CODE:
```
- func: xand(Tensor a, Tensor b) -> Tensor
  dispatch:
    CPU: _xand_cpu     # Assume this existed
    CUDA: _xand_cuda   # Assume this existed
    QuantizedCPU: quantized_xand
```

----------------------------------------

TITLE: Type Isolation in Package Importing
DESCRIPTION: Example showing how classes imported from a PackageImporter are specific to that importer, illustrating the isolation of types between packages and the loading environment.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_20

LANGUAGE: python
CODE:
```
from foo import MyClass

my_class_instance = MyClass()

with PackageExporter(f) as exporter:
    exporter.save_module("foo")

importer = PackageImporter(f)
```

----------------------------------------

TITLE: Specifying Breathe Version (Python)
DESCRIPTION: Specifies the required version for the Breathe Sphinx extension. Breathe is essential for integrating documentation generated by Doxygen (often used for C++ code) into Sphinx-based documentation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/requirements.txt#_snippet_6

LANGUAGE: Python
CODE:
```
breathe==4.34.0
```

----------------------------------------

TITLE: Representing With Statement with prim::Enter and prim::Exit in PyTorch JIT IR
DESCRIPTION: Shows the standard representation of a Python `with` statement in the PyTorch JIT IR during most of the compilation process. It uses a `prim::Enter` node to acquire the context value before the body operations and a `prim::Exit` node after the body to handle context teardown.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_8

LANGUAGE: JIT IR
CODE:
```
%2 : int = prim::Constant[value=1]()
%increment.1 : int = prim::Enter(%c.1)
%y.1 : Tensor = aten::add(%x.1, %increment.1, %2)
%11 : Tensor = prim::Exit(%c.1)
```

----------------------------------------

TITLE: Handling aten.addmm in PyTorch with Python
DESCRIPTION: The aten.addmm.default operator is analyzed for its application on three-tensor products, detailed by their shapes and data types (f16). It typically involves tensors of various dimensions, used to perform matrix multiplication followed by addition. Dependencies include having three-dimensional f16 tensors ready for such operations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/nvidia_deeprecommender_training.txt#2025-04-22_snippet_0

LANGUAGE: Python
CODE:
```
Operator: aten.addmm.default
cnt: 1, ((T([512], f16), T([256, 197951], f16), T([197951, 512], f16, stride=(1, 197951))), {})
cnt: 2, ((T([512], f16), T([256, 512], f16), T([512, 512], f16, stride=(1, 512))), {})
cnt: 1, ((T([1024], f16), T([256, 512], f16), T([512, 1024], f16, stride=(1, 512))), {})
cnt: 1, ((T([512], f16), T([256, 1024], f16), T([1024, 512], f16, stride=(1, 1024))), {})
cnt: 1, ((T([197951], f16), T([256, 512], f16), T([512, 197951], f16, stride=(1, 512))), {})
```

----------------------------------------

TITLE: Illustrating Python While Loop Transform
DESCRIPTION: Shows a Python example of a `while` loop containing a `continue` statement within an `if`, and its transformed equivalent used internally by the PyTorch JIT frontend's Exit Transform pass. The transformation replaces `continue` with boolean flags and conditional logic to flatten control flow for SSA conversion.
SOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#_snippet_13

LANGUAGE: python
CODE:
```
while i < 5:
  if i == 3:
    i += 1
    continue
  i += 2
```

LANGUAGE: python
CODE:
```
continue_loop = i < 5
while continue_loop:
  if i == 3:
    i = i + 1
    continue_loop = i < 5
    did_exit = True
  if did_exit:
    pass
  else:
    i = i + 2
    continue_loop = i < 5
```

----------------------------------------

TITLE: Scatter Operation in PyTorch
DESCRIPTION: This snippet shows a scatter operation, which is used to update values in a tensor based on indices and a source tensor.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
Operator: aten.scatter.src
cnt: 1, ((T([64, 196, 1000], f16), 1, T([64, 1, 1000], i64), T([64, 1, 1000], f16)), {})
```

----------------------------------------

TITLE: Registering Single Operator Benchmark in PyTorch
DESCRIPTION: This snippet shows how to register a single operator benchmark with the PyTorch benchmark suite using the generate_pt_test function.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_13

LANGUAGE: python
CODE:
```
op_bench.generate_pt_test(add_long_configs + add_short_configs, AddBenchmark)
```

----------------------------------------

TITLE: Describing aten.add.Tensor, add_, addmm Usage - PyTorch - Python
DESCRIPTION: Summarizes element-wise addition (add/add_) and matrix multiplication with addition (addmm) operators, often seen in residual and dense layers. Input and output tensor shapes and formats indicate use within deep convolutional blocks. add_ ops denote in-place modifications for memory efficiency, while addmm combines bias addition and matrix multiplication. Quantities reflect heavy usage throughout the model.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ecaresnet101d_training.txt#2025-04-22_snippet_2

LANGUAGE: Python
CODE:
```
Operator: aten.add.Tensor
cnt: 5, ((T([64, 2048, 7, 7], f16), T([64, 2048, 7, 7], f16)), {})
cnt: 46, ((T([64, 1024, 14, 14], f16), T([64, 1024, 14, 14], f16)), {})
cnt: 8, ((T([64, 512, 28, 28], f16), T([64, 512, 28, 28], f16)), {})
cnt: 6, ((T([64, 256, 56, 56], f16), T([64, 256, 56, 56], f16)), {})
cnt: 1, ((T([64, 64, 56, 56], f16), T([64, 64, 56, 56], f16)), {})
Operator: aten.add_.Tensor
cnt: 106, ((T([], i64), 1), {})
cnt: 3, ((T([64, 256, 56, 56], f16), T([64, 256, 56, 56], f16)), {})
cnt: 4, ((T([64, 512, 28, 28], f16), T([64, 512, 28, 28], f16)), {})
cnt: 23, ((T([64, 1024, 14, 14], f16), T([64, 1024, 14, 14], f16)), {})
cnt: 3, ((T([64, 2048, 7, 7], f16), T([64, 2048, 7, 7], f16)), {})
Operator: aten.addmm.default
cnt: 1, ((T([1000], f16), T([64, 2048], f16), T([2048, 1000], f16, stride=(1, 2048))), {})
```

----------------------------------------

TITLE: Mean Operation in DenseNet Global Average Pooling
DESCRIPTION: This snippet shows the mean operation used for global average pooling in DenseNet. It computes the mean across the spatial dimensions (-1, -2) of feature maps with shape [32, 1024, 7, 7], keeping the dimensions for broadcasting compatibility.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_11

LANGUAGE: python
CODE:
```
Operator: aten.mean.dim
cnt: 1, ((T([32, 1024, 7, 7], f16), [-1, -2], True), {})
```

----------------------------------------

TITLE: Adding Core Distributed Tests
DESCRIPTION: Adds core distributed tests for Backoff, FileStore, TCPStore, and HashStore components. Links against torch_cpu and gtest.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/c10d/CMakeLists.txt#2025-04-22_snippet_2

LANGUAGE: cmake
CODE:
```
c10d_add_test(BackoffTest.cpp LINK_LIBRARIES torch_cpu gtest_main INSTALL_TEST OFF)
c10d_add_test(FileStoreTest.cpp LINK_LIBRARIES torch_cpu gtest_main INSTALL_TEST ${INSTALL_TEST})
c10d_add_test(TCPStoreTest.cpp LINK_LIBRARIES torch_cpu gtest_main INSTALL_TEST ${INSTALL_TEST})
if(NOT WIN32)
  c10d_add_test(HashStoreTest.cpp LINK_LIBRARIES torch_cpu gtest_main INSTALL_TEST ${INSTALL_TEST})
endif()
```

----------------------------------------

TITLE: Configure Gold Linker Usage (CMake)
DESCRIPTION: Checks for the availability of the gold linker and configures CMake to use it for linking executables, shared libraries, and modules if available and not using distributed MPI.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#_snippet_15

LANGUAGE: CMake
CODE:
```
if(USE_GOLD_LINKER)
    if(USE_DISTRIBUTED AND USE_MPI)
      # Same issue as here with default MPI on Ubuntu
      # https://bugs.launchpad.net/ubuntu/+source/deal.ii/+bug/1841577
      message(WARNING "Refusing to use gold when USE_MPI=1")
    else()
      execute_process(
        COMMAND "${CMAKE_C_COMPILER}" -fuse-ld=gold -Wl,--version
        ERROR_QUIET
        OUTPUT_VARIABLE LD_VERSION)
      if(NOT "${LD_VERSION}" MATCHES "GNU gold")
        message(
          WARNING
            "USE_GOLD_LINKER was set but ld.gold isn't available, turning it off"
        )
        set(USE_GOLD_LINKER OFF)
      else()
        message(STATUS "ld.gold is available, using it to link")
        set(CMAKE_EXE_LINKER_FLAGS "${CMAKE_EXE_LINKER_FLAGS} -fuse-ld=gold")
        set(CMAKE_SHARED_LINKER_FLAGS
            "${CMAKE_SHARED_LINKER_FLAGS} -fuse-ld=gold")
        set(CMAKE_MODULE_LINKER_FLAGS
            "${CMAKE_MODULE_LINKER_FLAGS} -fuse-ld=gold")
      endif()
    endif()
  endif()
```

----------------------------------------

TITLE: PyTorch Operator Usage Count for aten._log_softmax_backward_data Operations
DESCRIPTION: Shows the call pattern for the _log_softmax_backward_data.default operator using half precision tensors of shape [128, 1000], with dimension 1 and output dtype f16.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
Operator: aten._log_softmax_backward_data.default
cnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})
```

----------------------------------------

TITLE: Importing torch.ao.ns.fx.utils module in Python
DESCRIPTION: This snippet shows how to import the torch.ao.ns.fx.utils module. This module contains utility functions for numeric comparisons and is also marked as an early prototype.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.ao.ns._numeric_suite_fx.rst#2025-04-22_snippet_1

LANGUAGE: Python
CODE:
```
import torch.ao.ns.fx.utils
```

----------------------------------------

TITLE: Analyzing Summation Operations in PyTorch
DESCRIPTION: This snippet demonstrates summation operations on 2D tensors. It includes both symbolic integer summation and default summation, likely used for loss calculation or feature aggregation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/shufflenet_v2_x1_0_training.txt#2025-04-22_snippet_3

LANGUAGE: python
CODE:
```
Operator: aten.sum.SymInt
cnt: 1, ((T([128, 1000], f16, stride=(0, 0)), [0], True), {})
Operator: aten.sum.default
cnt: 1, ((T([128, 1000], f16),), {})
```

----------------------------------------

TITLE: Push Git Tag to Origin Git
DESCRIPTION: This git command pushes a specific tag (e.g., 'v1.12.0-rc2') from the local repository to the remote 'origin' repository. This step makes the tag available remotely and triggers CI/CD processes associated with the tag, such as building and publishing release candidate binaries.
SOURCE: https://github.com/pytorch/pytorch/blob/main/RELEASE.md#_snippet_9

LANGUAGE: bash
CODE:
```
git push origin v1.12.0-rc2
```

----------------------------------------

TITLE: Implementing Custom Timer Server in PyTorch Distributed Elastic
DESCRIPTION: Base class for implementing custom timer servers. Extend this class to create your own timer server implementation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/timer.rst#2025-04-22_snippet_7

LANGUAGE: Python
CODE:
```
TimerServer
```

----------------------------------------

TITLE: Creating a TensorExpr C++ Test File Template
DESCRIPTION: Provides a basic template for a new TensorExpr C++ test file (e.g., `test_foo.cpp`). It includes the necessary header `test/cpp/tensorexpr/test_base.h` and demonstrates defining test cases as `void()` functions starting with the prefix `test` within the `torch::jit` namespace.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/tensorexpr/README.md#2025-04-22_snippet_0

LANGUAGE: cpp
CODE:
```
#include <test/cpp/tensorexpr/test_base.h>

// Tests go in torch::jit
namespace torch {
namespace jit {

// 1. Test cases are void() functions.
// 2. They start with the prefix `test`
void testCaseOne() {
    // ...
}

void testCaseTwo() {
    // ...
}
}
}
```

----------------------------------------

TITLE: Usage Examples for aten.mul.Tensor
DESCRIPTION: Logs element-wise tensor multiplication (`aten.mul.Tensor`). Most examples show multiplication of a 4D tensor with another 4D tensor where the last two dimensions are singleton (broadcastable), e.g., [128, C, H, W] * [128, C, 1, 1]. Other examples show element-wise multiplication of identically shaped tensors. All tensors are float16.
SOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/regnety_002_training.txt#2025-04-22_snippet_3

LANGUAGE: plaintext
CODE:
```
Operator: aten.mul.Tensor
cnt: 2, ((T([128, 24, 56, 56], f16), T([128, 24, 1, 1], f16)), {})
cnt: 2, ((T([128, 56, 28, 28], f16), T([128, 56, 1, 1], f16)), {})
cnt: 8, ((T([128, 152, 14, 14], f16), T([128, 152, 1, 1], f16)), {})
cnt: 14, ((T([128, 368, 7, 7], f16), T([128, 368, 1, 1], f16)), {})
cnt: 7, ((T([128, 368, 7, 7], f16), T([128, 368, 7, 7], f16)), {})
cnt: 4, ((T([128, 152, 14, 14], f16), T([128, 152, 14, 14], f16)), {})
cnt: 1, ((T([128, 56, 28, 28], f16), T([128, 56, 28, 28], f16)), {})
cnt: 1, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16)), {})
```

----------------------------------------

TITLE: Generated Triton Kernel Example
DESCRIPTION: Shows the automatically generated Triton kernel code that implements the fused cos and sin operations. This is the optimized code produced by the TorchInductor backend.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_get_started.rst#2025-04-22_snippet_1

LANGUAGE: python
CODE:
```
@pointwise(size_hints=[16384], filename=__file__, triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32'}, 'device': 0, 'constants': {}, 'mutated_arg_names': [], 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]})
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
   xnumel = 10000
   xoffset = tl.program_id(0) * XBLOCK
   xindex = xoffset + tl.arange(0, XBLOCK)[:]
   xmask = xindex < xnumel
   x0 = xindex
   tmp0 = tl.load(in_ptr0 + (x0), xmask, other=0.0)
   tmp1 = tl.cos(tmp0)
   tmp2 = tl.sin(tmp1)
   tl.store(out_ptr0 + (x0 + tl.zeros([XBLOCK], tl.int32)), tmp2, xmask)
```

----------------------------------------

TITLE: Conditionally Installing Shared Libraries CMake
DESCRIPTION: Installs the three generated shared libraries (`torchbind_test`, `jitbackend_test`, `backend_with_compiler`) to the 'lib' directory relative to the installation prefix. This step is performed only if the `INSTALL_TEST` flag is enabled.
SOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#_snippet_10

LANGUAGE: CMake
CODE:
```
if(INSTALL_TEST)
  install(TARGETS torchbind_test DESTINATION lib)
  install(TARGETS jitbackend_test DESTINATION lib)
  install(TARGETS backend_with_compiler DESTINATION lib)
endif()
```

----------------------------------------

TITLE: Rebuilding Specific PyTorch C++ File with Debug Info
DESCRIPTION: Demonstrates using the `tools/build_with_debinfo.py` script to rebuild a single source file (`torch/csrc/autograd/python_variable_indexing.cpp`) with debug symbols. This allows for more detailed debugging information in the symbolicated backtrace.
SOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#_snippet_52

LANGUAGE: Bash
CODE:
```
% ./tools/build_with_debinfo.py torch/csrc/autograd/python_variable_indexing.cpp
[1 / 2] Building caffe2/torch/CMakeFiles/torch_python.dir/csrc/autograd/python_variable_indexing.cpp.o
[2 / 2] Building lib/libtorch_python.dylib
```

----------------------------------------

TITLE: Implementing Custom Timer Client in PyTorch Distributed Elastic
DESCRIPTION: Base class for implementing custom timer clients. Extend this class to create your own timer client implementation.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/timer.rst#2025-04-22_snippet_8

LANGUAGE: Python
CODE:
```
TimerClient
```

----------------------------------------

TITLE: Configuring MPS Profiler Profile/Signpost Options (Environment Variable)
DESCRIPTION: Sets the profile and signpost bitmasks for the `MPSProfiler` using `PYTORCH_MPS_TRACE_SIGNPOSTS`. Available options can be found in the `ProfileOptions` and `SignpostTypes` enums within the `aten/src/ATen/mps/MPSProfiler.h` header file.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mps_environment_variables.rst#2025-04-22_snippet_2

LANGUAGE: plaintext
CODE:
```
PYTORCH_MPS_TRACE_SIGNPOSTS
```

----------------------------------------

TITLE: Defining Timer Request in PyTorch Distributed Elastic
DESCRIPTION: Class for defining timer requests used to pass messages between the server and client in custom timer implementations.
SOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/timer.rst#2025-04-22_snippet_6

LANGUAGE: Python
CODE:
```
TimerRequest
```

----------------------------------------

TITLE: Append Vulkan Test Source to List CMake
DESCRIPTION: Appends a single C++ source file (`vulkan_api_test.cpp`) located in the current source directory to the CMake list variable `ATen_VULKAN_TEST_SRCS`. This file contains tests specifically for the ATen Vulkan backend's API.
SOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/test/CMakeLists.txt#_snippet_5

LANGUAGE: CMake
CODE:
```
list(APPEND ATen_VULKAN_TEST_SRCS
  ${CMAKE_CURRENT_SOURCE_DIR}/vulkan_api_test.cpp)
```