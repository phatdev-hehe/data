TITLE: Specifying Python Dependencies in requirements.txt
DESCRIPTION: Lists all necessary Python packages and their versions for the Open-WebUI project using the standard `requirements.txt` format. This ensures environment reproducibility by defining dependencies for web serving (FastAPI, Uvicorn), AI (OpenAI, Langchain, Transformers), databases (SQLAlchemy, Peewee, vector DBs), document parsing, authentication, testing, cloud integration (AWS, Azure, Google Cloud), and tracing (OpenTelemetry).
SOURCE: https://github.com/open-webui/open-webui/blob/main/backend/requirements.txt#_snippet_0

LANGUAGE: plaintext
CODE:
```
fastapi==0.115.7
uvicorn[standard]==0.34.0
pydantic==2.10.6
python-multipart==0.0.20

python-socketio==5.13.0
python-jose==3.4.0
passlib[bcrypt]==1.7.4

requests==2.32.3
aiohttp==3.11.11
async-timeout
aiocache
aiofiles

sqlalchemy==2.0.38
alembic==1.14.0
peewee==3.17.9
peewee-migrate==1.12.2
psycopg2-binary==2.9.9
pgvector==0.4.0
PyMySQL==1.1.1
bcrypt==4.3.0

pymongo
redis
boto3==1.35.53

argon2-cffi==23.1.0
APScheduler==3.10.4

RestrictedPython==8.0

loguru==0.7.2
asgiref==3.8.1

# AI libraries
openai
anthropic
google-generativeai==0.8.4
tiktoken

langchain==0.3.19
langchain-community==0.3.18

fake-useragent==2.1.0
chromadb==0.6.3
pymilvus==2.5.0
qdrant-client~=1.12.0
opensearch-py==2.8.0
playwright==1.49.1 # Caution: version must match docker-compose.playwright.yaml
elasticsearch==8.17.1


transformers
sentence-transformers==3.3.1
accelerate
colbert-ai==0.2.21
einops==0.8.1


ftfy==6.2.3
pypdf==4.3.1
fpdf2==2.8.2
pymdown-extensions==10.14.2
docx2txt==0.8
python-pptx==1.0.0
unstructured==0.16.17
nltk==3.9.1
Markdown==3.7
pypandoc==1.15
pandas==2.2.3
openpyxl==3.1.5
pyxlsb==1.0.10
xlrd==2.0.1
validators==0.34.0
psutil
sentencepiece
soundfile==0.13.1
azure-ai-documentintelligence==1.0.0

pillow==11.1.0
opencv-python-headless==4.11.0.86
rapidocr-onnxruntime==1.3.24
rank-bm25==0.2.2

onnxruntime==1.20.1

faster-whisper==1.1.1

PyJWT[crypto]==2.10.1
authlib==1.4.1

black==25.1.0
langfuse==2.44.0
youtube-transcript-api==1.0.3
pytube==15.0.0

extract_msg
pydub
duckduckgo-search~=8.0.0

## Google Drive
google-api-python-client
google-auth-httplib2
google-auth-oauthlib

## Tests
docker~=7.1.0
pytest~=8.3.2
pytest-docker~=3.1.1

googleapis-common-protos==1.63.2
google-cloud-storage==2.19.0

azure-identity==1.21.0
azure-storage-blob==12.24.1


## LDAP
ldap3==2.9.1

## Firecrawl
firecrawl-py==1.12.0

# Sougou API SDK(Tencentcloud SDK)
tencentcloud-sdk-python==3.0.1336

## Trace
opentelemetry-api==1.31.1
opentelemetry-sdk==1.31.1
opentelemetry-exporter-otlp==1.31.1
opentelemetry-instrumentation==0.52b1
opentelemetry-instrumentation-fastapi==0.52b1
opentelemetry-instrumentation-sqlalchemy==0.52b1
opentelemetry-instrumentation-redis==0.52b1
opentelemetry-instrumentation-requests==0.52b1
opentelemetry-instrumentation-logging==0.52b1
opentelemetry-instrumentation-httpx==0.52b1
opentelemetry-instrumentation-aiohttp-client==0.52b1

```

----------------------------------------

TITLE: Running Open WebUI with Ollama in Docker (GPU Support) - Bash
DESCRIPTION: This code snippet demonstrates how to deploy Open WebUI bundled with Ollama on a system equipped with GPU support using Docker. The command runs the container in detached mode, exposes required ports, enables GPU access, and mounts persistent volumes for Ollama and backend data. Dependencies: Docker installed with GPU support (e.g., NVIDIA runtime). The main parameters include container naming, port mapping, volume definition, and restart policy. Outputs a running Docker container accessible at the specified port.
SOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_6

LANGUAGE: bash
CODE:
```
docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
```

----------------------------------------

TITLE: Running Open WebUI with Ollama in Docker (CPU Only) - Bash
DESCRIPTION: This snippet illustrates how to deploy Open WebUI with Ollama using Docker for systems without GPU hardware. The command omits the GPU flag and otherwise closely resembles the GPU version, running the container in detached mode with port mapping and volume mounts for persistent storage. Dependencies: Docker. All main parameters are configurable (e.g., mounting volumes, naming the container). Outputs a running instance accessible at the mapped port.
SOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_7

LANGUAGE: bash
CODE:
```
docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
```

----------------------------------------

TITLE: Running Open WebUI with Nvidia GPU Support via Docker (Bash)
DESCRIPTION: Runs Open WebUI using the CUDA-enabled Docker image (`ghcr.io/open-webui/open-webui:cuda`) to leverage Nvidia GPU acceleration. The `--gpus all` flag enables access to all available host GPUs. Requires Docker, the Nvidia Container Toolkit installed on the host, and a compatible Nvidia GPU. It maps port 3000, mounts the data volume, uses `--add-host` for potential host communication, names the container, and sets it to restart always.
SOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_4

LANGUAGE: bash
CODE:
```
docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda
```

----------------------------------------

TITLE: Running Open WebUI for OpenAI API Only via Docker (Bash)
DESCRIPTION: Runs Open WebUI in a Docker container configured exclusively for use with the OpenAI API. The `-e OPENAI_API_KEY` flag sets the necessary API key as an environment variable (replace `your_secret_key` with your actual key). It maps host port 3000 to container port 8080, mounts the `open-webui` volume for data persistence, names the container `open-webui`, and ensures automatic restarts. This setup does not require or connect to an Ollama instance.
SOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_5

LANGUAGE: bash
CODE:
```
docker run -d -p 3000:8080 -e OPENAI_API_KEY=your_secret_key -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

----------------------------------------

TITLE: Running Open WebUI with Local Ollama via Docker (Bash)
DESCRIPTION: Runs Open WebUI in a Docker container, configured for use with an Ollama instance running on the same host machine. It maps host port 3000 to container port 8080, uses `--add-host` to allow the container to reach the host's Ollama service, mounts a named volume `open-webui` to `/app/backend/data` for persistence, names the container `open-webui`, and ensures it restarts automatically. Uses the `main` tag of the official image.
SOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_2

LANGUAGE: bash
CODE:
```
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

----------------------------------------

TITLE: Running Open WebUI with Remote Ollama via Docker (Bash)
DESCRIPTION: Runs Open WebUI in a Docker container, configured to connect to an Ollama instance running on a different server. The `-e OLLAMA_BASE_URL` flag sets the environment variable to the URL of the remote Ollama service (replace `https://example.com` with the actual URL). It maps host port 3000, mounts the `open-webui` volume for data persistence, names the container, and ensures automatic restarts.
SOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_3

LANGUAGE: bash
CODE:
```
docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

----------------------------------------

TITLE: Installing Open WebUI via Pip (Bash)
DESCRIPTION: Installs the Open WebUI package using the Python package installer, pip. This command should be run in a terminal. It is required to have Python 3.11 installed for compatibility.
SOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
pip install open-webui
```

----------------------------------------

TITLE: Running Open WebUI Server via Pip (Bash)
DESCRIPTION: Starts the Open WebUI server after installation via pip. Execute this command in the terminal. The server will typically be accessible at http://localhost:8080 by default.
SOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
open-webui serve
```

----------------------------------------

TITLE: Resolving Open WebUI Connection Errors via Docker Host Networking - Bash
DESCRIPTION: This code demonstrates how to start the Open WebUI Docker container using host networking to resolve issues accessing Ollama at localhost addresses. It uses the --network=host flag, adjusts volume mapping, and sets an environment variable to specify the Ollama server URL. Dependencies: Docker. Key parameters include OLLAMA_BASE_URL and host networking mode, which can affect exposed ports and network visibility. Outputs the container running on the host network for seamless service connectivity.
SOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_8

LANGUAGE: bash
CODE:
```
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

----------------------------------------

TITLE: Enabling Offline Mode for HuggingFace Model Hub - Bash
DESCRIPTION: This snippet sets an environment variable to enable offline mode when running Open WebUI, preventing attempts to download models from HuggingFace's remote hub. Usage: 'export HF_HUB_OFFLINE=1' is run in the shell before starting the application or container. No external dependencies except for an environment supporting Bash or compatible shells. Only requires setting the variable; effective for offline environments or air-gapped deployments.
SOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_11

LANGUAGE: bash
CODE:
```
export HF_HUB_OFFLINE=1
```

----------------------------------------

TITLE: Running Open WebUI Development Branch with Custom Docker Options - Bash
DESCRIPTION: This snippet launches Open WebUI from the 'dev' branch using Docker, enabling access to the latest features. It includes custom host mapping, named container, volume mount for persistent data, and auto-restart policy. Dependencies: Docker. The code allows users to test new features while highlighting the risk of instability in development builds. Main parameters: port mapping, volume mount, host alias, and tag selection (':dev'). The result is a running, experimental container instance.
SOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_10

LANGUAGE: bash
CODE:
```
docker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --add-host=host.docker.internal:host-gateway --restart always ghcr.io/open-webui/open-webui:dev
```

----------------------------------------

TITLE: Updating Open WebUI Docker Container with Watchtower - Bash
DESCRIPTION: This code snippet updates a running Open WebUI Docker container to the latest image version using Watchtower. It launches Watchtower with direct access to the Docker socket, performs a one-time update on the specified container (by name), and then exits. Dependencies: Docker, Watchtower image. The main parameter is the name of the container to update. Outputs the update process logs and pulls updated images if available. Replace 'open-webui' with your custom container name as needed.
SOURCE: https://github.com/open-webui/open-webui/blob/main/README.md#_snippet_9

LANGUAGE: bash
CODE:
```
docker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui
```

----------------------------------------

TITLE: Applying Kustomize Configuration for GPU Deployment (Bash)
DESCRIPTION: This command uses kubectl with the '-k' flag to apply Kustomize configurations located in './kubernetes/manifest'. This typically includes base manifests plus overlays specific to GPU-enabled deployments, allowing Ollama and Open WebUI to utilize GPU resources on the Kubernetes cluster.
SOURCE: https://github.com/open-webui/open-webui/blob/main/INSTALLATION.md#_snippet_1

LANGUAGE: bash
CODE:
```
kubectl apply -k ./kubernetes/manifest
```

----------------------------------------

TITLE: Applying Base Kustomize Configuration for CPU Deployment (Bash)
DESCRIPTION: This command uses kubectl to apply the base Kubernetes manifests located in './kubernetes/manifest/base'. This setup is intended for deploying Ollama and Open WebUI in a CPU-only configuration on a Kubernetes cluster.
SOURCE: https://github.com/open-webui/open-webui/blob/main/INSTALLATION.md#_snippet_0

LANGUAGE: bash
CODE:
```
kubectl apply -f ./kubernetes/manifest/base
```

----------------------------------------

TITLE: Installing Helm Chart for CPU Deployment (Bash)
DESCRIPTION: This command installs the previously packaged Helm chart (ollama-webui-*.tgz) with the release name 'ollama-webui'. It uses the default chart values, typically configured for a CPU-only deployment of Ollama and Open WebUI.
SOURCE: https://github.com/open-webui/open-webui/blob/main/INSTALLATION.md#_snippet_3

LANGUAGE: bash
CODE:
```
helm install ollama-webui ./ollama-webui-*.tgz
```

----------------------------------------

TITLE: Packaging a Helm Chart (Bash)
DESCRIPTION: This command packages the Helm chart located in the './kubernetes/helm/' directory into a versioned chart archive file (.tgz). This step is a prerequisite before installing the chart using 'helm install'.
SOURCE: https://github.com/open-webui/open-webui/blob/main/INSTALLATION.md#_snippet_2

LANGUAGE: bash
CODE:
```
helm package ./kubernetes/helm/
```

----------------------------------------

TITLE: Installing Helm Chart for GPU Deployment (Bash)
DESCRIPTION: This command installs the packaged Helm chart (ollama-webui-*.tgz) with the release name 'ollama-webui', specifically configuring it for GPU usage. It uses the '--set' flag to override the default values, allocating one NVIDIA GPU resource ('nvidia.com/gpu="1"') to the Ollama component.
SOURCE: https://github.com/open-webui/open-webui/blob/main/INSTALLATION.md#_snippet_4

LANGUAGE: bash
CODE:
```
helm install ollama-webui ./ollama-webui-*.tgz --set ollama.resources.limits.nvidia.com/gpu="1"
```

----------------------------------------

TITLE: Configuring Apache VirtualHost with SSL for Open WebUI (Apache)
DESCRIPTION: Extends the basic VirtualHost config to add SSL termination for Open WebUI, using cert/key files and SSLProxy options. It specifies document root, backend proxying for HTTP and WebSocket, and enables additional SSL-related options such as SSLProxyEngine and SSLCACertificateFile. Assumes SSL certificates have been provisioned for the domain and that all SSL files are in place. Requires Apache SSL modules and the proper directory structure for certificate storage.
SOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_1

LANGUAGE: apache
CODE:
```
# For SSL
<VirtualHost 192.168.1.100:443>
    ServerName server.com
    DocumentRoot /home/server/public_html

    ProxyPass / http://server.com:3000/ nocanon
    ProxyPassReverse / http://server.com:3000/
    # Needed after 0.5
    ProxyPass / ws://server.com:3000/ nocanon
    ProxyPassReverse / ws://server.com:3000/

    SSLEngine on
    SSLCertificateFile /etc/ssl/virtualmin/170514456861234/ssl.cert
    SSLCertificateKeyFile /etc/ssl/virtualmin/170514456861234/ssl.key
    SSLProtocol all -SSLv2 -SSLv3 -TLSv1 -TLSv1.1

    SSLProxyEngine on
    SSLCACertificateFile /etc/ssl/virtualmin/170514456865864/ssl.ca
</VirtualHost>
```

----------------------------------------

TITLE: Configuring Apache VirtualHost for Open WebUI (Apache)
DESCRIPTION: Sets up an Apache VirtualHost directive for the Open WebUI user interface. This configuration listens on port 80 and proxies HTTP and WebSocket traffic to a backend server running on port 3000. It assumes the site will be hosted at 'server.com' and that the document root is set accordingly. Dependencies include Apache 2.x, enabled proxy and proxy websocket modules, and the Open WebUI application running and accessible at the specified backend address.
SOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_0

LANGUAGE: apache
CODE:
```
# Assuming you have a website hosting this UI at "server.com"
<VirtualHost 192.168.1.100:80>
    ServerName server.com
    DocumentRoot /home/server/public_html

    ProxyPass / http://server.com:3000/ nocanon
    ProxyPassReverse / http://server.com:3000/
    # Needed after 0.5
    ProxyPass / ws://server.com:3000/ nocanon
    ProxyPassReverse / ws://server.com:3000/

</VirtualHost>
```

----------------------------------------

TITLE: Apache VirtualHost for HTTP with HTTPS Redirect and Proxy for Ollama Model Server (Apache)
DESCRIPTION: Defines a VirtualHost for HTTP (port 80) serving the Ollama model API and automatically redirects all traffic to HTTPS. Contains directory permissions, proxy and SSL settings, Rewrite rules to force HTTPS, and reverse proxy directives to route requests to the backend Ollama server on a separate port. Assumes mod_rewrite and proxy modules are enabled.
SOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_10

LANGUAGE: apache
CODE:
```
<VirtualHost 192.168.254.109:80>
    DocumentRoot "/var/www/html/"
    ServerName models.server.city
    <Directory "/var/www/html/">
        Options None
        Require all granted
    </Directory>

    ProxyRequests Off
    ProxyPreserveHost On
    ProxyAddHeaders On
    SSLProxyEngine on

    ProxyPass / http://server.city:1000/ nocanon # or port 11434
    ProxyPassReverse / http://server.city:1000/ # or port 11434

    RewriteEngine on
    RewriteCond %{SERVER_NAME} =models.server.city
    RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,NE,R=permanent]
</VirtualHost>
```

----------------------------------------

TITLE: Apache VirtualHost with SSL and Proxy for Ollama Model Server (Apache)
DESCRIPTION: Creates an SSL-enabled VirtualHost configuration for serving the Ollama model API behind Apache. It includes document root, server name, directory access control, SSL settings through Let's Encrypt, and reverse proxying to the backend Ollama server. Assumes all specified SSL certificate files exist and that mod_ssl, proxy, and related modules are enabled.
SOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_9

LANGUAGE: apache
CODE:
```
# Assuming you have a website hosting this UI at "models.server.city"
<IfModule mod_ssl.c>
    <VirtualHost 192.168.254.109:443>
        DocumentRoot "/var/www/html/"
        ServerName models.server.city
        <Directory "/var/www/html/">
            Options None
            Require all granted
        </Directory>

        ProxyRequests Off
        ProxyPreserveHost On
        ProxyAddHeaders On
        SSLProxyEngine on

        ProxyPass / http://server.city:1000/ nocanon # or port 11434
        ProxyPassReverse / http://server.city:1000/ # or port 11434

        SSLCertificateFile /etc/letsencrypt/live/models.server.city/fullchain.pem
        SSLCertificateKeyFile /etc/letsencrypt/live/models.server.city/privkey.pem
        Include /etc/letsencrypt/options-ssl-apache.conf
    </VirtualHost>
</IfModule>
```

----------------------------------------

TITLE: Requesting and Installing SSL Certificates with Certbot (Bash)
DESCRIPTION: Runs Certbot to obtain and configure SSL certificates for the specified domain using the Apache plugin. The command 'certbot --apache -d <domain>' automates certificate retrieval, Apache SSL configuration, and key management. Assumes the domain is already pointing to the correct server and VirtualHost is enabled.
SOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_6

LANGUAGE: bash
CODE:
```
certbot --apache -d server.com
```

----------------------------------------

TITLE: Installing Certbot and Certbot Apache Plugin (Bash)
DESCRIPTION: Installs Certbot and its Apache plugin for managing SSL certificates with Let's Encrypt using snap and apt. Required for automating SSL certificate issuance and renewal on Apache hosts. The commands should be run with root or sudo privileges.
SOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_3

LANGUAGE: bash
CODE:
```
snap install certbot --classic
```

LANGUAGE: bash
CODE:
```
snap apt install python3-certbot-apache
```

----------------------------------------

TITLE: Enabling Apache Site Configuration (Bash)
DESCRIPTION: Uses the a2ensite Bash command, a Debian/Ubuntu utility, to enable a site configuration for Apache. The provided example enables 'server.com.conf'. This is a prerequisite for activating the provided VirtualHost and is necessary before requesting and setting up SSL.
SOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_2

LANGUAGE: bash
CODE:
```
a2ensite server.com.conf # this will enable the site. a2ensite is short for "Apache 2 Enable Site"
```

----------------------------------------

TITLE: Enabling Apache Site Configuration for Ollama Model Server (Bash)
DESCRIPTION: Enables the Apache configuration for the Ollama model domain using a2ensite. This command must be run before SSL certificate requests or reloading for changes to take effect. Run as root or with sudo privileges.
SOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_11

LANGUAGE: bash
CODE:
```
a2ensite models.server.city.conf
```

----------------------------------------

TITLE: Editing Site Configuration Files (Bash)
DESCRIPTION: Opens site configuration files using nano, a terminal-based text editor. The example usages are for editing 'server.com.conf' and 'models.server.city.conf', each corresponding to different Apache VirtualHost configurations. Ensures administrators can modify Apache configurations as described in the guide.
SOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_5

LANGUAGE: bash
CODE:
```
nano models.server.city.conf # match this with your ollama server domain
```

LANGUAGE: bash
CODE:
```
sudo nano /etc/systemd/system/ollama.service
```

----------------------------------------

TITLE: Configuring Ollama Systemd Service File (ini)
DESCRIPTION: Example configuration for the Ollama systemd unit file, allowing customization of IP/port, origin domains, environment, and command path. The file enables the Ollama server to listen on a chosen interface and port (such as 0.0.0.0:11434), supports configuration of CORS with OLLAMA_ORIGINS, and specifies the execution user/group. The '[Unit]', '[Service]', and '[Install]' sections are standard systemd. Ensure ollama binary is installed and paths exist as specified.
SOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_7

LANGUAGE: ini
CODE:
```
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
Environment="OLLAMA_HOST=0.0.0.0:11434" # this line is mandatory. You can also specify 192.168.254.109:DIFFERENT_PORT, format
Environment="OLLAMA_ORIGINS=http://192.168.254.106:11434,https://models.server.city" # this line is optional
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/s>

[Install]
WantedBy=default.target
```

----------------------------------------

TITLE: Setting Ollama Service Environment Variable (ini)
DESCRIPTION: Specifies the OLLAMA_HOST environment variable in the systemd Ollama service file. Required for Ollama to listen on a specific IP and port. Must be placed in the '[Service]' section of the unit file. No other functional dependencies except a valid Ollama server install.
SOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_8

LANGUAGE: ini
CODE:
```
Environment="OLLAMA_HOST=0.0.0.0:11434" # this line is mandatory. You can also specify
```

----------------------------------------

TITLE: Restarting Apache to Apply Configuration Changes (Bash)
DESCRIPTION: Reloads the Apache HTTP server to apply any new or changed configuration files using systemctl. Necessary after enabling sites, making config edits, or fetching new certificates. Requires root or sudo access.
SOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_12

LANGUAGE: bash
CODE:
```
systemctl reload apache2
```

----------------------------------------

TITLE: Navigating to Apache Sites-Available Directory (Bash)
DESCRIPTION: Uses cd to change the working directory to Apache's sites-available, where enabled/available VirtualHost configuration files reside. This is typically a preparatory step before creating or editing site-specific configurations.
SOURCE: https://github.com/open-webui/open-webui/blob/main/docs/apache.md#_snippet_4

LANGUAGE: bash
CODE:
```
cd /etc/apache2/sites-available/
```

----------------------------------------

TITLE: Connecting to External Databases using DATABASE_URL Environment Variable
DESCRIPTION: Details the usage of the 'DATABASE_URL' environment variable (added in v0.1.122) to connect Open WebUI to an external custom SQLite or Postgres database, enabling persistent storage outside the default setup.
SOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_11

LANGUAGE: plaintext
CODE:
```
DATABASE_URL
```

----------------------------------------

TITLE: Configuring HTTP Proxy for OpenAI/Ollama API Calls (Environment Variable)
DESCRIPTION: This environment variable, `http_proxy`, allows users to configure an HTTP proxy for outgoing API calls made to OpenAI and Ollama services within Open WebUI, as mentioned in version 0.2.4. Setting this variable directs relevant network traffic through the specified proxy server, facilitating usage in restricted network environments.
SOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_1

LANGUAGE: plaintext
CODE:
```
http_proxy
```

----------------------------------------

TITLE: Setting Batch Size for OpenAI Embeddings in RAG (Environment Variable)
DESCRIPTION: Introduced in version 0.2.3, the `RAG_EMBEDDING_OPENAI_BATCH_SIZE` environment variable allows configuration of the batch size for processing multiple embeddings simultaneously when using OpenAI for Retrieval-Augmented Generation (RAG). Increasing this value can enhance performance when dealing with large datasets by reducing the number of separate API calls.
SOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_2

LANGUAGE: plaintext
CODE:
```
RAG_EMBEDDING_OPENAI_BATCH_SIZE
```

----------------------------------------

TITLE: Configuring Advanced Ollama Parameters (num_thread, use_mmap, use_mlock)
DESCRIPTION: Version 0.2.1 added support for advanced configuration parameters for Ollama interactions: `num_thread` (controls the number of CPU threads used), `use_mmap` (enables memory mapping for model loading), and `use_mlock` (locks the model in RAM). These parameters allow for fine-tuning performance and resource usage of the Ollama backend.
SOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_4

LANGUAGE: plaintext
CODE:
```
'num_thread', 'use_mmap', and 'use_mlock'
```

----------------------------------------

TITLE: Accessing Models via Unified API Endpoint in Open WebUI
DESCRIPTION: Details the unified API endpoint '/api/models' used in Open WebUI (changed in v0.1.125) for retrieving model information. This change simplifies API interactions by consolidating model fetching requests to a single path.
SOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_6

LANGUAGE: plaintext
CODE:
```
/api/models
```

----------------------------------------

TITLE: Using Dynamic System Prompt Variables in Open WebUI
DESCRIPTION: Describes the usage of dynamic variables '{{CURRENT_DATE}}' and '{{USER_NAME}}' within system prompts in Open WebUI (added in v0.1.125). These variables allow for personalized and time-sensitive prompts based on the current date and the logged-in user's name.
SOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_5

LANGUAGE: plaintext
CODE:
```
{{CURRENT_DATE}}
```

LANGUAGE: plaintext
CODE:
```
{{USER_NAME}}
```

----------------------------------------

TITLE: Executing Python Code with Pre-installed Libraries in Open WebUI
DESCRIPTION: Lists the Python libraries available for local code execution within the browser in Open WebUI (added in v0.1.125). These include 'requests', 'beautifulsoup4', 'numpy', 'pandas', 'seaborn', 'matplotlib', 'scikit-learn', 'scipy', and 'regex', enabling various data processing and analysis tasks.
SOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_8

LANGUAGE: python
CODE:
```
'requests', 'beautifulsoup4', 'numpy', 'pandas', 'seaborn', 'matplotlib', 'scikit-learn', 'scipy', 'regex'
```

----------------------------------------

TITLE: Documenting Project Changes in Markdown Changelog - Markdown
DESCRIPTION: This snippet exemplifies the format for recording changes in a project changelog using Markdown. It uses sections for version numbers, release dates, and categorized lists (Added, Fixed, Changed) that describe each change. No external dependencies are required; the file is intended to be human-readable and accessible to all project contributors and users. Inputs are manually written entries, and outputs are rendered Markdown documents for transparency and auditing; care must be taken to follow the changelog format for consistency.
SOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_0

LANGUAGE: Markdown
CODE:
```
# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.1.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [0.6.5] - 2025-04-14

### Added

- 🛂 **Granular Voice Feature Permissions Per User Group**: Admins can now separately manage access to Speech-to-Text (record voice), Text-to-Speech (read aloud), and Tool Calls for each user group—giving teams tighter control over voice features and enhanced governance across roles.
- 🗣️ **Toggle Voice Activity Detection (VAD) for Whisper STT**: New environment variable lets you enable/disable VAD filtering with built-in Whisper speech-to-text, giving you flexibility to optimize for different audio quality and response accuracy levels.
- 📋 **Copy Formatted Response Mode**: You can now enable “Copy Formatted” in Settings > Interface to copy AI responses exactly as styled (with rich formatting, links, and structure preserved), making it faster and cleaner to paste into documents, emails, or reports.
- ⚙️ **Backend Stability and Performance Enhancements**: General backend refactoring improves system resilience, consistency, and overall reliability—offering smoother performance across workflows whether chatting, generating media, or using external tools.
- 🌎 **Translation Refinements Across Multiple Languages**: Updated translations deliver smoother language localization, clearer labels, and improved international usability throughout the UI—ensuring a better experience for non-English speakers.

### Fixed

- 🛠️ **LDAP Login Reliability Restored**: Resolved a critical issue where some LDAP setups failed due to attribute parsing—ensuring consistent, secure, and seamless user authentication across enterprise deployments.
- 🖼️ **Image Generation in Temporary Chats Now Works Properly**: Fixed a bug where image outputs weren’t generated during temporary chats—visual content can now be used reliably in all chat modes without interruptions.

## [0.6.4] - 2025-04-12

### Fixed

- 🛠️ **RAG_TEMPLATE Display Issue Resolved**: Fixed a formatting problem where the custom RAG_TEMPLATE wasn't correctly rendered in the interface—ensuring that custom retrieval prompts now appear exactly as intended for more reliable prompt engineering.

## [0.6.3] - 2025-04-12

### Added

- 🧪 **Auto-Artifact Detection Toggle**: Automatically detects artifacts in results—but now you can disable this behavior under advanced settings for full control.
- 🖼️ **Widescreen Mode for Shared Chats**: Shared link conversations now support widescreen layouts—perfect for presentations or easier review across wider displays.
- 🔁 **Reindex Knowledge Files on Demand**: Admins can now trigger reindexing of all knowledge files after changing embeddings—ensuring immediate alignment with new models for optimal RAG performance.
- 📄 **OpenAPI YAML Format Support**: External tools can now use YAML-format OpenAPI specs—making integration simpler for developers familiar with YAML-based configurations.
- 💬 **Message Content Copy Behavior**: Copy action now excludes 'details' tags—streamlining clipboard content when sharing or pasting summaries elsewhere.
- 🧭 **Sougou Web Search Integration**: New search engine option added—enhancing global relevance and diversity of search sources for multilingual users.
- 🧰 **Frontend Web Loader Engine Configuration**: Admins can now set preferred web loader engine for RAG workflows directly from the frontend—offering more control across setups.
- 👥 **Multi-Model Chat Permission Control**: Admins can manage access to multi-model chats per user group—allowing tighter governance in team environments.
- 🧱 **Persistent Configuration Can Be Disabled**: New environment variable lets advanced users and hosts turn off persistent configs—ideal for volatile or stateless deployments.
- 🧠 **Elixir Code Highlighting Support**: Elixir syntax is now beautifully rendered in code blocks—perfect for developers using this language in AI or automation projects.
- 🌐 **PWA External Manifest URL Support**: You can now define an external manifest.json—integrate Open WebUI seamlessly in managed or proxy-based PWA environments like Cloudflare Zero Trust.
- 🧪 **Azure AI Speech-to-Text Provider Integration**: Easily transcribe large audio files (up to 200MB) with high accuracy using Microsoft's Azure STT—fully configurable in Audio Settings.
- 🔏 **PKCE (Code Challenge Method) Support for OIDC**: Enhance your OIDC login security with Proof Key for Code Exchange—ideal for zero-trust and native client apps.
- ✨ **General UI/UX Enhancements**: Numerous refinements across layout, styling, and tool interactions—reducing visual noise and improving overall usability across key workflows.
- 🌍 **Translation Updates Across Multiple Languages**: Refined Catalan, Russian, Chinese (Simplified & Traditional), Hungarian, and Spanish translations for clearer navigation and instructions globally.

### Fixed

- 💥 **Chat Completion Error with Missing Models Resolved**: Fixed internal server error when referencing a model that doesn’t exist—ensuring graceful fallback and clear error guidance.
- 🔧 **Correct Knowledge Base Citations Restored**: Citations generated by RAG workflows now show accurate references—ensuring verifiability in outputs from sourced content.
- 🎙️ **Broken OGG/WebM Audio Upload Handling for OpenAI Fixed**: Uploading OGG or WebM files now converts properly to WAV before transcription—restoring accurate AI speech recognition workflows.
- 🔐 **Tool Server 'Session' Authentication Restored**: Previously broken session auth on external tool servers is now fully functional—ensuring secure and seamless access to connected tools.
- 🌐 **Folder-Based Chat Rename Now Updates Correctly**: Renaming chats in folders now reflects instantly everywhere—improving chat organization and clarity.
- 📜 **KaTeX Overflow Displays Fixed**: Math expressions now stay neatly within message bounds—preserving layout consistency even with long formulas.
- 🚫 **Stopping Ongoing Chat Fixed**: You can now return to an active (ongoing) chat and stop generation at any time—ensuring full control over sessions.
- 🔧 **TOOL_SERVERS / TOOL_SERVER_CONNECTIONS Indexing Issue Fixed**: Fixed a mismatch between tool lists and their access paths—restoring full function and preventing confusion in tool management.
- 🔐 **LDAP Login Handles Multiple Emails**: When LDAP returns multiple email attributes, the first valid one is now used—ensuring login success and account consistency.
- 🧩 **Model Visibility Toggle Fix**: Toggling model visibility now works even for untouched models—letting admins smoothly manage user access across base models.
- ⚙️ **Cross-Origin manifest.json Now Loads Properly**: Compatibility issues with Cloudflare Zero Trust (and others) resolved, allowing manifest.json to load behind authenticated proxies.

### Changed

- 🔒 **Default Access Scopes Set to Private for All Resources**: Models, tools, and knowledge are now private by default when created—ensuring better baseline security and visibility controls.
- 🧱 **General Backend Refactoring for Stability**: Numerous invisible improvements enhance backend scalability, security, and maintainability—powering upcoming features with a stronger foundation.
- 🧩 **Stable Dependency Upgrades**: Updated key platform libraries—Chromadb (0.6.3), pgvector (0.4.0), Azure Identity (1.21.0), and Youtube Transcript API (1.0.3)—for improved compatibility, functionality, and security.

## [0.6.2] - 2025-04-06

### Added

- 🌍 **Improved Global Language Support**: Expanded and refined translations across multiple languages to enhance clarity and consistency for international users.

### Fixed

- 🛠️ **Accurate Tool Descriptions from OpenAPI Servers**: External tools now use full endpoint descriptions instead of summaries when generating tool specifications—helping AI models understand tool purpose more precisely and choose the right tool more accurately in tool workflows.
- 🔧 **Precise Web Results Source Attribution**: Fixed a key issue where all web search results showed the same source ID—now each result gets its correct and distinct source, ensuring accurate citations and traceability.
- 🔍 **Clean Web Search Retrieval**: Web search now retains only results from URLs where real content was successfully fetched—improving accuracy and removing empty or broken links from citations.
- 🎵 **Audio File Upload Response Restored**: Resolved an issue where uploading audio files did not return valid responses, restoring smooth file handling for transcription and audio-based workflows.

### Changed

- 🧰 **General Backend Refactoring**: Multiple behind-the-scenes improvements streamline backend performance, reduce complexity, and ensure a more stable, maintainable system overall—making everything smoother without changing your workflow.

## [0.6.1] - 2025-04-05

```

----------------------------------------

TITLE: Storing User Settings Persistently in Open WebUI (config.json)
DESCRIPTION: Identifies 'config.json' as the file used by Open WebUI (added in v0.1.125) to persistently store user settings locally. This ensures settings are saved for convenience.
SOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_9

LANGUAGE: json
CODE:
```
config.json
```

----------------------------------------

TITLE: Disabling Authentication using WEBUI_AUTH Environment Variable
DESCRIPTION: Explains how to disable user authentication in Open WebUI by setting the 'WEBUI_AUTH' environment variable to False (added in v0.1.124). This option is intended only for fresh installations without existing users.
SOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_10

LANGUAGE: plaintext
CODE:
```
WEBUI_AUTH
```

----------------------------------------

TITLE: Enabling Configuration Reset on Startup (Environment Variable)
DESCRIPTION: Introduced in version 0.2.2, setting the `RESET_CONFIG_ON_START` environment variable instructs Open WebUI to reset its configuration settings to their default values upon application startup. This provides an easy way to revert any custom configurations without manual intervention.
SOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_3

LANGUAGE: plaintext
CODE:
```
RESET_CONFIG_ON_START
```

----------------------------------------

TITLE: Managing Bundled LiteLLM with ENABLE_LITELLM Environment Variable
DESCRIPTION: Describes the 'ENABLE_LITELLM' environment variable (mentioned in v0.1.122 documentation context) potentially used to manage memory usage related to the (now deprecated) bundled LiteLLM support in Open WebUI. Setting this likely controls whether LiteLLM-related components are loaded.
SOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_12

LANGUAGE: plaintext
CODE:
```
ENABLE_LITELLM
```

----------------------------------------

TITLE: Exporting LiteLLM Configuration File from Open WebUI
DESCRIPTION: Specifies the 'LiteLLM config.yaml' file which contains LiteLLM configuration settings. As bundled LiteLLM support is deprecated (v0.1.125), users migrating can export this file from admin settings > database > export LiteLLM config.yaml to use with a self-hosted LiteLLM instance.
SOURCE: https://github.com/open-webui/open-webui/blob/main/CHANGELOG.md#_snippet_7

LANGUAGE: yaml
CODE:
```
LiteLLM config.yaml
```

----------------------------------------

TITLE: Configuring robots.txt to Block All Crawlers
DESCRIPTION: This robots.txt configuration uses the wildcard `*` to target all user agents (web crawlers) and the `/` directive with `Disallow` to instruct them not to crawl any part of the website. This effectively prevents search engines and other compliant bots from indexing or accessing the site.
SOURCE: https://github.com/open-webui/open-webui/blob/main/static/robots.txt#_snippet_0

LANGUAGE: text
CODE:
```
User-agent: *
Disallow: /
```

----------------------------------------

TITLE: Running Open WebUI Docker Container with Host Network
DESCRIPTION: This Bash command runs the Open WebUI Docker container using the host's network stack (`--network=host`) to resolve common connection issues where the container cannot reach the Ollama server (typically running on the host). It maps a volume for data persistence, sets the necessary `OLLAMA_BASE_URL` environment variable to point to the Ollama API endpoint accessible from the host, names the container, and ensures it restarts automatically. Using `--network=host` changes the access port from the default 3000 to 8080.
SOURCE: https://github.com/open-webui/open-webui/blob/main/TROUBLESHOOTING.md#_snippet_0

LANGUAGE: bash
CODE:
```
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```